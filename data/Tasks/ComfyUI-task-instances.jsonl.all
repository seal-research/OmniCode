{"repo": "comfyanonymous/ComfyUI", "pull_number": 6300, "instance_id": "comfyanonymous__ComfyUI-6300", "issue_numbers": ["3539"], "base_commit": "1c99734e5abe0bb8169027155c62c54b9eb9ea47", "patch": "diff --git a/latent_preview.py b/latent_preview.py\nindex 07f9cc68e97..95d3cb7338e 100644\n--- a/latent_preview.py\n+++ b/latent_preview.py\n@@ -12,7 +12,10 @@\n def preview_to_image(latent_image):\n         latents_ubyte = (((latent_image + 1.0) / 2.0).clamp(0, 1)  # change scale from -1..1 to 0..1\n                             .mul(0xFF)  # to 0..255\n-                            ).to(device=\"cpu\", dtype=torch.uint8, non_blocking=comfy.model_management.device_supports_non_blocking(latent_image.device))\n+                            )\n+        if comfy.model_management.directml_enabled:\n+                latents_ubyte = latents_ubyte.to(dtype=torch.uint8)\n+        latents_ubyte = latents_ubyte.to(device=\"cpu\", dtype=torch.uint8, non_blocking=comfy.model_management.device_supports_non_blocking(latent_image.device))\n \n         return Image.fromarray(latents_ubyte.numpy())\n \n", "test_patch": "", "problem_statement": "Cannot handle this data type: (1, 1, 3), <f4\nHi,\r\n\r\nAfter 2 days without using, I updated comfyUI and now I get this error when I try to sample anything, seemingly happens when it tries to show a preview:\r\n\r\n!!! Exception during processing!!! Cannot handle this data type: (1, 1, 3), <f4\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\maveyyl\\AppData\\Roaming\\StabilityMatrix\\Packages\\ComfyUI\\venv\\lib\\site-packages\\PIL\\Image.py\", line 3130, in fromarray\r\n    mode, rawmode = _fromarray_typemap[typekey]\r\nKeyError: ((1, 1, 3), '<f4')\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\maveyyl\\AppData\\Roaming\\StabilityMatrix\\Packages\\ComfyUI\\execution.py\", line 151, in recursive_execute\r\n    output_data, output_ui = get_output_data(obj, input_data_all)\r\n  File \"C:\\Users\\maveyyl\\AppData\\Roaming\\StabilityMatrix\\Packages\\ComfyUI\\execution.py\", line 81, in get_output_data\r\n    return_values = map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True)\r\n  File \"C:\\Users\\maveyyl\\AppData\\Roaming\\StabilityMatrix\\Packages\\ComfyUI\\execution.py\", line 74, in map_node_over_list\r\n    results.append(getattr(obj, func)(**slice_dict(input_data_all, i)))\r\n  File \"C:\\Users\\maveyyl\\AppData\\Roaming\\StabilityMatrix\\Packages\\ComfyUI\\nodes.py\", line 1344, in sample\r\n    return common_ksampler(model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=denoise)\r\n  File \"C:\\Users\\maveyyl\\AppData\\Roaming\\StabilityMatrix\\Packages\\ComfyUI\\nodes.py\", line 1314, in common_ksampler\r\n    samples = comfy.sample.sample(model, noise, steps, cfg, sampler_name, scheduler, positive, negative, latent_image,\r\n  File \"C:\\Users\\maveyyl\\AppData\\Roaming\\StabilityMatrix\\Packages\\ComfyUI\\custom_nodes\\ComfyUI-Impact-Pack\\modules\\impact\\sample_error_enhancer.py\", line 9, in informative_sample\r\n    return original_sample(*args, **kwargs)  # This code helps interpret error messages that occur within exceptions but does not have any impact on other operations.\r\n  File \"C:\\Users\\maveyyl\\AppData\\Roaming\\StabilityMatrix\\Packages\\ComfyUI\\custom_nodes\\ComfyUI-AnimateDiff-Evolved\\animatediff\\sampling.py\", line 313, in motion_sample\r\n    return orig_comfy_sample(model, noise, *args, **kwargs)\r\n  File \"C:\\Users\\maveyyl\\AppData\\Roaming\\StabilityMatrix\\Packages\\ComfyUI\\comfy\\sample.py\", line 37, in sample\r\n    samples = sampler.sample(noise, positive, negative, cfg=cfg, latent_image=latent_image, start_step=start_step, last_step=last_step, force_full_denoise=force_full_denoise, denoise_mask=noise_mask, sigmas=sigmas, callback=callback, disable_pbar=disable_pbar, seed=seed)\r\n  File \"C:\\Users\\maveyyl\\AppData\\Roaming\\StabilityMatrix\\Packages\\ComfyUI\\comfy\\samplers.py\", line 761, in sample\r\n    return sample(self.model, noise, positive, negative, cfg, self.device, sampler, sigmas, self.model_options, latent_image=latent_image, denoise_mask=denoise_mask, callback=callback, disable_pbar=disable_pbar, seed=seed)\r\n  File \"C:\\Users\\maveyyl\\AppData\\Roaming\\StabilityMatrix\\Packages\\ComfyUI\\comfy\\samplers.py\", line 663, in sample\r\n    return cfg_guider.sample(noise, latent_image, sampler, sigmas, denoise_mask, callback, disable_pbar, seed)\r\n  File \"C:\\Users\\maveyyl\\AppData\\Roaming\\StabilityMatrix\\Packages\\ComfyUI\\comfy\\samplers.py\", line 650, in sample\r\n    output = self.inner_sample(noise, latent_image, device, sampler, sigmas, denoise_mask, callback, disable_pbar, seed)\r\n  File \"C:\\Users\\maveyyl\\AppData\\Roaming\\StabilityMatrix\\Packages\\ComfyUI\\comfy\\samplers.py\", line 629, in inner_sample\r\n    samples = sampler.sample(self, sigmas, extra_args, callback, noise, latent_image, denoise_mask, disable_pbar)\r\n  File \"C:\\Users\\maveyyl\\AppData\\Roaming\\StabilityMatrix\\Packages\\ComfyUI\\comfy\\samplers.py\", line 534, in sample\r\n    samples = self.sampler_function(model_k, noise, sigmas, extra_args=extra_args, callback=k_callback, disable=disable_pbar, **self.extra_options)\r\n  File \"C:\\Users\\maveyyl\\AppData\\Roaming\\StabilityMatrix\\Packages\\ComfyUI\\venv\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\maveyyl\\AppData\\Roaming\\StabilityMatrix\\Packages\\ComfyUI\\comfy\\k_diffusion\\sampling.py\", line 585, in sample_dpmpp_2m\r\n    callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\r\n  File \"C:\\Users\\maveyyl\\AppData\\Roaming\\StabilityMatrix\\Packages\\ComfyUI\\comfy\\samplers.py\", line 532, in <lambda>\r\n    k_callback = lambda x: callback(x[\"i\"], x[\"denoised\"], x[\"x\"], total_steps)\r\n  File \"C:\\Users\\maveyyl\\AppData\\Roaming\\StabilityMatrix\\Packages\\ComfyUI\\latent_preview.py\", line 94, in callback\r\n    preview_bytes = previewer.decode_latent_to_preview_image(preview_format, x0)\r\n  File \"C:\\Users\\maveyyl\\AppData\\Roaming\\StabilityMatrix\\Packages\\ComfyUI\\latent_preview.py\", line 18, in decode_latent_to_preview_image\r\n    preview_image = self.decode_latent_to_preview(x0)\r\n  File \"C:\\Users\\maveyyl\\AppData\\Roaming\\StabilityMatrix\\Packages\\ComfyUI\\latent_preview.py\", line 48, in decode_latent_to_preview\r\n    return Image.fromarray(latents_ubyte.numpy())\r\n  File \"C:\\Users\\maveyyl\\AppData\\Roaming\\StabilityMatrix\\Packages\\ComfyUI\\venv\\lib\\site-packages\\PIL\\Image.py\", line 3134, in fromarray\r\n    raise TypeError(msg) from e\r\nTypeError: Cannot handle this data type: (1, 1, 3), <f4\n", "hints_text": "After investigating a little the file latent_preview.py, function decode_latent_to_preview, which was modified a few days ago, the values are transformed into [0.0, 255.0], but the dtype stays torch.float32 instead of becoming torch.uint8.\r\n\r\nFor some reason the \"to\" method doesn't do the type change unless you do it alone like this:\r\n\r\n```Python\r\n        latents_ubyte = (((latent_image + 1) / 2)\r\n                            .clamp(0, 1)  # change scale from -1..1 to 0..1\r\n                            .mul(0xFF)  # to 0..255\r\n                            )\r\n        latents_ubyte = latents_ubyte.to(dtype=torch.uint8)\r\n        latents_ubyte = latents_ubyte.to(device=\"cpu\", dtype=torch.uint8, non_blocking=True)\r\n```\r\n\r\nNot sure if it doesn't beat the purpose though. Hope it helps.\nThe OS fix doesn't work for my windows 11 + AMD CPU + AMD GPU.\n@Maveyyl Thanks for the `latent_preview.py` clue. I tried yours but the result is blank. Tried using:\r\n```py\r\n        latents_ubyte = (((latent_image + 1) / 2)\r\n                            .clamp(0, 1)  # change scale from -1..1 to 0..1\r\n                            .mul(0xFF)  # to 0..255\r\n                            )\r\n        latents_ubyte = latents_ubyte.to(dtype=torch.uint8)\r\n        latents_ubyte = latents_ubyte.to(device=\"cpu\", dtype=torch.uint8, non_blocking=comfy.model_management.device_supports_non_blocking(latent_image.device))\r\n```\r\nand it's working perfectly. I'm not sure why though maybe this issue is AMD specific, but I hope this helps for others.\r\n\r\n```\r\nOS: Windows 10 x86_64\r\nCPU: AMD Ryzen 7 5700X (16) @ 3.393GHz\r\nGPU: AMD Radeon RX 6700 XT\r\n```\nI could almost guarantee that AMD devices don't support non-blocking anything on Windows (especially not with DirectML).  \r\nNone of the OpenCL extensions required to do it are there, the only way you'd get something like it is resizable bar enabled but since that's cache coherent I don't think the device itself considers it non-blocking even if the CPU can unless it needs to access it.   Knowing the DirectML backend, setting it to true uses the flag anyway but incorrectly and doesn't wait until it finishes when the  CPU tries to access it like it should which results in broken images.  \nIf that's the case the right fix is adding a:\r\n```\r\n    if directml_enabled:\r\n        return False\r\n```\r\nHere: https://github.com/comfyanonymous/ComfyUI/blob/master/comfy/model_management.py#L630\n> If that's the case the right fix is adding a:\r\n> \r\n> ```\r\n>     if directml_enabled:\r\n>         return False\r\n> ```\r\n> \r\n> Here: https://github.com/comfyanonymous/ComfyUI/blob/master/comfy/model_management.py#L630\r\n\r\nCan confirm this is working on my Radeon 6600XT\nAfter trying all of the suggested solutions, the only thing that worked was redownload and replace the `latent_preview.py` for an older one. In my case the one from 11 Mar 2024 was enough. It might not be a solution, but it's workaround for the moment.\nHaving same Issue, I'm using Stability Matrix with ComfyUI.  It used to work but I removed a few weeks ago and decide to try again.  CPU: AMD Ryzen 5 5600 and GPU: AMD Radeon RX 6700XT.\r\n\r\nERRORS: \r\n\r\nG:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\venv\\lib\\site-packages\\torch\\_dynamo\\external_utils.py:17: UserWarning: Set seed for `privateuseone` device does not take effect, please add API's `_is_in_bad_fork` and `manual_seed_all` to `privateuseone` device module.\r\n  return fn(*args, **kwargs)\r\nRequested to load BaseModel\r\nLoading 1 new model\r\nloading in lowvram mode 64.0\r\nG:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\comfy\\samplers.py:655: UserWarning: The operator 'aten::count_nonzero.dim_IntList' is not currently supported on the DML backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at C:\\__w\\1\\s\\pytorch-directml-plugin\\torch_directml\\csrc\\dml\\dml_cpu_fallback.cpp:17.)\r\n  if latent_image is not None and torch.count_nonzero(latent_image) > 0: #Don't shift the empty latent image.\r\n  0%|          | 0/20 [00:08<?, ?it/s]\r\n!!! Exception during processing!!! Cannot handle this data type: (1, 1, 3), <f4\r\nTraceback (most recent call last):\r\n  File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\venv\\lib\\site-packages\\PIL\\Image.py\", line 3130, in fromarray\r\n    mode, rawmode = _fromarray_typemap[typekey]\r\nKeyError: ((1, 1, 3), '<f4')\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\execution.py\", line 151, in recursive_execute\r\n    output_data, output_ui = get_output_data(obj, input_data_all)\r\n  File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\execution.py\", line 81, in get_output_data\r\n    return_values = map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True)\r\n  File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\execution.py\", line 74, in map_node_over_list\r\n    results.append(getattr(obj, func)(**slice_dict(input_data_all, i)))\r\n  File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\nodes.py\", line 1355, in sample\r\n    return common_ksampler(model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=denoise)\r\n  File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\nodes.py\", line 1325, in common_ksampler\r\n    samples = comfy.sample.sample(model, noise, steps, cfg, sampler_name, scheduler, positive, negative, latent_image,\r\n  File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\comfy\\sample.py\", line 43, in sample\r\n    samples = sampler.sample(noise, positive, negative, cfg=cfg, latent_image=latent_image, start_step=start_step, last_step=last_step, force_full_denoise=force_full_denoise, denoise_mask=noise_mask, sigmas=sigmas, callback=callback, disable_pbar=disable_pbar, seed=seed)\r\n  File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\comfy\\samplers.py\", line 794, in sample\r\n    return sample(self.model, noise, positive, negative, cfg, self.device, sampler, sigmas, self.model_options, latent_image=latent_image, denoise_mask=denoise_mask, callback=callback, disable_pbar=disable_pbar, seed=seed)\r\n  File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\comfy\\samplers.py\", line 696, in sample\r\n    return cfg_guider.sample(noise, latent_image, sampler, sigmas, denoise_mask, callback, disable_pbar, seed)\r\n  File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\comfy\\samplers.py\", line 683, in sample\r\n    output = self.inner_sample(noise, latent_image, device, sampler, sigmas, denoise_mask, callback, disable_pbar, seed)\r\n  File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\comfy\\samplers.py\", line 662, in inner_sample\r\n    samples = sampler.sample(self, sigmas, extra_args, callback, noise, latent_image, denoise_mask, disable_pbar)\r\n  File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\comfy\\samplers.py\", line 567, in sample\r\n    samples = self.sampler_function(model_k, noise, sigmas, extra_args=extra_args, callback=k_callback, disable=disable_pbar, **self.extra_options)\r\n  File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\venv\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\comfy\\k_diffusion\\sampling.py\", line 140, in sample_euler\r\n    callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigma_hat, 'denoised': denoised})\r\n  File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\comfy\\samplers.py\", line 565, in <lambda>\r\n    k_callback = lambda x: callback(x[\"i\"], x[\"denoised\"], x[\"x\"], total_steps)\r\n  File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\latent_preview.py\", line 91, in callback\r\n    preview_bytes = previewer.decode_latent_to_preview_image(preview_format, x0)\r\n  File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\latent_preview.py\", line 26, in decode_latent_to_preview_image\r\n    preview_image = self.decode_latent_to_preview(x0)\r\n  File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\latent_preview.py\", line 45, in decode_latent_to_preview\r\n    return preview_to_image(latent_image)\r\n  File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\latent_preview.py\", line 19, in preview_to_image\r\n    return Image.fromarray(latents_ubyte.numpy())\r\n  File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\venv\\lib\\site-packages\\PIL\\Image.py\", line 3134, in fromarray\r\n    raise TypeError(msg) from e\r\nTypeError: Cannot handle this data type: (1, 1, 3), <f4\r\n\r\n \n> Having same Issue, I'm using Stability Matrix with ComfyUI. It used to work but I removed a few weeks ago and decide to try again. CPU: AMD Ryzen 5 5600 and GPU: AMD Radeon RX 6700XT.\r\n> \r\n> ERRORS:\r\n> \r\n> G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\venv\\lib\\site-packages\\torch_dynamo\\external_utils.py:17: UserWarning: Set seed for `privateuseone` device does not take effect, please add API's `_is_in_bad_fork` and `manual_seed_all` to `privateuseone` device module. return fn(*args, **kwargs) Requested to load BaseModel Loading 1 new model loading in lowvram mode 64.0 G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\comfy\\samplers.py:655: UserWarning: The operator 'aten::count_nonzero.dim_IntList' is not currently supported on the DML backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at C:__w\\1\\s\\pytorch-directml-plugin\\torch_directml\\csrc\\dml\\dml_cpu_fallback.cpp:17.) if latent_image is not None and torch.count_nonzero(latent_image) > 0: #Don't shift the empty latent image. 0%| | 0/20 [00:08<?, ?it/s] !!! Exception during processing!!! Cannot handle this data type: (1, 1, 3), <f4 Traceback (most recent call last): File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\venv\\lib\\site-packages\\PIL\\Image.py\", line 3130, in fromarray mode, rawmode = _fromarray_typemap[typekey] KeyError: ((1, 1, 3), '<f4')\r\n> \r\n> The above exception was the direct cause of the following exception:\r\n> \r\n> Traceback (most recent call last): File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\execution.py\", line 151, in recursive_execute output_data, output_ui = get_output_data(obj, input_data_all) File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\execution.py\", line 81, in get_output_data return_values = map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True) File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\execution.py\", line 74, in map_node_over_list results.append(getattr(obj, func)(**slice_dict(input_data_all, i))) File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\nodes.py\", line 1355, in sample return common_ksampler(model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=denoise) File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\nodes.py\", line 1325, in common_ksampler samples = comfy.sample.sample(model, noise, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\comfy\\sample.py\", line 43, in sample samples = sampler.sample(noise, positive, negative, cfg=cfg, latent_image=latent_image, start_step=start_step, last_step=last_step, force_full_denoise=force_full_denoise, denoise_mask=noise_mask, sigmas=sigmas, callback=callback, disable_pbar=disable_pbar, seed=seed) File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\comfy\\samplers.py\", line 794, in sample return sample(self.model, noise, positive, negative, cfg, self.device, sampler, sigmas, self.model_options, latent_image=latent_image, denoise_mask=denoise_mask, callback=callback, disable_pbar=disable_pbar, seed=seed) File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\comfy\\samplers.py\", line 696, in sample return cfg_guider.sample(noise, latent_image, sampler, sigmas, denoise_mask, callback, disable_pbar, seed) File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\comfy\\samplers.py\", line 683, in sample output = self.inner_sample(noise, latent_image, device, sampler, sigmas, denoise_mask, callback, disable_pbar, seed) File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\comfy\\samplers.py\", line 662, in inner_sample samples = sampler.sample(self, sigmas, extra_args, callback, noise, latent_image, denoise_mask, disable_pbar) File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\comfy\\samplers.py\", line 567, in sample samples = self.sampler_function(model_k, noise, sigmas, extra_args=extra_args, callback=k_callback, disable=disable_pbar, **self.extra_options) File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\venv\\lib\\site-packages\\torch\\utils_contextlib.py\", line 115, in decorate_context return func(*args, **kwargs) File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\comfy\\k_diffusion\\sampling.py\", line 140, in sample_euler callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigma_hat, 'denoised': denoised}) File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\comfy\\samplers.py\", line 565, in k_callback = lambda x: callback(x[\"i\"], x[\"denoised\"], x[\"x\"], total_steps) File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\latent_preview.py\", line 91, in callback preview_bytes = previewer.decode_latent_to_preview_image(preview_format, x0) File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\latent_preview.py\", line 26, in decode_latent_to_preview_image preview_image = self.decode_latent_to_preview(x0) File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\latent_preview.py\", line 45, in decode_latent_to_preview return preview_to_image(latent_image) File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\latent_preview.py\", line 19, in preview_to_image return Image.fromarray(latents_ubyte.numpy()) File \"G:\\SD\\StabilityMatrix\\Data\\Packages\\ComfyUI\\venv\\lib\\site-packages\\PIL\\Image.py\", line 3134, in fromarray raise TypeError(msg) from e TypeError: Cannot handle this data type: (1, 1, 3), <f4\r\n\r\nTry changing your `latent_preview.py` file method:\r\n\r\n```py\r\ndef preview_to_image(latent_image):\r\n        latents_ubyte = (((latent_image + 1) / 2)\r\n                            .clamp(0, 1)  # change scale from -1..1 to 0..1\r\n                            .mul(0xFF)  # to 0..255\r\n                            )\r\n        latents_ubyte = latents_ubyte.to(dtype=torch.uint8)\r\n        latents_ubyte = latents_ubyte.to(device=\"cpu\", dtype=torch.uint8, non_blocking=comfy.model_management.device_supports_non_blocking(latent_image.device))\r\n\r\n        return Image.fromarray(latents_ubyte.numpy())\r\n```\nI initially got the cannot handle data type error, and the fix above, updating the preview_to_image method in latent_preview.py got me passed that, but now I'm getting blank output.\r\n\r\nRyzne 7950x3d/Radeon 7900xtx\nSounds like a NaN issue or something else is going on. Can you share a screenshot? I get blank images from time to time which is almost always a driver issue. You can try to reset your GPU with this python script. You must run Python in an administrator window.\r\n\r\n```py\r\nimport subprocess\r\n\r\ndef restart_gpu_driver():\r\n    # Define the command to get the Instance ID of the GPU device\r\n    command = 'powershell \"Get-PnpDevice | Where-Object { ($_.Class -eq \\'Display\\' ) -and ($_.Status -eq \\'OK\\')} | Select-Object -ExpandProperty InstanceId\"'\r\n    \r\n    try:\r\n        # Get Instance ID and strip any extra whitespace or newline characters\r\n        instanceID = subprocess.check_output(command, shell=True, text=True).strip()\r\n        print(\"Running on the selected GPU: \" + instanceID)\r\n        \r\n        # Define the commands to disable and enable the GPU device\r\n        disable_command = f'powershell \"Disable-PnpDevice -InstanceId \\'{instanceID}\\' -Confirm:$false\"'\r\n        enable_command = f'powershell \"Enable-PnpDevice -InstanceId \\'{instanceID}\\' -Confirm:$false\"'\r\n        \r\n        # Disable the GPU device\r\n        subprocess.run(disable_command, shell=True, check=True)\r\n        print(\"GPU disabled successfully.\")\r\n        \r\n        # Enable the GPU device\r\n        subprocess.run(enable_command, shell=True, check=True)\r\n        print(\"GPU enabled successfully.\")\r\n    except subprocess.CalledProcessError as e:\r\n        print(f\"An error occurred: {e}\")\r\n\r\n# Call the function to restart the GPU driver\r\nrestart_gpu_driver()\r\n```\r\n\r\nyour screen will flash. This script usually fixes any NaN issues I have with my 6600xt.\n> If that's the case the right fix is adding a:\r\n> \r\n> ```\r\n>     if directml_enabled:\r\n>         return False\r\n> ```\r\n> \r\n> Here: https://github.com/comfyanonymous/ComfyUI/blob/master/comfy/model_management.py#L630\r\n\r\nthis ones helped in addition with fix in latent_preview.py (probalby even not needed)\nForcing uint8 was for AMD devices, specifically \nIt goes away if preview disabled via manager, suggestions above didn't work for me (RX570 8gb VRAM, 32gb RAM --directml)\r\nhttps://www.reddit.com/r/StableDiffusion/comments/1cx2sqg/cmfyui_typeerror_cannot_handle_this_data_type_1_1/\n@dnswd Thanks! I tried your and it working, my GPU is AMD rx580\r\n\r\n>         latents_ubyte = (((latent_image + 1) / 2)\r\n>                             .clamp(0, 1)  # change scale from -1..1 to 0..1\r\n>                             .mul(0xFF)  # to 0..255\r\n>                             )\r\n>         latents_ubyte = latents_ubyte.to(dtype=torch.uint8)\r\n>         latents_ubyte = latents_ubyte.to(device=\"cpu\", dtype=torch.uint8, non_blocking=comfy.model_management.device_supports_non_blocking(latent_image.device))\n@dnswd\r\n@zyzz15620\r\nIt's work to me too, my Radeon RX 5500 XT Ksampler previewer is working now, problem is fixed. Thank you guys.\nHello I am also having a quite similar issue, but \r\n\r\n> @Maveyyl Thanks for the `latent_preview.py` clue. I tried yours but the result is blank. Tried using:\r\n> \r\n> ```python\r\n>         latents_ubyte = (((latent_image + 1) / 2)\r\n>                             .clamp(0, 1)  # change scale from -1..1 to 0..1\r\n>                             .mul(0xFF)  # to 0..255\r\n>                             )\r\n>         latents_ubyte = latents_ubyte.to(dtype=torch.uint8)\r\n>         latents_ubyte = latents_ubyte.to(device=\"cpu\", dtype=torch.uint8, non_blocking=comfy.model_management.device_supports_non_blocking(latent_image.device))\r\n> ```\r\n> \r\n> and it's working perfectly. I'm not sure why though maybe this issue is AMD specific, but I hope this helps for others.\r\n> \r\n> ```\r\n> OS: Windows 10 x86_64\r\n> CPU: AMD Ryzen 7 5700X (16) @ 3.393GHz\r\n> GPU: AMD Radeon RX 6700 XT\r\n> ```\r\n\r\nI'm having a quite similar issue, but somehow it did not work for me!\r\n\r\nMy system:\r\n\r\n- Windows 10 64-bit\r\n- AMD Radeon RTX 7900 (GPU)\r\n- Over 40 GB of RAM\r\n- Using ComfyUI in a virtual environment VENV and python\r\nWhen using a Custom Node (Efficient Nodes) I get this error when trying a simple prompt:\r\n\r\n===============================================================\r\n\r\n\r\n# ComfyUI Error Report\r\n## Error Details\r\n- **Node Type:** KSampler Adv. (Efficient)\r\n- **Exception Type:** TypeError\r\n- **Exception Message:** Cannot handle this data type: (1, 1, 3), <f4\r\n## Stack Trace\r\n```\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\execution.py\", line 323, in execute\r\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\execution.py\", line 198, in get_output_data\r\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\execution.py\", line 169, in _map_node_over_list\r\n    process_inputs(input_dict, i)\r\n\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\execution.py\", line 158, in process_inputs\r\n    results.append(getattr(obj, func)(**inputs))\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\custom_nodes\\efficiency-nodes-comfyui\\efficiency_nodes.py\", line 2225, in sample_adv\r\n    return super().sample(model, noise_seed, steps, cfg, sampler_name, scheduler, positive, negative,\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\custom_nodes\\efficiency-nodes-comfyui\\efficiency_nodes.py\", line 732, in sample\r\n    samples, images, gifs, preview = process_latent_image(model, seed, steps, cfg, sampler_name, scheduler,\r\n                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\custom_nodes\\efficiency-nodes-comfyui\\efficiency_nodes.py\", line 554, in process_latent_image\r\n    samples = KSamplerAdvanced().sample(model, add_noise, seed, steps, cfg, sampler_name, scheduler,\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\nodes.py\", line 1471, in sample\r\n    return common_ksampler(model, noise_seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=denoise, disable_noise=disable_noise, start_step=start_at_step, last_step=end_at_step, force_full_denoise=force_full_denoise)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\nodes.py\", line 1404, in common_ksampler\r\n    samples = comfy.sample.sample(model, noise, steps, cfg, sampler_name, scheduler, positive, negative, latent_image,\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\comfy\\sample.py\", line 43, in sample\r\n    samples = sampler.sample(noise, positive, negative, cfg=cfg, latent_image=latent_image, start_step=start_step, last_step=last_step, force_full_denoise=force_full_denoise, denoise_mask=noise_mask, sigmas=sigmas, callback=callback, disable_pbar=disable_pbar, seed=seed)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\comfy\\samplers.py\", line 829, in sample\r\n    return sample(self.model, noise, positive, negative, cfg, self.device, sampler, sigmas, self.model_options, latent_image=latent_image, denoise_mask=denoise_mask, callback=callback, disable_pbar=disable_pbar, seed=seed)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\comfy\\samplers.py\", line 729, in sample\r\n    return cfg_guider.sample(noise, latent_image, sampler, sigmas, denoise_mask, callback, disable_pbar, seed)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\comfy\\samplers.py\", line 716, in sample\r\n    output = self.inner_sample(noise, latent_image, device, sampler, sigmas, denoise_mask, callback, disable_pbar, seed)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\comfy\\samplers.py\", line 695, in inner_sample\r\n    samples = sampler.sample(self, sigmas, extra_args, callback, noise, latent_image, denoise_mask, disable_pbar)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\comfy\\samplers.py\", line 600, in sample\r\n    samples = self.sampler_function(model_k, noise, sigmas, extra_args=extra_args, callback=k_callback, disable=disable_pbar, **self.extra_options)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\n  File \"C:\\Users\\Sigma\\miniconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\comfy\\k_diffusion\\sampling.py\", line 175, in sample_euler_ancestral\r\n    callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\r\n\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\comfy\\samplers.py\", line 598, in <lambda>\r\n    k_callback = lambda x: callback(x[\"i\"], x[\"denoised\"], x[\"x\"], total_steps)\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\latent_preview.py\", line 99, in callback\r\n    preview_bytes = previewer.decode_latent_to_preview_image(preview_format, x0)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\latent_preview.py\", line 26, in decode_latent_to_preview_image\r\n    preview_image = self.decode_latent_to_preview(x0)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\latent_preview.py\", line 53, in decode_latent_to_preview\r\n    return preview_to_image(latent_image)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\latent_preview.py\", line 19, in preview_to_image\r\n    return Image.fromarray(latents_ubyte.numpy())\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\n  File \"C:\\Users\\Sigma\\miniconda3\\Lib\\site-packages\\PIL\\Image.py\", line 3134, in fromarray\r\n    raise TypeError(msg) from e\r\n\r\n```\r\n## System Information\r\n- **ComfyUI Version:** v0.2.3-6-g191a0d5\r\n- **Arguments:** main.py --directml\r\n- **OS:** nt\r\n- **Python Version:** 3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 15:03:56) [MSC v.1929 64 bit (AMD64)]\r\n- **Embedded Python:** false\r\n- **PyTorch Version:** 2.3.1+cpu\r\n## Devices\r\n\r\n- **Name:** privateuseone\r\n  - **Type:** privateuseone\r\n  - **VRAM Total:** 1073741824\r\n  - **VRAM Free:** 1073741824\r\n  - **Torch VRAM Total:** 1073741824\r\n  - **Torch VRAM Free:** 1073741824\r\n\r\n## Logs\r\n```\r\n2024-10-13 15:40:50,512 - root - INFO - Using directml with device: \r\n2024-10-13 15:40:50,522 - root - INFO - Total VRAM 1024 MB, total RAM 49027 MB\r\n2024-10-13 15:40:50,523 - root - INFO - pytorch version: 2.3.1+cpu\r\n2024-10-13 15:40:50,524 - root - INFO - Set vram state to: NORMAL_VRAM\r\n2024-10-13 15:40:50,524 - root - INFO - Device: privateuseone\r\n2024-10-13 15:40:52,021 - root - INFO - Using sub quadratic optimization for cross attention, if you have memory or speed issues try using: --use-split-cross-attention\r\n2024-10-13 15:40:54,036 - root - INFO - [Prompt Server] web root: D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\web\r\n2024-10-13 15:40:55,246 - root - INFO - \r\nImport times for custom nodes:\r\n2024-10-13 15:40:55,246 - root - INFO -    0.0 seconds: D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\custom_nodes\\websocket_image_save.py\r\n2024-10-13 15:40:55,248 - root - INFO -    0.0 seconds: D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\custom_nodes\\efficiency-nodes-comfyui\r\n2024-10-13 15:40:55,248 - root - INFO -    0.2 seconds: D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\custom_nodes\\ComfyUI-Manager\r\n2024-10-13 15:40:55,249 - root - INFO - \r\n2024-10-13 15:40:55,259 - root - INFO - Starting server\r\n\r\n2024-10-13 15:40:55,260 - root - INFO - To see the GUI go to: http://127.0.0.1:8188\r\n2024-10-13 15:41:14,768 - root - INFO - got prompt\r\n2024-10-13 15:41:15,114 - root - INFO - model weight dtype torch.float32, manual cast: None\r\n2024-10-13 15:41:15,118 - root - INFO - model_type EPS\r\n2024-10-13 15:41:21,214 - root - INFO - Using split attention in VAE\r\n2024-10-13 15:41:21,216 - root - INFO - Using split attention in VAE\r\n2024-10-13 15:41:21,936 - root - INFO - Requested to load SDXLClipModel\r\n2024-10-13 15:41:21,937 - root - INFO - Loading 1 new model\r\n2024-10-13 15:41:21,959 - root - INFO - loaded completely 0.0 1560.802734375 True\r\n2024-10-13 15:41:23,639 - root - INFO - Requested to load SDXLClipModel\r\n2024-10-13 15:41:23,640 - root - INFO - Loading 1 new model\r\n2024-10-13 15:41:28,314 - root - INFO - Requested to load SDXL\r\n2024-10-13 15:41:28,315 - root - INFO - Loading 1 new model\r\n2024-10-13 15:41:32,510 - root - INFO - loaded completely 0.0 9794.096694946289 True\r\n2024-10-13 15:41:32,994 - root - ERROR - !!! Exception during processing !!! Cannot handle this data type: (1, 1, 3), <f4\r\n2024-10-13 15:41:33,000 - root - ERROR - Traceback (most recent call last):\r\n  File \"C:\\Users\\Sigma\\miniconda3\\Lib\\site-packages\\PIL\\Image.py\", line 3130, in fromarray\r\n    mode, rawmode = _fromarray_typemap[typekey]\r\n                    ~~~~~~~~~~~~~~~~~~^^^^^^^^^\r\nKeyError: ((1, 1, 3), '<f4')\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\execution.py\", line 323, in execute\r\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\execution.py\", line 198, in get_output_data\r\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\execution.py\", line 169, in _map_node_over_list\r\n    process_inputs(input_dict, i)\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\execution.py\", line 158, in process_inputs\r\n    results.append(getattr(obj, func)(**inputs))\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\custom_nodes\\efficiency-nodes-comfyui\\efficiency_nodes.py\", line 2225, in sample_adv\r\n    return super().sample(model, noise_seed, steps, cfg, sampler_name, scheduler, positive, negative,\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\custom_nodes\\efficiency-nodes-comfyui\\efficiency_nodes.py\", line 732, in sample\r\n    samples, images, gifs, preview = process_latent_image(model, seed, steps, cfg, sampler_name, scheduler,\r\n                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\custom_nodes\\efficiency-nodes-comfyui\\efficiency_nodes.py\", line 554, in process_latent_image\r\n    samples = KSamplerAdvanced().sample(model, add_noise, seed, steps, cfg, sampler_name, scheduler,\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\nodes.py\", line 1471, in sample\r\n    return common_ksampler(model, noise_seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=denoise, disable_noise=disable_noise, start_step=start_at_step, last_step=end_at_step, force_full_denoise=force_full_denoise)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\nodes.py\", line 1404, in common_ksampler\r\n    samples = comfy.sample.sample(model, noise, steps, cfg, sampler_name, scheduler, positive, negative, latent_image,\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\comfy\\sample.py\", line 43, in sample\r\n    samples = sampler.sample(noise, positive, negative, cfg=cfg, latent_image=latent_image, start_step=start_step, last_step=last_step, force_full_denoise=force_full_denoise, denoise_mask=noise_mask, sigmas=sigmas, callback=callback, disable_pbar=disable_pbar, seed=seed)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\comfy\\samplers.py\", line 829, in sample\r\n    return sample(self.model, noise, positive, negative, cfg, self.device, sampler, sigmas, self.model_options, latent_image=latent_image, denoise_mask=denoise_mask, callback=callback, disable_pbar=disable_pbar, seed=seed)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\comfy\\samplers.py\", line 729, in sample\r\n    return cfg_guider.sample(noise, latent_image, sampler, sigmas, denoise_mask, callback, disable_pbar, seed)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\comfy\\samplers.py\", line 716, in sample\r\n    output = self.inner_sample(noise, latent_image, device, sampler, sigmas, denoise_mask, callback, disable_pbar, seed)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\comfy\\samplers.py\", line 695, in inner_sample\r\n    samples = sampler.sample(self, sigmas, extra_args, callback, noise, latent_image, denoise_mask, disable_pbar)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\comfy\\samplers.py\", line 600, in sample\r\n    samples = self.sampler_function(model_k, noise, sigmas, extra_args=extra_args, callback=k_callback, disable=disable_pbar, **self.extra_options)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\Sigma\\miniconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\comfy\\k_diffusion\\sampling.py\", line 175, in sample_euler_ancestral\r\n    callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\comfy\\samplers.py\", line 598, in <lambda>\r\n    k_callback = lambda x: callback(x[\"i\"], x[\"denoised\"], x[\"x\"], total_steps)\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\latent_preview.py\", line 99, in callback\r\n    preview_bytes = previewer.decode_latent_to_preview_image(preview_format, x0)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\latent_preview.py\", line 26, in decode_latent_to_preview_image\r\n    preview_image = self.decode_latent_to_preview(x0)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\latent_preview.py\", line 53, in decode_latent_to_preview\r\n    return preview_to_image(latent_image)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Private\\ComfyUI Virtual Environment\\ComfyUI\\latent_preview.py\", line 19, in preview_to_image\r\n    return Image.fromarray(latents_ubyte.numpy())\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\Sigma\\miniconda3\\Lib\\site-packages\\PIL\\Image.py\", line 3134, in fromarray\r\n    raise TypeError(msg) from e\r\nTypeError: Cannot handle this data type: (1, 1, 3), <f4\r\n\r\n2024-10-13 15:41:33,006 - root - INFO - Prompt executed in 18.23 seconds\r\n```\r\n## Attached Workflow\r\nPlease make sure that workflow does not contain any sensitive information such as API keys or passwords.\r\n```\r\n{\"last_node_id\":41,\"last_link_id\":75,\"nodes\":[{\"id\":20,\"type\":\"Note\",\"pos\":{\"0\":540,\"1\":-1013},\"size\":{\"0\":646.016357421875,\"1\":370.7940368652344},\"flags\":{},\"order\":0,\"mode\":0,\"inputs\":[],\"outputs\":[],\"properties\":{\"text\":\"\"},\"widgets_values\":[\"= EMBEDDINGS =\\n\\n[Pony]\\n\\n.POSITIVES\\n\\nzPDXLrl, zPDXL2\\n\\n.NEGATIVES\\n\\nzPDXLrl-neg, zPDXL2-neg, negative_hand, negative_hand-neg\\n\\n= EMBEDDINGS =\\n\\n[SDXL 1.0]\\n\\n.POSITIVES\\n\\nZipRealism, Zip2D\\n\\n.NEGATIVES\\n\\nDeepNegative_xl_v1, worst quality, low quality, logo, text, \\nwatermark, username:1, ac_neg1, ac_neg2, ZipRealism_Neg\"],\"color\":\"#432\",\"bgcolor\":\"#653\"},{\"id\":29,\"type\":\"CLIPSetLastLayer\",\"pos\":{\"0\":-1519,\"1\":161},\"size\":{\"0\":315,\"1\":58},\"flags\":{},\"order\":7,\"mode\":0,\"inputs\":[{\"name\":\"clip\",\"type\":\"CLIP\",\"link\":44}],\"outputs\":[{\"name\":\"CLIP\",\"type\":\"CLIP\",\"links\":[47,52],\"slot_index\":0,\"shape\":3}],\"properties\":{\"Node name for S&R\":\"CLIPSetLastLayer\"},\"widgets_values\":[-2]},{\"id\":13,\"type\":\"CLIPSetLastLayer\",\"pos\":{\"0\":-1610,\"1\":1740},\"size\":{\"0\":315,\"1\":58},\"flags\":{},\"order\":6,\"mode\":0,\"inputs\":[{\"name\":\"clip\",\"type\":\"CLIP\",\"link\":26}],\"outputs\":[{\"name\":\"CLIP\",\"type\":\"CLIP\",\"links\":null,\"shape\":3}],\"properties\":{\"Node name for S&R\":\"CLIPSetLastLayer\"},\"widgets_values\":[-1]},{\"id\":23,\"type\":\"LoraLoader\",\"pos\":{\"0\":-448,\"1\":1575},\"size\":{\"0\":490.43994140625,\"1\":126},\"flags\":{},\"order\":16,\"mode\":0,\"inputs\":[{\"name\":\"model\",\"type\":\"MODEL\",\"link\":31},{\"name\":\"clip\",\"type\":\"CLIP\",\"link\":32}],\"outputs\":[{\"name\":\"MODEL\",\"type\":\"MODEL\",\"links\":[35],\"slot_index\":0,\"shape\":3},{\"name\":\"CLIP\",\"type\":\"CLIP\",\"links\":null,\"shape\":3}],\"properties\":{\"Node name for S&R\":\"LoraLoader\"},\"widgets_values\":[\"Styles_For_Pony_Twilight.safetensors\",0.8,1]},{\"id\":24,\"type\":\"LoraLoader\",\"pos\":{\"0\":-448,\"1\":1792},\"size\":{\"0\":481.14862060546875,\"1\":126},\"flags\":{},\"order\":18,\"mode\":0,\"inputs\":[{\"name\":\"model\",\"type\":\"MODEL\",\"link\":35},{\"name\":\"clip\",\"type\":\"CLIP\",\"link\":34}],\"outputs\":[{\"name\":\"MODEL\",\"type\":\"MODEL\",\"links\":[38],\"slot_index\":0,\"shape\":3},{\"name\":\"CLIP\",\"type\":\"CLIP\",\"links\":null,\"shape\":3}],\"properties\":{\"Node name for S&R\":\"LoraLoader\"},\"widgets_values\":[\"ThePit_Style_Pony.safetensors\",0.3,1]},{\"id\":26,\"type\":\"LoraLoader\",\"pos\":{\"0\":-441,\"1\":2239},\"size\":{\"0\":464.44384765625,\"1\":133.8055419921875},\"flags\":{},\"order\":21,\"mode\":0,\"inputs\":[{\"name\":\"model\",\"type\":\"MODEL\",\"link\":41},{\"name\":\"clip\",\"type\":\"CLIP\",\"link\":40}],\"outputs\":[{\"name\":\"MODEL\",\"type\":\"MODEL\",\"links\":[63],\"slot_index\":0,\"shape\":3},{\"name\":\"CLIP\",\"type\":\"CLIP\",\"links\":null,\"shape\":3}],\"properties\":{\"Node name for S&R\":\"LoraLoader\"},\"widgets_values\":[\"Nipple_Rings_Pony_XL.safetensors\",1,1]},{\"id\":25,\"type\":\"LoraLoader\",\"pos\":{\"0\":-442,\"1\":2025},\"size\":{\"0\":477.1378173828125,\"1\":126},\"flags\":{},\"order\":20,\"mode\":0,\"inputs\":[{\"name\":\"model\",\"type\":\"MODEL\",\"link\":38},{\"name\":\"clip\",\"type\":\"CLIP\",\"link\":37}],\"outputs\":[{\"name\":\"MODEL\",\"type\":\"MODEL\",\"links\":[41],\"slot_index\":0,\"shape\":3},{\"name\":\"CLIP\",\"type\":\"CLIP\",\"links\":null,\"shape\":3}],\"properties\":{\"Node name for S&R\":\"LoraLoader\"},\"widgets_values\":[\"Expressive_H-000001.safetensors\",0.25,1]},{\"id\":14,\"type\":\"CLIPSetLastLayer\",\"pos\":{\"0\":-1610,\"1\":1580},\"size\":{\"0\":315,\"1\":58},\"flags\":{},\"order\":5,\"mode\":0,\"inputs\":[{\"name\":\"clip\",\"type\":\"CLIP\",\"link\":25}],\"outputs\":[{\"name\":\"CLIP\",\"type\":\"CLIP\",\"links\":[21,28,37,40,62],\"slot_index\":0,\"shape\":3}],\"properties\":{\"Node name for S&R\":\"CLIPSetLastLayer\"},\"widgets_values\":[-2]},{\"id\":5,\"type\":\"EmptyLatentImage\",\"pos\":{\"0\":338,\"1\":1816},\"size\":{\"0\":315,\"1\":106},\"flags\":{},\"order\":1,\"mode\":0,\"inputs\":[],\"outputs\":[{\"name\":\"LATENT\",\"type\":\"LATENT\",\"links\":[2],\"slot_index\":0}],\"properties\":{\"Node name for S&R\":\"EmptyLatentImage\"},\"widgets_values\":[1024,1024,1]},{\"id\":33,\"type\":\"LoraLoader\",\"pos\":{\"0\":-441,\"1\":2442},\"size\":{\"0\":451.3920593261719,\"1\":157.18954467773438},\"flags\":{},\"order\":22,\"mode\":0,\"inputs\":[{\"name\":\"model\",\"type\":\"MODEL\",\"link\":63},{\"name\":\"clip\",\"type\":\"CLIP\",\"link\":62}],\"outputs\":[{\"name\":\"MODEL\",\"type\":\"MODEL\",\"links\":[68],\"slot_index\":0,\"shape\":3},{\"name\":\"CLIP\",\"type\":\"CLIP\",\"links\":null,\"shape\":3}],\"properties\":{\"Node name for S&R\":\"LoraLoader\"},\"widgets_values\":[\"MeridaBraveXLP_Character-10.safetensors\",1,1]},{\"id\":22,\"type\":\"LoraLoader\",\"pos\":{\"0\":-455,\"1\":1346},\"size\":{\"0\":511.34735107421875,\"1\":126},\"flags\":{},\"order\":13,\"mode\":0,\"inputs\":[{\"name\":\"model\",\"type\":\"MODEL\",\"link\":29},{\"name\":\"clip\",\"type\":\"CLIP\",\"link\":28}],\"outputs\":[{\"name\":\"MODEL\",\"type\":\"MODEL\",\"links\":[31],\"slot_index\":0,\"shape\":3},{\"name\":\"CLIP\",\"type\":\"CLIP\",\"links\":null,\"shape\":3}],\"properties\":{\"Node name for S&R\":\"LoraLoader\"},\"widgets_values\":[\"detail-add-xl.safetensors\",2.5,1]},{\"id\":19,\"type\":\"LoraLoader\",\"pos\":{\"0\":-451,\"1\":1090},\"size\":{\"0\":524.8563842773438,\"1\":163.40432739257812},\"flags\":{},\"order\":9,\"mode\":0,\"inputs\":[{\"name\":\"model\",\"type\":\"MODEL\",\"link\":27},{\"name\":\"clip\",\"type\":\"CLIP\",\"link\":21}],\"outputs\":[{\"name\":\"MODEL\",\"type\":\"MODEL\",\"links\":[29],\"slot_index\":0,\"shape\":3},{\"name\":\"CLIP\",\"type\":\"CLIP\",\"links\":null,\"shape\":3}],\"properties\":{\"Node name for S&R\":\"LoraLoader\"},\"widgets_values\":[\"EnvyPonyPrettyEyes01.safetensors\",1,1.32]},{\"id\":30,\"type\":\"CLIPTextEncode\",\"pos\":{\"0\":-450,\"1\":110},\"size\":{\"0\":671.37548828125,\"1\":337.3836975097656},\"flags\":{},\"order\":11,\"mode\":0,\"inputs\":[{\"name\":\"clip\",\"type\":\"CLIP\",\"link\":52}],\"outputs\":[{\"name\":\"CONDITIONING\",\"type\":\"CONDITIONING\",\"links\":[53],\"slot_index\":0}],\"properties\":{\"Node name for S&R\":\"CLIPTextEncode\"},\"widgets_values\":[\"score_9, score_8_up, score_7_up, The woman's face should be gorgeous, and the skin should be ultra detailed. She should have a deep cleavage, (((massive sagging breasts))), (((dark areola))), partially exposed and cutout from the clothing. The image should be in a vertical layout and the camera settings should be set to 8k for ultra high resolution. hyper-realistic, highest quality, masterpiece, huge thick nipple rings, perfecteyes, expressiveh, GothelXLP, big hair, curly hair, blue eyes, crown, makeup, (nr, nipple rings), sex slave, naked, standing up, full body shot, masterpiece, ((high detailed eye iris)), perfect eye iris, ((wide hips)), (((huge pregnant belly))), (((pregnant))), (((overdue pregnancy))), (((thick golden navel ring piercing))), sexy gaze, sexy expression, sensual gaze, MeridaXLP, freckles, wavy hair, ginger hair, ginger, zPDXL2\"]},{\"id\":7,\"type\":\"CLIPTextEncode\",\"pos\":{\"0\":-460,\"1\":691},\"size\":{\"0\":651.00537109375,\"1\":240.86366271972656},\"flags\":{},\"order\":10,\"mode\":0,\"inputs\":[{\"name\":\"clip\",\"type\":\"CLIP\",\"link\":47}],\"outputs\":[{\"name\":\"CONDITIONING\",\"type\":\"CONDITIONING\",\"links\":[6],\"slot_index\":0}],\"properties\":{\"Node name for S&R\":\"CLIPTextEncode\"},\"widgets_values\":[\"score_6, score_5, score_4, pony, gaping, censored, furry, child, kid, chibi, 3d, photo, monochrome, elven ears, anime, multiple cocks, extra legs, extra hands, mutated legs, mutated hands, closed eyes, bad nipples, weird nipples, double nipple, covered breasts, covered nipples, low detailed iris, weird eyes, weird expression, surprised expression, startled face, skin artifacts, weird skin, cracked skin, segmented skin, fragmented skin, scaly skin, ((double navel)), extra leg, three legs, zPDXL2-neg, negative_hand, negative_hand-neg, ng_deepnegative_v1_75t, pony_negativeV2, boring_sdxl_v1, easynegative\"]},{\"id\":3,\"type\":\"KSampler\",\"pos\":{\"0\":1010,\"1\":1820},\"size\":{\"0\":315,\"1\":262},\"flags\":{},\"order\":23,\"mode\":0,\"inputs\":[{\"name\":\"model\",\"type\":\"MODEL\",\"link\":68},{\"name\":\"positive\",\"type\":\"CONDITIONING\",\"link\":53},{\"name\":\"negative\",\"type\":\"CONDITIONING\",\"link\":6},{\"name\":\"latent_image\",\"type\":\"LATENT\",\"link\":2}],\"outputs\":[{\"name\":\"LATENT\",\"type\":\"LATENT\",\"links\":[],\"slot_index\":0}],\"properties\":{\"Node name for S&R\":\"KSampler\"},\"widgets_values\":[302727281332961,\"randomize\",30,7,\"dpmpp_2m\",\"karras\",1]},{\"id\":4,\"type\":\"CheckpointLoaderSimple\",\"pos\":{\"0\":-2638,\"1\":770},\"size\":{\"0\":441.39984130859375,\"1\":164.43212890625},\"flags\":{},\"order\":2,\"mode\":0,\"inputs\":[],\"outputs\":[{\"name\":\"MODEL\",\"type\":\"MODEL\",\"links\":[27],\"slot_index\":0},{\"name\":\"CLIP\",\"type\":\"CLIP\",\"links\":[25,26,32,34,44],\"slot_index\":1},{\"name\":\"VAE\",\"type\":\"VAE\",\"links\":[],\"slot_index\":2}],\"properties\":{\"Node name for S&R\":\"CheckpointLoaderSimple\"},\"widgets_values\":[\"ponyDiffusionV6XL_v6StartWithThisOne.safetensors\"]},{\"id\":39,\"type\":\"Efficient Loader\",\"pos\":{\"0\":809,\"1\":-456},\"size\":{\"0\":615.7736206054688,\"1\":581.9341430664062},\"flags\":{},\"order\":3,\"mode\":0,\"inputs\":[{\"name\":\"lora_stack\",\"type\":\"LORA_STACK\",\"link\":null,\"shape\":7},{\"name\":\"cnet_stack\",\"type\":\"CONTROL_NET_STACK\",\"link\":null,\"shape\":7}],\"outputs\":[{\"name\":\"MODEL\",\"type\":\"MODEL\",\"links\":[73],\"slot_index\":0},{\"name\":\"CONDITIONING+\",\"type\":\"CONDITIONING\",\"links\":[72],\"slot_index\":1},{\"name\":\"CONDITIONING-\",\"type\":\"CONDITIONING\",\"links\":[71],\"slot_index\":2},{\"name\":\"LATENT\",\"type\":\"LATENT\",\"links\":[70],\"slot_index\":3},{\"name\":\"VAE\",\"type\":\"VAE\",\"links\":[69],\"slot_index\":4},{\"name\":\"CLIP\",\"type\":\"CLIP\",\"links\":null},{\"name\":\"DEPENDENCIES\",\"type\":\"DEPENDENCIES\",\"links\":null}],\"properties\":{\"Node name for S&R\":\"Efficient Loader\"},\"widgets_values\":[\"ponyDiffusionV6XL_v6StartWithThisOne.safetensors\",\"Baked VAE\",-1,\"None\",1,1,\"rainbow dash\",\"bad prompts, low quality, bad, horrible art, awful art, amateur art\",\"none\",\"comfy\",512,512,1],\"color\":\"#222233\",\"bgcolor\":\"#333355\",\"shape\":\"box\"},{\"id\":11,\"type\":\"ImageUpscaleWithModel\",\"pos\":{\"0\":2740,\"1\":160},\"size\":{\"0\":241.79998779296875,\"1\":46},\"flags\":{},\"order\":15,\"mode\":0,\"inputs\":[{\"name\":\"upscale_model\",\"type\":\"UPSCALE_MODEL\",\"link\":10},{\"name\":\"image\",\"type\":\"IMAGE\",\"link\":11}],\"outputs\":[{\"name\":\"IMAGE\",\"type\":\"IMAGE\",\"links\":[23],\"slot_index\":0,\"shape\":3}],\"properties\":{\"Node name for S&R\":\"ImageUpscaleWithModel\"},\"widgets_values\":[]},{\"id\":21,\"type\":\"ImageScaleBy\",\"pos\":{\"0\":3140,\"1\":190},\"size\":{\"0\":315,\"1\":82},\"flags\":{\"collapsed\":false},\"order\":17,\"mode\":0,\"inputs\":[{\"name\":\"image\",\"type\":\"IMAGE\",\"link\":23}],\"outputs\":[{\"name\":\"IMAGE\",\"type\":\"IMAGE\",\"links\":[24],\"slot_index\":0,\"shape\":3}],\"properties\":{\"Node name for S&R\":\"ImageScaleBy\"},\"widgets_values\":[\"nearest-exact\",2]},{\"id\":12,\"type\":\"SaveImage\",\"pos\":{\"0\":4140,\"1\":190},\"size\":{\"0\":315,\"1\":270},\"flags\":{},\"order\":19,\"mode\":0,\"inputs\":[{\"name\":\"images\",\"type\":\"IMAGE\",\"link\":24}],\"outputs\":[],\"properties\":{},\"widgets_values\":[\"ComfyUI\"]},{\"id\":9,\"type\":\"SaveImage\",\"pos\":{\"0\":3190,\"1\":-340},\"size\":{\"0\":210,\"1\":270},\"flags\":{},\"order\":14,\"mode\":0,\"inputs\":[{\"name\":\"images\",\"type\":\"IMAGE\",\"link\":9}],\"outputs\":[],\"properties\":{},\"widgets_values\":[\"ComfyUI\"]},{\"id\":8,\"type\":\"VAEDecode\",\"pos\":{\"0\":2370,\"1\":-50},\"size\":{\"0\":210,\"1\":46},\"flags\":{},\"order\":12,\"mode\":0,\"inputs\":[{\"name\":\"samples\",\"type\":\"LATENT\",\"link\":74},{\"name\":\"vae\",\"type\":\"VAE\",\"link\":75}],\"outputs\":[{\"name\":\"IMAGE\",\"type\":\"IMAGE\",\"links\":[9,11],\"slot_index\":0}],\"properties\":{\"Node name for S&R\":\"VAEDecode\"},\"widgets_values\":[]},{\"id\":40,\"type\":\"KSampler Adv. (Efficient)\",\"pos\":{\"0\":1596,\"1\":-411},\"size\":{\"0\":330,\"1\":422},\"flags\":{},\"order\":8,\"mode\":0,\"inputs\":[{\"name\":\"model\",\"type\":\"MODEL\",\"link\":73},{\"name\":\"positive\",\"type\":\"CONDITIONING\",\"link\":72},{\"name\":\"negative\",\"type\":\"CONDITIONING\",\"link\":71},{\"name\":\"latent_image\",\"type\":\"LATENT\",\"link\":70},{\"name\":\"optional_vae\",\"type\":\"VAE\",\"link\":69,\"shape\":7},{\"name\":\"script\",\"type\":\"SCRIPT\",\"link\":null,\"shape\":7}],\"outputs\":[{\"name\":\"MODEL\",\"type\":\"MODEL\",\"links\":null},{\"name\":\"CONDITIONING+\",\"type\":\"CONDITIONING\",\"links\":null},{\"name\":\"CONDITIONING-\",\"type\":\"CONDITIONING\",\"links\":null},{\"name\":\"LATENT\",\"type\":\"LATENT\",\"links\":[74],\"slot_index\":3},{\"name\":\"VAE\",\"type\":\"VAE\",\"links\":[75],\"slot_index\":4},{\"name\":\"IMAGE\",\"type\":\"IMAGE\",\"links\":null}],\"properties\":{\"Node name for S&R\":\"KSampler Adv. (Efficient)\"},\"widgets_values\":[\"enable\",234726052613130,\"randomize\",20,7,\"euler_ancestral\",\"normal\",0,10000,\"disable\",\"auto\",\"true\"],\"color\":\"#332222\",\"bgcolor\":\"#553333\",\"shape\":\"box\"},{\"id\":10,\"type\":\"UpscaleModelLoader\",\"pos\":{\"0\":2185,\"1\":201},\"size\":{\"0\":315,\"1\":58},\"flags\":{},\"order\":4,\"mode\":0,\"inputs\":[],\"outputs\":[{\"name\":\"UPSCALE_MODEL\",\"type\":\"UPSCALE_MODEL\",\"links\":[10],\"slot_index\":0,\"shape\":3}],\"properties\":{\"Node name for S&R\":\"UpscaleModelLoader\"},\"widgets_values\":[\"4xUltrasharp_4xUltrasharpV10.pth\"]}],\"links\":[[2,5,0,3,3,\"LATENT\"],[6,7,0,3,2,\"CONDITIONING\"],[9,8,0,9,0,\"IMAGE\"],[10,10,0,11,0,\"UPSCALE_MODEL\"],[11,8,0,11,1,\"IMAGE\"],[21,14,0,19,1,\"CLIP\"],[23,11,0,21,0,\"IMAGE\"],[24,21,0,12,0,\"IMAGE\"],[25,4,1,14,0,\"CLIP\"],[26,4,1,13,0,\"CLIP\"],[27,4,0,19,0,\"MODEL\"],[28,14,0,22,1,\"CLIP\"],[29,19,0,22,0,\"MODEL\"],[31,22,0,23,0,\"MODEL\"],[32,4,1,23,1,\"CLIP\"],[34,4,1,24,1,\"CLIP\"],[35,23,0,24,0,\"MODEL\"],[37,14,0,25,1,\"CLIP\"],[38,24,0,25,0,\"MODEL\"],[40,14,0,26,1,\"CLIP\"],[41,25,0,26,0,\"MODEL\"],[44,4,1,29,0,\"CLIP\"],[47,29,0,7,0,\"CLIP\"],[52,29,0,30,0,\"CLIP\"],[53,30,0,3,1,\"CONDITIONING\"],[62,14,0,33,1,\"CLIP\"],[63,26,0,33,0,\"MODEL\"],[68,33,0,3,0,\"MODEL\"],[69,39,4,40,4,\"VAE\"],[70,39,3,40,3,\"LATENT\"],[71,39,2,40,2,\"CONDITIONING\"],[72,39,1,40,1,\"CONDITIONING\"],[73,39,0,40,0,\"MODEL\"],[74,40,3,8,0,\"LATENT\"],[75,40,4,8,1,\"VAE\"]],\"groups\":[],\"config\":{},\"extra\":{\"ds\":{\"scale\":0.8264462809917361,\"offset\":[-428.83336378983165,589.5737936854584]}},\"version\":0.4}\r\n```\r\n\r\n## Additional Context\r\n(Please add any additional context or steps to reproduce the error here)\r\n\r\n==================================================================================\r\n\r\n\r\nAny ideas of what I can do to fix this?\nit's the same issue as before.\r\nyour error is coming from here:\r\nhttps://github.com/comfyanonymous/ComfyUI/blob/cc9cf6d1bd957d764ad418258b61d7e08187573b/latent_preview.py#L14\r\n\r\nChange the `preview_to_image` method in the `latent_preview.py` file to this:\r\n```py\r\ndef preview_to_image(latent_image):\r\n        latents_ubyte = (((latent_image + 1) / 2)\r\n                    .clamp(0, 1)  # change scale from -1..1 to 0..1\r\n                    .mul(0xFF)  # to 0..255\r\n                    )\r\n        latents_ubyte = latents_ubyte.to(dtype=torch.uint8)\r\n        latents_ubyte = latents_ubyte.to(device=\"cpu\", dtype=torch.uint8, non_blocking=comfy.model_management.device_supports_non_blocking(latent_image.device))\r\n\r\n        return Image.fromarray(latents_ubyte.numpy())\r\n```\r\n\r\nFor some reason our AMD gpus don't like the way it's coded. Converting it to an 8-bit unsigned int before dumping it on the CPU (if necessary) fixes it. Make sure you have the latest version of ComfyUI before editing the code.\n> it's the same issue as before. your error is coming from here:\r\n> \r\n> https://github.com/comfyanonymous/ComfyUI/blob/cc9cf6d1bd957d764ad418258b61d7e08187573b/latent_preview.py#L14\r\n> \r\n> Change the `preview_to_image` method in the `latent_preview.py` file to this:\r\n> \r\n> ```python\r\n> def preview_to_image(latent_image):\r\n>         latents_ubyte = (((latent_image + 1) / 2)\r\n>                     .clamp(0, 1)  # change scale from -1..1 to 0..1\r\n>                     .mul(0xFF)  # to 0..255\r\n>                     )\r\n>         latents_ubyte = latents_ubyte.to(dtype=torch.uint8)\r\n>         latents_ubyte = latents_ubyte.to(device=\"cpu\", dtype=torch.uint8, non_blocking=comfy.model_management.device_supports_non_blocking(latent_image.device))\r\n> \r\n>         return Image.fromarray(latents_ubyte.numpy())\r\n> ```\r\n> \r\n> For some reason our AMD gpus don't like the way it's coded. Converting it to an 8-bit unsigned int before dumping it on the CPU (if necessary) fixes it. Make sure you have the latest version of ComfyUI before editing the code.\r\n\r\nThis one finally worked for me too!\nHad perfectly working old install of Comfy UI. But naively pressed -  Update ALL.\r\nNow got non working K-Sampler with error:\r\n`KeyError: ((1, 1, 3), '<f4')\r\n`\r\nAfter 6 month - passed after the first post, nothing fixed and users must manually fix this shit code made by \"professionals\". Pathetic.. ", "created_at": "2024-12-31T21:15:48Z"}
{"repo": "comfyanonymous/ComfyUI", "pull_number": 4992, "instance_id": "comfyanonymous__ComfyUI-4992", "issue_numbers": ["4449"], "base_commit": "e7d47827364e628f1e89c7392ec00f9b78f6bca6", "patch": "diff --git a/comfy/k_diffusion/sampling.py b/comfy/k_diffusion/sampling.py\nindex e9e4edcc65f..70273d9d57d 100644\n--- a/comfy/k_diffusion/sampling.py\n+++ b/comfy/k_diffusion/sampling.py\n@@ -1154,3 +1154,36 @@ def post_cfg_function(args):\n         if sigmas[i + 1] > 0:\n             x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * s_noise * sigma_up\n     return x\n+\n+@torch.no_grad()\n+def sample_dpmpp_2m_cfg_pp(model, x, sigmas, extra_args=None, callback=None, disable=None):\n+    \"\"\"DPM-Solver++(2M).\"\"\"\n+    extra_args = {} if extra_args is None else extra_args\n+    s_in = x.new_ones([x.shape[0]])\n+    t_fn = lambda sigma: sigma.log().neg()\n+\n+    old_uncond_denoised = None\n+    uncond_denoised = None\n+    def post_cfg_function(args):\n+        nonlocal uncond_denoised\n+        uncond_denoised = args[\"uncond_denoised\"]\n+        return args[\"denoised\"]\n+    \n+    model_options = extra_args.get(\"model_options\", {}).copy()\n+    extra_args[\"model_options\"] = comfy.model_patcher.set_model_options_post_cfg_function(model_options, post_cfg_function, disable_cfg1_optimization=True)\n+\n+    for i in trange(len(sigmas) - 1, disable=disable):\n+        denoised = model(x, sigmas[i] * s_in, **extra_args)\n+        if callback is not None:\n+            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n+        t, t_next = t_fn(sigmas[i]), t_fn(sigmas[i + 1])\n+        h = t_next - t\n+        if old_uncond_denoised is None or sigmas[i + 1] == 0:\n+            denoised_mix = -torch.exp(-h) * uncond_denoised\n+        else:\n+            h_last = t - t_fn(sigmas[i - 1])\n+            r = h_last / h\n+            denoised_mix = -torch.exp(-h) * uncond_denoised - torch.expm1(-h) * (1 / (2 * r)) * (denoised - old_uncond_denoised)\n+        x = denoised + denoised_mix + torch.exp(-h) * x\n+        old_uncond_denoised = uncond_denoised\n+    return x\n\\ No newline at end of file\ndiff --git a/comfy/samplers.py b/comfy/samplers.py\nindex a20f65d4a3b..1ecb41dda56 100644\n--- a/comfy/samplers.py\n+++ b/comfy/samplers.py\n@@ -571,7 +571,7 @@ def max_denoise(self, model_wrap, sigmas):\n \n KSAMPLER_NAMES = [\"euler\", \"euler_cfg_pp\", \"euler_ancestral\", \"euler_ancestral_cfg_pp\", \"heun\", \"heunpp2\",\"dpm_2\", \"dpm_2_ancestral\",\n                   \"lms\", \"dpm_fast\", \"dpm_adaptive\", \"dpmpp_2s_ancestral\", \"dpmpp_2s_ancestral_cfg_pp\", \"dpmpp_sde\", \"dpmpp_sde_gpu\",\n-                  \"dpmpp_2m\", \"dpmpp_2m_sde\", \"dpmpp_2m_sde_gpu\", \"dpmpp_3m_sde\", \"dpmpp_3m_sde_gpu\", \"ddpm\", \"lcm\",\n+                  \"dpmpp_2m\", \"dpmpp_2m_cfg_pp\", \"dpmpp_2m_sde\", \"dpmpp_2m_sde_gpu\", \"dpmpp_3m_sde\", \"dpmpp_3m_sde_gpu\", \"ddpm\", \"lcm\",\n                   \"ipndm\", \"ipndm_v\", \"deis\"]\n \n class KSAMPLER(Sampler):\n", "test_patch": "", "problem_statement": "Add more CFG++ (cfgpp) samplers like DPM++ 2M, etc.\n### Feature Idea\n\nCFG++ (cfgpp) supports samplers other than Euler/DDIM (most notably DPM++ 2M), but ComfyUI only supports Euler CFG++.\r\n\r\nSee https://github.com/CFGpp-diffusion/CFGpp/blob/main/latent_sdxl.py#L642 for more info on implementation details.\n\n### Existing Solutions\n\nhttps://github.com/CFGpp-diffusion/CFGpp/blob/main/latent_sdxl.py#L642\n\n### Other\n\n_No response_\n", "hints_text": "", "created_at": "2024-09-20T04:53:34Z"}
{"repo": "comfyanonymous/ComfyUI", "pull_number": 4565, "instance_id": "comfyanonymous__ComfyUI-4565", "issue_numbers": ["4563"], "base_commit": "bb4416dd5b2d7c2f34dc17e18761dd6b3d8b6ead", "patch": "diff --git a/comfy/model_detection.py b/comfy/model_detection.py\nindex c05975cc91c..1edbcda4d0f 100644\n--- a/comfy/model_detection.py\n+++ b/comfy/model_detection.py\n@@ -472,9 +472,15 @@ def unet_config_from_diffusers_unet(state_dict, dtype=None):\n             'transformer_depth': [0, 1, 1], 'channel_mult': [1, 2, 4], 'transformer_depth_middle': -2, 'use_linear_in_transformer': False,\n             'context_dim': 768, 'num_head_channels': 64, 'transformer_depth_output': [0, 0, 1, 1, 1, 1],\n             'use_temporal_attention': False, 'use_temporal_resblock': False}\n+    \n+    SD15_diffusers_inpaint = {'use_checkpoint': False, 'image_size': 32, 'out_channels': 4, 'use_spatial_transformer': True, 'legacy': False, 'adm_in_channels': None,\n+            'dtype': dtype, 'in_channels': 9, 'model_channels': 320, 'num_res_blocks': [2, 2, 2, 2], 'transformer_depth': [1, 1, 1, 1, 1, 1, 0, 0],\n+            'channel_mult': [1, 2, 4, 4], 'transformer_depth_middle': 1, 'use_linear_in_transformer': False, 'context_dim': 768, 'num_heads': 8,\n+            'transformer_depth_output': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n+            'use_temporal_attention': False, 'use_temporal_resblock': False}  \n \n \n-    supported_models = [SDXL, SDXL_refiner, SD21, SD15, SD21_uncliph, SD21_unclipl, SDXL_mid_cnet, SDXL_small_cnet, SDXL_diffusers_inpaint, SSD_1B, Segmind_Vega, KOALA_700M, KOALA_1B, SD09_XS, SD_XS, SDXL_diffusers_ip2p]\n+    supported_models = [SDXL, SDXL_refiner, SD21, SD15, SD21_uncliph, SD21_unclipl, SDXL_mid_cnet, SDXL_small_cnet, SDXL_diffusers_inpaint, SSD_1B, Segmind_Vega, KOALA_700M, KOALA_1B, SD09_XS, SD_XS, SDXL_diffusers_ip2p, SD15_diffusers_inpaint]\n \n     for unet_config in supported_models:\n         matches = True\n", "test_patch": "", "problem_statement": "Loading SD15 Inpainting model in diffusers format checkpoint results in ERROR: Unsupported UNET\n### Expected Behavior\n\nModel loads correctly, such as when use a normal version of the model (In this case: raemumix_v81 was selected): \r\nmodel weight dtype torch.float16, manual cast: None\r\nmodel_type EPS\r\nclip missing: ['text_projection.weight']\r\nRequested to load SD1ClipModel\r\nLoading 1 new model\r\nloaded completely 0.0 235.84423828125 True\r\nC:\\Users\\Admin\\ComfyUI_windows_portable_nvidia\\ComfyUI_windows_portable\\ComfyUI\\comfy\\ldm\\modules\\attention.py:407: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\r\n  out = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=mask, dropout_p=0.0, is_causal=False)\r\nRequested to load AutoencoderKL\r\nLoading 1 new model\r\nloaded completely 0.0 159.55708122253418 True\r\nRequested to load BaseModel\r\nLoading 1 new model\r\nloaded completely 0.0 1639.406135559082 True\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:06<00:00,  3.09it/s]\r\nPrompt executed in 10.46 seconds\r\n\n\n### Actual Behavior\n\nRuntimeError: ERROR: Could not detect model type of: C:\\Users\\Admin\\ComfyUI_windows_portable_nvidia\\ComfyUI_windows_portable\\ComfyUI\\models\\diffusion_models\\raemumix_v81-inpainting.safetensors\r\n\n\n### Steps to Reproduce\n\n1. Use normal inpainting workflow, see image below: \r\n![image](https://github.com/user-attachments/assets/dbfa352d-d0d8-4045-ba45-e03bdebabcdf)\r\n2. Click render and see the error pop-up.\n\n### Debug Logs\n\n```powershell\nRequested to load SD1ClipModel\r\nLoading 1 new model\r\nloaded completely 0.0 235.84423828125 True\r\nC:\\Users\\Admin\\ComfyUI_windows_portable_nvidia\\ComfyUI_windows_portable\\ComfyUI\\comfy\\ldm\\modules\\attention.py:407: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\r\n  out = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=mask, dropout_p=0.0, is_causal=False)\r\nRequested to load AutoencoderKL\r\nLoading 1 new model\r\nloaded completely 0.0 159.55708122253418 True\r\nRequested to load BaseModel\r\nLoading 1 new model\r\nloaded completely 0.0 1639.406135559082 True\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:06<00:00,  3.09it/s]\r\nPrompt executed in 10.46 seconds\r\ngot prompt\r\nUsing pytorch attention in VAE\r\nUsing pytorch attention in VAE\r\nERROR UNSUPPORTED UNET C:\\Users\\Admin\\ComfyUI_windows_portable_nvidia\\ComfyUI_windows_portable\\ComfyUI\\models\\diffusion_models\\raemumix_v81-inpainting.safetensors\r\n!!! Exception during processing !!! ERROR: Could not detect model type of: C:\\Users\\Admin\\ComfyUI_windows_portable_nvidia\\ComfyUI_windows_portable\\ComfyUI\\models\\diffusion_models\\raemumix_v81-inpainting.safetensors\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Admin\\ComfyUI_windows_portable_nvidia\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 316, in execute\r\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\Admin\\ComfyUI_windows_portable_nvidia\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 191, in get_output_data\r\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\Admin\\ComfyUI_windows_portable_nvidia\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 168, in _map_node_over_list\r\n    process_inputs(input_dict, i)\r\n  File \"C:\\Users\\Admin\\ComfyUI_windows_portable_nvidia\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 157, in process_inputs\r\n    results.append(getattr(obj, func)(**inputs))\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\Admin\\ComfyUI_windows_portable_nvidia\\ComfyUI_windows_portable\\ComfyUI\\nodes.py\", line 877, in load_unet\r\n    model = comfy.sd.load_diffusion_model(unet_path, model_options=model_options)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\Admin\\ComfyUI_windows_portable_nvidia\\ComfyUI_windows_portable\\ComfyUI\\comfy\\sd.py\", line 676, in load_diffusion_model\r\n    raise RuntimeError(\"ERROR: Could not detect model type of: {}\".format(unet_path))\r\nRuntimeError: ERROR: Could not detect model type of: C:\\Users\\Admin\\ComfyUI_windows_portable_nvidia\\ComfyUI_windows_portable\\ComfyUI\\models\\diffusion_models\\raemumix_v81-inpainting.safetensors\n```\n\n\n### Other\n\n_No response_\n", "hints_text": "", "created_at": "2024-08-23T07:48:10Z"}
{"repo": "comfyanonymous/ComfyUI", "pull_number": 3915, "instance_id": "comfyanonymous__ComfyUI-3915", "issue_numbers": ["3885"], "base_commit": "521421f53ee1ba74304dfaa138b0f851093e1595", "patch": "diff --git a/web/scripts/changeTracker.js b/web/scripts/changeTracker.js\nindex 041c83122e0..39bc4a8104b 100644\n--- a/web/scripts/changeTracker.js\n+++ b/web/scripts/changeTracker.js\n@@ -173,9 +173,11 @@ export class ChangeTracker {\n \t\tconst onNodeAdded = LiteGraph.LGraph.prototype.onNodeAdded;\n \t\tLiteGraph.LGraph.prototype.onNodeAdded = function () {\n \t\t\tconst v = onNodeAdded?.apply(this, arguments);\n-\t\t\tconst ct = changeTracker();\n-\t\t\tif (!ct.isOurLoad) {\n-\t\t\t\tct.checkState();\n+\t\t\tif (!app?.configuringGraph) {\n+\t\t\t\tconst ct = changeTracker();\n+\t\t\t\tif (!ct.isOurLoad) {\n+\t\t\t\t\tct.checkState();\n+\t\t\t\t}\n \t\t\t}\n \t\t\treturn v;\n \t\t};\n", "test_patch": "", "problem_statement": "Adding node from searchbar isn't a tracked change\n### Expected Behavior\n\nAdding nodes via dblclick>search should dispatch graphChanged event.\n\n### Actual Behavior\n\nAdding a node from search bar isn't tracked by change tracker, graphChanged event isn't sent to listeners. New state isn't observed until a separate event triggers a new state check.\n\n### Steps to Reproduce\n\n1. Add console.log that prints on graphChanged events\r\n2. Double click graph -> Click to add node from search modal\r\n\r\nAlternatively, add two nodes consecutively this way, without inputs in between, then try to use the undo hotkey.\n\n### Debug Logs\n\n```powershell\n.\n```\n\n\n### Other\n\n_No response_\n", "hints_text": "", "created_at": "2024-07-01T02:09:03Z"}
{"repo": "comfyanonymous/ComfyUI", "pull_number": 2876, "instance_id": "comfyanonymous__ComfyUI-2876", "issue_numbers": ["2851", "2671"], "base_commit": "f81dbe26e2e363c28ad043db67b59c11bb33f446", "patch": "diff --git a/comfy/samplers.py b/comfy/samplers.py\nindex c795f208d80..a4a55951178 100644\n--- a/comfy/samplers.py\n+++ b/comfy/samplers.py\n@@ -276,6 +276,8 @@ def __init__(self, model):\n         self.inner_model = model\n     def forward(self, x, sigma, uncond, cond, cond_scale, denoise_mask, model_options={}, seed=None):\n         if denoise_mask is not None:\n+            if \"denoise_mask_function\" in model_options:\n+                denoise_mask = model_options[\"denoise_mask_function\"](sigma, denoise_mask)\n             latent_mask = 1. - denoise_mask\n             x = x * denoise_mask + (self.latent_image + self.noise * sigma.reshape([sigma.shape[0]] + [1] * (len(self.noise.shape) - 1))) * latent_mask\n         out = self.inner_model(x, sigma, cond=cond, uncond=uncond, cond_scale=cond_scale, model_options=model_options, seed=seed)\ndiff --git a/comfy_extras/nodes_differential_diffusion.py b/comfy_extras/nodes_differential_diffusion.py\nnew file mode 100644\nindex 00000000000..48c95602fff\n--- /dev/null\n+++ b/comfy_extras/nodes_differential_diffusion.py\n@@ -0,0 +1,97 @@\n+# code adapted from https://github.com/exx8/differential-diffusion\n+\n+import torch\n+import inspect\n+\n+class DifferentialDiffusion():\n+    @classmethod\n+    def INPUT_TYPES(s):\n+        return {\"required\": {\"model\": (\"MODEL\", ),\n+                            }}\n+    RETURN_TYPES = (\"MODEL\",)\n+    FUNCTION = \"apply\"\n+    CATEGORY = \"_for_testing\"\n+    INIT = False\n+\n+    @classmethod\n+    def IS_CHANGED(s, *args, **kwargs):\n+        DifferentialDiffusion.INIT = s.INIT = True\n+        return \"\"\n+\n+    def __init__(self) -> None:\n+        DifferentialDiffusion.INIT = False\n+        self.sigmas: torch.Tensor = None\n+        self.thresholds: torch.Tensor = None\n+        self.mask_i = None\n+        self.valid_sigmas = False\n+        self.varying_sigmas_samplers = [\"dpmpp_2s\", \"dpmpp_sde\", \"dpm_2\", \"heun\", \"restart\"]\n+\n+    def apply(self, model):\n+        model = model.clone()\n+        model.model_options[\"denoise_mask_function\"] = self.forward\n+        return (model,)\n+    \n+    def init_sigmas(self, sigma: torch.Tensor, denoise_mask: torch.Tensor):\n+        self.__init__()\n+        self.sigmas, sampler = find_outer_instance(\"sigmas\", callback=get_sigmas_and_sampler) or (None, \"\")\n+        self.valid_sigmas = not (\"sample_\" not in sampler or any(s in sampler for s in self.varying_sigmas_samplers)) or \"generic\" in sampler\n+        if self.sigmas is None:\n+            self.sigmas = sigma[:1].repeat(2)\n+            self.sigmas[-1].zero_()\n+        self.sigmas_min = self.sigmas.min()\n+        self.sigmas_max = self.sigmas.max()\n+        self.thresholds = torch.linspace(1, 0, self.sigmas.shape[0], dtype=sigma.dtype, device=sigma.device)\n+        self.thresholds_min_len = self.thresholds.shape[0] - 1\n+        if self.valid_sigmas:\n+            thresholds = self.thresholds[:-1].reshape(-1, 1, 1, 1, 1)\n+            mask = denoise_mask.unsqueeze(0)\n+            mask = (mask >= thresholds).to(denoise_mask.dtype)\n+            self.mask_i = iter(mask)\n+    \n+    def forward(self, sigma: torch.Tensor, denoise_mask: torch.Tensor):\n+        if self.sigmas is None or DifferentialDiffusion.INIT:\n+            self.init_sigmas(sigma, denoise_mask)\n+        if self.valid_sigmas:\n+            try:\n+                return next(self.mask_i)\n+            except StopIteration:\n+                self.valid_sigmas = False\n+        if self.thresholds_min_len > 1:\n+            nearest_idx = (self.sigmas - sigma[0]).abs().argmin()\n+            if not self.thresholds_min_len > nearest_idx:\n+                nearest_idx = -2\n+            threshold = self.thresholds[nearest_idx]\n+        else:\n+            threshold = (sigma[0] - self.sigmas_min) / (self.sigmas_max - self.sigmas_min)\n+        return (denoise_mask >= threshold).to(denoise_mask.dtype)\n+\n+def get_sigmas_and_sampler(frame, target):\n+    found = frame.f_locals[target]\n+    if isinstance(found, torch.Tensor) and found[-1] < 0.1:\n+        return found, frame.f_code.co_name\n+    return False\n+\n+def find_outer_instance(target: str, target_type=None, callback=None):\n+    frame = inspect.currentframe()\n+    i = 0\n+    while frame and i < 100:\n+        if target in frame.f_locals:\n+            if callback is not None:\n+                res = callback(frame, target)\n+                if res:\n+                    return res\n+            else:\n+                found = frame.f_locals[target]\n+                if isinstance(found, target_type):\n+                    return found\n+        frame = frame.f_back\n+        i += 1\n+    return None\n+\n+    \n+NODE_CLASS_MAPPINGS = {\n+    \"DifferentialDiffusion\": DifferentialDiffusion,\n+}\n+NODE_DISPLAY_NAME_MAPPINGS = {\n+    \"DifferentialDiffusion\": \"Differential Diffusion\",\n+}\ndiff --git a/nodes.py b/nodes.py\nindex a577c212628..b759d22cef2 100644\n--- a/nodes.py\n+++ b/nodes.py\n@@ -1961,6 +1961,7 @@ def init_custom_nodes():\n         \"nodes_photomaker.py\",\n         \"nodes_cond.py\",\n         \"nodes_stable_cascade.py\",\n+        \"nodes_differential_diffusion.py\",\n     ]\n \n     for node_file in extras_files:\n", "test_patch": "", "problem_statement": "Differential Diffusion: Giving Each Pixel Its Strength\nHello,\r\nI would like to suggest implementing my paper: Differential Diffusion: Giving Each Pixel Its Strength.\r\nThe paper allows a user to edit a picture by a change map that describes how much each region should change.\r\nThe editing process is typically guided by textual instructions, although it can also be applied without guidance.\r\nWe support both continuous and discrete editing.\r\nOur framework is training and fine tuning free! And has negligible penalty of the inference time.\r\nOur implementation is diffusers-based.\r\nWe already tested it on 4 different diffusion models (Kadinsky, DeepFloyd IF, SD, SD XL).\r\nWe are confident that the framework can also be ported to other diffusion models, such as SD Turbo, Stable Cascade, and amused.\r\nI notice that you usually stick to white==change convention, which is opposite to the convention we used in the paper.\r\nThe paper can be thought of as a generalization to some of the existing techniques.\r\nA black map is just regular txt2img (\"0\"),\r\nA map of one color (which isn't black) can be thought as img2img,\r\nA map of two colors which one color is white can be thought as inpaint.\r\nAnd the rest? It's completely new!\r\nIn the paper, we suggest some further applications such as soft inpainting and strength visualization.\r\n\r\nSite:\r\nhttps://differential-diffusion.github.io/\r\nPaper:\r\nhttps://differential-diffusion.github.io/paper.pdf\r\nRepo:\r\nhttps://github.com/exx8/differential-diffusion\nFeature Request: Support Differential Diffusion for inpainting.\nThis is a nice alternative to standard inpainting, it allows for the mask to be a gradient for control of strength on top of denoising.\r\n\r\n\r\nhttps://github.com/exx8/differential-diffusion\n", "hints_text": "So is the primary difference between this and something like the QRCodeMonster controlnet in speed?  The controlnets can slow things down quite a bit but it has the same effect in terms of masking out which areas will change when applied on a latent.  \r\n\r\nHere's a fun thought for you... have you considered allowing the use of normal maps as a method of describing directionality in the area with two channels and overall change with the intensity of the third?   I don't know how hard this would be but it would be iteresting if normals could be painted such that the direction of things like water flows, trees, grass, and hair would generally try to follow the specified direction.  It could probably be used to help fix things that tend to generate wrong on a regular basis (e.g. try the prompt \"smoking a cigarette\" on a character or human and it'll nearly always be floating but on the off chance it's in their mouth it's almost always backwards.  This would allow a bit of control.\nthis looks very cool, but apparently the code hasn\u2019t been released yet. \nTrue, but it's laid out quite neatly in the paper. I just don't have the chops yet to implement it.\nThey've now released their code!\r\nSince the time of my post, I learned of a PR that had been sitting on the automatic1111 repo to perform the same task, I don't know how it compares. https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/14208", "created_at": "2024-02-23T04:35:03Z"}
{"repo": "comfyanonymous/ComfyUI", "pull_number": 2859, "instance_id": "comfyanonymous__ComfyUI-2859", "issue_numbers": ["2858"], "base_commit": "18c151b3e3f6838fab4028e7a8ba526e30e610d3", "patch": "diff --git a/comfy_extras/nodes_perpneg.py b/comfy_extras/nodes_perpneg.py\nindex 45e4d418f4f..64bbc1dcd42 100644\n--- a/comfy_extras/nodes_perpneg.py\n+++ b/comfy_extras/nodes_perpneg.py\n@@ -35,7 +35,7 @@ def cfg_function(args):\n \n             pos = noise_pred_pos - noise_pred_nocond\n             neg = noise_pred_neg - noise_pred_nocond\n-            perp = ((torch.mul(pos, neg).sum())/(torch.norm(neg)**2)) * neg\n+            perp = neg - ((torch.mul(neg, pos).sum())/(torch.norm(pos)**2)) * pos\n             perp_neg = perp * neg_scale\n             cfg_result = noise_pred_nocond + cond_scale*(pos - perp_neg)\n             cfg_result = x - cfg_result\n", "test_patch": "", "problem_statement": "Perp-Neg implementation is wrong, parallel component not ignored\nhttps://github.com/comfyanonymous/ComfyUI/blob/18c151b3e3f6838fab4028e7a8ba526e30e610d3/comfy_extras/nodes_perpneg.py#L38-L40\r\n\r\nThe Perp-Neg node does not match the [paper](https://arxiv.org/pdf/2304.04968.pdf) (pytorch code in Appendix A.1).\r\nWhen positive and negative prompt are the same, the result should be the same as an empty negative prompt because the prompts are completely parallel (i.e. there is no perpendicular component).\r\n\r\nPositive: \"forest\"\r\nNegative: \"\"\r\n![ComfyUI_00194_](https://github.com/comfyanonymous/ComfyUI/assets/114889020/6c86c0d5-ec8b-478d-aa29-e41794bd3184)\r\n\r\nPositive: \"forest\"\r\nNegative: \"forest\"\r\n![ComfyUI_00195_](https://github.com/comfyanonymous/ComfyUI/assets/114889020/fa4f2938-bc7a-48cb-8750-f76e6426dd4e)\r\n\r\nI'll submit a PR in a bit.\n", "hints_text": "", "created_at": "2024-02-21T09:39:37Z"}
{"repo": "comfyanonymous/ComfyUI", "pull_number": 2255, "instance_id": "comfyanonymous__ComfyUI-2255", "issue_numbers": ["2242"], "base_commit": "57926635e8d84ae9eea4a0416cc75e363f5ede45", "patch": "diff --git a/web/extensions/core/undoRedo.js b/web/extensions/core/undoRedo.js\nindex c6613b0f02d..3cb137520f4 100644\n--- a/web/extensions/core/undoRedo.js\n+++ b/web/extensions/core/undoRedo.js\n@@ -71,24 +71,21 @@ function graphEqual(a, b, root = true) {\n }\n \n const undoRedo = async (e) => {\n+\tconst updateState = async (source, target) => {\n+\t\tconst prevState = source.pop();\n+\t\tif (prevState) {\n+\t\t\ttarget.push(activeState);\n+\t\t\tisOurLoad = true;\n+\t\t\tawait app.loadGraphData(prevState, false);\n+\t\t\tactiveState = prevState;\n+\t\t}\n+\t}\n \tif (e.ctrlKey || e.metaKey) {\n \t\tif (e.key === \"y\") {\n-\t\t\tconst prevState = redo.pop();\n-\t\t\tif (prevState) {\n-\t\t\t\tundo.push(activeState);\n-\t\t\t\tisOurLoad = true;\n-\t\t\t\tawait app.loadGraphData(prevState);\n-\t\t\t\tactiveState = prevState;\n-\t\t\t}\n+\t\t\tupdateState(redo, undo);\n \t\t\treturn true;\n \t\t} else if (e.key === \"z\") {\n-\t\t\tconst prevState = undo.pop();\n-\t\t\tif (prevState) {\n-\t\t\t\tredo.push(activeState);\n-\t\t\t\tisOurLoad = true;\n-\t\t\t\tawait app.loadGraphData(prevState);\n-\t\t\t\tactiveState = prevState;\n-\t\t\t}\n+\t\t\tupdateState(undo, redo);\n \t\t\treturn true;\n \t\t}\n \t}\ndiff --git a/web/scripts/app.js b/web/scripts/app.js\nindex 5faf41fb36b..d2a6f4de425 100644\n--- a/web/scripts/app.js\n+++ b/web/scripts/app.js\n@@ -1559,9 +1559,12 @@ export class ComfyApp {\n \t/**\n \t * Populates the graph with the specified workflow data\n \t * @param {*} graphData A serialized graph object\n+\t * @param { boolean } clean If the graph state, e.g. images, should be cleared\n \t */\n-\tasync loadGraphData(graphData) {\n-\t\tthis.clean();\n+\tasync loadGraphData(graphData, clean = true) {\n+\t\tif (clean !== false) {\n+\t\t\tthis.clean();\n+\t\t}\n \n \t\tlet reset_invalid_values = false;\n \t\tif (!graphData) {\n", "test_patch": "", "problem_statement": "Undo will clear all preview images, please fix it.\nmany preview images in the workflow can provide me with reference. When the seed does not change or the parameters related to the preview image remain unchanged, the preview image will not be refreshed, and only comfyui can be restarted to solve the problem. This is too troublesome, please fix it. Thank you\n", "hints_text": "", "created_at": "2023-12-11T12:52:13Z"}
{"repo": "comfyanonymous/ComfyUI", "pull_number": 2207, "instance_id": "comfyanonymous__ComfyUI-2207", "issue_numbers": ["2007"], "base_commit": "2db86b4676ed2b5c8551beea25dd2ef3fe3c4f66", "patch": "diff --git a/comfy_extras/nodes_hypertile.py b/comfy_extras/nodes_hypertile.py\nindex 0d7d4c95483..15736b835bb 100644\n--- a/comfy_extras/nodes_hypertile.py\n+++ b/comfy_extras/nodes_hypertile.py\n@@ -2,9 +2,10 @@\n \n import math\n from einops import rearrange\n-import random\n+# Use torch rng for consistency across generations\n+from torch import randint\n \n-def random_divisor(value: int, min_value: int, /, max_options: int = 1, counter = 0) -> int:\n+def random_divisor(value: int, min_value: int, /, max_options: int = 1) -> int:\n     min_value = min(min_value, value)\n \n     # All big divisors of value (inclusive)\n@@ -12,8 +13,7 @@ def random_divisor(value: int, min_value: int, /, max_options: int = 1, counter\n \n     ns = [value // i for i in divisors[:max_options]]  # has at least 1 element\n \n-    random.seed(counter)\n-    idx = random.randint(0, len(ns) - 1)\n+    idx = randint(low=0, high=len(ns) - 1, size=(1,)).item()\n \n     return ns[idx]\n \n@@ -42,7 +42,6 @@ def patch(self, model, tile_size, swap_size, max_depth, scale_depth):\n \n         latent_tile_size = max(32, tile_size) // 8\n         self.temp = None\n-        self.counter = 1\n \n         def hypertile_in(q, k, v, extra_options):\n             if q.shape[-1] in apply_to:\n@@ -53,10 +52,8 @@ def hypertile_in(q, k, v, extra_options):\n                 h, w = round(math.sqrt(hw * aspect_ratio)), round(math.sqrt(hw / aspect_ratio))\n \n                 factor = 2**((q.shape[-1] // model_channels) - 1) if scale_depth else 1\n-                nh = random_divisor(h, latent_tile_size * factor, swap_size, self.counter)\n-                self.counter += 1\n-                nw = random_divisor(w, latent_tile_size * factor, swap_size, self.counter)\n-                self.counter += 1\n+                nh = random_divisor(h, latent_tile_size * factor, swap_size)\n+                nw = random_divisor(w, latent_tile_size * factor, swap_size)\n \n                 if nh * nw > 1:\n                     q = rearrange(q, \"b (nh h nw w) c -> (b nh nw) (h w) c\", h=h // nh, w=w // nw, nh=nh, nw=nw)\n", "test_patch": "", "problem_statement": "HyperTile node is nondeterministic across executions and messes with global randomness\nThe HyperTile node uses the random module and seeds the global random with its own counter variable.\r\n\r\nUnfortunately, this counter variable is retained across executions if the HyperTile parameters don't change, and so every execution will have different results.\r\n\r\nThe effect on global random can be avoided just by using a `random.Random()` instance instead of a counter, but since ComfyUI doesn't provide any kind of after-exec function for nodes, there doesn't seem to be a way to reset it to its initial state after one prompt is executed.\r\n\r\nI suppose you could work around this by setting having IS_CHANGED return something so that the node gets always executed, thus reinitializing randomness, but that might cause any nodes that come after the HyperTile node to needlessly re-execute.\r\n\r\n\n", "hints_text": "This makes things deterministic but at the expense of running the node every time. Dunno if there's a better way to force it.\r\n```\r\ndiff --git a/comfy_extras/nodes_hypertile.py b/comfy_extras/nodes_hypertile.py\r\nindex 0d7d4c9..63f45f8 100644\r\n--- a/comfy_extras/nodes_hypertile.py\r\n+++ b/comfy_extras/nodes_hypertile.py\r\n@@ -4,7 +4,7 @@ import math\r\n from einops import rearrange\r\n import random\r\n \r\n-def random_divisor(value: int, min_value: int, /, max_options: int = 1, counter = 0) -> int:\r\n+def random_divisor(value: int, min_value: int, /, max_options: int = 1, random = random) -> int:\r\n     min_value = min(min_value, value)\r\n \r\n     # All big divisors of value (inclusive)\r\n@@ -12,12 +12,18 @@ def random_divisor(value: int, min_value: int, /, max_options: int = 1, counter\r\n \r\n     ns = [value // i for i in divisors[:max_options]]  # has at least 1 element\r\n \r\n-    random.seed(counter)\r\n     idx = random.randint(0, len(ns) - 1)\r\n \r\n     return ns[idx]\r\n \r\n class HyperTile:\r\n+    counter = 0\r\n+\r\n+    @classmethod\r\n+    def IS_CHANGED(*args, **kwargs):\r\n+        HyperTile.counter += 1\r\n+        return HyperTile.counter\r\n+\r\n     @classmethod\r\n     def INPUT_TYPES(s):\r\n         return {\"required\": { \"model\": (\"MODEL\",),\r\n@@ -42,7 +48,7 @@ class HyperTile:\r\n \r\n         latent_tile_size = max(32, tile_size) // 8\r\n         self.temp = None\r\n-        self.counter = 1\r\n+        self.rand = random.Random(1)\r\n \r\n         def hypertile_in(q, k, v, extra_options):\r\n             if q.shape[-1] in apply_to:\r\n@@ -53,10 +59,8 @@ class HyperTile:\r\n                 h, w = round(math.sqrt(hw * aspect_ratio)), round(math.sqrt(hw / aspect_ratio))\r\n \r\n                 factor = 2**((q.shape[-1] // model_channels) - 1) if scale_depth else 1\r\n-                nh = random_divisor(h, latent_tile_size * factor, swap_size, self.counter)\r\n-                self.counter += 1\r\n-                nw = random_divisor(w, latent_tile_size * factor, swap_size, self.counter)\r\n-                self.counter += 1\r\n+                nh = random_divisor(h, latent_tile_size * factor, swap_size, self.rand)\r\n+                nw = random_divisor(w, latent_tile_size * factor, swap_size, self.rand)\r\n \r\n                 if nh * nw > 1:\r\n                     q = rearrange(q, \"b (nh h nw w) c -> (b nh nw) (h w) c\", h=h // nh, w=w // nw, nh=nh, nw=nw)\r\n-- \r\n2.39.3\r\n```\n> The HyperTile node uses the random module and seeds the global random with its own counter variable.\r\n> \r\n> Unfortunately, this counter variable is retained across executions if the HyperTile parameters don't change, and so every execution will have different results.\r\n> \r\n> The effect on global random can be avoided just by using a `random.Random()` instance instead of a counter, but since ComfyUI doesn't provide any kind of after-exec function for nodes, there doesn't seem to be a way to reset it to its initial state after one prompt is executed.\r\n> \r\n> I suppose you could work around this by setting having IS_CHANGED return something so that the node gets always executed, thus reinitializing randomness, but that might cause any nodes that come after the HyperTile node to needlessly re-execute.\r\n\r\nI've been tinkering with hypertile in my workflow so I'm extremely interested in this, was hoping someone would come into the discussion because my knowledge is limited. \r\n\r\nBut would this help things? \r\n\r\nhttps://github.com/comfyanonymous/ComfyUI/pull/931\r\n\r\nThe while loop I imagine, would. ", "created_at": "2023-12-06T19:19:31Z"}
{"repo": "comfyanonymous/ComfyUI", "pull_number": 1724, "instance_id": "comfyanonymous__ComfyUI-1724", "issue_numbers": ["1723"], "base_commit": "88733c997fd807a572d4a214d2c15fc5dd17b3c6", "patch": "diff --git a/web/scripts/app.js b/web/scripts/app.js\nindex 7698d0f1173..3cf3585d28c 100644\n--- a/web/scripts/app.js\n+++ b/web/scripts/app.js\n@@ -1592,7 +1592,7 @@ export class ComfyApp {\n \t\t\t\t\t\t\t\tall_inputs = all_inputs.concat(Object.keys(parent.inputs))\n \t\t\t\t\t\t\t\tfor (let parent_input in all_inputs) {\n \t\t\t\t\t\t\t\t\tparent_input = all_inputs[parent_input];\n-\t\t\t\t\t\t\t\t\tif (parent.inputs[parent_input].type === node.inputs[i].type) {\n+\t\t\t\t\t\t\t\t\tif (parent.inputs[parent_input]?.type === node.inputs[i].type) {\n \t\t\t\t\t\t\t\t\t\tlink = parent.getInputLink(parent_input);\n \t\t\t\t\t\t\t\t\t\tif (link) {\n \t\t\t\t\t\t\t\t\t\t\tparent = parent.getInputNode(parent_input);\n", "test_patch": "", "problem_statement": "Bypass fails if there are more outputs than inputs on bypassed node and last output is connected\n```\r\napp.js:1596 Uncaught (in promise) TypeError: Cannot read properties of undefined (reading 'type')\r\n    at ComfyApp.graphToPrompt (app.js:1596:42)\r\n```\r\n\r\nAt line 1596, the first value of `parent_input` tried is `link.origin_slot` (because line 1591). This means `parent.inputs[parent_input]` is undefined, but `.type` is read from it.\r\n\r\nFixed by changing line 1596 from\r\n```JavaScript\r\n\t\t\t\t\t\t\t\t\tif (parent.inputs[parent_input].type === node.inputs[i].type) {\r\n```\r\nto\r\n```JavaScript\r\n\t\t\t\t\t\t\t\t\tif (parent.inputs[parent_input]?.type === node.inputs[i].type) {\r\n```\n", "hints_text": "", "created_at": "2023-10-12T03:28:51Z"}
{"repo": "comfyanonymous/ComfyUI", "pull_number": 1463, "instance_id": "comfyanonymous__ComfyUI-1463", "issue_numbers": ["1346"], "base_commit": "e85be36bd2c12f335abdf75669b994c535bbb126", "patch": "diff --git a/server.py b/server.py\nindex be33f410062..d040604998f 100644\n--- a/server.py\n+++ b/server.py\n@@ -603,7 +603,7 @@ async def publish_loop(self):\n             await self.send(*msg)\n \n     async def start(self, address, port, verbose=True, call_on_start=None):\n-        runner = web.AppRunner(self.app)\n+        runner = web.AppRunner(self.app, access_log=None)\n         await runner.setup()\n         site = web.TCPSite(runner, address, port)\n         await site.start()\n", "test_patch": "", "problem_statement": "Lots of web server connection info spam after pulling yesterday's source, --dont-print-server doesn't work\nMostly it's winsock and access logs, but there's a ton of spam because custom nodes all have their own JS files and stylesheets.  Just mousing over the graph caused multiple reloads of JS / CSS for some reason, so it's not very reasonable to display INFO level stuff in the shell even in verbose mode.  The main issue being that it buries the custom node loading messages, and those need to be gone over every time I pull a new revision of Comfy or the larger node sets to make sure something that hasn't been updated in a while isn't broken since some of them try to modify built-in nodes without mentioning it and screw up the entire process.  Previously they were the last thing visible before some spam from mtb so I could just switch to the cmd window and look for load fail messages.    Logging it to file could be reasonable if somebody was actually insane enough to run a public facing python web server and needed to keep everything for analysis.   \r\n\r\nThe normal Windows way of doing this would be to create a category in event viewer for the web server but leave it disabled  unless the user subscribes to that category of events because they need them.  Most use-cases won't because a localhost connection failure requires that the entire TCP/IP stack be intentionally corrupted, which can be done if you really want to, but in that case Windows will stop working correctly too.  \r\n\r\nA very small portion of a run in which I didn't actually do anything except switch browser tabs looks like this right up until I killed it:\r\n```\r\nC:\\Programs\\ComfyUI>comfy\r\n** ComfyUI start up time: 2023-08-27 04:35:47.286040\r\n\r\nPrestartup times for custom nodes:\r\n   0.0 seconds: C:\\Programs\\ComfyUI\\custom_nodes\\rgthree-comfy\r\n   0.0 seconds: C:\\Programs\\ComfyUI\\custom_nodes\\ComfyUI-Manager\r\n\r\nUsing directml with device: AMD Radeon RX 7900 XTX\r\nTotal VRAM 1024 MB, total RAM 524144 MB\r\nSet vram state to: NORMAL_VRAM\r\nDevice: privateuseone\r\nUsing sub quadratic optimization for cross attention, if you have memory or speed issues try using: --use-split-cross-attention\r\nbin C:\\Programs\\Python310\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so\r\nC:\\Programs\\Python310\\lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\r\n  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\r\nfunction 'cadam32bit_grad_fp32' not found\r\n2023-08-27 04:35:50.418341: I tensorflow/c/logging.cc:34] Successfully opened dynamic library C:\\Programs\\Python310\\lib\\site-packages\\tensorflow-plugins/directml/directml.d6f03b303ac3c4f2eeb8ca631688c9757b361310.dll\r\n2023-08-27 04:35:50.418428: I tensorflow/c/logging.cc:34] Successfully opened dynamic library dxgi.dll\r\n2023-08-27 04:35:50.419733: I tensorflow/c/logging.cc:34] Successfully opened dynamic library d3d12.dll\r\n2023-08-27 04:35:50.420613: I tensorflow/c/logging.cc:34] DirectML device enumeration: found 1 compatible adapters.\r\n\r\nSNIP ~100 lines of add-on init messages which get buried immediately \r\n\r\nINFO:aiohttp.access:127.0.0.1 [27/Aug/2023:08:35:55 +0000] \"GET /lib/litegraph.css HTTP/1.1\" 304 203 \"http://127.0.0.1:8188/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:102.0) Gecko/20100101 Firefox/102.0\"\r\n\r\nSNIP 200+ more lines of HTTP access logs\r\n\r\nINFO:aiohttp.access:127.0.0.1 [27/Aug/2023:08:35:56 +0000] \"GET /lib/fabric.js HTTP/1.1\" 304 204 \"http://127.0.0.1:8188/extensions/AlekPet_Nodes/painter_node.js\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:102.0) Gecko/20100101 Firefox/102.0\"\r\nINFO:aiohttp.access:127.0.0.1 [27/Aug/2023:08:35:56 +0000] \"GET /extensions/ComfyUI-Custom-Scripts/js/common/spinner.css HTTP/1.1\" 200 259 \"http://127.0.0.1:8188/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:102.0) Gecko/20100101 Firefox/102.0\"\r\nINFO:aiohttp.access:127.0.0.1 [27/Aug/2023:08:35:56 +0000] \"GET /extensions/ComfyUI-Custom-Scripts/js/loraInfo.css HTTP/1.1\" 200 260 \"http://127.0.0.1:8188/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:102.0) Gecko/20100101 Firefox/102.0\"\r\nINFO:aiohttp.access:127.0.0.1 [27/Aug/2023:08:35:56 +0000] \"GET /extensions/ComfyUI-Custom-Scripts/js/common/lightbox.css HTTP/1.1\" 200 260 \"http://127.0.0.1:8188/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:102.0) Gecko/20100101 Firefox/102.0\"\r\nINFO:aiohttp.access:127.0.0.1 [27/Aug/2023:08:35:56 +0000] \"GET /favicon.ico HTTP/1.1\" 404 173 \"http://127.0.0.1:8188/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:102.0) Gecko/20100101 Firefox/102.0\"\r\nINFO:aiohttp.access:127.0.0.1 [27/Aug/2023:08:35:56 +0000] \"GET /mtb/debug HTTP/1.1\" 200 176 \"http://127.0.0.1:8188/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:102.0) Gecko/20100101 Firefox/102.0\"\r\nINFO:aiohttp.access:127.0.0.1 [27/Aug/2023:08:35:56 +0000] \"GET /object_info HTTP/1.1\" 200 1155636 \"http://127.0.0.1:8188/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:102.0) Gecko/20100101 Firefox/102.0\"\r\nINFO:aiohttp.access:127.0.0.1 [27/Aug/2023:08:35:57 +0000] \"GET /component/get_workflows HTTP/1.1\" 404 173 \"http://127.0.0.1:8188/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:102.0) Gecko/20100101 Firefox/102.0\"\r\nINFO:aiohttp.access:127.0.0.1 [27/Aug/2023:08:35:57 +0000] \"POST /mtb/debug HTTP/1.1\" 200 189 \"http://127.0.0.1:8188/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:102.0) Gecko/20100101 Firefox/102.0\"\r\nINFO:aiohttp.access:127.0.0.1 [27/Aug/2023:08:35:57 +0000] \"GET /pysssss/workflows HTTP/1.1\" 200 159 \"http://127.0.0.1:8188/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:102.0) Gecko/20100101 Firefox/102.0\"\r\nINFO:aiohttp.access:127.0.0.1 [27/Aug/2023:08:35:57 +0000] \"GET /extensions/ComfyUI-Custom-Scripts/js/assets/canvas2svg.js HTTP/1.1\" 304 203 \"http://127.0.0.1:8188/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:102.0) Gecko/20100101 Firefox/102.0\"\r\nINFO:aiohttp.access:127.0.0.1 [27/Aug/2023:08:35:57 +0000] \"GET /view?filename=QRCode.png&type=input&subfolder= HTTP/1.1\" 304 233 \"http://127.0.0.1:8188/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:102.0) Gecko/20100101 Firefox/102.0\"\r\nINFO:websockets.server:connection open\r\nINFO:aiohttp.access:127.0.0.1 [27/Aug/2023:08:36:17 +0000] \"POST /pysssss/workflows HTTP/1.1\" 201 155 \"http://127.0.0.1:8188/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:102.0) Gecko/20100101 Firefox/102.0\"\r\nINFO:aiohttp.access:127.0.0.1 [27/Aug/2023:08:36:17 +0000] \"GET /pysssss/workflows HTTP/1.1\" 200 168 \"http://127.0.0.1:8188/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:102.0) Gecko/20100101 Firefox/102.0\"\r\nINFO:aiohttp.access:127.0.0.1 [27/Aug/2023:08:36:14 +0000] \"GET /ws HTTP/1.1\" 101 0 \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:102.0) Gecko/20100101 Firefox/102.0\"\r\nINFO:websockets.server:connection closed\r\n```\r\n\r\nI was able to do a quick hack to drop the aiohttp debug levels when the command line option is set by modifying server.py's start method:\r\n\r\n```python\r\n    async def start(self, address, port, verbose=True, call_on_start=None):        \r\n        \r\n        runner = web.AppRunner(self.app)\r\n        await runner.setup()\r\n        \r\n        if not verbose and isinstance(runner.app.logger, logging.Logger):\r\n            print(\"Setting loglevel to warning on server.\")\r\n            aiohttp.log.web_logger.setLevel(logging.WARNING)\r\n            aiohttp.log.access_logger.setLevel(logging.WARNING)\r\n            aiohttp.log.server_logger.setLevel(logging.WARNING)\r\n            aiohttp.log.client_logger.setLevel(logging.WARNING)\r\n            aiohttp.log.ws_logger.setLevel(logging.WARNING)\r\n            aiohttp.log.internal_logger.setLevel(logging.WARNING)\r\n        ....\r\n\r\n```\r\n\r\nThe [aiohttp page has suggestions on how to do this](https://docs.aiohttp.org/en/stable/logging.html) that are probably more correct, but the spammiest one (access) needs to be set to a custom logger in the run_app call and if you're going to do that you might as well just use the one with the same naming convention and set it wherever you feel like, IMO.  The websockets ones should probably be disabled when the command line flag is used too, but there aren't very many of them. \r\n\r\nNote that I never saw anything from web_logger, server_logger, client_logger, or ws_logger, but I didn't feel like tracking this down again and I'll forget in a day. \r\n\r\nThe best option would be globally changing Logger output level for everything to avoid some other library flooding the terminal in a month, but since python somehow managed to create an inline documentation format that seems to contain more formatting than the actual docs it generates on their site and nothing in the source file was immediately obvious (nor was I sure that the web server library would actually consistently use the Logger for output or not reset log levels somewhere at that point) I didn't feel like hunting through it further.  \n", "hints_text": "Can you share your `pip list` output?(Maybe as file attachment...)\n[pip_list.txt](https://github.com/comfyanonymous/ComfyUI/files/12454950/pip_list.txt)\r\n\r\nHere you go.  I just noticed the requirements.txt doesn't specify a version for aiohttp and mine just stayed on 3.8.4 but .github\\workflows\\windows_release_nightly_pytorch.yml lists aiohttp==3.8.5, if this makes any difference.  I updated through the ComfyUI manager module instead of git pull this time which (I think?) handles running the requiements again.   It seems like pip tries to pull as little as possible from the requirements list as long as any strict minimum version requirements are met, anyway.", "created_at": "2023-09-09T04:13:10Z"}
{"repo": "comfyanonymous/ComfyUI", "pull_number": 695, "instance_id": "comfyanonymous__ComfyUI-695", "issue_numbers": ["681"], "base_commit": "91449472448f1fbf607720e3822f5d055300899a", "patch": "diff --git a/execution.py b/execution.py\nindex 25f2fcacdac..1a9a1ff73b9 100644\n--- a/execution.py\n+++ b/execution.py\n@@ -102,13 +102,21 @@ def get_output_data(obj, input_data_all):\n         ui = {k: [y for x in uis for y in x[k]] for k in uis[0].keys()}\n     return output, ui\n \n+def format_value(x):\n+    if x is None:\n+        return None\n+    elif isinstance(x, (int, float, bool, str)):\n+        return x\n+    else:\n+        return str(x)\n+\n def recursive_execute(server, prompt, outputs, current_item, extra_data, executed, prompt_id, outputs_ui):\n     unique_id = current_item\n     inputs = prompt[unique_id]['inputs']\n     class_type = prompt[unique_id]['class_type']\n     class_def = nodes.NODE_CLASS_MAPPINGS[class_type]\n     if unique_id in outputs:\n-        return\n+        return (True, None, None)\n \n     for x in inputs:\n         input_data = inputs[x]\n@@ -117,22 +125,64 @@ def recursive_execute(server, prompt, outputs, current_item, extra_data, execute\n             input_unique_id = input_data[0]\n             output_index = input_data[1]\n             if input_unique_id not in outputs:\n-                recursive_execute(server, prompt, outputs, input_unique_id, extra_data, executed, prompt_id, outputs_ui)\n-\n-    input_data_all = get_input_data(inputs, class_def, unique_id, outputs, prompt, extra_data)\n-    if server.client_id is not None:\n-        server.last_node_id = unique_id\n-        server.send_sync(\"executing\", { \"node\": unique_id, \"prompt_id\": prompt_id }, server.client_id)\n-    obj = class_def()\n-\n-    output_data, output_ui = get_output_data(obj, input_data_all)\n-    outputs[unique_id] = output_data\n-    if len(output_ui) > 0:\n-        outputs_ui[unique_id] = output_ui\n+                result = recursive_execute(server, prompt, outputs, input_unique_id, extra_data, executed, prompt_id, outputs_ui)\n+                if result[0] is not True:\n+                    # Another node failed further upstream\n+                    return result\n+\n+    input_data_all = None\n+    try:\n+        input_data_all = get_input_data(inputs, class_def, unique_id, outputs, prompt, extra_data)\n         if server.client_id is not None:\n-            server.send_sync(\"executed\", { \"node\": unique_id, \"output\": output_ui, \"prompt_id\": prompt_id }, server.client_id)\n+            server.last_node_id = unique_id\n+            server.send_sync(\"executing\", { \"node\": unique_id, \"prompt_id\": prompt_id }, server.client_id)\n+        obj = class_def()\n+\n+        output_data, output_ui = get_output_data(obj, input_data_all)\n+        outputs[unique_id] = output_data\n+        if len(output_ui) > 0:\n+            outputs_ui[unique_id] = output_ui\n+            if server.client_id is not None:\n+                server.send_sync(\"executed\", { \"node\": unique_id, \"output\": output_ui, \"prompt_id\": prompt_id }, server.client_id)\n+    except comfy.model_management.InterruptProcessingException as iex:\n+        print(\"Processing interrupted\")\n+\n+        # skip formatting inputs/outputs\n+        error_details = {\n+            \"node_id\": unique_id,\n+        }\n+\n+        return (False, error_details, iex)\n+    except Exception as ex:\n+        typ, _, tb = sys.exc_info()\n+        exception_type = full_type_name(typ)\n+        input_data_formatted = {}\n+        if input_data_all is not None:\n+            input_data_formatted = {}\n+            for name, inputs in input_data_all.items():\n+                input_data_formatted[name] = [format_value(x) for x in inputs]\n+\n+        output_data_formatted = {}\n+        for node_id, node_outputs in outputs.items():\n+            output_data_formatted[node_id] = [[format_value(x) for x in l] for l in node_outputs]\n+\n+        print(\"!!! Exception during processing !!!\")\n+        print(traceback.format_exc())\n+\n+        error_details = {\n+            \"node_id\": unique_id,\n+            \"exception_message\": str(ex),\n+            \"exception_type\": exception_type,\n+            \"traceback\": traceback.format_tb(tb),\n+            \"current_inputs\": input_data_formatted,\n+            \"current_outputs\": output_data_formatted\n+        }\n+        return (False, error_details, ex)\n+\n     executed.add(unique_id)\n \n+    return (True, None, None)\n+\n def recursive_will_execute(prompt, outputs, current_item):\n     unique_id = current_item\n     inputs = prompt[unique_id]['inputs']\n@@ -210,6 +260,48 @@ def __init__(self, server):\n         self.old_prompt = {}\n         self.server = server\n \n+    def handle_execution_error(self, prompt_id, prompt, current_outputs, executed, error, ex):\n+        node_id = error[\"node_id\"]\n+        class_type = prompt[node_id][\"class_type\"]\n+\n+        # First, send back the status to the frontend depending\n+        # on the exception type\n+        if isinstance(ex, comfy.model_management.InterruptProcessingException):\n+            mes = {\n+                \"prompt_id\": prompt_id,\n+                \"node_id\": node_id,\n+                \"node_type\": class_type,\n+                \"executed\": list(executed),\n+            }\n+            self.server.send_sync(\"execution_interrupted\", mes, self.server.client_id)\n+        else:\n+            if self.server.client_id is not None:\n+                mes = {\n+                    \"prompt_id\": prompt_id,\n+                    \"node_id\": node_id,\n+                    \"node_type\": class_type,\n+                    \"executed\": list(executed),\n+\n+                    \"exception_message\": error[\"exception_message\"],\n+                    \"exception_type\": error[\"exception_type\"],\n+                    \"traceback\": error[\"traceback\"],\n+                    \"current_inputs\": error[\"current_inputs\"],\n+                    \"current_outputs\": error[\"current_outputs\"],\n+                }\n+                self.server.send_sync(\"execution_error\", mes, self.server.client_id)\n+\n+        # Next, remove the subsequent outputs since they will not be executed\n+        to_delete = []\n+        for o in self.outputs:\n+            if (o not in current_outputs) and (o not in executed):\n+                to_delete += [o]\n+                if o in self.old_prompt:\n+                    d = self.old_prompt.pop(o)\n+                    del d\n+        for o in to_delete:\n+            d = self.outputs.pop(o)\n+            del d\n+\n     def execute(self, prompt, prompt_id, extra_data={}, execute_outputs=[]):\n         nodes.interrupt_processing(False)\n \n@@ -244,42 +336,29 @@ def execute(self, prompt, prompt_id, extra_data={}, execute_outputs=[]):\n             if self.server.client_id is not None:\n                 self.server.send_sync(\"execution_cached\", { \"nodes\": list(current_outputs) , \"prompt_id\": prompt_id}, self.server.client_id)\n             executed = set()\n-            try:\n-                to_execute = []\n-                for x in list(execute_outputs):\n-                    to_execute += [(0, x)]\n-\n-                while len(to_execute) > 0:\n-                    #always execute the output that depends on the least amount of unexecuted nodes first\n-                    to_execute = sorted(list(map(lambda a: (len(recursive_will_execute(prompt, self.outputs, a[-1])), a[-1]), to_execute)))\n-                    x = to_execute.pop(0)[-1]\n-\n-                    recursive_execute(self.server, prompt, self.outputs, x, extra_data, executed, prompt_id, self.outputs_ui)\n-            except Exception as e:\n-                if isinstance(e, comfy.model_management.InterruptProcessingException):\n-                    print(\"Processing interrupted\")\n-                else:\n-                    message = str(traceback.format_exc())\n-                    print(message)\n-                    if self.server.client_id is not None:\n-                        self.server.send_sync(\"execution_error\", { \"message\": message, \"prompt_id\": prompt_id }, self.server.client_id)\n-\n-                to_delete = []\n-                for o in self.outputs:\n-                    if (o not in current_outputs) and (o not in executed):\n-                        to_delete += [o]\n-                        if o in self.old_prompt:\n-                            d = self.old_prompt.pop(o)\n-                            del d\n-                for o in to_delete:\n-                    d = self.outputs.pop(o)\n-                    del d\n-            finally:\n-                for x in executed:\n-                    self.old_prompt[x] = copy.deepcopy(prompt[x])\n-                self.server.last_node_id = None\n-                if self.server.client_id is not None:\n-                    self.server.send_sync(\"executing\", { \"node\": None, \"prompt_id\": prompt_id }, self.server.client_id)\n+            output_node_id = None\n+            to_execute = []\n+\n+            for node_id in list(execute_outputs):\n+                to_execute += [(0, node_id)]\n+\n+            while len(to_execute) > 0:\n+                #always execute the output that depends on the least amount of unexecuted nodes first\n+                to_execute = sorted(list(map(lambda a: (len(recursive_will_execute(prompt, self.outputs, a[-1])), a[-1]), to_execute)))\n+                output_node_id = to_execute.pop(0)[-1]\n+\n+                # This call shouldn't raise anything if there's an error deep in\n+                # the actual SD code, instead it will report the node where the\n+                # error was raised\n+                success, error, ex = recursive_execute(self.server, prompt, self.outputs, output_node_id, extra_data, executed, prompt_id, self.outputs_ui)\n+                if success is not True:\n+                    self.handle_execution_error(prompt_id, prompt, current_outputs, executed, error, ex)\n+\n+            for x in executed:\n+                self.old_prompt[x] = copy.deepcopy(prompt[x])\n+            self.server.last_node_id = None\n+            if self.server.client_id is not None:\n+                self.server.send_sync(\"executing\", { \"node\": None, \"prompt_id\": prompt_id }, self.server.client_id)\n \n         print(\"Prompt executed in {:.2f} seconds\".format(time.perf_counter() - execution_start_time))\n         gc.collect()\n@@ -297,57 +376,202 @@ def validate_inputs(prompt, item, validated):\n \n     class_inputs = obj_class.INPUT_TYPES()\n     required_inputs = class_inputs['required']\n+\n+    errors = []\n+    valid = True\n+\n     for x in required_inputs:\n         if x not in inputs:\n-            return (False, \"Required input is missing. {}, {}\".format(class_type, x), unique_id)\n+            error = {\n+                \"type\": \"required_input_missing\",\n+                \"message\": \"Required input is missing\",\n+                \"details\": f\"{x}\",\n+                \"extra_info\": {\n+                    \"input_name\": x\n+                }\n+            }\n+            errors.append(error)\n+            continue\n+\n         val = inputs[x]\n         info = required_inputs[x]\n         type_input = info[0]\n         if isinstance(val, list):\n             if len(val) != 2:\n-                return (False, \"Bad Input. {}, {}\".format(class_type, x), unique_id)\n+                error = {\n+                    \"type\": \"bad_linked_input\",\n+                    \"message\": \"Bad linked input, must be a length-2 list of [node_id, slot_index]\",\n+                    \"details\": f\"{x}\",\n+                    \"extra_info\": {\n+                        \"input_name\": x,\n+                        \"input_config\": info,\n+                        \"received_value\": val\n+                    }\n+                }\n+                errors.append(error)\n+                continue\n+\n             o_id = val[0]\n             o_class_type = prompt[o_id]['class_type']\n             r = nodes.NODE_CLASS_MAPPINGS[o_class_type].RETURN_TYPES\n             if r[val[1]] != type_input:\n-                return (False, \"Return type mismatch. {}, {}, {} != {}\".format(class_type, x, r[val[1]], type_input), unique_id)\n-            r = validate_inputs(prompt, o_id, validated)\n-            if r[0] == False:\n-                validated[o_id] = r\n-                return r\n+                received_type = r[val[1]]\n+                details = f\"{x}, {received_type} != {type_input}\"\n+                error = {\n+                    \"type\": \"return_type_mismatch\",\n+                    \"message\": \"Return type mismatch between linked nodes\",\n+                    \"details\": details,\n+                    \"extra_info\": {\n+                        \"input_name\": x,\n+                        \"input_config\": info,\n+                        \"received_type\": received_type,\n+                        \"linked_node\": val\n+                    }\n+                }\n+                errors.append(error)\n+                continue\n+            try:\n+                r = validate_inputs(prompt, o_id, validated)\n+                if r[0] is False:\n+                    # `r` will be set in `validated[o_id]` already\n+                    valid = False\n+                    continue\n+            except Exception as ex:\n+                typ, _, tb = sys.exc_info()\n+                valid = False\n+                exception_type = full_type_name(typ)\n+                reasons = [{\n+                    \"type\": \"exception_during_inner_validation\",\n+                    \"message\": \"Exception when validating inner node\",\n+                    \"details\": str(ex),\n+                    \"extra_info\": {\n+                        \"input_name\": x,\n+                        \"input_config\": info,\n+                        \"exception_message\": str(ex),\n+                        \"exception_type\": exception_type,\n+                        \"traceback\": traceback.format_tb(tb),\n+                        \"linked_node\": val\n+                    }\n+                }]\n+                validated[o_id] = (False, reasons, o_id)\n+                continue\n         else:\n-            if type_input == \"INT\":\n-                val = int(val)\n-                inputs[x] = val\n-            if type_input == \"FLOAT\":\n-                val = float(val)\n-                inputs[x] = val\n-            if type_input == \"STRING\":\n-                val = str(val)\n-                inputs[x] = val\n+            try:\n+                if type_input == \"INT\":\n+                    val = int(val)\n+                    inputs[x] = val\n+                if type_input == \"FLOAT\":\n+                    val = float(val)\n+                    inputs[x] = val\n+                if type_input == \"STRING\":\n+                    val = str(val)\n+                    inputs[x] = val\n+            except Exception as ex:\n+                error = {\n+                    \"type\": \"invalid_input_type\",\n+                    \"message\": f\"Failed to convert an input value to a {type_input} value\",\n+                    \"details\": f\"{x}, {val}, {ex}\",\n+                    \"extra_info\": {\n+                        \"input_name\": x,\n+                        \"input_config\": info,\n+                        \"received_value\": val,\n+                        \"exception_message\": str(ex)\n+                    }\n+                }\n+                errors.append(error)\n+                continue\n \n             if len(info) > 1:\n                 if \"min\" in info[1] and val < info[1][\"min\"]:\n-                    return (False, \"Value {} smaller than min of {}. {}, {}\".format(val, info[1][\"min\"], class_type, x), unique_id)\n+                    error = {\n+                        \"type\": \"value_smaller_than_min\",\n+                        \"message\": \"Value {} smaller than min of {}\".format(val, info[1][\"min\"]),\n+                        \"details\": f\"{x}\",\n+                        \"extra_info\": {\n+                            \"input_name\": x,\n+                            \"input_config\": info,\n+                            \"received_value\": val,\n+                        }\n+                    }\n+                    errors.append(error)\n+                    continue\n                 if \"max\" in info[1] and val > info[1][\"max\"]:\n-                    return (False, \"Value {} bigger than max of {}. {}, {}\".format(val, info[1][\"max\"], class_type, x), unique_id)\n+                    error = {\n+                        \"type\": \"value_bigger_than_max\",\n+                        \"message\": \"Value {} bigger than max of {}\".format(val, info[1][\"max\"]),\n+                        \"details\": f\"{x}\",\n+                        \"extra_info\": {\n+                            \"input_name\": x,\n+                            \"input_config\": info,\n+                            \"received_value\": val,\n+                        }\n+                    }\n+                    errors.append(error)\n+                    continue\n \n             if hasattr(obj_class, \"VALIDATE_INPUTS\"):\n                 input_data_all = get_input_data(inputs, obj_class, unique_id)\n                 #ret = obj_class.VALIDATE_INPUTS(**input_data_all)\n                 ret = map_node_over_list(obj_class, input_data_all, \"VALIDATE_INPUTS\")\n-                for r in ret:\n-                    if r != True:\n-                        return (False, \"{}, {}\".format(class_type, r), unique_id)\n+                for i, r in enumerate(ret):\n+                    if r is not True:\n+                        details = f\"{x}\"\n+                        if r is not False:\n+                            details += f\" - {str(r)}\"\n+\n+                        error = {\n+                            \"type\": \"custom_validation_failed\",\n+                            \"message\": \"Custom validation failed for node\",\n+                            \"details\": details,\n+                            \"extra_info\": {\n+                                \"input_name\": x,\n+                                \"input_config\": info,\n+                                \"received_value\": val,\n+                            }\n+                        }\n+                        errors.append(error)\n+                        continue\n             else:\n                 if isinstance(type_input, list):\n                     if val not in type_input:\n-                        return (False, \"Value not in list. {}, {}: {} not in {}\".format(class_type, x, val, type_input), unique_id)\n+                        input_config = info\n+                        list_info = \"\"\n+\n+                        # Don't send back gigantic lists like if they're lots of\n+                        # scanned model filepaths\n+                        if len(type_input) > 20:\n+                            list_info = f\"(list of length {len(type_input)})\"\n+                            input_config = None\n+                        else:\n+                            list_info = str(type_input)\n+\n+                        error = {\n+                            \"type\": \"value_not_in_list\",\n+                            \"message\": \"Value not in list\",\n+                            \"details\": f\"{x}: '{val}' not in {list_info}\",\n+                            \"extra_info\": {\n+                                \"input_name\": x,\n+                                \"input_config\": input_config,\n+                                \"received_value\": val,\n+                            }\n+                        }\n+                        errors.append(error)\n+                        continue\n+\n+    if len(errors) > 0 or valid is not True:\n+        ret = (False, errors, unique_id)\n+    else:\n+        ret = (True, [], unique_id)\n \n-    ret = (True, \"\", unique_id)\n     validated[unique_id] = ret\n     return ret\n \n+def full_type_name(klass):\n+    module = klass.__module__\n+    if module == 'builtins':\n+        return klass.__qualname__\n+    return module + '.' + klass.__qualname__\n+\n def validate_prompt(prompt):\n     outputs = set()\n     for x in prompt:\n@@ -356,7 +580,13 @@ def validate_prompt(prompt):\n             outputs.add(x)\n \n     if len(outputs) == 0:\n-        return (False, \"Prompt has no outputs\", [], [])\n+        error = {\n+            \"type\": \"prompt_no_outputs\",\n+            \"message\": \"Prompt has no outputs\",\n+            \"details\": \"\",\n+            \"extra_info\": {}\n+        }\n+        return (False, error, [], [])\n \n     good_outputs = set()\n     errors = []\n@@ -364,34 +594,72 @@ def validate_prompt(prompt):\n     validated = {}\n     for o in outputs:\n         valid = False\n-        reason = \"\"\n+        reasons = []\n         try:\n             m = validate_inputs(prompt, o, validated)\n             valid = m[0]\n-            reason = m[1]\n-            node_id = m[2]\n-        except Exception as e:\n-            print(traceback.format_exc())\n+            reasons = m[1]\n+        except Exception as ex:\n+            typ, _, tb = sys.exc_info()\n             valid = False\n-            reason = \"Parsing error\"\n-            node_id = None\n-\n-        if valid == True:\n+            exception_type = full_type_name(typ)\n+            reasons = [{\n+                \"type\": \"exception_during_validation\",\n+                \"message\": \"Exception when validating node\",\n+                \"details\": str(ex),\n+                \"extra_info\": {\n+                    \"exception_type\": exception_type,\n+                    \"traceback\": traceback.format_tb(tb)\n+                }\n+            }]\n+            validated[o] = (False, reasons, o)\n+\n+        if valid is True:\n             good_outputs.add(o)\n         else:\n-            print(\"Failed to validate prompt for output {} {}\".format(o, reason))\n-            print(\"output will be ignored\")\n-            errors += [(o, reason)]\n-            if node_id is not None:\n-                if node_id not in node_errors:\n-                    node_errors[node_id] = {\"message\": reason, \"dependent_outputs\": []}\n-                node_errors[node_id][\"dependent_outputs\"].append(o)\n+            print(f\"Failed to validate prompt for output {o}:\")\n+            if len(reasons) > 0:\n+                print(\"* (prompt):\")\n+                for reason in reasons:\n+                    print(f\"  - {reason['message']}: {reason['details']}\")\n+            errors += [(o, reasons)]\n+            for node_id, result in validated.items():\n+                valid = result[0]\n+                reasons = result[1]\n+                # If a node upstream has errors, the nodes downstream will also\n+                # be reported as invalid, but there will be no errors attached.\n+                # So don't return those nodes as having errors in the response.\n+                if valid is not True and len(reasons) > 0:\n+                    if node_id not in node_errors:\n+                        class_type = prompt[node_id]['class_type']\n+                        node_errors[node_id] = {\n+                            \"errors\": reasons,\n+                            \"dependent_outputs\": [],\n+                            \"class_type\": class_type\n+                        }\n+                        print(f\"* {class_type} {node_id}:\")\n+                        for reason in reasons:\n+                            print(f\"  - {reason['message']}: {reason['details']}\")\n+                    node_errors[node_id][\"dependent_outputs\"].append(o)\n+            print(\"Output will be ignored\")\n \n     if len(good_outputs) == 0:\n-        errors_list = \"\\n\".join(set(map(lambda a: \"{}\".format(a[1]), errors)))\n-        return (False, \"Prompt has no properly connected outputs\\n {}\".format(errors_list), list(good_outputs), node_errors)\n-\n-    return (True, \"\", list(good_outputs), node_errors)\n+        errors_list = []\n+        for o, errors in errors:\n+            for error in errors:\n+                errors_list.append(f\"{error['message']}: {error['details']}\")\n+        errors_list = \"\\n\".join(errors_list)\n+\n+        error = {\n+            \"type\": \"prompt_outputs_failed_validation\",\n+            \"message\": \"Prompt outputs failed validation\",\n+            \"details\": errors_list,\n+            \"extra_info\": {}\n+        }\n+\n+        return (False, error, list(good_outputs), node_errors)\n+\n+    return (True, None, list(good_outputs), node_errors)\n \n \n class PromptQueue:\ndiff --git a/web/scripts/api.js b/web/scripts/api.js\nindex 4f061c35878..378165b3ad3 100644\n--- a/web/scripts/api.js\n+++ b/web/scripts/api.js\n@@ -88,6 +88,12 @@ class ComfyApi extends EventTarget {\n \t\t\t\t\tcase \"executed\":\n \t\t\t\t\t\tthis.dispatchEvent(new CustomEvent(\"executed\", { detail: msg.data }));\n \t\t\t\t\t\tbreak;\n+\t\t\t\t\tcase \"execution_start\":\n+\t\t\t\t\t\tthis.dispatchEvent(new CustomEvent(\"execution_start\", { detail: msg.data }));\n+\t\t\t\t\t\tbreak;\n+\t\t\t\t\tcase \"execution_error\":\n+\t\t\t\t\t\tthis.dispatchEvent(new CustomEvent(\"execution_error\", { detail: msg.data }));\n+\t\t\t\t\t\tbreak;\n \t\t\t\t\tdefault:\n \t\t\t\t\t\tif (this.#registered.has(msg.type)) {\n \t\t\t\t\t\t\tthis.dispatchEvent(new CustomEvent(msg.type, { detail: msg.data }));\ndiff --git a/web/scripts/app.js b/web/scripts/app.js\nindex 97b7c8d319e..e8ab32cf95c 100644\n--- a/web/scripts/app.js\n+++ b/web/scripts/app.js\n@@ -771,16 +771,27 @@ export class ComfyApp {\n \t\tLGraphCanvas.prototype.drawNodeShape = function (node, ctx, size, fgcolor, bgcolor, selected, mouse_over) {\n \t\t\tconst res = origDrawNodeShape.apply(this, arguments);\n \n+\t\t\tconst nodeErrors = self.lastPromptError?.node_errors[node.id];\n+\n \t\t\tlet color = null;\n+\t\t\tlet lineWidth = 1;\n \t\t\tif (node.id === +self.runningNodeId) {\n \t\t\t\tcolor = \"#0f0\";\n \t\t\t} else if (self.dragOverNode && node.id === self.dragOverNode.id) {\n \t\t\t\tcolor = \"dodgerblue\";\n \t\t\t}\n+\t\t\telse if (self.lastPromptError != null && nodeErrors?.errors) {\n+\t\t\t\tcolor = \"red\";\n+\t\t\t\tlineWidth = 2;\n+\t\t\t}\n+\t\t\telse if (self.lastExecutionError && +self.lastExecutionError.node_id === node.id) {\n+\t\t\t\tcolor = \"#f0f\";\n+\t\t\t\tlineWidth = 2;\n+\t\t\t}\n \n \t\t\tif (color) {\n \t\t\t\tconst shape = node._shape || node.constructor.shape || LiteGraph.ROUND_SHAPE;\n-\t\t\t\tctx.lineWidth = 1;\n+\t\t\t\tctx.lineWidth = lineWidth;\n \t\t\t\tctx.globalAlpha = 0.8;\n \t\t\t\tctx.beginPath();\n \t\t\t\tif (shape == LiteGraph.BOX_SHAPE)\n@@ -807,11 +818,28 @@ export class ComfyApp {\n \t\t\t\tctx.stroke();\n \t\t\t\tctx.strokeStyle = fgcolor;\n \t\t\t\tctx.globalAlpha = 1;\n+\t\t\t}\n \n-\t\t\t\tif (self.progress) {\n-\t\t\t\t\tctx.fillStyle = \"green\";\n-\t\t\t\t\tctx.fillRect(0, 0, size[0] * (self.progress.value / self.progress.max), 6);\n-\t\t\t\t\tctx.fillStyle = bgcolor;\n+\t\t\tif (self.progress && node.id === +self.runningNodeId) {\n+\t\t\t\tctx.fillStyle = \"green\";\n+\t\t\t\tctx.fillRect(0, 0, size[0] * (self.progress.value / self.progress.max), 6);\n+\t\t\t\tctx.fillStyle = bgcolor;\n+\t\t\t}\n+\n+\t\t\t// Highlight inputs that failed validation\n+\t\t\tif (nodeErrors) {\n+\t\t\t\tctx.lineWidth = 2;\n+\t\t\t\tctx.strokeStyle = \"red\";\n+\t\t\t\tfor (const error of nodeErrors.errors) {\n+\t\t\t\t\tif (error.extra_info && error.extra_info.input_name) {\n+\t\t\t\t\t\tconst inputIndex = node.findInputSlot(error.extra_info.input_name)\n+\t\t\t\t\t\tif (inputIndex !== -1) {\n+\t\t\t\t\t\t\tlet pos = node.getConnectionPos(true, inputIndex);\n+\t\t\t\t\t\t\tctx.beginPath();\n+\t\t\t\t\t\t\tctx.arc(pos[0] - node.pos[0], pos[1] - node.pos[1], 12, 0, 2 * Math.PI, false)\n+\t\t\t\t\t\t\tctx.stroke();\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n \n@@ -869,6 +897,17 @@ export class ComfyApp {\n \t\t\t}\n \t\t});\n \n+\t\tapi.addEventListener(\"execution_start\", ({ detail }) => {\n+\t\t\tthis.lastExecutionError = null\n+\t\t});\n+\n+\t\tapi.addEventListener(\"execution_error\", ({ detail }) => {\n+\t\t\tthis.lastExecutionError = detail;\n+\t\t\tconst formattedError = this.#formatExecutionError(detail);\n+\t\t\tthis.ui.dialog.show(formattedError);\n+\t\t\tthis.canvas.draw(true, true);\n+\t\t});\n+\n \t\tapi.init();\n \t}\n \n@@ -1243,6 +1282,43 @@ export class ComfyApp {\n \t\treturn { workflow, output };\n \t}\n \n+\t#formatPromptError(error) {\n+\t\tif (error == null) {\n+\t\t\treturn \"(unknown error)\"\n+\t\t}\n+\t\telse if (typeof error === \"string\") {\n+\t\t\treturn error;\n+\t\t}\n+\t\telse if (error.stack && error.message) {\n+\t\t\treturn error.toString()\n+\t\t}\n+\t\telse if (error.response) {\n+\t\t\tlet message = error.response.error.message;\n+\t\t\tif (error.response.error.details)\n+\t\t\tmessage += \": \" + error.response.error.details;\n+\t\t\tfor (const [nodeID, nodeError] of Object.entries(error.response.node_errors)) {\n+\t\t\tmessage += \"\\n\" + nodeError.class_type + \":\"\n+\t\t\t\tfor (const errorReason of nodeError.errors) {\n+\t\t\t\t\tmessage += \"\\n    - \" + errorReason.message + \": \" + errorReason.details\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\treturn message\n+\t\t}\n+\t\treturn \"(unknown error)\"\n+\t}\n+\n+\t#formatExecutionError(error) {\n+\t\tif (error == null) {\n+\t\t\treturn \"(unknown error)\"\n+\t\t}\n+\n+\t\tconst traceback = error.traceback.join(\"\")\n+\t\tconst nodeId = error.node_id\n+\t\tconst nodeType = error.node_type\n+\n+\t\treturn `Error occurred when executing ${nodeType}:\\n\\n${error.message}\\n\\n${traceback}`\n+\t}\n+\n \tasync queuePrompt(number, batchCount = 1) {\n \t\tthis.#queueItems.push({ number, batchCount });\n \n@@ -1250,8 +1326,10 @@ export class ComfyApp {\n \t\tif (this.#processingQueue) {\n \t\t\treturn;\n \t\t}\n-\t\n+\n \t\tthis.#processingQueue = true;\n+\t\tthis.lastPromptError = null;\n+\n \t\ttry {\n \t\t\twhile (this.#queueItems.length) {\n \t\t\t\t({ number, batchCount } = this.#queueItems.pop());\n@@ -1262,7 +1340,12 @@ export class ComfyApp {\n \t\t\t\t\ttry {\n \t\t\t\t\t\tawait api.queuePrompt(number, p);\n \t\t\t\t\t} catch (error) {\n-\t\t\t\t\t\tthis.ui.dialog.show(error.response.error || error.toString());\n+\t\t\t\t\t\tconst formattedError = this.#formatPromptError(error)\n+\t\t\t\t\t\tthis.ui.dialog.show(formattedError);\n+\t\t\t\t\t\tif (error.response) {\n+\t\t\t\t\t\t\tthis.lastPromptError = error.response;\n+\t\t\t\t\t\t\tthis.canvas.draw(true, true);\n+\t\t\t\t\t\t}\n \t\t\t\t\t\tbreak;\n \t\t\t\t\t}\n \n@@ -1360,6 +1443,8 @@ export class ComfyApp {\n \t */\n \tclean() {\n \t\tthis.nodeOutputs = {};\n+\t\tthis.lastPromptError = null;\n+\t\tthis.lastExecutionError = null;\n \t}\n }\n \n", "test_patch": "", "problem_statement": "Show nodes that failed validation from `/prompt` input\nIt would be a hash with fields like `nodeID`, `inputName`, `message`, `details` and would contain the actual value/expected range in the response\n", "hints_text": "No fields yet but it returns a list of the nodes that failed validation: https://github.com/comfyanonymous/ComfyUI/commit/ffc56c53c9cccfcc21c92fe14cb095bb32ea2744", "created_at": "2023-05-25T16:50:54Z"}
{"repo": "comfyanonymous/ComfyUI", "pull_number": 620, "instance_id": "comfyanonymous__ComfyUI-620", "issue_numbers": ["617"], "base_commit": "af9cc1fb6a88e604700d3f57638ab23b9f607e9e", "patch": "diff --git a/web/scripts/app.js b/web/scripts/app.js\nindex ada1708dc63..98c0e07999a 100644\n--- a/web/scripts/app.js\n+++ b/web/scripts/app.js\n@@ -703,7 +703,7 @@ export class ComfyApp {\n \t\t\t\tctx.globalAlpha = 0.8;\n \t\t\t\tctx.beginPath();\n \t\t\t\tif (shape == LiteGraph.BOX_SHAPE)\n-\t\t\t\t\tctx.rect(-6, -6 + LiteGraph.NODE_TITLE_HEIGHT, 12 + size[0] + 1, 12 + size[1] + LiteGraph.NODE_TITLE_HEIGHT);\n+\t\t\t\t\tctx.rect(-6, -6 - LiteGraph.NODE_TITLE_HEIGHT, 12 + size[0] + 1, 12 + size[1] + LiteGraph.NODE_TITLE_HEIGHT);\n \t\t\t\telse if (shape == LiteGraph.ROUND_SHAPE || (shape == LiteGraph.CARD_SHAPE && node.flags.collapsed))\n \t\t\t\t\tctx.roundRect(\n \t\t\t\t\t\t-6,\n@@ -715,12 +715,11 @@ export class ComfyApp {\n \t\t\t\telse if (shape == LiteGraph.CARD_SHAPE)\n \t\t\t\t\tctx.roundRect(\n \t\t\t\t\t\t-6,\n-\t\t\t\t\t\t-6 + LiteGraph.NODE_TITLE_HEIGHT,\n+\t\t\t\t\t\t-6 - LiteGraph.NODE_TITLE_HEIGHT,\n \t\t\t\t\t\t12 + size[0] + 1,\n \t\t\t\t\t\t12 + size[1] + LiteGraph.NODE_TITLE_HEIGHT,\n-\t\t\t\t\t\tthis.round_radius * 2,\n-\t\t\t\t\t\t2\n-\t\t\t\t\t);\n+\t\t\t\t\t\t[this.round_radius * 2, this.round_radius * 2, 2, 2]\n+\t\t\t\t);\n \t\t\t\telse if (shape == LiteGraph.CIRCLE_SHAPE)\n \t\t\t\t\tctx.arc(size[0] * 0.5, size[1] * 0.5, size[0] * 0.5 + 6, 0, Math.PI * 2);\n \t\t\t\tctx.strokeStyle = color;\n", "test_patch": "", "problem_statement": "Green progress bounding box is offset on all BOX nodes\nGreen progress bounding box is offset on all BOX nodes\r\n![Screenshot_20230504_205401](https://user-images.githubusercontent.com/8983883/236368731-d864723e-e912-4c8a-b670-7e8f4a0b5e61.png)\r\n\n", "hints_text": "", "created_at": "2023-05-05T09:34:31Z"}
{"repo": "comfyanonymous/ComfyUI", "pull_number": 555, "instance_id": "comfyanonymous__ComfyUI-555", "issue_numbers": ["549"], "base_commit": "6908f9c94992b32fbb96be0f6cd8c5b362d72a77", "patch": "diff --git a/web/scripts/api.js b/web/scripts/api.js\nindex 2b90c2abc83..d29faa5bae9 100644\n--- a/web/scripts/api.js\n+++ b/web/scripts/api.js\n@@ -35,7 +35,7 @@ class ComfyApi extends EventTarget {\n \t\t}\n \n \t\tlet opened = false;\n-\t\tlet existingSession = sessionStorage[\"Comfy.SessionId\"] || \"\";\n+\t\tlet existingSession = window.name;\n \t\tif (existingSession) {\n \t\t\texistingSession = \"?clientId=\" + existingSession;\n \t\t}\n@@ -75,7 +75,7 @@ class ComfyApi extends EventTarget {\n \t\t\t\t\tcase \"status\":\n \t\t\t\t\t\tif (msg.data.sid) {\n \t\t\t\t\t\t\tthis.clientId = msg.data.sid;\n-\t\t\t\t\t\t\tsessionStorage[\"Comfy.SessionId\"] = this.clientId;\n+\t\t\t\t\t\t\twindow.name = this.clientId;\n \t\t\t\t\t\t}\n \t\t\t\t\t\tthis.dispatchEvent(new CustomEvent(\"status\", { detail: msg.data.status }));\n \t\t\t\t\t\tbreak;\n", "test_patch": "", "problem_statement": "Duplicate tab causes original tab to lose socket updates\nDuplicating a tab also duplicates session storage, the new tab then reuses the socket id from the original, causing it to no longer receive updates\n", "hints_text": "", "created_at": "2023-04-23T09:37:39Z"}
{"repo": "comfyanonymous/ComfyUI", "pull_number": 515, "instance_id": "comfyanonymous__ComfyUI-515", "issue_numbers": ["461"], "base_commit": "04d9bc13afd684a5bd4cb637e26972bb5aee43d1", "patch": "diff --git a/web/extensions/core/widgetInputs.js b/web/extensions/core/widgetInputs.js\nindex 2b360341936..67a59fb324a 100644\n--- a/web/extensions/core/widgetInputs.js\n+++ b/web/extensions/core/widgetInputs.js\n@@ -333,7 +333,20 @@ app.registerExtension({\n \t\t\t\tconst config1 = this.outputs[0].widget.config;\n \t\t\t\tconst config2 = input.widget.config;\n \n-\t\t\t\tif (config1[0] !== config2[0]) return false;\n+\t\t\t\tif (config1[0] instanceof Array) {\n+\t\t\t\t\t// These checks shouldnt actually be necessary as the types should match\n+\t\t\t\t\t// but double checking doesn't hurt\n+\n+\t\t\t\t\t// New input isnt a combo\n+\t\t\t\t\tif (!(config2[0] instanceof Array)) return false;\n+\t\t\t\t\t// New imput combo has a different size\n+\t\t\t\t\tif (config1[0].length !== config2[0].length) return false;\n+\t\t\t\t\t// New input combo has different elements\n+\t\t\t\t\tif (config1[0].find((v, i) => config2[0][i] !== v)) return false;\n+\t\t\t\t} else if (config1[0] !== config2[0]) {\n+\t\t\t\t\t// Configs dont match\n+\t\t\t\t\treturn false;\n+\t\t\t\t}\n \n \t\t\t\tfor (const k in config1[1]) {\n \t\t\t\t\tif (k !== \"default\") {\n", "test_patch": "", "problem_statement": "[Bug] Can't connect primitive nodes to some converted inputs of cloned nodes\nSo far, I've only seen it in both KSamplers with _sampler_name_, _scheduler_, _add_noise_, and _return_with_leftover_noise_\r\n\r\nTo reproduce:\r\n - make a KSampler\r\n - convert widgets to inputs\r\n - clone the KSAMPLER\r\n - can't connect some of the inputs of both nodes to a primitive node\r\n\r\nIf converted back to the widget and back to input again it fixes itself\n", "hints_text": "noticed same behaviour like day or 1.5 ago, but didn't pay attention to this issue", "created_at": "2023-04-15T09:55:50Z"}
{"repo": "comfyanonymous/ComfyUI", "pull_number": 514, "instance_id": "comfyanonymous__ComfyUI-514", "issue_numbers": ["506"], "base_commit": "04d9bc13afd684a5bd4cb637e26972bb5aee43d1", "patch": "diff --git a/web/scripts/pnginfo.js b/web/scripts/pnginfo.js\nindex 31f4707391b..209b562a6fc 100644\n--- a/web/scripts/pnginfo.js\n+++ b/web/scripts/pnginfo.js\n@@ -131,6 +131,7 @@ export async function importA1111(graph, parameters) {\n \t\t\t}\n \n \t\t\tfunction replaceEmbeddings(text) {\n+\t\t\t\tif(!embeddings.length) return text;\n \t\t\t\treturn text.replaceAll(\n \t\t\t\t\tnew RegExp(\n \t\t\t\t\t\t\"\\\\b(\" + embeddings.map((e) => e.replace(/[.*+?^${}()|[\\]\\\\]/g, \"\\\\$&\")).join(\"\\\\b|\\\\b\") + \")\\\\b\",\n", "test_patch": "", "problem_statement": "Prompts loaded incorrectly from certain PNGs when there are no Embeddings available\nIf there are no embeddings in the embeddings folder, prompts loaded from some PNGs (mainly those generated by A1111 webgui) will load the entire prompt as if every word was an embedding, eg\r\n\r\n`embedding:masterpieceembedding:, embedding:bestembedding: embedding:qualityembedding:, embedding:masterpieceembedding:, embedding:asukaembedding: embedding:langleyembedding: embedding:sittingembedding: embedding:crossembedding: embedding:leggedembedding: embedding:onembedding: embedding:aembedding: embedding:chairembedding:`\r\n\r\nAdding a random embedding to the embeddings folder fixed the issue, but the issue returns when the embedding is removed. This didn't seem to happen with the PNGs generated by comfyui, but it did affect every other file I had on hand. This was on the latest portable windows build.\r\n\r\nNot a major issue, but it made transitioning from A1111 a bit confusing.\n", "hints_text": "", "created_at": "2023-04-15T09:30:13Z"}
{"repo": "comfyanonymous/ComfyUI", "pull_number": 413, "instance_id": "comfyanonymous__ComfyUI-413", "issue_numbers": ["397"], "base_commit": "349f15ed6f999807c38f18779c080ce714ae25fd", "patch": "diff --git a/comfy/cli_args.py b/comfy/cli_args.py\nindex a27dc7a7f83..5133e0ae57c 100644\n--- a/comfy/cli_args.py\n+++ b/comfy/cli_args.py\n@@ -4,8 +4,10 @@\n \n parser.add_argument(\"--listen\", nargs=\"?\", const=\"0.0.0.0\", default=\"127.0.0.1\", type=str, help=\"Specify the IP address to listen on (default: 127.0.0.1). If --listen is provided without an argument, it defaults to 0.0.0.0. (listens on all)\")\n parser.add_argument(\"--port\", type=int, default=8188, help=\"Set the listen port.\")\n+parser.add_argument(\"--cors\", default=None, nargs=\"?\", const=\"*\", help=\"Enable CORS (Cross-Origin Resource Sharing) with optional origin or allow all with default '*'.\")\n parser.add_argument(\"--extra-model-paths-config\", type=str, default=None, help=\"Load an extra_model_paths.yaml file.\")\n parser.add_argument(\"--output-directory\", type=str, default=None, help=\"Set the ComfyUI output directory.\")\n+parser.add_argument(\"--cuda-device\", type=int, default=None, help=\"Set the id of the cuda device this instance will use.\")\n parser.add_argument(\"--dont-upcast-attention\", action=\"store_true\", help=\"Disable upcasting of attention. Can boost speed but increase the chances of black images.\")\n \n attn_group = parser.add_mutually_exclusive_group()\n@@ -13,7 +15,6 @@\n attn_group.add_argument(\"--use-pytorch-cross-attention\", action=\"store_true\", help=\"Use the new pytorch 2.0 cross attention function.\")\n \n parser.add_argument(\"--disable-xformers\", action=\"store_true\", help=\"Disable xformers.\")\n-parser.add_argument(\"--cuda-device\", type=int, default=None, help=\"Set the id of the cuda device this instance will use.\")\n \n vram_group = parser.add_mutually_exclusive_group()\n vram_group.add_argument(\"--highvram\", action=\"store_true\", help=\"By default models will be unloaded to CPU memory after being used. This option keeps them in GPU memory.\")\ndiff --git a/server.py b/server.py\nindex 840d9a4e7c9..a9c0b45990e 100644\n--- a/server.py\n+++ b/server.py\n@@ -18,6 +18,7 @@\n     sys.exit()\n \n import mimetypes\n+from comfy.cli_args import args\n \n \n @web.middleware\n@@ -27,6 +28,23 @@ async def cache_control(request: web.Request, handler):\n         response.headers.setdefault('Cache-Control', 'no-cache')\n     return response\n \n+def create_cors_middleware(allowed_origin: str):\n+    @web.middleware\n+    async def cors_middleware(request: web.Request, handler):\n+        if request.method == \"OPTIONS\":\n+            # Pre-flight request. Reply successfully:\n+            response = web.Response()\n+        else:\n+            response = await handler(request)\n+\n+        response.headers['Access-Control-Allow-Origin'] = allowed_origin\n+        response.headers['Access-Control-Allow-Methods'] = 'POST, GET, DELETE, PUT, OPTIONS'\n+        response.headers['Access-Control-Allow-Headers'] = 'Content-Type, Authorization'\n+        response.headers['Access-Control-Allow-Credentials'] = 'true'\n+        return response\n+\n+    return cors_middleware\n+\n class PromptServer():\n     def __init__(self, loop):\n         PromptServer.instance = self\n@@ -37,7 +55,12 @@ def __init__(self, loop):\n         self.loop = loop\n         self.messages = asyncio.Queue()\n         self.number = 0\n-        self.app = web.Application(client_max_size=20971520, middlewares=[cache_control])\n+\n+        middlewares = [cache_control]\n+        if args.cors:\n+            middlewares.append(create_cors_middleware(args.cors))\n+\n+        self.app = web.Application(client_max_size=20971520, middlewares=middlewares)\n         self.sockets = dict()\n         self.web_root = os.path.join(os.path.dirname(\n             os.path.realpath(__file__)), \"web\")\n", "test_patch": "", "problem_statement": "CORS support in backend\nI want to test a frontend on a different port than the backend on `localhost`, but get blocked by CORS.\r\n\r\nSadly however `aiohttp_cors` has a 5-year-old bug that breaks CORS support for POST requests (while GET works fine): https://github.com/aio-libs/aiohttp-cors/issues/155\r\n\r\nI found that I had to use this code to work around it, but I couldn't get around modifying the backend https://github.com/home-assistant/core/issues/40513#issuecomment-700654471\n", "hints_text": "Isn't CORS just a header? Why would we need a library for that?\nI don't know, I wasn't sure how complicated it was since it seemed their library was the first search result for \"cors aiohttp.\" If there's a simpler way to add it it should be fine.", "created_at": "2023-04-05T17:16:52Z"}
{"repo": "comfyanonymous/ComfyUI", "pull_number": 409, "instance_id": "comfyanonymous__ComfyUI-409", "issue_numbers": ["387"], "base_commit": "f84f2508cc45a014cc27e023e9623db0450d237e", "patch": "diff --git a/comfy/model_management.py b/comfy/model_management.py\nindex 7dda073dc51..92c59efe746 100644\n--- a/comfy/model_management.py\n+++ b/comfy/model_management.py\n@@ -18,10 +18,17 @@ class VRAMState(Enum):\n total_vram_available_mb = -1\n \n accelerate_enabled = False\n+xpu_available = False\n \n try:\n     import torch\n-    total_vram = torch.cuda.mem_get_info(torch.cuda.current_device())[1] / (1024 * 1024)\n+    try:\n+        import intel_extension_for_pytorch as ipex\n+        if torch.xpu.is_available():\n+            xpu_available = True\n+            total_vram = torch.xpu.get_device_properties(torch.xpu.current_device()).total_memory / (1024 * 1024)\n+    except:\n+        total_vram = torch.cuda.mem_get_info(torch.cuda.current_device())[1] / (1024 * 1024)\n     total_ram = psutil.virtual_memory().total / (1024 * 1024)\n     if not args.normalvram and not args.cpu:\n         if total_vram <= 4096:\n@@ -122,6 +129,7 @@ def load_model_gpu(model):\n     global current_loaded_model\n     global vram_state\n     global model_accelerated\n+    global xpu_available\n \n     if model is current_loaded_model:\n         return\n@@ -140,14 +148,17 @@ def load_model_gpu(model):\n         pass\n     elif vram_state == VRAMState.NORMAL_VRAM or vram_state == VRAMState.HIGH_VRAM:\n         model_accelerated = False\n-        real_model.cuda()\n+        if xpu_available:\n+            real_model.to(\"xpu\")\n+        else:\n+            real_model.cuda()\n     else:\n         if vram_state == VRAMState.NO_VRAM:\n             device_map = accelerate.infer_auto_device_map(real_model, max_memory={0: \"256MiB\", \"cpu\": \"16GiB\"})\n         elif vram_state == VRAMState.LOW_VRAM:\n             device_map = accelerate.infer_auto_device_map(real_model, max_memory={0: \"{}MiB\".format(total_vram_available_mb), \"cpu\": \"16GiB\"})\n \n-        accelerate.dispatch_model(real_model, device_map=device_map, main_device=\"cuda\")\n+        accelerate.dispatch_model(real_model, device_map=device_map, main_device=\"xpu\" if xpu_available else \"cuda\")\n         model_accelerated = True\n     return current_loaded_model\n \n@@ -173,8 +184,12 @@ def load_controlnet_gpu(models):\n \n def load_if_low_vram(model):\n     global vram_state\n+    global xpu_available\n     if vram_state == VRAMState.LOW_VRAM or vram_state == VRAMState.NO_VRAM:\n-        return model.cuda()\n+        if xpu_available:\n+            return model.to(\"xpu\")\n+        else:\n+            return model.cuda()\n     return model\n \n def unload_if_low_vram(model):\n@@ -184,12 +199,16 @@ def unload_if_low_vram(model):\n     return model\n \n def get_torch_device():\n+    global xpu_available\n     if vram_state == VRAMState.MPS:\n         return torch.device(\"mps\")\n     if vram_state == VRAMState.CPU:\n         return torch.device(\"cpu\")\n     else:\n-        return torch.cuda.current_device()\n+        if xpu_available:\n+            return torch.device(\"xpu\")\n+        else:\n+            return torch.cuda.current_device()\n \n def get_autocast_device(dev):\n     if hasattr(dev, 'type'):\n@@ -219,6 +238,7 @@ def pytorch_attention_enabled():\n     return ENABLE_PYTORCH_ATTENTION\n \n def get_free_memory(dev=None, torch_free_too=False):\n+    global xpu_available\n     if dev is None:\n         dev = get_torch_device()\n \n@@ -226,12 +246,16 @@ def get_free_memory(dev=None, torch_free_too=False):\n         mem_free_total = psutil.virtual_memory().available\n         mem_free_torch = mem_free_total\n     else:\n-        stats = torch.cuda.memory_stats(dev)\n-        mem_active = stats['active_bytes.all.current']\n-        mem_reserved = stats['reserved_bytes.all.current']\n-        mem_free_cuda, _ = torch.cuda.mem_get_info(dev)\n-        mem_free_torch = mem_reserved - mem_active\n-        mem_free_total = mem_free_cuda + mem_free_torch\n+        if xpu_available:\n+            mem_free_total = torch.xpu.get_device_properties(dev).total_memory - torch.xpu.memory_allocated(dev)\n+            mem_free_torch = mem_free_total\n+        else:\n+            stats = torch.cuda.memory_stats(dev)\n+            mem_active = stats['active_bytes.all.current']\n+            mem_reserved = stats['reserved_bytes.all.current']\n+            mem_free_cuda, _ = torch.cuda.mem_get_info(dev)\n+            mem_free_torch = mem_reserved - mem_active\n+            mem_free_total = mem_free_cuda + mem_free_torch\n \n     if torch_free_too:\n         return (mem_free_total, mem_free_torch)\n@@ -256,7 +280,8 @@ def mps_mode():\n     return vram_state == VRAMState.MPS\n \n def should_use_fp16():\n-    if cpu_mode() or mps_mode():\n+    global xpu_available\n+    if cpu_mode() or mps_mode() or xpu_available:\n         return False #TODO ?\n \n     if torch.cuda.is_bf16_supported():\ndiff --git a/requirements.txt b/requirements.txt\nindex 3b4040a2918..0527b31df1a 100644\n--- a/requirements.txt\n+++ b/requirements.txt\n@@ -4,7 +4,7 @@ torchsde\n einops\n open-clip-torch\n transformers>=4.25.1\n-safetensors\n+safetensors>=0.3.0\n pytorch_lightning\n aiohttp\n accelerate\n", "test_patch": "", "problem_statement": "[Feature Request] Support Intel Extension for PyTorch (IPEX)\nSimilar to https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/6417\r\n\r\nUseful links:\r\n- https://github.com/intel/intel-extension-for-pytorch\r\n- https://pytorch.org/tutorials/recipes/intel_extension_for_pytorch.html\r\n\n", "hints_text": "I don't have an arc card but if it's like MPS it should be possible to implement with just some modifications to the comfy/model_management.py file like this pull request for example: https://github.com/comfyanonymous/ComfyUI/pull/244\n> I don't have an arc card but if it's like MPS it should be possible to implement with just some modifications to the comfy/model_management.py file like this pull request for example: #244\r\n\r\nOk, I will try to modify this section to support IPEX.\r\nOne more question: is torchaudio a required dependency? It is mentioned in the README, but doesn't seem to be used within the source code.\r\n\nIt's not required. There's a few dependencies that get pulled in that are not actually required, I just don't have time to properly get rid of them.", "created_at": "2023-04-05T13:28:58Z"}
{"repo": "comfyanonymous/ComfyUI", "pull_number": 207, "instance_id": "comfyanonymous__ComfyUI-207", "issue_numbers": ["201"], "base_commit": "451447bd9f719745ec234951095f87ba02921dc7", "patch": "diff --git a/web/scripts/app.js b/web/scripts/app.js\nindex dc61c5a66b7..fd410cd3009 100644\n--- a/web/scripts/app.js\n+++ b/web/scripts/app.js\n@@ -494,6 +494,7 @@ class ComfyApp {\n \n \t\t// Create and mount the LiteGraph in the DOM\n \t\tconst canvasEl = (this.canvasEl = Object.assign(document.createElement(\"canvas\"), { id: \"graph-canvas\" }));\n+\t\tcanvasEl.tabIndex = \"1\"\n \t\tdocument.body.prepend(canvasEl);\n \n \t\tthis.graph = new LGraph();\n", "test_patch": "", "problem_statement": "Remove by \"Delete\" button\nAnd maybe ctrl+c ctrl+v functionality, or i something missed?\n", "hints_text": "I've tracked down this issue I think and it is related to tab indexes and keydown events.  I've made a PR", "created_at": "2023-03-21T18:34:05Z"}
