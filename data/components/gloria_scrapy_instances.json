[
    {
        "repo": "scrapy/scrapy",
        "pull_number": 6542,
        "instance_id": "scrapy__scrapy-6542",
        "issue_numbers": [
            "6505"
        ],
        "base_commit": "ab5cb7c7d9e268b501009d991d97ca19b6f7fe96",
        "patch": "diff --git a/scrapy/contracts/__init__.py b/scrapy/contracts/__init__.py\nindex 9071395e3d9..3b4f932a014 100644\n--- a/scrapy/contracts/__init__.py\n+++ b/scrapy/contracts/__init__.py\n@@ -38,9 +38,7 @@ def add_pre_hook(self, request: Request, results: TestResult) -> Request:\n             assert cb is not None\n \n             @wraps(cb)\n-            def wrapper(  # pylint: disable=inconsistent-return-statements\n-                response: Response, **cb_kwargs: Any\n-            ) -> list[Any]:\n+            def wrapper(response: Response, **cb_kwargs: Any) -> list[Any]:\n                 try:\n                     results.startTest(self.testcase_pre)\n                     self.pre_process(response)\n@@ -51,13 +49,10 @@ def wrapper(  # pylint: disable=inconsistent-return-statements\n                     results.addError(self.testcase_pre, sys.exc_info())\n                 else:\n                     results.addSuccess(self.testcase_pre)\n-                finally:\n-                    cb_result = cb(response, **cb_kwargs)\n-                    if isinstance(cb_result, (AsyncGenerator, CoroutineType)):\n-                        raise TypeError(\"Contracts don't support async callbacks\")\n-                    return list(  # pylint: disable=return-in-finally\n-                        cast(Iterable[Any], iterate_spider_output(cb_result))\n-                    )\n+                cb_result = cb(response, **cb_kwargs)\n+                if isinstance(cb_result, (AsyncGenerator, CoroutineType)):\n+                    raise TypeError(\"Contracts don't support async callbacks\")\n+                return list(cast(Iterable[Any], iterate_spider_output(cb_result)))\n \n             request.callback = wrapper\n \n@@ -69,9 +64,7 @@ def add_post_hook(self, request: Request, results: TestResult) -> Request:\n             assert cb is not None\n \n             @wraps(cb)\n-            def wrapper(  # pylint: disable=inconsistent-return-statements\n-                response: Response, **cb_kwargs: Any\n-            ) -> list[Any]:\n+            def wrapper(response: Response, **cb_kwargs: Any) -> list[Any]:\n                 cb_result = cb(response, **cb_kwargs)\n                 if isinstance(cb_result, (AsyncGenerator, CoroutineType)):\n                     raise TypeError(\"Contracts don't support async callbacks\")\n@@ -86,8 +79,7 @@ def wrapper(  # pylint: disable=inconsistent-return-statements\n                     results.addError(self.testcase_post, sys.exc_info())\n                 else:\n                     results.addSuccess(self.testcase_post)\n-                finally:\n-                    return output  # pylint: disable=return-in-finally\n+                return output\n \n             request.callback = wrapper\n \n",
        "test_patch": "diff --git a/tests/test_contracts.py b/tests/test_contracts.py\nindex d578b3af450..b0cb92d12d9 100644\n--- a/tests/test_contracts.py\n+++ b/tests/test_contracts.py\n@@ -556,3 +556,61 @@ def test_inherited_contracts(self):\n \n         requests = self.conman.from_spider(spider, self.results)\n         self.assertTrue(requests)\n+\n+\n+class CustomFailContractPreProcess(Contract):\n+    name = \"test_contract\"\n+\n+    def pre_process(self, response):\n+        raise KeyboardInterrupt(\"Pre-process exception\")\n+\n+\n+class CustomFailContractPostProcess(Contract):\n+    name = \"test_contract\"\n+\n+    def post_process(self, response):\n+        raise KeyboardInterrupt(\"Post-process exception\")\n+\n+\n+class CustomContractPrePostProcess(unittest.TestCase):\n+\n+    def setUp(self):\n+        self.results = TextTestResult(stream=None, descriptions=False, verbosity=0)\n+\n+    def test_pre_hook_keyboard_interrupt(self):\n+        spider = TestSpider()\n+        response = ResponseMock()\n+        contract = CustomFailContractPreProcess(spider.returns_request)\n+        conman = ContractsManager([contract])\n+\n+        try:\n+            request = conman.from_method(spider.returns_request, self.results)\n+            contract.add_pre_hook(request, self.results)\n+            # Expect this to raise a KeyboardInterrupt\n+            request.callback(response, **request.cb_kwargs)\n+        except KeyboardInterrupt as e:\n+            self.assertEqual(str(e), \"Pre-process exception\")\n+        else:\n+            self.fail(\"KeyboardInterrupt not raised\")\n+\n+        self.assertFalse(self.results.failures)\n+        self.assertFalse(self.results.errors)\n+\n+    def test_post_hook_keyboard_interrupt(self):\n+        spider = TestSpider()\n+        response = ResponseMock()\n+        contract = CustomFailContractPostProcess(spider.returns_request)\n+        conman = ContractsManager([contract])\n+\n+        try:\n+            request = conman.from_method(spider.returns_request, self.results)\n+            contract.add_post_hook(request, self.results)\n+            # Expect this to raise a KeyboardInterrupt\n+            request.callback(response, **request.cb_kwargs)\n+        except KeyboardInterrupt as e:\n+            self.assertEqual(str(e), \"Post-process exception\")\n+        else:\n+            self.fail(\"KeyboardInterrupt not raised\")\n+\n+        self.assertFalse(self.results.failures)\n+        self.assertFalse(self.results.errors)\n",
        "problem_statement": "return in finally can swallow exception\n### Description\r\n\r\nThere are two places in `scrapy/contracts/__init__.py` where a `finally:` body contains a `return` statement, which would swallow any in-flight exception. \r\n\r\nThis means that if a `BaseException` (such as `KeyboardInterrupt`) is raised from the body, or any exception is raised from one of the `except:` clause, it will not propagate on as expected. \r\n\r\nThe pylint warning about this was suppressed in [this commit](https://github.com/scrapy/scrapy/commit/991121fa91aee4d428ae09e75427d4e91970a41b) but it doesn't seem like there was justification for that.\r\n\r\nThese are the two locations:\r\nhttps://github.com/scrapy/scrapy/blame/b4bad97eae6bcce790a626d244c63589f4134408/scrapy/contracts/__init__.py#L56\r\n\r\nhttps://github.com/scrapy/scrapy/blame/b4bad97eae6bcce790a626d244c63589f4134408/scrapy/contracts/__init__.py#L86\r\n\r\nSee also https://docs.python.org/3/tutorial/errors.html#defining-clean-up-actions.\n",
        "hints_text": "> If the finally clause executes a [break](https://docs.python.org/3/reference/simple_stmts.html#break), [continue](https://docs.python.org/3/reference/simple_stmts.html#continue) or [return](https://docs.python.org/3/reference/simple_stmts.html#return) statement, exceptions are not re-raised.\r\n\r\nTIL\nHey,  \r\nWhat about re-raising the issue with a general raise statement in every except block along with putting the return statement outside the finally block?  \r\nIf this solution seems promising, I'd like to contribute to the same. I would appreciate your insights.\nI don't remember why I silenced them :-/\nIs this issue still open?\n@divyranjan17 it is.\r\n\r\n@AdityaS8804 I don't think that makes sense to me.\nFrom a quick look, it seems to me that simply taking the return part out of finally, and put it after the try-except blocks, could be all that\u2019s needed. But we should also first write tests that detect the issues before we address those issues.\n> From a quick look, it seems to me that simply taking the return part out of finally, and put it after the try-except blocks, could be all that\u2019s needed.\r\n\r\nThis matches my first impression.\nIs there a way to reproduce a this failure?\n> Is there a way to reproduce a this failure?\r\n\r\nFor the first issue, for example, it seems like raising `KeyboardInterrupt` from an implementation of https://docs.scrapy.org/en/2.11/topics/contracts.html#scrapy.contracts.Contract.pre_process should see that exception raise, but will instead silence it.\n\r\nI can think of 3 ways to tackle this issue\r\n\r\n#### Solution 1: Using a Temporary Variable for Return Value\r\nWe can capture the callback result in a variable outside the `finally` block and then return it at the end of the function. By avoiding `return` inside `finally`, exceptions propagate naturally, allowing errors to be handled as expected.\r\n\r\n**Simply:**\r\n\r\n- Store the callback output in a variable (e.g., `cb_result`).\r\n- Avoid using `return` in the `finally` block.\r\n- Return `cb_result` at the end of the function, outside of any `try/finally` structure.\r\n\r\n```python\r\ncb_result = None\r\ntry:\r\n    cb_result = cb(response, **cb_kwargs)\r\nfinally:\r\n    pass  # Any final cleanup here\r\nreturn list(iterate_spider_output(cb_result))\r\n```\r\n\r\n#### Solution 2: Separating Error Logging and Result Processing\r\n- Create a helper function, `log_results()`, to handle logging outcomes.\r\n- Call `log_results()` within `try/except` blocks to process success or errors.\r\n- Return `cb_result` outside of the `try` block without `finally`.\r\n\r\n```python\r\ndef log_results(testcase, exception_info=None):\r\n    if exception_info:\r\n        # Log failure\r\n```\r\n\r\n#### Solution 3: Wrapping Return Values with Exception Handling\r\n- Define `process_result` to manage callback outputs while capturing exceptions.\r\n- Invoke `process_result` instead of a direct return in the callback.\r\n- Ensure all exception info is handled without using a return in `finally`.\r\n\r\n```python\r\ndef process_result(cb, response, **cb_kwargs):\r\n    try:\r\n        cb_result = cb(response, **cb_kwargs)\r\n    except Exception as exc:\r\n        log_error(exc)\r\n    return list(iterate_spider_output(cb_result))\r\n```\r\n\nIs this AI-generated?\nyes Sol.1 and Sol.3 were suggested by github copilot \nThat's unfortunate, especially as the way forward was already suggested earlier.\n[Here](https://github.com/scrapy/scrapy/issues/6505#issuecomment-2434584223), specifically.\n> [Here](https://github.com/scrapy/scrapy/issues/6505#issuecomment-2434584223), specifically.\r\n\r\nUnderstood",
        "created_at": "2024-11-14T03:19:30Z",
        "version": "2.11",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "tests/test_contracts.py"
        ]
    },
    {
        "repo": "scrapy/scrapy",
        "pull_number": 6540,
        "instance_id": "scrapy__scrapy-6540",
        "issue_numbers": [
            "6534"
        ],
        "base_commit": "b042ad255db139adc740cd97047b6607889f9f1c",
        "patch": "diff --git a/docs/topics/email.rst b/docs/topics/email.rst\nindex d6a7ad354cb..8f7a2357a5a 100644\n--- a/docs/topics/email.rst\n+++ b/docs/topics/email.rst\n@@ -27,13 +27,13 @@ the standard ``__init__`` method:\n \n     mailer = MailSender()\n \n-Or you can instantiate it passing a Scrapy settings object, which will respect\n-the :ref:`settings <topics-email-settings>`:\n+Or you can instantiate it passing a :class:`scrapy.Crawler` instance, which\n+will respect the :ref:`settings <topics-email-settings>`:\n \n .. skip: start\n .. code-block:: python\n \n-    mailer = MailSender.from_settings(settings)\n+    mailer = MailSender.from_crawler(crawler)\n \n And here is how to use it to send an e-mail (without attachments):\n \n@@ -81,13 +81,13 @@ rest of the framework.\n     :param smtpssl: enforce using a secure SSL connection\n     :type smtpssl: bool\n \n-    .. classmethod:: from_settings(settings)\n+    .. classmethod:: from_crawler(crawler)\n \n-        Instantiate using a Scrapy settings object, which will respect\n-        :ref:`these Scrapy settings <topics-email-settings>`.\n+        Instantiate using a :class:`scrapy.Crawler` instance, which will\n+        respect :ref:`these Scrapy settings <topics-email-settings>`.\n \n-        :param settings: the e-mail recipients\n-        :type settings: :class:`scrapy.settings.Settings` object\n+        :param crawler: the crawler\n+        :type settings: :class:`scrapy.Crawler` object\n \n     .. method:: send(to, subject, body, cc=None, attachs=(), mimetype='text/plain', charset=None)\n \ndiff --git a/docs/topics/request-response.rst b/docs/topics/request-response.rst\nindex 7c15b67e8f3..710e2e1314e 100644\n--- a/docs/topics/request-response.rst\n+++ b/docs/topics/request-response.rst\n@@ -488,7 +488,7 @@ A request fingerprinter is a class that must implement the following method:\n    :param request: request to fingerprint\n    :type request: scrapy.http.Request\n \n-Additionally, it may also implement the following methods:\n+Additionally, it may also implement the following method:\n \n .. classmethod:: from_crawler(cls, crawler)\n    :noindex:\n@@ -504,13 +504,6 @@ Additionally, it may also implement the following methods:\n    :param crawler: crawler that uses this request fingerprinter\n    :type crawler: :class:`~scrapy.crawler.Crawler` object\n \n-.. classmethod:: from_settings(cls, settings)\n-\n-   If present, and ``from_crawler`` is not defined, this class method is called\n-   to create a request fingerprinter instance from a\n-   :class:`~scrapy.settings.Settings` object. It must return a new instance of\n-   the request fingerprinter.\n-\n .. currentmodule:: scrapy.http\n \n The :meth:`fingerprint` method of the default request fingerprinter,\ndiff --git a/scrapy/core/downloader/contextfactory.py b/scrapy/core/downloader/contextfactory.py\nindex f80f832a706..8e17eab9aa7 100644\n--- a/scrapy/core/downloader/contextfactory.py\n+++ b/scrapy/core/downloader/contextfactory.py\n@@ -21,6 +21,7 @@\n     ScrapyClientTLSOptions,\n     openssl_methods,\n )\n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.misc import build_from_crawler, load_object\n \n if TYPE_CHECKING:\n@@ -69,6 +70,31 @@ def from_settings(\n         method: int = SSL.SSLv23_METHOD,\n         *args: Any,\n         **kwargs: Any,\n+    ) -> Self:\n+        warnings.warn(\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return cls._from_settings(settings, method, *args, **kwargs)\n+\n+    @classmethod\n+    def from_crawler(\n+        cls,\n+        crawler: Crawler,\n+        method: int = SSL.SSLv23_METHOD,\n+        *args: Any,\n+        **kwargs: Any,\n+    ) -> Self:\n+        return cls._from_settings(crawler.settings, method, *args, **kwargs)\n+\n+    @classmethod\n+    def _from_settings(\n+        cls,\n+        settings: BaseSettings,\n+        method: int = SSL.SSLv23_METHOD,\n+        *args: Any,\n+        **kwargs: Any,\n     ) -> Self:\n         tls_verbose_logging: bool = settings.getbool(\n             \"DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING\"\ndiff --git a/scrapy/dupefilters.py b/scrapy/dupefilters.py\nindex d37d2741a48..7b8eea135e7 100644\n--- a/scrapy/dupefilters.py\n+++ b/scrapy/dupefilters.py\n@@ -1,9 +1,11 @@\n from __future__ import annotations\n \n import logging\n+import warnings\n from pathlib import Path\n from typing import TYPE_CHECKING\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.job import job_dir\n from scrapy.utils.request import (\n     RequestFingerprinter,\n@@ -26,6 +28,15 @@\n class BaseDupeFilter:\n     @classmethod\n     def from_settings(cls, settings: BaseSettings) -> Self:\n+        warnings.warn(\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return cls()\n+\n+    @classmethod\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         return cls()\n \n     def request_seen(self, request: Request) -> bool:\n@@ -72,17 +83,31 @@ def from_settings(\n         *,\n         fingerprinter: RequestFingerprinterProtocol | None = None,\n     ) -> Self:\n-        debug = settings.getbool(\"DUPEFILTER_DEBUG\")\n-        return cls(job_dir(settings), debug, fingerprinter=fingerprinter)\n+        warnings.warn(\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return cls._from_settings(settings, fingerprinter=fingerprinter)\n \n     @classmethod\n     def from_crawler(cls, crawler: Crawler) -> Self:\n         assert crawler.request_fingerprinter\n-        return cls.from_settings(\n+        return cls._from_settings(\n             crawler.settings,\n             fingerprinter=crawler.request_fingerprinter,\n         )\n \n+    @classmethod\n+    def _from_settings(\n+        cls,\n+        settings: BaseSettings,\n+        *,\n+        fingerprinter: RequestFingerprinterProtocol | None = None,\n+    ) -> Self:\n+        debug = settings.getbool(\"DUPEFILTER_DEBUG\")\n+        return cls(job_dir(settings), debug, fingerprinter=fingerprinter)\n+\n     def request_seen(self, request: Request) -> bool:\n         fp = self.request_fingerprint(request)\n         if fp in self.fingerprints:\ndiff --git a/scrapy/extensions/feedexport.py b/scrapy/extensions/feedexport.py\nindex 6ab88dbb467..27f0b79ae01 100644\n--- a/scrapy/extensions/feedexport.py\n+++ b/scrapy/extensions/feedexport.py\n@@ -62,6 +62,11 @@ def build_storage(\n     preargs: Iterable[Any] = (),\n     **kwargs: Any,\n ) -> _StorageT:\n+    warnings.warn(\n+        \"scrapy.extensions.feedexport.build_storage() is deprecated, call the builder directly.\",\n+        category=ScrapyDeprecationWarning,\n+        stacklevel=2,\n+    )\n     kwargs[\"feed_options\"] = feed_options\n     return builder(*preargs, uri, *args, **kwargs)\n \n@@ -248,8 +253,7 @@ def from_crawler(\n         *,\n         feed_options: dict[str, Any] | None = None,\n     ) -> Self:\n-        return build_storage(\n-            cls,\n+        return cls(\n             uri,\n             access_key=crawler.settings[\"AWS_ACCESS_KEY_ID\"],\n             secret_key=crawler.settings[\"AWS_SECRET_ACCESS_KEY\"],\n@@ -323,10 +327,9 @@ def from_crawler(\n         *,\n         feed_options: dict[str, Any] | None = None,\n     ) -> Self:\n-        return build_storage(\n-            cls,\n+        return cls(\n             uri,\n-            crawler.settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\"),\n+            use_active_mode=crawler.settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\"),\n             feed_options=feed_options,\n         )\n \n@@ -407,15 +410,12 @@ def start_exporting(self) -> None:\n             self.exporter.start_exporting()\n             self._exporting = True\n \n-    def _get_instance(\n-        self, objcls: type[BaseItemExporter], *args: Any, **kwargs: Any\n-    ) -> BaseItemExporter:\n-        return build_from_crawler(objcls, self.crawler, *args, **kwargs)\n-\n     def _get_exporter(\n         self, file: IO[bytes], format: str, *args: Any, **kwargs: Any\n     ) -> BaseItemExporter:\n-        return self._get_instance(self.exporters[format], file, *args, **kwargs)\n+        return build_from_crawler(\n+            self.exporters[format], self.crawler, file, *args, **kwargs\n+        )\n \n     def finish_exporting(self) -> None:\n         if self._exporting:\n@@ -692,34 +692,8 @@ def _storage_supported(self, uri: str, feed_options: dict[str, Any]) -> bool:\n     def _get_storage(\n         self, uri: str, feed_options: dict[str, Any]\n     ) -> FeedStorageProtocol:\n-        \"\"\"Fork of create_instance specific to feed storage classes\n-\n-        It supports not passing the *feed_options* parameters to classes that\n-        do not support it, and issuing a deprecation warning instead.\n-        \"\"\"\n         feedcls = self.storages.get(urlparse(uri).scheme, self.storages[\"file\"])\n-        crawler = getattr(self, \"crawler\", None)\n-\n-        def build_instance(\n-            builder: type[FeedStorageProtocol], *preargs: Any\n-        ) -> FeedStorageProtocol:\n-            return build_storage(\n-                builder, uri, feed_options=feed_options, preargs=preargs\n-            )\n-\n-        instance: FeedStorageProtocol\n-        if crawler and hasattr(feedcls, \"from_crawler\"):\n-            instance = build_instance(feedcls.from_crawler, crawler)\n-            method_name = \"from_crawler\"\n-        elif hasattr(feedcls, \"from_settings\"):\n-            instance = build_instance(feedcls.from_settings, self.settings)\n-            method_name = \"from_settings\"\n-        else:\n-            instance = build_instance(feedcls)\n-            method_name = \"__new__\"\n-        if instance is None:\n-            raise TypeError(f\"{feedcls.__qualname__}.{method_name} returned None\")\n-        return instance\n+        return build_from_crawler(feedcls, self.crawler, uri, feed_options=feed_options)\n \n     def _get_uri_params(\n         self,\ndiff --git a/scrapy/extensions/memusage.py b/scrapy/extensions/memusage.py\nindex 73d864d5dc1..d7f810107bd 100644\n--- a/scrapy/extensions/memusage.py\n+++ b/scrapy/extensions/memusage.py\n@@ -48,7 +48,7 @@ def __init__(self, crawler: Crawler):\n         self.check_interval: float = crawler.settings.getfloat(\n             \"MEMUSAGE_CHECK_INTERVAL_SECONDS\"\n         )\n-        self.mail: MailSender = MailSender.from_settings(crawler.settings)\n+        self.mail: MailSender = MailSender.from_crawler(crawler)\n         crawler.signals.connect(self.engine_started, signal=signals.engine_started)\n         crawler.signals.connect(self.engine_stopped, signal=signals.engine_stopped)\n \ndiff --git a/scrapy/extensions/statsmailer.py b/scrapy/extensions/statsmailer.py\nindex 600eebcf2de..22162864205 100644\n--- a/scrapy/extensions/statsmailer.py\n+++ b/scrapy/extensions/statsmailer.py\n@@ -33,7 +33,7 @@ def from_crawler(cls, crawler: Crawler) -> Self:\n         recipients: list[str] = crawler.settings.getlist(\"STATSMAILER_RCPTS\")\n         if not recipients:\n             raise NotConfigured\n-        mail: MailSender = MailSender.from_settings(crawler.settings)\n+        mail: MailSender = MailSender.from_crawler(crawler)\n         assert crawler.stats\n         o = cls(crawler.stats, recipients, mail)\n         crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\ndiff --git a/scrapy/mail.py b/scrapy/mail.py\nindex ce7beb77307..3c40fea34c6 100644\n--- a/scrapy/mail.py\n+++ b/scrapy/mail.py\n@@ -7,6 +7,7 @@\n from __future__ import annotations\n \n import logging\n+import warnings\n from email import encoders as Encoders\n from email.mime.base import MIMEBase\n from email.mime.multipart import MIMEMultipart\n@@ -19,6 +20,7 @@\n from twisted.internet import ssl\n from twisted.internet.defer import Deferred\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.python import to_bytes\n \n@@ -32,6 +34,7 @@\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy.crawler import Crawler\n     from scrapy.settings import BaseSettings\n \n \n@@ -72,6 +75,19 @@ def __init__(\n \n     @classmethod\n     def from_settings(cls, settings: BaseSettings) -> Self:\n+        warnings.warn(\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return cls._from_settings(settings)\n+\n+    @classmethod\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n+        return cls._from_settings(crawler.settings)\n+\n+    @classmethod\n+    def _from_settings(cls, settings: BaseSettings) -> Self:\n         return cls(\n             smtphost=settings[\"MAIL_HOST\"],\n             mailfrom=settings[\"MAIL_FROM\"],\ndiff --git a/scrapy/middleware.py b/scrapy/middleware.py\nindex b6a4278952b..2b67dcd21a1 100644\n--- a/scrapy/middleware.py\n+++ b/scrapy/middleware.py\n@@ -2,12 +2,13 @@\n \n import logging\n import pprint\n+import warnings\n from collections import defaultdict, deque\n from typing import TYPE_CHECKING, Any, TypeVar, cast\n \n-from scrapy.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.utils.defer import process_chain, process_parallel\n-from scrapy.utils.misc import build_from_crawler, build_from_settings, load_object\n+from scrapy.utils.misc import build_from_crawler, load_object\n \n if TYPE_CHECKING:\n     from collections.abc import Callable, Iterable\n@@ -20,7 +21,7 @@\n \n     from scrapy import Spider\n     from scrapy.crawler import Crawler\n-    from scrapy.settings import Settings\n+    from scrapy.settings import BaseSettings, Settings\n \n     _P = ParamSpec(\"_P\")\n \n@@ -50,8 +51,33 @@ def __init__(self, *middlewares: Any) -> None:\n     def _get_mwlist_from_settings(cls, settings: Settings) -> list[Any]:\n         raise NotImplementedError\n \n+    @staticmethod\n+    def _build_from_settings(objcls: type[_T], settings: BaseSettings) -> _T:\n+        if hasattr(objcls, \"from_settings\"):\n+            instance = objcls.from_settings(settings)  # type: ignore[attr-defined]\n+            method_name = \"from_settings\"\n+        else:\n+            instance = objcls()\n+            method_name = \"__new__\"\n+        if instance is None:\n+            raise TypeError(f\"{objcls.__qualname__}.{method_name} returned None\")\n+        return cast(_T, instance)\n+\n     @classmethod\n     def from_settings(cls, settings: Settings, crawler: Crawler | None = None) -> Self:\n+        warnings.warn(\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return cls._from_settings(settings, crawler)\n+\n+    @classmethod\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n+        return cls._from_settings(crawler.settings, crawler)\n+\n+    @classmethod\n+    def _from_settings(cls, settings: Settings, crawler: Crawler | None = None) -> Self:\n         mwlist = cls._get_mwlist_from_settings(settings)\n         middlewares = []\n         enabled = []\n@@ -61,7 +87,7 @@ def from_settings(cls, settings: Settings, crawler: Crawler | None = None) -> Se\n                 if crawler is not None:\n                     mw = build_from_crawler(mwcls, crawler)\n                 else:\n-                    mw = build_from_settings(mwcls, settings)\n+                    mw = MiddlewareManager._build_from_settings(mwcls, settings)\n                 middlewares.append(mw)\n                 enabled.append(clspath)\n             except NotConfigured as e:\n@@ -82,10 +108,6 @@ def from_settings(cls, settings: Settings, crawler: Crawler | None = None) -> Se\n         )\n         return cls(*middlewares)\n \n-    @classmethod\n-    def from_crawler(cls, crawler: Crawler) -> Self:\n-        return cls.from_settings(crawler.settings, crawler)\n-\n     def _add_middleware(self, mw: Any) -> None:\n         if hasattr(mw, \"open_spider\"):\n             self.methods[\"open_spider\"].append(mw.open_spider)\ndiff --git a/scrapy/pipelines/files.py b/scrapy/pipelines/files.py\nindex 4a8639c220b..196b54acb7f 100644\n--- a/scrapy/pipelines/files.py\n+++ b/scrapy/pipelines/files.py\n@@ -12,6 +12,7 @@\n import logging\n import mimetypes\n import time\n+import warnings\n from collections import defaultdict\n from contextlib import suppress\n from ftplib import FTP\n@@ -24,16 +25,17 @@\n from twisted.internet.defer import Deferred, maybeDeferred\n from twisted.internet.threads import deferToThread\n \n-from scrapy.exceptions import IgnoreRequest, NotConfigured\n+from scrapy.exceptions import IgnoreRequest, NotConfigured, ScrapyDeprecationWarning\n from scrapy.http import Request, Response\n from scrapy.http.request import NO_CALLBACK\n from scrapy.pipelines.media import FileInfo, FileInfoOrError, MediaPipeline\n-from scrapy.settings import Settings\n+from scrapy.settings import BaseSettings, Settings\n from scrapy.utils.boto import is_botocore_available\n from scrapy.utils.datatypes import CaseInsensitiveDict\n+from scrapy.utils.deprecate import method_is_overridden\n from scrapy.utils.ftp import ftp_store_file\n from scrapy.utils.log import failure_to_exc_info\n-from scrapy.utils.python import to_bytes\n+from scrapy.utils.python import get_func_args, global_object_name, to_bytes\n from scrapy.utils.request import referer_str\n \n if TYPE_CHECKING:\n@@ -46,6 +48,7 @@\n     from typing_extensions import Self\n \n     from scrapy import Spider\n+    from scrapy.crawler import Crawler\n \n \n logger = logging.getLogger(__name__)\n@@ -443,12 +446,24 @@ def __init__(\n         store_uri: str | PathLike[str],\n         download_func: Callable[[Request, Spider], Response] | None = None,\n         settings: Settings | dict[str, Any] | None = None,\n+        *,\n+        crawler: Crawler | None = None,\n     ):\n         store_uri = _to_string(store_uri)\n         if not store_uri:\n             raise NotConfigured\n \n-        if isinstance(settings, dict) or settings is None:\n+        if crawler is not None:\n+            if settings is not None:\n+                warnings.warn(\n+                    f\"FilesPipeline.__init__() was called with a crawler instance and a settings instance\"\n+                    f\" when creating {global_object_name(self.__class__)}. The settings instance will be ignored\"\n+                    f\" and crawler.settings will be used. The settings argument will be removed in a future Scrapy version.\",\n+                    category=ScrapyDeprecationWarning,\n+                    stacklevel=2,\n+                )\n+            settings = crawler.settings\n+        elif isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n         cls_name = \"FilesPipeline\"\n         self.store: FilesStoreProtocol = self._get_store(store_uri)\n@@ -467,10 +482,54 @@ def __init__(\n             resolve(\"FILES_RESULT_FIELD\"), self.FILES_RESULT_FIELD\n         )\n \n-        super().__init__(download_func=download_func, settings=settings)\n+        super().__init__(\n+            download_func=download_func,\n+            settings=settings if not crawler else None,\n+            crawler=crawler,\n+        )\n \n     @classmethod\n     def from_settings(cls, settings: Settings) -> Self:\n+        warnings.warn(\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return cls._from_settings(settings, None)\n+\n+    @classmethod\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n+        if method_is_overridden(cls, FilesPipeline, \"from_settings\"):\n+            warnings.warn(\n+                f\"{global_object_name(cls)} overrides FilesPipeline.from_settings().\"\n+                f\" This method is deprecated and won't be called in future Scrapy versions,\"\n+                f\" please update your code so that it overrides from_crawler() instead.\",\n+                category=ScrapyDeprecationWarning,\n+            )\n+            o = cls.from_settings(crawler.settings)\n+            o._finish_init(crawler)\n+            return o\n+        return cls._from_settings(crawler.settings, crawler)\n+\n+    @classmethod\n+    def _from_settings(cls, settings: Settings, crawler: Crawler | None) -> Self:\n+        cls._update_stores(settings)\n+        store_uri = settings[\"FILES_STORE\"]\n+        if \"crawler\" in get_func_args(cls.__init__):\n+            o = cls(store_uri, crawler=crawler)\n+        else:\n+            o = cls(store_uri, settings=settings)\n+            if crawler:\n+                o._finish_init(crawler)\n+            warnings.warn(\n+                f\"{global_object_name(cls)}.__init__() doesn't take a crawler argument.\"\n+                \" This is deprecated and the argument will be required in future Scrapy versions.\",\n+                category=ScrapyDeprecationWarning,\n+            )\n+        return o\n+\n+    @classmethod\n+    def _update_stores(cls, settings: BaseSettings) -> None:\n         s3store: type[S3FilesStore] = cast(type[S3FilesStore], cls.STORE_SCHEMES[\"s3\"])\n         s3store.AWS_ACCESS_KEY_ID = settings[\"AWS_ACCESS_KEY_ID\"]\n         s3store.AWS_SECRET_ACCESS_KEY = settings[\"AWS_SECRET_ACCESS_KEY\"]\n@@ -494,9 +553,6 @@ def from_settings(cls, settings: Settings) -> Self:\n         ftp_store.FTP_PASSWORD = settings[\"FTP_PASSWORD\"]\n         ftp_store.USE_ACTIVE_MODE = settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\")\n \n-        store_uri = settings[\"FILES_STORE\"]\n-        return cls(store_uri, settings=settings)\n-\n     def _get_store(self, uri: str) -> FilesStoreProtocol:\n         if Path(uri).is_absolute():  # to support win32 paths like: C:\\\\some\\dir\n             scheme = \"file\"\ndiff --git a/scrapy/pipelines/images.py b/scrapy/pipelines/images.py\nindex 2c4c9376e49..e86e7c4930e 100644\n--- a/scrapy/pipelines/images.py\n+++ b/scrapy/pipelines/images.py\n@@ -8,25 +8,19 @@\n \n import functools\n import hashlib\n+import warnings\n from contextlib import suppress\n from io import BytesIO\n-from typing import TYPE_CHECKING, Any, cast\n+from typing import TYPE_CHECKING, Any\n \n from itemadapter import ItemAdapter\n \n-from scrapy.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.http import Request, Response\n from scrapy.http.request import NO_CALLBACK\n-from scrapy.pipelines.files import (\n-    FileException,\n-    FilesPipeline,\n-    FTPFilesStore,\n-    GCSFilesStore,\n-    S3FilesStore,\n-    _md5sum,\n-)\n+from scrapy.pipelines.files import FileException, FilesPipeline, _md5sum\n from scrapy.settings import Settings\n-from scrapy.utils.python import to_bytes\n+from scrapy.utils.python import get_func_args, global_object_name, to_bytes\n \n if TYPE_CHECKING:\n     from collections.abc import Callable, Iterable\n@@ -38,6 +32,7 @@\n     from typing_extensions import Self\n \n     from scrapy import Spider\n+    from scrapy.crawler import Crawler\n     from scrapy.pipelines.media import FileInfoOrError, MediaPipeline\n \n \n@@ -64,6 +59,8 @@ def __init__(\n         store_uri: str | PathLike[str],\n         download_func: Callable[[Request, Spider], Response] | None = None,\n         settings: Settings | dict[str, Any] | None = None,\n+        *,\n+        crawler: Crawler | None = None,\n     ):\n         try:\n             from PIL import Image\n@@ -74,9 +71,24 @@ def __init__(\n                 \"ImagesPipeline requires installing Pillow 4.0.0 or later\"\n             )\n \n-        super().__init__(store_uri, settings=settings, download_func=download_func)\n+        super().__init__(\n+            store_uri,\n+            settings=settings if not crawler else None,\n+            download_func=download_func,\n+            crawler=crawler,\n+        )\n \n-        if isinstance(settings, dict) or settings is None:\n+        if crawler is not None:\n+            if settings is not None:\n+                warnings.warn(\n+                    f\"ImagesPipeline.__init__() was called with a crawler instance and a settings instance\"\n+                    f\" when creating {global_object_name(self.__class__)}. The settings instance will be ignored\"\n+                    f\" and crawler.settings will be used. The settings argument will be removed in a future Scrapy version.\",\n+                    category=ScrapyDeprecationWarning,\n+                    stacklevel=2,\n+                )\n+            settings = crawler.settings\n+        elif isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n \n         resolve = functools.partial(\n@@ -108,32 +120,21 @@ def __init__(\n         )\n \n     @classmethod\n-    def from_settings(cls, settings: Settings) -> Self:\n-        s3store: type[S3FilesStore] = cast(type[S3FilesStore], cls.STORE_SCHEMES[\"s3\"])\n-        s3store.AWS_ACCESS_KEY_ID = settings[\"AWS_ACCESS_KEY_ID\"]\n-        s3store.AWS_SECRET_ACCESS_KEY = settings[\"AWS_SECRET_ACCESS_KEY\"]\n-        s3store.AWS_SESSION_TOKEN = settings[\"AWS_SESSION_TOKEN\"]\n-        s3store.AWS_ENDPOINT_URL = settings[\"AWS_ENDPOINT_URL\"]\n-        s3store.AWS_REGION_NAME = settings[\"AWS_REGION_NAME\"]\n-        s3store.AWS_USE_SSL = settings[\"AWS_USE_SSL\"]\n-        s3store.AWS_VERIFY = settings[\"AWS_VERIFY\"]\n-        s3store.POLICY = settings[\"IMAGES_STORE_S3_ACL\"]\n-\n-        gcs_store: type[GCSFilesStore] = cast(\n-            type[GCSFilesStore], cls.STORE_SCHEMES[\"gs\"]\n-        )\n-        gcs_store.GCS_PROJECT_ID = settings[\"GCS_PROJECT_ID\"]\n-        gcs_store.POLICY = settings[\"IMAGES_STORE_GCS_ACL\"] or None\n-\n-        ftp_store: type[FTPFilesStore] = cast(\n-            type[FTPFilesStore], cls.STORE_SCHEMES[\"ftp\"]\n-        )\n-        ftp_store.FTP_USERNAME = settings[\"FTP_USER\"]\n-        ftp_store.FTP_PASSWORD = settings[\"FTP_PASSWORD\"]\n-        ftp_store.USE_ACTIVE_MODE = settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\")\n-\n+    def _from_settings(cls, settings: Settings, crawler: Crawler | None) -> Self:\n+        cls._update_stores(settings)\n         store_uri = settings[\"IMAGES_STORE\"]\n-        return cls(store_uri, settings=settings)\n+        if \"crawler\" in get_func_args(cls.__init__):\n+            o = cls(store_uri, crawler=crawler)\n+        else:\n+            o = cls(store_uri, settings=settings)\n+            if crawler:\n+                o._finish_init(crawler)\n+            warnings.warn(\n+                f\"{global_object_name(cls)}.__init__() doesn't take a crawler argument.\"\n+                \" This is deprecated and the argument will be required in future Scrapy versions.\",\n+                category=ScrapyDeprecationWarning,\n+            )\n+        return o\n \n     def file_downloaded(\n         self,\ndiff --git a/scrapy/pipelines/media.py b/scrapy/pipelines/media.py\nindex b10ec147b34..6d7808c31b4 100644\n--- a/scrapy/pipelines/media.py\n+++ b/scrapy/pipelines/media.py\n@@ -2,6 +2,7 @@\n \n import functools\n import logging\n+import warnings\n from abc import ABC, abstractmethod\n from collections import defaultdict\n from typing import (\n@@ -20,12 +21,14 @@\n from twisted.python.failure import Failure\n from twisted.python.versions import Version\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http.request import NO_CALLBACK, Request\n from scrapy.settings import Settings\n from scrapy.utils.datatypes import SequenceExclude\n from scrapy.utils.defer import defer_result, mustbe_deferred\n from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.misc import arg_to_iter\n+from scrapy.utils.python import get_func_args, global_object_name\n \n if TYPE_CHECKING:\n     from collections.abc import Callable\n@@ -38,7 +41,6 @@\n     from scrapy.http import Response\n     from scrapy.utils.request import RequestFingerprinter\n \n-\n _T = TypeVar(\"_T\")\n \n \n@@ -51,13 +53,13 @@ class FileInfo(TypedDict):\n \n FileInfoOrError = Union[tuple[Literal[True], FileInfo], tuple[Literal[False], Failure]]\n \n-\n logger = logging.getLogger(__name__)\n \n \n class MediaPipeline(ABC):\n     crawler: Crawler\n     _fingerprinter: RequestFingerprinter\n+    _modern_init = False\n \n     LOG_FAILED_RESULTS: bool = True\n \n@@ -74,10 +76,22 @@ def __init__(\n         self,\n         download_func: Callable[[Request, Spider], Response] | None = None,\n         settings: Settings | dict[str, Any] | None = None,\n+        *,\n+        crawler: Crawler | None = None,\n     ):\n         self.download_func = download_func\n \n-        if isinstance(settings, dict) or settings is None:\n+        if crawler is not None:\n+            if settings is not None:\n+                warnings.warn(\n+                    f\"MediaPipeline.__init__() was called with a crawler instance and a settings instance\"\n+                    f\" when creating {global_object_name(self.__class__)}. The settings instance will be ignored\"\n+                    f\" and crawler.settings will be used. The settings argument will be removed in a future Scrapy version.\",\n+                    category=ScrapyDeprecationWarning,\n+                    stacklevel=2,\n+                )\n+            settings = crawler.settings\n+        elif isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n         resolve = functools.partial(\n             self._key_for_pipe, base_class_name=\"MediaPipeline\", settings=settings\n@@ -87,6 +101,27 @@ def __init__(\n         )\n         self._handle_statuses(self.allow_redirects)\n \n+        if crawler:\n+            self._finish_init(crawler)\n+            self._modern_init = True\n+        else:\n+            warnings.warn(\n+                f\"MediaPipeline.__init__() was called without the crawler argument\"\n+                f\" when creating {global_object_name(self.__class__)}.\"\n+                f\" This is deprecated and the argument will be required in future Scrapy versions.\",\n+                category=ScrapyDeprecationWarning,\n+                stacklevel=2,\n+            )\n+\n+    def _finish_init(self, crawler: Crawler) -> None:\n+        # This was done in from_crawler() before 2.12, now it's done in __init__()\n+        # if the crawler was passed to it and may be needed to be called in other\n+        # deprecated code paths explicitly too. After the crawler argument of __init__()\n+        # becomes mandatory this should be inlined there.\n+        self.crawler = crawler\n+        assert crawler.request_fingerprinter\n+        self._fingerprinter = crawler.request_fingerprinter\n+\n     def _handle_statuses(self, allow_redirects: bool) -> None:\n         self.handle_httpstatus_list = None\n         if allow_redirects:\n@@ -112,13 +147,19 @@ def _key_for_pipe(\n     @classmethod\n     def from_crawler(cls, crawler: Crawler) -> Self:\n         pipe: Self\n-        try:\n+        if hasattr(cls, \"from_settings\"):\n             pipe = cls.from_settings(crawler.settings)  # type: ignore[attr-defined]\n-        except AttributeError:\n+        elif \"crawler\" in get_func_args(cls.__init__):\n+            pipe = cls(crawler=crawler)\n+        else:\n             pipe = cls()\n-        pipe.crawler = crawler\n-        assert crawler.request_fingerprinter\n-        pipe._fingerprinter = crawler.request_fingerprinter\n+            warnings.warn(\n+                f\"{global_object_name(cls)}.__init__() doesn't take a crawler argument.\"\n+                \" This is deprecated and the argument will be required in future Scrapy versions.\",\n+                category=ScrapyDeprecationWarning,\n+            )\n+        if not pipe._modern_init:\n+            pipe._finish_init(crawler)\n         return pipe\n \n     def open_spider(self, spider: Spider) -> None:\ndiff --git a/scrapy/spidermiddlewares/urllength.py b/scrapy/spidermiddlewares/urllength.py\nindex 191adb6cd32..a1cd1bb7cfa 100644\n--- a/scrapy/spidermiddlewares/urllength.py\n+++ b/scrapy/spidermiddlewares/urllength.py\n@@ -7,9 +7,10 @@\n from __future__ import annotations\n \n import logging\n+import warnings\n from typing import TYPE_CHECKING, Any\n \n-from scrapy.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.http import Request, Response\n \n if TYPE_CHECKING:\n@@ -19,6 +20,7 @@\n     from typing_extensions import Self\n \n     from scrapy import Spider\n+    from scrapy.crawler import Crawler\n     from scrapy.settings import BaseSettings\n \n \n@@ -31,6 +33,19 @@ def __init__(self, maxlength: int):\n \n     @classmethod\n     def from_settings(cls, settings: BaseSettings) -> Self:\n+        warnings.warn(\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return cls._from_settings(settings)\n+\n+    @classmethod\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n+        return cls._from_settings(crawler.settings)\n+\n+    @classmethod\n+    def _from_settings(cls, settings: BaseSettings) -> Self:\n         maxlength = settings.getint(\"URLLENGTH_LIMIT\")\n         if not maxlength:\n             raise NotConfigured\ndiff --git a/scrapy/utils/misc.py b/scrapy/utils/misc.py\nindex 12c09839f0f..a408a205dda 100644\n--- a/scrapy/utils/misc.py\n+++ b/scrapy/utils/misc.py\n@@ -26,7 +26,6 @@\n \n     from scrapy import Spider\n     from scrapy.crawler import Crawler\n-    from scrapy.settings import BaseSettings\n \n \n _ITERABLE_SINGLE_VALUES = dict, Item, str, bytes\n@@ -150,7 +149,7 @@ def create_instance(objcls, settings, crawler, *args, **kwargs):\n     \"\"\"\n     warnings.warn(\n         \"The create_instance() function is deprecated. \"\n-        \"Please use build_from_crawler() or build_from_settings() instead.\",\n+        \"Please use build_from_crawler() instead.\",\n         category=ScrapyDeprecationWarning,\n         stacklevel=2,\n     )\n@@ -176,7 +175,7 @@ def create_instance(objcls, settings, crawler, *args, **kwargs):\n def build_from_crawler(\n     objcls: type[T], crawler: Crawler, /, *args: Any, **kwargs: Any\n ) -> T:\n-    \"\"\"Construct a class instance using its ``from_crawler`` constructor.\n+    \"\"\"Construct a class instance using its ``from_crawler`` or ``from_settings`` constructor.\n \n     ``*args`` and ``**kwargs`` are forwarded to the constructor.\n \n@@ -186,6 +185,14 @@ def build_from_crawler(\n         instance = objcls.from_crawler(crawler, *args, **kwargs)  # type: ignore[attr-defined]\n         method_name = \"from_crawler\"\n     elif hasattr(objcls, \"from_settings\"):\n+        warnings.warn(\n+            f\"{objcls.__qualname__} has from_settings() but not from_crawler().\"\n+            \" This is deprecated and calling from_settings() will be removed in a future\"\n+            \" Scrapy version. You can implement a simple from_crawler() that calls\"\n+            \" from_settings() with crawler.settings.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n         instance = objcls.from_settings(crawler.settings, *args, **kwargs)  # type: ignore[attr-defined]\n         method_name = \"from_settings\"\n     else:\n@@ -196,26 +203,6 @@ def build_from_crawler(\n     return cast(T, instance)\n \n \n-def build_from_settings(\n-    objcls: type[T], settings: BaseSettings, /, *args: Any, **kwargs: Any\n-) -> T:\n-    \"\"\"Construct a class instance using its ``from_settings`` constructor.\n-\n-    ``*args`` and ``**kwargs`` are forwarded to the constructor.\n-\n-    Raises ``TypeError`` if the resulting instance is ``None``.\n-    \"\"\"\n-    if hasattr(objcls, \"from_settings\"):\n-        instance = objcls.from_settings(settings, *args, **kwargs)  # type: ignore[attr-defined]\n-        method_name = \"from_settings\"\n-    else:\n-        instance = objcls(*args, **kwargs)\n-        method_name = \"__new__\"\n-    if instance is None:\n-        raise TypeError(f\"{objcls.__qualname__}.{method_name} returned None\")\n-    return cast(T, instance)\n-\n-\n @contextmanager\n def set_environ(**kwargs: str) -> Iterator[None]:\n     \"\"\"Temporarily set environment variables inside the context manager and\n",
        "test_patch": "diff --git a/tests/test_dupefilters.py b/tests/test_dupefilters.py\nindex 9ba8bd64f40..4fd648f4834 100644\n--- a/tests/test_dupefilters.py\n+++ b/tests/test_dupefilters.py\n@@ -33,14 +33,6 @@ def from_crawler(cls, crawler):\n         return df\n \n \n-class FromSettingsRFPDupeFilter(RFPDupeFilter):\n-    @classmethod\n-    def from_settings(cls, settings, *, fingerprinter=None):\n-        df = super().from_settings(settings, fingerprinter=fingerprinter)\n-        df.method = \"from_settings\"\n-        return df\n-\n-\n class DirectDupeFilter:\n     method = \"n/a\"\n \n@@ -56,16 +48,6 @@ def test_df_from_crawler_scheduler(self):\n         self.assertTrue(scheduler.df.debug)\n         self.assertEqual(scheduler.df.method, \"from_crawler\")\n \n-    def test_df_from_settings_scheduler(self):\n-        settings = {\n-            \"DUPEFILTER_DEBUG\": True,\n-            \"DUPEFILTER_CLASS\": FromSettingsRFPDupeFilter,\n-        }\n-        crawler = get_crawler(settings_dict=settings)\n-        scheduler = Scheduler.from_crawler(crawler)\n-        self.assertTrue(scheduler.df.debug)\n-        self.assertEqual(scheduler.df.method, \"from_settings\")\n-\n     def test_df_direct_scheduler(self):\n         settings = {\n             \"DUPEFILTER_CLASS\": DirectDupeFilter,\ndiff --git a/tests/test_middleware.py b/tests/test_middleware.py\nindex a42c7b3d1e2..3a1cf19ad30 100644\n--- a/tests/test_middleware.py\n+++ b/tests/test_middleware.py\n@@ -2,7 +2,7 @@\n \n from scrapy.exceptions import NotConfigured\n from scrapy.middleware import MiddlewareManager\n-from scrapy.settings import Settings\n+from scrapy.utils.test import get_crawler\n \n \n class M1:\n@@ -23,8 +23,6 @@ def open_spider(self, spider):\n     def close_spider(self, spider):\n         pass\n \n-    pass\n-\n \n class M3:\n     def process(self, response, request, spider):\n@@ -83,7 +81,7 @@ def test_enabled(self):\n         self.assertEqual(mwman.middlewares, (m1, m2, m3))\n \n     def test_enabled_from_settings(self):\n-        settings = Settings()\n-        mwman = TestMiddlewareManager.from_settings(settings)\n+        crawler = get_crawler()\n+        mwman = TestMiddlewareManager.from_crawler(crawler)\n         classes = [x.__class__ for x in mwman.middlewares]\n         self.assertEqual(classes, [M1, M3])\ndiff --git a/tests/test_pipeline_files.py b/tests/test_pipeline_files.py\nindex 47840caaa16..9dcb3e4d18d 100644\n--- a/tests/test_pipeline_files.py\n+++ b/tests/test_pipeline_files.py\n@@ -2,6 +2,7 @@\n import os\n import random\n import time\n+import warnings\n from datetime import datetime\n from io import BytesIO\n from pathlib import Path\n@@ -25,7 +26,6 @@\n     GCSFilesStore,\n     S3FilesStore,\n )\n-from scrapy.settings import Settings\n from scrapy.utils.test import (\n     assert_gcs_environ,\n     get_crawler,\n@@ -217,8 +217,8 @@ class CustomFilesPipeline(FilesPipeline):\n             def file_path(self, request, response=None, info=None, item=None):\n                 return f'full/{item.get(\"path\")}'\n \n-        file_path = CustomFilesPipeline.from_settings(\n-            Settings({\"FILES_STORE\": self.tempdir})\n+        file_path = CustomFilesPipeline.from_crawler(\n+            get_crawler(None, {\"FILES_STORE\": self.tempdir})\n         ).file_path\n         item = {\"path\": \"path-to-store-file\"}\n         request = Request(\"http://example.com\")\n@@ -235,7 +235,9 @@ def tearDown(self):\n     def test_item_fields_default(self):\n         url = \"http://www.example.com/files/1.txt\"\n         item = self.item_class(name=\"item1\", file_urls=[url])\n-        pipeline = FilesPipeline.from_settings(Settings({\"FILES_STORE\": self.tempdir}))\n+        pipeline = FilesPipeline.from_crawler(\n+            get_crawler(None, {\"FILES_STORE\": self.tempdir})\n+        )\n         requests = list(pipeline.get_media_requests(item, None))\n         self.assertEqual(requests[0].url, url)\n         results = [(True, {\"url\": url})]\n@@ -247,13 +249,14 @@ def test_item_fields_default(self):\n     def test_item_fields_override_settings(self):\n         url = \"http://www.example.com/files/1.txt\"\n         item = self.item_class(name=\"item1\", custom_file_urls=[url])\n-        pipeline = FilesPipeline.from_settings(\n-            Settings(\n+        pipeline = FilesPipeline.from_crawler(\n+            get_crawler(\n+                None,\n                 {\n                     \"FILES_STORE\": self.tempdir,\n                     \"FILES_URLS_FIELD\": \"custom_file_urls\",\n                     \"FILES_RESULT_FIELD\": \"custom_files\",\n-                }\n+                },\n             )\n         )\n         requests = list(pipeline.get_media_requests(item, None))\n@@ -371,8 +374,10 @@ def test_different_settings_for_different_instances(self):\n         different settings.\n         \"\"\"\n         custom_settings = self._generate_fake_settings()\n-        another_pipeline = FilesPipeline.from_settings(Settings(custom_settings))\n-        one_pipeline = FilesPipeline(self.tempdir)\n+        another_pipeline = FilesPipeline.from_crawler(\n+            get_crawler(None, custom_settings)\n+        )\n+        one_pipeline = FilesPipeline(self.tempdir, crawler=get_crawler(None))\n         for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n             default_value = self.default_cls_settings[pipe_attr]\n             self.assertEqual(getattr(one_pipeline, pipe_attr), default_value)\n@@ -385,7 +390,7 @@ def test_subclass_attributes_preserved_if_no_settings(self):\n         If subclasses override class attributes and there are no special settings those values should be kept.\n         \"\"\"\n         pipe_cls = self._generate_fake_pipeline()\n-        pipe = pipe_cls.from_settings(Settings({\"FILES_STORE\": self.tempdir}))\n+        pipe = pipe_cls.from_crawler(get_crawler(None, {\"FILES_STORE\": self.tempdir}))\n         for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n             custom_value = getattr(pipe, pipe_ins_attr)\n             self.assertNotEqual(custom_value, self.default_cls_settings[pipe_attr])\n@@ -398,7 +403,7 @@ def test_subclass_attrs_preserved_custom_settings(self):\n         \"\"\"\n         pipeline_cls = self._generate_fake_pipeline()\n         settings = self._generate_fake_settings()\n-        pipeline = pipeline_cls.from_settings(Settings(settings))\n+        pipeline = pipeline_cls.from_crawler(get_crawler(None, settings))\n         for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n             value = getattr(pipeline, pipe_ins_attr)\n             setting_value = settings.get(settings_attr)\n@@ -414,8 +419,8 @@ def test_no_custom_settings_for_subclasses(self):\n         class UserDefinedFilesPipeline(FilesPipeline):\n             pass\n \n-        user_pipeline = UserDefinedFilesPipeline.from_settings(\n-            Settings({\"FILES_STORE\": self.tempdir})\n+        user_pipeline = UserDefinedFilesPipeline.from_crawler(\n+            get_crawler(None, {\"FILES_STORE\": self.tempdir})\n         )\n         for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n             # Values from settings for custom pipeline should be set on pipeline instance.\n@@ -433,7 +438,9 @@ class UserDefinedFilesPipeline(FilesPipeline):\n \n         prefix = UserDefinedFilesPipeline.__name__.upper()\n         settings = self._generate_fake_settings(prefix=prefix)\n-        user_pipeline = UserDefinedFilesPipeline.from_settings(Settings(settings))\n+        user_pipeline = UserDefinedFilesPipeline.from_crawler(\n+            get_crawler(None, settings)\n+        )\n         for pipe_attr, settings_attr, pipe_inst_attr in self.file_cls_attr_settings_map:\n             # Values from settings for custom pipeline should be set on pipeline instance.\n             custom_value = settings.get(prefix + \"_\" + settings_attr)\n@@ -448,7 +455,7 @@ def test_custom_settings_and_class_attrs_for_subclasses(self):\n         pipeline_cls = self._generate_fake_pipeline()\n         prefix = pipeline_cls.__name__.upper()\n         settings = self._generate_fake_settings(prefix=prefix)\n-        user_pipeline = pipeline_cls.from_settings(Settings(settings))\n+        user_pipeline = pipeline_cls.from_crawler(get_crawler(None, settings))\n         for (\n             pipe_cls_attr,\n             settings_attr,\n@@ -463,8 +470,8 @@ class UserDefinedFilesPipeline(FilesPipeline):\n             DEFAULT_FILES_RESULT_FIELD = \"this\"\n             DEFAULT_FILES_URLS_FIELD = \"that\"\n \n-        pipeline = UserDefinedFilesPipeline.from_settings(\n-            Settings({\"FILES_STORE\": self.tempdir})\n+        pipeline = UserDefinedFilesPipeline.from_crawler(\n+            get_crawler(None, {\"FILES_STORE\": self.tempdir})\n         )\n         self.assertEqual(\n             pipeline.files_result_field,\n@@ -484,7 +491,7 @@ def test_user_defined_subclass_default_key_names(self):\n         class UserPipe(FilesPipeline):\n             pass\n \n-        pipeline_cls = UserPipe.from_settings(Settings(settings))\n+        pipeline_cls = UserPipe.from_crawler(get_crawler(None, settings))\n \n         for pipe_attr, settings_attr, pipe_inst_attr in self.file_cls_attr_settings_map:\n             expected_value = settings.get(settings_attr)\n@@ -495,8 +502,8 @@ class CustomFilesPipelineWithPathLikeDir(FilesPipeline):\n             def file_path(self, request, response=None, info=None, *, item=None):\n                 return Path(\"subdir\") / Path(request.url).name\n \n-        pipeline = CustomFilesPipelineWithPathLikeDir.from_settings(\n-            Settings({\"FILES_STORE\": Path(\"./Temp\")})\n+        pipeline = CustomFilesPipelineWithPathLikeDir.from_crawler(\n+            get_crawler(None, {\"FILES_STORE\": Path(\"./Temp\")})\n         )\n         request = Request(\"http://example.com/image01.jpg\")\n         self.assertEqual(pipeline.file_path(request), Path(\"subdir/image01.jpg\"))\n@@ -687,3 +694,75 @@ def _prepare_request_object(item_url, flags=None):\n         item_url,\n         meta={\"response\": Response(item_url, status=200, body=b\"data\", flags=flags)},\n     )\n+\n+\n+# this is separate from the one in test_pipeline_media.py to specifically test FilesPipeline subclasses\n+class BuildFromCrawlerTestCase(unittest.TestCase):\n+    def setUp(self):\n+        self.tempdir = mkdtemp()\n+        self.crawler = get_crawler(None, {\"FILES_STORE\": self.tempdir})\n+\n+    def tearDown(self):\n+        rmtree(self.tempdir)\n+\n+    def test_simple(self):\n+        class Pipeline(FilesPipeline):\n+            pass\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 0)\n+            assert pipe.store\n+\n+    def test_has_old_init(self):\n+        class Pipeline(FilesPipeline):\n+            def __init__(self, store_uri, download_func=None, settings=None):\n+                super().__init__(store_uri, download_func, settings)\n+                self._init_called = True\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 2)\n+            assert pipe._init_called\n+\n+    def test_has_from_settings(self):\n+        class Pipeline(FilesPipeline):\n+            _from_settings_called = False\n+\n+            @classmethod\n+            def from_settings(cls, settings):\n+                o = super().from_settings(settings)\n+                o._from_settings_called = True\n+                return o\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 3)\n+            assert pipe.store\n+            assert pipe._from_settings_called\n+\n+    def test_has_from_crawler_and_init(self):\n+        class Pipeline(FilesPipeline):\n+            _from_crawler_called = False\n+\n+            @classmethod\n+            def from_crawler(cls, crawler):\n+                settings = crawler.settings\n+                store_uri = settings[\"FILES_STORE\"]\n+                o = cls(store_uri, crawler=crawler)\n+                o._from_crawler_called = True\n+                return o\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 0)\n+            assert pipe.store\n+            assert pipe._from_crawler_called\ndiff --git a/tests/test_pipeline_images.py b/tests/test_pipeline_images.py\nindex dfeead999d5..3ffef410249 100644\n--- a/tests/test_pipeline_images.py\n+++ b/tests/test_pipeline_images.py\n@@ -13,7 +13,7 @@\n from scrapy.http import Request, Response\n from scrapy.item import Field, Item\n from scrapy.pipelines.images import ImageException, ImagesPipeline\n-from scrapy.settings import Settings\n+from scrapy.utils.test import get_crawler\n \n skip_pillow: str | None\n try:\n@@ -33,7 +33,8 @@ class ImagesPipelineTestCase(unittest.TestCase):\n \n     def setUp(self):\n         self.tempdir = mkdtemp()\n-        self.pipeline = ImagesPipeline(self.tempdir)\n+        crawler = get_crawler()\n+        self.pipeline = ImagesPipeline(self.tempdir, crawler=crawler)\n \n     def tearDown(self):\n         rmtree(self.tempdir)\n@@ -123,8 +124,8 @@ def thumb_path(\n             ):\n                 return f\"thumb/{thumb_id}/{item.get('path')}\"\n \n-        thumb_path = CustomImagesPipeline.from_settings(\n-            Settings({\"IMAGES_STORE\": self.tempdir})\n+        thumb_path = CustomImagesPipeline.from_crawler(\n+            get_crawler(None, {\"IMAGES_STORE\": self.tempdir})\n         ).thumb_path\n         item = {\"path\": \"path-to-store-file\"}\n         request = Request(\"http://example.com\")\n@@ -218,8 +219,8 @@ class ImagesPipelineTestCaseFieldsMixin:\n     def test_item_fields_default(self):\n         url = \"http://www.example.com/images/1.jpg\"\n         item = self.item_class(name=\"item1\", image_urls=[url])\n-        pipeline = ImagesPipeline.from_settings(\n-            Settings({\"IMAGES_STORE\": \"s3://example/images/\"})\n+        pipeline = ImagesPipeline.from_crawler(\n+            get_crawler(None, {\"IMAGES_STORE\": \"s3://example/images/\"})\n         )\n         requests = list(pipeline.get_media_requests(item, None))\n         self.assertEqual(requests[0].url, url)\n@@ -232,13 +233,14 @@ def test_item_fields_default(self):\n     def test_item_fields_override_settings(self):\n         url = \"http://www.example.com/images/1.jpg\"\n         item = self.item_class(name=\"item1\", custom_image_urls=[url])\n-        pipeline = ImagesPipeline.from_settings(\n-            Settings(\n+        pipeline = ImagesPipeline.from_crawler(\n+            get_crawler(\n+                None,\n                 {\n                     \"IMAGES_STORE\": \"s3://example/images/\",\n                     \"IMAGES_URLS_FIELD\": \"custom_image_urls\",\n                     \"IMAGES_RESULT_FIELD\": \"custom_images\",\n-                }\n+                },\n             )\n         )\n         requests = list(pipeline.get_media_requests(item, None))\n@@ -389,9 +391,8 @@ def test_different_settings_for_different_instances(self):\n         have different settings.\n         \"\"\"\n         custom_settings = self._generate_fake_settings()\n-        default_settings = Settings()\n-        default_sts_pipe = ImagesPipeline(self.tempdir, settings=default_settings)\n-        user_sts_pipe = ImagesPipeline.from_settings(Settings(custom_settings))\n+        default_sts_pipe = ImagesPipeline(self.tempdir, crawler=get_crawler(None))\n+        user_sts_pipe = ImagesPipeline.from_crawler(get_crawler(None, custom_settings))\n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             expected_default_value = self.default_pipeline_settings.get(pipe_attr)\n             custom_value = custom_settings.get(settings_attr)\n@@ -407,7 +408,9 @@ def test_subclass_attrs_preserved_default_settings(self):\n         from class attributes.\n         \"\"\"\n         pipeline_cls = self._generate_fake_pipeline_subclass()\n-        pipeline = pipeline_cls.from_settings(Settings({\"IMAGES_STORE\": self.tempdir}))\n+        pipeline = pipeline_cls.from_crawler(\n+            get_crawler(None, {\"IMAGES_STORE\": self.tempdir})\n+        )\n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             # Instance attribute (lowercase) must be equal to class attribute (uppercase).\n             attr_value = getattr(pipeline, pipe_attr.lower())\n@@ -421,7 +424,7 @@ def test_subclass_attrs_preserved_custom_settings(self):\n         \"\"\"\n         pipeline_cls = self._generate_fake_pipeline_subclass()\n         settings = self._generate_fake_settings()\n-        pipeline = pipeline_cls.from_settings(Settings(settings))\n+        pipeline = pipeline_cls.from_crawler(get_crawler(None, settings))\n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             # Instance attribute (lowercase) must be equal to\n             # value defined in settings.\n@@ -439,8 +442,8 @@ def test_no_custom_settings_for_subclasses(self):\n         class UserDefinedImagePipeline(ImagesPipeline):\n             pass\n \n-        user_pipeline = UserDefinedImagePipeline.from_settings(\n-            Settings({\"IMAGES_STORE\": self.tempdir})\n+        user_pipeline = UserDefinedImagePipeline.from_crawler(\n+            get_crawler(None, {\"IMAGES_STORE\": self.tempdir})\n         )\n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             # Values from settings for custom pipeline should be set on pipeline instance.\n@@ -458,7 +461,9 @@ class UserDefinedImagePipeline(ImagesPipeline):\n \n         prefix = UserDefinedImagePipeline.__name__.upper()\n         settings = self._generate_fake_settings(prefix=prefix)\n-        user_pipeline = UserDefinedImagePipeline.from_settings(Settings(settings))\n+        user_pipeline = UserDefinedImagePipeline.from_crawler(\n+            get_crawler(None, settings)\n+        )\n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             # Values from settings for custom pipeline should be set on pipeline instance.\n             custom_value = settings.get(prefix + \"_\" + settings_attr)\n@@ -473,7 +478,7 @@ def test_custom_settings_and_class_attrs_for_subclasses(self):\n         pipeline_cls = self._generate_fake_pipeline_subclass()\n         prefix = pipeline_cls.__name__.upper()\n         settings = self._generate_fake_settings(prefix=prefix)\n-        user_pipeline = pipeline_cls.from_settings(Settings(settings))\n+        user_pipeline = pipeline_cls.from_crawler(get_crawler(None, settings))\n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             custom_value = settings.get(prefix + \"_\" + settings_attr)\n             self.assertNotEqual(custom_value, self.default_pipeline_settings[pipe_attr])\n@@ -484,8 +489,8 @@ class UserDefinedImagePipeline(ImagesPipeline):\n             DEFAULT_IMAGES_URLS_FIELD = \"something\"\n             DEFAULT_IMAGES_RESULT_FIELD = \"something_else\"\n \n-        pipeline = UserDefinedImagePipeline.from_settings(\n-            Settings({\"IMAGES_STORE\": self.tempdir})\n+        pipeline = UserDefinedImagePipeline.from_crawler(\n+            get_crawler(None, {\"IMAGES_STORE\": self.tempdir})\n         )\n         self.assertEqual(\n             pipeline.images_result_field,\n@@ -506,7 +511,7 @@ def test_user_defined_subclass_default_key_names(self):\n         class UserPipe(ImagesPipeline):\n             pass\n \n-        pipeline_cls = UserPipe.from_settings(Settings(settings))\n+        pipeline_cls = UserPipe.from_crawler(get_crawler(None, settings))\n \n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             expected_value = settings.get(settings_attr)\ndiff --git a/tests/test_pipeline_media.py b/tests/test_pipeline_media.py\nindex c979e45d70a..58a2d367825 100644\n--- a/tests/test_pipeline_media.py\n+++ b/tests/test_pipeline_media.py\n@@ -1,5 +1,7 @@\n from __future__ import annotations\n \n+import warnings\n+\n from testfixtures import LogCapture\n from twisted.internet import reactor\n from twisted.internet.defer import Deferred, inlineCallbacks\n@@ -11,7 +13,6 @@\n from scrapy.http.request import NO_CALLBACK\n from scrapy.pipelines.files import FileException\n from scrapy.pipelines.media import MediaPipeline\n-from scrapy.settings import Settings\n from scrapy.spiders import Spider\n from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.signal import disconnect_all\n@@ -175,8 +176,8 @@ def test_default_process_item(self):\n \n \n class MockedMediaPipeline(UserDefinedPipeline):\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n+    def __init__(self, *args, crawler=None, **kwargs):\n+        super().__init__(*args, crawler=crawler, **kwargs)\n         self._mockcalled = []\n \n     def download(self, request, info):\n@@ -377,7 +378,7 @@ def test_key_for_pipe(self):\n class MediaPipelineAllowRedirectSettingsTestCase(unittest.TestCase):\n \n     def _assert_request_no3xx(self, pipeline_class, settings):\n-        pipe = pipeline_class(settings=Settings(settings))\n+        pipe = pipeline_class(crawler=get_crawler(None, settings))\n         request = Request(\"http://url\")\n         pipe._modify_media_request(request)\n \n@@ -410,3 +411,115 @@ def test_subclass_specific_setting(self):\n         self._assert_request_no3xx(\n             UserDefinedPipeline, {\"USERDEFINEDPIPELINE_MEDIA_ALLOW_REDIRECTS\": True}\n         )\n+\n+\n+class BuildFromCrawlerTestCase(unittest.TestCase):\n+    def setUp(self):\n+        self.crawler = get_crawler(None, {\"FILES_STORE\": \"/foo\"})\n+\n+    def test_simple(self):\n+        class Pipeline(UserDefinedPipeline):\n+            pass\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 0)\n+\n+    def test_has_old_init(self):\n+        class Pipeline(UserDefinedPipeline):\n+            def __init__(self):\n+                super().__init__()\n+                self._init_called = True\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 2)\n+            assert pipe._init_called\n+\n+    def test_has_from_settings(self):\n+        class Pipeline(UserDefinedPipeline):\n+            _from_settings_called = False\n+\n+            @classmethod\n+            def from_settings(cls, settings):\n+                o = cls()\n+                o._from_settings_called = True\n+                return o\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 1)\n+            assert pipe._from_settings_called\n+\n+    def test_has_from_settings_and_init(self):\n+        class Pipeline(UserDefinedPipeline):\n+            _from_settings_called = False\n+\n+            def __init__(self, store_uri, settings):\n+                super().__init__()\n+                self._init_called = True\n+\n+            @classmethod\n+            def from_settings(cls, settings):\n+                store_uri = settings[\"FILES_STORE\"]\n+                o = cls(store_uri, settings=settings)\n+                o._from_settings_called = True\n+                return o\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 1)\n+            assert pipe._from_settings_called\n+            assert pipe._init_called\n+\n+    def test_has_from_crawler_and_init(self):\n+        class Pipeline(UserDefinedPipeline):\n+            _from_crawler_called = False\n+\n+            def __init__(self, store_uri, settings, *, crawler):\n+                super().__init__(crawler=crawler)\n+                self._init_called = True\n+\n+            @classmethod\n+            def from_crawler(cls, crawler):\n+                settings = crawler.settings\n+                store_uri = settings[\"FILES_STORE\"]\n+                o = cls(store_uri, settings=settings, crawler=crawler)\n+                o._from_crawler_called = True\n+                return o\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 0)\n+            assert pipe._from_crawler_called\n+            assert pipe._init_called\n+\n+    def test_has_from_crawler(self):\n+        class Pipeline(UserDefinedPipeline):\n+            _from_crawler_called = False\n+\n+            @classmethod\n+            def from_crawler(cls, crawler):\n+                settings = crawler.settings\n+                o = super().from_crawler(crawler)\n+                o._from_crawler_called = True\n+                o.store_uri = settings[\"FILES_STORE\"]\n+                return o\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            # this and the next assert will fail as MediaPipeline.from_crawler() wasn't called\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 0)\n+            assert pipe._from_crawler_called\ndiff --git a/tests/test_spidermiddleware_urllength.py b/tests/test_spidermiddleware_urllength.py\nindex 9111e4c82ab..1a0f2e223c4 100644\n--- a/tests/test_spidermiddleware_urllength.py\n+++ b/tests/test_spidermiddleware_urllength.py\n@@ -3,7 +3,6 @@\n from testfixtures import LogCapture\n \n from scrapy.http import Request, Response\n-from scrapy.settings import Settings\n from scrapy.spidermiddlewares.urllength import UrlLengthMiddleware\n from scrapy.spiders import Spider\n from scrapy.utils.test import get_crawler\n@@ -12,12 +11,10 @@\n class TestUrlLengthMiddleware(TestCase):\n     def setUp(self):\n         self.maxlength = 25\n-        settings = Settings({\"URLLENGTH_LIMIT\": self.maxlength})\n-\n-        crawler = get_crawler(Spider)\n+        crawler = get_crawler(Spider, {\"URLLENGTH_LIMIT\": self.maxlength})\n         self.spider = crawler._create_spider(\"foo\")\n         self.stats = crawler.stats\n-        self.mw = UrlLengthMiddleware.from_settings(settings)\n+        self.mw = UrlLengthMiddleware.from_crawler(crawler)\n \n         self.response = Response(\"http://scrapytest.org\")\n         self.short_url_req = Request(\"http://scrapytest.org/\")\ndiff --git a/tests/test_utils_misc/__init__.py b/tests/test_utils_misc/__init__.py\nindex 4d8e715210d..f71b2b034a9 100644\n--- a/tests/test_utils_misc/__init__.py\n+++ b/tests/test_utils_misc/__init__.py\n@@ -10,7 +10,6 @@\n from scrapy.utils.misc import (\n     arg_to_iter,\n     build_from_crawler,\n-    build_from_settings,\n     create_instance,\n     load_object,\n     rel_has_nofollow,\n@@ -197,39 +196,6 @@ def _test_with_crawler(mock, settings, crawler):\n         with self.assertRaises(TypeError):\n             build_from_crawler(m, crawler, *args, **kwargs)\n \n-    def test_build_from_settings(self):\n-        settings = mock.MagicMock()\n-        args = (True, 100.0)\n-        kwargs = {\"key\": \"val\"}\n-\n-        def _test_with_settings(mock, settings):\n-            build_from_settings(mock, settings, *args, **kwargs)\n-            if hasattr(mock, \"from_settings\"):\n-                mock.from_settings.assert_called_once_with(settings, *args, **kwargs)\n-                self.assertEqual(mock.call_count, 0)\n-            else:\n-                mock.assert_called_once_with(*args, **kwargs)\n-\n-        # Check usage of correct constructor using three mocks:\n-        #   1. with no alternative constructors\n-        #   2. with from_settings() constructor\n-        #   3. with from_settings() and from_crawler() constructor\n-        spec_sets = (\n-            [\"__qualname__\"],\n-            [\"__qualname__\", \"from_settings\"],\n-            [\"__qualname__\", \"from_settings\", \"from_crawler\"],\n-        )\n-        for specs in spec_sets:\n-            m = mock.MagicMock(spec_set=specs)\n-            _test_with_settings(m, settings)\n-            m.reset_mock()\n-\n-        # Check adoption of crawler settings\n-        m = mock.MagicMock(spec_set=[\"__qualname__\", \"from_settings\"])\n-        m.from_settings.return_value = None\n-        with self.assertRaises(TypeError):\n-            build_from_settings(m, settings, *args, **kwargs)\n-\n     def test_set_environ(self):\n         assert os.environ.get(\"some_test_environ\") is None\n         with set_environ(some_test_environ=\"test_value\"):\ndiff --git a/tests/test_utils_request.py b/tests/test_utils_request.py\nindex 965d050a4da..0a3e3b00be5 100644\n--- a/tests/test_utils_request.py\n+++ b/tests/test_utils_request.py\n@@ -8,6 +8,7 @@\n \n import pytest\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Request\n from scrapy.utils.python import to_bytes\n from scrapy.utils.request import (\n@@ -384,7 +385,9 @@ def fingerprint(self, request):\n             \"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter,\n             \"FINGERPRINT\": b\"fingerprint\",\n         }\n-        crawler = get_crawler(settings_dict=settings)\n+        with warnings.catch_warnings():\n+            warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n+            crawler = get_crawler(settings_dict=settings)\n \n         request = Request(\"http://www.example.com\")\n         fingerprint = crawler.request_fingerprinter.fingerprint(request)\ndiff --git a/tests/test_webclient.py b/tests/test_webclient.py\nindex cce119001ac..1797d5e1fcb 100644\n--- a/tests/test_webclient.py\n+++ b/tests/test_webclient.py\n@@ -9,25 +9,18 @@\n \n import OpenSSL.SSL\n from twisted.internet import defer, reactor\n-from twisted.trial import unittest\n-from twisted.web import resource, server, static, util\n-\n-try:\n-    from twisted.internet.testing import StringTransport\n-except ImportError:\n-    # deprecated in Twisted 19.7.0\n-    # (remove once we bump our requirement past that version)\n-    from twisted.test.proto_helpers import StringTransport\n-\n from twisted.internet.defer import inlineCallbacks\n+from twisted.internet.testing import StringTransport\n from twisted.protocols.policies import WrappingFactory\n+from twisted.trial import unittest\n+from twisted.web import resource, server, static, util\n \n from scrapy.core.downloader import webclient as client\n from scrapy.core.downloader.contextfactory import ScrapyClientContextFactory\n from scrapy.http import Headers, Request\n-from scrapy.settings import Settings\n-from scrapy.utils.misc import build_from_settings\n+from scrapy.utils.misc import build_from_crawler\n from scrapy.utils.python import to_bytes, to_unicode\n+from scrapy.utils.test import get_crawler\n from tests.mockserver import (\n     BrokenDownloadResource,\n     ErrorResource,\n@@ -469,22 +462,22 @@ class WebClientCustomCiphersSSLTestCase(WebClientSSLTestCase):\n \n     def testPayload(self):\n         s = \"0123456789\" * 10\n-        settings = Settings({\"DOWNLOADER_CLIENT_TLS_CIPHERS\": self.custom_ciphers})\n-        client_context_factory = build_from_settings(\n-            ScrapyClientContextFactory, settings\n+        crawler = get_crawler(\n+            settings_dict={\"DOWNLOADER_CLIENT_TLS_CIPHERS\": self.custom_ciphers}\n         )\n+        client_context_factory = build_from_crawler(ScrapyClientContextFactory, crawler)\n         return getPage(\n             self.getURL(\"payload\"), body=s, contextFactory=client_context_factory\n         ).addCallback(self.assertEqual, to_bytes(s))\n \n     def testPayloadDisabledCipher(self):\n         s = \"0123456789\" * 10\n-        settings = Settings(\n-            {\"DOWNLOADER_CLIENT_TLS_CIPHERS\": \"ECDHE-RSA-AES256-GCM-SHA384\"}\n-        )\n-        client_context_factory = build_from_settings(\n-            ScrapyClientContextFactory, settings\n+        crawler = get_crawler(\n+            settings_dict={\n+                \"DOWNLOADER_CLIENT_TLS_CIPHERS\": \"ECDHE-RSA-AES256-GCM-SHA384\"\n+            }\n         )\n+        client_context_factory = build_from_crawler(ScrapyClientContextFactory, crawler)\n         d = getPage(\n             self.getURL(\"payload\"), body=s, contextFactory=client_context_factory\n         )\n",
        "problem_statement": "Don't ship `build_from_settings()`\nWe discussed this with @kmike and decided that we want to ship `build_from_crawler()` but not `build_from_settings()` as a last-minute follow-up to #5523/#6169. This, to my knowledge, has two consequences:\r\n\r\n1. We need to change `scrapy.middleware.MiddlewareManager` to require a `Crawler` instance for building.\r\n2. Users that use `create_instance()` and pass `settings` but not `crawler` will need to change the logic when migrating to `build_from_crawler()`, but we think they should normally have a Crawler instance there.\r\n\r\n`build_from_crawler()` also has a wrong docstring as it doesn't mention `from_settings()`.\n",
        "hints_text": "`build_from_settings()` is also used in a test where we create a Settings instance to create a component instance with it and don't need a Crawler - this should be easy to fix.\r\n\r\nRegarding `scrapy.middleware.MiddlewareManager`: it currently has a `from_crawler()` that just calls `from_settings()` and `from_settings()` that can optionally take a Crawler instance. If the Crawler instance is passed, it is also passed to created middlewares via `build_from_crawler()` and if it isn't passed, the middlewares are created with `build_from_settings()`. So the ideal state is simple: it has a `from_crawler()` but not `from_settings()` and always passed the Crawler instance to created middleware. But as a temporary backwards-compatible state we want to keep both methods (with appropriate deprecation warnings) and be able to create middlewares without a Crawler instance, for which we should either keep (inlined?) a `build_from_settings()` function or use `create_instance()` there (which will print additional warnings). Thoughts?\r\n\r\nIn Scrapy itself `MiddlewareManager` is not used directly but only as a base class for: `scrapy.extension.ExtensionManager`, `scrapy.core.spidermw.SpiderMiddlewareManager`, `scrapy.core.downloader.middleware.DownloaderMiddlewareManager`, `scrapy.pipelines.ItemPipelineManager`. None of these override either `from_crawler()` or `from_settings()`. Instances of all of these are created via `from_crawler()`. Only `ItemPipelineManager` can be replaced (via `ITEM_PROCESSOR`) but it still needs to implement `from_crawler()` due to the previous statement. So we can safely drop the code path that doesn't take and pass a Crawler instance.\n(Additionally, if we don't have `build_from_settings()` we will be able deprecate the whole `from_settings()` interface later, though I don't know if we should)\n> we should either keep (inlined?) a `build_from_settings()` function or use `create_instance()` there (which will print additional warnings). Thoughts?\r\n\r\nI think it\u2019s best to avoid the `create_instance()` warning, be it by silencing it or by using an in-lined `build_from_settings()`, no opinion on that part. The `from_settings()` class method should emit its own, specific deprecation warning.\r\n\r\n> Additionally, if we don't have `build_from_settings()` we will be able deprecate the whole `from_settings()` interface later, though I don't know if we should\r\n\r\nUnless there is a good reason not to, it sounds consistent with deprecating `create_instance()` without adding `build_from_settings()`, so we should consider doing it for 2.12 already.",
        "created_at": "2024-11-12T16:34:43Z",
        "version": "2.11",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "tests/test_utils_request.py::CustomRequestFingerprinterTestCase::test_from_settings",
            "tests/test_webclient.py::WebClientCustomCiphersSSLTestCase::testPayload",
            "tests/test_webclient.py::WebClientCustomCiphersSSLTestCase::testPayloadDisabledCipher"
        ]
    },
    {
        "repo": "scrapy/scrapy",
        "pull_number": 6469,
        "instance_id": "scrapy__scrapy-6469",
        "issue_numbers": [
            "6468"
        ],
        "base_commit": "6ce0342beb1a5b588f353e52fe03d5e0ec84d938",
        "patch": "diff --git a/docs/topics/contracts.rst b/docs/topics/contracts.rst\nindex 2d61026e9a5..61aef4bbb42 100644\n--- a/docs/topics/contracts.rst\n+++ b/docs/topics/contracts.rst\n@@ -20,13 +20,13 @@ following example:\n         This function parses a sample response. Some contracts are mingled\n         with this docstring.\n \n-        @url http://www.amazon.com/s?field-keywords=selfish+gene\n+        @url http://www.example.com/s?field-keywords=selfish+gene\n         @returns items 1 16\n         @returns requests 0 0\n         @scrapes Title Author Year Price\n         \"\"\"\n \n-This callback is tested using three built-in contracts:\n+You can use the following contracts:\n \n .. module:: scrapy.contracts.default\n \n@@ -46,6 +46,14 @@ This callback is tested using three built-in contracts:\n \n     @cb_kwargs {\"arg1\": \"value1\", \"arg2\": \"value2\", ...}\n \n+.. class:: MetadataContract\n+\n+    This contract (``@meta``) sets the :attr:`meta <scrapy.Request.meta>`\n+    attribute for the sample request. It must be a valid JSON dictionary.\n+    ::\n+\n+    @meta {\"arg1\": \"value1\", \"arg2\": \"value2\", ...}\n+\n .. class:: ReturnsContract\n \n     This contract (``@returns``) sets lower and upper bounds for the items and\ndiff --git a/scrapy/contracts/default.py b/scrapy/contracts/default.py\nindex 71ca4168af9..e7b11d426ff 100644\n--- a/scrapy/contracts/default.py\n+++ b/scrapy/contracts/default.py\n@@ -35,6 +35,20 @@ def adjust_request_args(self, args: Dict[str, Any]) -> Dict[str, Any]:\n         return args\n \n \n+class MetadataContract(Contract):\n+    \"\"\"Contract to set metadata arguments for the request.\n+    The value should be JSON-encoded dictionary, e.g.:\n+\n+    @meta {\"arg1\": \"some value\"}\n+    \"\"\"\n+\n+    name = \"meta\"\n+\n+    def adjust_request_args(self, args: Dict[str, Any]) -> Dict[str, Any]:\n+        args[\"meta\"] = json.loads(\" \".join(self.args))\n+        return args\n+\n+\n class ReturnsContract(Contract):\n     \"\"\"Contract to check the output of a callback\n \ndiff --git a/scrapy/settings/default_settings.py b/scrapy/settings/default_settings.py\nindex 932475fb5ad..7ba0128a597 100644\n--- a/scrapy/settings/default_settings.py\n+++ b/scrapy/settings/default_settings.py\n@@ -333,6 +333,7 @@\n SPIDER_CONTRACTS_BASE = {\n     \"scrapy.contracts.default.UrlContract\": 1,\n     \"scrapy.contracts.default.CallbackKeywordArgumentsContract\": 1,\n+    \"scrapy.contracts.default.MetadataContract\": 1,\n     \"scrapy.contracts.default.ReturnsContract\": 2,\n     \"scrapy.contracts.default.ScrapesContract\": 3,\n }\n",
        "test_patch": "diff --git a/tests/test_contracts.py b/tests/test_contracts.py\nindex c9c12f0d804..d578b3af450 100644\n--- a/tests/test_contracts.py\n+++ b/tests/test_contracts.py\n@@ -8,6 +8,7 @@\n from scrapy.contracts import Contract, ContractsManager\n from scrapy.contracts.default import (\n     CallbackKeywordArgumentsContract,\n+    MetadataContract,\n     ReturnsContract,\n     ScrapesContract,\n     UrlContract,\n@@ -29,6 +30,10 @@ class ResponseMock:\n     url = \"http://scrapy.org\"\n \n \n+class ResponseMetaMock(ResponseMock):\n+    meta = None\n+\n+\n class CustomSuccessContract(Contract):\n     name = \"custom_success_contract\"\n \n@@ -195,6 +200,33 @@ def invalid_regex_with_valid_contract(self, response):\n         \"\"\"\n         pass\n \n+    def returns_request_meta(self, response):\n+        \"\"\"method which returns request\n+        @url https://example.org\n+        @meta {\"cookiejar\": \"session1\"}\n+        @returns requests 1\n+        \"\"\"\n+        return Request(\n+            \"https://example.org\", meta=response.meta, callback=self.returns_item_meta\n+        )\n+\n+    def returns_item_meta(self, response):\n+        \"\"\"method which returns item\n+        @url http://scrapy.org\n+        @meta {\"key\": \"example\"}\n+        @returns items 1 1\n+        \"\"\"\n+        return TestItem(name=\"example\", url=response.url)\n+\n+    def returns_error_missing_meta(self, response):\n+        \"\"\"method which depends of metadata be defined\n+\n+        @url http://scrapy.org\n+        @returns items 1\n+        \"\"\"\n+        key = response.meta[\"key\"]\n+        yield {key: \"value\"}\n+\n \n class CustomContractSuccessSpider(Spider):\n     name = \"custom_contract_success_spider\"\n@@ -224,6 +256,7 @@ class ContractsManagerTest(unittest.TestCase):\n     contracts = [\n         UrlContract,\n         CallbackKeywordArgumentsContract,\n+        MetadataContract,\n         ReturnsContract,\n         ScrapesContract,\n         CustomFormContract,\n@@ -328,6 +361,52 @@ def test_cb_kwargs(self):\n         request.callback(response, **request.cb_kwargs)\n         self.should_error()\n \n+    def test_meta(self):\n+        spider = TestSpider()\n+\n+        # extract contracts correctly\n+        contracts = self.conman.extract_contracts(spider.returns_request_meta)\n+        self.assertEqual(len(contracts), 3)\n+        self.assertEqual(\n+            frozenset(type(x) for x in contracts),\n+            frozenset([UrlContract, MetadataContract, ReturnsContract]),\n+        )\n+\n+        contracts = self.conman.extract_contracts(spider.returns_item_meta)\n+        self.assertEqual(len(contracts), 3)\n+        self.assertEqual(\n+            frozenset(type(x) for x in contracts),\n+            frozenset([UrlContract, MetadataContract, ReturnsContract]),\n+        )\n+\n+        response = ResponseMetaMock()\n+\n+        # returns_request\n+        request = self.conman.from_method(spider.returns_request_meta, self.results)\n+        assert request.meta[\"cookiejar\"] == \"session1\"\n+        response.meta = request.meta\n+        request.callback(response)\n+        assert response.meta[\"cookiejar\"] == \"session1\"\n+        self.should_succeed()\n+\n+        response = ResponseMetaMock()\n+\n+        # returns_item\n+        request = self.conman.from_method(spider.returns_item_meta, self.results)\n+        assert request.meta[\"key\"] == \"example\"\n+        response.meta = request.meta\n+        request.callback(ResponseMetaMock)\n+        assert response.meta[\"key\"] == \"example\"\n+        self.should_succeed()\n+\n+        response = ResponseMetaMock()\n+\n+        request = self.conman.from_method(\n+            spider.returns_error_missing_meta, self.results\n+        )\n+        request.callback(response)\n+        self.should_error()\n+\n     def test_returns(self):\n         spider = TestSpider()\n         response = ResponseMock()\n",
        "problem_statement": "Add support for meta in Spider Contracts\n<!--\r\n\r\nThanks for taking an interest in Scrapy!\r\n\r\nIf you have a question that starts with \"How to...\", please see the Scrapy Community page: https://scrapy.org/community/.\r\nThe GitHub issue tracker's purpose is to deal with bug reports and feature requests for the project itself.\r\n\r\nKeep in mind that by filing an issue, you are expected to comply with Scrapy's Code of Conduct, including treating everyone with respect: https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\r\n\r\nThe following is a suggested template to structure your pull request, you can find more guidelines at https://doc.scrapy.org/en/latest/contributing.html#writing-patches and https://doc.scrapy.org/en/latest/contributing.html#submitting-patches\r\n\r\n-->\r\n\r\n## Summary\r\n\r\nToday we support `cb_kwargs` but we have scenarios where the data is passed using `meta`.\r\n\r\n## Motivation\r\n\r\nI have some spiders that use `meta` to store information, e.g: `cookiejar`\r\n\r\n## Describe alternatives you've considered\r\n\r\nUse callback args.\r\n\n",
        "hints_text": "Agreed this would be a nice feature to support",
        "created_at": "2024-08-27T02:22:06Z",
        "version": "2.11",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "tests/test_contracts.py",
            "tests/test_contracts.py::ContractsManagerTest::test_meta"
        ]
    },
    {
        "repo": "scrapy/scrapy",
        "pull_number": 6374,
        "instance_id": "scrapy__scrapy-6374",
        "issue_numbers": [
            "6361"
        ],
        "base_commit": "631fc65fadb874629787ae5f7fdd876b9ec96a29",
        "patch": "diff --git a/.flake8 b/.flake8\nindex 62ccad9cf47..cf1a96476c2 100644\n--- a/.flake8\n+++ b/.flake8\n@@ -16,6 +16,7 @@ per-file-ignores =\n     scrapy/linkextractors/__init__.py:E402,F401\n     scrapy/selector/__init__.py:F401\n     scrapy/spiders/__init__.py:E402,F401\n+    tests/CrawlerRunner/change_reactor.py:E402\n \n     # Issues pending a review:\n     scrapy/utils/url.py:F403,F405\ndiff --git a/docs/topics/practices.rst b/docs/topics/practices.rst\nindex cd359b1473e..1500011e7b0 100644\n--- a/docs/topics/practices.rst\n+++ b/docs/topics/practices.rst\n@@ -92,7 +92,6 @@ reactor after ``MySpider`` has finished running.\n \n .. code-block:: python\n \n-    from twisted.internet import reactor\n     import scrapy\n     from scrapy.crawler import CrawlerRunner\n     from scrapy.utils.log import configure_logging\n@@ -107,6 +106,37 @@ reactor after ``MySpider`` has finished running.\n     runner = CrawlerRunner()\n \n     d = runner.crawl(MySpider)\n+\n+    from twisted.internet import reactor\n+\n+    d.addBoth(lambda _: reactor.stop())\n+    reactor.run()  # the script will block here until the crawling is finished\n+\n+Same example but using a non-default reactor, it's only necessary call\n+``install_reactor`` if you are using ``CrawlerRunner`` since ``CrawlerProcess`` already does this automatically.\n+\n+.. code-block:: python\n+\n+    import scrapy\n+    from scrapy.crawler import CrawlerRunner\n+    from scrapy.utils.log import configure_logging\n+\n+\n+    class MySpider(scrapy.Spider):\n+        # Your spider definition\n+        ...\n+\n+\n+    configure_logging({\"LOG_FORMAT\": \"%(levelname)s: %(message)s\"})\n+\n+    from scrapy.utils.reactor import install_reactor\n+\n+    install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n+    runner = CrawlerRunner()\n+    d = runner.crawl(MySpider)\n+\n+    from twisted.internet import reactor\n+\n     d.addBoth(lambda _: reactor.stop())\n     reactor.run()  # the script will block here until the crawling is finished\n \n@@ -151,7 +181,6 @@ Same example using :class:`~scrapy.crawler.CrawlerRunner`:\n .. code-block:: python\n \n     import scrapy\n-    from twisted.internet import reactor\n     from scrapy.crawler import CrawlerRunner\n     from scrapy.utils.log import configure_logging\n     from scrapy.utils.project import get_project_settings\n@@ -173,6 +202,9 @@ Same example using :class:`~scrapy.crawler.CrawlerRunner`:\n     runner.crawl(MySpider1)\n     runner.crawl(MySpider2)\n     d = runner.join()\n+\n+    from twisted.internet import reactor\n+\n     d.addBoth(lambda _: reactor.stop())\n \n     reactor.run()  # the script will block here until all crawling jobs are finished\n@@ -181,7 +213,7 @@ Same example but running the spiders sequentially by chaining the deferreds:\n \n .. code-block:: python\n \n-    from twisted.internet import reactor, defer\n+    from twisted.internet import defer\n     from scrapy.crawler import CrawlerRunner\n     from scrapy.utils.log import configure_logging\n     from scrapy.utils.project import get_project_settings\n@@ -209,6 +241,8 @@ Same example but running the spiders sequentially by chaining the deferreds:\n         reactor.stop()\n \n \n+    from twisted.internet import reactor\n+\n     crawl()\n     reactor.run()  # the script will block here until the last crawl call is finished\n \ndiff --git a/scrapy/crawler.py b/scrapy/crawler.py\nindex ccfe788913a..4fe5987a783 100644\n--- a/scrapy/crawler.py\n+++ b/scrapy/crawler.py\n@@ -129,6 +129,8 @@ def _apply_settings(self) -> None:\n             if is_asyncio_reactor_installed() and event_loop:\n                 verify_installed_asyncio_event_loop(event_loop)\n \n+            log_reactor_info()\n+\n         self.extensions = ExtensionManager.from_crawler(self)\n         self.settings.freeze()\n \n",
        "test_patch": "diff --git a/tests/CrawlerRunner/change_reactor.py b/tests/CrawlerRunner/change_reactor.py\nnew file mode 100644\nindex 00000000000..b20aa0c7cbf\n--- /dev/null\n+++ b/tests/CrawlerRunner/change_reactor.py\n@@ -0,0 +1,31 @@\n+from scrapy import Spider\n+from scrapy.crawler import CrawlerRunner\n+from scrapy.utils.log import configure_logging\n+\n+\n+class NoRequestsSpider(Spider):\n+    name = \"no_request\"\n+\n+    custom_settings = {\n+        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+    }\n+\n+    def start_requests(self):\n+        return []\n+\n+\n+configure_logging({\"LOG_FORMAT\": \"%(levelname)s: %(message)s\", \"LOG_LEVEL\": \"DEBUG\"})\n+\n+\n+from scrapy.utils.reactor import install_reactor\n+\n+install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n+\n+runner = CrawlerRunner()\n+\n+d = runner.crawl(NoRequestsSpider)\n+\n+from twisted.internet import reactor\n+\n+d.addBoth(callback=lambda _: reactor.stop())\n+reactor.run()\ndiff --git a/tests/test_crawler.py b/tests/test_crawler.py\nindex 989208694cb..791ea1faa66 100644\n--- a/tests/test_crawler.py\n+++ b/tests/test_crawler.py\n@@ -926,3 +926,11 @@ def test_response_ip_address(self):\n         self.assertIn(\"INFO: Host: not.a.real.domain\", log)\n         self.assertIn(\"INFO: Type: <class 'ipaddress.IPv4Address'>\", log)\n         self.assertIn(\"INFO: IP address: 127.0.0.1\", log)\n+\n+    def test_change_default_reactor(self):\n+        log = self.run_script(\"change_reactor.py\")\n+        self.assertIn(\n+            \"DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+            log,\n+        )\n+        self.assertIn(\"DEBUG: Using asyncio event loop\", log)\n",
        "problem_statement": "Remove top-level reactor imports from CrawlerProces/CrawlerRunner examples \nThere are several code examples on https://docs.scrapy.org/en/latest/topics/practices.html that have a top-level `from twisted.internet import reactor`, which is problematic (breaks when the settings specify a non-default reactor) and needs to be fixed.\n",
        "hints_text": "For this we should check if we have `TWISTED_REACTOR` setting defined (`get_project_settings`) and if is we call `install_reactor` before importing `reactor`? \nI think it's enough to move the imports inside blocks so that they only run after the setting is applied (i.e. after `Crawler.crawl()`, so after `CrawlerRunner.crawl()`). The changed examples should be tested with a non-default reactor setting value in any case.\r\n\r\nIf/when that's not possible to do it makes sense to add `install_reactor()` to examples I think.\n@wRAR I was testing this and noticed that if we have `TWISTED_REACTOR` in `custom_settings` but we don't call `install_reactor` we always get an exception when Scrapy runs `_apply_settings` method (using `CrawlerRunner`):\r\n\r\n```\r\nException: The installed reactor (twisted.internet.selectreactor.SelectReactor) does not match the requested one (twisted.internet.asyncioreactor.AsyncioSelectorReactor)\r\n```\r\n\r\nBecause `init_reactor`  is `False`:\r\n\r\nhttps://github.com/scrapy/scrapy/blob/631fc65fadb874629787ae5f7fdd876b9ec96a29/scrapy/crawler.py#L119\r\n\r\nI see `init_reactor` parameter https://github.com/scrapy/scrapy/blob/631fc65fadb874629787ae5f7fdd876b9ec96a29/scrapy/crawler.py#L79 but when we use `runner.crawl` from `CrawlerRunner` theres no way to override this parameter, when is created:\r\n\r\nhttps://github.com/scrapy/scrapy/blob/631fc65fadb874629787ae5f7fdd876b9ec96a29/scrapy/crawler.py#L330-L334\r\n\r\nP.S: If I switch to `CrawlerProcess` works (even without calling `install_reactor`), just to confirm if this is expected.\r\n\r\nHere my snippet:\r\n\r\n```python\r\nfrom scrapy import Spider\r\nfrom scrapy.http import Request\r\nfrom scrapy.crawler import CrawlerRunner\r\nfrom scrapy.utils.log import configure_logging\r\nfrom scrapy.utils.project import get_project_settings\r\n\r\n\r\nclass MySpider1(Spider):\r\n    name = \"my_spider\"\r\n\r\n    custom_settings = {\r\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\r\n    }\r\n\r\n    def start_requests(self):\r\n        yield Request(url=\"https://httpbin.org/anything\")\r\n\r\n    def parse(self, response):\r\n        yield response.json()\r\n\r\n\r\nclass MySpider2(Spider):\r\n    name = \"my_spider2\"\r\n\r\n    custom_settings = {\r\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\r\n    }\r\n\r\n    def start_requests(self):\r\n        yield Request(url=\"https://httpbin.org/anything\")\r\n\r\n    def parse(self, response):\r\n        yield response.json()\r\n\r\n\r\nconfigure_logging()\r\nsettings = get_project_settings()\r\nrunner = CrawlerRunner(settings)\r\n# from scrapy.utils.reactor import install_reactor\r\n# install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\r\nrunner.crawl(MySpider1)\r\nrunner.crawl(MySpider2)\r\nfrom twisted.internet import reactor\r\nd = runner.join()\r\nd.addBoth(lambda _: reactor.stop())\r\nreactor.run()\r\n```\n`CrawlerRunner` indeed requires you to install (and start) the reactor in your code, so it makes sense that `CrawlerRunner` examples show installing a non-default reactor manually.\n> `CrawlerRunner` indeed requires you to install (and start) the reactor in your code, so it makes sense that `CrawlerRunner` examples show installing a non-default reactor manually.\r\n\r\nGot it, thanks!",
        "created_at": "2024-05-22T10:51:11Z",
        "version": "2.11",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "tests/test_crawler.py::CrawlerRunnerSubprocess::test_change_default_reactor",
            "tests/test_crawler.py"
        ]
    },
    {
        "repo": "scrapy/scrapy",
        "pull_number": 6368,
        "instance_id": "scrapy__scrapy-6368",
        "issue_numbers": [
            "6365"
        ],
        "base_commit": "631fc65fadb874629787ae5f7fdd876b9ec96a29",
        "patch": "diff --git a/scrapy/pipelines/media.py b/scrapy/pipelines/media.py\nindex fd5e70cb903..5b03731a42f 100644\n--- a/scrapy/pipelines/media.py\n+++ b/scrapy/pipelines/media.py\n@@ -2,6 +2,7 @@\n \n import functools\n import logging\n+from abc import ABC, abstractmethod\n from collections import defaultdict\n from typing import TYPE_CHECKING\n \n@@ -27,7 +28,7 @@ def _DUMMY_CALLBACK(response):\n     return response\n \n \n-class MediaPipeline:\n+class MediaPipeline(ABC):\n     LOG_FAILED_RESULTS = True\n \n     class SpiderInfo:\n@@ -55,14 +56,6 @@ def _handle_statuses(self, allow_redirects):\n             self.handle_httpstatus_list = SequenceExclude(range(300, 400))\n \n     def _key_for_pipe(self, key, base_class_name=None, settings=None):\n-        \"\"\"\n-        >>> MediaPipeline()._key_for_pipe(\"IMAGES\")\n-        'IMAGES'\n-        >>> class MyPipe(MediaPipeline):\n-        ...     pass\n-        >>> MyPipe()._key_for_pipe(\"IMAGES\", base_class_name=\"MediaPipeline\")\n-        'MYPIPE_IMAGES'\n-        \"\"\"\n         class_name = self.__class__.__name__\n         formatted_key = f\"{class_name.upper()}_{key}\"\n         if (\n@@ -197,21 +190,25 @@ def _cache_result_and_execute_waiters(self, result, fp, info):\n             defer_result(result).chainDeferred(wad)\n \n     # Overridable Interface\n+    @abstractmethod\n     def media_to_download(self, request, info, *, item=None):\n         \"\"\"Check request before starting download\"\"\"\n-        pass\n+        raise NotImplementedError()\n \n+    @abstractmethod\n     def get_media_requests(self, item, info):\n         \"\"\"Returns the media requests to download\"\"\"\n-        pass\n+        raise NotImplementedError()\n \n+    @abstractmethod\n     def media_downloaded(self, response, request, info, *, item=None):\n         \"\"\"Handler for success downloads\"\"\"\n-        return response\n+        raise NotImplementedError()\n \n+    @abstractmethod\n     def media_failed(self, failure, request, info):\n         \"\"\"Handler for failed downloads\"\"\"\n-        return failure\n+        raise NotImplementedError()\n \n     def item_completed(self, results, item, info):\n         \"\"\"Called per item when all media requests has been processed\"\"\"\n@@ -226,6 +223,7 @@ def item_completed(self, results, item, info):\n                     )\n         return item\n \n+    @abstractmethod\n     def file_path(self, request, response=None, info=None, *, item=None):\n         \"\"\"Returns the path where downloaded media should be stored\"\"\"\n-        pass\n+        raise NotImplementedError()\n",
        "test_patch": "diff --git a/tests/test_pipeline_media.py b/tests/test_pipeline_media.py\nindex d4dde4a4036..76345355169 100644\n--- a/tests/test_pipeline_media.py\n+++ b/tests/test_pipeline_media.py\n@@ -1,4 +1,3 @@\n-import io\n from typing import Optional\n \n from testfixtures import LogCapture\n@@ -11,7 +10,6 @@\n from scrapy.http import Request, Response\n from scrapy.http.request import NO_CALLBACK\n from scrapy.pipelines.files import FileException\n-from scrapy.pipelines.images import ImagesPipeline\n from scrapy.pipelines.media import MediaPipeline\n from scrapy.settings import Settings\n from scrapy.spiders import Spider\n@@ -35,8 +33,26 @@ def _mocked_download_func(request, info):\n     return response() if callable(response) else response\n \n \n+class UserDefinedPipeline(MediaPipeline):\n+\n+    def media_to_download(self, request, info, *, item=None):\n+        pass\n+\n+    def get_media_requests(self, item, info):\n+        pass\n+\n+    def media_downloaded(self, response, request, info, *, item=None):\n+        return {}\n+\n+    def media_failed(self, failure, request, info):\n+        return failure\n+\n+    def file_path(self, request, response=None, info=None, *, item=None):\n+        return \"\"\n+\n+\n class BaseMediaPipelineTestCase(unittest.TestCase):\n-    pipeline_class = MediaPipeline\n+    pipeline_class = UserDefinedPipeline\n     settings = None\n \n     def setUp(self):\n@@ -54,54 +70,6 @@ def tearDown(self):\n             if not name.startswith(\"_\"):\n                 disconnect_all(signal)\n \n-    def test_default_media_to_download(self):\n-        request = Request(\"http://url\")\n-        assert self.pipe.media_to_download(request, self.info) is None\n-\n-    def test_default_get_media_requests(self):\n-        item = {\"name\": \"name\"}\n-        assert self.pipe.get_media_requests(item, self.info) is None\n-\n-    def test_default_media_downloaded(self):\n-        request = Request(\"http://url\")\n-        response = Response(\"http://url\", body=b\"\")\n-        assert self.pipe.media_downloaded(response, request, self.info) is response\n-\n-    def test_default_media_failed(self):\n-        request = Request(\"http://url\")\n-        fail = Failure(Exception())\n-        assert self.pipe.media_failed(fail, request, self.info) is fail\n-\n-    def test_default_item_completed(self):\n-        item = {\"name\": \"name\"}\n-        assert self.pipe.item_completed([], item, self.info) is item\n-\n-        # Check that failures are logged by default\n-        fail = Failure(Exception())\n-        results = [(True, 1), (False, fail)]\n-\n-        with LogCapture() as log:\n-            new_item = self.pipe.item_completed(results, item, self.info)\n-\n-        assert new_item is item\n-        assert len(log.records) == 1\n-        record = log.records[0]\n-        assert record.levelname == \"ERROR\"\n-        self.assertTupleEqual(record.exc_info, failure_to_exc_info(fail))\n-\n-        # disable failure logging and check again\n-        self.pipe.LOG_FAILED_RESULTS = False\n-        with LogCapture() as log:\n-            new_item = self.pipe.item_completed(results, item, self.info)\n-        assert new_item is item\n-        assert len(log.records) == 0\n-\n-    @inlineCallbacks\n-    def test_default_process_item(self):\n-        item = {\"name\": \"name\"}\n-        new_item = yield self.pipe.process_item(item, self.spider)\n-        assert new_item is item\n-\n     def test_modify_media_request(self):\n         request = Request(\"http://url\")\n         self.pipe._modify_media_request(request)\n@@ -175,8 +143,38 @@ def test_should_remove_req_res_references_before_caching_the_results(self):\n         context = getattr(info.downloaded[fp].value, \"__context__\", None)\n         self.assertIsNone(context)\n \n+    def test_default_item_completed(self):\n+        item = {\"name\": \"name\"}\n+        assert self.pipe.item_completed([], item, self.info) is item\n+\n+        # Check that failures are logged by default\n+        fail = Failure(Exception())\n+        results = [(True, 1), (False, fail)]\n+\n+        with LogCapture() as log:\n+            new_item = self.pipe.item_completed(results, item, self.info)\n+\n+        assert new_item is item\n+        assert len(log.records) == 1\n+        record = log.records[0]\n+        assert record.levelname == \"ERROR\"\n+        self.assertTupleEqual(record.exc_info, failure_to_exc_info(fail))\n+\n+        # disable failure logging and check again\n+        self.pipe.LOG_FAILED_RESULTS = False\n+        with LogCapture() as log:\n+            new_item = self.pipe.item_completed(results, item, self.info)\n+        assert new_item is item\n+        assert len(log.records) == 0\n+\n+    @inlineCallbacks\n+    def test_default_process_item(self):\n+        item = {\"name\": \"name\"}\n+        new_item = yield self.pipe.process_item(item, self.spider)\n+        assert new_item is item\n+\n \n-class MockedMediaPipeline(MediaPipeline):\n+class MockedMediaPipeline(UserDefinedPipeline):\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n         self._mockcalled = []\n@@ -232,7 +230,7 @@ def test_result_succeed(self):\n         )\n         item = {\"requests\": req}\n         new_item = yield self.pipe.process_item(item, self.spider)\n-        self.assertEqual(new_item[\"results\"], [(True, rsp)])\n+        self.assertEqual(new_item[\"results\"], [(True, {})])\n         self.assertEqual(\n             self.pipe._mockcalled,\n             [\n@@ -277,7 +275,7 @@ def test_mix_of_success_and_failure(self):\n         req2 = Request(\"http://url2\", meta={\"response\": fail})\n         item = {\"requests\": [req1, req2]}\n         new_item = yield self.pipe.process_item(item, self.spider)\n-        self.assertEqual(new_item[\"results\"], [(True, rsp1), (False, fail)])\n+        self.assertEqual(new_item[\"results\"], [(True, {}), (False, fail)])\n         m = self.pipe._mockcalled\n         # only once\n         self.assertEqual(m[0], \"get_media_requests\")  # first hook called\n@@ -315,7 +313,7 @@ def test_results_are_cached_across_multiple_items(self):\n         item = {\"requests\": req1}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertTrue(new_item is item)\n-        self.assertEqual(new_item[\"results\"], [(True, rsp1)])\n+        self.assertEqual(new_item[\"results\"], [(True, {})])\n \n         # rsp2 is ignored, rsp1 must be in results because request fingerprints are the same\n         req2 = Request(\n@@ -325,7 +323,7 @@ def test_results_are_cached_across_multiple_items(self):\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertTrue(new_item is item)\n         self.assertEqual(self.fingerprint(req1), self.fingerprint(req2))\n-        self.assertEqual(new_item[\"results\"], [(True, rsp1)])\n+        self.assertEqual(new_item[\"results\"], [(True, {})])\n \n     @inlineCallbacks\n     def test_results_are_cached_for_requests_of_single_item(self):\n@@ -337,7 +335,7 @@ def test_results_are_cached_for_requests_of_single_item(self):\n         item = {\"requests\": [req1, req2]}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertTrue(new_item is item)\n-        self.assertEqual(new_item[\"results\"], [(True, rsp1), (True, rsp1)])\n+        self.assertEqual(new_item[\"results\"], [(True, {}), (True, {})])\n \n     @inlineCallbacks\n     def test_wait_if_request_is_downloading(self):\n@@ -363,7 +361,7 @@ def rsp2_func():\n         req2 = Request(req1.url, meta={\"response\": rsp2_func})\n         item = {\"requests\": [req1, req2]}\n         new_item = yield self.pipe.process_item(item, self.spider)\n-        self.assertEqual(new_item[\"results\"], [(True, rsp1), (True, rsp1)])\n+        self.assertEqual(new_item[\"results\"], [(True, {}), (True, {})])\n \n     @inlineCallbacks\n     def test_use_media_to_download_result(self):\n@@ -376,57 +374,15 @@ def test_use_media_to_download_result(self):\n             [\"get_media_requests\", \"media_to_download\", \"item_completed\"],\n         )\n \n-\n-class MockedMediaPipelineDeprecatedMethods(ImagesPipeline):\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-        self._mockcalled = []\n-\n-    def get_media_requests(self, item, info):\n-        item_url = item[\"image_urls\"][0]\n-        output_img = io.BytesIO()\n-        img = Image.new(\"RGB\", (60, 30), color=\"red\")\n-        img.save(output_img, format=\"JPEG\")\n-        return Request(\n-            item_url,\n-            meta={\n-                \"response\": Response(item_url, status=200, body=output_img.getvalue())\n-            },\n+    def test_key_for_pipe(self):\n+        self.assertEqual(\n+            self.pipe._key_for_pipe(\"IMAGES\", base_class_name=\"MediaPipeline\"),\n+            \"MOCKEDMEDIAPIPELINE_IMAGES\",\n         )\n \n-    def inc_stats(self, *args, **kwargs):\n-        return True\n-\n-    def media_to_download(self, request, info):\n-        self._mockcalled.append(\"media_to_download\")\n-        return super().media_to_download(request, info)\n-\n-    def media_downloaded(self, response, request, info):\n-        self._mockcalled.append(\"media_downloaded\")\n-        return super().media_downloaded(response, request, info)\n-\n-    def file_downloaded(self, response, request, info):\n-        self._mockcalled.append(\"file_downloaded\")\n-        return super().file_downloaded(response, request, info)\n-\n-    def file_path(self, request, response=None, info=None):\n-        self._mockcalled.append(\"file_path\")\n-        return super().file_path(request, response, info)\n-\n-    def thumb_path(self, request, thumb_id, response=None, info=None):\n-        self._mockcalled.append(\"thumb_path\")\n-        return super().thumb_path(request, thumb_id, response, info)\n-\n-    def get_images(self, response, request, info):\n-        self._mockcalled.append(\"get_images\")\n-        return super().get_images(response, request, info)\n-\n-    def image_downloaded(self, response, request, info):\n-        self._mockcalled.append(\"image_downloaded\")\n-        return super().image_downloaded(response, request, info)\n-\n \n class MediaPipelineAllowRedirectSettingsTestCase(unittest.TestCase):\n+\n     def _assert_request_no3xx(self, pipeline_class, settings):\n         pipe = pipeline_class(settings=Settings(settings))\n         request = Request(\"http://url\")\n@@ -452,18 +408,11 @@ def _assert_request_no3xx(self, pipeline_class, settings):\n             else:\n                 self.assertNotIn(status, request.meta[\"handle_httpstatus_list\"])\n \n-    def test_standard_setting(self):\n-        self._assert_request_no3xx(MediaPipeline, {\"MEDIA_ALLOW_REDIRECTS\": True})\n-\n     def test_subclass_standard_setting(self):\n-        class UserDefinedPipeline(MediaPipeline):\n-            pass\n \n         self._assert_request_no3xx(UserDefinedPipeline, {\"MEDIA_ALLOW_REDIRECTS\": True})\n \n     def test_subclass_specific_setting(self):\n-        class UserDefinedPipeline(MediaPipeline):\n-            pass\n \n         self._assert_request_no3xx(\n             UserDefinedPipeline, {\"USERDEFINEDPIPELINE_MEDIA_ALLOW_REDIRECTS\": True}\n",
        "problem_statement": "Fix overridable methods in MediaPipeline\n`MediaPipeline` defines several empty or almost empty \"overridable\" methods, which return things inconsistent with their overrides. I propose making all of them raise `NotImplementedError`. Alternatively `MediaPipeline` should just be made an abstract class and all those methods made abstract methods, but I have no idea if that will break anything (e.g. do all children always override all of those methods?).\r\n\r\nAnother problem is existing tests, that test specifically that e.g. `MediaPipeline.media_downloaded()` returns a response, which makes no sense to me (normally `media_downloaded()` returns a file info dict), so all those need to be changed or removed.\r\n\r\nAnd another problem, indirectly related to this, is that this interface is very poorly documented, most of these functions are not mentioned in the docs at all, so it's not always clear what should they take and return (and the code uses many of them as callbacks in long callback chains so it's not clear even from the code).\n",
        "hints_text": "",
        "created_at": "2024-05-18T19:20:40Z",
        "version": "2.11",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "tests/test_pipeline_media.py"
        ]
    },
    {
        "repo": "scrapy/scrapy",
        "pull_number": 6352,
        "instance_id": "scrapy__scrapy-6352",
        "issue_numbers": [
            "6340",
            "6340"
        ],
        "base_commit": "ae7bb849f50af0b91eea4f022d93ad201e545c06",
        "patch": "diff --git a/docs/news.rst b/docs/news.rst\nindex fafea0bf8aa..7db4e59a10e 100644\n--- a/docs/news.rst\n+++ b/docs/news.rst\n@@ -3,6 +3,20 @@\n Release notes\n =============\n \n+\n+.. _release-VERSION:\n+\n+Scrapy VERSION (YYYY-MM-DD)\n+---------------------------\n+\n+Deprecations\n+~~~~~~~~~~~~\n+\n+-   :func:`scrapy.core.downloader.Downloader._get_slot_key` is now deprecated.\n+    Consider using its corresponding public method get_slot_key() instead.\n+    (:issue:`6340`)\n+\n+\n .. _release-2.11.1:\n \n Scrapy 2.11.1 (2024-02-14)\ndiff --git a/scrapy/core/downloader/__init__.py b/scrapy/core/downloader/__init__.py\nindex 98e1af6fb1c..0ab3bdb779b 100644\n--- a/scrapy/core/downloader/__init__.py\n+++ b/scrapy/core/downloader/__init__.py\n@@ -1,4 +1,5 @@\n import random\n+import warnings\n from collections import deque\n from datetime import datetime\n from time import time\n@@ -10,6 +11,7 @@\n from scrapy import Request, Spider, signals\n from scrapy.core.downloader.handlers import DownloadHandlers\n from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Response\n from scrapy.resolver import dnscache\n from scrapy.settings import BaseSettings\n@@ -125,7 +127,7 @@ def needs_backout(self) -> bool:\n         return len(self.active) >= self.total_concurrency\n \n     def _get_slot(self, request: Request, spider: Spider) -> Tuple[str, Slot]:\n-        key = self._get_slot_key(request, spider)\n+        key = self.get_slot_key(request)\n         if key not in self.slots:\n             slot_settings = self.per_slot_settings.get(key, {})\n             conc = (\n@@ -143,7 +145,7 @@ def _get_slot(self, request: Request, spider: Spider) -> Tuple[str, Slot]:\n \n         return key, self.slots[key]\n \n-    def _get_slot_key(self, request: Request, spider: Optional[Spider]) -> str:\n+    def get_slot_key(self, request: Request) -> str:\n         if self.DOWNLOAD_SLOT in request.meta:\n             return cast(str, request.meta[self.DOWNLOAD_SLOT])\n \n@@ -153,6 +155,14 @@ def _get_slot_key(self, request: Request, spider: Optional[Spider]) -> str:\n \n         return key\n \n+    def _get_slot_key(self, request: Request, spider: Optional[Spider]) -> str:\n+        warnings.warn(\n+            \"Use of this protected method is deprecated. Consider using its corresponding public method get_slot_key() instead.\",\n+            ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return self.get_slot_key(request)\n+\n     def _enqueue_request(self, request: Request, spider: Spider) -> Deferred:\n         key, slot = self._get_slot(request, spider)\n         request.meta[self.DOWNLOAD_SLOT] = key\ndiff --git a/scrapy/pqueues.py b/scrapy/pqueues.py\nindex 773825c5e41..58a47ef0ff0 100644\n--- a/scrapy/pqueues.py\n+++ b/scrapy/pqueues.py\n@@ -180,7 +180,7 @@ def stats(self, possible_slots: Iterable[str]) -> List[Tuple[int, str]]:\n         return [(self._active_downloads(slot), slot) for slot in possible_slots]\n \n     def get_slot_key(self, request: Request) -> str:\n-        return self.downloader._get_slot_key(request, None)\n+        return self.downloader.get_slot_key(request)\n \n     def _active_downloads(self, slot: str) -> int:\n         \"\"\"Return a number of requests in a Downloader for a given slot\"\"\"\n",
        "test_patch": "diff --git a/tests/test_scheduler.py b/tests/test_scheduler.py\nindex 37099dae676..02b50baa3a6 100644\n--- a/tests/test_scheduler.py\n+++ b/tests/test_scheduler.py\n@@ -25,7 +25,7 @@ class MockDownloader:\n     def __init__(self):\n         self.slots = {}\n \n-    def _get_slot_key(self, request, spider):\n+    def get_slot_key(self, request):\n         if Downloader.DOWNLOAD_SLOT in request.meta:\n             return request.meta[Downloader.DOWNLOAD_SLOT]\n \n@@ -273,14 +273,14 @@ def test_logic(self):\n         while self.scheduler.has_pending_requests():\n             request = self.scheduler.next_request()\n             # pylint: disable=protected-access\n-            slot = downloader._get_slot_key(request, None)\n+            slot = downloader.get_slot_key(request)\n             dequeued_slots.append(slot)\n             downloader.increment(slot)\n             requests.append(request)\n \n         for request in requests:\n             # pylint: disable=protected-access\n-            slot = downloader._get_slot_key(request, None)\n+            slot = downloader.get_slot_key(request)\n             downloader.decrement(slot)\n \n         self.assertTrue(\n",
        "problem_statement": "Deprecate the `spider` argument to `Downloader._get_slot_key()`\nThe `spider` argument is not used inside the method since 2012, but we can't remove it as external code calls it (either because it subclasses `Downloader` or because it wants the slot name).\r\n\r\nActually maybe we want to promote it to a public method for the second reason? Not sure how often it's needed but see e.g. https://github.com/scrapy-plugins/scrapy-zyte-api/blob/a2284c8cdf157ef6d36c1cc413933761c5ed792b/scrapy_zyte_api/_middlewares.py#L32\nDeprecate the `spider` argument to `Downloader._get_slot_key()`\nThe `spider` argument is not used inside the method since 2012, but we can't remove it as external code calls it (either because it subclasses `Downloader` or because it wants the slot name).\r\n\r\nActually maybe we want to promote it to a public method for the second reason? Not sure how often it's needed but see e.g. https://github.com/scrapy-plugins/scrapy-zyte-api/blob/a2284c8cdf157ef6d36c1cc413933761c5ed792b/scrapy_zyte_api/_middlewares.py#L32\n",
        "hints_text": "+1 to make it public.\r\n\r\nAlthough I wonder if, for examples like the linked one, the right approach could be to allow customizing how slots IDs are generated instead.\n@wRAR do you want to promote this method to public and keep the old method with some Deprecate message ?\nYeah, let's make it public and without the extra argument and keep the old one with a deprecation warning.\n+1 to make it public.\r\n\r\nAlthough I wonder if, for examples like the linked one, the right approach could be to allow customizing how slots IDs are generated instead.\n@wRAR do you want to promote this method to public and keep the old method with some Deprecate message ?\nYeah, let's make it public and without the extra argument and keep the old one with a deprecation warning.",
        "created_at": "2024-05-09T18:52:31Z",
        "version": "2.11",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "tests/test_scheduler.py"
        ]
    },
    {
        "repo": "scrapy/scrapy",
        "pull_number": 6347,
        "instance_id": "scrapy__scrapy-6347",
        "issue_numbers": [
            "6342",
            "6342"
        ],
        "base_commit": "4300a1d240c7c2c21a4ef0c1c60c3d844493e516",
        "patch": "diff --git a/docs/topics/downloader-middleware.rst b/docs/topics/downloader-middleware.rst\nindex 1abbc49684f..d4cd062fe38 100644\n--- a/docs/topics/downloader-middleware.rst\n+++ b/docs/topics/downloader-middleware.rst\n@@ -884,6 +884,10 @@ Meta tags within these tags are ignored.\n    The default value of :setting:`METAREFRESH_IGNORE_TAGS` changed from\n    ``['script', 'noscript']`` to ``[]``.\n \n+.. versionchanged:: VERSION\n+   The default value of :setting:`METAREFRESH_IGNORE_TAGS` changed from\n+   ``[]`` to ``['noscript']``.\n+\n .. setting:: METAREFRESH_MAXDELAY\n \n METAREFRESH_MAXDELAY\ndiff --git a/scrapy/settings/default_settings.py b/scrapy/settings/default_settings.py\nindex 2b3d95a0e14..d7ac7ec350f 100644\n--- a/scrapy/settings/default_settings.py\n+++ b/scrapy/settings/default_settings.py\n@@ -239,7 +239,7 @@\n MEMUSAGE_WARNING_MB = 0\n \n METAREFRESH_ENABLED = True\n-METAREFRESH_IGNORE_TAGS = []\n+METAREFRESH_IGNORE_TAGS = [\"noscript\"]\n METAREFRESH_MAXDELAY = 100\n \n NEWSPIDER_MODULE = \"\"\n",
        "test_patch": "diff --git a/tests/test_downloadermiddleware_redirect.py b/tests/test_downloadermiddleware_redirect.py\nindex 10b8ca9afb9..83ff259823a 100644\n--- a/tests/test_downloadermiddleware_redirect.py\n+++ b/tests/test_downloadermiddleware_redirect.py\n@@ -395,9 +395,8 @@ def test_ignore_tags_default(self):\n             \"\"\"content=\"0;URL='http://example.org/newpage'\"></noscript>\"\"\"\n         )\n         rsp = HtmlResponse(req.url, body=body.encode())\n-        req2 = self.mw.process_response(req, rsp, self.spider)\n-        assert isinstance(req2, Request)\n-        self.assertEqual(req2.url, \"http://example.org/newpage\")\n+        response = self.mw.process_response(req, rsp, self.spider)\n+        assert isinstance(response, Response)\n \n     def test_ignore_tags_1_x_list(self):\n         \"\"\"Test that Scrapy 1.x behavior remains possible\"\"\"\n",
        "problem_statement": "Set METAREFRESH_IGNORE_TAGS to [\"noscript\"] by default\nI was wrong in https://github.com/scrapy/scrapy/issues/3844. The default value should be `[\"noscript\"]`, to deal with [antibot behaviors](https://github.com/scrapy/scrapy/commit/ec1ef0235f9deee0c263c9b31652d3e74a754acc).\r\n\r\nFound by @mukthy.\nSet METAREFRESH_IGNORE_TAGS to [\"noscript\"] by default\nI was wrong in https://github.com/scrapy/scrapy/issues/3844. The default value should be `[\"noscript\"]`, to deal with [antibot behaviors](https://github.com/scrapy/scrapy/commit/ec1ef0235f9deee0c263c9b31652d3e74a754acc).\r\n\r\nFound by @mukthy.\n",
        "hints_text": "on it\non it",
        "created_at": "2024-05-08T16:39:49Z",
        "version": "2.11",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "tests/test_downloadermiddleware_redirect.py"
        ]
    },
    {
        "repo": "scrapy/scrapy",
        "pull_number": 6269,
        "instance_id": "scrapy__scrapy-6269",
        "issue_numbers": [
            "6263"
        ],
        "base_commit": "bf149356fc6e519e92fb55150a60b40b14e45ae8",
        "patch": "diff --git a/scrapy/downloadermiddlewares/httpcompression.py b/scrapy/downloadermiddlewares/httpcompression.py\nindex aa3abe85379..0e5e215ac8e 100644\n--- a/scrapy/downloadermiddlewares/httpcompression.py\n+++ b/scrapy/downloadermiddlewares/httpcompression.py\n@@ -29,7 +29,10 @@\n ACCEPTED_ENCODINGS: List[bytes] = [b\"gzip\", b\"deflate\"]\n \n try:\n-    import brotli  # noqa: F401\n+    try:\n+        import brotli  # noqa: F401\n+    except ImportError:\n+        import brotlicffi  # noqa: F401\n except ImportError:\n     pass\n else:\ndiff --git a/scrapy/utils/_compression.py b/scrapy/utils/_compression.py\nindex 7c40d0a02d1..84c255c28f9 100644\n--- a/scrapy/utils/_compression.py\n+++ b/scrapy/utils/_compression.py\n@@ -5,7 +5,10 @@\n from scrapy.exceptions import ScrapyDeprecationWarning\n \n try:\n-    import brotli\n+    try:\n+        import brotli\n+    except ImportError:\n+        import brotlicffi as brotli\n except ImportError:\n     pass\n else:\n@@ -17,9 +20,9 @@\n                 \"You have brotlipy installed, and Scrapy will use it, but \"\n                 \"Scrapy support for brotlipy is deprecated and will stop \"\n                 \"working in a future version of Scrapy. brotlipy itself is \"\n-                \"deprecated, it has been superseded by brotlicffi (not \"\n-                \"currently supported by Scrapy). Please, uninstall brotlipy \"\n-                \"and install brotli instead. brotlipy has the same import \"\n+                \"deprecated, it has been superseded by brotlicffi. \"\n+                \"Please, uninstall brotlipy \"\n+                \"and install brotli or brotlicffi instead. brotlipy has the same import \"\n                 \"name as brotli, so keeping both installed is strongly \"\n                 \"discouraged.\"\n             ),\n",
        "test_patch": "diff --git a/tests/requirements.txt b/tests/requirements.txt\nindex 5b75674f513..ca5f6ddbd93 100644\n--- a/tests/requirements.txt\n+++ b/tests/requirements.txt\n@@ -11,8 +11,7 @@ uvloop; platform_system != \"Windows\"\n \n bpython  # optional for shell wrapper tests\n brotli; implementation_name != 'pypy'  # optional for HTTP compress downloader middleware tests\n-# 1.1.0 is broken on PyPy: https://github.com/google/brotli/issues/1072\n-brotli==1.0.9; implementation_name == 'pypy'  # optional for HTTP compress downloader middleware tests\n+brotlicffi; implementation_name == 'pypy'  # optional for HTTP compress downloader middleware tests\n zstandard; implementation_name != 'pypy'  # optional for HTTP compress downloader middleware tests\n ipython\n pywin32; sys_platform == \"win32\"\ndiff --git a/tests/test_downloadermiddleware_httpcompression.py b/tests/test_downloadermiddleware_httpcompression.py\nindex ae5569d0a8a..7c36f748e35 100644\n--- a/tests/test_downloadermiddleware_httpcompression.py\n+++ b/tests/test_downloadermiddleware_httpcompression.py\n@@ -130,7 +130,10 @@ def test_process_response_gzip(self):\n \n     def test_process_response_br(self):\n         try:\n-            import brotli  # noqa: F401\n+            try:\n+                import brotli  # noqa: F401\n+            except ImportError:\n+                import brotlicffi  # noqa: F401\n         except ImportError:\n             raise SkipTest(\"no brotli\")\n         response = self._getresponse(\"br\")\n@@ -448,7 +451,10 @@ def _test_compression_bomb_setting(self, compression_id):\n \n     def test_compression_bomb_setting_br(self):\n         try:\n-            import brotli  # noqa: F401\n+            try:\n+                import brotli  # noqa: F401\n+            except ImportError:\n+                import brotlicffi  # noqa: F401\n         except ImportError:\n             raise SkipTest(\"no brotli\")\n         self._test_compression_bomb_setting(\"br\")\n@@ -486,7 +492,10 @@ class DownloadMaxSizeSpider(Spider):\n \n     def test_compression_bomb_spider_attr_br(self):\n         try:\n-            import brotli  # noqa: F401\n+            try:\n+                import brotli  # noqa: F401\n+            except ImportError:\n+                import brotlicffi  # noqa: F401\n         except ImportError:\n             raise SkipTest(\"no brotli\")\n         self._test_compression_bomb_spider_attr(\"br\")\n@@ -522,7 +531,10 @@ def _test_compression_bomb_request_meta(self, compression_id):\n \n     def test_compression_bomb_request_meta_br(self):\n         try:\n-            import brotli  # noqa: F401\n+            try:\n+                import brotli  # noqa: F401\n+            except ImportError:\n+                import brotlicffi  # noqa: F401\n         except ImportError:\n             raise SkipTest(\"no brotli\")\n         self._test_compression_bomb_request_meta(\"br\")\n@@ -568,7 +580,10 @@ def _test_download_warnsize_setting(self, compression_id):\n \n     def test_download_warnsize_setting_br(self):\n         try:\n-            import brotli  # noqa: F401\n+            try:\n+                import brotli  # noqa: F401\n+            except ImportError:\n+                import brotlicffi  # noqa: F401\n         except ImportError:\n             raise SkipTest(\"no brotli\")\n         self._test_download_warnsize_setting(\"br\")\n@@ -616,7 +631,10 @@ class DownloadWarnSizeSpider(Spider):\n \n     def test_download_warnsize_spider_attr_br(self):\n         try:\n-            import brotli  # noqa: F401\n+            try:\n+                import brotli  # noqa: F401\n+            except ImportError:\n+                import brotlicffi  # noqa: F401\n         except ImportError:\n             raise SkipTest(\"no brotli\")\n         self._test_download_warnsize_spider_attr(\"br\")\n@@ -662,7 +680,10 @@ def _test_download_warnsize_request_meta(self, compression_id):\n \n     def test_download_warnsize_request_meta_br(self):\n         try:\n-            import brotli  # noqa: F401\n+            try:\n+                import brotli  # noqa: F401\n+            except ImportError:\n+                import brotlicffi  # noqa: F401\n         except ImportError:\n             raise SkipTest(\"no brotli\")\n         self._test_download_warnsize_request_meta(\"br\")\n",
        "problem_statement": "Add brotlicffi support\nCurrently, brotli compression is supported when using `brotli` or `brotlipy` (deprecated). We should also support it thorugh `brotlicffi`, the new name of `brotlipy`, which performs worse than `brotli` but works on PyPy.\n",
        "hints_text": "",
        "created_at": "2024-03-06T01:20:27Z",
        "version": "2.11",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "tests/requirements.txt",
            "tests/test_downloadermiddleware_httpcompression.py"
        ]
    },
    {
        "repo": "scrapy/scrapy",
        "pull_number": 6257,
        "instance_id": "scrapy__scrapy-6257",
        "issue_numbers": [
            "6254"
        ],
        "base_commit": "532cc8a517b31dca4ca28d0a35d25d1a790c9801",
        "patch": "diff --git a/pylintrc b/pylintrc\nindex 78004e78ac8..c60e4e16a33 100644\n--- a/pylintrc\n+++ b/pylintrc\n@@ -68,7 +68,6 @@ disable=abstract-method,\n         too-many-public-methods,\n         too-many-return-statements,\n         unbalanced-tuple-unpacking,\n-        unnecessary-comprehension,\n         unnecessary-dunder-call,\n         unnecessary-pass,\n         unreachable,\n@@ -77,7 +76,6 @@ disable=abstract-method,\n         unused-private-member,\n         unused-variable,\n         unused-wildcard-import,\n-        use-dict-literal,\n         used-before-assignment,\n         useless-return,\n         wildcard-import,\ndiff --git a/scrapy/downloadermiddlewares/httpcompression.py b/scrapy/downloadermiddlewares/httpcompression.py\nindex f0ad24f72a6..aa3abe85379 100644\n--- a/scrapy/downloadermiddlewares/httpcompression.py\n+++ b/scrapy/downloadermiddlewares/httpcompression.py\n@@ -135,7 +135,7 @@ def process_response(\n                 respcls = responsetypes.from_args(\n                     headers=response.headers, url=response.url, body=decoded_body\n                 )\n-                kwargs = dict(cls=respcls, body=decoded_body)\n+                kwargs = {\"cls\": respcls, \"body\": decoded_body}\n                 if issubclass(respcls, TextResponse):\n                     # force recalculating the encoding until we make sure the\n                     # responsetypes guessing is reliable\ndiff --git a/scrapy/spiders/crawl.py b/scrapy/spiders/crawl.py\nindex ebb4f598456..2a3913da582 100644\n--- a/scrapy/spiders/crawl.py\n+++ b/scrapy/spiders/crawl.py\n@@ -85,7 +85,7 @@ def _build_request(self, rule_index, link):\n             url=link.url,\n             callback=self._callback,\n             errback=self._errback,\n-            meta=dict(rule=rule_index, link_text=link.text),\n+            meta={\"rule\": rule_index, \"link_text\": link.text},\n         )\n \n     def _requests_to_follow(self, response):\ndiff --git a/scrapy/utils/python.py b/scrapy/utils/python.py\nindex 7b408c49cf4..1e7364e494d 100644\n--- a/scrapy/utils/python.py\n+++ b/scrapy/utils/python.py\n@@ -162,7 +162,7 @@ def _chunk_iter() -> Generator[Tuple[str, int], Any, None]:\n         pattern = re.compile(pattern)\n \n     for chunk, offset in _chunk_iter():\n-        matches = [match for match in pattern.finditer(chunk)]\n+        matches = list(pattern.finditer(chunk))\n         if matches:\n             start, end = matches[-1].span()\n             return offset + start, offset + end\n",
        "test_patch": "diff --git a/tests/test_crawl.py b/tests/test_crawl.py\nindex 96d43b2b96d..6cde4ed8c50 100644\n--- a/tests/test_crawl.py\n+++ b/tests/test_crawl.py\n@@ -76,11 +76,11 @@ def test_randomized_delay(self):\n \n     @defer.inlineCallbacks\n     def _test_delay(self, total, delay, randomize=False):\n-        crawl_kwargs = dict(\n-            maxlatency=delay * 2,\n-            mockserver=self.mockserver,\n-            total=total,\n-        )\n+        crawl_kwargs = {\n+            \"maxlatency\": delay * 2,\n+            \"mockserver\": self.mockserver,\n+            \"total\": total,\n+        }\n         tolerance = 1 - (0.6 if randomize else 0.2)\n \n         settings = {\"DOWNLOAD_DELAY\": delay, \"RANDOMIZE_DOWNLOAD_DELAY\": randomize}\ndiff --git a/tests/test_downloadermiddleware_cookies.py b/tests/test_downloadermiddleware_cookies.py\nindex 4a81a638ee1..425fabcc7a8 100644\n--- a/tests/test_downloadermiddleware_cookies.py\n+++ b/tests/test_downloadermiddleware_cookies.py\n@@ -320,7 +320,7 @@ def test_local_domain(self):\n \n     @pytest.mark.xfail(reason=\"Cookie header is not currently being processed\")\n     def test_keep_cookie_from_default_request_headers_middleware(self):\n-        DEFAULT_REQUEST_HEADERS = dict(Cookie=\"default=value; asdf=qwerty\")\n+        DEFAULT_REQUEST_HEADERS = {\"Cookie\": \"default=value; asdf=qwerty\"}\n         mw_default_headers = DefaultHeadersMiddleware(DEFAULT_REQUEST_HEADERS.items())\n         # overwrite with values from 'cookies' request argument\n         req1 = Request(\"http://example.org\", cookies={\"default\": \"something\"})\ndiff --git a/tests/test_downloadermiddleware_httpauth.py b/tests/test_downloadermiddleware_httpauth.py\nindex fc110e6cc2c..500af65364a 100644\n--- a/tests/test_downloadermiddleware_httpauth.py\n+++ b/tests/test_downloadermiddleware_httpauth.py\n@@ -59,7 +59,7 @@ def test_auth_subdomain(self):\n         self.assertEqual(req.headers[\"Authorization\"], basic_auth_header(\"foo\", \"bar\"))\n \n     def test_auth_already_set(self):\n-        req = Request(\"http://example.com/\", headers=dict(Authorization=\"Digest 123\"))\n+        req = Request(\"http://example.com/\", headers={\"Authorization\": \"Digest 123\"})\n         assert self.mw.process_request(req, self.spider) is None\n         self.assertEqual(req.headers[\"Authorization\"], b\"Digest 123\")\n \n@@ -79,6 +79,6 @@ def test_auth(self):\n         self.assertEqual(req.headers[\"Authorization\"], basic_auth_header(\"foo\", \"bar\"))\n \n     def test_auth_already_set(self):\n-        req = Request(\"http://example.com/\", headers=dict(Authorization=\"Digest 123\"))\n+        req = Request(\"http://example.com/\", headers={\"Authorization\": \"Digest 123\"})\n         assert self.mw.process_request(req, self.spider) is None\n         self.assertEqual(req.headers[\"Authorization\"], b\"Digest 123\")\ndiff --git a/tests/test_exporters.py b/tests/test_exporters.py\nindex 59b724495d1..fa938904412 100644\n--- a/tests/test_exporters.py\n+++ b/tests/test_exporters.py\n@@ -152,7 +152,7 @@ def test_invalid_option(self):\n \n     def test_nested_item(self):\n         i1 = self.item_class(name=\"Joseph\", age=\"22\")\n-        i2 = dict(name=\"Maria\", age=i1)\n+        i2 = {\"name\": \"Maria\", \"age\": i1}\n         i3 = self.item_class(name=\"Jesus\", age=i2)\n         ie = self._get_exporter()\n         exported = ie.export_item(i3)\n@@ -185,7 +185,7 @@ def test_export_list(self):\n \n     def test_export_item_dict_list(self):\n         i1 = self.item_class(name=\"Joseph\", age=\"22\")\n-        i2 = dict(name=\"Maria\", age=[i1])\n+        i2 = {\"name\": \"Maria\", \"age\": [i1]}\n         i3 = self.item_class(name=\"Jesus\", age=[i2])\n         ie = self._get_exporter()\n         exported = ie.export_item(i3)\n@@ -373,7 +373,7 @@ class TestItem2(Item):\n \n     def test_join_multivalue_not_strings(self):\n         self.assertExportResult(\n-            item=dict(name=\"John\", friends=[4, 8]),\n+            item={\"name\": \"John\", \"friends\": [4, 8]},\n             include_headers_line=False,\n             expected='\"[4, 8]\",John\\r\\n',\n         )\n@@ -388,14 +388,14 @@ def test_nonstring_types_item(self):\n     def test_errors_default(self):\n         with self.assertRaises(UnicodeEncodeError):\n             self.assertExportResult(\n-                item=dict(text=\"W\\u0275\\u200Brd\"),\n+                item={\"text\": \"W\\u0275\\u200Brd\"},\n                 expected=None,\n                 encoding=\"windows-1251\",\n             )\n \n     def test_errors_xmlcharrefreplace(self):\n         self.assertExportResult(\n-            item=dict(text=\"W\\u0275\\u200Brd\"),\n+            item={\"text\": \"W\\u0275\\u200Brd\"},\n             include_headers_line=False,\n             expected=\"W&#629;&#8203;rd\\r\\n\",\n             encoding=\"windows-1251\",\n@@ -455,8 +455,8 @@ def test_multivalued_fields(self):\n         )\n \n     def test_nested_item(self):\n-        i1 = dict(name=\"foo\\xa3hoo\", age=\"22\")\n-        i2 = dict(name=\"bar\", age=i1)\n+        i1 = {\"name\": \"foo\\xa3hoo\", \"age\": \"22\"}\n+        i2 = {\"name\": \"bar\", \"age\": i1}\n         i3 = self.item_class(name=\"buz\", age=i2)\n \n         self.assertExportResult(\n@@ -478,8 +478,8 @@ def test_nested_item(self):\n         )\n \n     def test_nested_list_item(self):\n-        i1 = dict(name=\"foo\")\n-        i2 = dict(name=\"bar\", v2={\"egg\": [\"spam\"]})\n+        i1 = {\"name\": \"foo\"}\n+        i2 = {\"name\": \"bar\", \"v2\": {\"egg\": [\"spam\"]}}\n         i3 = self.item_class(name=\"buz\", age=[i1, i2])\n \n         self.assertExportResult(\n@@ -534,7 +534,7 @@ def _check_output(self):\n \n     def test_nested_item(self):\n         i1 = self.item_class(name=\"Joseph\", age=\"22\")\n-        i2 = dict(name=\"Maria\", age=i1)\n+        i2 = {\"name\": \"Maria\", \"age\": i1}\n         i3 = self.item_class(name=\"Jesus\", age=i2)\n         self.ie.start_exporting()\n         self.ie.export_item(i3)\n@@ -622,9 +622,9 @@ def test_nested_item(self):\n         self.assertEqual(exported, [expected])\n \n     def test_nested_dict_item(self):\n-        i1 = dict(name=\"Joseph\\xa3\", age=\"22\")\n+        i1 = {\"name\": \"Joseph\\xa3\", \"age\": \"22\"}\n         i2 = self.item_class(name=\"Maria\", age=i1)\n-        i3 = dict(name=\"Jesus\", age=i2)\n+        i3 = {\"name\": \"Jesus\", \"age\": i2}\n         self.ie.start_exporting()\n         self.ie.export_item(i3)\n         self.ie.finish_exporting()\ndiff --git a/tests/test_linkextractors.py b/tests/test_linkextractors.py\nindex 6b4df90d888..217c7a29904 100644\n--- a/tests/test_linkextractors.py\n+++ b/tests/test_linkextractors.py\n@@ -37,7 +37,7 @@ def test_extract_all_links(self):\n             page4_url = \"http://example.com/page%204.html\"\n \n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                     Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n@@ -55,7 +55,7 @@ def test_extract_all_links(self):\n         def test_extract_filter_allow(self):\n             lx = self.extractor_cls(allow=(\"sample\",))\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                     Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n@@ -70,7 +70,7 @@ def test_extract_filter_allow(self):\n         def test_extract_filter_allow_with_duplicates(self):\n             lx = self.extractor_cls(allow=(\"sample\",), unique=False)\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                     Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n@@ -93,7 +93,7 @@ def test_extract_filter_allow_with_duplicates(self):\n         def test_extract_filter_allow_with_duplicates_canonicalize(self):\n             lx = self.extractor_cls(allow=(\"sample\",), unique=False, canonicalize=True)\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                     Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n@@ -116,7 +116,7 @@ def test_extract_filter_allow_with_duplicates_canonicalize(self):\n         def test_extract_filter_allow_no_duplicates_canonicalize(self):\n             lx = self.extractor_cls(allow=(\"sample\",), unique=True, canonicalize=True)\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                     Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n@@ -127,7 +127,7 @@ def test_extract_filter_allow_no_duplicates_canonicalize(self):\n         def test_extract_filter_allow_and_deny(self):\n             lx = self.extractor_cls(allow=(\"sample\",), deny=(\"3\",))\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                     Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n@@ -137,7 +137,7 @@ def test_extract_filter_allow_and_deny(self):\n         def test_extract_filter_allowed_domains(self):\n             lx = self.extractor_cls(allow_domains=(\"google.com\",))\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://www.google.com/something\", text=\"\"),\n                 ],\n@@ -148,7 +148,7 @@ def test_extraction_using_single_values(self):\n \n             lx = self.extractor_cls(allow=\"sample\")\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                     Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n@@ -162,7 +162,7 @@ def test_extraction_using_single_values(self):\n \n             lx = self.extractor_cls(allow=\"sample\", deny=\"3\")\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                     Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n@@ -171,7 +171,7 @@ def test_extraction_using_single_values(self):\n \n             lx = self.extractor_cls(allow_domains=\"google.com\")\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://www.google.com/something\", text=\"\"),\n                 ],\n@@ -179,7 +179,7 @@ def test_extraction_using_single_values(self):\n \n             lx = self.extractor_cls(deny_domains=\"example.com\")\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://www.google.com/something\", text=\"\"),\n                 ],\n@@ -265,7 +265,7 @@ def test_matches(self):\n         def test_restrict_xpaths(self):\n             lx = self.extractor_cls(restrict_xpaths=('//div[@id=\"subwrapper\"]',))\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                     Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n@@ -337,7 +337,7 @@ def test_restrict_css_and_restrict_xpaths_together(self):\n                 restrict_css=(\"#subwrapper + a\",),\n             )\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                     Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n@@ -705,7 +705,7 @@ def test_link_wrong_href(self):\n             response = HtmlResponse(\"http://example.org/index.html\", body=html)\n             lx = self.extractor_cls()\n             self.assertEqual(\n-                [link for link in lx.extract_links(response)],\n+                list(lx.extract_links(response)),\n                 [\n                     Link(\n                         url=\"http://example.org/item1.html\",\n@@ -758,7 +758,7 @@ def test_link_wrong_href(self):\n         response = HtmlResponse(\"http://example.org/index.html\", body=html)\n         lx = self.extractor_cls()\n         self.assertEqual(\n-            [link for link in lx.extract_links(response)],\n+            list(lx.extract_links(response)),\n             [\n                 Link(\n                     url=\"http://example.org/item1.html\", text=\"Item 1\", nofollow=False\n@@ -779,7 +779,7 @@ def test_link_restrict_text(self):\n         # Simple text inclusion test\n         lx = self.extractor_cls(restrict_text=\"dog\")\n         self.assertEqual(\n-            [link for link in lx.extract_links(response)],\n+            list(lx.extract_links(response)),\n             [\n                 Link(\n                     url=\"http://example.org/item2.html\",\n@@ -791,7 +791,7 @@ def test_link_restrict_text(self):\n         # Unique regex test\n         lx = self.extractor_cls(restrict_text=r\"of.*dog\")\n         self.assertEqual(\n-            [link for link in lx.extract_links(response)],\n+            list(lx.extract_links(response)),\n             [\n                 Link(\n                     url=\"http://example.org/item2.html\",\n@@ -803,7 +803,7 @@ def test_link_restrict_text(self):\n         # Multiple regex test\n         lx = self.extractor_cls(restrict_text=[r\"of.*dog\", r\"of.*cat\"])\n         self.assertEqual(\n-            [link for link in lx.extract_links(response)],\n+            list(lx.extract_links(response)),\n             [\n                 Link(\n                     url=\"http://example.org/item1.html\",\n@@ -834,7 +834,7 @@ def test_skip_bad_links(self):\n         response = HtmlResponse(\"http://example.org/index.html\", body=html)\n         lx = self.extractor_cls()\n         self.assertEqual(\n-            [link for link in lx.extract_links(response)],\n+            list(lx.extract_links(response)),\n             [\n                 Link(\n                     url=\"http://example.org/item2.html\",\ndiff --git a/tests/test_loader_deprecated.py b/tests/test_loader_deprecated.py\nindex 99cdf88d96f..528efa142a7 100644\n--- a/tests/test_loader_deprecated.py\n+++ b/tests/test_loader_deprecated.py\n@@ -565,37 +565,37 @@ class NoInputReprocessingFromDictTest(unittest.TestCase):\n     \"\"\"\n \n     def test_avoid_reprocessing_with_initial_values_single(self):\n-        il = NoInputReprocessingDictLoader(item=dict(title=\"foo\"))\n+        il = NoInputReprocessingDictLoader(item={\"title\": \"foo\"})\n         il_loaded = il.load_item()\n-        self.assertEqual(il_loaded, dict(title=\"foo\"))\n+        self.assertEqual(il_loaded, {\"title\": \"foo\"})\n         self.assertEqual(\n-            NoInputReprocessingDictLoader(item=il_loaded).load_item(), dict(title=\"foo\")\n+            NoInputReprocessingDictLoader(item=il_loaded).load_item(), {\"title\": \"foo\"}\n         )\n \n     def test_avoid_reprocessing_with_initial_values_list(self):\n-        il = NoInputReprocessingDictLoader(item=dict(title=[\"foo\", \"bar\"]))\n+        il = NoInputReprocessingDictLoader(item={\"title\": [\"foo\", \"bar\"]})\n         il_loaded = il.load_item()\n-        self.assertEqual(il_loaded, dict(title=\"foo\"))\n+        self.assertEqual(il_loaded, {\"title\": \"foo\"})\n         self.assertEqual(\n-            NoInputReprocessingDictLoader(item=il_loaded).load_item(), dict(title=\"foo\")\n+            NoInputReprocessingDictLoader(item=il_loaded).load_item(), {\"title\": \"foo\"}\n         )\n \n     def test_avoid_reprocessing_without_initial_values_single(self):\n         il = NoInputReprocessingDictLoader()\n         il.add_value(\"title\", \"foo\")\n         il_loaded = il.load_item()\n-        self.assertEqual(il_loaded, dict(title=\"FOO\"))\n+        self.assertEqual(il_loaded, {\"title\": \"FOO\"})\n         self.assertEqual(\n-            NoInputReprocessingDictLoader(item=il_loaded).load_item(), dict(title=\"FOO\")\n+            NoInputReprocessingDictLoader(item=il_loaded).load_item(), {\"title\": \"FOO\"}\n         )\n \n     def test_avoid_reprocessing_without_initial_values_list(self):\n         il = NoInputReprocessingDictLoader()\n         il.add_value(\"title\", [\"foo\", \"bar\"])\n         il_loaded = il.load_item()\n-        self.assertEqual(il_loaded, dict(title=\"FOO\"))\n+        self.assertEqual(il_loaded, {\"title\": \"FOO\"})\n         self.assertEqual(\n-            NoInputReprocessingDictLoader(item=il_loaded).load_item(), dict(title=\"FOO\")\n+            NoInputReprocessingDictLoader(item=il_loaded).load_item(), {\"title\": \"FOO\"}\n         )\n \n \ndiff --git a/tests/test_mail.py b/tests/test_mail.py\nindex 2535e58db26..ff15053978a 100644\n--- a/tests/test_mail.py\n+++ b/tests/test_mail.py\n@@ -91,7 +91,7 @@ def test_send_attach(self):\n         self.assertEqual(attach.get_payload(decode=True), b\"content\")\n \n     def _catch_mail_sent(self, **kwargs):\n-        self.catched_msg = dict(**kwargs)\n+        self.catched_msg = {**kwargs}\n \n     def test_send_utf8(self):\n         subject = \"s\u00fcbj\u00e8\u00e7t\"\ndiff --git a/tests/test_pipeline_crawl.py b/tests/test_pipeline_crawl.py\nindex be9811980df..5a9a217cee3 100644\n--- a/tests/test_pipeline_crawl.py\n+++ b/tests/test_pipeline_crawl.py\n@@ -140,7 +140,7 @@ def _assert_files_download_failure(self, crawler, items, code, logs):\n         self.assertEqual(logs.count(file_dl_failure), 3)\n \n         # check that no files were written to the media store\n-        self.assertEqual([x for x in self.tmpmediastore.iterdir()], [])\n+        self.assertEqual(list(self.tmpmediastore.iterdir()), [])\n \n     @defer.inlineCallbacks\n     def test_download_media(self):\ndiff --git a/tests/test_pipeline_files.py b/tests/test_pipeline_files.py\nindex e7000e3140c..0babde4d90f 100644\n--- a/tests/test_pipeline_files.py\n+++ b/tests/test_pipeline_files.py\n@@ -221,7 +221,7 @@ def file_path(self, request, response=None, info=None, item=None):\n         file_path = CustomFilesPipeline.from_settings(\n             Settings({\"FILES_STORE\": self.tempdir})\n         ).file_path\n-        item = dict(path=\"path-to-store-file\")\n+        item = {\"path\": \"path-to-store-file\"}\n         request = Request(\"http://example.com\")\n         self.assertEqual(file_path(request, item=item), \"full/path-to-store-file\")\n \ndiff --git a/tests/test_pipeline_images.py b/tests/test_pipeline_images.py\nindex 2e2e06b89a9..18a2454b3db 100644\n--- a/tests/test_pipeline_images.py\n+++ b/tests/test_pipeline_images.py\n@@ -132,7 +132,7 @@ def thumb_path(\n         thumb_path = CustomImagesPipeline.from_settings(\n             Settings({\"IMAGES_STORE\": self.tempdir})\n         ).thumb_path\n-        item = dict(path=\"path-to-store-file\")\n+        item = {\"path\": \"path-to-store-file\"}\n         request = Request(\"http://example.com\")\n         self.assertEqual(\n             thumb_path(request, \"small\", item=item), \"thumb/small/path-to-store-file\"\n@@ -433,14 +433,14 @@ class ImagesPipelineTestCaseCustomSettings(unittest.TestCase):\n     ]\n \n     # This should match what is defined in ImagesPipeline.\n-    default_pipeline_settings = dict(\n-        MIN_WIDTH=0,\n-        MIN_HEIGHT=0,\n-        EXPIRES=90,\n-        THUMBS={},\n-        IMAGES_URLS_FIELD=\"image_urls\",\n-        IMAGES_RESULT_FIELD=\"images\",\n-    )\n+    default_pipeline_settings = {\n+        \"MIN_WIDTH\": 0,\n+        \"MIN_HEIGHT\": 0,\n+        \"EXPIRES\": 90,\n+        \"THUMBS\": {},\n+        \"IMAGES_URLS_FIELD\": \"image_urls\",\n+        \"IMAGES_RESULT_FIELD\": \"images\",\n+    }\n \n     def setUp(self):\n         self.tempdir = mkdtemp()\ndiff --git a/tests/test_pipeline_media.py b/tests/test_pipeline_media.py\nindex d477b59be40..d4dde4a4036 100644\n--- a/tests/test_pipeline_media.py\n+++ b/tests/test_pipeline_media.py\n@@ -59,7 +59,7 @@ def test_default_media_to_download(self):\n         assert self.pipe.media_to_download(request, self.info) is None\n \n     def test_default_get_media_requests(self):\n-        item = dict(name=\"name\")\n+        item = {\"name\": \"name\"}\n         assert self.pipe.get_media_requests(item, self.info) is None\n \n     def test_default_media_downloaded(self):\n@@ -73,7 +73,7 @@ def test_default_media_failed(self):\n         assert self.pipe.media_failed(fail, request, self.info) is fail\n \n     def test_default_item_completed(self):\n-        item = dict(name=\"name\")\n+        item = {\"name\": \"name\"}\n         assert self.pipe.item_completed([], item, self.info) is item\n \n         # Check that failures are logged by default\n@@ -98,7 +98,7 @@ def test_default_item_completed(self):\n \n     @inlineCallbacks\n     def test_default_process_item(self):\n-        item = dict(name=\"name\")\n+        item = {\"name\": \"name\"}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         assert new_item is item\n \n@@ -226,11 +226,11 @@ def test_result_succeed(self):\n         rsp = Response(\"http://url1\")\n         req = Request(\n             \"http://url1\",\n-            meta=dict(response=rsp),\n+            meta={\"response\": rsp},\n             callback=self._callback,\n             errback=self._errback,\n         )\n-        item = dict(requests=req)\n+        item = {\"requests\": req}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertEqual(new_item[\"results\"], [(True, rsp)])\n         self.assertEqual(\n@@ -250,11 +250,11 @@ def test_result_failure(self):\n         fail = Failure(Exception())\n         req = Request(\n             \"http://url1\",\n-            meta=dict(response=fail),\n+            meta={\"response\": fail},\n             callback=self._callback,\n             errback=self._errback,\n         )\n-        item = dict(requests=req)\n+        item = {\"requests\": req}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertEqual(new_item[\"results\"], [(False, fail)])\n         self.assertEqual(\n@@ -272,10 +272,10 @@ def test_result_failure(self):\n     def test_mix_of_success_and_failure(self):\n         self.pipe.LOG_FAILED_RESULTS = False\n         rsp1 = Response(\"http://url1\")\n-        req1 = Request(\"http://url1\", meta=dict(response=rsp1))\n+        req1 = Request(\"http://url1\", meta={\"response\": rsp1})\n         fail = Failure(Exception())\n-        req2 = Request(\"http://url2\", meta=dict(response=fail))\n-        item = dict(requests=[req1, req2])\n+        req2 = Request(\"http://url2\", meta={\"response\": fail})\n+        item = {\"requests\": [req1, req2]}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertEqual(new_item[\"results\"], [(True, rsp1), (False, fail)])\n         m = self.pipe._mockcalled\n@@ -294,7 +294,7 @@ def test_mix_of_success_and_failure(self):\n     def test_get_media_requests(self):\n         # returns single Request (without callback)\n         req = Request(\"http://url\")\n-        item = dict(requests=req)  # pass a single item\n+        item = {\"requests\": req}  # pass a single item\n         new_item = yield self.pipe.process_item(item, self.spider)\n         assert new_item is item\n         self.assertIn(self.fingerprint(req), self.info.downloaded)\n@@ -302,7 +302,7 @@ def test_get_media_requests(self):\n         # returns iterable of Requests\n         req1 = Request(\"http://url1\")\n         req2 = Request(\"http://url2\")\n-        item = dict(requests=iter([req1, req2]))\n+        item = {\"requests\": iter([req1, req2])}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         assert new_item is item\n         assert self.fingerprint(req1) in self.info.downloaded\n@@ -311,17 +311,17 @@ def test_get_media_requests(self):\n     @inlineCallbacks\n     def test_results_are_cached_across_multiple_items(self):\n         rsp1 = Response(\"http://url1\")\n-        req1 = Request(\"http://url1\", meta=dict(response=rsp1))\n-        item = dict(requests=req1)\n+        req1 = Request(\"http://url1\", meta={\"response\": rsp1})\n+        item = {\"requests\": req1}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertTrue(new_item is item)\n         self.assertEqual(new_item[\"results\"], [(True, rsp1)])\n \n         # rsp2 is ignored, rsp1 must be in results because request fingerprints are the same\n         req2 = Request(\n-            req1.url, meta=dict(response=Response(\"http://donot.download.me\"))\n+            req1.url, meta={\"response\": Response(\"http://donot.download.me\")}\n         )\n-        item = dict(requests=req2)\n+        item = {\"requests\": req2}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertTrue(new_item is item)\n         self.assertEqual(self.fingerprint(req1), self.fingerprint(req2))\n@@ -330,11 +330,11 @@ def test_results_are_cached_across_multiple_items(self):\n     @inlineCallbacks\n     def test_results_are_cached_for_requests_of_single_item(self):\n         rsp1 = Response(\"http://url1\")\n-        req1 = Request(\"http://url1\", meta=dict(response=rsp1))\n+        req1 = Request(\"http://url1\", meta={\"response\": rsp1})\n         req2 = Request(\n-            req1.url, meta=dict(response=Response(\"http://donot.download.me\"))\n+            req1.url, meta={\"response\": Response(\"http://donot.download.me\")}\n         )\n-        item = dict(requests=[req1, req2])\n+        item = {\"requests\": [req1, req2]}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertTrue(new_item is item)\n         self.assertEqual(new_item[\"results\"], [(True, rsp1), (True, rsp1)])\n@@ -359,16 +359,16 @@ def rsp1_func():\n         def rsp2_func():\n             self.fail(\"it must cache rsp1 result and must not try to redownload\")\n \n-        req1 = Request(\"http://url\", meta=dict(response=rsp1_func))\n-        req2 = Request(req1.url, meta=dict(response=rsp2_func))\n-        item = dict(requests=[req1, req2])\n+        req1 = Request(\"http://url\", meta={\"response\": rsp1_func})\n+        req2 = Request(req1.url, meta={\"response\": rsp2_func})\n+        item = {\"requests\": [req1, req2]}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertEqual(new_item[\"results\"], [(True, rsp1), (True, rsp1)])\n \n     @inlineCallbacks\n     def test_use_media_to_download_result(self):\n-        req = Request(\"http://url\", meta=dict(result=\"ITSME\", response=self.fail))\n-        item = dict(requests=req)\n+        req = Request(\"http://url\", meta={\"result\": \"ITSME\", \"response\": self.fail})\n+        item = {\"requests\": req}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertEqual(new_item[\"results\"], [(True, \"ITSME\")])\n         self.assertEqual(\ndiff --git a/tests/test_scheduler.py b/tests/test_scheduler.py\nindex f8465a5ffed..37099dae676 100644\n--- a/tests/test_scheduler.py\n+++ b/tests/test_scheduler.py\n@@ -45,15 +45,15 @@ def close(self):\n \n class MockCrawler(Crawler):\n     def __init__(self, priority_queue_cls, jobdir):\n-        settings = dict(\n-            SCHEDULER_DEBUG=False,\n-            SCHEDULER_DISK_QUEUE=\"scrapy.squeues.PickleLifoDiskQueue\",\n-            SCHEDULER_MEMORY_QUEUE=\"scrapy.squeues.LifoMemoryQueue\",\n-            SCHEDULER_PRIORITY_QUEUE=priority_queue_cls,\n-            JOBDIR=jobdir,\n-            DUPEFILTER_CLASS=\"scrapy.dupefilters.BaseDupeFilter\",\n-            REQUEST_FINGERPRINTER_IMPLEMENTATION=\"2.7\",\n-        )\n+        settings = {\n+            \"SCHEDULER_DEBUG\": False,\n+            \"SCHEDULER_DISK_QUEUE\": \"scrapy.squeues.PickleLifoDiskQueue\",\n+            \"SCHEDULER_MEMORY_QUEUE\": \"scrapy.squeues.LifoMemoryQueue\",\n+            \"SCHEDULER_PRIORITY_QUEUE\": priority_queue_cls,\n+            \"JOBDIR\": jobdir,\n+            \"DUPEFILTER_CLASS\": \"scrapy.dupefilters.BaseDupeFilter\",\n+            \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n+        }\n         super().__init__(Spider, settings)\n         self.engine = MockEngine(downloader=MockDownloader())\n         self.stats = load_object(self.settings[\"STATS_CLASS\"])(self)\n@@ -338,10 +338,10 @@ def test_integration_downloader_aware_priority_queue(self):\n \n class TestIncompatibility(unittest.TestCase):\n     def _incompatible(self):\n-        settings = dict(\n-            SCHEDULER_PRIORITY_QUEUE=\"scrapy.pqueues.DownloaderAwarePriorityQueue\",\n-            CONCURRENT_REQUESTS_PER_IP=1,\n-        )\n+        settings = {\n+            \"SCHEDULER_PRIORITY_QUEUE\": \"scrapy.pqueues.DownloaderAwarePriorityQueue\",\n+            \"CONCURRENT_REQUESTS_PER_IP\": 1,\n+        }\n         crawler = get_crawler(Spider, settings)\n         scheduler = Scheduler.from_crawler(crawler)\n         spider = Spider(name=\"spider\")\ndiff --git a/tests/test_spidermiddleware_offsite.py b/tests/test_spidermiddleware_offsite.py\nindex ea45b769869..837f1c2c8f5 100644\n--- a/tests/test_spidermiddleware_offsite.py\n+++ b/tests/test_spidermiddleware_offsite.py\n@@ -16,10 +16,10 @@ def setUp(self):\n         self.mw.spider_opened(self.spider)\n \n     def _get_spiderargs(self):\n-        return dict(\n-            name=\"foo\",\n-            allowed_domains=[\"scrapytest.org\", \"scrapy.org\", \"scrapy.test.org\"],\n-        )\n+        return {\n+            \"name\": \"foo\",\n+            \"allowed_domains\": [\"scrapytest.org\", \"scrapy.org\", \"scrapy.test.org\"],\n+        }\n \n     def test_process_spider_output(self):\n         res = Response(\"http://scrapytest.org\")\n@@ -50,7 +50,7 @@ def test_process_spider_output(self):\n \n class TestOffsiteMiddleware2(TestOffsiteMiddleware):\n     def _get_spiderargs(self):\n-        return dict(name=\"foo\", allowed_domains=None)\n+        return {\"name\": \"foo\", \"allowed_domains\": None}\n \n     def test_process_spider_output(self):\n         res = Response(\"http://scrapytest.org\")\n@@ -61,13 +61,16 @@ def test_process_spider_output(self):\n \n class TestOffsiteMiddleware3(TestOffsiteMiddleware2):\n     def _get_spiderargs(self):\n-        return dict(name=\"foo\")\n+        return {\"name\": \"foo\"}\n \n \n class TestOffsiteMiddleware4(TestOffsiteMiddleware3):\n     def _get_spiderargs(self):\n         bad_hostname = urlparse(\"http:////scrapytest.org\").hostname\n-        return dict(name=\"foo\", allowed_domains=[\"scrapytest.org\", None, bad_hostname])\n+        return {\n+            \"name\": \"foo\",\n+            \"allowed_domains\": [\"scrapytest.org\", None, bad_hostname],\n+        }\n \n     def test_process_spider_output(self):\n         res = Response(\"http://scrapytest.org\")\ndiff --git a/tests/test_utils_iterators.py b/tests/test_utils_iterators.py\nindex ee22e6675d3..ec377bb19ad 100644\n--- a/tests/test_utils_iterators.py\n+++ b/tests/test_utils_iterators.py\n@@ -355,7 +355,7 @@ def test_csviter_defaults(self):\n         response = TextResponse(url=\"http://example.com/\", body=body)\n         csv = csviter(response)\n \n-        result = [row for row in csv]\n+        result = list(csv)\n         self.assertEqual(\n             result,\n             [\n@@ -377,7 +377,7 @@ def test_csviter_delimiter(self):\n         csv = csviter(response, delimiter=\"\\t\")\n \n         self.assertEqual(\n-            [row for row in csv],\n+            list(csv),\n             [\n                 {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n                 {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n@@ -394,7 +394,7 @@ def test_csviter_quotechar(self):\n         csv1 = csviter(response1, quotechar=\"'\")\n \n         self.assertEqual(\n-            [row for row in csv1],\n+            list(csv1),\n             [\n                 {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n                 {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n@@ -407,7 +407,7 @@ def test_csviter_quotechar(self):\n         csv2 = csviter(response2, delimiter=\"|\", quotechar=\"'\")\n \n         self.assertEqual(\n-            [row for row in csv2],\n+            list(csv2),\n             [\n                 {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n                 {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n@@ -422,7 +422,7 @@ def test_csviter_wrong_quotechar(self):\n         csv = csviter(response)\n \n         self.assertEqual(\n-            [row for row in csv],\n+            list(csv),\n             [\n                 {\"'id'\": \"1\", \"'name'\": \"'alpha'\", \"'value'\": \"'foobar'\"},\n                 {\n@@ -441,7 +441,7 @@ def test_csviter_delimiter_binary_response_assume_utf8_encoding(self):\n         csv = csviter(response, delimiter=\"\\t\")\n \n         self.assertEqual(\n-            [row for row in csv],\n+            list(csv),\n             [\n                 {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n                 {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n@@ -458,7 +458,7 @@ def test_csviter_headers(self):\n         csv = csviter(response, headers=[h.decode(\"utf-8\") for h in headers])\n \n         self.assertEqual(\n-            [row for row in csv],\n+            list(csv),\n             [\n                 {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n                 {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n@@ -475,7 +475,7 @@ def test_csviter_falserow(self):\n         csv = csviter(response)\n \n         self.assertEqual(\n-            [row for row in csv],\n+            list(csv),\n             [\n                 {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n                 {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\ndiff --git a/tests/test_utils_template.py b/tests/test_utils_template.py\nindex cbe80e157d1..fc42c0d2f4d 100644\n--- a/tests/test_utils_template.py\n+++ b/tests/test_utils_template.py\n@@ -16,7 +16,7 @@ def tearDown(self):\n         rmtree(self.tmp_path)\n \n     def test_simple_render(self):\n-        context = dict(project_name=\"proj\", name=\"spi\", classname=\"TheSpider\")\n+        context = {\"project_name\": \"proj\", \"name\": \"spi\", \"classname\": \"TheSpider\"}\n         template = \"from ${project_name}.spiders.${name} import ${classname}\"\n         rendered = \"from proj.spiders.spi import TheSpider\"\n \n",
        "problem_statement": "Fix and re-enable `unnecessary-comprehension` and `use-dict-literal` pylint tags\nBoth are valid simplification hints.\n",
        "hints_text": "",
        "created_at": "2024-02-28T19:40:38Z",
        "version": "2.11",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "tests/test_crawl.py",
            "tests/test_downloadermiddleware_cookies.py",
            "tests/test_downloadermiddleware_httpauth.py",
            "tests/test_exporters.py",
            "tests/test_linkextractors.py",
            "tests/test_loader_deprecated.py",
            "tests/test_mail.py",
            "tests/test_pipeline_crawl.py",
            "tests/test_pipeline_files.py",
            "tests/test_pipeline_images.py",
            "tests/test_pipeline_media.py",
            "tests/test_scheduler.py",
            "tests/test_spidermiddleware_offsite.py",
            "tests/test_utils_iterators.py",
            "tests/test_utils_template.py"
        ]
    },
    {
        "repo": "scrapy/scrapy",
        "pull_number": 6239,
        "instance_id": "scrapy__scrapy-6239",
        "issue_numbers": [
            "5932",
            "6178"
        ],
        "base_commit": "ee1189512f652fae72f013c9d4759976b8b69994",
        "patch": "diff --git a/docs/topics/feed-exports.rst b/docs/topics/feed-exports.rst\nindex f64bbac06a0..922b765db7e 100644\n--- a/docs/topics/feed-exports.rst\n+++ b/docs/topics/feed-exports.rst\n@@ -390,7 +390,13 @@ Each plugin is a class that must implement the following methods:\n \n .. method:: close(self)\n \n-    Close the target file object.\n+    Clean up the plugin.\n+\n+    For example, you might want to close a file wrapper that you might have\n+    used to compress data written into the file received in the ``__init__``\n+    method.\n+\n+    .. warning:: Do not close the file from the ``__init__`` method.\n \n To pass a parameter to your plugin, use :ref:`feed options <feed-options>`. You\n can then access those parameters from the ``__init__`` method of your plugin.\ndiff --git a/scrapy/extensions/postprocessing.py b/scrapy/extensions/postprocessing.py\nindex 79e3b1656ea..17969c5b0d9 100644\n--- a/scrapy/extensions/postprocessing.py\n+++ b/scrapy/extensions/postprocessing.py\n@@ -42,7 +42,6 @@ def write(self, data: bytes) -> int:\n \n     def close(self) -> None:\n         self.gzipfile.close()\n-        self.file.close()\n \n \n class Bz2Plugin:\n@@ -69,7 +68,6 @@ def write(self, data: bytes) -> int:\n \n     def close(self) -> None:\n         self.bz2file.close()\n-        self.file.close()\n \n \n class LZMAPlugin:\n@@ -111,7 +109,6 @@ def write(self, data: bytes) -> int:\n \n     def close(self) -> None:\n         self.lzmafile.close()\n-        self.file.close()\n \n \n # io.IOBase is subclassed here, so that exporters can use the PostProcessingManager\n",
        "test_patch": "diff --git a/tests/test_feedexport.py b/tests/test_feedexport.py\nindex 277555608e6..d7560b5ff58 100644\n--- a/tests/test_feedexport.py\n+++ b/tests/test_feedexport.py\n@@ -1731,6 +1731,7 @@ def open(self, spider):\n \n             def store(self, file):\n                 Storage.store_file = file\n+                Storage.file_was_closed = file.closed\n                 file.close()\n \n         settings = {\n@@ -1746,6 +1747,7 @@ def store(self, file):\n         }\n         yield self.exported_no_data(settings)\n         self.assertIs(Storage.open_file, Storage.store_file)\n+        self.assertFalse(Storage.file_was_closed)\n \n \n class FeedPostProcessedExportsTest(FeedExportTestBase):\n",
        "problem_statement": "_store_in_thread must not receive a closed file when using postprocessing plugins\nWhen using post-processing plugins, the FeedExporter closes the temporary file. This behavior has been introduced in commit 500aaa258fc72e943d7bbd258900aeb5af6c8b0d.\r\n\r\nSome storage backends, such as GCSFeedStorage, expect an open file in the _store_in_thread method.\r\n\r\nTo solve the problem we should only close the e.g. GzipFile instance, not the temporary file. Removing the line self.file.close() in GzipPlugin, Bz2Plugin and LZMAPlugin should resolve the error. I think this change would be a clean solution because usually the component that created a resource should be responsible to close that resource - so closing the BinaryIO object in the plugins is a bit unexpected.\r\n\r\nFurthermore, GCSFeedExporter should be modified to close the file in its _store_in_thread method after the upload. FTPFeedStorage and S3FeedStorage do this as well.\nresolve issue 5932\nFixes #5932 \r\n\r\nRemoved the lines closing the temporary file in GzipPlugin, Bz2Plugin, and LMZAPlugin in postprocessing.py.\r\n\r\nPlaced file closes in GCSFeedExporter, FTPFeedStorage, and S3FeedStorage in feedexport.py.\n",
        "hints_text": "I'm interested in working on this issue. It seems like a valuable improvement and aligns with my interests. I hope this issue still open and available for contributions?\nPlease, feel free to give it a try.\n## [Codecov](https://app.codecov.io/gh/scrapy/scrapy/pull/6178?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) Report\n> Merging [#6178](https://app.codecov.io/gh/scrapy/scrapy/pull/6178?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) (329175c) into [master](https://app.codecov.io/gh/scrapy/scrapy/commit/04024f1e796f99e77dda544396722fb9203f1d49?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) (04024f1) will **decrease** coverage by `45.57%`.\n> Report is 21 commits behind head on master.\n> The diff coverage is `0.00%`.\n\n<details><summary>Additional details and impacted files</summary>\n\n\n```diff\n@@             Coverage Diff             @@\n##           master    #6178       +/-   ##\n===========================================\n- Coverage   88.75%   43.18%   -45.57%     \n===========================================\n  Files         160      159        -1     \n  Lines       11414    11581      +167     \n  Branches     1860     1885       +25     \n===========================================\n- Hits        10130     5001     -5129     \n- Misses        976     6188     +5212     \n- Partials      308      392       +84     \n```\n\n| [Files](https://app.codecov.io/gh/scrapy/scrapy/pull/6178?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) | Coverage \u0394 | |\n|---|---|---|\n| [scrapy/extensions/postprocessing.py](https://app.codecov.io/gh/scrapy/scrapy/pull/6178?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy#diff-c2NyYXB5L2V4dGVuc2lvbnMvcG9zdHByb2Nlc3NpbmcucHk=) | `41.26% <\u00f8> (-57.22%)` | :arrow_down: |\n| [scrapy/extensions/feedexport.py](https://app.codecov.io/gh/scrapy/scrapy/pull/6178?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy#diff-c2NyYXB5L2V4dGVuc2lvbnMvZmVlZGV4cG9ydC5weQ==) | `67.93% <0.00%> (-24.74%)` | :arrow_down: |\n\n... and [141 files with indirect coverage changes](https://app.codecov.io/gh/scrapy/scrapy/pull/6178/indirect-changes?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy)\n\n</details>\nIt seems this approach breaks some feedexport scenarios.",
        "created_at": "2024-02-21T10:13:15Z",
        "version": "2.11",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "tests/test_feedexport.py"
        ]
    },
    {
        "repo": "scrapy/scrapy",
        "pull_number": 6229,
        "instance_id": "scrapy__scrapy-6229",
        "issue_numbers": [
            "6228"
        ],
        "base_commit": "c4e4b9b56e7fe10c5e7472b152dd47253a97af5b",
        "patch": "diff --git a/docs/topics/media-pipeline.rst b/docs/topics/media-pipeline.rst\nindex da0587aa465..c96dd0f991b 100644\n--- a/docs/topics/media-pipeline.rst\n+++ b/docs/topics/media-pipeline.rst\n@@ -532,14 +532,14 @@ See here the methods that you can override in your custom Files Pipeline:\n       .. code-block:: python\n \n         from pathlib import PurePosixPath\n-        from urllib.parse import urlparse\n+        from scrapy.utils.httpobj import urlparse_cached\n \n         from scrapy.pipelines.files import FilesPipeline\n \n \n         class MyFilesPipeline(FilesPipeline):\n             def file_path(self, request, response=None, info=None, *, item=None):\n-                return \"files/\" + PurePosixPath(urlparse(request.url).path).name\n+                return \"files/\" + PurePosixPath(urlparse_cached(request).path).name\n \n       Similarly, you can use the ``item`` to determine the file path based on some item \n       property.\n@@ -690,14 +690,14 @@ See here the methods that you can override in your custom Images Pipeline:\n       .. code-block:: python\n \n         from pathlib import PurePosixPath\n-        from urllib.parse import urlparse\n+        from scrapy.utils.httpobj import urlparse_cached\n \n         from scrapy.pipelines.images import ImagesPipeline\n \n \n         class MyImagesPipeline(ImagesPipeline):\n             def file_path(self, request, response=None, info=None, *, item=None):\n-                return \"files/\" + PurePosixPath(urlparse(request.url).path).name\n+                return \"files/\" + PurePosixPath(urlparse_cached(request).path).name\n \n       Similarly, you can use the ``item`` to determine the file path based on some item \n       property.\ndiff --git a/scrapy/core/http2/stream.py b/scrapy/core/http2/stream.py\nindex 39d5921f4ec..0f282d83d38 100644\n--- a/scrapy/core/http2/stream.py\n+++ b/scrapy/core/http2/stream.py\n@@ -2,7 +2,6 @@\n from enum import Enum\n from io import BytesIO\n from typing import TYPE_CHECKING, Dict, List, Optional, Tuple\n-from urllib.parse import urlparse\n \n from h2.errors import ErrorCodes\n from h2.exceptions import H2Error, ProtocolError, StreamClosedError\n@@ -15,6 +14,7 @@\n from scrapy.http import Request\n from scrapy.http.headers import Headers\n from scrapy.responsetypes import responsetypes\n+from scrapy.utils.httpobj import urlparse_cached\n \n if TYPE_CHECKING:\n     from scrapy.core.http2.protocol import H2ClientProtocol\n@@ -185,7 +185,7 @@ def get_response(self) -> Deferred:\n \n     def check_request_url(self) -> bool:\n         # Make sure that we are sending the request to the correct URL\n-        url = urlparse(self._request.url)\n+        url = urlparse_cached(self._request)\n         return (\n             url.netloc == str(self._protocol.metadata[\"uri\"].host, \"utf-8\")\n             or url.netloc == str(self._protocol.metadata[\"uri\"].netloc, \"utf-8\")\n@@ -194,7 +194,7 @@ def check_request_url(self) -> bool:\n         )\n \n     def _get_request_headers(self) -> List[Tuple[str, str]]:\n-        url = urlparse(self._request.url)\n+        url = urlparse_cached(self._request)\n \n         path = url.path\n         if url.query:\ndiff --git a/scrapy/downloadermiddlewares/redirect.py b/scrapy/downloadermiddlewares/redirect.py\nindex 83afdf7d7dc..24089afea88 100644\n--- a/scrapy/downloadermiddlewares/redirect.py\n+++ b/scrapy/downloadermiddlewares/redirect.py\n@@ -2,7 +2,7 @@\n \n import logging\n from typing import TYPE_CHECKING, Any, List, Union, cast\n-from urllib.parse import urljoin, urlparse\n+from urllib.parse import urljoin\n \n from w3lib.url import safe_url_string\n \n@@ -125,7 +125,7 @@ def process_response(\n         assert response.headers[\"Location\"] is not None\n         location = safe_url_string(response.headers[\"Location\"])\n         if response.headers[\"Location\"].startswith(b\"//\"):\n-            request_scheme = urlparse(request.url).scheme\n+            request_scheme = urlparse_cached(request).scheme\n             location = request_scheme + \"://\" + location.lstrip(\"/\")\n \n         redirected_url = urljoin(request.url, location)\n",
        "test_patch": "diff --git a/tests/CrawlerRunner/ip_address.py b/tests/CrawlerRunner/ip_address.py\nindex 23260ab0d10..5bf7512bc7e 100644\n--- a/tests/CrawlerRunner/ip_address.py\n+++ b/tests/CrawlerRunner/ip_address.py\n@@ -9,6 +9,7 @@\n \n from scrapy import Request, Spider\n from scrapy.crawler import CrawlerRunner\n+from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.log import configure_logging\n from tests.mockserver import MockDNSServer, MockServer\n \n@@ -30,7 +31,7 @@ def start_requests(self):\n         yield Request(self.url)\n \n     def parse(self, response):\n-        netloc = urlparse(response.url).netloc\n+        netloc = urlparse_cached(response).netloc\n         host = netloc.split(\":\")[0]\n         self.logger.info(f\"Host: {host}\")\n         self.logger.info(f\"Type: {type(response.ip_address)}\")\ndiff --git a/tests/test_http_cookies.py b/tests/test_http_cookies.py\nindex 9e43b72b056..8b555491496 100644\n--- a/tests/test_http_cookies.py\n+++ b/tests/test_http_cookies.py\n@@ -1,8 +1,8 @@\n from unittest import TestCase\n-from urllib.parse import urlparse\n \n from scrapy.http import Request, Response\n from scrapy.http.cookies import WrappedRequest, WrappedResponse\n+from scrapy.utils.httpobj import urlparse_cached\n \n \n class WrappedRequestTest(TestCase):\n@@ -17,12 +17,12 @@ def test_get_full_url(self):\n         self.assertEqual(self.wrapped.full_url, self.request.url)\n \n     def test_get_host(self):\n-        self.assertEqual(self.wrapped.get_host(), urlparse(self.request.url).netloc)\n-        self.assertEqual(self.wrapped.host, urlparse(self.request.url).netloc)\n+        self.assertEqual(self.wrapped.get_host(), urlparse_cached(self.request).netloc)\n+        self.assertEqual(self.wrapped.host, urlparse_cached(self.request).netloc)\n \n     def test_get_type(self):\n-        self.assertEqual(self.wrapped.get_type(), urlparse(self.request.url).scheme)\n-        self.assertEqual(self.wrapped.type, urlparse(self.request.url).scheme)\n+        self.assertEqual(self.wrapped.get_type(), urlparse_cached(self.request).scheme)\n+        self.assertEqual(self.wrapped.type, urlparse_cached(self.request).scheme)\n \n     def test_is_unverifiable(self):\n         self.assertFalse(self.wrapped.is_unverifiable())\ndiff --git a/tests/test_http_request.py b/tests/test_http_request.py\nindex 6dc9ec8b7fb..04fcaa2315d 100644\n--- a/tests/test_http_request.py\n+++ b/tests/test_http_request.py\n@@ -5,7 +5,7 @@\n import xmlrpc.client\n from typing import Any, Dict, List\n from unittest import mock\n-from urllib.parse import parse_qs, unquote_to_bytes, urlparse\n+from urllib.parse import parse_qs, unquote_to_bytes\n \n from scrapy.http import (\n     FormRequest,\n@@ -16,6 +16,7 @@\n     XmlRpcRequest,\n )\n from scrapy.http.request import NO_CALLBACK\n+from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.python import to_bytes, to_unicode\n \n \n@@ -617,8 +618,8 @@ def test_from_response_duplicate_form_key(self):\n             method=\"GET\",\n             formdata=((\"foo\", \"bar\"), (\"foo\", \"baz\")),\n         )\n-        self.assertEqual(urlparse(req.url).hostname, \"www.example.com\")\n-        self.assertEqual(urlparse(req.url).query, \"foo=bar&foo=baz\")\n+        self.assertEqual(urlparse_cached(req).hostname, \"www.example.com\")\n+        self.assertEqual(urlparse_cached(req).query, \"foo=bar&foo=baz\")\n \n     def test_from_response_override_duplicate_form_key(self):\n         response = _buildresponse(\n@@ -666,8 +667,8 @@ def test_from_response_get(self):\n             response, formdata={\"one\": [\"two\", \"three\"], \"six\": \"seven\"}\n         )\n         self.assertEqual(r1.method, \"GET\")\n-        self.assertEqual(urlparse(r1.url).hostname, \"www.example.com\")\n-        self.assertEqual(urlparse(r1.url).path, \"/this/get.php\")\n+        self.assertEqual(urlparse_cached(r1).hostname, \"www.example.com\")\n+        self.assertEqual(urlparse_cached(r1).path, \"/this/get.php\")\n         fs = _qs(r1)\n         self.assertEqual(set(fs[b\"test\"]), {b\"val1\", b\"val2\"})\n         self.assertEqual(set(fs[b\"one\"]), {b\"two\", b\"three\"})\ndiff --git a/tests/test_scheduler_base.py b/tests/test_scheduler_base.py\nindex 76ca777a87a..5db2e4e509b 100644\n--- a/tests/test_scheduler_base.py\n+++ b/tests/test_scheduler_base.py\n@@ -1,6 +1,6 @@\n from typing import Dict, Optional\n from unittest import TestCase\n-from urllib.parse import urljoin, urlparse\n+from urllib.parse import urljoin\n \n from testfixtures import LogCapture\n from twisted.internet import defer\n@@ -9,6 +9,7 @@\n from scrapy.core.scheduler import BaseScheduler\n from scrapy.http import Request\n from scrapy.spiders import Spider\n+from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.request import fingerprint\n from scrapy.utils.test import get_crawler\n from tests.mockserver import MockServer\n@@ -57,7 +58,7 @@ def __init__(self, mockserver, *args, **kwargs):\n         self.start_urls = map(mockserver.url, PATHS)\n \n     def parse(self, response):\n-        return {\"path\": urlparse(response.url).path}\n+        return {\"path\": urlparse_cached(response).path}\n \n \n class InterfaceCheckMixin:\n",
        "problem_statement": "Replace urlparse with urlparse_cached where possible\nLook for regular expression `urllib.*urlparse` in the code base (docs included), and see if replacing the use of `urllib.parse.urlparse` with `scrapy.utils.httpobj.urlparse_cached` is feasible (I think it should as long as there is a `request` object involved).\n",
        "hints_text": "",
        "created_at": "2024-02-20T10:13:05Z",
        "version": "2.11",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "tests/CrawlerRunner/ip_address.py",
            "tests/test_http_cookies.py",
            "tests/test_http_request.py",
            "tests/test_scheduler_base.py"
        ]
    },
    {
        "repo": "scrapy/scrapy",
        "pull_number": 6151,
        "instance_id": "scrapy__scrapy-6151",
        "issue_numbers": [
            "3690",
            "3691"
        ],
        "base_commit": "4d31277bc67169460dc2d8bca80946df8b355b8f",
        "patch": "diff --git a/docs/topics/downloader-middleware.rst b/docs/topics/downloader-middleware.rst\nindex 11a3fcb94f4..5acc6daa999 100644\n--- a/docs/topics/downloader-middleware.rst\n+++ b/docs/topics/downloader-middleware.rst\n@@ -797,9 +797,12 @@ OffsiteMiddleware\n    :attr:`~scrapy.Spider.allowed_domains` attribute, or the\n    attribute is empty, the offsite middleware will allow all requests.\n \n-   If the request has the :attr:`~scrapy.Request.dont_filter` attribute\n-   set, the offsite middleware will allow the request even if its domain is not\n-   listed in allowed domains.\n+   .. reqmeta:: allow_offsite\n+\n+   If the request has the :attr:`~scrapy.Request.dont_filter` attribute set to\n+   ``True`` or :attr:`Request.meta` has ``allow_offsite`` set to ``True``, then\n+   the OffsiteMiddleware will allow the request even if its domain is not listed\n+   in allowed domains.\n \n RedirectMiddleware\n ------------------\ndiff --git a/docs/topics/request-response.rst b/docs/topics/request-response.rst\nindex 710e2e1314e..056e2ed383a 100644\n--- a/docs/topics/request-response.rst\n+++ b/docs/topics/request-response.rst\n@@ -145,9 +145,9 @@ Request objects\n     :type priority: int\n \n     :param dont_filter: indicates that this request should not be filtered by\n-       the scheduler. This is used when you want to perform an identical\n-       request multiple times, to ignore the duplicates filter. Use it with\n-       care, or you will get into crawling loops. Default to ``False``.\n+       the scheduler or some middlewares. This is used when you want to perform\n+       an identical request multiple times, to ignore the duplicates filter.\n+       Use it with care, or you will get into crawling loops. Default to ``False``.\n     :type dont_filter: bool\n \n     :param errback: a function that will be called if any exception was\n@@ -661,6 +661,7 @@ are some special keys recognized by Scrapy and its built-in extensions.\n \n Those are:\n \n+* :reqmeta:`allow_offsite`\n * :reqmeta:`autothrottle_dont_adjust_delay`\n * :reqmeta:`bindaddress`\n * :reqmeta:`cookiejar`\ndiff --git a/scrapy/downloadermiddlewares/offsite.py b/scrapy/downloadermiddlewares/offsite.py\nindex a69f531a75a..a2cff65e7ef 100644\n--- a/scrapy/downloadermiddlewares/offsite.py\n+++ b/scrapy/downloadermiddlewares/offsite.py\n@@ -40,7 +40,11 @@ def request_scheduled(self, request: Request, spider: Spider) -> None:\n         self.process_request(request, spider)\n \n     def process_request(self, request: Request, spider: Spider) -> None:\n-        if request.dont_filter or self.should_follow(request, spider):\n+        if (\n+            request.dont_filter\n+            or request.meta.get(\"allow_offsite\")\n+            or self.should_follow(request, spider)\n+        ):\n             return\n         domain = urlparse_cached(request).hostname\n         if domain and domain not in self.domains_seen:\ndiff --git a/scrapy/spidermiddlewares/offsite.py b/scrapy/spidermiddlewares/offsite.py\nindex d3ed64ef546..95e753830be 100644\n--- a/scrapy/spidermiddlewares/offsite.py\n+++ b/scrapy/spidermiddlewares/offsite.py\n@@ -61,7 +61,11 @@ async def process_spider_output_async(\n     def _filter(self, request: Any, spider: Spider) -> bool:\n         if not isinstance(request, Request):\n             return True\n-        if request.dont_filter or self.should_follow(request, spider):\n+        if (\n+            request.dont_filter\n+            or request.meta.get(\"allow_offsite\")\n+            or self.should_follow(request, spider)\n+        ):\n             return True\n         domain = urlparse_cached(request).hostname\n         if domain and domain not in self.domains_seen:\n",
        "test_patch": "diff --git a/tests/test_downloadermiddleware_offsite.py b/tests/test_downloadermiddleware_offsite.py\nindex fec56a39f23..23a1d06dac0 100644\n--- a/tests/test_downloadermiddleware_offsite.py\n+++ b/tests/test_downloadermiddleware_offsite.py\n@@ -64,6 +64,37 @@ def test_process_request_dont_filter(value, filtered):\n         assert mw.process_request(request, spider) is None\n \n \n+@pytest.mark.parametrize(\n+    (\"allow_offsite\", \"dont_filter\", \"filtered\"),\n+    (\n+        (True, UNSET, False),\n+        (True, None, False),\n+        (True, False, False),\n+        (True, True, False),\n+        (False, UNSET, True),\n+        (False, None, True),\n+        (False, False, True),\n+        (False, True, False),\n+    ),\n+)\n+def test_process_request_allow_offsite(allow_offsite, dont_filter, filtered):\n+    crawler = get_crawler(Spider)\n+    spider = crawler._create_spider(name=\"a\", allowed_domains=[\"a.example\"])\n+    mw = OffsiteMiddleware.from_crawler(crawler)\n+    mw.spider_opened(spider)\n+    kwargs = {\"meta\": {}}\n+    if allow_offsite is not UNSET:\n+        kwargs[\"meta\"][\"allow_offsite\"] = allow_offsite\n+    if dont_filter is not UNSET:\n+        kwargs[\"dont_filter\"] = dont_filter\n+    request = Request(\"https://b.example\", **kwargs)\n+    if filtered:\n+        with pytest.raises(IgnoreRequest):\n+            mw.process_request(request, spider)\n+    else:\n+        assert mw.process_request(request, spider) is None\n+\n+\n @pytest.mark.parametrize(\n     \"value\",\n     (\ndiff --git a/tests/test_spidermiddleware_offsite.py b/tests/test_spidermiddleware_offsite.py\nindex 837f1c2c8f5..906928e0126 100644\n--- a/tests/test_spidermiddleware_offsite.py\n+++ b/tests/test_spidermiddleware_offsite.py\n@@ -29,6 +29,7 @@ def test_process_spider_output(self):\n             Request(\"http://scrapy.org/1\"),\n             Request(\"http://sub.scrapy.org/1\"),\n             Request(\"http://offsite.tld/letmepass\", dont_filter=True),\n+            Request(\"http://offsite-2.tld/allow\", meta={\"allow_offsite\": True}),\n             Request(\"http://scrapy.test.org/\"),\n             Request(\"http://scrapy.test.org:8000/\"),\n         ]\n",
        "problem_statement": "Separate Attribute to allow offsite requests\nRight now Request.dont_filter attribute is used for two intentions. To allow duplicated requests and to allow offsite requests. This behavior is not desirable since, you could want to allow a duplicate request but still want to filter out offsite requests and vice versa.\nAttribute to control offsite filtering\nThis is a fix for #3690\n",
        "hints_text": "\n# [Codecov](https://codecov.io/gh/scrapy/scrapy/pull/3691?src=pr&el=h1) Report\n> Merging [#3691](https://codecov.io/gh/scrapy/scrapy/pull/3691?src=pr&el=desc) into [master](https://codecov.io/gh/scrapy/scrapy/commit/d346b8cb0f7e206f9a878cfadcd62ef981e85310?src=pr&el=desc) will **increase** coverage by `0.17%`.\n> The diff coverage is `100%`.\n\n\n```diff\n@@            Coverage Diff             @@\n##           master    #3691      +/-   ##\n==========================================\n+ Coverage   84.54%   84.71%   +0.17%     \n==========================================\n  Files         167      168       +1     \n  Lines        9420     9460      +40     \n  Branches     1402     1407       +5     \n==========================================\n+ Hits         7964     8014      +50     \n+ Misses       1199     1188      -11     \n- Partials      257      258       +1\n```\n\n| [Impacted Files](https://codecov.io/gh/scrapy/scrapy/pull/3691?src=pr&el=tree) | Coverage \u0394 | |\n|---|---|---|\n| [scrapy/spidermiddlewares/offsite.py](https://codecov.io/gh/scrapy/scrapy/pull/3691/diff?src=pr&el=tree#diff-c2NyYXB5L3NwaWRlcm1pZGRsZXdhcmVzL29mZnNpdGUucHk=) | `100% <100%> (\u00f8)` | :arrow_up: |\n| [scrapy/utils/trackref.py](https://codecov.io/gh/scrapy/scrapy/pull/3691/diff?src=pr&el=tree#diff-c2NyYXB5L3V0aWxzL3RyYWNrcmVmLnB5) | `83.78% <0%> (-2.71%)` | :arrow_down: |\n| [scrapy/http/\\_\\_init\\_\\_.py](https://codecov.io/gh/scrapy/scrapy/pull/3691/diff?src=pr&el=tree#diff-c2NyYXB5L2h0dHAvX19pbml0X18ucHk=) | `100% <0%> (\u00f8)` | :arrow_up: |\n| [scrapy/http/request/json\\_request.py](https://codecov.io/gh/scrapy/scrapy/pull/3691/diff?src=pr&el=tree#diff-c2NyYXB5L2h0dHAvcmVxdWVzdC9qc29uX3JlcXVlc3QucHk=) | `93.75% <0%> (\u00f8)` | |\n| [scrapy/settings/default\\_settings.py](https://codecov.io/gh/scrapy/scrapy/pull/3691/diff?src=pr&el=tree#diff-c2NyYXB5L3NldHRpbmdzL2RlZmF1bHRfc2V0dGluZ3MucHk=) | `98.64% <0%> (\u00f8)` | :arrow_up: |\n| [scrapy/item.py](https://codecov.io/gh/scrapy/scrapy/pull/3691/diff?src=pr&el=tree#diff-c2NyYXB5L2l0ZW0ucHk=) | `98.48% <0%> (+0.07%)` | :arrow_up: |\n| [scrapy/extensions/feedexport.py](https://codecov.io/gh/scrapy/scrapy/pull/3691/diff?src=pr&el=tree#diff-c2NyYXB5L2V4dGVuc2lvbnMvZmVlZGV4cG9ydC5weQ==) | `84.9% <0%> (+6.43%)` | :arrow_up: |\n\nLGTM",
        "created_at": "2023-11-22T05:32:53Z",
        "version": "2.12",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "tests/test_downloadermiddleware_offsite.py",
            "tests/test_spidermiddleware_offsite.py"
        ]
    },
    {
        "repo": "scrapy/scrapy",
        "pull_number": 6064,
        "instance_id": "scrapy__scrapy-6064",
        "issue_numbers": [
            "6024",
            "6030"
        ],
        "base_commit": "c65567988da2f6dd8ad894cf0cf57f2c074be10f",
        "patch": "diff --git a/scrapy/crawler.py b/scrapy/crawler.py\nindex 22fd65be7af..6f54e62e990 100644\n--- a/scrapy/crawler.py\n+++ b/scrapy/crawler.py\n@@ -404,8 +404,8 @@ def start(\n         :param bool stop_after_crawl: stop or not the reactor when all\n             crawlers have finished\n \n-        :param bool install_signal_handlers: whether to install the shutdown\n-            handlers (default: True)\n+        :param bool install_signal_handlers: whether to install the OS signal\n+            handlers from Twisted and Scrapy (default: True)\n         \"\"\"\n         from twisted.internet import reactor\n \n@@ -416,15 +416,17 @@ def start(\n                 return\n             d.addBoth(self._stop_reactor)\n \n-        if install_signal_handlers:\n-            install_shutdown_handlers(self._signal_shutdown)\n         resolver_class = load_object(self.settings[\"DNS_RESOLVER\"])\n         resolver = create_instance(resolver_class, self.settings, self, reactor=reactor)\n         resolver.install_on_reactor()\n         tp = reactor.getThreadPool()\n         tp.adjustPoolsize(maxthreads=self.settings.getint(\"REACTOR_THREADPOOL_MAXSIZE\"))\n         reactor.addSystemEventTrigger(\"before\", \"shutdown\", self.stop)\n-        reactor.run(installSignalHandlers=False)  # blocking call\n+        if install_signal_handlers:\n+            reactor.addSystemEventTrigger(\n+                \"after\", \"startup\", install_shutdown_handlers, self._signal_shutdown\n+            )\n+        reactor.run(installSignalHandlers=install_signal_handlers)  # blocking call\n \n     def _graceful_stop_reactor(self) -> Deferred:\n         d = self.stop()\ndiff --git a/scrapy/utils/ossignal.py b/scrapy/utils/ossignal.py\nindex 2334ea79242..db9a7127372 100644\n--- a/scrapy/utils/ossignal.py\n+++ b/scrapy/utils/ossignal.py\n@@ -19,13 +19,10 @@ def install_shutdown_handlers(\n     function: SignalHandlerT, override_sigint: bool = True\n ) -> None:\n     \"\"\"Install the given function as a signal handler for all common shutdown\n-    signals (such as SIGINT, SIGTERM, etc). If override_sigint is ``False`` the\n-    SIGINT handler won't be install if there is already a handler in place\n-    (e.g.  Pdb)\n+    signals (such as SIGINT, SIGTERM, etc). If ``override_sigint`` is ``False`` the\n+    SIGINT handler won't be installed if there is already a handler in place\n+    (e.g. Pdb)\n     \"\"\"\n-    from twisted.internet import reactor\n-\n-    reactor._handleSignals()\n     signal.signal(signal.SIGTERM, function)\n     if signal.getsignal(signal.SIGINT) == signal.default_int_handler or override_sigint:\n         signal.signal(signal.SIGINT, function)\ndiff --git a/setup.py b/setup.py\nindex 47c0af0b045..405633f5552 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -6,8 +6,7 @@\n \n \n install_requires = [\n-    # 23.8.0 incompatibility: https://github.com/scrapy/scrapy/issues/6024\n-    \"Twisted>=18.9.0,<23.8.0\",\n+    \"Twisted>=18.9.0\",\n     \"cryptography>=36.0.0\",\n     \"cssselect>=0.9.1\",\n     \"itemloaders>=1.0.1\",\n",
        "test_patch": "diff --git a/scrapy/utils/testproc.py b/scrapy/utils/testproc.py\nindex 5f7a7db14b2..0688e014be0 100644\n--- a/scrapy/utils/testproc.py\n+++ b/scrapy/utils/testproc.py\n@@ -2,7 +2,7 @@\n \n import os\n import sys\n-from typing import Iterable, Optional, Tuple, cast\n+from typing import Iterable, List, Optional, Tuple, cast\n \n from twisted.internet.defer import Deferred\n from twisted.internet.error import ProcessTerminated\n@@ -26,14 +26,15 @@ def execute(\n         env = os.environ.copy()\n         if settings is not None:\n             env[\"SCRAPY_SETTINGS_MODULE\"] = settings\n+        assert self.command\n         cmd = self.prefix + [self.command] + list(args)\n         pp = TestProcessProtocol()\n-        pp.deferred.addBoth(self._process_finished, cmd, check_code)\n+        pp.deferred.addCallback(self._process_finished, cmd, check_code)\n         reactor.spawnProcess(pp, cmd[0], cmd, env=env, path=self.cwd)\n         return pp.deferred\n \n     def _process_finished(\n-        self, pp: TestProcessProtocol, cmd: str, check_code: bool\n+        self, pp: TestProcessProtocol, cmd: List[str], check_code: bool\n     ) -> Tuple[int, bytes, bytes]:\n         if pp.exitcode and check_code:\n             msg = f\"process {cmd} exit with code {pp.exitcode}\"\ndiff --git a/tests/CrawlerProcess/sleeping.py b/tests/CrawlerProcess/sleeping.py\nnew file mode 100644\nindex 00000000000..420d9d328ff\n--- /dev/null\n+++ b/tests/CrawlerProcess/sleeping.py\n@@ -0,0 +1,24 @@\n+from twisted.internet.defer import Deferred\n+\n+import scrapy\n+from scrapy.crawler import CrawlerProcess\n+from scrapy.utils.defer import maybe_deferred_to_future\n+\n+\n+class SleepingSpider(scrapy.Spider):\n+    name = \"sleeping\"\n+\n+    start_urls = [\"data:,;\"]\n+\n+    async def parse(self, response):\n+        from twisted.internet import reactor\n+\n+        d = Deferred()\n+        reactor.callLater(3, d.callback, None)\n+        await maybe_deferred_to_future(d)\n+\n+\n+process = CrawlerProcess(settings={})\n+\n+process.crawl(SleepingSpider)\n+process.start()\ndiff --git a/tests/requirements.txt b/tests/requirements.txt\nindex c07fda2d688..d4bfead40cf 100644\n--- a/tests/requirements.txt\n+++ b/tests/requirements.txt\n@@ -1,5 +1,6 @@\n # Tests requirements\n attrs\n+pexpect >= 4.8.0\n pyftpdlib >= 1.5.8\n pytest\n pytest-cov==4.0.0\ndiff --git a/tests/test_command_shell.py b/tests/test_command_shell.py\nindex 6589381f3b8..7d87eb62c4a 100644\n--- a/tests/test_command_shell.py\n+++ b/tests/test_command_shell.py\n@@ -1,11 +1,15 @@\n+import sys\n+from io import BytesIO\n from pathlib import Path\n \n+from pexpect.popen_spawn import PopenSpawn\n from twisted.internet import defer\n from twisted.trial import unittest\n \n from scrapy.utils.testproc import ProcessTest\n from scrapy.utils.testsite import SiteTest\n from tests import NON_EXISTING_RESOLVABLE, tests_datadir\n+from tests.mockserver import MockServer\n \n \n class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n@@ -133,3 +137,25 @@ def test_shell_fetch_async(self):\n         args = [\"-c\", code, \"--set\", f\"TWISTED_REACTOR={reactor_path}\"]\n         _, _, err = yield self.execute(args, check_code=True)\n         self.assertNotIn(b\"RuntimeError: There is no current event loop in thread\", err)\n+\n+\n+class InteractiveShellTest(unittest.TestCase):\n+    def test_fetch(self):\n+        args = (\n+            sys.executable,\n+            \"-m\",\n+            \"scrapy.cmdline\",\n+            \"shell\",\n+        )\n+        logfile = BytesIO()\n+        p = PopenSpawn(args, timeout=5)\n+        p.logfile_read = logfile\n+        p.expect_exact(\"Available Scrapy objects\")\n+        with MockServer() as mockserver:\n+            p.sendline(f\"fetch('{mockserver.url('/')}')\")\n+            p.sendline(\"type(response)\")\n+            p.expect_exact(\"HtmlResponse\")\n+        p.sendeof()\n+        p.wait()\n+        logfile.seek(0)\n+        self.assertNotIn(\"Traceback\", logfile.read().decode())\ndiff --git a/tests/test_crawler.py b/tests/test_crawler.py\nindex 2b141e89454..60b92377dd6 100644\n--- a/tests/test_crawler.py\n+++ b/tests/test_crawler.py\n@@ -1,13 +1,16 @@\n import logging\n import os\n import platform\n+import signal\n import subprocess\n import sys\n import warnings\n from pathlib import Path\n+from typing import List\n \n import pytest\n from packaging.version import parse as parse_version\n+from pexpect.popen_spawn import PopenSpawn\n from pytest import mark, raises\n from twisted.internet import defer\n from twisted.trial import unittest\n@@ -289,9 +292,12 @@ class ScriptRunnerMixin:\n     script_dir: Path\n     cwd = os.getcwd()\n \n-    def run_script(self, script_name: str, *script_args):\n+    def get_script_args(self, script_name: str, *script_args: str) -> List[str]:\n         script_path = self.script_dir / script_name\n-        args = [sys.executable, str(script_path)] + list(script_args)\n+        return [sys.executable, str(script_path)] + list(script_args)\n+\n+    def run_script(self, script_name: str, *script_args: str) -> str:\n+        args = self.get_script_args(script_name, *script_args)\n         p = subprocess.Popen(\n             args,\n             env=get_mockserver_env(),\n@@ -517,6 +523,29 @@ def test_args_change_settings(self):\n         self.assertIn(\"Spider closed (finished)\", log)\n         self.assertIn(\"The value of FOO is 42\", log)\n \n+    def test_shutdown_graceful(self):\n+        sig = signal.SIGINT if sys.platform != \"win32\" else signal.SIGBREAK\n+        args = self.get_script_args(\"sleeping.py\")\n+        p = PopenSpawn(args, timeout=5)\n+        p.expect_exact(\"Spider opened\")\n+        p.expect_exact(\"Crawled (200)\")\n+        p.kill(sig)\n+        p.expect_exact(\"shutting down gracefully\")\n+        p.expect_exact(\"Spider closed (shutdown)\")\n+        p.wait()\n+\n+    def test_shutdown_forced(self):\n+        sig = signal.SIGINT if sys.platform != \"win32\" else signal.SIGBREAK\n+        args = self.get_script_args(\"sleeping.py\")\n+        p = PopenSpawn(args, timeout=5)\n+        p.expect_exact(\"Spider opened\")\n+        p.expect_exact(\"Crawled (200)\")\n+        p.kill(sig)\n+        p.expect_exact(\"shutting down gracefully\")\n+        p.kill(sig)\n+        p.expect_exact(\"forcing unclean shutdown\")\n+        p.wait()\n+\n \n class CrawlerRunnerSubprocess(ScriptRunnerMixin, unittest.TestCase):\n     script_dir = Path(__file__).parent.resolve() / \"CrawlerRunner\"\n",
        "problem_statement": "Problem with Twisted AttributeError: 'EPollReactor' object has no attribute '_handleSignals'\n### Description\r\n\r\nSince the release of Twisted 23.8.0 recently, I've had problems running Scrapy. I had to fix to the previous version, Twisted==22.10.0\r\n\r\n### Steps to Reproduce\r\n\r\n1. Install Scrapy in a new environment\r\n2. Run .start() from an CrawlerProcess object\r\n\r\n**Expected behavior:** Scrapy to crawl\r\n\r\n**Actual behavior:** \r\n![image](https://github.com/scrapy/scrapy/assets/14026017/4d769673-52ee-4039-887f-c810017d48e2)\r\n\r\n**Reproduces how often:** Always in the new version\r\n\r\n### Versions\r\n\r\nScrapy>=2.9.0\r\n\r\n### Additional context\r\n\r\nI've seen this fix on [stackoverflow](https://stackoverflow.com/questions/76995567/error-when-crawl-data-epollreactor-object-has-no-attribute-handlesignals)\r\n\n[wip] reactor handlesignals\nFix to https://github.com/scrapy/scrapy/issues/6024\r\n\r\nFrom my observations of scrapy code and related twisted pull request I conclude that call of `reactor._handleSignals()` or it's equivalent from new twisted version - are not needed inside `install_shutdown_handlers` to run scrapy.\r\n\r\nAt least on my local environment with both new/old twisted versions I didn't noticed anything unexpected.\n",
        "hints_text": "I'm having the same issue, but if you revert back to `pip install Twisted==22.10.0 ` it's working.\nI assume that changes from twisted side that caused this issue came from this issue https://github.com/twisted/twisted/issues/11751 and this pull request https://github.com/twisted/twisted/pull/11752\nThis affects both `scrapy crawl` and direct usage of `CrawlerProcess`. \nLooks like we are calling `reactor._handleSignals()` in `scrapy.utils.ossignal.install_shutdown_handlers()` so that we can override the Twisted handlers. Twisted itself calls `reactor._handleSignals()` in `_SignalReactorMixin._reallyStartRunning()`, I assume this happens later.\r\n\r\nIt seems that the equivalent code in new Twisted is in `twisted.internet._signals._WithSignalHandling.install()` but this is even more private and even more tied to internal logic so I hesitate to use it. It may be possible to redo our logic instead.\r\n\r\nUnfortunately, this isn't covered by tests and deleting `reactor._handleSignals()` doesn't fail them :)\r\n\r\nOverall, this looks like releasing a version with the Twisted version restriction is much more viable than quickly fixing this.\nAfter looking at the code for some more I think we need to check the order of handler installation in Scrapy with older Twisted in order to understand how it works. For example, I'm not sure how do other handlers work if Twisted resets them on reactor.run() (as the comments in the new logic suggest).\nJust popping my head in here to say that this might be something to discuss on the Twisted mailing list, if there are requirements that Scrapy has for signal handling that we've missed!\r\n\r\nhttps://mail.python.org/mailman3/lists/twisted.python.org/\nPrepared pull request with removed failing call of `_handleSignals` - with this change scrapy works on my local environment as expected (both earlier 22.10.0 and new 23.8.0 version of twisted checked locally). At least it looks like expected from my first view.\r\n\r\nAlso it is noteable fact that the most of scrapy code parts related to handling these signals([/scrapy/utils/ossignal.py](https://github.com/scrapy/scrapy/blob/2.10.0/scrapy/utils/ossignal.py), [/scrapy/crawler.py](https://github.com/scrapy/scrapy/blob/2.10.0/scrapy/crawler.py#L337-L356)) looks nearly the same as 10+ years ago when It appeared. \r\n\r\nSo I assume that this call of  `_handleSignals` was needed for.. much earlier versions of twisted however I am not aware how twisted looked like 10+ years ago (I am not aware of deep internals of recent versions of twisted as well)\nWe just released 2.10.1 with the Twisted version restricted as a workaround for this.\n## [Codecov](https://app.codecov.io/gh/scrapy/scrapy/pull/6030?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) Report\n> Merging [#6030](https://app.codecov.io/gh/scrapy/scrapy/pull/6030?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) (8c99f44) into [master](https://app.codecov.io/gh/scrapy/scrapy/commit/0e7fc60b89b0ba5cdf72709ff2e70bfe6ccecbdc?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) (0e7fc60) will **decrease** coverage by `0.01%`.\n> The diff coverage is `n/a`.\n\n> :exclamation: Current head 8c99f44 differs from pull request most recent head 47bcf1a. Consider uploading reports for the commit 47bcf1a to get more accurate results\n\n```diff\n@@            Coverage Diff             @@\n##           master    #6030      +/-   ##\n==========================================\n- Coverage   88.92%   88.92%   -0.01%     \n==========================================\n  Files         162      162              \n  Lines       11445    11443       -2     \n  Branches     1861     1861              \n==========================================\n- Hits        10178    10176       -2     \n  Misses        962      962              \n  Partials      305      305              \n```\n\n| [Files Changed](https://app.codecov.io/gh/scrapy/scrapy/pull/6030?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) | Coverage \u0394 | |\n|---|---|---|\n| [scrapy/utils/ossignal.py](https://app.codecov.io/gh/scrapy/scrapy/pull/6030?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy#diff-c2NyYXB5L3V0aWxzL29zc2lnbmFsLnB5) | `75.00% <\u00f8> (-2.78%)` | :arrow_down: |\n\nI think Twisted sets handlers for INT and BREAK conditionally but for TERM unconditionally, so I would expect our TERM handler to be overridden in at least some cases if this line is removed.\nAfter thinking about it for some more I think it makes sense that calling or not calling this doesn't make a difference, whatever the call order. But can you please check if with old Twisted a simple `scrapy crawl` has the TERM handler last set by Scrapy or by Twisted?\nIn `CrawlerProcess.start`.  `reactor.run` - called with `installSignalHandlers=False` which means that reactor will not apply it's default handlers. And as far as I understand in this case handlers from scrapy `install_shutdown_handlers` will be the only applied signal handlers\r\nhttps://github.com/scrapy/scrapy/blob/a320e5f6a421ea3bae06d2f63d29bae9d327f580/scrapy/crawler.py#L389-L397\r\n\r\nWhile if we will run scrapy with `CrawlerRunner` with simply `reactor.run()` as documented on https://docs.scrapy.org/en/latest/topics/practices.html#running-multiple-spiders-in-the-same-process - reactor will run with reactor default signal handlers (and no scrapy `install_shutdown_handlers` as it called in `CrawlerProcess` only) \n> reactor.run - called with installSignalHandlers=False\r\n\r\nOh!\r\n\r\nThen yeah, it probably doesn't make sense. OTOH `twisted.internet.posixbase.PosixReactorBase._handleSignals()` also installs a SIGCHLD handler, but I don't know how useful is it for Scrapy.\nAfter discussing this with @kmike we want this:\r\n\r\n1. An integration test that makes sure that spawning a spider process and sending it a Ctrl-C (or a similar signal) once leads to a graceful shutdown while sending it twice leads to an immediate exit. We may already have such test, I haven't checked.\r\n2. Making sure that spawning a process with `subprocess` and with `reactor.spawnProcess()` works on a POSIX OS. It will also be interesting to check if this process becomes a zombie or is reaped correctly, both with old and new code. It will also be interesting to check `twisted.internet.utils.getProcessValue()` as it may depend on SIGCHLD handling.\r\n3. Making sure that the new code works on Scrapy Cloud as expected.\n> We may already have such test, I haven't checked.\r\n\r\nI don\u2019t think we do. It may be useful for https://github.com/scrapy/scrapy/issues/4749 as well.\nFor (2): \r\n\r\n- `subprocess.run()` doesn't leave a zombie\r\n- `getProcessValue()` returns the exit code and doesn't leave a zombie\r\n- `reactor.spawnProcess` leaves a zombie and doesn't fire the protocol's `processEnded` handler\r\n\r\nThe last point is an important difference, I don't want to break this. So we want to either call the SIGCHLD-related Twisted code directly in `install_shutdown_handlers`, or remove `installSignalHandlers=False` and try to work with that.\r\n\r\nFor the first option it's again calling some private code which I don't want to do. For the second option it means Twisted will overwrite our SIGTERM and SIGBREAK handlers but leave our SIGINT handler. We can try finding a later point at which to install our handlers.\nhttps://stackoverflow.com/questions/74929947/twisted-application-ignoring-a-certain-unix-signal-is-it-possible suggests `reactor.addSystemEventTrigger('after', 'startup', install_your_handlers)` ",
        "created_at": "2023-09-24T14:17:06Z",
        "version": "2.11",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "scrapy/utils/testproc.py",
            "tests/requirements.txt",
            "tests/test_command_shell.py",
            "tests/test_crawler.py"
        ]
    },
    {
        "repo": "scrapy/scrapy",
        "pull_number": 5508,
        "instance_id": "scrapy__scrapy-5508",
        "issue_numbers": [
            "5504",
            "5504"
        ],
        "base_commit": "078622cfb0ee364acba5d91a20244f9c1ee87d30",
        "patch": "diff --git a/docs/topics/media-pipeline.rst b/docs/topics/media-pipeline.rst\nindex 7dff78390f5..2513faae268 100644\n--- a/docs/topics/media-pipeline.rst\n+++ b/docs/topics/media-pipeline.rst\n@@ -656,6 +656,26 @@ See here the methods that you can override in your custom Images Pipeline:\n       .. versionadded:: 2.4\n          The *item* parameter.\n \n+   .. method:: ImagesPipeline.thumb_path(self, request, thumb_id, response=None, info=None, *, item=None)\n+\n+      This method is called for every item of  :setting:`IMAGES_THUMBS` per downloaded item. It returns the\n+      thumbnail download path of the image originating from the specified\n+      :class:`response <scrapy.http.Response>`.\n+\n+      In addition to ``response``, this method receives the original\n+      :class:`request <scrapy.Request>`,\n+      ``thumb_id``,\n+      :class:`info <scrapy.pipelines.media.MediaPipeline.SpiderInfo>` and\n+      :class:`item <scrapy.Item>`.\n+\n+      You can override this method to customize the thumbnail download path of each image.\n+      You can use the ``item`` to determine the file path based on some item\n+      property.\n+\n+      By default the :meth:`thumb_path` method returns\n+      ``thumbs/<size name>/<request URL hash>.<extension>``.\n+\n+\n    .. method:: ImagesPipeline.get_media_requests(item, info)\n \n       Works the same way as :meth:`FilesPipeline.get_media_requests` method,\ndiff --git a/scrapy/pipelines/images.py b/scrapy/pipelines/images.py\nindex 9c99dc69ee8..6b97190ee58 100644\n--- a/scrapy/pipelines/images.py\n+++ b/scrapy/pipelines/images.py\n@@ -141,7 +141,7 @@ def get_images(self, response, request, info, *, item=None):\n         yield path, image, buf\n \n         for thumb_id, size in self.thumbs.items():\n-            thumb_path = self.thumb_path(request, thumb_id, response=response, info=info)\n+            thumb_path = self.thumb_path(request, thumb_id, response=response, info=info, item=item)\n             thumb_image, thumb_buf = self.convert_image(image, size)\n             yield thumb_path, thumb_image, thumb_buf\n \n@@ -179,6 +179,6 @@ def file_path(self, request, response=None, info=None, *, item=None):\n         image_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n         return f'full/{image_guid}.jpg'\n \n-    def thumb_path(self, request, thumb_id, response=None, info=None):\n+    def thumb_path(self, request, thumb_id, response=None, info=None, *, item=None):\n         thumb_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n         return f'thumbs/{thumb_id}/{thumb_guid}.jpg'\ndiff --git a/scrapy/pipelines/media.py b/scrapy/pipelines/media.py\nindex d1bccf32355..430c37227de 100644\n--- a/scrapy/pipelines/media.py\n+++ b/scrapy/pipelines/media.py\n@@ -121,7 +121,7 @@ def _process_request(self, request, info, item):\n     def _make_compatible(self):\n         \"\"\"Make overridable methods of MediaPipeline and subclasses backwards compatible\"\"\"\n         methods = [\n-            \"file_path\", \"media_to_download\", \"media_downloaded\",\n+            \"file_path\", \"thumb_path\", \"media_to_download\", \"media_downloaded\",\n             \"file_downloaded\", \"image_downloaded\", \"get_images\"\n         ]\n \n",
        "test_patch": "diff --git a/tests/test_pipeline_images.py b/tests/test_pipeline_images.py\nindex c69cd0e4a7e..dd94d296b33 100644\n--- a/tests/test_pipeline_images.py\n+++ b/tests/test_pipeline_images.py\n@@ -93,6 +93,22 @@ def test_thumbnail_name(self):\n                                     info=object()),\n                          'thumbs/50/850233df65a5b83361798f532f1fc549cd13cbe9.jpg')\n \n+    def test_thumbnail_name_from_item(self):\n+        \"\"\"\n+        Custom thumbnail name based on item data, overriding default implementation\n+        \"\"\"\n+\n+        class CustomImagesPipeline(ImagesPipeline):\n+            def thumb_path(self, request, thumb_id, response=None, info=None, item=None):\n+                return f\"thumb/{thumb_id}/{item.get('path')}\"\n+\n+        thumb_path = CustomImagesPipeline.from_settings(Settings(\n+            {'IMAGES_STORE': self.tempdir}\n+        )).thumb_path\n+        item = dict(path='path-to-store-file')\n+        request = Request(\"http://example.com\")\n+        self.assertEqual(thumb_path(request, 'small', item=item), 'thumb/small/path-to-store-file')\n+\n     def test_convert_image(self):\n         SIZE = (100, 100)\n         # straigh forward case: RGB and JPEG\ndiff --git a/tests/test_pipeline_media.py b/tests/test_pipeline_media.py\nindex 893d4305200..a802c7cf114 100644\n--- a/tests/test_pipeline_media.py\n+++ b/tests/test_pipeline_media.py\n@@ -1,4 +1,5 @@\n from typing import Optional\n+import io\n \n from testfixtures import LogCapture\n from twisted.trial import unittest\n@@ -355,9 +356,12 @@ def __init__(self, *args, **kwargs):\n \n     def get_media_requests(self, item, info):\n         item_url = item['image_urls'][0]\n+        output_img = io.BytesIO()\n+        img = Image.new('RGB', (60, 30), color='red')\n+        img.save(output_img, format='JPEG')\n         return Request(\n             item_url,\n-            meta={'response': Response(item_url, status=200, body=b'data')}\n+            meta={'response': Response(item_url, status=200, body=output_img.getvalue())}\n         )\n \n     def inc_stats(self, *args, **kwargs):\n@@ -379,9 +383,13 @@ def file_path(self, request, response=None, info=None):\n         self._mockcalled.append('file_path')\n         return super(MockedMediaPipelineDeprecatedMethods, self).file_path(request, response, info)\n \n+    def thumb_path(self, request, thumb_id, response=None, info=None):\n+        self._mockcalled.append('thumb_path')\n+        return super(MockedMediaPipelineDeprecatedMethods, self).thumb_path(request, thumb_id, response, info)\n+\n     def get_images(self, response, request, info):\n         self._mockcalled.append('get_images')\n-        return []\n+        return super(MockedMediaPipelineDeprecatedMethods, self).get_images(response, request, info)\n \n     def image_downloaded(self, response, request, info):\n         self._mockcalled.append('image_downloaded')\n@@ -392,7 +400,11 @@ class MediaPipelineDeprecatedMethodsTestCase(unittest.TestCase):\n     skip = skip_pillow\n \n     def setUp(self):\n-        self.pipe = MockedMediaPipelineDeprecatedMethods(store_uri='store-uri', download_func=_mocked_download_func)\n+        self.pipe = MockedMediaPipelineDeprecatedMethods(\n+            store_uri='store-uri',\n+            download_func=_mocked_download_func,\n+            settings=Settings({\"IMAGES_THUMBS\": {'small': (50, 50)}})\n+        )\n         self.pipe.open_spider(None)\n         self.item = dict(image_urls=['http://picsum.photos/id/1014/200/300'], images=[])\n \n@@ -444,6 +456,16 @@ def test_file_path_called(self):\n         )\n         self._assert_method_called_with_warnings('file_path', message, warnings)\n \n+    @inlineCallbacks\n+    def test_thumb_path_called(self):\n+        yield self.pipe.process_item(self.item, None)\n+        warnings = self.flushWarnings([MediaPipeline._compatible])\n+        message = (\n+            'thumb_path(self, request, thumb_id, response=None, info=None) is deprecated, '\n+            'please use thumb_path(self, request, thumb_id, response=None, info=None, *, item=None)'\n+        )\n+        self._assert_method_called_with_warnings('thumb_path', message, warnings)\n+\n     @inlineCallbacks\n     def test_get_images_called(self):\n         yield self.pipe.process_item(self.item, None)\n",
        "problem_statement": "ImagesPipeline.thumb_path should allow access to item\n## Summary\r\n\r\nImagesPipeline should pass on item to thumb_path function as additional argument.\r\n\r\n## Motivation\r\n\r\nThe ImagesPipeline.file_path function allows access to item. So when an user overwrites this function and makes the file name\r\ndepended on the item itself, it makes sense to have the thumb path to be depended on the item as well.\r\n\r\nThis way main image file and thumbnail image file can have the same name and it is easily possible to jump between different sizes of the image.\r\n\r\nExample of how a file and thumbnail could be build:\r\n```python\r\nfile_path = f\"full/{item['ID']}.jpg\"\r\nthumb_path = f\"thumbs/small/{item['ID']}.jpg\"\r\n```\r\n\r\nSo I suggest to change the signature of ImagesPipeline.thumb_path to the following:\r\n```python\r\ndef thumb_path(self, request, thumb_id, response=None, info=None, *, item=None):\r\n```\r\nwhich makes the functions thumb_path and file_path more consistent.\r\n\r\n## Describe alternatives you've considered\r\n\r\nAs a workaround when I overwrite `file_path` I set `request._item = item` which I can then access in `thumb_path` function.\r\n\r\n## Additional context\r\n\r\nIn case this is of interest I am happy to work on a pull request.\r\n\nImagesPipeline.thumb_path should allow access to item\n## Summary\r\n\r\nImagesPipeline should pass on item to thumb_path function as additional argument.\r\n\r\n## Motivation\r\n\r\nThe ImagesPipeline.file_path function allows access to item. So when an user overwrites this function and makes the file name\r\ndepended on the item itself, it makes sense to have the thumb path to be depended on the item as well.\r\n\r\nThis way main image file and thumbnail image file can have the same name and it is easily possible to jump between different sizes of the image.\r\n\r\nExample of how a file and thumbnail could be build:\r\n```python\r\nfile_path = f\"full/{item['ID']}.jpg\"\r\nthumb_path = f\"thumbs/small/{item['ID']}.jpg\"\r\n```\r\n\r\nSo I suggest to change the signature of ImagesPipeline.thumb_path to the following:\r\n```python\r\ndef thumb_path(self, request, thumb_id, response=None, info=None, *, item=None):\r\n```\r\nwhich makes the functions thumb_path and file_path more consistent.\r\n\r\n## Describe alternatives you've considered\r\n\r\nAs a workaround when I overwrite `file_path` I set `request._item = item` which I can then access in `thumb_path` function.\r\n\r\n## Additional context\r\n\r\nIn case this is of interest I am happy to work on a pull request.\r\n\n",
        "hints_text": "\n",
        "created_at": "2022-05-24T10:33:12Z",
        "version": "2.6",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "tests/test_pipeline_images.py",
            "tests/test_pipeline_media.py"
        ]
    },
    {
        "repo": "scrapy/scrapy",
        "pull_number": 5514,
        "instance_id": "scrapy__scrapy-5514",
        "issue_numbers": [
            "5512"
        ],
        "base_commit": "bafe874ecca8f2ab86611739202806ca0cdba844",
        "patch": "diff --git a/.github/workflows/checks.yml b/.github/workflows/checks.yml\nindex 98fa44c7ff6..b26f344ffb0 100644\n--- a/.github/workflows/checks.yml\n+++ b/.github/workflows/checks.yml\n@@ -19,7 +19,7 @@ jobs:\n         - python-version: 3.8\n           env:\n             TOXENV: pylint\n-        - python-version: 3.6\n+        - python-version: 3.7\n           env:\n             TOXENV: typing\n         - python-version: \"3.10\"  # Keep in sync with .readthedocs.yml\ndiff --git a/README.rst b/README.rst\nindex 6b563d638f9..b543a30f49c 100644\n--- a/README.rst\n+++ b/README.rst\n@@ -57,7 +57,7 @@ including a list of features.\n Requirements\n ============\n \n-* Python 3.6+\n+* Python 3.7+\n * Works on Linux, Windows, macOS, BSD\n \n Install\ndiff --git a/docs/contributing.rst b/docs/contributing.rst\nindex 4d2580a6c8b..946bdc23e10 100644\n--- a/docs/contributing.rst\n+++ b/docs/contributing.rst\n@@ -232,15 +232,15 @@ To run a specific test (say ``tests/test_loader.py``) use:\n \n To run the tests on a specific :doc:`tox <tox:index>` environment, use\n ``-e <name>`` with an environment name from ``tox.ini``. For example, to run\n-the tests with Python 3.6 use::\n+the tests with Python 3.7 use::\n \n-    tox -e py36\n+    tox -e py37\n \n You can also specify a comma-separated list of environments, and use :ref:`tox\u2019s\n parallel mode <tox:parallel_mode>` to run the tests on multiple environments in\n parallel::\n \n-    tox -e py36,py38 -p auto\n+    tox -e py37,py38 -p auto\n \n To pass command-line options to :doc:`pytest <pytest:index>`, add them after\n ``--`` in your call to :doc:`tox <tox:index>`. Using ``--`` overrides the\n@@ -250,9 +250,9 @@ default positional arguments (``scrapy tests``) after ``--`` as well::\n     tox -- scrapy tests -x  # stop after first failure\n \n You can also use the `pytest-xdist`_ plugin. For example, to run all tests on\n-the Python 3.6 :doc:`tox <tox:index>` environment using all your CPU cores::\n+the Python 3.7 :doc:`tox <tox:index>` environment using all your CPU cores::\n \n-    tox -e py36 -- scrapy tests -n auto\n+    tox -e py37 -- scrapy tests -n auto\n \n To see coverage report install :doc:`coverage <coverage:index>`\n (``pip install coverage``) and run:\ndiff --git a/docs/intro/install.rst b/docs/intro/install.rst\nindex b8d3a16bccd..1f01c068dfc 100644\n--- a/docs/intro/install.rst\n+++ b/docs/intro/install.rst\n@@ -9,8 +9,8 @@ Installation guide\n Supported Python versions\n =========================\n \n-Scrapy requires Python 3.6+, either the CPython implementation (default) or\n-the PyPy 7.2.0+ implementation (see :ref:`python:implementations`).\n+Scrapy requires Python 3.7+, either the CPython implementation (default) or\n+the PyPy 7.3.5+ implementation (see :ref:`python:implementations`).\n \n .. _intro-install-scrapy:\n \ndiff --git a/docs/topics/items.rst b/docs/topics/items.rst\nindex 7cd482d0746..16701438179 100644\n--- a/docs/topics/items.rst\n+++ b/docs/topics/items.rst\n@@ -102,11 +102,6 @@ Additionally, ``dataclass`` items also allow to:\n * define custom field metadata through :func:`dataclasses.field`, which can be used to\n   :ref:`customize serialization <topics-exporters-field-serialization>`.\n \n-They work natively in Python 3.7 or later, or using the `dataclasses\n-backport`_ in Python 3.6.\n-\n-.. _dataclasses backport: https://pypi.org/project/dataclasses/\n-\n Example::\n \n     from dataclasses import dataclass\ndiff --git a/docs/topics/media-pipeline.rst b/docs/topics/media-pipeline.rst\nindex 7dff78390f5..7457d09cdad 100644\n--- a/docs/topics/media-pipeline.rst\n+++ b/docs/topics/media-pipeline.rst\n@@ -70,7 +70,7 @@ The advantage of using the :class:`ImagesPipeline` for image files is that you\n can configure some extra functions like generating thumbnails and filtering\n the images based on their size.\n \n-The Images Pipeline requires Pillow_ 4.0.0 or greater. It is used for\n+The Images Pipeline requires Pillow_ 7.1.0 or greater. It is used for\n thumbnailing and normalizing images to JPEG/RGB format.\n \n .. _Pillow: https://github.com/python-pillow/Pillow\ndiff --git a/scrapy/__init__.py b/scrapy/__init__.py\nindex 396f98219f8..86e5843963c 100644\n--- a/scrapy/__init__.py\n+++ b/scrapy/__init__.py\n@@ -28,8 +28,8 @@\n \n \n # Check minimum required Python version\n-if sys.version_info < (3, 6):\n-    print(f\"Scrapy {__version__} requires Python 3.6+\")\n+if sys.version_info < (3, 7):\n+    print(f\"Scrapy {__version__} requires Python 3.7+\")\n     sys.exit(1)\n \n \ndiff --git a/scrapy/utils/py36.py b/scrapy/utils/py36.py\ndeleted file mode 100644\nindex 653e2bbbb49..00000000000\n--- a/scrapy/utils/py36.py\n+++ /dev/null\n@@ -1,11 +0,0 @@\n-import warnings\n-\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-from scrapy.utils.asyncgen import collect_asyncgen  # noqa: F401\n-\n-\n-warnings.warn(\n-    \"Module `scrapy.utils.py36` is deprecated, please import from `scrapy.utils.asyncgen` instead.\",\n-    category=ScrapyDeprecationWarning,\n-    stacklevel=2,\n-)\ndiff --git a/setup.py b/setup.py\nindex d86c0f285d0..ed197273fe5 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -19,35 +19,29 @@ def has_environment_marker_platform_impl_support():\n \n \n install_requires = [\n-    'Twisted>=17.9.0',\n-    'cryptography>=2.0',\n+    'Twisted>=18.9.0',\n+    'cryptography>=2.8',\n     'cssselect>=0.9.1',\n     'itemloaders>=1.0.1',\n     'parsel>=1.5.0',\n-    'pyOpenSSL>=16.2.0',\n+    'pyOpenSSL>=19.1.0',\n     'queuelib>=1.4.2',\n     'service_identity>=16.0.0',\n     'w3lib>=1.17.0',\n-    'zope.interface>=4.1.3',\n+    'zope.interface>=5.1.0',\n     'protego>=0.1.15',\n     'itemadapter>=0.1.0',\n     'setuptools',\n     'tldextract',\n+    'lxml>=4.3.0',\n ]\n extras_require = {}\n cpython_dependencies = [\n-    'lxml>=3.5.0',\n     'PyDispatcher>=2.0.5',\n ]\n if has_environment_marker_platform_impl_support():\n     extras_require[':platform_python_implementation == \"CPython\"'] = cpython_dependencies\n     extras_require[':platform_python_implementation == \"PyPy\"'] = [\n-        # Earlier lxml versions are affected by\n-        # https://foss.heptapod.net/pypy/pypy/-/issues/2498,\n-        # which was fixed in Cython 0.26, released on 2017-06-19, and used to\n-        # generate the C headers of lxml release tarballs published since then, the\n-        # first of which was:\n-        'lxml>=4.0.0',\n         'PyPyDispatcher>=2.1.0',\n     ]\n else:\n@@ -84,7 +78,6 @@ def has_environment_marker_platform_impl_support():\n         'Operating System :: OS Independent',\n         'Programming Language :: Python',\n         'Programming Language :: Python :: 3',\n-        'Programming Language :: Python :: 3.6',\n         'Programming Language :: Python :: 3.7',\n         'Programming Language :: Python :: 3.8',\n         'Programming Language :: Python :: 3.9',\n@@ -95,7 +88,7 @@ def has_environment_marker_platform_impl_support():\n         'Topic :: Software Development :: Libraries :: Application Frameworks',\n         'Topic :: Software Development :: Libraries :: Python Modules',\n     ],\n-    python_requires='>=3.6',\n+    python_requires='>=3.7',\n     install_requires=install_requires,\n     extras_require=extras_require,\n )\ndiff --git a/tox.ini b/tox.ini\nindex 6951b6d1632..ab8a715c2c9 100644\n--- a/tox.ini\n+++ b/tox.ini\n@@ -11,15 +11,13 @@ minversion = 1.7.0\n deps =\n     -rtests/requirements.txt\n     # mitmproxy does not support PyPy\n-    # mitmproxy does not support Windows when running Python < 3.7\n     # Python 3.9+ requires mitmproxy >= 5.3.0\n     # mitmproxy >= 5.3.0 requires h2 >= 4.0, Twisted 21.2 requires h2 < 4.0\n     #mitmproxy >= 5.3.0; python_version >= '3.9' and implementation_name != 'pypy'\n     # The tests hang with mitmproxy 8.0.0: https://github.com/scrapy/scrapy/issues/5454\n-    mitmproxy >= 4.0.4, < 8; python_version >= '3.7' and python_version < '3.9' and implementation_name != 'pypy'\n-    mitmproxy >= 4.0.4, < 5; python_version >= '3.6' and python_version < '3.7' and platform_system != 'Windows' and implementation_name != 'pypy'\n+    mitmproxy >= 4.0.4, < 8; python_version < '3.9' and implementation_name != 'pypy'\n     # newer markupsafe is incompatible with deps of old mitmproxy (which we get on Python 3.7 and lower)\n-    markupsafe < 2.1.0; python_version >= '3.6' and python_version < '3.8' and implementation_name != 'pypy'\n+    markupsafe < 2.1.0; python_version < '3.8' and implementation_name != 'pypy'\n     # Extras\n     botocore>=1.4.87\n passenv =\n@@ -44,7 +42,6 @@ deps =\n     types-pyOpenSSL==20.0.3\n     types-setuptools==57.0.0\n commands =\n-    pip install types-dataclasses  # remove once py36 support is dropped\n     mypy --show-error-codes {posargs: scrapy tests}\n \n [testenv:security]\n@@ -75,18 +72,19 @@ commands =\n \n [pinned]\n deps =\n-    cryptography==2.0\n+    cryptography==2.8\n     cssselect==0.9.1\n     h2==3.0\n     itemadapter==0.1.0\n     parsel==1.5.0\n     Protego==0.1.15\n-    pyOpenSSL==16.2.0\n+    pyOpenSSL==19.1.0\n     queuelib==1.4.2\n     service_identity==16.0.0\n-    Twisted[http2]==17.9.0\n+    Twisted[http2]==18.9.0\n     w3lib==1.17.0\n-    zope.interface==4.1.3\n+    zope.interface==5.1.0\n+    lxml==4.3.0\n     -rtests/requirements.txt\n \n     # mitmproxy 4.0.4+ requires upgrading some of the pinned dependencies\n@@ -95,7 +93,7 @@ deps =\n     # Extras\n     botocore==1.4.87\n     google-cloud-storage==1.29.0\n-    Pillow==4.0.0\n+    Pillow==7.1.0\n setenv =\n     _SCRAPY_PINNED=true\n install_command =\n@@ -104,7 +102,6 @@ install_command =\n [testenv:pinned]\n deps =\n     {[pinned]deps}\n-    lxml==3.5.0\n     PyDispatcher==2.0.5\n install_command = {[pinned]install_command}\n setenv =\n@@ -114,9 +111,6 @@ setenv =\n basepython = python3\n deps =\n     {[pinned]deps}\n-    # First lxml version that includes a Windows wheel for Python 3.6, so we do\n-    # not need to build lxml from sources in a CI Windows job:\n-    lxml==3.8.0\n     PyDispatcher==2.0.5\n install_command = {[pinned]install_command}\n setenv =\n@@ -155,7 +149,6 @@ commands =\n basepython = {[testenv:pypy3]basepython}\n deps =\n     {[pinned]deps}\n-    lxml==4.0.0\n     PyPyDispatcher==2.1.0\n commands = {[testenv:pypy3]commands}\n install_command = {[pinned]install_command}\n",
        "test_patch": "diff --git a/.github/workflows/tests-macos.yml b/.github/workflows/tests-macos.yml\nindex 3aaf688c712..7819a4e12f9 100644\n--- a/.github/workflows/tests-macos.yml\n+++ b/.github/workflows/tests-macos.yml\n@@ -7,7 +7,7 @@ jobs:\n     strategy:\n       fail-fast: false\n       matrix:\n-        python-version: [\"3.6\", \"3.7\", \"3.8\", \"3.9\", \"3.10\"]\n+        python-version: [\"3.7\", \"3.8\", \"3.9\", \"3.10\"]\n \n     steps:\n     - uses: actions/checkout@v2\ndiff --git a/.github/workflows/tests-ubuntu.yml b/.github/workflows/tests-ubuntu.yml\nindex 1fc8d914b88..be40c7c7111 100644\n--- a/.github/workflows/tests-ubuntu.yml\n+++ b/.github/workflows/tests-ubuntu.yml\n@@ -8,9 +8,6 @@ jobs:\n       fail-fast: false\n       matrix:\n         include:\n-        - python-version: 3.7\n-          env:\n-            TOXENV: py\n         - python-version: 3.8\n           env:\n             TOXENV: py\n@@ -26,19 +23,19 @@ jobs:\n         - python-version: pypy3\n           env:\n             TOXENV: pypy3\n-            PYPY_VERSION: 3.6-v7.3.3\n+            PYPY_VERSION: 3.9-v7.3.9\n \n         # pinned deps\n-        - python-version: 3.6.12\n+        - python-version: 3.7.13\n           env:\n             TOXENV: pinned\n-        - python-version: 3.6.12\n+        - python-version: 3.7.13\n           env:\n             TOXENV: asyncio-pinned\n         - python-version: pypy3\n           env:\n             TOXENV: pypy3-pinned\n-            PYPY_VERSION: 3.6-v7.2.0\n+            PYPY_VERSION: 3.7-v7.3.5\n \n         # extras\n         # extra-deps includes reppy, which does not support Python 3.9\ndiff --git a/.github/workflows/tests-windows.yml b/.github/workflows/tests-windows.yml\nindex ab738511897..955b9b44909 100644\n--- a/.github/workflows/tests-windows.yml\n+++ b/.github/workflows/tests-windows.yml\n@@ -8,12 +8,9 @@ jobs:\n       fail-fast: false\n       matrix:\n         include:\n-        - python-version: 3.6\n-          env:\n-            TOXENV: windows-pinned\n         - python-version: 3.7\n           env:\n-            TOXENV: py\n+            TOXENV: windows-pinned\n         - python-version: 3.8\n           env:\n             TOXENV: py\ndiff --git a/tests/requirements.txt b/tests/requirements.txt\nindex d2a8aae1b57..d9373dfa808 100644\n--- a/tests/requirements.txt\n+++ b/tests/requirements.txt\n@@ -1,14 +1,12 @@\n # Tests requirements\n attrs\n-dataclasses; python_version == '3.6'\n pyftpdlib\n pytest\n pytest-cov==3.0.0\n pytest-xdist\n sybil >= 1.3.0  # https://github.com/cjw296/sybil/issues/20#issuecomment-605433422\n testfixtures\n-uvloop < 0.15.0; platform_system != \"Windows\" and python_version == '3.6'\n-uvloop; platform_system != \"Windows\" and python_version > '3.6'\n+uvloop; platform_system != \"Windows\"\n \n # optional for shell wrapper tests\n bpython\ndiff --git a/tests/test_utils_python.py b/tests/test_utils_python.py\nindex 4b3964154b7..7dec5624a20 100644\n--- a/tests/test_utils_python.py\n+++ b/tests/test_utils_python.py\n@@ -3,7 +3,6 @@\n import operator\n import platform\n import unittest\n-from datetime import datetime\n from itertools import count\n from warnings import catch_warnings, filterwarnings\n \n@@ -224,12 +223,7 @@ def __call__(self, a, b, c):\n         elif platform.python_implementation() == 'PyPy':\n             self.assertEqual(get_func_args(str.split, stripself=True), ['sep', 'maxsplit'])\n             self.assertEqual(get_func_args(operator.itemgetter(2), stripself=True), ['obj'])\n-\n-            build_date = datetime.strptime(platform.python_build()[1], '%b %d %Y')\n-            if build_date >= datetime(2020, 4, 7):  # PyPy 3.6-v7.3.1\n-                self.assertEqual(get_func_args(\" \".join, stripself=True), ['iterable'])\n-            else:\n-                self.assertEqual(get_func_args(\" \".join, stripself=True), ['list'])\n+            self.assertEqual(get_func_args(\" \".join, stripself=True), ['iterable'])\n \n     def test_without_none_values(self):\n         self.assertEqual(without_none_values([1, None, 3, 4]), [1, 3, 4])\n",
        "problem_statement": "Drop Python 3.6 support\n[It went end-of-life on December 2021](https://endoflife.date/python).\n",
        "hints_text": "",
        "created_at": "2022-06-03T10:20:47Z",
        "version": "2.6",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            ".github/workflows/tests-macos.yml",
            ".github/workflows/tests-ubuntu.yml",
            ".github/workflows/tests-windows.yml",
            "tests/requirements.txt",
            "tests/test_utils_python.py"
        ]
    },
    {
        "repo": "scrapy/scrapy",
        "pull_number": 5497,
        "instance_id": "scrapy__scrapy-5497",
        "issue_numbers": [
            "3264",
            "5376"
        ],
        "base_commit": "b2afcbfe2bf090827540d072866bef0d1ab3a3e8",
        "patch": "diff --git a/scrapy/commands/parse.py b/scrapy/commands/parse.py\nindex a3f6b96f420..99fc8f955ba 100644\n--- a/scrapy/commands/parse.py\n+++ b/scrapy/commands/parse.py\n@@ -146,7 +146,8 @@ def set_spidercls(self, url, opts):\n \n         def _start_requests(spider):\n             yield self.prepare_request(spider, Request(url), opts)\n-        self.spidercls.start_requests = _start_requests\n+        if self.spidercls:\n+            self.spidercls.start_requests = _start_requests\n \n     def start_parsing(self, url, opts):\n         self.crawler_process.crawl(self.spidercls, **opts.spargs)\n",
        "test_patch": "diff --git a/tests/test_command_parse.py b/tests/test_command_parse.py\nindex f21ee971d38..0d992be5657 100644\n--- a/tests/test_command_parse.py\n+++ b/tests/test_command_parse.py\n@@ -1,6 +1,7 @@\n import os\n import argparse\n from os.path import join, abspath, isfile, exists\n+\n from twisted.internet import defer\n from scrapy.commands import parse\n from scrapy.settings import Settings\n@@ -222,6 +223,11 @@ def test_crawlspider_no_matching_rule(self):\n         self.assertRegex(_textmode(out), r\"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n         self.assertIn(\"\"\"Cannot find a rule that matches\"\"\", _textmode(stderr))\n \n+    @defer.inlineCallbacks\n+    def test_crawlspider_not_exists_with_not_matched_url(self):\n+        status, out, stderr = yield self.execute([self.url('/invalid_url')])\n+        self.assertEqual(status, 0)\n+\n     @defer.inlineCallbacks\n     def test_output_flag(self):\n         \"\"\"Checks if a file was created successfully having\n",
        "problem_statement": "Command parse unhandled error :AttributeError: 'NoneType' object has no attribute 'start_requests'\nScrapy version :1.5.0\r\nWhen i run the command **scrapy parse http://www.baidu.com**, and the url www.baidu.com dosn't  have spider matched , then i got the error:\r\n\r\n> 2018-03-11 16:23:35 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: DouTu)\r\n> 2018-03-11 16:23:35 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.9.0, Python 2.7.12 (default, Dec  4 2017, 14:50:18) - [GCC 5.4.0 20160609], pyOpenSSL 17.5.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-38-generic-x86_64-with-Ubuntu-16.04-xenial\r\n> 2018-05-18 16:23:35 [scrapy.commands.parse] ERROR: Unable to find spider for: http://www.baidu.com\r\n> Traceback (most recent call last):\r\n>   File \"/home/wangsir/code/sourceWorkSpace/scrapy/cmdline.py\", line 239, in <module>\r\n>     execute(['scrapy','parse','http://www.baidu.com'])\r\n>   File \"/home/wangsir/code/sourceWorkSpace/scrapy/cmdline.py\", line 168, in execute\r\n>     _run_print_help(parser, _run_command, cmd, args, opts)\r\n>   File \"/home/wangsir/code/sourceWorkSpace/scrapy/cmdline.py\", line 98, in _run_print_help\r\n>     func(*a, **kw)\r\n>   File \"/home/wangsir/code/sourceWorkSpace/scrapy/cmdline.py\", line 176, in _run_command\r\n>     cmd.run(args, opts)\r\n>   File \"/home/wangsir/code/sourceWorkSpace/scrapy/commands/parse.py\", line 250, in run\r\n>     self.set_spidercls(url, opts)\r\n>   File \"/home/wangsir/code/sourceWorkSpace/scrapy/commands/parse.py\", line 151, in set_spidercls\r\n>     self.spidercls.start_requests = _start_requests\r\n> AttributeError: 'NoneType' object has no attribute 'start_requests'.\r\n\r\nThe failed reason should be follwing code(scrapy/commands/parse.py line 151):\r\n         **`self.spidercls.start_requests = _start_requests`**\r\nbecause the url www.baidu.com dosn't  have spider matched,so self.spidercls is none,so self.spidercls.start_requests throw the error.\nFix command parse unhandled error :AttributeError: 'NoneType' object has no attribute 'start_requests'(#3264)\nReopening @wangrenlei's PR \r\nFixes #3264\n",
        "hints_text": "For those who need a quick-and-dirty workaround, specify the spider `--spider=NAME_OF_MY_SPIDER`\nThe original PR, #3265, looks correct to me, someone just needs to create a new one, import the commit from that one and publish it.\nHello , I think i can help with this issue if someone could give me a little insight .\nhi @PushanAgrawal , as you can see above your comment there is already a PR open for it.\n# [Codecov](https://codecov.io/gh/scrapy/scrapy/pull/5376?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) Report\n> Merging [#5376](https://codecov.io/gh/scrapy/scrapy/pull/5376?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) (837038d) into [master](https://codecov.io/gh/scrapy/scrapy/commit/92764d68e272079b6004c194bf237f0e1c8ee95d?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) (92764d6) will **decrease** coverage by `0.46%`.\n> The diff coverage is `91.89%`.\n\n> :exclamation: Current head 837038d differs from pull request most recent head e7ccec1. Consider uploading reports for the commit e7ccec1 to get more accurate results\n\n\n```diff\n@@            Coverage Diff             @@\n##           master    #5376      +/-   ##\n==========================================\n- Coverage   88.58%   88.11%   -0.47%     \n==========================================\n  Files         163      163              \n  Lines       10635    10628       -7     \n  Branches     1812     1810       -2     \n==========================================\n- Hits         9421     9365      -56     \n- Misses        938      985      +47     \n- Partials      276      278       +2     \n```\n\n| [Impacted Files](https://codecov.io/gh/scrapy/scrapy/pull/5376?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) | Coverage \u0394 | |\n|---|---|---|\n| [scrapy/commands/parse.py](https://codecov.io/gh/scrapy/scrapy/pull/5376/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy#diff-c2NyYXB5L2NvbW1hbmRzL3BhcnNlLnB5) | `20.11% <0.00%> (-0.12%)` | :arrow_down: |\n| [scrapy/http/response/text.py](https://codecov.io/gh/scrapy/scrapy/pull/5376/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy#diff-c2NyYXB5L2h0dHAvcmVzcG9uc2UvdGV4dC5weQ==) | `100.00% <\u00f8> (\u00f8)` | |\n| [scrapy/downloadermiddlewares/stats.py](https://codecov.io/gh/scrapy/scrapy/pull/5376/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy#diff-c2NyYXB5L2Rvd25sb2FkZXJtaWRkbGV3YXJlcy9zdGF0cy5weQ==) | `91.66% <91.66%> (-0.93%)` | :arrow_down: |\n| [scrapy/exporters.py](https://codecov.io/gh/scrapy/scrapy/pull/5376/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy#diff-c2NyYXB5L2V4cG9ydGVycy5weQ==) | `100.00% <100.00%> (\u00f8)` | |\n| [scrapy/extensions/feedexport.py](https://codecov.io/gh/scrapy/scrapy/pull/5376/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy#diff-c2NyYXB5L2V4dGVuc2lvbnMvZmVlZGV4cG9ydC5weQ==) | `91.23% <100.00%> (-3.85%)` | :arrow_down: |\n| [scrapy/item.py](https://codecov.io/gh/scrapy/scrapy/pull/5376/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy#diff-c2NyYXB5L2l0ZW0ucHk=) | `98.36% <100.00%> (-0.39%)` | :arrow_down: |\n| [scrapy/settings/\\_\\_init\\_\\_.py](https://codecov.io/gh/scrapy/scrapy/pull/5376/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy#diff-c2NyYXB5L3NldHRpbmdzL19faW5pdF9fLnB5) | `93.06% <100.00%> (+0.08%)` | :arrow_up: |\n| [scrapy/spiders/feed.py](https://codecov.io/gh/scrapy/scrapy/pull/5376/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy#diff-c2NyYXB5L3NwaWRlcnMvZmVlZC5weQ==) | `73.84% <100.00%> (+7.69%)` | :arrow_up: |\n| [scrapy/utils/misc.py](https://codecov.io/gh/scrapy/scrapy/pull/5376/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy#diff-c2NyYXB5L3V0aWxzL21pc2MucHk=) | `97.72% <100.00%> (\u00f8)` | |\n| [scrapy/utils/response.py](https://codecov.io/gh/scrapy/scrapy/pull/5376/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy#diff-c2NyYXB5L3V0aWxzL3Jlc3BvbnNlLnB5) | `90.19% <100.00%> (+0.40%)` | :arrow_up: |\n| ... and [13 more](https://codecov.io/gh/scrapy/scrapy/pull/5376/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) | |\n\nHey @wRAR, I am new to this, in fact this is my first PR. Is there something else I need to do here? Thanks.\nIdeally, the pull requests should pass all tests, and if not you should find out why and address any issue.\r\n\r\nIt is possible that some of the broken tests are unrelated to your changes though. Let me close and reopen the pull request to re-trigger the tests, and see if they pass.\nThe tests indeed fail for different reasons but this code is not covered by tests. I think it shouldn't be hard to make a test that runs the failing command (even though all existing tests in test_command_parse.py pass --spider explicitly).",
        "created_at": "2022-05-06T22:13:45Z",
        "version": "2.6",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "tests/test_command_parse.py"
        ]
    },
    {
        "repo": "scrapy/scrapy",
        "pull_number": 5482,
        "instance_id": "scrapy__scrapy-5482",
        "issue_numbers": [
            "5481"
        ],
        "base_commit": "aead27bcbdf7c2a4d959dcb357c7f12cc8411739",
        "patch": "diff --git a/scrapy/commands/parse.py b/scrapy/commands/parse.py\nindex a3f6b96f420..8e52d0d7661 100644\n--- a/scrapy/commands/parse.py\n+++ b/scrapy/commands/parse.py\n@@ -51,7 +51,7 @@ def add_options(self, parser):\n         parser.add_argument(\"--cbkwargs\", dest=\"cbkwargs\",\n                             help=\"inject extra callback kwargs into the Request, it must be a valid raw json string\")\n         parser.add_argument(\"-d\", \"--depth\", dest=\"depth\", type=int, default=1,\n-                            help=\"maximum depth for parsing requests [default: %default]\")\n+                            help=\"maximum depth for parsing requests [default: %(default)s]\")\n         parser.add_argument(\"-v\", \"--verbose\", dest=\"verbose\", action=\"store_true\",\n                             help=\"print each depth level one by one\")\n \n",
        "test_patch": "diff --git a/tests/test_commands.py b/tests/test_commands.py\nindex b5e6c2b8bc4..76d5f3935b4 100644\n--- a/tests/test_commands.py\n+++ b/tests/test_commands.py\n@@ -930,3 +930,17 @@ def start_requests(self):\n         args = ['-o', 'example1.json', '-O', 'example2.json']\n         log = self.get_log(spider_code, args=args)\n         self.assertIn(\"error: Please use only one of -o/--output and -O/--overwrite-output\", log)\n+\n+\n+class HelpMessageTest(CommandTest):\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.commands = [\"parse\", \"startproject\", \"view\", \"crawl\", \"edit\",\n+                         \"list\", \"fetch\", \"settings\", \"shell\", \"runspider\",\n+                         \"version\", \"genspider\", \"check\", \"bench\"]\n+\n+    def test_help_messages(self):\n+        for command in self.commands:\n+            _, out, _ = self.proc(command, \"-h\")\n+            self.assertIn(\"Usage\", out)\n",
        "problem_statement": "scrapy parse -h throws error\n### Description\r\nrunning `scrapy parse -h` from inside of a project throws an error.\r\n\r\n### Steps to Reproduce\r\n1.  `scrapy startproject example`\r\n2.  `cd example`\r\n3.  `scrapy parse -h`\r\n\r\n**Expected behavior:**  It should show the help message output for the `parse` command\r\n**Actual behavior:**  Throws error and displays traceback information\r\n**Reproduces how often:**  Every time I tried it.\r\n\r\n### Versions\r\nv2.6.1\r\n\r\n### Additional context\r\n\r\nI identified the issue and will submit a PR shortly. \n",
        "hints_text": "I can reproduce. For completeness, here's the full output I'm getting:\r\n\r\n```\r\n$ scrapy parse -h\r\nTraceback (most recent call last):\r\n  File \"/home/eugenio/zyte/scrapy/venv-scrapy/bin/scrapy\", line 33, in <module>\r\n    sys.exit(load_entry_point('Scrapy', 'console_scripts', 'scrapy')())\r\n  File \"/home/eugenio/zyte/scrapy/scrapy/cmdline.py\", line 141, in execute\r\n    opts, args = parser.parse_known_args(args=argv[1:])\r\n  File \"/usr/local/lib/python3.9/argparse.py\", line 1851, in parse_known_args\r\n    namespace, args = self._parse_known_args(args, namespace)\r\n  File \"/usr/local/lib/python3.9/argparse.py\", line 2060, in _parse_known_args\r\n    start_index = consume_optional(start_index)\r\n  File \"/usr/local/lib/python3.9/argparse.py\", line 2000, in consume_optional\r\n    take_action(action, args, option_string)\r\n  File \"/usr/local/lib/python3.9/argparse.py\", line 1928, in take_action\r\n    action(self, namespace, argument_values, option_string)\r\n  File \"/usr/local/lib/python3.9/argparse.py\", line 1092, in __call__\r\n    parser.print_help()\r\n  File \"/usr/local/lib/python3.9/argparse.py\", line 2548, in print_help\r\n    self._print_message(self.format_help(), file)\r\n  File \"/usr/local/lib/python3.9/argparse.py\", line 2532, in format_help\r\n    return formatter.format_help()\r\n  File \"/usr/local/lib/python3.9/argparse.py\", line 283, in format_help\r\n    help = self._root_section.format_help()\r\n  File \"/usr/local/lib/python3.9/argparse.py\", line 214, in format_help\r\n    item_help = join([func(*args) for func, args in self.items])\r\n  File \"/usr/local/lib/python3.9/argparse.py\", line 214, in <listcomp>\r\n    item_help = join([func(*args) for func, args in self.items])\r\n  File \"/usr/local/lib/python3.9/argparse.py\", line 214, in format_help\r\n    item_help = join([func(*args) for func, args in self.items])\r\n  File \"/usr/local/lib/python3.9/argparse.py\", line 214, in <listcomp>\r\n    item_help = join([func(*args) for func, args in self.items])\r\n  File \"/usr/local/lib/python3.9/argparse.py\", line 530, in _format_action\r\n    help_text = self._expand_help(action)\r\n  File \"/usr/local/lib/python3.9/argparse.py\", line 626, in _expand_help\r\n    return self._get_help_string(action) % params\r\nTypeError: %d format: a number is required, not dict\r\n```\r\n\r\nI suspect this was valid with `optparse` and broke on the move to `argparse` (#5374)",
        "created_at": "2022-04-17T01:20:10Z",
        "version": "2.6",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "tests/test_commands.py"
        ]
    },
    {
        "repo": "scrapy/scrapy",
        "pull_number": 5436,
        "instance_id": "scrapy__scrapy-5436",
        "issue_numbers": [
            "5435"
        ],
        "base_commit": "23537a0f9580bfb28ac5d8b88f37df47e838f463",
        "patch": "diff --git a/scrapy/crawler.py b/scrapy/crawler.py\nindex a638254f1ba..d669d93a899 100644\n--- a/scrapy/crawler.py\n+++ b/scrapy/crawler.py\n@@ -290,6 +290,7 @@ def __init__(self, settings=None, install_root_handler=True):\n         super().__init__(settings)\n         configure_logging(self.settings, install_root_handler)\n         log_scrapy_info(self.settings)\n+        self._initialized_reactor = False\n \n     def _signal_shutdown(self, signum, _):\n         from twisted.internet import reactor\n@@ -310,7 +311,9 @@ def _signal_kill(self, signum, _):\n     def _create_crawler(self, spidercls):\n         if isinstance(spidercls, str):\n             spidercls = self.spider_loader.load(spidercls)\n-        return Crawler(spidercls, self.settings, init_reactor=True)\n+        init_reactor = not self._initialized_reactor\n+        self._initialized_reactor = True\n+        return Crawler(spidercls, self.settings, init_reactor=init_reactor)\n \n     def start(self, stop_after_crawl=True, install_signal_handlers=True):\n         \"\"\"\n",
        "test_patch": "diff --git a/tests/CrawlerProcess/multi.py b/tests/CrawlerProcess/multi.py\nnew file mode 100644\nindex 00000000000..aaa1af5c547\n--- /dev/null\n+++ b/tests/CrawlerProcess/multi.py\n@@ -0,0 +1,16 @@\n+import scrapy\n+from scrapy.crawler import CrawlerProcess\n+\n+\n+class NoRequestsSpider(scrapy.Spider):\n+    name = 'no_request'\n+\n+    def start_requests(self):\n+        return []\n+\n+\n+process = CrawlerProcess(settings={})\n+\n+process.crawl(NoRequestsSpider)\n+process.crawl(NoRequestsSpider)\n+process.start()\ndiff --git a/tests/test_crawler.py b/tests/test_crawler.py\nindex 8f62271096e..95752538257 100644\n--- a/tests/test_crawler.py\n+++ b/tests/test_crawler.py\n@@ -302,6 +302,12 @@ def test_simple(self):\n         self.assertIn('Spider closed (finished)', log)\n         self.assertNotIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n \n+    def test_multi(self):\n+        log = self.run_script('multi.py')\n+        self.assertIn('Spider closed (finished)', log)\n+        self.assertNotIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n+        self.assertNotIn(\"ReactorAlreadyInstalledError\", log)\n+\n     def test_asyncio_enabled_no_reactor(self):\n         log = self.run_script('asyncio_enabled_no_reactor.py')\n         self.assertIn('Spider closed (finished)', log)\n",
        "problem_statement": "2.6.0 breaks calling multiple Spider in CrawlerProcess()\n<!--\r\n\r\nThanks for taking an interest in Scrapy!\r\n\r\nIf you have a question that starts with \"How to...\", please see the Scrapy Community page: https://scrapy.org/community/.\r\nThe GitHub issue tracker's purpose is to deal with bug reports and feature requests for the project itself.\r\n\r\nKeep in mind that by filing an issue, you are expected to comply with Scrapy's Code of Conduct, including treating everyone with respect: https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\r\n\r\nThe following is a suggested template to structure your issue, you can find more guidelines at https://doc.scrapy.org/en/latest/contributing.html#reporting-bugs\r\n\r\n-->\r\n\r\n### Description\r\n\r\nSince 2.6.0, it breaks calling multiple Spiders from CrawlerProcess() as shown in the common practices \r\n\r\nhttps://docs.scrapy.org/en/latest/topics/practices.html#running-multiple-spiders-in-the-same-process\r\n\r\n### Steps to Reproduce\r\n\r\n1. use Scrapy=>2.6.0\r\n2. following is the code to reproduce\r\n\r\n```\r\nimport scrapy\r\nfrom scrapy.crawler import CrawlerProcess\r\nfrom scrapy.http import Request\r\n\r\n\r\nclass MySpider(scrapy.Spider):\r\n    name = 'MySpider'\r\n\r\n    def __init__(self, url, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.url = url\r\n\r\n    def start_requests(self):\r\n        yield Request(url=self.url, callback=self.parse)\r\n\r\n    def parse(self, response):\r\n        print(response.url)\r\n\r\n\r\nprocess = CrawlerProcess({\r\n    'DEPTH_LIMIT': 1,\r\n    'DEPTH_PRIORITY': 1\r\n})\r\nprocess.crawl(MySpider, url='https://www.google.com')\r\nprocess.crawl(MySpider, url='https://www.google.co.jp')\r\nprocess.start()\r\n```\r\n\r\n**Expected behavior:** [What you expect to happen]\r\n\r\nFollowing is the result from Scrapy 2.5.1\r\n\r\n```\r\n2022-03-02 18:49:45 [scrapy.utils.log] INFO: Scrapy 2.5.1 started (bot: scrapybot)\r\n2022-03-02 18:49:45 [scrapy.utils.log] INFO: Versions: lxml 4.8.0.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.1.0, Python 3.9.10 (main, Jan 17 2022, 08:36:28) - [GCC 11.2.1 20210728 (Red Hat 11.2.1-1)], pyOpenSSL 22.0.0 (OpenSSL 1.1.1m  14 Dec 2021), cryptography 36.0.1, Platform Linux-3.10.0-1160.59.1.el7.x86_64-x86_64-with-glibc2.17\r\n2022-03-02 18:49:45 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\r\n2022-03-02 18:49:45 [scrapy.crawler] INFO: Overridden settings:\r\n{'DEPTH_LIMIT': 1, 'DEPTH_PRIORITY': 1}\r\n2022-03-02 18:49:45 [scrapy.extensions.telnet] INFO: Telnet Password: afe09d724aae9642\r\n2022-03-02 18:49:45 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.corestats.CoreStats',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.memusage.MemoryUsage',\r\n 'scrapy.extensions.logstats.LogStats']\r\n2022-03-02 18:49:45 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n2022-03-02 18:49:45 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n2022-03-02 18:49:45 [scrapy.middleware] INFO: Enabled item pipelines:\r\n[]\r\n2022-03-02 18:49:45 [scrapy.core.engine] INFO: Spider opened\r\n2022-03-02 18:49:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n2022-03-02 18:49:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\r\n2022-03-02 18:49:45 [scrapy.crawler] INFO: Overridden settings:\r\n{'DEPTH_LIMIT': 1, 'DEPTH_PRIORITY': 1}\r\n2022-03-02 18:49:45 [scrapy.extensions.telnet] INFO: Telnet Password: bd1670acfb7fb550\r\n2022-03-02 18:49:45 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.corestats.CoreStats',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.memusage.MemoryUsage',\r\n 'scrapy.extensions.logstats.LogStats']\r\n2022-03-02 18:49:45 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n2022-03-02 18:49:45 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n2022-03-02 18:49:45 [scrapy.middleware] INFO: Enabled item pipelines:\r\n[]\r\n2022-03-02 18:49:45 [scrapy.core.engine] INFO: Spider opened\r\n2022-03-02 18:49:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n2022-03-02 18:49:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024\r\n2022-03-02 18:49:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.google.com> (referer: None)\r\n2022-03-02 18:49:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.google.co.jp> (referer: None)\r\nhttps://www.google.com\r\nhttps://www.google.co.jp\r\n2022-03-02 18:49:46 [scrapy.core.engine] INFO: Closing spider (finished)\r\n2022-03-02 18:49:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\r\n{'downloader/request_bytes': 214,\r\n 'downloader/request_count': 1,\r\n 'downloader/request_method_count/GET': 1,\r\n 'downloader/response_bytes': 7675,\r\n 'downloader/response_count': 1,\r\n 'downloader/response_status_count/200': 1,\r\n 'elapsed_time_seconds': 0.465932,\r\n 'finish_reason': 'finished',\r\n 'finish_time': datetime.datetime(2022, 3, 2, 9, 49, 46, 66477),\r\n 'httpcompression/response_bytes': 15980,\r\n 'httpcompression/response_count': 1,\r\n 'log_count/DEBUG': 2,\r\n 'log_count/INFO': 19,\r\n 'memusage/max': 47611904,\r\n 'memusage/startup': 47611904,\r\n 'response_received_count': 1,\r\n 'scheduler/dequeued': 1,\r\n 'scheduler/dequeued/memory': 1,\r\n 'scheduler/enqueued': 1,\r\n 'scheduler/enqueued/memory': 1,\r\n 'start_time': datetime.datetime(2022, 3, 2, 9, 49, 45, 600545)}\r\n2022-03-02 18:49:46 [scrapy.core.engine] INFO: Spider closed (finished)\r\n2022-03-02 18:49:46 [scrapy.core.engine] INFO: Closing spider (finished)\r\n2022-03-02 18:49:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\r\n{'downloader/request_bytes': 216,\r\n 'downloader/request_count': 1,\r\n 'downloader/request_method_count/GET': 1,\r\n 'downloader/response_bytes': 7602,\r\n 'downloader/response_count': 1,\r\n 'downloader/response_status_count/200': 1,\r\n 'elapsed_time_seconds': 0.431935,\r\n 'finish_reason': 'finished',\r\n 'finish_time': datetime.datetime(2022, 3, 2, 9, 49, 46, 98011),\r\n 'httpcompression/response_bytes': 14794,\r\n 'httpcompression/response_count': 1,\r\n 'log_count/DEBUG': 2,\r\n 'log_count/INFO': 13,\r\n 'memusage/max': 47669248,\r\n 'memusage/startup': 47669248,\r\n 'response_received_count': 1,\r\n 'scheduler/dequeued': 1,\r\n 'scheduler/dequeued/memory': 1,\r\n 'scheduler/enqueued': 1,\r\n 'scheduler/enqueued/memory': 1,\r\n 'start_time': datetime.datetime(2022, 3, 2, 9, 49, 45, 666076)}\r\n2022-03-02 18:49:46 [scrapy.core.engine] INFO: Spider closed (finished)\r\n```\r\n\r\n**Actual behavior:** [What actually happens]\r\n\r\nSpider fails with twisted.internet.error.ReactorAlreadyInstalledError\r\n\r\n```\r\n2022-03-02 18:49:12 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\r\n2022-03-02 18:49:12 [scrapy.utils.log] INFO: Versions: lxml 4.8.0.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.1.0, Python 3.9.10 (main, Jan 17 2022, 08:36:28) - [GCC 11.2.1 20210728 (Red Hat 11.2.1-1)], pyOpenSSL 22.0.0 (OpenSSL 1.1.1m  14 Dec 2021), cryptography 36.0.1, Platform Linux-3.10.0-1160.59.1.el7.x86_64-x86_64-with-glibc2.17\r\n2022-03-02 18:49:12 [scrapy.crawler] INFO: Overridden settings:\r\n{'DEPTH_LIMIT': 1, 'DEPTH_PRIORITY': 1}\r\n2022-03-02 18:49:12 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\r\n2022-03-02 18:49:12 [scrapy.extensions.telnet] INFO: Telnet Password: ce57e6aa863bb786\r\n2022-03-02 18:49:12 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.corestats.CoreStats',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.memusage.MemoryUsage',\r\n 'scrapy.extensions.logstats.LogStats']\r\n2022-03-02 18:49:13 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n2022-03-02 18:49:13 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n2022-03-02 18:49:13 [scrapy.middleware] INFO: Enabled item pipelines:\r\n[]\r\n2022-03-02 18:49:13 [scrapy.core.engine] INFO: Spider opened\r\n2022-03-02 18:49:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n2022-03-02 18:49:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\r\n2022-03-02 18:49:13 [scrapy.crawler] INFO: Overridden settings:\r\n{'DEPTH_LIMIT': 1, 'DEPTH_PRIORITY': 1}\r\nTraceback (most recent call last):\r\n  File \"/home/kusanagi/work/scrapy/test.py\", line 25, in <module>\r\n    process.crawl(MySpider, url='https://www.google.co.jp')\r\n  File \"/usr/local/lib/python3.9/site-packages/scrapy/crawler.py\", line 205, in crawl\r\n    crawler = self.create_crawler(crawler_or_spidercls)\r\n  File \"/usr/local/lib/python3.9/site-packages/scrapy/crawler.py\", line 238, in create_crawler\r\n    return self._create_crawler(crawler_or_spidercls)\r\n  File \"/usr/local/lib/python3.9/site-packages/scrapy/crawler.py\", line 313, in _create_crawler\r\n    return Crawler(spidercls, self.settings, init_reactor=True)\r\n  File \"/usr/local/lib/python3.9/site-packages/scrapy/crawler.py\", line 82, in __init__\r\n    default.install()\r\n  File \"/usr/local/lib/python3.9/site-packages/twisted/internet/epollreactor.py\", line 256, in install\r\n    installReactor(p)\r\n  File \"/usr/local/lib/python3.9/site-packages/twisted/internet/main.py\", line 32, in installReactor\r\n    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\r\ntwisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\r\n```\r\n\r\n**Reproduces how often:** [What percentage of the time does it reproduce?]\r\n\r\nAlways.\r\n\r\n### Versions\r\n\r\nScrapy       : 2.6.1\r\nlxml         : 4.8.0.0\r\nlibxml2      : 2.9.12\r\ncssselect    : 1.1.0\r\nparsel       : 1.6.0\r\nw3lib        : 1.22.0\r\nTwisted      : 22.1.0\r\nPython       : 3.9.10 (main, Jan 17 2022, 08:36:28) - [GCC 11.2.1 20210728 (Red Hat 11.2.1-1)]\r\npyOpenSSL    : 22.0.0 (OpenSSL 1.1.1m  14 Dec 2021)\r\ncryptography : 36.0.1\r\nPlatform     : Linux-3.10.0-1160.59.1.el7.x86_64-x86_64-with-glibc2.17\r\n\r\n### Additional context\r\n\r\nThe intension of using the same MySpider but from CrawlerProcess is to call Scrapy programatically using different initial url and some tweaks to parser depending on the initial url.\r\n\r\nI think this is very fair usage and was working fine before 2.6.0.\n",
        "hints_text": "Is it only reproducible if the spider class is the same?\nNo, it happens even if different spider class is used.\r\nI just copied complete MySpider class as MySpider2 and used MySpider2 for the second crawl.\r\n\r\n```\r\nprocess.crawl(MySpider, url='https://www.google.com')\r\nprocess.crawl(MySpider2, url='https://www.google.co.jp')\r\n```\r\n\r\nFollowing is the last traceback.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/kusanagi/work/scrapy/test.py\", line 39, in <module>\r\n    process.crawl(MySpider2, url='https://www.google.co.jp')\r\n  File \"/usr/local/lib/python3.9/site-packages/scrapy/crawler.py\", line 205, in crawl\r\n    crawler = self.create_crawler(crawler_or_spidercls)\r\n  File \"/usr/local/lib/python3.9/site-packages/scrapy/crawler.py\", line 238, in create_crawler\r\n    return self._create_crawler(crawler_or_spidercls)\r\n  File \"/usr/local/lib/python3.9/site-packages/scrapy/crawler.py\", line 313, in _create_crawler\r\n    return Crawler(spidercls, self.settings, init_reactor=True)\r\n  File \"/usr/local/lib/python3.9/site-packages/scrapy/crawler.py\", line 82, in __init__\r\n    default.install()\r\n  File \"/usr/local/lib/python3.9/site-packages/twisted/internet/epollreactor.py\", line 256, in install\r\n    installReactor(p)\r\n  File \"/usr/local/lib/python3.9/site-packages/twisted/internet/main.py\", line 32, in installReactor\r\n    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\r\ntwisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\r\n```\r\n\nI have identified https://github.com/scrapy/scrapy/commit/60c8838554a79e70c22a7c6a57baedfcaf521444 as the cause (things work with its parent commit, https://github.com/scrapy/scrapy/commit/46ef9cf771789f1db513bbf2f65243d3320ce695). Working on a fix.",
        "created_at": "2022-03-02T15:07:24Z",
        "version": "2.6",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "tests/test_crawler.py"
        ]
    },
    {
        "repo": "scrapy/scrapy",
        "pull_number": 5406,
        "instance_id": "scrapy__scrapy-5406",
        "issue_numbers": [
            "5386"
        ],
        "base_commit": "e2e2ffd0d162cfed5a2e82e9fb9472dbf233c919",
        "patch": "diff --git a/scrapy/mail.py b/scrapy/mail.py\nindex 2a25ccd4499..b8cc28335fa 100644\n--- a/scrapy/mail.py\n+++ b/scrapy/mail.py\n@@ -12,7 +12,9 @@\n from email.utils import formatdate\n from io import BytesIO\n \n+from twisted.python.versions import Version\n from twisted.internet import defer, ssl\n+from twisted import version as twisted_version\n \n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.python import to_bytes\n@@ -126,16 +128,11 @@ def _sent_failed(self, failure, to, cc, subject, nattachs):\n                       'mailattachs': nattachs, 'mailerr': errstr})\n \n     def _sendmail(self, to_addrs, msg):\n-        # Import twisted.mail here because it is not available in python3\n         from twisted.internet import reactor\n-        from twisted.mail.smtp import ESMTPSenderFactory\n         msg = BytesIO(msg)\n         d = defer.Deferred()\n-        factory = ESMTPSenderFactory(\n-            self.smtpuser, self.smtppass, self.mailfrom, to_addrs, msg, d,\n-            heloFallback=True, requireAuthentication=False, requireTransportSecurity=self.smtptls,\n-        )\n-        factory.noisy = False\n+\n+        factory = self._create_sender_factory(to_addrs, msg, d)\n \n         if self.smtpssl:\n             reactor.connectSSL(self.smtphost, self.smtpport, factory, ssl.ClientContextFactory())\n@@ -143,3 +140,20 @@ def _sendmail(self, to_addrs, msg):\n             reactor.connectTCP(self.smtphost, self.smtpport, factory)\n \n         return d\n+\n+    def _create_sender_factory(self, to_addrs, msg, d):\n+        from twisted.mail.smtp import ESMTPSenderFactory\n+\n+        factory_keywords = {\n+            'heloFallback': True,\n+            'requireAuthentication': False,\n+            'requireTransportSecurity': self.smtptls\n+        }\n+\n+        # Newer versions of twisted require the hostname to use STARTTLS\n+        if twisted_version >= Version('twisted', 21, 2, 0):\n+            factory_keywords['hostname'] = self.smtphost\n+\n+        factory = ESMTPSenderFactory(self.smtpuser, self.smtppass, self.mailfrom, to_addrs, msg, d, **factory_keywords)\n+        factory.noisy = False\n+        return factory\n",
        "test_patch": "diff --git a/tests/test_mail.py b/tests/test_mail.py\nindex 9b248fbfadc..fd02020ee59 100644\n--- a/tests/test_mail.py\n+++ b/tests/test_mail.py\n@@ -4,6 +4,11 @@\n from io import BytesIO\n from email.charset import Charset\n \n+from twisted.internet._sslverify import ClientTLSOptions\n+from twisted.internet.ssl import ClientContextFactory\n+from twisted.python.versions import Version\n+from twisted.internet import defer\n+from twisted import version as twisted_version\n from scrapy.mail import MailSender\n \n \n@@ -121,6 +126,17 @@ def test_send_attach_utf8(self):\n         self.assertEqual(text.get_charset(), Charset('utf-8'))\n         self.assertEqual(attach.get_payload(decode=True).decode('utf-8'), body)\n \n+    def test_create_sender_factory_with_host(self):\n+        mailsender = MailSender(debug=False, smtphost='smtp.testhost.com')\n+\n+        factory = mailsender._create_sender_factory(to_addrs=['test@scrapy.org'], msg='test', d=defer.Deferred())\n+\n+        context = factory.buildProtocol('test@scrapy.org').context\n+        if twisted_version >= Version('twisted', 21, 2, 0):\n+            self.assertIsInstance(context, ClientTLSOptions)\n+        else:\n+            self.assertIsInstance(context, ClientContextFactory)\n+\n \n if __name__ == \"__main__\":\n     unittest.main()\n",
        "problem_statement": "Fix SMTP STARTTLS for Twisted >= 21.2.0\n## Summary\r\n\r\nThe [Mail settings](https://docs.scrapy.org/en/latest/topics/email.html#topics-email-settings) don't have an option to choose a TLS version. Only to enforce upgrading connections to use SSL/TLS.\r\nMail servers like smtp.office365.com dropped support for TLS1.0 and TLS1.1 and now require TLS1.2: https://techcommunity.microsoft.com/t5/exchange-team-blog/new-opt-in-endpoint-available-for-smtp-auth-clients-still/ba-p/2659652 \r\n\r\nIt seems that scrapy mail doesn't support TLS1.2. The error message (with `MAIL_TLS = True`):\r\n\r\n`[scrapy.mail] Unable to send mail: To=['user@gmail.com'] Cc=[] Subject=\"Test\" Attachs=0- 421 b'4.7.66 TLS 1.0 and 1.1 are not supported. Please upgrade/update your client to support TLS 1.2. Visit https://aka.ms/smtp_auth_tls. [AM6P194CA0047.EURP194.PROD.OUTLOOK.COM]'` \r\n\r\n## Motivation\r\n\r\nWithout TLS1.2 it's not possible anymore to send mails via smtp.office365.com. An option to use TLS1.2 would fix this issue\r\n\n",
        "hints_text": "Scrapy doesn't configure protocol versions explicitly when using `ESMTPSenderFactory` and neither does that class, so I would expect it to use global OpenSSL settings, including TLS 1.2 or 1.3 if supported. See https://twistedmatrix.com/documents/current/core/howto/ssl.html\r\n\r\nCan you show the library versions printed by Scrapy?\nIt depends on the version of scrapy and which library version it uses for OpenSSL?\r\nI use scrapy 2.0.1.\r\nScrapy prints the following when starting a job:\r\n`[scrapy.utils.log] Versions: lxml 4.5.0.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.8.2 (default, Feb 26 2020, 15:09:34) - [GCC 8.3.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d 10 Sep 2019), cryptography 2.8, Platform Linux-4.15.0-66-generic-x86_64-with-glibc2.2.5`\r\nThanks for your help!\nI suggest trying newer pyOpenSSL/cryptography/Twisted versions first.\nI upgraded the versions to the newest:\r\n`cryptography==36.0.1\r\npyOpenSSL==22.0.0\r\ntwisted==21.7.0`\r\n\r\nNow I get a new error:\r\n`502 Server does not support secure communication via TLS / SSL`\r\n\r\nSo it definitely looks like the issue is my mail server now. I will close this feature request.\r\nThank you for your quick help\nHi @wRAR I might still need your help.\r\n\r\nThe smtp server supports TLS1.2. I also tested this by creating a script with `smtplib`.\r\nThis script successfully sends my email using TLS1.2:\r\n\r\n```\r\nimport smtplib, ssl\r\n\r\nport = 587\r\nsmtp_server = 'smtp.office365.com'\r\nsender_email = 'test1@test.com'\r\nreceiver_email = 'test2@test.com'\r\nuser = 'test3@test.de'\r\npassword = input('Type your password and press enter:')\r\nmessage = 'test'\r\n\r\ncontext = ssl.create_default_context()\r\ncontext.minimum_version = ssl.TLSVersion.TLSv1_2\r\nwith smtplib.SMTP(smtp_server, port) as server:\r\n    server.ehlo()  # Can be omitted\r\n    server.starttls(context=context)\r\n    server.ehlo()  # Can be omitted\r\n    server.login(user, password)\r\n    server.sendmail(sender_email, receiver_email, message)\r\n```\r\n\r\nI can't seem to get it to work with `scrapy`. I created a new scrapy project with a newer version(`2.5.1`) as well.\r\nThese are all my versions:\r\n`[scrapy.utils.log] INFO: Versions: lxml 4.7.1.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.7.0, Python 3.9.7 (default, Sep  3 2021, 12:37:55) - [Clang 12.0.5 (clang-1205.0.22.9)], pyOpenSSL 22.0.0 (OpenSSL 1.1.1m  14 Dec 2021), cryptography 36.0.1, Platform macOS-11.6.2-x86_64-i386-64bit`\r\n\r\nI still get the error:\r\n`[scrapy.mail] ERROR: Unable to send mail: To=['test@test.com'] Cc=[] Subject=\"tls-test\" Attachs=0- 502 Server does not support secure communication via TLS / SSL`\r\n\r\nDo you have an idea what I could try next?\nAre you using MAIL_TLS and port 587?\nBoth yes\nI'll try to check this later\nHi @wRAR, sorry to bother you. \r\nDo you have some more ideas of what I could try? \r\n\nActually \"Server does not support secure communication via TLS / SSL\" is a client error from Twisted and it's misleading because in fact it's the client that doesn't support STARTTLS.\r\n\r\nSince Twisted 21.2.0 (https://github.com/twisted/twisted/commit/abef1218a9013223ee237d8179b4705cbfd716c0), `ESMTPSenderFactory` needs to be passed `hostname` to use STARTTLS. Unfortunately this isn't documented, but the whole of `ESMTPSenderFactory` isn't, the public interface seems to be `sendmail()` which we don't use.\r\n\r\nThe fix to this seems to be just passing `hostname=self.smtphost` to `ESMTPSenderFactory()` in `MailSender._sendmail()`, but only for Twisted 21.2.0 and above.",
        "created_at": "2022-02-09T20:24:48Z",
        "version": "2.5",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "tests/test_mail.py"
        ]
    },
    {
        "repo": "scrapy/scrapy",
        "pull_number": 5394,
        "instance_id": "scrapy__scrapy-5394",
        "issue_numbers": [
            "5391"
        ],
        "base_commit": "b282a7af012a4804eb91bdd850df3b86065b3fd6",
        "patch": "diff --git a/scrapy/spiders/feed.py b/scrapy/spiders/feed.py\nindex bef2d6b2478..79e12e030a1 100644\n--- a/scrapy/spiders/feed.py\n+++ b/scrapy/spiders/feed.py\n@@ -123,7 +123,7 @@ def parse_rows(self, response):\n         process_results methods for pre and post-processing purposes.\n         \"\"\"\n \n-        for row in csviter(response, self.delimiter, self.headers, self.quotechar):\n+        for row in csviter(response, self.delimiter, self.headers, quotechar=self.quotechar):\n             ret = iterate_spider_output(self.parse_row(response, row))\n             for result_item in self.process_results(response, ret):\n                 yield result_item\n",
        "test_patch": "diff --git a/tests/test_spider.py b/tests/test_spider.py\nindex a7c3ee04871..68934999995 100644\n--- a/tests/test_spider.py\n+++ b/tests/test_spider.py\n@@ -21,6 +21,7 @@\n )\n from scrapy.linkextractors import LinkExtractor\n from scrapy.utils.test import get_crawler\n+from tests import get_testdata\n \n \n class SpiderTest(unittest.TestCase):\n@@ -167,6 +168,23 @@ class CSVFeedSpiderTest(SpiderTest):\n \n     spider_class = CSVFeedSpider\n \n+    def test_parse_rows(self):\n+        body = get_testdata('feeds', 'feed-sample6.csv')\n+        response = Response(\"http://example.org/dummy.csv\", body=body)\n+\n+        class _CrawlSpider(self.spider_class):\n+            name = \"test\"\n+            delimiter = \",\"\n+            quotechar = \"'\"\n+\n+            def parse_row(self, response, row):\n+                return row\n+\n+        spider = _CrawlSpider()\n+        rows = list(spider.parse_rows(response))\n+        assert rows[0] == {'id': '1', 'name': 'alpha', 'value': 'foobar'}\n+        assert len(rows) == 4\n+\n \n class CrawlSpiderTest(SpiderTest):\n \n",
        "problem_statement": "Incorrect initialization of `csviter` in `scrapy.spiders.feed.CSVFeedSpider`\n### Description\r\n\r\nAccording to the master branch, `scrapy.spiders.feed.CSVFeedSpider` initializes csviter like this:\r\n```python\r\nfor row in csviter(response, self.delimiter, self.headers, self.quotechar):\r\n```\r\nhttps://github.com/scrapy/scrapy/blob/master/scrapy/spiders/feed.py#L126\r\n\r\nBut `csviter` definition says:\r\n`def csviter(obj, delimiter=None, headers=None, encoding=None, quotechar=None):`\r\nhttps://github.com/scrapy/scrapy/blob/master/scrapy/utils/iterators.py#L96\r\n\r\nSo effectively it passed quotechar instead of encoding\r\n\r\n### Steps to Reproduce\r\n\r\n1. Check the first link\r\n2. Check the second link\r\n3. [and so on...]\r\n\r\n**Expected behavior:** CSVFeedSpider has a separate setting for the encoding or specify param names when calling csviter\r\n\r\n**Actual behavior:** Mess\r\n\r\n**Reproduces how often:** 100%\r\n\r\n### Versions\r\n\r\n```\r\nScrapy       : 2.5.0\r\nlxml         : 4.6.3.0\r\nlibxml2      : 2.9.10\r\ncssselect    : 1.1.0\r\nparsel       : 1.6.0\r\nw3lib        : 1.22.0\r\nTwisted      : 21.2.0\r\nPython       : 3.9.8 (main, Nov 10 2021, 09:21:22) - [Clang 13.0.0 (clang-1300.0.29.3)]\r\npyOpenSSL    : 20.0.1 (OpenSSL 1.1.1k  25 Mar 2021)\r\ncryptography : 3.4.7\r\nPlatform     : macOS-11.6-x86_64-i386-64bit\r\n```\r\n\r\n### Additional context\r\n\r\nPlease fix it :)\n",
        "hints_text": "",
        "created_at": "2022-02-05T18:27:07Z",
        "version": "2.5",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "tests/test_spider.py::CrawlerProcessSubprocess::test_multi"
        ]
    },
    {
        "repo": "scrapy/scrapy",
        "pull_number": 5374,
        "instance_id": "scrapy__scrapy-5374",
        "issue_numbers": [
            "5366"
        ],
        "base_commit": "9be878fc09cf71bb2cb98695f5042cb344bd2e25",
        "patch": "diff --git a/scrapy/cmdline.py b/scrapy/cmdline.py\nindex 91482ce0156..491c4beabd2 100644\n--- a/scrapy/cmdline.py\n+++ b/scrapy/cmdline.py\n@@ -1,13 +1,13 @@\n import sys\n import os\n-import optparse\n+import argparse\n import cProfile\n import inspect\n import pkg_resources\n \n import scrapy\n from scrapy.crawler import CrawlerProcess\n-from scrapy.commands import ScrapyCommand\n+from scrapy.commands import ScrapyCommand, ScrapyHelpFormatter\n from scrapy.exceptions import UsageError\n from scrapy.utils.misc import walk_modules\n from scrapy.utils.project import inside_project, get_project_settings\n@@ -123,8 +123,6 @@ def execute(argv=None, settings=None):\n     inproject = inside_project()\n     cmds = _get_commands_dict(settings, inproject)\n     cmdname = _pop_command_name(argv)\n-    parser = optparse.OptionParser(formatter=optparse.TitledHelpFormatter(),\n-                                   conflict_handler='resolve')\n     if not cmdname:\n         _print_commands(settings, inproject)\n         sys.exit(0)\n@@ -133,12 +131,14 @@ def execute(argv=None, settings=None):\n         sys.exit(2)\n \n     cmd = cmds[cmdname]\n-    parser.usage = f\"scrapy {cmdname} {cmd.syntax()}\"\n-    parser.description = cmd.long_desc()\n+    parser = argparse.ArgumentParser(formatter_class=ScrapyHelpFormatter,\n+                                     usage=f\"scrapy {cmdname} {cmd.syntax()}\",\n+                                     conflict_handler='resolve',\n+                                     description=cmd.long_desc())\n     settings.setdict(cmd.default_settings, priority='command')\n     cmd.settings = settings\n     cmd.add_options(parser)\n-    opts, args = parser.parse_args(args=argv[1:])\n+    opts, args = parser.parse_known_args(args=argv[1:])\n     _run_print_help(parser, cmd.process_options, args, opts)\n \n     cmd.crawler_process = CrawlerProcess(settings)\ndiff --git a/scrapy/commands/__init__.py b/scrapy/commands/__init__.py\nindex 5f1dabd33f8..fb304b8c0fb 100644\n--- a/scrapy/commands/__init__.py\n+++ b/scrapy/commands/__init__.py\n@@ -2,7 +2,7 @@\n Base class for Scrapy commands\n \"\"\"\n import os\n-from optparse import OptionGroup\n+import argparse\n from typing import Any, Dict\n \n from twisted.python import failure\n@@ -59,22 +59,20 @@ def add_options(self, parser):\n         \"\"\"\n         Populate option parse with options available for this command\n         \"\"\"\n-        group = OptionGroup(parser, \"Global Options\")\n-        group.add_option(\"--logfile\", metavar=\"FILE\",\n-                         help=\"log file. if omitted stderr will be used\")\n-        group.add_option(\"-L\", \"--loglevel\", metavar=\"LEVEL\", default=None,\n-                         help=f\"log level (default: {self.settings['LOG_LEVEL']})\")\n-        group.add_option(\"--nolog\", action=\"store_true\",\n-                         help=\"disable logging completely\")\n-        group.add_option(\"--profile\", metavar=\"FILE\", default=None,\n-                         help=\"write python cProfile stats to FILE\")\n-        group.add_option(\"--pidfile\", metavar=\"FILE\",\n-                         help=\"write process ID to FILE\")\n-        group.add_option(\"-s\", \"--set\", action=\"append\", default=[], metavar=\"NAME=VALUE\",\n-                         help=\"set/override setting (may be repeated)\")\n-        group.add_option(\"--pdb\", action=\"store_true\", help=\"enable pdb on failure\")\n-\n-        parser.add_option_group(group)\n+        group = parser.add_argument_group(title='Global Options')\n+        group.add_argument(\"--logfile\", metavar=\"FILE\",\n+                           help=\"log file. if omitted stderr will be used\")\n+        group.add_argument(\"-L\", \"--loglevel\", metavar=\"LEVEL\", default=None,\n+                           help=f\"log level (default: {self.settings['LOG_LEVEL']})\")\n+        group.add_argument(\"--nolog\", action=\"store_true\",\n+                           help=\"disable logging completely\")\n+        group.add_argument(\"--profile\", metavar=\"FILE\", default=None,\n+                           help=\"write python cProfile stats to FILE\")\n+        group.add_argument(\"--pidfile\", metavar=\"FILE\",\n+                           help=\"write process ID to FILE\")\n+        group.add_argument(\"-s\", \"--set\", action=\"append\", default=[], metavar=\"NAME=VALUE\",\n+                           help=\"set/override setting (may be repeated)\")\n+        group.add_argument(\"--pdb\", action=\"store_true\", help=\"enable pdb on failure\")\n \n     def process_options(self, args, opts):\n         try:\n@@ -114,14 +112,14 @@ class BaseRunSpiderCommand(ScrapyCommand):\n     \"\"\"\n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"-a\", dest=\"spargs\", action=\"append\", default=[], metavar=\"NAME=VALUE\",\n-                          help=\"set spider argument (may be repeated)\")\n-        parser.add_option(\"-o\", \"--output\", metavar=\"FILE\", action=\"append\",\n-                          help=\"append scraped items to the end of FILE (use - for stdout)\")\n-        parser.add_option(\"-O\", \"--overwrite-output\", metavar=\"FILE\", action=\"append\",\n-                          help=\"dump scraped items into FILE, overwriting any existing file\")\n-        parser.add_option(\"-t\", \"--output-format\", metavar=\"FORMAT\",\n-                          help=\"format to use for dumping items\")\n+        parser.add_argument(\"-a\", dest=\"spargs\", action=\"append\", default=[], metavar=\"NAME=VALUE\",\n+                            help=\"set spider argument (may be repeated)\")\n+        parser.add_argument(\"-o\", \"--output\", metavar=\"FILE\", action=\"append\",\n+                            help=\"append scraped items to the end of FILE (use - for stdout)\")\n+        parser.add_argument(\"-O\", \"--overwrite-output\", metavar=\"FILE\", action=\"append\",\n+                            help=\"dump scraped items into FILE, overwriting any existing file\")\n+        parser.add_argument(\"-t\", \"--output-format\", metavar=\"FORMAT\",\n+                            help=\"format to use for dumping items\")\n \n     def process_options(self, args, opts):\n         ScrapyCommand.process_options(self, args, opts)\n@@ -137,3 +135,30 @@ def process_options(self, args, opts):\n                 opts.overwrite_output,\n             )\n             self.settings.set('FEEDS', feeds, priority='cmdline')\n+\n+\n+class ScrapyHelpFormatter(argparse.HelpFormatter):\n+    \"\"\"\n+    Help Formatter for scrapy command line help messages.\n+    \"\"\"\n+    def __init__(self, prog, indent_increment=2, max_help_position=24, width=None):\n+        super().__init__(prog, indent_increment=indent_increment,\n+                         max_help_position=max_help_position, width=width)\n+\n+    def _join_parts(self, part_strings):\n+        parts = self.format_part_strings(part_strings)\n+        return super()._join_parts(parts)\n+\n+    def format_part_strings(self, part_strings):\n+        \"\"\"\n+        Underline and title case command line help message headers.\n+        \"\"\"\n+        if part_strings and part_strings[0].startswith(\"usage: \"):\n+            part_strings[0] = \"Usage\\n=====\\n  \" + part_strings[0][len('usage: '):]\n+        headings = [i for i in range(len(part_strings)) if part_strings[i].endswith(':\\n')]\n+        for index in headings[::-1]:\n+            char = '-' if \"Global Options\" in part_strings[index] else '='\n+            part_strings[index] = part_strings[index][:-2].title()\n+            underline = ''.join([\"\\n\", (char * len(part_strings[index])), \"\\n\"])\n+            part_strings.insert(index + 1, underline)\n+        return part_strings\ndiff --git a/scrapy/commands/check.py b/scrapy/commands/check.py\nindex ae21d86e6a7..a16f4beb7d5 100644\n--- a/scrapy/commands/check.py\n+++ b/scrapy/commands/check.py\n@@ -49,10 +49,10 @@ def short_desc(self):\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"-l\", \"--list\", dest=\"list\", action=\"store_true\",\n-                          help=\"only list contracts, without checking them\")\n-        parser.add_option(\"-v\", \"--verbose\", dest=\"verbose\", default=False, action='store_true',\n-                          help=\"print contract tests for all spiders\")\n+        parser.add_argument(\"-l\", \"--list\", dest=\"list\", action=\"store_true\",\n+                            help=\"only list contracts, without checking them\")\n+        parser.add_argument(\"-v\", \"--verbose\", dest=\"verbose\", default=False, action='store_true',\n+                            help=\"print contract tests for all spiders\")\n \n     def run(self, args, opts):\n         # load contracts\ndiff --git a/scrapy/commands/fetch.py b/scrapy/commands/fetch.py\nindex 95f87e8c388..9b2ebb37fb0 100644\n--- a/scrapy/commands/fetch.py\n+++ b/scrapy/commands/fetch.py\n@@ -26,11 +26,11 @@ def long_desc(self):\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"--spider\", dest=\"spider\", help=\"use this spider\")\n-        parser.add_option(\"--headers\", dest=\"headers\", action=\"store_true\",\n-                          help=\"print response HTTP headers instead of body\")\n-        parser.add_option(\"--no-redirect\", dest=\"no_redirect\", action=\"store_true\", default=False,\n-                          help=\"do not handle HTTP 3xx status codes and print response as-is\")\n+        parser.add_argument(\"--spider\", dest=\"spider\", help=\"use this spider\")\n+        parser.add_argument(\"--headers\", dest=\"headers\", action=\"store_true\",\n+                            help=\"print response HTTP headers instead of body\")\n+        parser.add_argument(\"--no-redirect\", dest=\"no_redirect\", action=\"store_true\", default=False,\n+                            help=\"do not handle HTTP 3xx status codes and print response as-is\")\n \n     def _print_headers(self, headers, prefix):\n         for key, values in headers.items():\ndiff --git a/scrapy/commands/genspider.py b/scrapy/commands/genspider.py\nindex 2082a4974bb..ed5f588e92b 100644\n--- a/scrapy/commands/genspider.py\n+++ b/scrapy/commands/genspider.py\n@@ -44,16 +44,16 @@ def short_desc(self):\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"-l\", \"--list\", dest=\"list\", action=\"store_true\",\n-                          help=\"List available templates\")\n-        parser.add_option(\"-e\", \"--edit\", dest=\"edit\", action=\"store_true\",\n-                          help=\"Edit spider after creating it\")\n-        parser.add_option(\"-d\", \"--dump\", dest=\"dump\", metavar=\"TEMPLATE\",\n-                          help=\"Dump template to standard output\")\n-        parser.add_option(\"-t\", \"--template\", dest=\"template\", default=\"basic\",\n-                          help=\"Uses a custom template.\")\n-        parser.add_option(\"--force\", dest=\"force\", action=\"store_true\",\n-                          help=\"If the spider already exists, overwrite it with the template\")\n+        parser.add_argument(\"-l\", \"--list\", dest=\"list\", action=\"store_true\",\n+                            help=\"List available templates\")\n+        parser.add_argument(\"-e\", \"--edit\", dest=\"edit\", action=\"store_true\",\n+                            help=\"Edit spider after creating it\")\n+        parser.add_argument(\"-d\", \"--dump\", dest=\"dump\", metavar=\"TEMPLATE\",\n+                            help=\"Dump template to standard output\")\n+        parser.add_argument(\"-t\", \"--template\", dest=\"template\", default=\"basic\",\n+                            help=\"Uses a custom template.\")\n+        parser.add_argument(\"--force\", dest=\"force\", action=\"store_true\",\n+                            help=\"If the spider already exists, overwrite it with the template\")\n \n     def run(self, args, opts):\n         if opts.list:\ndiff --git a/scrapy/commands/parse.py b/scrapy/commands/parse.py\nindex 52118db1b3b..a3f6b96f420 100644\n--- a/scrapy/commands/parse.py\n+++ b/scrapy/commands/parse.py\n@@ -32,28 +32,28 @@ def short_desc(self):\n \n     def add_options(self, parser):\n         BaseRunSpiderCommand.add_options(self, parser)\n-        parser.add_option(\"--spider\", dest=\"spider\", default=None,\n-                          help=\"use this spider without looking for one\")\n-        parser.add_option(\"--pipelines\", action=\"store_true\",\n-                          help=\"process items through pipelines\")\n-        parser.add_option(\"--nolinks\", dest=\"nolinks\", action=\"store_true\",\n-                          help=\"don't show links to follow (extracted requests)\")\n-        parser.add_option(\"--noitems\", dest=\"noitems\", action=\"store_true\",\n-                          help=\"don't show scraped items\")\n-        parser.add_option(\"--nocolour\", dest=\"nocolour\", action=\"store_true\",\n-                          help=\"avoid using pygments to colorize the output\")\n-        parser.add_option(\"-r\", \"--rules\", dest=\"rules\", action=\"store_true\",\n-                          help=\"use CrawlSpider rules to discover the callback\")\n-        parser.add_option(\"-c\", \"--callback\", dest=\"callback\",\n-                          help=\"use this callback for parsing, instead looking for a callback\")\n-        parser.add_option(\"-m\", \"--meta\", dest=\"meta\",\n-                          help=\"inject extra meta into the Request, it must be a valid raw json string\")\n-        parser.add_option(\"--cbkwargs\", dest=\"cbkwargs\",\n-                          help=\"inject extra callback kwargs into the Request, it must be a valid raw json string\")\n-        parser.add_option(\"-d\", \"--depth\", dest=\"depth\", type=\"int\", default=1,\n-                          help=\"maximum depth for parsing requests [default: %default]\")\n-        parser.add_option(\"-v\", \"--verbose\", dest=\"verbose\", action=\"store_true\",\n-                          help=\"print each depth level one by one\")\n+        parser.add_argument(\"--spider\", dest=\"spider\", default=None,\n+                            help=\"use this spider without looking for one\")\n+        parser.add_argument(\"--pipelines\", action=\"store_true\",\n+                            help=\"process items through pipelines\")\n+        parser.add_argument(\"--nolinks\", dest=\"nolinks\", action=\"store_true\",\n+                            help=\"don't show links to follow (extracted requests)\")\n+        parser.add_argument(\"--noitems\", dest=\"noitems\", action=\"store_true\",\n+                            help=\"don't show scraped items\")\n+        parser.add_argument(\"--nocolour\", dest=\"nocolour\", action=\"store_true\",\n+                            help=\"avoid using pygments to colorize the output\")\n+        parser.add_argument(\"-r\", \"--rules\", dest=\"rules\", action=\"store_true\",\n+                            help=\"use CrawlSpider rules to discover the callback\")\n+        parser.add_argument(\"-c\", \"--callback\", dest=\"callback\",\n+                            help=\"use this callback for parsing, instead looking for a callback\")\n+        parser.add_argument(\"-m\", \"--meta\", dest=\"meta\",\n+                            help=\"inject extra meta into the Request, it must be a valid raw json string\")\n+        parser.add_argument(\"--cbkwargs\", dest=\"cbkwargs\",\n+                            help=\"inject extra callback kwargs into the Request, it must be a valid raw json string\")\n+        parser.add_argument(\"-d\", \"--depth\", dest=\"depth\", type=int, default=1,\n+                            help=\"maximum depth for parsing requests [default: %default]\")\n+        parser.add_argument(\"-v\", \"--verbose\", dest=\"verbose\", action=\"store_true\",\n+                            help=\"print each depth level one by one\")\n \n     @property\n     def max_level(self):\ndiff --git a/scrapy/commands/settings.py b/scrapy/commands/settings.py\nindex 8d49e440fa1..1b2e2601e2a 100644\n--- a/scrapy/commands/settings.py\n+++ b/scrapy/commands/settings.py\n@@ -18,16 +18,16 @@ def short_desc(self):\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"--get\", dest=\"get\", metavar=\"SETTING\",\n-                          help=\"print raw setting value\")\n-        parser.add_option(\"--getbool\", dest=\"getbool\", metavar=\"SETTING\",\n-                          help=\"print setting value, interpreted as a boolean\")\n-        parser.add_option(\"--getint\", dest=\"getint\", metavar=\"SETTING\",\n-                          help=\"print setting value, interpreted as an integer\")\n-        parser.add_option(\"--getfloat\", dest=\"getfloat\", metavar=\"SETTING\",\n-                          help=\"print setting value, interpreted as a float\")\n-        parser.add_option(\"--getlist\", dest=\"getlist\", metavar=\"SETTING\",\n-                          help=\"print setting value, interpreted as a list\")\n+        parser.add_argument(\"--get\", dest=\"get\", metavar=\"SETTING\",\n+                            help=\"print raw setting value\")\n+        parser.add_argument(\"--getbool\", dest=\"getbool\", metavar=\"SETTING\",\n+                            help=\"print setting value, interpreted as a boolean\")\n+        parser.add_argument(\"--getint\", dest=\"getint\", metavar=\"SETTING\",\n+                            help=\"print setting value, interpreted as an integer\")\n+        parser.add_argument(\"--getfloat\", dest=\"getfloat\", metavar=\"SETTING\",\n+                            help=\"print setting value, interpreted as a float\")\n+        parser.add_argument(\"--getlist\", dest=\"getlist\", metavar=\"SETTING\",\n+                            help=\"print setting value, interpreted as a list\")\n \n     def run(self, args, opts):\n         settings = self.crawler_process.settings\ndiff --git a/scrapy/commands/shell.py b/scrapy/commands/shell.py\nindex de81986d8cf..f67a5886a37 100644\n--- a/scrapy/commands/shell.py\n+++ b/scrapy/commands/shell.py\n@@ -33,12 +33,12 @@ def long_desc(self):\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"-c\", dest=\"code\",\n-                          help=\"evaluate the code in the shell, print the result and exit\")\n-        parser.add_option(\"--spider\", dest=\"spider\",\n-                          help=\"use this spider\")\n-        parser.add_option(\"--no-redirect\", dest=\"no_redirect\", action=\"store_true\", default=False,\n-                          help=\"do not handle HTTP 3xx status codes and print response as-is\")\n+        parser.add_argument(\"-c\", dest=\"code\",\n+                            help=\"evaluate the code in the shell, print the result and exit\")\n+        parser.add_argument(\"--spider\", dest=\"spider\",\n+                            help=\"use this spider\")\n+        parser.add_argument(\"--no-redirect\", dest=\"no_redirect\", action=\"store_true\", default=False,\n+                            help=\"do not handle HTTP 3xx status codes and print response as-is\")\n \n     def update_vars(self, vars):\n         \"\"\"You can use this function to update the Scrapy objects that will be\ndiff --git a/scrapy/commands/version.py b/scrapy/commands/version.py\nindex 1237610cbed..c6a3c273af4 100644\n--- a/scrapy/commands/version.py\n+++ b/scrapy/commands/version.py\n@@ -16,8 +16,8 @@ def short_desc(self):\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"--verbose\", \"-v\", dest=\"verbose\", action=\"store_true\",\n-                          help=\"also display twisted/python/platform info (useful for bug reports)\")\n+        parser.add_argument(\"--verbose\", \"-v\", dest=\"verbose\", action=\"store_true\",\n+                            help=\"also display twisted/python/platform info (useful for bug reports)\")\n \n     def run(self, args, opts):\n         if opts.verbose:\ndiff --git a/scrapy/commands/view.py b/scrapy/commands/view.py\nindex c8f873334a8..b1f52abe206 100644\n--- a/scrapy/commands/view.py\n+++ b/scrapy/commands/view.py\n@@ -1,3 +1,4 @@\n+import argparse\n from scrapy.commands import fetch\n from scrapy.utils.response import open_in_browser\n \n@@ -12,7 +13,7 @@ def long_desc(self):\n \n     def add_options(self, parser):\n         super().add_options(parser)\n-        parser.remove_option(\"--headers\")\n+        parser.add_argument('--headers', help=argparse.SUPPRESS)\n \n     def _print_response(self, response, opts):\n         open_in_browser(response)\n",
        "test_patch": "diff --git a/tests/test_command_parse.py b/tests/test_command_parse.py\nindex ed3848d8832..f21ee971d38 100644\n--- a/tests/test_command_parse.py\n+++ b/tests/test_command_parse.py\n@@ -1,6 +1,9 @@\n import os\n+import argparse\n from os.path import join, abspath, isfile, exists\n from twisted.internet import defer\n+from scrapy.commands import parse\n+from scrapy.settings import Settings\n from scrapy.utils.testsite import SiteTest\n from scrapy.utils.testproc import ProcessTest\n from scrapy.utils.python import to_unicode\n@@ -239,3 +242,19 @@ def test_output_flag(self):\n         content = '[\\n{},\\n{\"foo\": \"bar\"}\\n]'\n         with open(file_path, 'r') as f:\n             self.assertEqual(f.read(), content)\n+\n+    def test_parse_add_options(self):\n+        command = parse.Command()\n+        command.settings = Settings()\n+        parser = argparse.ArgumentParser(\n+            prog='scrapy', formatter_class=argparse.HelpFormatter,\n+            conflict_handler='resolve', prefix_chars='-'\n+        )\n+        command.add_options(parser)\n+        namespace = parser.parse_args(\n+            ['--verbose', '--nolinks', '-d', '2', '--spider', self.spider_name]\n+        )\n+        self.assertTrue(namespace.nolinks)\n+        self.assertEqual(namespace.depth, 2)\n+        self.assertEqual(namespace.spider, self.spider_name)\n+        self.assertTrue(namespace.verbose)\ndiff --git a/tests/test_commands.py b/tests/test_commands.py\nindex 7473b53dfec..7cd19b29ae0 100644\n--- a/tests/test_commands.py\n+++ b/tests/test_commands.py\n@@ -1,6 +1,6 @@\n import inspect\n import json\n-import optparse\n+import argparse\n import os\n import platform\n import re\n@@ -23,7 +23,7 @@\n from twisted.trial import unittest\n \n import scrapy\n-from scrapy.commands import ScrapyCommand\n+from scrapy.commands import view, ScrapyCommand, ScrapyHelpFormatter\n from scrapy.commands.startproject import IGNORE\n from scrapy.settings import Settings\n from scrapy.utils.python import to_unicode\n@@ -37,19 +37,28 @@ class CommandSettings(unittest.TestCase):\n     def setUp(self):\n         self.command = ScrapyCommand()\n         self.command.settings = Settings()\n-        self.parser = optparse.OptionParser(\n-            formatter=optparse.TitledHelpFormatter(),\n-            conflict_handler='resolve',\n-        )\n+        self.parser = argparse.ArgumentParser(formatter_class=ScrapyHelpFormatter,\n+                                              conflict_handler='resolve')\n         self.command.add_options(self.parser)\n \n     def test_settings_json_string(self):\n         feeds_json = '{\"data.json\": {\"format\": \"json\"}, \"data.xml\": {\"format\": \"xml\"}}'\n-        opts, args = self.parser.parse_args(args=['-s', f'FEEDS={feeds_json}', 'spider.py'])\n+        opts, args = self.parser.parse_known_args(args=['-s', f'FEEDS={feeds_json}', 'spider.py'])\n         self.command.process_options(args, opts)\n         self.assertIsInstance(self.command.settings['FEEDS'], scrapy.settings.BaseSettings)\n         self.assertEqual(dict(self.command.settings['FEEDS']), json.loads(feeds_json))\n \n+    def test_help_formatter(self):\n+        formatter = ScrapyHelpFormatter(prog='scrapy')\n+        part_strings = ['usage: scrapy genspider [options] <name> <domain>\\n\\n',\n+                        '\\n', 'optional arguments:\\n', '\\n', 'Global Options:\\n']\n+        self.assertEqual(\n+            formatter._join_parts(part_strings),\n+            ('Usage\\n=====\\n  scrapy genspider [options] <name> <domain>\\n\\n\\n'\n+             'Optional Arguments\\n==================\\n\\n'\n+             'Global Options\\n--------------\\n')\n+        )\n+\n \n class ProjectTest(unittest.TestCase):\n     project_name = 'testproject'\n@@ -812,6 +821,21 @@ def test_run(self):\n         self.assertNotIn('Unhandled Error', log)\n \n \n+class ViewCommandTest(CommandTest):\n+\n+    def test_methods(self):\n+        command = view.Command()\n+        command.settings = Settings()\n+        parser = argparse.ArgumentParser(prog='scrapy', prefix_chars='-',\n+                                         formatter_class=ScrapyHelpFormatter,\n+                                         conflict_handler='resolve')\n+        command.add_options(parser)\n+        self.assertEqual(command.short_desc(),\n+                         \"Open URL in browser, as seen by Scrapy\")\n+        self.assertIn(\"URL using the Scrapy downloader and show its\",\n+                      command.long_desc())\n+\n+\n class CrawlCommandTest(CommandTest):\n \n     def crawl(self, code, args=()):\n",
        "problem_statement": "Move from optparse to argparse\nThe [optparse](https://docs.python.org/3/library/optparse.html) module has been deprecated since `3.2` in lieu of [argparse](https://docs.python.org/3/library/argparse.html).\r\n\r\nAt the time of this writing, we only have 3 modules that use it directly:\r\n\r\n- https://github.com/scrapy/scrapy/blob/master/scrapy/cmdline.py\r\n- https://github.com/scrapy/scrapy/blob/master/scrapy/commands/__init__.py\r\n- https://github.com/scrapy/scrapy/blob/master/tests/test_commands.py\r\n\r\nI'm filing this ticket to keep track of this task. In addition there were several attempts on this in the past that hasn't pushed through:\r\n\r\n- https://github.com/scrapy/scrapy/pull/1118\r\n- https://github.com/scrapy/scrapy/pull/829\n",
        "hints_text": "Can I work on this, please?\nHi @Gallaecio I am new in the open source world\r\ncan I work on this? if this is not available let me know if anything else is there from which I can start my Journey\n@kawishqayyum @LusiferAjay you don't need a permission to work on any issue\nHello, I would like to work on this issue. How should I go about solving this issue?",
        "created_at": "2022-01-22T07:10:57Z",
        "version": "2.5",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "tests/test_command_parse.py",
            "tests/test_commands.py"
        ]
    },
    {
        "repo": "scrapy/scrapy",
        "pull_number": 5320,
        "instance_id": "scrapy__scrapy-5320",
        "issue_numbers": [
            "5319"
        ],
        "base_commit": "28eba610e22c0d2a42e830b4e64746edf44598f9",
        "patch": "diff --git a/scrapy/utils/response.py b/scrapy/utils/response.py\nindex b3ef7b4637f..8b109dced2a 100644\n--- a/scrapy/utils/response.py\n+++ b/scrapy/utils/response.py\n@@ -3,8 +3,9 @@\n scrapy.http.Response objects\n \"\"\"\n import os\n-import webbrowser\n+import re\n import tempfile\n+import webbrowser\n from typing import Any, Callable, Iterable, Optional, Tuple, Union\n from weakref import WeakKeyDictionary\n \n@@ -80,8 +81,9 @@ def open_in_browser(\n     body = response.body\n     if isinstance(response, HtmlResponse):\n         if b'<base' not in body:\n-            repl = f'<head><base href=\"{response.url}\">'\n-            body = body.replace(b'<head>', to_bytes(repl))\n+            repl = fr'\\1<base href=\"{response.url}\">'\n+            body = re.sub(b\"<!--.*?-->\", b\"\", body, flags=re.DOTALL)\n+            body = re.sub(rb\"(<head(?:>|\\s.*?>))\", to_bytes(repl), body)\n         ext = '.html'\n     elif isinstance(response, TextResponse):\n         ext = '.txt'\n",
        "test_patch": "diff --git a/tests/test_utils_response.py b/tests/test_utils_response.py\nindex d6f4c0bb59e..0a09f610927 100644\n--- a/tests/test_utils_response.py\n+++ b/tests/test_utils_response.py\n@@ -83,3 +83,56 @@ def test_response_status_message(self):\n         self.assertEqual(response_status_message(200), '200 OK')\n         self.assertEqual(response_status_message(404), '404 Not Found')\n         self.assertEqual(response_status_message(573), \"573 Unknown Status\")\n+\n+    def test_inject_base_url(self):\n+        url = \"http://www.example.com\"\n+\n+        def check_base_url(burl):\n+            path = urlparse(burl).path\n+            if not os.path.exists(path):\n+                path = burl.replace('file://', '')\n+            with open(path, \"rb\") as f:\n+                bbody = f.read()\n+            self.assertEqual(bbody.count(b'<base href=\"' + to_bytes(url) + b'\">'), 1)\n+            return True\n+\n+        r1 = HtmlResponse(url, body=b\"\"\"\n+        <html>\n+            <head><title>Dummy</title></head>\n+            <body><p>Hello world.</p></body>\n+        </html>\"\"\")\n+        r2 = HtmlResponse(url, body=b\"\"\"\n+        <html>\n+            <head id=\"foo\"><title>Dummy</title></head>\n+            <body>Hello world.</body>\n+        </html>\"\"\")\n+        r3 = HtmlResponse(url, body=b\"\"\"\n+        <html>\n+            <head><title>Dummy</title></head>\n+            <body>\n+                <header>Hello header</header>\n+                <p>Hello world.</p>\n+            </body>\n+        </html>\"\"\")\n+        r4 = HtmlResponse(url, body=b\"\"\"\n+        <html>\n+            <!-- <head>Dummy comment</head> -->\n+            <head><title>Dummy</title></head>\n+            <body><p>Hello world.</p></body>\n+        </html>\"\"\")\n+        r5 = HtmlResponse(url, body=b\"\"\"\n+        <html>\n+            <!--[if IE]>\n+            <head><title>IE head</title></head>\n+            <![endif]-->\n+            <!--[if !IE]>-->\n+            <head><title>Standard head</title></head>\n+            <!--<![endif]-->\n+            <body><p>Hello world.</p></body>\n+        </html>\"\"\")\n+\n+        assert open_in_browser(r1, _openfunc=check_base_url), \"Inject base url\"\n+        assert open_in_browser(r2, _openfunc=check_base_url), \"Inject base url with argumented head\"\n+        assert open_in_browser(r3, _openfunc=check_base_url), \"Inject unique base url with misleading tag\"\n+        assert open_in_browser(r4, _openfunc=check_base_url), \"Inject unique base url with misleading comment\"\n+        assert open_in_browser(r5, _openfunc=check_base_url), \"Inject unique base url with conditional comment\"\n",
        "problem_statement": "Open in Browser `<base>` replacement will fail if `<head>` has attributes\n### Description\r\n\r\nWhen using `open_in_browser()` feature, Scrapy will try to add a `<base>` tag to ensure remote resources are loaded, and to make external links to work in our local browser. This feature rely on the following code:\r\n\r\nhttps://github.com/scrapy/scrapy/blob/06f3d12c1208c380f9f1a16cb36ba2dfa3c244c5/scrapy/utils/response.py#L81-L84\r\n\r\nSome website are using attributes on the `<head>` tag, which will prevent the `<base>` tag to be injected, and therefore external resources to be loaded.\r\n\r\n### How to reproduce the issue \r\n\r\nSimply create a basic spider [following Scrapy tutorial](https://docs.scrapy.org/en/latest/intro/tutorial.html) and use the following code:\r\n\r\n```py\r\nimport scrapy\r\nfrom scrapy.utils.response import open_in_browser\r\n\r\nclass ExampleSpider(scrapy.Spider):\r\n    name = 'example'\r\n    allowed_domains = ['example.com']\r\n    start_urls = [\r\n        'https://example.com/head-without-argument.html', \r\n        'https://example.com/head-with-argument.html']\r\n\r\n    def parse(self, response):\r\n        open_in_browser(response)\r\n        pass\r\n```\r\n\r\nFor the scrapped pages itselves, use the simplest code possible (I've not been able to quickly find a public page using arguments on `<head>`, sorry):\r\n\r\n```html\r\n<!DOCTYPE html>\r\n<html>\r\n  <!-- head-without-argument.html -->\r\n  <head>\r\n    <title>Title</title>\r\n  </head>\r\n  <body>\r\n    <p>Foo</p>\r\n    <img src=\"./assets/image.jpg\">\r\n  </body>\r\n</html>\r\n```\r\n\r\n<!-- -->\r\n\r\n```html\r\n<!DOCTYPE html>\r\n<html>\r\n  <!-- head-with-argument.html -->\r\n  <head id=\"example\">\r\n    <title>Title</title>\r\n  </head>\r\n  <body>\r\n    <p>Foo</p>\r\n    <img src=\"./assets/image.jpg\">\r\n  </body>\r\n</html>\r\n```\r\n\r\nThen run the spider with `scrapy crawl example` and you'll see that:\r\n1. `head-without-argument.html` output renders resource correctly\r\n2. `head-with-argument.html` output doesn't render resource\r\n\r\n### How to fix the issue\r\n\r\nAt the very least, the literal `replace()` function should be replace by a regex replacement:\r\n```py\r\n if isinstance(response, HtmlResponse): \r\n     if b'<base' not in body: \r\n         repl = f'\\\\1<base href=\"{response.url}\">' \r\n         body = re.sub(b\"(<head.*?>)\", to_bytes(repl), body)\r\n```\r\n\r\n### Environment \r\n```\r\nScrapy       : 2.5.1\r\nlxml         : 4.6.3.0\r\nlibxml2      : 2.9.4\r\ncssselect    : 1.1.0\r\nparsel       : 1.6.0\r\nw3lib        : 1.22.0\r\nTwisted      : 21.7.0\r\nPython       : 3.9.7 (default, Sep  3 2021, 04:31:11) - [Clang 12.0.5 (clang-1205.0.22.9)]\r\npyOpenSSL    : 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021)\r\ncryptography : 35.0.0\r\nPlatform     : macOS-11.6-arm64-arm-64bit\r\n```\n",
        "hints_text": "",
        "created_at": "2021-11-15T10:15:57Z",
        "version": "2.5",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "tests/test_utils_response.py"
        ]
    }
]