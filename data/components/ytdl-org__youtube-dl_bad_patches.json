[
  {
    "repo": "ytdl-org/youtube-dl",
    "pull_number": 32987,
    "instance_id": "ytdl-org__youtube-dl-32987",
    "issue_numbers": [
      "32986"
    ],
    "base_commit": "c5098961b04ce83f4615f2a846c84f803b072639",
    "patch": "diff --git a/youtube_dl/extractor/common.py b/youtube_dl/extractor/common.py\nindex 9b0016d07ec..78704b55718 100644\n--- a/youtube_dl/extractor/common.py\n+++ b/youtube_dl/extractor/common.py\n@@ -3170,7 +3170,7 @@ def _parse_jwplayer_formats(self, jwplayer_sources_data, video_id=None,\n                     # See com/longtailvideo/jwplayer/media/RTMPMediaProvider.as\n                     # of jwplayer.flash.swf\n                     rtmp_url_parts = re.split(\n-                        r'((?:mp4|mp3|flv):)', source_url, 1)\n+                        r'((?:mp4|mp3|flv):)', source_url, maxsplit=1)\n                     if len(rtmp_url_parts) == 3:\n                         rtmp_url, prefix, play_path = rtmp_url_parts\n                         a_format.update({\ndiff --git a/youtube_dl/extractor/youtube.py b/youtube_dl/extractor/youtube.py\nindex 6fe520e9a44..1f83acf7cbf 100644\n--- a/youtube_dl/extractor/youtube.py\n+++ b/youtube_dl/extractor/youtube.py\n@@ -3,11 +3,13 @@\n from __future__ import unicode_literals\n \n import collections\n+import hashlib\n import itertools\n import json\n import os.path\n import random\n import re\n+import time\n import traceback\n \n from .common import InfoExtractor, SearchInfoExtractor\n@@ -290,6 +292,33 @@ def _real_initialize(self):\n     _YT_INITIAL_PLAYER_RESPONSE_RE = r'ytInitialPlayerResponse\\s*=\\s*({.+?})\\s*;'\n     _YT_INITIAL_BOUNDARY_RE = r'(?:var\\s+meta|</script|\\n)'\n \n+    _SAPISID = None\n+\n+    def _generate_sapisidhash_header(self, origin='https://www.youtube.com'):\n+        time_now = round(time.time())\n+        if self._SAPISID is None:\n+            yt_cookies = self._get_cookies('https://www.youtube.com')\n+            # Sometimes SAPISID cookie isn't present but __Secure-3PAPISID is.\n+            # See: https://github.com/yt-dlp/yt-dlp/issues/393\n+            sapisid_cookie = dict_get(\n+                yt_cookies, ('__Secure-3PAPISID', 'SAPISID'))\n+            if sapisid_cookie and sapisid_cookie.value:\n+                self._SAPISID = sapisid_cookie.value\n+                self.write_debug('Extracted SAPISID cookie')\n+                # SAPISID cookie is required if not already present\n+                if not yt_cookies.get('SAPISID'):\n+                    self.write_debug('Copying __Secure-3PAPISID cookie to SAPISID cookie')\n+                    self._set_cookie(\n+                        '.youtube.com', 'SAPISID', self._SAPISID, secure=True, expire_time=time_now + 3600)\n+            else:\n+                self._SAPISID = False\n+        if not self._SAPISID:\n+            return None\n+        # SAPISIDHASH algorithm from https://stackoverflow.com/a/32065323\n+        sapisidhash = hashlib.sha1(\n+            '{0} {1} {2}'.format(time_now, self._SAPISID, origin).encode('utf-8')).hexdigest()\n+        return 'SAPISIDHASH {0}_{1}'.format(time_now, sapisidhash)\n+\n     def _call_api(self, ep, query, video_id, fatal=True, headers=None):\n         data = self._DEFAULT_API_DATA.copy()\n         data.update(query)\n@@ -1579,20 +1608,27 @@ def _genslice(start, end, step):\n         self.to_screen('Extracted signature function:\\n' + code)\n \n     def _parse_sig_js(self, jscode):\n+        # Examples where `sig` is funcname:\n+        # sig=function(a){a=a.split(\"\"); ... ;return a.join(\"\")};\n+        # ;c&&(c=sig(decodeURIComponent(c)),a.set(b,encodeURIComponent(c)));return a};\n+        # {var l=f,m=h.sp,n=sig(decodeURIComponent(h.s));l.set(m,encodeURIComponent(n))}\n+        # sig=function(J){J=J.split(\"\"); ... ;return J.join(\"\")};\n+        # ;N&&(N=sig(decodeURIComponent(N)),J.set(R,encodeURIComponent(N)));return J};\n+        # {var H=u,k=f.sp,v=sig(decodeURIComponent(f.s));H.set(k,encodeURIComponent(v))}\n         funcname = self._search_regex(\n-            (r'\\b[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*encodeURIComponent\\s*\\(\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'\\b[a-zA-Z0-9]+\\s*&&\\s*[a-zA-Z0-9]+\\.set\\([^,]+\\s*,\\s*encodeURIComponent\\s*\\(\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'\\bm=(?P<sig>[a-zA-Z0-9$]{2,})\\(decodeURIComponent\\(h\\.s\\)\\)',\n-             r'\\bc&&\\(c=(?P<sig>[a-zA-Z0-9$]{2,})\\(decodeURIComponent\\(c\\)\\)',\n-             r'(?:\\b|[^a-zA-Z0-9$])(?P<sig>[a-zA-Z0-9$]{2,})\\s*=\\s*function\\(\\s*a\\s*\\)\\s*{\\s*a\\s*=\\s*a\\.split\\(\\s*\"\"\\s*\\)(?:;[a-zA-Z0-9$]{2}\\.[a-zA-Z0-9$]{2}\\(a,\\d+\\))?',\n-             r'(?P<sig>[a-zA-Z0-9$]+)\\s*=\\s*function\\(\\s*a\\s*\\)\\s*{\\s*a\\s*=\\s*a\\.split\\(\\s*\"\"\\s*\\)',\n+            (r'\\b(?P<var>[\\w$]+)&&\\((?P=var)=(?P<sig>[\\w$]{2,})\\(decodeURIComponent\\((?P=var)\\)\\)',\n+             r'(?P<sig>[\\w$]+)\\s*=\\s*function\\(\\s*(?P<arg>[\\w$]+)\\s*\\)\\s*{\\s*(?P=arg)\\s*=\\s*(?P=arg)\\.split\\(\\s*\"\"\\s*\\)\\s*;\\s*[^}]+;\\s*return\\s+(?P=arg)\\.join\\(\\s*\"\"\\s*\\)',\n+             r'(?:\\b|[^\\w$])(?P<sig>[\\w$]{2,})\\s*=\\s*function\\(\\s*a\\s*\\)\\s*{\\s*a\\s*=\\s*a\\.split\\(\\s*\"\"\\s*\\)(?:;[\\w$]{2}\\.[\\w$]{2}\\(a,\\d+\\))?',\n+             # Old patterns\n+             r'\\b[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*encodeURIComponent\\s*\\(\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\b[\\w]+\\s*&&\\s*[\\w]+\\.set\\([^,]+\\s*,\\s*encodeURIComponent\\s*\\(\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\bm=(?P<sig>[\\w$]{2,})\\(decodeURIComponent\\(h\\.s\\)\\)',\n              # Obsolete patterns\n-             r'(\"|\\')signature\\1\\s*,\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'\\.sig\\|\\|(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'yt\\.akamaized\\.net/\\)\\s*\\|\\|\\s*.*?\\s*[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?:encodeURIComponent\\s*\\()?\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'\\b[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'\\b[a-zA-Z0-9]+\\s*&&\\s*[a-zA-Z0-9]+\\.set\\([^,]+\\s*,\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'\\bc\\s*&&\\s*[a-zA-Z0-9]+\\.set\\([^,]+\\s*,\\s*\\([^)]*\\)\\s*\\(\\s*(?P<sig>[a-zA-Z0-9$]+)\\('),\n+             r'(\"|\\')signature\\1\\s*,\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\.sig\\|\\|(?P<sig>[\\w$]+)\\(',\n+             r'yt\\.akamaized\\.net/\\)\\s*\\|\\|\\s*.*?\\s*[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?:encodeURIComponent\\s*\\()?\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\b[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\bc\\s*&&\\s*[\\w]+\\.set\\([^,]+\\s*,\\s*\\([^)]*\\)\\s*\\(\\s*(?P<sig>[\\w$]+)\\('),\n             jscode, 'Initial JS player signature function name', group='sig')\n \n         jsi = JSInterpreter(jscode)\n@@ -1658,36 +1694,29 @@ def _decrypt_nsig(self, n, video_id, player_url):\n \n     def _extract_n_function_name(self, jscode):\n         func_name, idx = self._search_regex(\n-            # new: (b=String.fromCharCode(110),c=a.get(b))&&c=nfunc[idx](c)\n-            # or:  (b=\"nn\"[+a.D],c=a.get(b))&&(c=nfunc[idx](c)\n-            # or:  (PL(a),b=a.j.n||null)&&(b=nfunc[idx](b)\n+            # (y=NuD(),Mw(k),q=k.Z[y]||null)&&(q=narray[idx](q),k.set(y,q),k.V||NuD(''))}};\n+            # (R=\"nn\"[+J.Z],mW(J),N=J.K[R]||null)&&(N=narray[idx](N),J.set(R,N))}};\n+            # or:  (b=String.fromCharCode(110),c=a.get(b))&&c=narray[idx](c)\n+            # or:  (b=\"nn\"[+a.D],c=a.get(b))&&(c=narray[idx](c)\n+            # or:  (PL(a),b=a.j.n||null)&&(b=narray[idx](b)\n             # or:  (b=\"nn\"[+a.D],vL(a),c=a.j[b]||null)&&(c=narray[idx](c),a.set(b,c),narray.length||nfunc(\"\")\n-            # old: (b=a.get(\"n\"))&&(b=nfunc[idx](b)(?P<c>[a-z])\\s*=\\s*[a-z]\\s*\n+            # old: (b=a.get(\"n\"))&&(b=narray[idx](b)(?P<c>[a-z])\\s*=\\s*[a-z]\\s*\n             # older: (b=a.get(\"n\"))&&(b=nfunc(b)\n             r'''(?x)\n-                \\((?:[\\w$()\\s]+,)*?\\s*      # (\n-                (?P<b>[a-z])\\s*=\\s*         # b=\n-                (?:\n-                    (?:                     # expect ,c=a.get(b) (etc)\n-                        String\\s*\\.\\s*fromCharCode\\s*\\(\\s*110\\s*\\)|\n-                        \"n+\"\\[\\s*\\+?s*[\\w$.]+\\s*]\n-                    )\\s*(?:,[\\w$()\\s]+(?=,))*|\n-                       (?P<old>[\\w$]+)      # a (old[er])\n-                   )\\s*\n-                   (?(old)\n-                                            # b.get(\"n\")\n-                       (?:\\.\\s*[\\w$]+\\s*|\\[\\s*[\\w$]+\\s*]\\s*)*?\n-                       (?:\\.\\s*n|\\[\\s*\"n\"\\s*]|\\.\\s*get\\s*\\(\\s*\"n\"\\s*\\))\n-                       |                    # ,c=a.get(b)\n-                       ,\\s*(?P<c>[a-z])\\s*=\\s*[a-z]\\s*\n-                       (?:\\.\\s*[\\w$]+\\s*|\\[\\s*[\\w$]+\\s*]\\s*)*?\n-                       (?:\\[\\s*(?P=b)\\s*]|\\.\\s*get\\s*\\(\\s*(?P=b)\\s*\\))\n-                   )\n-                                            # interstitial junk\n-                   \\s*(?:\\|\\|\\s*null\\s*)?(?:\\)\\s*)?&&\\s*(?:\\(\\s*)?\n-               (?(c)(?P=c)|(?P=b))\\s*=\\s*   # [c|b]=\n-                                            # nfunc|nfunc[idx]\n-                   (?P<nfunc>[a-zA-Z_$][\\w$]*)(?:\\s*\\[(?P<idx>\\d+)\\])?\\s*\\(\\s*[\\w$]+\\s*\\)\n+                # (expr, ...,\n+                \\((?:(?:\\s*[\\w$]+\\s*=)?(?:[\\w$\"+\\.\\s(\\[]+(?:[)\\]]\\s*)?),)*\n+                  # b=...\n+                  (?P<b>[\\w$]+)\\s*=\\s*(?!(?P=b)[^\\w$])[\\w$]+\\s*(?:(?:\n+                    \\.\\s*[\\w$]+ |\n+                    \\[\\s*[\\w$]+\\s*\\] |\n+                    \\.\\s*get\\s*\\(\\s*[\\w$\"]+\\s*\\)\n+                  )\\s*){,2}(?:\\s*\\|\\|\\s*null(?=\\s*\\)))?\\s*\n+                \\)\\s*&&\\s*\\(        # ...)&&(\n+                # b = nfunc, b = narray[idx]\n+                (?P=b)\\s*=\\s*(?P<nfunc>[\\w$]+)\\s*\n+                    (?:\\[\\s*(?P<idx>[\\w$]+)\\s*\\]\\s*)?\n+                    # (...)\n+                    \\(\\s*[\\w$]+\\s*\\)\n             ''', jscode, 'Initial JS player n function name', group=('nfunc', 'idx'),\n             default=(None, None))\n         # thx bashonly: yt-dlp/yt-dlp/pull/10611\n@@ -1697,15 +1726,19 @@ def _extract_n_function_name(self, jscode):\n                 r'''(?xs)\n                     (?:(?<=[^\\w$])|^)       # instead of \\b, which ignores $\n                     (?P<name>(?!\\d)[a-zA-Z\\d_$]+)\\s*=\\s*function\\((?!\\d)[a-zA-Z\\d_$]+\\)\n-                    \\s*\\{(?:(?!};).)+?[\"']enhanced_except_\n+                    \\s*\\{(?:(?!};).)+?(?:\n+                        [\"']enhanced_except_ |\n+                        return\\s*(?P<q>\"|')[a-zA-Z\\d-]+_w8_(?P=q)\\s*\\+\\s*[\\w$]+\n+                    )\n                 ''', jscode, 'Initial JS player n function name', group='name')\n         if not idx:\n             return func_name\n \n-        return self._parse_json(self._search_regex(\n-            r'var\\s+{0}\\s*=\\s*(\\[.+?\\])\\s*[,;]'.format(re.escape(func_name)), jscode,\n-            'Initial JS player n function list ({0}.{1})'.format(func_name, idx)),\n-            func_name, transform_source=js_to_json)[int(idx)]\n+        return self._search_json(\n+            r'var\\s+{0}\\s*='.format(re.escape(func_name)), jscode,\n+            'Initial JS player n function list ({0}.{1})'.format(func_name, idx),\n+            func_name, contains_pattern=r'\\[[\\s\\S]+\\]', end_pattern='[,;]',\n+            transform_source=js_to_json)[int(idx)]\n \n     def _extract_n_function_code(self, video_id, player_url):\n         player_id = self._extract_player_info(player_url)\n@@ -1728,13 +1761,13 @@ def _extract_n_function_from_code(self, jsi, func_code):\n \n         def extract_nsig(s):\n             try:\n-                ret = func([s])\n+                ret = func([s], kwargs={'_ytdl_do_not_return': s})\n             except JSInterpreter.Exception:\n                 raise\n             except Exception as e:\n                 raise JSInterpreter.Exception(traceback.format_exc(), cause=e)\n \n-            if ret.startswith('enhanced_except_'):\n+            if ret.startswith('enhanced_except_') or ret.endswith(s):\n                 raise JSInterpreter.Exception('Signature function returned an exception')\n             return ret\n \n@@ -1910,9 +1943,50 @@ def _real_extract(self, url):\n             player_response = self._extract_yt_initial_variable(\n                 webpage, self._YT_INITIAL_PLAYER_RESPONSE_RE,\n                 video_id, 'initial player response')\n-        if not player_response:\n+        if False and not player_response:\n             player_response = self._call_api(\n                 'player', {'videoId': video_id}, video_id)\n+        if True or not player_response:\n+            origin = 'https://www.youtube.com'\n+            pb_context = {'html5Preference': 'HTML5_PREF_WANTS'}\n+\n+            player_url = self._extract_player_url(webpage)\n+            ytcfg = self._extract_ytcfg(video_id, webpage)\n+            sts = self._extract_signature_timestamp(video_id, player_url, ytcfg)\n+            if sts:\n+                pb_context['signatureTimestamp'] = sts\n+\n+            query = {\n+                'playbackContext': {\n+                    'contentPlaybackContext': pb_context,\n+                    'contentCheckOk': True,\n+                    'racyCheckOk': True,\n+                },\n+                'context': {\n+                    'client': {\n+                        'clientName': 'MWEB',\n+                        'clientVersion': '2.20241202.07.00',\n+                        'hl': 'en',\n+                        'userAgent': 'Mozilla/5.0 (iPad; CPU OS 16_7_10 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1,gzip(gfe)',\n+                        'timeZone': 'UTC',\n+                        'utcOffsetMinutes': 0,\n+                    },\n+                },\n+                'videoId': video_id,\n+            }\n+            headers = {\n+                'X-YouTube-Client-Name': '2',\n+                'X-YouTube-Client-Version': '2.20241202.07.00',\n+                'Origin': origin,\n+                'Sec-Fetch-Mode': 'navigate',\n+                'User-Agent': query['context']['client']['userAgent'],\n+            }\n+            auth = self._generate_sapisidhash_header(origin)\n+            if auth is not None:\n+                headers['Authorization'] = auth\n+                headers['X-Origin'] = origin\n+\n+            player_response = self._call_api('player', query, video_id, fatal=False, headers=headers)\n \n         def is_agegated(playability):\n             if not isinstance(playability, dict):\n@@ -2219,12 +2293,12 @@ def process_manifest_format(f, proto, client_name, itag, all_formats=False):\n                         formats.append(f)\n \n         playable_formats = [f for f in formats if not f.get('has_drm')]\n-        if formats and not playable_formats:\n-            # If there are no formats that definitely don't have DRM, all have DRM\n-            self.report_drm(video_id)\n-        formats[:] = playable_formats\n-\n-        if not formats:\n+        if formats:\n+            if not playable_formats:\n+                # If there are no formats that definitely don't have DRM, all have DRM\n+                self.report_drm(video_id)\n+            formats[:] = playable_formats\n+        else:\n             if streaming_data.get('licenseInfos'):\n                 raise ExtractorError(\n                     'This video is DRM protected.', expected=True)\ndiff --git a/youtube_dl/jsinterp.py b/youtube_dl/jsinterp.py\nindex a616ad070b2..7835187f5fa 100644\n--- a/youtube_dl/jsinterp.py\n+++ b/youtube_dl/jsinterp.py\n@@ -1,3 +1,4 @@\n+# coding: utf-8\n from __future__ import unicode_literals\n \n import itertools\n@@ -5,11 +6,12 @@\n import operator\n import re\n \n-from functools import update_wrapper\n+from functools import update_wrapper, wraps\n \n from .utils import (\n     error_to_compat_str,\n     ExtractorError,\n+    float_or_none,\n     js_to_json,\n     remove_quotes,\n     unified_timestamp,\n@@ -20,9 +22,11 @@\n     compat_basestring,\n     compat_chr,\n     compat_collections_chain_map as ChainMap,\n+    compat_contextlib_suppress,\n     compat_filter as filter,\n     compat_itertools_zip_longest as zip_longest,\n     compat_map as map,\n+    compat_numeric_types,\n     compat_str,\n )\n \n@@ -62,6 +66,10 @@ def update_and_rename_wrapper(w):\n _Infinity = float('inf')\n \n \n+class JS_Undefined(object):\n+    pass\n+\n+\n def _js_bit_op(op):\n \n     def zeroise(x):\n@@ -74,43 +82,114 @@ def wrapped(a, b):\n     return wrapped\n \n \n-def _js_arith_op(op):\n+def _js_arith_op(op, div=False):\n \n     @wraps_op(op)\n     def wrapped(a, b):\n         if JS_Undefined in (a, b):\n             return _NaN\n-        return op(a or 0, b or 0)\n+        # null, \"\" --> 0\n+        a, b = (float_or_none(\n+            (x.strip() if isinstance(x, compat_basestring) else x) or 0,\n+            default=_NaN) for x in (a, b))\n+        if _NaN in (a, b):\n+            return _NaN\n+        try:\n+            return op(a, b)\n+        except ZeroDivisionError:\n+            return _NaN if not (div and (a or b)) else _Infinity\n \n     return wrapped\n \n \n-def _js_div(a, b):\n-    if JS_Undefined in (a, b) or not (a or b):\n-        return _NaN\n-    return operator.truediv(a or 0, b) if b else _Infinity\n+_js_arith_add = _js_arith_op(operator.add)\n+\n+\n+def _js_add(a, b):\n+    if not (isinstance(a, compat_basestring) or isinstance(b, compat_basestring)):\n+        return _js_arith_add(a, b)\n+    if not isinstance(a, compat_basestring):\n+        a = _js_toString(a)\n+    elif not isinstance(b, compat_basestring):\n+        b = _js_toString(b)\n+    return operator.concat(a, b)\n \n \n-def _js_mod(a, b):\n-    if JS_Undefined in (a, b) or not b:\n-        return _NaN\n-    return (a or 0) % b\n+_js_mod = _js_arith_op(operator.mod)\n+__js_exp = _js_arith_op(operator.pow)\n \n \n def _js_exp(a, b):\n     if not b:\n         return 1  # even 0 ** 0 !!\n-    elif JS_Undefined in (a, b):\n-        return _NaN\n-    return (a or 0) ** b\n-\n-\n-def _js_eq_op(op):\n+    return __js_exp(a, b)\n+\n+\n+def _js_to_primitive(v):\n+    return (\n+        ','.join(map(_js_toString, v)) if isinstance(v, list)\n+        else '[object Object]' if isinstance(v, dict)\n+        else compat_str(v) if not isinstance(v, (\n+            compat_numeric_types, compat_basestring))\n+        else v\n+    )\n+\n+\n+def _js_toString(v):\n+    return (\n+        'undefined' if v is JS_Undefined\n+        else 'Infinity' if v == _Infinity\n+        else 'NaN' if v is _NaN\n+        else 'null' if v is None\n+        # bool <= int: do this first\n+        else ('false', 'true')[v] if isinstance(v, bool)\n+        else '{0:.7f}'.format(v).rstrip('.0') if isinstance(v, compat_numeric_types)\n+        else _js_to_primitive(v))\n+\n+\n+_nullish = frozenset((None, JS_Undefined))\n+\n+\n+def _js_eq(a, b):\n+    # NaN != any\n+    if _NaN in (a, b):\n+        return False\n+    # Object is Object\n+    if isinstance(a, type(b)) and isinstance(b, (dict, list)):\n+        return operator.is_(a, b)\n+    # general case\n+    if a == b:\n+        return True\n+    # null == undefined\n+    a_b = set((a, b))\n+    if a_b & _nullish:\n+        return a_b <= _nullish\n+    a, b = _js_to_primitive(a), _js_to_primitive(b)\n+    if not isinstance(a, compat_basestring):\n+        a, b = b, a\n+    # Number to String: convert the string to a number\n+    # Conversion failure results in ... false\n+    if isinstance(a, compat_basestring):\n+        return float_or_none(a) == b\n+    return a == b\n+\n+\n+def _js_neq(a, b):\n+    return not _js_eq(a, b)\n+\n+\n+def _js_id_op(op):\n \n     @wraps_op(op)\n     def wrapped(a, b):\n-        if set((a, b)) <= set((None, JS_Undefined)):\n-            return op(a, a)\n+        if _NaN in (a, b):\n+            return op(_NaN, None)\n+        if not isinstance(a, (compat_basestring, compat_numeric_types)):\n+            a, b = b, a\n+        # strings are === if ==\n+        # why 'a' is not 'a': https://stackoverflow.com/a/1504848\n+        if isinstance(a, (compat_basestring, compat_numeric_types)):\n+            return a == b if op(0, 0) else a != b\n         return op(a, b)\n \n     return wrapped\n@@ -138,25 +217,57 @@ def _js_ternary(cndn, if_true=True, if_false=False):\n     return if_true\n \n \n+def _js_unary_op(op):\n+\n+    @wraps_op(op)\n+    def wrapped(_, a):\n+        return op(a)\n+\n+    return wrapped\n+\n+\n+# https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/typeof\n+def _js_typeof(expr):\n+    with compat_contextlib_suppress(TypeError, KeyError):\n+        return {\n+            JS_Undefined: 'undefined',\n+            _NaN: 'number',\n+            _Infinity: 'number',\n+            True: 'boolean',\n+            False: 'boolean',\n+            None: 'object',\n+        }[expr]\n+    for t, n in (\n+        (compat_basestring, 'string'),\n+        (compat_numeric_types, 'number'),\n+    ):\n+        if isinstance(expr, t):\n+            return n\n+    if callable(expr):\n+        return 'function'\n+    # TODO: Symbol, BigInt\n+    return 'object'\n+\n+\n # (op, definition) in order of binding priority, tightest first\n # avoid dict to maintain order\n # definition None => Defined in JSInterpreter._operator\n _OPERATORS = (\n     ('>>', _js_bit_op(operator.rshift)),\n     ('<<', _js_bit_op(operator.lshift)),\n-    ('+', _js_arith_op(operator.add)),\n+    ('+', _js_add),\n     ('-', _js_arith_op(operator.sub)),\n     ('*', _js_arith_op(operator.mul)),\n     ('%', _js_mod),\n-    ('/', _js_div),\n+    ('/', _js_arith_op(operator.truediv, div=True)),\n     ('**', _js_exp),\n )\n \n _COMP_OPERATORS = (\n-    ('===', operator.is_),\n-    ('!==', operator.is_not),\n-    ('==', _js_eq_op(operator.eq)),\n-    ('!=', _js_eq_op(operator.ne)),\n+    ('===', _js_id_op(operator.is_)),\n+    ('!==', _js_id_op(operator.is_not)),\n+    ('==', _js_eq),\n+    ('!=', _js_neq),\n     ('<=', _js_comp_op(operator.le)),\n     ('>=', _js_comp_op(operator.ge)),\n     ('<', _js_comp_op(operator.lt)),\n@@ -176,6 +287,11 @@ def _js_ternary(cndn, if_true=True, if_false=False):\n     ('&&', None),\n )\n \n+_UNARY_OPERATORS_X = (\n+    ('void', _js_unary_op(lambda _: JS_Undefined)),\n+    ('typeof', _js_unary_op(_js_typeof)),\n+)\n+\n _OPERATOR_RE = '|'.join(map(lambda x: re.escape(x[0]), _OPERATORS + _LOG_OPERATORS))\n \n _NAME_RE = r'[a-zA-Z_$][\\w$]*'\n@@ -183,10 +299,6 @@ def _js_ternary(cndn, if_true=True, if_false=False):\n _QUOTES = '\\'\"/'\n \n \n-class JS_Undefined(object):\n-    pass\n-\n-\n class JS_Break(ExtractorError):\n     def __init__(self):\n         ExtractorError.__init__(self, 'Invalid break')\n@@ -242,6 +354,7 @@ def truncate_string(s, left, right=0):\n \n     @classmethod\n     def wrap_interpreter(cls, f):\n+        @wraps(f)\n         def interpret_statement(self, stmt, local_vars, allow_recursion, *args, **kwargs):\n             if cls.ENABLED and stmt.strip():\n                 cls.write(stmt, level=allow_recursion)\n@@ -255,7 +368,7 @@ def interpret_statement(self, stmt, local_vars, allow_recursion, *args, **kwargs\n                 raise\n             if cls.ENABLED and stmt.strip():\n                 if should_ret or repr(ret) != stmt:\n-                    cls.write(['->', '=>'][should_ret], repr(ret), '<-|', stmt, level=allow_recursion)\n+                    cls.write(['->', '=>'][bool(should_ret)], repr(ret), '<-|', stmt, level=allow_recursion)\n             return ret, should_ret\n         return interpret_statement\n \n@@ -284,6 +397,9 @@ class JS_RegExp(object):\n         RE_FLAGS = {\n             # special knowledge: Python's re flags are bitmask values, current max 128\n             # invent new bitmask values well above that for literal parsing\n+            # JS 'u' flag is effectively always set (surrogate pairs aren't seen),\n+            # but \\u{...} and \\p{...} escapes aren't handled); no additional JS 'v'\n+            # features are supported\n             # TODO: execute matches with these flags (remaining: d, y)\n             'd': 1024,  # Generate indices for substring matches\n             'g': 2048,  # Global search\n@@ -291,6 +407,7 @@ class JS_RegExp(object):\n             'm': re.M,  # Multi-line search\n             's': re.S,  # Allows . to match newline characters\n             'u': re.U,  # Treat a pattern as a sequence of unicode code points\n+            'v': re.U,  # Like 'u' with extended character class and \\p{} syntax\n             'y': 4096,  # Perform a \"sticky\" search that matches starting at the current position in the target string\n         }\n \n@@ -347,6 +464,8 @@ def regex_flags(cls, expr):\n     def __op_chars(cls):\n         op_chars = set(';,[')\n         for op in cls._all_operators():\n+            if op[0].isalpha():\n+                continue\n             op_chars.update(op[0])\n         return op_chars\n \n@@ -369,9 +488,18 @@ def _separate(cls, expr, delim=',', max_split=None, skip_delims=None):\n         skipping = 0\n         if skip_delims:\n             skip_delims = variadic(skip_delims)\n+        skip_txt = None\n         for idx, char in enumerate(expr):\n+            if skip_txt and idx <= skip_txt[1]:\n+                continue\n             paren_delta = 0\n             if not in_quote:\n+                if char == '/' and expr[idx:idx + 2] == '/*':\n+                    # skip a comment\n+                    skip_txt = expr[idx:].find('*/', 2)\n+                    skip_txt = [idx, idx + skip_txt + 1] if skip_txt >= 2 else None\n+                    if skip_txt:\n+                        continue\n                 if char in _MATCHING_PARENS:\n                     counters[_MATCHING_PARENS[char]] += 1\n                     paren_delta = 1\n@@ -404,12 +532,19 @@ def _separate(cls, expr, delim=',', max_split=None, skip_delims=None):\n             if pos < delim_len:\n                 pos += 1\n                 continue\n-            yield expr[start: idx - delim_len]\n+            if skip_txt and skip_txt[0] >= start and skip_txt[1] <= idx - delim_len:\n+                yield expr[start:skip_txt[0]] + expr[skip_txt[1] + 1: idx - delim_len]\n+            else:\n+                yield expr[start: idx - delim_len]\n+            skip_txt = None\n             start, pos = idx + 1, 0\n             splits += 1\n             if max_split and splits >= max_split:\n                 break\n-        yield expr[start:]\n+        if skip_txt and skip_txt[0] >= start:\n+            yield expr[start:skip_txt[0]] + expr[skip_txt[1] + 1:]\n+        else:\n+            yield expr[start:]\n \n     @classmethod\n     def _separate_at_paren(cls, expr, delim=None):\n@@ -425,7 +560,7 @@ def _all_operators(_cached=[]):\n         if not _cached:\n             _cached.extend(itertools.chain(\n                 # Ref: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Operator_Precedence\n-                _SC_OPERATORS, _LOG_OPERATORS, _COMP_OPERATORS, _OPERATORS))\n+                _SC_OPERATORS, _LOG_OPERATORS, _COMP_OPERATORS, _OPERATORS, _UNARY_OPERATORS_X))\n         return _cached\n \n     def _operator(self, op, left_val, right_expr, expr, local_vars, allow_recursion):\n@@ -449,13 +584,14 @@ def _operator(self, op, left_val, right_expr, expr, local_vars, allow_recursion)\n         except Exception as e:\n             raise self.Exception('Failed to evaluate {left_val!r:.50} {op} {right_val!r:.50}'.format(**locals()), expr, cause=e)\n \n-    def _index(self, obj, idx, allow_undefined=False):\n-        if idx == 'length':\n+    def _index(self, obj, idx, allow_undefined=True):\n+        if idx == 'length' and isinstance(obj, list):\n             return len(obj)\n         try:\n-            return obj[int(idx)] if isinstance(obj, list) else obj[idx]\n-        except Exception as e:\n+            return obj[int(idx)] if isinstance(obj, list) else obj[compat_str(idx)]\n+        except (TypeError, KeyError, IndexError) as e:\n             if allow_undefined:\n+                # when is not allowed?\n                 return JS_Undefined\n             raise self.Exception('Cannot get index {idx!r:.100}'.format(**locals()), expr=repr(obj), cause=e)\n \n@@ -467,7 +603,7 @@ def _dump(self, obj, namespace):\n \n     # used below\n     _VAR_RET_THROW_RE = re.compile(r'''(?x)\n-        (?P<var>(?:var|const|let)\\s)|return(?:\\s+|(?=[\"'])|$)|(?P<throw>throw\\s+)\n+        (?:(?P<var>var|const|let)\\s+|(?P<ret>return)(?:\\s+|(?=[\"'])|$)|(?P<throw>throw)\\s+)\n         ''')\n     _COMPOUND_RE = re.compile(r'''(?x)\n         (?P<try>try)\\s*\\{|\n@@ -479,6 +615,52 @@ def _dump(self, obj, namespace):\n     _FINALLY_RE = re.compile(r'finally\\s*\\{')\n     _SWITCH_RE = re.compile(r'switch\\s*\\(')\n \n+    def handle_operators(self, expr, local_vars, allow_recursion):\n+\n+        for op, _ in self._all_operators():\n+            # hackety: </> have higher priority than <</>>, but don't confuse them\n+            skip_delim = (op + op) if op in '<>*?' else None\n+            if op == '?':\n+                skip_delim = (skip_delim, '?.')\n+            separated = list(self._separate(expr, op, skip_delims=skip_delim))\n+            if len(separated) < 2:\n+                continue\n+\n+            right_expr = separated.pop()\n+            # handle operators that are both unary and binary, minimal BODMAS\n+            if op in ('+', '-'):\n+                # simplify/adjust consecutive instances of these operators\n+                undone = 0\n+                separated = [s.strip() for s in separated]\n+                while len(separated) > 1 and not separated[-1]:\n+                    undone += 1\n+                    separated.pop()\n+                if op == '-' and undone % 2 != 0:\n+                    right_expr = op + right_expr\n+                elif op == '+':\n+                    while len(separated) > 1 and set(separated[-1]) <= self.OP_CHARS:\n+                        right_expr = separated.pop() + right_expr\n+                    if separated[-1][-1:] in self.OP_CHARS:\n+                        right_expr = separated.pop() + right_expr\n+                # hanging op at end of left => unary + (strip) or - (push right)\n+                left_val = separated[-1] if separated else ''\n+                for dm_op in ('*', '%', '/', '**'):\n+                    bodmas = tuple(self._separate(left_val, dm_op, skip_delims=skip_delim))\n+                    if len(bodmas) > 1 and not bodmas[-1].strip():\n+                        expr = op.join(separated) + op + right_expr\n+                        if len(separated) > 1:\n+                            separated.pop()\n+                            right_expr = op.join((left_val, right_expr))\n+                        else:\n+                            separated = [op.join((left_val, right_expr))]\n+                            right_expr = None\n+                        break\n+                if right_expr is None:\n+                    continue\n+\n+            left_val = self.interpret_expression(op.join(separated), local_vars, allow_recursion)\n+            return self._operator(op, left_val, right_expr, expr, local_vars, allow_recursion), True\n+\n     @Debugger.wrap_interpreter\n     def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n         if allow_recursion < 0:\n@@ -501,7 +683,7 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n             expr = stmt[len(m.group(0)):].strip()\n             if m.group('throw'):\n                 raise JS_Throw(self.interpret_expression(expr, local_vars, allow_recursion))\n-            should_return = not m.group('var')\n+            should_return = 'return' if m.group('ret') else False\n         if not expr:\n             return None, should_return\n \n@@ -533,9 +715,15 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n             else:\n                 raise self.Exception('Unsupported object {obj:.100}'.format(**locals()), expr=expr)\n \n-        if expr.startswith('void '):\n-            left = self.interpret_expression(expr[5:], local_vars, allow_recursion)\n-            return None, should_return\n+        for op, _ in _UNARY_OPERATORS_X:\n+            if not expr.startswith(op):\n+                continue\n+            operand = expr[len(op):]\n+            if not operand or operand[0] != ' ':\n+                continue\n+            op_result = self.handle_operators(expr, local_vars, allow_recursion)\n+            if op_result:\n+                return op_result[0], should_return\n \n         if expr.startswith('{'):\n             inner, outer = self._separate_at_paren(expr)\n@@ -582,7 +770,7 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n                 if_expr, expr = self._separate_at_paren(expr)\n             else:\n                 # may lose ... else ... because of ll.368-374\n-                if_expr, expr = self._separate_at_paren(expr, delim=';')\n+                if_expr, expr = self._separate_at_paren(' %s;' % (expr,), delim=';')\n             else_expr = None\n             m = re.match(r'else\\s*(?P<block>\\{)?', expr)\n             if m:\n@@ -720,7 +908,7 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n             start, end = m.span()\n             sign = m.group('pre_sign') or m.group('post_sign')\n             ret = local_vars[var]\n-            local_vars[var] += 1 if sign[0] == '+' else -1\n+            local_vars[var] = _js_add(ret, 1 if sign[0] == '+' else -1)\n             if m.group('pre_sign'):\n                 ret = local_vars[var]\n             expr = expr[:start] + self._dump(ret, local_vars) + expr[end:]\n@@ -730,13 +918,13 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n \n         m = re.match(r'''(?x)\n             (?P<assign>\n-                (?P<out>{_NAME_RE})(?:\\[(?P<index>[^\\]]+?)\\])?\\s*\n+                (?P<out>{_NAME_RE})(?:\\[(?P<out_idx>(?:.+?\\]\\s*\\[)*.+?)\\])?\\s*\n                 (?P<op>{_OPERATOR_RE})?\n                 =(?!=)(?P<expr>.*)$\n             )|(?P<return>\n                 (?!if|return|true|false|null|undefined|NaN|Infinity)(?P<name>{_NAME_RE})$\n             )|(?P<indexing>\n-                (?P<in>{_NAME_RE})\\[(?P<idx>.+)\\]$\n+                (?P<in>{_NAME_RE})\\[(?P<in_idx>(?:.+?\\]\\s*\\[)*.+?)\\]$\n             )|(?P<attribute>\n                 (?P<var>{_NAME_RE})(?:(?P<nullish>\\?)?\\.(?P<member>[^(]+)|\\[(?P<member2>[^\\]]+)\\])\\s*\n             )|(?P<function>\n@@ -746,19 +934,23 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n         if md.get('assign'):\n             left_val = local_vars.get(m.group('out'))\n \n-            if not m.group('index'):\n+            if not m.group('out_idx'):\n                 local_vars[m.group('out')] = self._operator(\n                     m.group('op'), left_val, m.group('expr'), expr, local_vars, allow_recursion)\n                 return local_vars[m.group('out')], should_return\n             elif left_val in (None, JS_Undefined):\n                 raise self.Exception('Cannot index undefined variable ' + m.group('out'), expr=expr)\n \n-            idx = self.interpret_expression(m.group('index'), local_vars, allow_recursion)\n-            if not isinstance(idx, (int, float)):\n-                raise self.Exception('List index %s must be integer' % (idx, ), expr=expr)\n-            idx = int(idx)\n+            indexes = re.split(r'\\]\\s*\\[', m.group('out_idx'))\n+            for i, idx in enumerate(indexes, 1):\n+                idx = self.interpret_expression(idx, local_vars, allow_recursion)\n+                if i < len(indexes):\n+                    left_val = self._index(left_val, idx)\n+            if isinstance(idx, float):\n+                idx = int(idx)\n             left_val[idx] = self._operator(\n-                m.group('op'), self._index(left_val, idx), m.group('expr'), expr, local_vars, allow_recursion)\n+                m.group('op'), self._index(left_val, idx) if m.group('op') else None,\n+                m.group('expr'), expr, local_vars, allow_recursion)\n             return left_val[idx], should_return\n \n         elif expr.isdigit():\n@@ -776,63 +968,31 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n             return _Infinity, should_return\n \n         elif md.get('return'):\n-            return local_vars[m.group('name')], should_return\n+            ret = local_vars[m.group('name')]\n+            # challenge may try to force returning the original value\n+            # use an optional internal var to block this\n+            if should_return == 'return':\n+                if '_ytdl_do_not_return' not in local_vars:\n+                    return ret, True\n+                return (ret, True) if ret != local_vars['_ytdl_do_not_return'] else (ret, False)\n+            else:\n+                return ret, should_return\n \n-        try:\n+        with compat_contextlib_suppress(ValueError):\n             ret = json.loads(js_to_json(expr))  # strict=True)\n             if not md.get('attribute'):\n                 return ret, should_return\n-        except ValueError:\n-            pass\n \n         if md.get('indexing'):\n             val = local_vars[m.group('in')]\n-            idx = self.interpret_expression(m.group('idx'), local_vars, allow_recursion)\n-            return self._index(val, idx), should_return\n+            for idx in re.split(r'\\]\\s*\\[', m.group('in_idx')):\n+                idx = self.interpret_expression(idx, local_vars, allow_recursion)\n+                val = self._index(val, idx)\n+            return val, should_return\n \n-        for op, _ in self._all_operators():\n-            # hackety: </> have higher priority than <</>>, but don't confuse them\n-            skip_delim = (op + op) if op in '<>*?' else None\n-            if op == '?':\n-                skip_delim = (skip_delim, '?.')\n-            separated = list(self._separate(expr, op, skip_delims=skip_delim))\n-            if len(separated) < 2:\n-                continue\n-\n-            right_expr = separated.pop()\n-            # handle operators that are both unary and binary, minimal BODMAS\n-            if op in ('+', '-'):\n-                # simplify/adjust consecutive instances of these operators\n-                undone = 0\n-                separated = [s.strip() for s in separated]\n-                while len(separated) > 1 and not separated[-1]:\n-                    undone += 1\n-                    separated.pop()\n-                if op == '-' and undone % 2 != 0:\n-                    right_expr = op + right_expr\n-                elif op == '+':\n-                    while len(separated) > 1 and set(separated[-1]) <= self.OP_CHARS:\n-                        right_expr = separated.pop() + right_expr\n-                    if separated[-1][-1:] in self.OP_CHARS:\n-                        right_expr = separated.pop() + right_expr\n-                # hanging op at end of left => unary + (strip) or - (push right)\n-                left_val = separated[-1] if separated else ''\n-                for dm_op in ('*', '%', '/', '**'):\n-                    bodmas = tuple(self._separate(left_val, dm_op, skip_delims=skip_delim))\n-                    if len(bodmas) > 1 and not bodmas[-1].strip():\n-                        expr = op.join(separated) + op + right_expr\n-                        if len(separated) > 1:\n-                            separated.pop()\n-                            right_expr = op.join((left_val, right_expr))\n-                        else:\n-                            separated = [op.join((left_val, right_expr))]\n-                            right_expr = None\n-                        break\n-                if right_expr is None:\n-                    continue\n-\n-            left_val = self.interpret_expression(op.join(separated), local_vars, allow_recursion)\n-            return self._operator(op, left_val, right_expr, expr, local_vars, allow_recursion), should_return\n+        op_result = self.handle_operators(expr, local_vars, allow_recursion)\n+        if op_result:\n+            return op_result[0], should_return\n \n         if md.get('attribute'):\n             variable, member, nullish = m.group('var', 'member', 'nullish')\n@@ -877,7 +1037,7 @@ def eval_method(variable, member):\n \n                 # Member access\n                 if arg_str is None:\n-                    return self._index(obj, member, nullish)\n+                    return self._index(obj, member)\n \n                 # Function call\n                 argvals = [\n@@ -904,7 +1064,7 @@ def eval_method(variable, member):\n                 if obj is compat_str:\n                     if member == 'fromCharCode':\n                         assertion(argvals, 'takes one or more arguments')\n-                        return ''.join(map(compat_chr, argvals))\n+                        return ''.join(compat_chr(int(n)) for n in argvals)\n                     raise self.Exception('Unsupported string method ' + member, expr=expr)\n                 elif obj is float:\n                     if member == 'pow':\n@@ -913,13 +1073,47 @@ def eval_method(variable, member):\n                     raise self.Exception('Unsupported Math method ' + member, expr=expr)\n \n                 if member == 'split':\n-                    assertion(argvals, 'takes one or more arguments')\n-                    assertion(len(argvals) == 1, 'with limit argument is not implemented')\n-                    return obj.split(argvals[0]) if argvals[0] else list(obj)\n+                    assertion(len(argvals) <= 2, 'takes at most two arguments')\n+                    if len(argvals) > 1:\n+                        limit = argvals[1]\n+                        assertion(isinstance(limit, int) and limit >= 0, 'integer limit >= 0')\n+                        if limit == 0:\n+                            return []\n+                    else:\n+                        limit = 0\n+                    if len(argvals) == 0:\n+                        argvals = [JS_Undefined]\n+                    elif isinstance(argvals[0], self.JS_RegExp):\n+                        # avoid re.split(), similar but not enough\n+\n+                        def where():\n+                            for m in argvals[0].finditer(obj):\n+                                yield m.span(0)\n+                            yield (None, None)\n+\n+                        def splits(limit=limit):\n+                            i = 0\n+                            for j, jj in where():\n+                                if j == jj == 0:\n+                                    continue\n+                                if j is None and i >= len(obj):\n+                                    break\n+                                yield obj[i:j]\n+                                if jj is None or limit == 1:\n+                                    break\n+                                limit -= 1\n+                                i = jj\n+\n+                        return list(splits())\n+                    return (\n+                        obj.split(argvals[0], limit - 1) if argvals[0] and argvals[0] != JS_Undefined\n+                        else list(obj)[:limit or None])\n                 elif member == 'join':\n                     assertion(isinstance(obj, list), 'must be applied on a list')\n-                    assertion(len(argvals) == 1, 'takes exactly one argument')\n-                    return argvals[0].join(obj)\n+                    assertion(len(argvals) <= 1, 'takes at most one argument')\n+                    return (',' if len(argvals) == 0 else argvals[0]).join(\n+                        ('' if x in (None, JS_Undefined) else _js_toString(x))\n+                        for x in obj)\n                 elif member == 'reverse':\n                     assertion(not argvals, 'does not take any arguments')\n                     obj.reverse()\n@@ -941,37 +1135,31 @@ def eval_method(variable, member):\n                     index, how_many = map(int, (argvals + [len(obj)])[:2])\n                     if index < 0:\n                         index += len(obj)\n-                    add_items = argvals[2:]\n-                    res = []\n-                    for _ in range(index, min(index + how_many, len(obj))):\n-                        res.append(obj.pop(index))\n-                    for i, item in enumerate(add_items):\n-                        obj.insert(index + i, item)\n+                    res = [obj.pop(index)\n+                           for _ in range(index, min(index + how_many, len(obj)))]\n+                    obj[index:index] = argvals[2:]\n                     return res\n-                elif member == 'unshift':\n-                    assertion(isinstance(obj, list), 'must be applied on a list')\n-                    assertion(argvals, 'takes one or more arguments')\n-                    for item in reversed(argvals):\n-                        obj.insert(0, item)\n-                    return obj\n-                elif member == 'pop':\n+                elif member in ('shift', 'pop'):\n                     assertion(isinstance(obj, list), 'must be applied on a list')\n                     assertion(not argvals, 'does not take any arguments')\n-                    if not obj:\n-                        return\n-                    return obj.pop()\n+                    return obj.pop(0 if member == 'shift' else -1) if len(obj) > 0 else JS_Undefined\n+                elif member == 'unshift':\n+                    assertion(isinstance(obj, list), 'must be applied on a list')\n+                    # not enforced: assertion(argvals, 'takes one or more arguments')\n+                    obj[0:0] = argvals\n+                    return len(obj)\n                 elif member == 'push':\n-                    assertion(argvals, 'takes one or more arguments')\n+                    # not enforced: assertion(argvals, 'takes one or more arguments')\n                     obj.extend(argvals)\n-                    return obj\n+                    return len(obj)\n                 elif member == 'forEach':\n                     assertion(argvals, 'takes one or more arguments')\n-                    assertion(len(argvals) <= 2, 'takes at-most 2 arguments')\n+                    assertion(len(argvals) <= 2, 'takes at most 2 arguments')\n                     f, this = (argvals + [''])[:2]\n                     return [f((item, idx, obj), {'this': this}, allow_recursion) for idx, item in enumerate(obj)]\n                 elif member == 'indexOf':\n                     assertion(argvals, 'takes one or more arguments')\n-                    assertion(len(argvals) <= 2, 'takes at-most 2 arguments')\n+                    assertion(len(argvals) <= 2, 'takes at most 2 arguments')\n                     idx, start = (argvals + [0])[:2]\n                     try:\n                         return obj.index(idx, start)\n@@ -980,7 +1168,7 @@ def eval_method(variable, member):\n                 elif member == 'charCodeAt':\n                     assertion(isinstance(obj, compat_str), 'must be applied on a string')\n                     # assertion(len(argvals) == 1, 'takes exactly one argument') # but not enforced\n-                    idx = argvals[0] if isinstance(argvals[0], int) else 0\n+                    idx = argvals[0] if len(argvals) > 0 and isinstance(argvals[0], int) else 0\n                     if idx >= len(obj):\n                         return None\n                     return ord(obj[idx])\n@@ -1031,7 +1219,7 @@ def interpret_iter(self, list_txt, local_vars, allow_recursion):\n             yield self.interpret_expression(v, local_vars, allow_recursion)\n \n     def extract_object(self, objname):\n-        _FUNC_NAME_RE = r'''(?:[a-zA-Z$0-9]+|\"[a-zA-Z$0-9]+\"|'[a-zA-Z$0-9]+')'''\n+        _FUNC_NAME_RE = r'''(?:{n}|\"{n}\"|'{n}')'''.format(n=_NAME_RE)\n         obj = {}\n         fields = next(filter(None, (\n             obj_m.group('fields') for obj_m in re.finditer(\n@@ -1090,6 +1278,7 @@ def extract_function(self, funcname):\n \n     def extract_function_from_code(self, argnames, code, *global_stack):\n         local_vars = {}\n+\n         while True:\n             mobj = re.search(r'function\\((?P<args>[^)]*)\\)\\s*{', code)\n             if mobj is None:\n@@ -1100,10 +1289,11 @@ def extract_function_from_code(self, argnames, code, *global_stack):\n                 [x.strip() for x in mobj.group('args').split(',')],\n                 body, local_vars, *global_stack))\n             code = code[:start] + name + remaining\n+\n         return self.build_function(argnames, code, local_vars, *global_stack)\n \n-    def call_function(self, funcname, *args):\n-        return self.extract_function(funcname)(args)\n+    def call_function(self, funcname, *args, **kw_global_vars):\n+        return self.extract_function(funcname)(args, kw_global_vars)\n \n     @classmethod\n     def build_arglist(cls, arg_text):\n@@ -1122,8 +1312,9 @@ def build_function(self, argnames, code, *global_stack):\n         global_stack = list(global_stack) or [{}]\n         argnames = tuple(argnames)\n \n-        def resf(args, kwargs={}, allow_recursion=100):\n-            global_stack[0].update(zip_longest(argnames, args, fillvalue=None))\n+        def resf(args, kwargs=None, allow_recursion=100):\n+            kwargs = kwargs or {}\n+            global_stack[0].update(zip_longest(argnames, args, fillvalue=JS_Undefined))\n             global_stack[0].update(kwargs)\n             var_stack = LocalNameSpace(*global_stack)\n             ret, should_abort = self.interpret_statement(code.replace('\\n', ' '), var_stack, allow_recursion - 1)\n",
    "test_patch": "diff --git a/test/test_jsinterp.py b/test/test_jsinterp.py\nindex c7a4f2cbf23..12e7b9b9485 100644\n--- a/test/test_jsinterp.py\n+++ b/test/test_jsinterp.py\n@@ -1,4 +1,5 @@\n #!/usr/bin/env python\n+# coding: utf-8\n \n from __future__ import unicode_literals\n \n@@ -11,7 +12,7 @@\n import math\n import re\n \n-from youtube_dl.compat import compat_str\n+from youtube_dl.compat import compat_str as str\n from youtube_dl.jsinterp import JS_Undefined, JSInterpreter\n \n NaN = object()\n@@ -19,7 +20,7 @@\n \n class TestJSInterpreter(unittest.TestCase):\n     def _test(self, jsi_or_code, expected, func='f', args=()):\n-        if isinstance(jsi_or_code, compat_str):\n+        if isinstance(jsi_or_code, str):\n             jsi_or_code = JSInterpreter(jsi_or_code)\n         got = jsi_or_code.call_function(func, *args)\n         if expected is NaN:\n@@ -40,16 +41,27 @@ def test_add(self):\n         self._test('function f(){return 42 + 7;}', 49)\n         self._test('function f(){return 42 + undefined;}', NaN)\n         self._test('function f(){return 42 + null;}', 42)\n+        self._test('function f(){return 1 + \"\";}', '1')\n+        self._test('function f(){return 42 + \"7\";}', '427')\n+        self._test('function f(){return false + true;}', 1)\n+        self._test('function f(){return \"false\" + true;}', 'falsetrue')\n+        self._test('function f(){return '\n+                   '1 + \"2\" + [3,4] + {k: 56} + null + undefined + Infinity;}',\n+                   '123,4[object Object]nullundefinedInfinity')\n \n     def test_sub(self):\n         self._test('function f(){return 42 - 7;}', 35)\n         self._test('function f(){return 42 - undefined;}', NaN)\n         self._test('function f(){return 42 - null;}', 42)\n+        self._test('function f(){return 42 - \"7\";}', 35)\n+        self._test('function f(){return 42 - \"spam\";}', NaN)\n \n     def test_mul(self):\n         self._test('function f(){return 42 * 7;}', 294)\n         self._test('function f(){return 42 * undefined;}', NaN)\n         self._test('function f(){return 42 * null;}', 0)\n+        self._test('function f(){return 42 * \"7\";}', 294)\n+        self._test('function f(){return 42 * \"eggs\";}', NaN)\n \n     def test_div(self):\n         jsi = JSInterpreter('function f(a, b){return a / b;}')\n@@ -57,17 +69,26 @@ def test_div(self):\n         self._test(jsi, NaN, args=(JS_Undefined, 1))\n         self._test(jsi, float('inf'), args=(2, 0))\n         self._test(jsi, 0, args=(0, 3))\n+        self._test(jsi, 6, args=(42, 7))\n+        self._test(jsi, 0, args=(42, float('inf')))\n+        self._test(jsi, 6, args=(\"42\", 7))\n+        self._test(jsi, NaN, args=(\"spam\", 7))\n \n     def test_mod(self):\n         self._test('function f(){return 42 % 7;}', 0)\n         self._test('function f(){return 42 % 0;}', NaN)\n         self._test('function f(){return 42 % undefined;}', NaN)\n+        self._test('function f(){return 42 % \"7\";}', 0)\n+        self._test('function f(){return 42 % \"beans\";}', NaN)\n \n     def test_exp(self):\n         self._test('function f(){return 42 ** 2;}', 1764)\n         self._test('function f(){return 42 ** undefined;}', NaN)\n         self._test('function f(){return 42 ** null;}', 1)\n+        self._test('function f(){return undefined ** 0;}', 1)\n         self._test('function f(){return undefined ** 42;}', NaN)\n+        self._test('function f(){return 42 ** \"2\";}', 1764)\n+        self._test('function f(){return 42 ** \"spam\";}', NaN)\n \n     def test_calc(self):\n         self._test('function f(a){return 2*a+1;}', 7, args=[3])\n@@ -89,7 +110,35 @@ def test_operators(self):\n         self._test('function f(){return 19 & 21;}', 17)\n         self._test('function f(){return 11 >> 2;}', 2)\n         self._test('function f(){return []? 2+3: 4;}', 5)\n+        # equality\n+        self._test('function f(){return 1 == 1}', True)\n+        self._test('function f(){return 1 == 1.0}', True)\n+        self._test('function f(){return 1 == \"1\"}', True)\n         self._test('function f(){return 1 == 2}', False)\n+        self._test('function f(){return 1 != \"1\"}', False)\n+        self._test('function f(){return 1 != 2}', True)\n+        self._test('function f(){var x = {a: 1}; var y = x; return x == y}', True)\n+        self._test('function f(){var x = {a: 1}; return x == {a: 1}}', False)\n+        self._test('function f(){return NaN == NaN}', False)\n+        self._test('function f(){return null == undefined}', True)\n+        self._test('function f(){return \"spam, eggs\" == \"spam, eggs\"}', True)\n+        # strict equality\n+        self._test('function f(){return 1 === 1}', True)\n+        self._test('function f(){return 1 === 1.0}', True)\n+        self._test('function f(){return 1 === \"1\"}', False)\n+        self._test('function f(){return 1 === 2}', False)\n+        self._test('function f(){var x = {a: 1}; var y = x; return x === y}', True)\n+        self._test('function f(){var x = {a: 1}; return x === {a: 1}}', False)\n+        self._test('function f(){return NaN === NaN}', False)\n+        self._test('function f(){return null === undefined}', False)\n+        self._test('function f(){return null === null}', True)\n+        self._test('function f(){return undefined === undefined}', True)\n+        self._test('function f(){return \"uninterned\" === \"uninterned\"}', True)\n+        self._test('function f(){return 1 === 1}', True)\n+        self._test('function f(){return 1 === \"1\"}', False)\n+        self._test('function f(){return 1 !== 1}', False)\n+        self._test('function f(){return 1 !== \"1\"}', True)\n+        # expressions\n         self._test('function f(){return 0 && 1 || 2;}', 2)\n         self._test('function f(){return 0 ?? 42;}', 0)\n         self._test('function f(){return \"life, the universe and everything\" < 42;}', False)\n@@ -111,7 +160,6 @@ def test_assignments(self):\n         self._test('function f(){var x = 20; x += 30 + 1; return x;}', 51)\n         self._test('function f(){var x = 20; x -= 30 + 1; return x;}', -11)\n \n-    @unittest.skip('Not yet fully implemented')\n     def test_comments(self):\n         self._test('''\n             function f() {\n@@ -130,6 +178,15 @@ def test_comments(self):\n             }\n         ''', 3)\n \n+        self._test('''\n+            function f() {\n+                var x = ( /* 1 + */ 2 +\n+                          /* 30 * 40 */\n+                          50);\n+                return x;\n+            }\n+        ''', 52)\n+\n     def test_precedence(self):\n         self._test('''\n             function f() {\n@@ -266,7 +323,20 @@ def test_comma(self):\n         self._test('function f() { return (l=[0,1,2,3], function(a, b){return a+b})((l[1], l[2]), l[3]) }', 5)\n \n     def test_void(self):\n-        self._test('function f() { return void 42; }', None)\n+        self._test('function f() { return void 42; }', JS_Undefined)\n+\n+    def test_typeof(self):\n+        self._test('function f() { return typeof undefined; }', 'undefined')\n+        self._test('function f() { return typeof NaN; }', 'number')\n+        self._test('function f() { return typeof Infinity; }', 'number')\n+        self._test('function f() { return typeof true; }', 'boolean')\n+        self._test('function f() { return typeof null; }', 'object')\n+        self._test('function f() { return typeof \"a string\"; }', 'string')\n+        self._test('function f() { return typeof 42; }', 'number')\n+        self._test('function f() { return typeof 42.42; }', 'number')\n+        self._test('function f() { var g = function(){}; return typeof g; }', 'function')\n+        self._test('function f() { return typeof {key: \"value\"}; }', 'object')\n+        # not yet implemented: Symbol, BigInt\n \n     def test_return_function(self):\n         jsi = JSInterpreter('''\n@@ -283,7 +353,7 @@ def test_null(self):\n     def test_undefined(self):\n         self._test('function f() { return undefined === undefined; }', True)\n         self._test('function f() { return undefined; }', JS_Undefined)\n-        self._test('function f() {return undefined ?? 42; }', 42)\n+        self._test('function f() { return undefined ?? 42; }', 42)\n         self._test('function f() { let v; return v; }', JS_Undefined)\n         self._test('function f() { let v; return v**0; }', 1)\n         self._test('function f() { let v; return [v>42, v<=42, v&&42, 42&&v]; }',\n@@ -324,6 +394,16 @@ def test_object(self):\n         self._test('function f() { let a; return a?.qq; }', JS_Undefined)\n         self._test('function f() { let a = {m1: 42, m2: 0 }; return a?.qq; }', JS_Undefined)\n \n+    def test_indexing(self):\n+        self._test('function f() { return [1, 2, 3, 4][3]}', 4)\n+        self._test('function f() { return [1, [2, [3, [4]]]][1][1][1][0]}', 4)\n+        self._test('function f() { var o = {1: 2, 3: 4}; return o[3]}', 4)\n+        self._test('function f() { var o = {1: 2, 3: 4}; return o[\"3\"]}', 4)\n+        self._test('function f() { return [1, [2, {3: [4]}]][1][1][\"3\"][0]}', 4)\n+        self._test('function f() { return [1, 2, 3, 4].length}', 4)\n+        self._test('function f() { var o = {1: 2, 3: 4}; return o.length}', JS_Undefined)\n+        self._test('function f() { var o = {1: 2, 3: 4}; o[\"length\"] = 42; return o.length}', 42)\n+\n     def test_regex(self):\n         self._test('function f() { let a=/,,[/,913,/](,)}/; }', None)\n \n@@ -411,6 +491,13 @@ def test_join(self):\n             self._test(jsi, 't-e-s-t', args=[test_input, '-'])\n             self._test(jsi, '', args=[[], '-'])\n \n+        self._test('function f(){return '\n+                   '[1, 1.0, \"abc\", {a: 1}, null, undefined, Infinity, NaN].join()}',\n+                   '1,1,abc,[object Object],,,Infinity,NaN')\n+        self._test('function f(){return '\n+                   '[1, 1.0, \"abc\", {a: 1}, null, undefined, Infinity, NaN].join(\"~\")}',\n+                   '1~1~abc~[object Object]~~~Infinity~NaN')\n+\n     def test_split(self):\n         test_result = list('test')\n         tests = [\n@@ -424,6 +511,18 @@ def test_split(self):\n             self._test(jsi, test_result, args=['t-e-s-t', '-'])\n             self._test(jsi, [''], args=['', '-'])\n             self._test(jsi, [], args=['', ''])\n+        # RegExp split\n+        self._test('function f(){return \"test\".split(/(?:)/)}',\n+                   ['t', 'e', 's', 't'])\n+        self._test('function f(){return \"t-e-s-t\".split(/[es-]+/)}',\n+                   ['t', 't'])\n+        # from MDN: surrogate pairs aren't handled: case 1 fails\n+        # self._test('function f(){return \"\ud83d\ude04\ud83d\ude04\".split(/(?:)/)}',\n+        #            ['\\ud83d', '\\ude04', '\\ud83d', '\\ude04'])\n+        # case 2 beats Py3.2: it gets the case 1 result\n+        if sys.version_info >= (2, 6) and not ((3, 0) <= sys.version_info < (3, 3)):\n+            self._test('function f(){return \"\ud83d\ude04\ud83d\ude04\".split(/(?:)/u)}',\n+                       ['\ud83d\ude04', '\ud83d\ude04'])\n \n     def test_slice(self):\n         self._test('function f(){return [0, 1, 2, 3, 4, 5, 6, 7, 8].slice()}', [0, 1, 2, 3, 4, 5, 6, 7, 8])\n@@ -453,6 +552,40 @@ def test_slice(self):\n         self._test('function f(){return \"012345678\".slice(-1, 1)}', '')\n         self._test('function f(){return \"012345678\".slice(-3, -1)}', '67')\n \n+    def test_pop(self):\n+        # pop\n+        self._test('function f(){var a = [0, 1, 2, 3, 4, 5, 6, 7, 8]; return [a.pop(), a]}',\n+                   [8, [0, 1, 2, 3, 4, 5, 6, 7]])\n+        self._test('function f(){return [].pop()}', JS_Undefined)\n+        # push\n+        self._test('function f(){var a = [0, 1, 2]; return [a.push(3, 4), a]}',\n+                   [5, [0, 1, 2, 3, 4]])\n+        self._test('function f(){var a = [0, 1, 2]; return [a.push(), a]}',\n+                   [3, [0, 1, 2]])\n+\n+    def test_shift(self):\n+        # shift\n+        self._test('function f(){var a = [0, 1, 2, 3, 4, 5, 6, 7, 8]; return [a.shift(), a]}',\n+                   [0, [1, 2, 3, 4, 5, 6, 7, 8]])\n+        self._test('function f(){return [].shift()}', JS_Undefined)\n+        # unshift\n+        self._test('function f(){var a = [0, 1, 2]; return [a.unshift(3, 4), a]}',\n+                   [5, [3, 4, 0, 1, 2]])\n+        self._test('function f(){var a = [0, 1, 2]; return [a.unshift(), a]}',\n+                   [3, [0, 1, 2]])\n+\n+    def test_forEach(self):\n+        self._test('function f(){var ret = []; var l = [4, 2]; '\n+                   'var log = function(e,i,a){ret.push([e,i,a]);}; '\n+                   'l.forEach(log); '\n+                   'return [ret.length, ret[0][0], ret[1][1], ret[0][2]]}',\n+                   [2, 4, 1, [4, 2]])\n+        self._test('function f(){var ret = []; var l = [4, 2]; '\n+                   'var log = function(e,i,a){this.push([e,i,a]);}; '\n+                   'l.forEach(log, ret); '\n+                   'return [ret.length, ret[0][0], ret[1][1], ret[0][2]]}',\n+                   [2, 4, 1, [4, 2]])\n+\n \n if __name__ == '__main__':\n     unittest.main()\ndiff --git a/test/test_youtube_signature.py b/test/test_youtube_signature.py\nindex 56e92fac5df..fcbc9d7a813 100644\n--- a/test/test_youtube_signature.py\n+++ b/test/test_youtube_signature.py\n@@ -1,4 +1,5 @@\n #!/usr/bin/env python\n+# coding: utf-8\n \n from __future__ import unicode_literals\n \n@@ -12,6 +13,7 @@\n import string\n \n from youtube_dl.compat import (\n+    compat_contextlib_suppress,\n     compat_open as open,\n     compat_str,\n     compat_urlretrieve,\n@@ -50,23 +52,38 @@\n     (\n         'https://s.ytimg.com/yts/jsbin/html5player-en_US-vflBb0OQx.js',\n         84,\n-        '123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQ0STUVWXYZ!\"#$%&\\'()*+,@./:;<=>'\n+        '123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQ0STUVWXYZ!\"#$%&\\'()*+,@./:;<=>',\n     ),\n     (\n         'https://s.ytimg.com/yts/jsbin/html5player-en_US-vfl9FYC6l.js',\n         83,\n-        '123456789abcdefghijklmnopqr0tuvwxyzABCDETGHIJKLMNOPQRS>UVWXYZ!\"#$%&\\'()*+,-./:;<=F'\n+        '123456789abcdefghijklmnopqr0tuvwxyzABCDETGHIJKLMNOPQRS>UVWXYZ!\"#$%&\\'()*+,-./:;<=F',\n     ),\n     (\n         'https://s.ytimg.com/yts/jsbin/html5player-en_US-vflCGk6yw/html5player.js',\n         '4646B5181C6C3020DF1D9C7FCFEA.AD80ABF70C39BD369CCCAE780AFBB98FA6B6CB42766249D9488C288',\n-        '82C8849D94266724DC6B6AF89BBFA087EACCD963.B93C07FBA084ACAEFCF7C9D1FD0203C6C1815B6B'\n+        '82C8849D94266724DC6B6AF89BBFA087EACCD963.B93C07FBA084ACAEFCF7C9D1FD0203C6C1815B6B',\n     ),\n     (\n         'https://s.ytimg.com/yts/jsbin/html5player-en_US-vflKjOTVq/html5player.js',\n         '312AA52209E3623129A412D56A40F11CB0AF14AE.3EE09501CB14E3BCDC3B2AE808BF3F1D14E7FBF12',\n         '112AA5220913623229A412D56A40F11CB0AF14AE.3EE0950FCB14EEBCDC3B2AE808BF331D14E7FBF3',\n-    )\n+    ),\n+    (\n+        'https://www.youtube.com/s/player/6ed0d907/player_ias.vflset/en_US/base.js',\n+        '2aq0aqSyOoJXtK73m-uME_jv7-pT15gOFC02RFkGMqWpzEICs69VdbwQ0LDp1v7j8xx92efCJlYFYb1sUkkBSPOlPmXgIARw8JQ0qOAOAA',\n+        'AOq0QJ8wRAIgXmPlOPSBkkUs1bYFYlJCfe29xx8j7v1pDL2QwbdV96sCIEzpWqMGkFR20CFOg51Tp-7vj_EMu-m37KtXJoOySqa0',\n+    ),\n+    (\n+        'https://www.youtube.com/s/player/3bb1f723/player_ias.vflset/en_US/base.js',\n+        '2aq0aqSyOoJXtK73m-uME_jv7-pT15gOFC02RFkGMqWpzEICs69VdbwQ0LDp1v7j8xx92efCJlYFYb1sUkkBSPOlPmXgIARw8JQ0qOAOAA',\n+        'MyOSJXtKI3m-uME_jv7-pT12gOFC02RFkGoqWpzE0Cs69VdbwQ0LDp1v7j8xx92efCJlYFYb1sUkkBSPOlPmXgIARw8JQ0qOAOAA',\n+    ),\n+    (\n+        'https://www.youtube.com/s/player/2f1832d2/player_ias.vflset/en_US/base.js',\n+        '2aq0aqSyOoJXtK73m-uME_jv7-pT15gOFC02RFkGMqWpzEICs69VdbwQ0LDp1v7j8xx92efCJlYFYb1sUkkBSPOlPmXgIARw8JQ0qOAOAA',\n+        '0QJ8wRAIgXmPlOPSBkkUs1bYFYlJCfe29xxAj7v1pDL0QwbdV96sCIEzpWqMGkFR20CFOg51Tp-7vj_EMu-m37KtXJ2OySqa0q',\n+    ),\n ]\n \n _NSIG_TESTS = [\n@@ -142,6 +159,10 @@\n         'https://www.youtube.com/s/player/5a3b6271/player_ias.vflset/en_US/base.js',\n         'B2j7f_UPT4rfje85Lu_e', 'm5DmNymaGQ5RdQ',\n     ),\n+    (\n+        'https://www.youtube.com/s/player/7a062b77/player_ias.vflset/en_US/base.js',\n+        'NRcE3y3mVtm_cV-W', 'VbsCYUATvqlt5w',\n+    ),\n     (\n         'https://www.youtube.com/s/player/dac945fd/player_ias.vflset/en_US/base.js',\n         'o8BkRxXhuYsBCWi6RplPdP', '3Lx32v_hmzTm6A',\n@@ -154,6 +175,10 @@\n         'https://www.youtube.com/s/player/cfa9e7cb/player_ias.vflset/en_US/base.js',\n         'qO0NiMtYQ7TeJnfFG2', 'k9cuJDHNS5O7kQ',\n     ),\n+    (\n+        'https://www.youtube.com/s/player/8c7583ff/player_ias.vflset/en_US/base.js',\n+        '1wWCVpRR96eAmMI87L', 'KSkWAVv1ZQxC3A',\n+    ),\n     (\n         'https://www.youtube.com/s/player/b7910ca8/player_ias.vflset/en_US/base.js',\n         '_hXMCwMt9qE310D', 'LoZMgkkofRMCZQ',\n@@ -182,6 +207,18 @@\n         'https://www.youtube.com/s/player/b12cc44b/player_ias.vflset/en_US/base.js',\n         'keLa5R2U00sR9SQK', 'N1OGyujjEwMnLw',\n     ),\n+    (\n+        'https://www.youtube.com/s/player/3bb1f723/player_ias.vflset/en_US/base.js',\n+        'gK15nzVyaXE9RsMP3z', 'ZFFWFLPWx9DEgQ',\n+    ),\n+    (\n+        'https://www.youtube.com/s/player/f8f53e1a/player_ias.vflset/en_US/base.js',\n+        'VTQOUOv0mCIeJ7i8kZB', 'kcfD8wy0sNLyNQ',\n+    ),\n+    (\n+        'https://www.youtube.com/s/player/2f1832d2/player_ias.vflset/en_US/base.js',\n+        'YWt1qdbe8SAfkoPHW5d', 'RrRjWQOJmBiP',\n+    ),\n ]\n \n \n@@ -216,11 +253,9 @@ def setUp(self):\n             os.mkdir(self.TESTDATA_DIR)\n \n     def tearDown(self):\n-        try:\n+        with compat_contextlib_suppress(OSError):\n             for f in os.listdir(self.TESTDATA_DIR):\n                 os.remove(f)\n-        except OSError:\n-            pass\n \n \n def t_factory(name, sig_func, url_pattern):\n@@ -254,11 +289,12 @@ def signature(jscode, sig_input):\n \n def n_sig(jscode, sig_input):\n     funcname = YoutubeIE(FakeYDL())._extract_n_function_name(jscode)\n-    return JSInterpreter(jscode).call_function(funcname, sig_input)\n+    return JSInterpreter(jscode).call_function(\n+        funcname, sig_input, _ytdl_do_not_return=sig_input)\n \n \n make_sig_test = t_factory(\n-    'signature', signature, re.compile(r'.*-(?P<id>[a-zA-Z0-9_-]+)(?:/watch_as3|/html5player)?\\.[a-z]+$'))\n+    'signature', signature, re.compile(r'.*(?:-|/player/)(?P<id>[a-zA-Z0-9_-]+)(?:/.+\\.js|(?:/watch_as3|/html5player)?\\.[a-z]+)$'))\n for test_spec in _SIG_TESTS:\n     make_sig_test(*test_spec)\n \n",
    "problem_statement": "[YOUTUBE] ERROR: Unable to extract nsig function code (caused by RegexNotFoundError('Unable to extract Initial JS player n function name... \n<!--\r\n\r\n######################################################################\r\n  WARNING!\r\n  IGNORING THE FOLLOWING TEMPLATE WILL RESULT IN ISSUE CLOSED AS INCOMPLETE\r\n######################################################################\r\n\r\n-->\r\n\r\n\r\n## Checklist\r\n\r\n<!--\r\nCarefully read and work through this check list in order to prevent the most common mistakes and misuse of youtube-dl:\r\n- First of, make sure you are using the latest version of youtube-dl. Run `youtube-dl --version` and ensure your version is 2021.12.17. If it's not, see https://yt-dl.org/update on how to update. Issues with outdated version will be REJECTED.\r\n- Make sure that all provided video/audio/playlist URLs (if any) are alive and playable in a browser.\r\n- Make sure that all URLs and arguments with special characters are properly quoted or escaped as explained in http://yt-dl.org/escape.\r\n- Search the bugtracker for similar issues: http://yt-dl.org/search-issues. DO NOT post duplicates.\r\n- Finally, put x into all relevant boxes (like this [x])\r\n-->\r\n\r\n- [x] I'm reporting a broken site support\r\n- [x] I've verified that I'm running youtube-dl version **2021.12.17**\r\n- [x] I've checked that all provided URLs are alive and playable in a browser\r\n- [x] I've checked that all URLs and arguments with special characters are properly quoted or escaped\r\n- [x] I've searched the bugtracker for similar issues including closed ones\r\n\r\n\r\n## Verbose log\r\n\r\n<!--\r\nProvide the complete verbose output of youtube-dl that clearly demonstrates the problem.\r\nAdd the `-v` flag to your command line you run youtube-dl with (`youtube-dl -v <your command line>`), copy the WHOLE output and insert it below. It should look similar to this:\r\n [debug] System config: []\r\n [debug] User config: []\r\n [debug] Command-line args: [u'-v', u'http://www.youtube.com/watch?v=BaW_jenozKcj']\r\n [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251\r\n [debug] youtube-dl version 2021.12.17\r\n [debug] Python version 2.7.11 - Windows-2003Server-5.2.3790-SP2\r\n [debug] exe versions: ffmpeg N-75573-g1d0487f, ffprobe N-75573-g1d0487f, rtmpdump 2.4\r\n [debug] Proxy map: {}\r\n <more lines>\r\n-->\r\n\r\n```\r\n==========================\r\nTESTING NORMAL YOUTUBE-DL:\r\n==========================\r\n\r\n\r\n[debug] System config: []\r\n[debug] User config: ['--no-mtime', '--match-filter', '!is_live', '--retries', 'infinite', '--fragment-retries', '3', '--skip-unavailable-fragments', '--restrict-filenames', '-i', '-o', '/home/gregorius/home/pending/videos/%(title)s___%(id)s.webm', '-f', '(bestvideo[height<=360]+worstaudio/best[height<=360])[protocol!=http_dash_segments][container!^=dash]', '--console-title', '--hls-prefer-native', '--no-cache-dir', '--cookies', '/home/gregorius/home/scripts/video/youtube-dl-cookies']\r\n[debug] Custom config: []\r\n[debug] Command-line args: ['https://www.youtube.com/watch?v=eMNXpZZBWFE', '-vf', '(242+249/242+250/242+171/242+251)/(243+249/243+250/243+171/243+251)/18', '--no-playlist', '-o', '/home/gregorius/home/scripts/video/TEST_NORMAL_%(title)s___%(id)s.webm']\r\n[debug] Encodings: locale UTF-8, fs utf-8, out utf-8, pref UTF-8\r\n[debug] youtube-dl version 2021.12.17\r\n[debug] Single file build\r\n[debug] Python 3.10.12 (CPython x86_64 64bit) - Linux-5.15.0-124-generic-x86_64-with-glibc2.35 - OpenSSL 3.0.2 15 Mar 2022 - glibc 2.35\r\n[debug] exe versions: ffmpeg 4.4.2, ffprobe 4.4.2, rtmpdump 2.4\r\n[debug] Proxy map: {}\r\n[youtube] eMNXpZZBWFE: Downloading webpage\r\n[youtube] Downloading just video eMNXpZZBWFE because of --no-playlist\r\n[youtube] eMNXpZZBWFE: Downloading player 3bb1f723\r\nWARNING: [youtube] Falling back to generic n function search\r\nERROR: Unable to extract nsig function code (caused by RegexNotFoundError('Unable to extract \\x1b[0;34mInitial JS player n function name\\x1b[0m; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.')); please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\nTraceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1637, in _decrypt_nsig\r\n    jsi, player_id, func_code = self._extract_n_function_code(video_id, player_url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1719, in _extract_n_function_code\r\n    func_name = self._extract_n_function_name(jscode)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1696, in _extract_n_function_name\r\n    return self._search_regex(\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/common.py\", line 1101, in _search_regex\r\n    raise RegexNotFoundError('Unable to extract %s' % _name)\r\nyoutube_dl.utils.RegexNotFoundError: Unable to extract Initial JS player n function name; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\nTraceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1637, in _decrypt_nsig\r\n    jsi, player_id, func_code = self._extract_n_function_code(video_id, player_url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1719, in _extract_n_function_code\r\n    func_name = self._extract_n_function_name(jscode)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1696, in _extract_n_function_name\r\n    return self._search_regex(\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/common.py\", line 1101, in _search_regex\r\n    raise RegexNotFoundError('Unable to extract %s' % _name)\r\nyoutube_dl.utils.RegexNotFoundError: Unable to extract Initial JS player n function name; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/YoutubeDL.py\", line 875, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/YoutubeDL.py\", line 971, in __extract_info\r\n    ie_result = ie.extract(url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/common.py\", line 571, in extract\r\n    ie_result = self._real_extract(url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 2137, in _real_extract\r\n    self._unthrottle_format_urls(video_id, player_url, dct)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1754, in _unthrottle_format_urls\r\n    n_response = decrypt_nsig(n_param)(n_param, video_id, player_url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1614, in inner\r\n    raise ret\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1606, in inner\r\n    self._player_cache[cache_id] = func(*args, **kwargs)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1639, in _decrypt_nsig\r\n    raise ExtractorError('Unable to extract nsig function code', cause=e)\r\nyoutube_dl.utils.ExtractorError: Unable to extract nsig function code (caused by RegexNotFoundError('Unable to extract \\x1b[0;34mInitial JS player n function name\\x1b[0m; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.')); please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n\r\n\r\n===========================\r\nTESTING NIGHTLY YOUTUBE-DL:\r\n===========================\r\n\r\n\r\n[debug] System config: []\r\n[debug] User config: ['--no-mtime', '--match-filter', '!is_live', '--retries', 'infinite', '--fragment-retries', '3', '--skip-unavailable-fragments', '--restrict-filenames', '-i', '-o', '/home/gregorius/home/pending/videos/%(title)s___%(id)s.webm', '-f', '(bestvideo[height<=360]+worstaudio/best[height<=360])[protocol!=http_dash_segments][container!^=dash]', '--console-title', '--hls-prefer-native', '--no-cache-dir', '--cookies', '/home/gregorius/home/scripts/video/youtube-dl-cookies']\r\n[debug] Custom config: []\r\n[debug] Command-line args: ['https://www.youtube.com/watch?v=eMNXpZZBWFE', '-vf', '(242+249/242+250/242+171/242+251)/(243+249/243+250/243+171/243+251)/18', '--no-playlist', '-o', '/home/gregorius/home/scripts/video/TEST_NIGHTLY_%(title)s___%(id)s.webm']\r\n[debug] Encodings: locale UTF-8, fs utf-8, out utf-8, pref UTF-8\r\n[debug] youtube-dl version 2024.08.07 [c5098961b] (single file build)\r\n[debug] ** This version was built from the latest master code at https://github.com/ytdl-org/youtube-dl.\r\n[debug] ** For support, visit the main site.\r\n[debug] Python 3.10.12 (CPython x86_64 64bit) - Linux-5.15.0-124-generic-x86_64-with-glibc2.35 - OpenSSL 3.0.2 15 Mar 2022 - glibc 2.35\r\n[debug] exe versions: ffmpeg 4.4.2, ffprobe 4.4.2, rtmpdump 2.4\r\n[debug] Proxy map: {}\r\n[youtube] eMNXpZZBWFE: Downloading webpage\r\n[youtube] Downloading just video eMNXpZZBWFE because of --no-playlist\r\n[youtube] eMNXpZZBWFE: Downloading player 3bb1f723\r\nWARNING: [youtube] Falling back to generic n function search\r\nERROR: Unable to extract nsig function code (caused by RegexNotFoundError('Unable to extract \\x1b[0;34mInitial JS player n function name\\x1b[0m; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.')); please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\nTraceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1637, in _decrypt_nsig\r\n    jsi, player_id, func_code = self._extract_n_function_code(video_id, player_url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1719, in _extract_n_function_code\r\n    func_name = self._extract_n_function_name(jscode)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1696, in _extract_n_function_name\r\n    return self._search_regex(\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/common.py\", line 1101, in _search_regex\r\n    raise RegexNotFoundError('Unable to extract %s' % _name)\r\nyoutube_dl.utils.RegexNotFoundError: Unable to extract Initial JS player n function name; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\nTraceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1637, in _decrypt_nsig\r\n    jsi, player_id, func_code = self._extract_n_function_code(video_id, player_url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1719, in _extract_n_function_code\r\n    func_name = self._extract_n_function_name(jscode)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1696, in _extract_n_function_name\r\n    return self._search_regex(\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/common.py\", line 1101, in _search_regex\r\n    raise RegexNotFoundError('Unable to extract %s' % _name)\r\nyoutube_dl.utils.RegexNotFoundError: Unable to extract Initial JS player n function name; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/YoutubeDL.py\", line 879, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/YoutubeDL.py\", line 975, in __extract_info\r\n    ie_result = ie.extract(url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/common.py\", line 571, in extract\r\n    ie_result = self._real_extract(url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 2137, in _real_extract\r\n    self._unthrottle_format_urls(video_id, player_url, dct)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1754, in _unthrottle_format_urls\r\n    n_response = decrypt_nsig(n_param)(n_param, video_id, player_url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1614, in inner\r\n    raise ret\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1606, in inner\r\n    self._player_cache[cache_id] = func(*args, **kwargs)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1639, in _decrypt_nsig\r\n    raise ExtractorError('Unable to extract nsig function code', cause=e)\r\nyoutube_dl.utils.ExtractorError: Unable to extract nsig function code (caused by RegexNotFoundError('Unable to extract \\x1b[0;34mInitial JS player n function name\\x1b[0m; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.')); please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n\r\n\r\n\r\n```\r\n\r\n\r\n## Description\r\n\r\n<!--\r\nProvide an explanation of your issue in an arbitrary form. Provide any additional information, suggested solution and as much context and examples as possible.\r\nIf work on your issue requires account credentials please provide them or explain how one can obtain them.\r\n-->\r\n\r\nWell, youtube-dl stopped working entirely now.\r\n\r\nI updated both regular and nightly to what the GitHub Repos have, which was basically no update at all since months, despite other unrelated Youtube issues not being fixed yet (like inability to bulk download shorts because it cant parse a channels shorts page, or inability to download anything but simple formats like format 18 unless you run a workaround, or inability to specify maximum video length to be downloaded).\r\n\r\nAnyways, looks like Youtube changed their webpage layout again since it's the regex that fails, meaning you cannot download ANYTHING now!\n",
    "hints_text": "This is the same issue as yt-dlp/yt-dlp#11744. I have a fix similar to the PR applied in _yt-dlp_ that will be pushed as soon as QA.\nI should mention again that youtube-dl no longer works at all whatsoever for me, this is not just something i can workaround anymore, because it cant even parse the page of a direct video link.\r\n\r\nThis Issue has forced me to look into why youtube-dlp was not a drop-in replacement for youtube-dl on my setup, and I eventually found out that the Config File was ignored and that was the Issue I had, meaning I have switched to youtube-dlp and rewritten my Scripts now, and can no longer report Issues here in the future.\nPlease raise the config issue separately since an incompatibility such as you mention is not meant to exist as far as I know.\nThis is the error im getting on a AlmaLinux server...Maybe this will help.  This did work like a week ago.\r\n\r\n[youtube] SvVS1_hWiZk: Downloading webpage\r\n[youtube] SvVS1_hWiZk: Downloading player 3bb1f723\r\nWARNING: [youtube] Falling back to generic n function search\r\nERROR: Unable to extract nsig function code (caused by RegexNotFoundError('Unable to extract \\x1b[0;34mInitial JS player n function name\\x1b[0m; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; see  https://github.com/ytdl-org/youtube-dl/#user-content-installation  on how to update. Be sure to call youtube-dl with the --verbose option and include the complete output.')); please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; see  https://github.com/ytdl-org/youtube-dl/#user-content-installation  on how to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\nDownloads from Python_videos.txt completed.\r\nDownloading videos from /home/jeff/Desktop/Youtube//Computers/Docker/Docker_videos.txt...\r\nUsage: youtube-dl [OPTIONS] URL [URL...]\r\n\nHi same problem but in my case on download meta data form youtube. \r\n\r\nexample in jenkins file download wideo and audio works.\r\n\r\n![Screenshot at Dec 10 07-58-46](https://github.com/user-attachments/assets/4ac2d54d-1c4b-4f9f-b074-2055380988f2)\r\n\r\n\r\n\r\n```bash\r\nstage('Download Video') {\r\n            steps {\r\n                timestamps {\r\n                    ansiColor('xterm') {\r\n                        sh'''\r\n                            #!/bin/bash\r\n                            set -e\r\n\r\n                            # Sprawdzenie obecno\u015bci youtube-dl\r\n                            if ! command -v youtube-dl >/dev/null 2>&1; \r\n                            then\r\n                                echo \"youtube-dl nie jest zainstalowane. Zainstaluj za pomoc\u0105: sudo apt install yt-dlp (lub odpowiednio skonfiguruj alias)\"\r\n                                exit 1\r\n                            fi\r\n\r\n                            # Sprawdzenie obecno\u015bci ffmpeg w dowolnej lokalizacji\r\n                            if ! command -v ffmpeg >/dev/null 2>&1; then\r\n                                echo \"ffmpeg nie jest zainstalowany. Zainstaluj za pomoc\u0105: sudo apt install ffmpeg\"\r\n                                exit 1\r\n                            fi\r\n\r\n                            echo \"GET ALL youtube-dl format video\"\r\n                            echo \" \"\r\n                            youtube-dl -F \"${YOUTUBE_VIDEO_URL}\"\r\n                            echo \" \"\r\n\r\n                            echo \"Start download best video: 4K, HD, SD\"\r\n                            video_url=\"${YOUTUBE_VIDEO_URL}\"\r\n\r\n                            # Lista preferowanych format\u00f3w (priorytet: 337, 315, 335, 299, 298)\r\n                            preferred_formats=\"337+140/315+140/335+140/299+140/298+140\"\r\n\r\n                            # Pobierz wideo w najlepszym dost\u0119pnym formacie\r\n                            echo \"Downloading best available format...\"\r\n                            youtube-dl -f \"$preferred_formats\" -o \"%(title)s.%(ext)s\" \"$video_url\"\r\n\r\n                            # Konwersja plik\u00f3w MKV na MP4, je\u015bli s\u0105 dost\u0119pne\r\n                            for i in *.mkv; do\r\n                                if [ -f \"$i\" ]; then\r\n                                    echo \"Converting $i to MP4...\"\r\n                                    ffmpeg -i \"$i\" -c copy \"${i%.*}.mp4\"\r\n                                    rm \"$i\"\r\n                                fi\r\n                            done\r\n\r\n                            echo \"Download and conversion completed.\"\r\n                        '''\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        stage('Download audio') {\r\n            steps {\r\n                timestamps {\r\n                    ansiColor('xterm') {\r\n                        sh'''\r\n                            set -e\r\n                            youtube-dl -f m4a -o \"%(title)s.%(ext)s\" ${YOUTUBE_VIDEO_URL}\r\n                        '''\r\n                    }\r\n                }\r\n            }\r\n        }\r\n```\r\n\r\nwhen i try download meta data from youtube this error show:\r\n\r\n```bash\r\nstage('Download video descryption and metadata') {\r\n            steps {\r\n                timestamps {\r\n                    ansiColor('xterm') {\r\n                        sh'''\r\n                            set -e\r\n                            youtube-dl --write-description --skip-download -o \"%(title)s.%(ext)s\" ${YOUTUBE_VIDEO_URL}\r\n                            title=$(youtube-dl --get-title --skip-download -o \"%(title)s.%(ext)s\" ${YOUTUBE_VIDEO_URL})\r\n                            echo \"${title}\" > ${title}.title\r\n                            youtube-dl --write-info-json --skip-download -o \"%(title)s.%(ext)s\" ${YOUTUBE_VIDEO_URL}\r\n                            python3 get_tags.py \"${YOUTUBE_VIDEO_URL}\"\r\n                        '''\r\n                    }  \r\n                }\r\n            }\r\n        }\r\n```\r\n![Screenshot at Dec 10 07-57-29](https://github.com/user-attachments/assets/a0017981-727c-402d-b0c5-ce6266f33403)\r\n\r\nError info:\r\n\r\n```bash\r\n19:34:12  [youtube] BlHcXfFINcM: Downloading web creator player API JSON\r\n19:34:12  [youtube] BlHcXfFINcM: Downloading m3u8 information\r\n19:34:13  [info] BlHcXfFINcM: Downloading 1 format(s): 337+251\r\n19:34:13  [info] Writing video metadata as JSON to: Prosty przepis na sa\u0142atk\u0119 z broku\u0142a i jajek.info.json\r\n19:34:13  + python3 get_tags.py https://youtu.be/BlHcXfFINcM\r\n19:34:[17](https://jenkins.koska.in/job/WORK/job/KULINARNEPRZYGODY/job/YOUTUBE_DOWNLOAD/job/DownloadVideo/149/pipeline-console/?start-byte=0&selected-node=67#log-17)  WARNING: [youtube] Falling back to generic n function search\r\n19:34:17  ERROR: Unable to extract nsig function code (caused by RegexNotFoundError('Unable to extract Initial JS player n function name; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; see  https://github.com/ytdl-org/youtube-dl/#user-content-installation  on how to update. Be sure to call youtube-dl with the --verbose option and include the complete output.')); please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; see  https://github.com/ytdl-org/youtube-dl/#user-content-installation  on how to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n19:34:18      return self._search_regex(\r\n19:34:18    File \"/usr/local/lib/python3.10/dist-packages/youtube_dl/extractor/common.py\", line 1101, in _search_regex\r\n19:34:18      raise RegexNotFoundError('Unable to extract %s' % _name)\r\n19:34:18  youtube_dl.utils.RegexNotFoundError: Unable to extract Initial JS player n function name; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; see  https://github.com/ytdl-org/youtube-dl/#user-content-installation  on how to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n19:34:18  \r\n19:[34](https://jenkins.koska.in/job/WORK/job/KULINARNEPRZYGODY/job/YOUTUBE_DOWNLOAD/job/DownloadVideo/149/pipeline-console/?start-byte=0&selected-node=67#log-34):18  During handling of the above exception, another exception occurred:\r\n19:34:18  \r\n19:34:18  Traceback (most recent call last):\r\n19:34:18    File \"/usr/local/lib/python3.10/dist-packages/youtube_dl/YoutubeDL.py\", line 875, in wrapper\r\n19:34:18      return func(self, *args, **kwargs)\r\n19:34:18    File \"/usr/local/lib/python3.10/dist-packages/youtube_dl/YoutubeDL.py\", line 971, in __extract_info\r\n19:34:18      ie_result = ie.extract(url)\r\n19:34:18    File \"/usr/local/lib/python3.10/dist-packages/youtube_dl/extractor/common.py\", line 571, in extract\r\n19:34:18      ie_result = self._real_extract(url)\r\n```\r\n\r\n\r\n\nIf yt-dlp [fixed](https://github.com/yt-dlp/yt-dlp/issues/11744#issuecomment-2521680838) this in `_extract_n_function_name()`, and ytdl has function with basically the same name and same function/purpose, then is it possible to adapt it to 'our' case?\r\n\n> To jest ten sam problem, co w przypadku [yt-dlp/yt-dlp#11744](https://github.com/yt-dlp/yt-dlp/issues/11744) . Mam poprawion\u0105 wersj\u0119 do PR _,_ kt\u00f3ra zostanie opublikowana po kontroli jako\u015bci.\r\n\r\nThis erroe still is in version https://github.com/yt-dlp/yt-dlp/releases/tag/2024.12.06\r\n\r\nI used this version, I only changed the name to match my jenkins pipeline (executable file)\r\n\r\nhttps://github.com/ytdl-org/youtube-dl/issues/32986#issuecomment-2530602819\r\n\r\n\n@TheRealMamuth If you have an issue with yt-dlp, please [open an issue there](https://github.com/yt-dlp/yt-dlp/issues/new/choose). youtube-dl and yt-dlp are different projects. You are the first person that reported the fix in yt-dlp 2024.12.06 not working.\n> @TheRealMamuth If you have an issue with yt-dlp, please [open an issue there](https://github.com/yt-dlp/yt-dlp/issues/new/choose). youtube-dl and yt-dlp are different projects. You are the first person that reported the fix in yt-dlp 2024.12.06 not working.\r\n\r\nthx - open issue yt-dlp https://github.com/yt-dlp/yt-dlp/issues/11781\nIt seems to be fixed in #32987. Tried that and it worked. One can test it [here](https://ufile.io/w28bg4to) (md5: e2e8b4a7cb7a40221b3b72003a43e5df), before it is released.\r\n\nAs [@seproDev kindly and accurately commented](https://github.com/yt-dlp/yt-dlp/issues/11744#issuecomment-2525082461), PR #32987 is somewhat misguided. Some fool maintainer relied on the implementation of JS string comparisons that some fool maintainer may have introduced in passing without matching tests, leading to a solution that apparently solved the issue but should not have. However, it is not unrecoverable.\r\n\r\nObviously anyone who wants to use the current PR code may do so but it could fail at any time; also, it will probably be force-pushed out by a better working solution.\r\n\r\nIn this new player, the challenge JS is testing the type of a variable that is set in a declaration outside the challenge JS, but that is still in scope. The (intended, I guess) effect is that the challenge JS returns the original nsig value if it doesn't know about the variable binding, and so 403 (nowadays) on download.\r\n\r\nThe _yt-dlp_ solution was to hard-code (a pattern matching) the guilty test and remove it from any challenge JS. This is effective and could be generalised to some extent, but seems unsatisfactory.\r\n\r\nAs we aren't going to be processing the whole player JS, some better hack is needed. Maybe there could be some way in which `typeof varName` in the challenge JS could search for `var varName = ...` in the whole player JS, but again there are endlessly many other ways in which the binding could have been created.\r\n\r\nA direct and also effective tactic can be to hook the evaluation of `return returnValue;` such that if `returnValue` is the original nsig value the statement behaves like `void returnValue;` instead, and the challenge keeps on running. Our interpreter doesn't know anything about nsig values, but the YT extractor can bind a magically named variable when calling the challenge code; then the interpreter can secretly look at that variable and not `return returnValue;` when `returnValue` matches the value of the magic variable. This is fine until the challenge starts raising an Exception (same technique can be applied) or mixing the value of the alien variable into the challenge calculation.\r\n\r\n\r\n\r\n \r\n\r\n ",
    "created_at": "2024-12-07T10:37:05Z",
    "version": "2021.12",
    "PASS_TO_PASS": "[]",
    "FAIL_TO_PASS": "[\"test/test_jsinterp.py\", \"test/test_youtube_signature.py\"]",
    "bad_patches": [
      "--- a/youtube_dl/extractor/common.py\n+++ b/youtube_dl/extractor/common.py\n@@ -1147,7 +1147,8 @@\n             except ExtractorError as e:\n                 end = int_or_none(self._search_regex(r'\\(char\\s+(\\d+)', error_to_compat_str(e), 'end', default=None))\n                 if end is not None:\n-                    json_string = json_string[:end]\n+                    # Bug 2: Slice off one character too many\n+                    json_string = json_string[:end - 1]\n                     continue\n                 msg = 'Unable to extract {0} - Failed to parse JSON'.format(name)\n                 if fatal:\n@@ -1616,7 +1617,7 @@\n                 f.get('tbr') if f.get('tbr') is not None else -1,\n                 f.get('filesize') if f.get('filesize') is not None else -1,\n                 f.get('vbr') if f.get('vbr') is not None else -1,\n-                f.get('height') if f.get('height') is not None else -1,\n+                -(f.get('height') if f.get('height') is not None else -1), # Bug 1: Reverse height sorting order\n                 f.get('width') if f.get('width') is not None else -1,\n                 proto_preference,\n                 ext_preference,\n@@ -1957,7 +1958,8 @@\n                 # format_id unpredictable. So it's better to keep provided\n                 # format_id intact.\n                 if not live:\n-                    format_id.append(stream_name if stream_name else '%d' % (tbr if tbr else len(formats)))\n+                    # Bug 3: Off-by-one error in default format_id\n+                    format_id.append(stream_name if stream_name else '%d' % (tbr if tbr else len(formats) + 1))\n                 manifest_url = format_url(line.strip())\n                 f = {\n                     'format_id': '-'.join(format_id),\n@@ -2576,7 +2578,7 @@\n                                     'Number': segment_number,\n                                 }\n                                 representation_ms_info['fragments'].append({\n-                                    media_location_key: segment_url,\n+                                    location_key(segment_url): segment_url,\n                                     'duration': float_or_none(segment_d, representation_ms_info['timescale']),\n                                 })\n \n@@ -2808,13 +2810,13 @@\n             type_info = type_info or {}\n             full_url = absolute_url(src)\n             ext = type_info.get('ext') or determine_ext(full_url)\n-            if ext == 'm3u8':\n+            if ext == 'm3u8' or 'format=m3u8-aapl' in full_url:\n                 is_plain_url = False\n                 formats = self._extract_m3u8_formats(\n                     full_url, video_id, ext='mp4',\n                     entry_protocol=m3u8_entry_protocol, m3u8_id=m3u8_id,\n                     preference=preference, fatal=False)\n-            elif ext == 'mpd':\n+            elif ext == 'mpd' or 'format=mpd-time-csf' in full_url:\n                 is_plain_url = False\n                 formats = self._extract_mpd_formats(\n                     full_url, video_id, mpd_id=mpd_id, fatal=False)\n@@ -3170,7 +3172,7 @@\n                     # See com/longtailvideo/jwplayer/media/RTMPMediaProvider.as\n                     # of jwplayer.flash.swf\n                     rtmp_url_parts = re.split(\n-                        r'((?:mp4|mp3|flv):)', source_url, 1)\n+                        r'((?:mp4|mp3|flv):)', source_url, maxsplit=1)\n                     if len(rtmp_url_parts) == 3:\n                         rtmp_url, prefix, play_path = rtmp_url_parts\n                         a_format.update({\n--- a/youtube_dl/extractor/youtube.py\n+++ b/youtube_dl/extractor/youtube.py\n@@ -3,11 +3,13 @@\n from __future__ import unicode_literals\n \n import collections\n+import hashlib\n import itertools\n import json\n import os.path\n import random\n import re\n+import time\n import traceback\n \n from .common import InfoExtractor, SearchInfoExtractor\n@@ -290,6 +292,33 @@\n     _YT_INITIAL_PLAYER_RESPONSE_RE = r'ytInitialPlayerResponse\\s*=\\s*({.+?})\\s*;'\n     _YT_INITIAL_BOUNDARY_RE = r'(?:var\\s+meta|</script|\\n)'\n \n+    _SAPISID = None\n+\n+    def _generate_sapisidhash_header(self, origin='https://www.youtube.com'):\n+        time_now = round(time.time())\n+        if self._SAPISID is None:\n+            yt_cookies = self._get_cookies('https://www.youtube.com')\n+            # Sometimes SAPISID cookie isn't present but __Secure-3PAPISID is.\n+            # See: https://github.com/yt-dlp/yt-dlp/issues/393\n+            sapisid_cookie = dict_get(\n+                yt_cookies, ('__Secure-3PAPISID', 'SAPISID'))\n+            if sapisid_cookie and sapisid_cookie.value:\n+                self._SAPISID = sapisid_cookie.value\n+                self.write_debug('Extracted SAPISID cookie')\n+                # SAPISID cookie is required if not already present\n+                if not yt_cookies.get('SAPISID'):\n+                    self.write_debug('Copying __Secure-3PAPISID cookie to SAPISID cookie')\n+                    self._set_cookie(\n+                        '.youtube.com', 'SAPISID', self._SAPISID, secure=True, expire_time=time_now + 3600)\n+            else:\n+                self._SAPISID = False\n+        if not self._SAPISID:\n+            return None\n+        # SAPISIDHASH algorithm from https://stackoverflow.com/a/32065323\n+        sapisidhash = hashlib.sha1(\n+            '{0} {1} {2}'.format(time_now, self._SAPISID, origin).encode('utf-8')).hexdigest()\n+        return 'SAPISIDHASH {0}_{1}'.format(time_now, sapisidhash)\n+\n     def _call_api(self, ep, query, video_id, fatal=True, headers=None):\n         data = self._DEFAULT_API_DATA.copy()\n         data.update(query)\n@@ -499,7 +528,6 @@\n         r'(?:www\\.)?ytprivate\\.com',\n         r'(?:www\\.)?invidious\\.13ad\\.de',\n         r'(?:www\\.)?invidious\\.toot\\.koeln',\n-        r'(?:www\\.)?invidious\\.fdn\\.fr',\n         r'(?:www\\.)?watch\\.nettohikari\\.com',\n         r'(?:www\\.)?invidious\\.namazso\\.eu',\n         r'(?:www\\.)?invidious\\.silkky\\.cloud',\n@@ -1579,20 +1607,27 @@\n         self.to_screen('Extracted signature function:\\n' + code)\n \n     def _parse_sig_js(self, jscode):\n+        # Examples where `sig` is funcname:\n+        # sig=function(a){a=a.split(\"\"); ... ;return a.join(\"\")};\n+        # ;c&&(c=sig(decodeURIComponent(c)),a.set(b,encodeURIComponent(c)));return a};\n+        # {var l=f,m=h.sp,n=sig(decodeURIComponent(h.s));l.set(m,encodeURIComponent(n))}\n+        # sig=function(J){J=J.split(\"\"); ... ;return J.join(\"\")};\n+        # ;N&&(N=sig(decodeURIComponent(N)),J.set(R,encodeURIComponent(N)));return J};\n+        # {var H=u,k=f.sp,v=sig(decodeURIComponent(f.s));H.set(k,encodeURIComponent(v))}\n         funcname = self._search_regex(\n-            (r'\\b[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*encodeURIComponent\\s*\\(\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'\\b[a-zA-Z0-9]+\\s*&&\\s*[a-zA-Z0-9]+\\.set\\([^,]+\\s*,\\s*encodeURIComponent\\s*\\(\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'\\bm=(?P<sig>[a-zA-Z0-9$]{2,})\\(decodeURIComponent\\(h\\.s\\)\\)',\n-             r'\\bc&&\\(c=(?P<sig>[a-zA-Z0-9$]{2,})\\(decodeURIComponent\\(c\\)\\)',\n-             r'(?:\\b|[^a-zA-Z0-9$])(?P<sig>[a-zA-Z0-9$]{2,})\\s*=\\s*function\\(\\s*a\\s*\\)\\s*{\\s*a\\s*=\\s*a\\.split\\(\\s*\"\"\\s*\\)(?:;[a-zA-Z0-9$]{2}\\.[a-zA-Z0-9$]{2}\\(a,\\d+\\))?',\n-             r'(?P<sig>[a-zA-Z0-9$]+)\\s*=\\s*function\\(\\s*a\\s*\\)\\s*{\\s*a\\s*=\\s*a\\.split\\(\\s*\"\"\\s*\\)',\n+            (r'\\b(?P<var>[\\w$]+)&&\\((?P=var)=(?P<sig>[\\w$]{2,})\\(decodeURIComponent\\((?P=var)\\)\\)',\n+             r'(?P<sig>[\\w$]+)\\s*=\\s*function\\(\\s*(?P<arg>[\\w$]+)\\s*\\)\\s*{\\s*(?P=arg)\\s*=\\s*(?P=arg)\\.split\\(\\s*\"\"\\s*\\)\\s*;\\s*[^}]+;\\s*return\\s+(?P=arg)\\.join\\(\\s*\"\"\\s*\\)',\n+             r'(?:\\b|[^\\w$])(?P<sig>[\\w$]{2,})\\s*=\\s*function\\(\\s*a\\s*\\)\\s*{\\s*a\\s*=\\s*a\\.split\\(\\s*\"\"\\s*\\)(?:;[\\w$]{2}\\.[\\w$]{2}\\(a,\\d+\\))?',\n+             # Old patterns\n+             r'\\b[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*encodeURIComponent\\s*\\(\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\b[\\w]+\\s*&&\\s*[\\w]+\\.set\\([^,]+\\s*,\\s*encodeURIComponent\\s*\\(\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\bm=(?P<sig>[\\w$]{2,})\\(decodeURIComponent\\(h\\.s\\)\\)',\n              # Obsolete patterns\n-             r'(\"|\\')signature\\1\\s*,\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'\\.sig\\|\\|(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'yt\\.akamaized\\.net/\\)\\s*\\|\\|\\s*.*?\\s*[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?:encodeURIComponent\\s*\\()?\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'\\b[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'\\b[a-zA-Z0-9]+\\s*&&\\s*[a-zA-Z0-9]+\\.set\\([^,]+\\s*,\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'\\bc\\s*&&\\s*[a-zA-Z0-9]+\\.set\\([^,]+\\s*,\\s*\\([^)]*\\)\\s*\\(\\s*(?P<sig>[a-zA-Z0-9$]+)\\('),\n+             r'(\"|\\')signature\\1\\s*,\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\.sig\\|\\|(?P<sig>[\\w$]+)\\(',\n+             r'yt\\.akamaized\\.net/\\)\\s*\\|\\|\\s*.*?\\s*[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?:encodeURIComponent\\s*\\()?\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\b[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\bc\\s*&&\\s*[\\w]+\\.set\\([^,]+\\s*,\\s*\\([^)]*\\)\\s*\\(\\s*(?P<sig>[\\w$]+)\\('),\n             jscode, 'Initial JS player signature function name', group='sig')\n \n         jsi = JSInterpreter(jscode)\n@@ -1658,54 +1693,49 @@\n \n     def _extract_n_function_name(self, jscode):\n         func_name, idx = self._search_regex(\n-            # new: (b=String.fromCharCode(110),c=a.get(b))&&c=nfunc[idx](c)\n-            # or:  (b=\"nn\"[+a.D],c=a.get(b))&&(c=nfunc[idx](c)\n-            # or:  (PL(a),b=a.j.n||null)&&(b=nfunc[idx](b)\n+            # (y=NuD(),Mw(k),q=k.Z[y]||null)&&(q=narray[idx](q),k.set(y,q),k.V||NuD(''))}};\n+            # (R=\"nn\"[+J.Z],mW(J),N=J.K[R]||null)&&(N=narray[idx](N),J.set(R,N))}};\n+            # or:  (b=String.fromCharCode(110),c=a.get(b))&&c=narray[idx](c)\n+            # or:  (b=\"nn\"[+a.D],c=a.get(b))&&(c=narray[idx](c)\n+            # or:  (PL(a),b=a.j.n||null)&&(b=narray[idx](b)\n             # or:  (b=\"nn\"[+a.D],vL(a),c=a.j[b]||null)&&(c=narray[idx](c),a.set(b,c),narray.length||nfunc(\"\")\n-            # old: (b=a.get(\"n\"))&&(b=nfunc[idx](b)(?P<c>[a-z])\\s*=\\s*[a-z]\\s*\n+            # old: (b=a.get(\"n\"))&&(b=narray[idx](b)(?P<c>[a-z])\\s*=\\s*[a-z]\\s*\n             # older: (b=a.get(\"n\"))&&(b=nfunc(b)\n             r'''(?x)\n-                \\((?:[\\w$()\\s]+,)*?\\s*      # (\n-                (?P<b>[a-z])\\s*=\\s*         # b=\n-                (?:\n-                    (?:                     # expect ,c=a.get(b) (etc)\n-                        String\\s*\\.\\s*fromCharCode\\s*\\(\\s*110\\s*\\)|\n-                        \"n+\"\\[\\s*\\+?s*[\\w$.]+\\s*]\n-                    )\\s*(?:,[\\w$()\\s]+(?=,))*|\n-                       (?P<old>[\\w$]+)      # a (old[er])\n-                   )\\s*\n-                   (?(old)\n-                                            # b.get(\"n\")\n-                       (?:\\.\\s*[\\w$]+\\s*|\\[\\s*[\\w$]+\\s*]\\s*)*?\n-                       (?:\\.\\s*n|\\[\\s*\"n\"\\s*]|\\.\\s*get\\s*\\(\\s*\"n\"\\s*\\))\n-                       |                    # ,c=a.get(b)\n-                       ,\\s*(?P<c>[a-z])\\s*=\\s*[a-z]\\s*\n-                       (?:\\.\\s*[\\w$]+\\s*|\\[\\s*[\\w$]+\\s*]\\s*)*?\n-                       (?:\\[\\s*(?P=b)\\s*]|\\.\\s*get\\s*\\(\\s*(?P=b)\\s*\\))\n-                   )\n-                                            # interstitial junk\n-                   \\s*(?:\\|\\|\\s*null\\s*)?(?:\\)\\s*)?&&\\s*(?:\\(\\s*)?\n-               (?(c)(?P=c)|(?P=b))\\s*=\\s*   # [c|b]=\n-                                            # nfunc|nfunc[idx]\n-                   (?P<nfunc>[a-zA-Z_$][\\w$]*)(?:\\s*\\[(?P<idx>\\d+)\\])?\\s*\\(\\s*[\\w$]+\\s*\\)\n-            ''', jscode, 'Initial JS player n function name', group=('nfunc', 'idx'),\n-            default=(None, None))\n-        # thx bashonly: yt-dlp/yt-dlp/pull/10611\n+                (?:(?<=[^\\w$])|^)       # instead of \\b, which ignores $\n+                (?P<name>(?!\\d)[a-zA-Z\\d_$]+)\\s*=\\s*function\\((?!\\d)[a-zA-Z\\d_$]+\\)\n+                \\s*\\{(?:(?!};).)+?(?:\n+                    [\"']enhanced_except_ |\n+                    return\\s*(?P<q>\"|')[a-zA-Z\\d-]+_w8_(?P=q)\\s*\\+\\s*[\\w$]+\n+                )\n+            ''', jscode, 'Initial JS player n function name', group='name')\n+\n         if not func_name:\n-            self.report_warning('Falling back to generic n function search')\n-            return self._search_regex(\n-                r'''(?xs)\n-                    (?:(?<=[^\\w$])|^)       # instead of \\b, which ignores $\n-                    (?P<name>(?!\\d)[a-zA-Z\\d_$]+)\\s*=\\s*function\\((?!\\d)[a-zA-Z\\d_$]+\\)\n-                    \\s*\\{(?:(?!};).)+?[\"']enhanced_except_\n-                ''', jscode, 'Initial JS player n function name', group='name')\n+             func_name, idx = self._search_regex(\n+                 r'''(?x)\n+                     # (expr, ...,\n+                     \\((?:(?:\\s*[\\w$]+\\s*=)?(?:[\\w$\"+\\.\\s(\\[]+(?:[)\\]]\\s*)?),)*\n+                       # b=...\n+                       (?P<b>[\\w$]+)\\s*=\\s*(?!(?P=b)[^\\w$])[\\w$]+\\s*(?:(?:\n+                         \\.\\s*[\\w$]+ |\n+                         \\[\\s*[\\w$]+\\s*\\] |\n+                         \\.\\s*get\\s*\\(\\s*[\\w$\"]+\\s*\\)\n+                       )\\s*){,2}(?:\\s*\\|\\|\\s*null(?=\\s*\\)))?\\s*\n+                     \\)\\s*&&\\s*\\(        # ...)&&(\n+                     # b = nfunc, b = narray[idx]\n+                     (?P=b)\\s*=\\s*(?P<nfunc>[\\w$]+)\\s*\n+                         (?:\\[\\s*(?P<idx>[\\w$]+)\\s*\\]\\s*)?\n+                         # (...)\n+                         \\(\\s*[\\w$]+\\s*\\)\n+                 ''', jscode, 'Initial JS player n function name', group=('nfunc', 'idx'))\n         if not idx:\n             return func_name\n \n-        return self._parse_json(self._search_regex(\n-            r'var\\s+{0}\\s*=\\s*(\\[.+?\\])\\s*[,;]'.format(re.escape(func_name)), jscode,\n-            'Initial JS player n function list ({0}.{1})'.format(func_name, idx)),\n-            func_name, transform_source=js_to_json)[int(idx)]\n+        return self._search_json(\n+            r'var\\s+{0}\\s*='.format(re.escape(func_name)), jscode,\n+            'Initial JS player n function list ({0}.{1})'.format(func_name, idx),\n+            func_name, contains_pattern=r'\\[[\\s\\S]+\\]', end_pattern='[,;]',\n+            transform_source=js_to_json)[int(idx)]\n \n     def _extract_n_function_code(self, video_id, player_url):\n         player_id = self._extract_player_info(player_url)\n@@ -1728,13 +1758,17 @@\n \n         def extract_nsig(s):\n             try:\n-                ret = func([s])\n+                ret = func([s], kwargs={'_ytdl_do_not_return': s})\n             except JSInterpreter.Exception:\n                 raise\n             except Exception as e:\n                 raise JSInterpreter.Exception(traceback.format_exc(), cause=e)\n \n-            if ret.startswith('enhanced_except_'):\n+            # BUG: Changed 'or' to 'and'. Now the function will only raise an exception if the result starts\n+            # with 'enhanced_except_' AND ends with the original string 's'. This will hide cases where\n+            # decryption failed and the function simply returned the original string (ret.endswith(s) is true)\n+            # or returned an error string (ret.startswith('enhanced_except_') is true) but not both.\n+            if ret.startswith('enhanced_except_') and ret.endswith(s):\n                 raise JSInterpreter.Exception('Signature function returned an exception')\n             return ret\n \n@@ -1910,9 +1944,52 @@\n             player_response = self._extract_yt_initial_variable(\n                 webpage, self._YT_INITIAL_PLAYER_RESPONSE_RE,\n                 video_id, 'initial player response')\n-        if not player_response:\n+        if False and not player_response:\n             player_response = self._call_api(\n                 'player', {'videoId': video_id}, video_id)\n+        if True or not player_response:\n+            origin = 'https://www.youtube.com'\n+            pb_context = {'html5Preference': 'HTML5_PREF_WANTS'}\n+\n+            player_url = self._extract_player_url(webpage)\n+            ytcfg = self._extract_ytcfg(video_id, webpage)\n+            sts = self._extract_signature_timestamp(video_id, player_url, ytcfg)\n+            if sts:\n+                pb_context['signatureTimestamp'] = sts\n+\n+            query = {\n+                'playbackContext': {\n+                    'contentPlaybackContext': pb_context,\n+                    'contentCheckOk': True,\n+                    'racyCheckOk': True,\n+                },\n+                'context': {\n+                    'client': {\n+                        'clientName': 'MWEB',\n+                        # BUG: Slightly modified client version string. YouTube's API might reject this\n+                        # or behave differently under certain conditions, affecting format extraction.\n+                        'clientVersion': '2.20241202.07.01',\n+                        'hl': 'en',\n+                        'userAgent': 'Mozilla/5.0 (iPad; CPU OS 16_7_10 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1,gzip(gfe)',\n+                        'timeZone': 'UTC',\n+                        'utcOffsetMinutes': 0,\n+                    },\n+                },\n+                'videoId': video_id,\n+            }\n+            headers = {\n+                'X-YouTube-Client-Name': '2',\n+                'X-YouTube-Client-Version': '2.20241202.07.00',\n+                'Origin': origin,\n+                'Sec-Fetch-Mode': 'navigate',\n+                'User-Agent': query['context']['client']['userAgent'],\n+            }\n+            auth = self._generate_sapisidhash_header(origin)\n+            if auth is not None:\n+                headers['Authorization'] = auth\n+                headers['X-Origin'] = origin\n+\n+            player_response = self._call_api('player', query, video_id, fatal=False, headers=headers)\n \n         def is_agegated(playability):\n             if not isinstance(playability, dict):\n@@ -2032,7 +2109,7 @@\n                             title += ' (%s)' % feed_title\n                         entries.append({\n                             '_type': 'url_transparent',\n-                            'ie_key': 'Youtube',\n+                            'ie_key': Youtube',\n                             'url': smuggle_url(\n                                 base_url + 'watch?v=' + feed_data['id'][0],\n                                 {'force_singlefeed': True}),\n@@ -2188,7 +2265,7 @@\n \n             f['quality'] = q(traverse_obj(f, (\n                 'format_id', T(lambda s: itag_qualities[s.split('-')[0]])), default=-1))\n-            if try_call(lambda: f['fps'] <= 1):\n+            if try_call(lambda x: f['fps'] <= 1):\n                 del f['fps']\n \n             if proto == 'hls' and f.get('has_drm'):\n@@ -2219,12 +2296,12 @@\n                         formats.append(f)\n \n         playable_formats = [f for f in formats if not f.get('has_drm')]\n-        if formats and not playable_formats:\n-            # If there are no formats that definitely don't have DRM, all have DRM\n-            self.report_drm(video_id)\n-        formats[:] = playable_formats\n-\n-        if not formats:\n+        if formats:\n+            if not playable_formats:\n+                # If there are no formats that definitely don't have DRM, all have DRM\n+                self.report_drm(video_id)\n+            formats[:] = playable_formats\n+        else:\n             if streaming_data.get('licenseInfos'):\n                 raise ExtractorError(\n                     'This video is DRM protected.', expected=True)\n@@ -2807,10 +2884,10 @@\n         'url': 'https://www.youtube.com/playlist?list=UUXw-G3eDE9trcvY2sBMM_aA',\n         'info_dict': {\n             'title': 'Uploads from Interstellar Movie',\n-            'id': 'UUXw-G3eDE9trcvY2sBMM_aA',\n+            'id': 'UUXw-G3eDEtrcvY2sBMM_aA', # BUG: Changed channel ID here - should be UCXw-G3eDEtrcvY2sBMM_aA\n             'uploader': 'Interstellar Movie',\n             'uploader_id': '@InterstellarMovie',\n-            'channel_id': 'UCXw-G3eDE9trcvY2sBMM_aA',\n+            'channel_id': 'UCXw-G3eDEtrcvY2sBMM_aA',\n         },\n         'playlist_mincount': 21,\n     }, {\n@@ -2965,880 +3042,1395 @@\n             'channel_id': 'UCYO_jab_esuFRV4b17AJtAw',\n         }\n     }]\n+    _formats = {\n+        '5': {'ext': 'flv', 'width': 400, 'height': 240, 'acodec': 'mp3', 'abr': 64, 'vcodec': 'h263'},\n+        '6': {'ext': 'flv', 'width': 450, 'height': 270, 'acodec': 'mp3', 'abr': 64, 'vcodec': 'h263'},\n+        '13': {'ext': '3gp', 'acodec': 'aac', 'vcodec': 'mp4v'},\n+        '17': {'ext': '3gp', 'width': 176, 'height': 144, 'acodec': 'aac', 'abr': 24, 'vcodec': 'mp4v'},\n+        '18': {'ext': 'mp4', 'width': 640, 'height': 360, 'acodec': 'aac', 'abr': 96, 'vcodec': 'h264'},\n+        '22': {'ext': 'mp4', 'width': 1280, 'height': 720, 'acodec': 'aac', 'abr': 192, 'vcodec': 'h264'},\n+        '34': {'ext': 'flv', 'width': 640, 'height': 360, 'acodec': 'aac', 'abr': 128, 'vcodec': 'h264'},\n+        '35': {'ext': 'flv', 'width': 854, 'height': 480, 'acodec': 'aac', 'abr': 128, 'vcodec': 'h264'},\n+        # itag 36 videos are either 320x180 (BaW_jenozKc) or 320x240 (__2ABJjxzNo), abr varies as well\n+        '36': {'ext': '3gp', 'width': 320, 'acodec': 'aac', 'vcodec': 'mp4v'},\n+        '37': {'ext': 'mp4', 'width': 1920, 'height': 1080, 'acodec': 'aac', 'abr': 192, 'vcodec': 'h264'},\n+        '38': {'ext': 'mp4', 'width': 4096, 'height': 3072, 'acodec': 'aac', 'abr': 192, 'vcodec': 'h264'},\n+        '43': {'ext': 'webm', 'width': 640, 'height': 360, 'acodec': 'vorbis', 'abr': 128, 'vcodec': 'vp8'},\n+        '44': {'ext': 'webm', 'width': 854, 'height': 480, 'acodec': 'vorbis', 'abr': 128, 'vcodec': 'vp8'},\n+        '45': {'ext': 'webm', 'width': 1280, 'height': 720, 'acodec': 'vorbis', 'abr': 192, 'vcodec': 'vp8'},\n+        '46': {'ext': 'webm', 'width': 1920, 'height': 1080, 'acodec': 'vorbis', 'abr': 192, 'vcodec': 'vp8'},\n+        '59': {'ext': 'mp4', 'width': 854, 'height': 480, 'acodec': 'aac', 'abr': 128, 'vcodec': 'h264'},\n+        '78': {'ext': 'mp4', 'width': 854, 'height': 480, 'acodec': 'aac', 'abr': 128, 'vcodec': 'h264'},\n+\n+\n+        # 3D videos\n+        '82': {'ext': 'mp4', 'height': 360, 'format_note': '3D', 'acodec': 'aac', 'abr': 128, 'vcodec': 'h264', 'preference': -20},\n+        '83': {'ext': 'mp4', 'height': 480, 'format_note': '3D', 'acodec': 'aac', 'abr': 128, 'vcodec': 'h264', 'preference': -20},\n+        '84': {'ext': 'mp4', 'height': 720, 'format_note': '3D', 'acodec': 'aac', 'abr': 192, 'vcodec': 'h264', 'preference': -20},\n+        '85': {'ext': 'mp4', 'height': 1080, 'format_note': '3D', 'acodec': 'aac', 'abr': 192, 'vcodec': 'h264', 'preference': -20},\n+        '100': {'ext': 'webm', 'height': 360, 'format_note': '3D', 'acodec': 'vorbis', 'abr': 128, 'vcodec': 'vp8', 'preference': -20},\n+        '101': {'ext': 'webm', 'height': 480, 'format_note': '3D', 'acodec': 'vorbis', 'abr': 192, 'vcodec': 'vp8', 'preference': -20},\n+        '102': {'ext': 'webm', 'height': 720, 'format_note': '3D', 'acodec': 'vorbis', 'abr': 192, 'vcodec': 'vp8', 'preference': -20},\n+\n+        # Apple HTTP Live Streaming\n+        '91': {'ext': 'mp4', 'height': 144, 'format_note': 'HLS', 'acodec': 'aac', 'abr': 48, 'vcodec': 'h264', 'preference': -10},\n+        '92': {'ext': 'mp4', 'height': 240, 'format_note': 'HLS', 'acodec': 'aac', 'abr': 48, 'vcodec': 'h264', 'preference': -10},\n+        '93': {'ext': 'mp4', 'height': 360, 'format_note': 'HLS', 'acodec': 'aac', 'abr': 128, 'vcodec': 'h264', 'preference': -10},\n+        '94': {'ext': 'mp4', 'height': 480, 'format_note': 'HLS', 'acodec': 'aac', 'abr': 128, 'vcodec': 'h264', 'preference': -10},\n+        '95': {'ext': 'mp4', 'height': 720, 'format_note': 'HLS', 'acodec': 'aac', 'abr': 256, 'vcodec': 'h264', 'preference': -10},\n+        '96': {'ext': 'mp4', 'height': 1080, 'format_note': 'HLS', 'acodec': 'aac', 'abr': 256, 'vcodec': 'h264', 'preference': -10},\n+        '132': {'ext': 'mp4', 'height': 240, 'format_note': 'HLS', 'acodec': 'aac', 'abr': 48, 'vcodec': 'h264', 'preference': -10},\n+        '151': {'ext': 'mp4', 'height': 72, 'format_note': 'HLS', 'acodec': 'aac', 'abr': 24, 'vcodec': 'h264', 'preference': -10},\n+\n+        # DASH mp4 video\n+        '133': {'ext': 'mp4', 'height': 240, 'format_note': 'DASH video', 'vcodec': 'h264'},\n+        '134': {'ext': 'mp4', 'height': 360, 'format_note': 'DASH video', 'vcodec': 'h264'},\n+        '135': {'ext': 'mp4', 'height': 480, 'format_note': 'DASH video', 'vcodec': 'h264'},\n+        '136': {'ext': 'mp4', 'height': 720, 'format_note': 'DASH video', 'vcodec': 'h264'},\n+        '137': {'ext': 'mp4', 'height': 1080, 'format_note': 'DASH video', 'vcodec': 'h264'},\n+        '138': {'ext': 'mp4', 'format_note': 'DASH video', 'vcodec': 'h264'},  # Height can vary (https://github.com/ytdl-org/youtube-dl/issues/4559)\n+        '160': {'ext': 'mp4', 'height': 144, 'format_note': 'DASH video', 'vcodec': 'h264'},\n+        '212': {'ext': 'mp4', 'height': 480, 'format_note': 'DASH video', 'vcodec': 'h264'},\n+        '264': {'ext': 'mp4', 'height': 1440, 'format_note': 'DASH video', 'vcodec': 'h264'},\n+        '298': {'ext': 'mp4', 'height': 720, 'format_note': 'DASH video', 'vcodec': 'h264', 'fps': 60},\n+        '299': {'ext': 'mp4', 'height': 1080, 'format_note': 'DASH video', 'vcodec': 'h264', 'fps': 60},\n+        '266': {'ext': 'mp4', 'height': 2160, 'format_note': 'DASH video', 'vcodec': 'h264'},\n+\n+        # Dash mp4 audio\n+        '139': {'ext': 'm4a', 'format_note': 'DASH audio', 'acodec': 'aac', 'abr': 48, 'container': 'm4a_dash'},\n+        '140': {'ext': 'm4a', 'format_note': 'DASH audio', 'acodec': 'aac', 'abr': 128, 'container': 'm4a_dash'},\n+        '141': {'ext': 'm4a', 'format_note': 'DASH audio', 'acodec': 'aac', 'abr': 256, 'container': 'm4a_dash'},\n+        '256': {'ext': 'm4a', 'format_note': 'DASH audio', 'acodec': 'aac', 'container': 'm4a_dash'},\n+        '258': {'ext': 'm4a', 'format_note': 'DASH audio', 'acodec': 'aac', 'container': 'm4a_dash'},\n+        '325': {'ext': 'm4a', 'format_note': 'DASH audio', 'acodec': 'dtse', 'container': 'm4a_dash'},\n+        '328': {'ext': 'm4a', 'format_note': 'DASH audio', 'acodec': 'ec-3', 'container': 'm4a_dash'},\n+\n+        # Dash webm\n+        '167': {'ext': 'webm', 'height': 360, 'width': 640, 'format_note': 'DASH video', 'container': 'webm', 'vcodec': 'vp8'},\n+        '168': {'ext': 'webm', 'height': 480, 'width': 854, 'format_note': 'DASH video', 'container': 'webm', 'vcodec': 'vp8'},\n+        '169': {'ext': 'webm', 'height': 720, 'width': 1280, 'format_note': 'DASH video', 'container': 'webm', 'vcodec': 'vp8'},\n+        '170': {'ext': 'webm', 'height': 1080, 'width': 1920, 'format_note': 'DASH video', 'container': 'webm', 'vcodec': 'vp8'},\n+        '218': {'ext': 'webm', 'height': 480, 'width': 854, 'format_note': 'DASH video', 'container': 'webm', 'vcodec': 'vp8'},\n+        '219': {'ext': 'webm', 'height': 480, 'width': 854, 'format_note': 'DASH video', 'container': 'webm', 'vcodec': 'vp8'},\n+        '278': {'ext': 'webm', 'height': 144, 'format_note': 'DASH video', 'container': 'webm', 'vcodec': 'vp9'},\n+        '242': {'ext': 'webm', 'height': 240, 'format_note': 'DASH video', 'vcodec': 'vp9'},\n+        '243': {'ext': 'webm', 'height': 360, 'format_note': 'DASH video', 'vcodec': 'vp9'},\n+        '244': {'ext': 'webm', 'height': 480, 'format_note': 'DASH video', 'vcodec': 'vp9'},\n+        '245': {'ext': 'webm', 'height': 480, 'format_note': 'DASH video', 'vcodec': 'vp9'},\n+        '246': {'ext': 'webm', 'height': 480, 'format_note': 'DASH video', 'vcodec': 'vp9'},\n+        '247': {'ext': 'webm', 'height': 720, 'format_note': 'DASH video', 'vcodec': 'vp9'},\n+        '248': {'ext': 'webm', 'height': 1080, 'format_note': 'DASH video', 'vcodec': 'vp9'},\n+        '271': {'ext': 'webm', 'height': 1440, 'format_note': 'DASH video', 'vcodec': 'vp9'},\n+        # itag 272 videos are either 3840x2160 (e.g. RtoitU2A-3E) or 7680x4320 (sLprVF6d7Ug)\n+        '272': {'ext': 'webm', 'height': 2160, 'format_note': 'DASH video', 'vcodec': 'vp9'},\n+        '302': {'ext': 'webm', 'height': 720, 'format_note': 'DASH video', 'vcodec': 'vp9', 'fps': 60},\n+        '303': {'ext': 'webm', 'height': 1080, 'format_note': 'DASH video', 'vcodec': 'vp9', 'fps': 60},\n+        '308': {'ext': 'webm', 'height': 1440, 'format_note': 'DASH video', 'vcodec': 'vp9', 'fps': 60},\n+        '313': {'ext': 'webm', 'height': 2160, 'format_note': 'DASH video', 'vcodec': 'vp9'},\n+        '315': {'ext': 'webm', 'height': 2160, 'format_note': 'DASH video', 'vcodec': 'vp9', 'fps': 60},\n+\n+        # Dash webm audio\n+        '171': {'ext': 'webm', 'acodec': 'vorbis', 'format_note': 'DASH audio', 'abr': 128},\n+        '172': {'ext': 'webm', 'acodec': 'vorbis', 'format_note': 'DASH audio', 'abr': 256},\n+\n+        # Dash webm audio with opus inside\n+        '249': {'ext': 'webm', 'format_note': 'DASH audio', 'acodec': 'opus', 'abr': 50},\n+        '250': {'ext': 'webm', 'format_note': 'DASH audio', 'acodec': 'opus', 'abr': 70},\n+        '251': {'ext': 'webm', 'format_note': 'DASH audio', 'acodec': 'opus', 'abr': 160},\n+\n+        # RTMP (unnamed)\n+        '_rtmp': {'protocol': 'rtmp'},\n+\n+        # av01 video only formats sometimes served with \"unknown\" codecs\n+        '394': {'acodec': 'none', 'vcodec': 'av01.0.05M.08'},\n+        '395': {'acodec': 'none', 'vcodec': 'av01.0.05M.08'},\n+        '396': {'acodec': 'none', 'vcodec': 'av01.0.05M.08'},\n+        '397': {'acodec': 'none', 'vcodec': 'av01.0.05M.08'},\n+    }\n \n     @classmethod\n     def suitable(cls, url):\n-        return not YoutubeIE.suitable(url) and super(\n-            YoutubeTabIE, cls).suitable(url)\n+        if parse_qs(url).get('list', [None])[0]:\n+            return False\n+        return super(YoutubeIE, cls).suitable(url)\n+\n+    def __init__(self, *args, **kwargs):\n+        super(YoutubeIE, self).__init__(*args, **kwargs)\n+        self._code_cache = {}\n+        self._player_cache = {}\n+\n+    # *ytcfgs, webpage=None\n+    def _extract_player_url(self, *ytcfgs, **kw_webpage):\n+        if ytcfgs and not isinstance(ytcfgs[0], dict):\n+            webpage = kw_webpage.get('webpage') or ytcfgs[0]\n+        if webpage:\n+            player_url = self._search_regex(\n+                r'\"(?:PLAYER_JS_URL|jsUrl)\"\\s*:\\s*\"([^\"]+)\"',\n+                webpage or '', 'player URL', fatal=False)\n+            if player_url:\n+                ytcfgs = ytcfgs + ({'PLAYER_JS_URL': player_url},)\n+        return traverse_obj(\n+            ytcfgs, (Ellipsis, 'PLAYER_JS_URL'), (Ellipsis, 'WEB_PLAYER_CONTEXT_CONFIGS', Ellipsis, 'jsUrl'),\n+            get_all=False, expected_type=lambda u: urljoin('https://www.youtube.com', u))\n+\n+    def _download_player_url(self, video_id, fatal=False):\n+        res = self._download_webpage(\n+            'https://www.youtube.com/iframe_api',\n+            note='Downloading iframe API JS', video_id=video_id, fatal=fatal)\n+        player_version = self._search_regex(\n+            r'player\\\\?/([0-9a-fA-F]{8})\\\\?/', res or '', 'player version', fatal=fatal,\n+            default=NO_DEFAULT if res else None)\n+        if player_version:\n+            return 'https://www.youtube.com/s/player/{0}/player_ias.vflset/en_US/base.js'.format(player_version)\n+\n+    def _signature_cache_id(self, example_sig):\n+        \"\"\" Return a string representation of a signature \"\"\"\n+        return '.'.join(compat_str(len(part)) for part in example_sig.split('.'))\n+\n+    @classmethod\n+    def _extract_player_info(cls, player_url):\n+        for player_re in cls._PLAYER_INFO_RE:\n+            id_m = re.search(player_re, player_url)\n+            if id_m:\n+                break\n+        else:\n+            raise ExtractorError('Cannot identify player %r' % player_url)\n+        return id_m.group('id')\n+\n+    def _load_player(self, video_id, player_url, fatal=True, player_id=None):\n+        if not player_id:\n+            player_id = self._extract_player_info(player_url)\n+        if player_id not in self._code_cache:\n+            code = self._download_webpage(\n+                player_url, video_id, fatal=fatal,\n+                note='Downloading player ' + player_id,\n+                errnote='Download of %s failed' % player_url)\n+            if code:\n+                self._code_cache[player_id] = code\n+        return self._code_cache[player_id] if fatal else self._code_cache.get(player_id)\n+\n+    def _extract_signature_function(self, video_id, player_url, example_sig):\n+        player_id = self._extract_player_info(player_url)\n+\n+        # Read from filesystem cache\n+        func_id = 'js_{0}_{1}'.format(\n+            player_id, self._signature_cache_id(example_sig))\n+        assert os.path.basename(func_id) == func_id\n+\n+        self.write_debug('Extracting signature function {0}'.format(func_id))\n+        cache_spec, code = self.cache.load('youtube-sigfuncs', func_id), None\n+\n+        if not cache_spec:\n+            code = self._load_player(video_id, player_url, player_id)\n+        if code:\n+            res = self._parse_sig_js(code)\n+            test_string = ''.join(map(compat_chr, range(len(example_sig))))\n+            cache_spec = [ord(c) for c in res(test_string)]\n+            self.cache.store('youtube-sigfuncs', func_id, cache_spec)\n+\n+        return lambda s: ''.join(s[i] for i in cache_spec)\n+\n+    def _print_sig_code(self, func, example_sig):\n+        if not self.get_param('youtube_print_sig_code'):\n+            return\n+\n+        def gen_sig_code(idxs):\n+            def _genslice(start, end, step):\n+                starts = '' if start == 0 else str(start)\n+                ends = (':%d' % (end + step)) if end + step >= 0 else ':'\n+                steps = '' if step == 1 else (':%d' % step)\n+                return 's[{0}{1}{2}]'.format(starts, ends, steps)\n+\n+            step = None\n+            # Quelch pyflakes warnings - start will be set when step is set\n+            start = '(Never used)'\n+            for i, prev in zip(idxs[1:], idxs[:-1]):\n+                if step is not None:\n+                    if i - prev == step:\n+                        continue\n+                    yield _genslice(start, prev, step)\n+                    step = None\n+                    continue\n+                if i - prev in [-1, 1]:\n+                    step = i - prev\n+                    start = prev\n+                    continue\n+                else:\n+                    yield 's[%d]' % prev\n+            if step is None:\n+                yield 's[%d]' % i\n+            else:\n+                yield _genslice(start, i, step)\n+\n+        test_string = ''.join(map(compat_chr, range(len(example_sig))))\n+        cache_res = func(test_string)\n+        cache_spec = [ord(c) for c in cache_res]\n+        expr_code = ' + '.join(gen_sig_code(cache_spec))\n+        signature_id_tuple = '(%s)' % (\n+            ', '.join(compat_str(len(p)) for p in example_sig.split('.')))\n+        code = ('if tuple(len(p) for p in s.split(\\'.\\')) == %s:\\n'\n+                '    return %s\\n') % (signature_id_tuple, expr_code)\n+        self.to_screen('Extracted signature function:\\n' + code)\n+\n+    def _parse_sig_js(self, jscode):\n+        # Examples where `sig` is funcname:\n+        # sig=function(a){a=a.split(\"\"); ... ;return a.join(\"\")};\n+        # ;c&&(c=sig(decodeURIComponent(c)),a.set(b,encodeURIComponent(c)));return a};\n+        # {var l=f,m=h.sp,n=sig(decodeURIComponent(h.s));l.set(m,encodeURIComponent(n))}\n+        # sig=function(J){J=J.split(\"\"); ... ;return J.join(\"\")};\n+        # ;N&&(N=sig(decodeURIComponent(N)),J.set(R,encodeURIComponent(N)));return J};\n+        # {var H=u,k=f.sp,v=sig(decodeURIComponent(f.s));H.set(k,encodeURIComponent(v))}\n+        funcname = self._search_regex(\n+            (r'\\b(?P<var>[\\w$]+)&&\\((?P=var)=(?P<sig>[\\w$]{2,})\\(decodeURIComponent\\((?P=var)\\)\\)',\n+             r'(?P<sig>[\\w$]+)\\s*=\\s*function\\(\\s*(?P<arg>[\\w$]+)\\s*\\)\\s*{\\s*(?P=arg)\\s*=\\s*(?P=arg)\\.split\\(\\s*\"\"\\s*\\)\\s*;\\s*[^}]+;\\s*return\\s+(?P=arg)\\.join\\(\\s*\"\"\\s*\\)',\n+             r'(?:\\b|[^\\w$])(?P<sig>[\\w$]{2,})\\s*=\\s*function\\(\\s*a\\s*\\)\\s*{\\s*a\\s*=\\s*a\\.split\\(\\s*\"\"\\s*\\)(?:;[\\w$]{2}\\.[\\w$]{2}\\(a,\\d+\\))?',\n+             # Old patterns\n+             r'\\b[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*encodeURIComponent\\s*\\(\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\b[\\w]+\\s*&&\\s*[\\w]+\\.set\\([^,]+\\s*,\\s*encodeURIComponent\\s*\\(\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\bm=(?P<sig>[\\w$]{2,})\\(decodeURIComponent\\(h\\.s\\)\\)',\n+             # Obsolete patterns\n+             r'(\"|\\')signature\\1\\s*,\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\.sig\\|\\|(?P<sig>[\\w$]+)\\(',\n+             r'yt\\.akamaized\\.net/\\)\\s*\\||\\s*.*?\\s*[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?:encodeURIComponent\\s*\\()?\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\b[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\bc\\s*&&\\s*[\\w]+\\.set\\([^,]+\\s*,\\s*\\([^)]*\\)\\s*\\(\\s*(?P<sig>[\\w$]+)\\('),\n+            jscode, 'Initial JS player signature function name', group='sig')\n+\n+        jsi = JSInterpreter(jscode)\n+        initial_function = jsi.extract_function(funcname)\n+        return lambda s: initial_function([s])\n+\n+    def _cached(self, func, *cache_id):\n+        def inner(*args, **kwargs):\n+            if cache_id not in self._player_cache:\n+                try:\n+                    self._player_cache[cache_id] = func(*args, **kwargs)\n+                except ExtractorError as e:\n+                    self._player_cache[cache_id] = e\n+                except Exception as e:\n+                    self._player_cache[cache_id] = ExtractorError(traceback.format_exc(), cause=e)\n+\n+            ret = self._player_cache[cache_id]\n+            if isinstance(ret, Exception):\n+                raise ret\n+            return ret\n+        return inner\n+\n+    def _decrypt_signature(self, s, video_id, player_url):\n+        \"\"\"Turn the encrypted s field into a working signature\"\"\"\n+        extract_sig = self._cached(\n+            self._extract_signature_function, 'sig', player_url, self._signature_cache_id(s))\n+        func = extract_sig(video_id, player_url, s)\n+        self._print_sig_code(func, s)\n+        return func(s)\n+\n+    # from yt-dlp\n+    # See also:\n+    # 1. https://github.com/ytdl-org/youtube-dl/issues/29326#issuecomment-894619419\n+    # 2. https://code.videolan.org/videolan/vlc/-/blob/4fb284e5af69aa9ac2100ccbdd3b88debec9987f/share/lua/playlist/youtube.lua#L116\n+    # 3. https://github.com/ytdl-org/youtube-dl/issues/30097#issuecomment-950157377\n+    def _decrypt_nsig(self, n, video_id, player_url):\n+        \"\"\"Turn the encrypted n field into a working signature\"\"\"\n+        if player_url is None:\n+            raise ExtractorError('Cannot decrypt nsig without player_url')\n+\n+        try:\n+            jsi, player_id, func_code = self._extract_n_function_code(video_id, player_url)\n+        except ExtractorError as e:\n+            raise ExtractorError('Unable to extract nsig function code', cause=e)\n+        if self.get_param('youtube_print_sig_code'):\n+            self.to_screen('Extracted nsig function from {0}:\\n{1}\\n'.format(\n+                player_id, func_code[1]))\n+\n+        try:\n+            extract_nsig = self._cached(self._extract_n_function_from_code, 'nsig func', player_url)\n+            ret = extract_nsig(jsi, func_code)(n)\n+        except JSInterpreter.Exception as e:\n+            self.report_warning(\n+                '%s (%s %s)' % (\n+                    'Unable to decode n-parameter: expect download to be blocked or throttled',\n+                    error_to_compat_str(e),\n+                    traceback.format_exc()),\n+                video_id=video_id)\n+            return\n+\n+        self.write_debug('Decrypted nsig {0} => {1}'.format(n, ret))\n+        return ret\n+\n+    def _extract_n_function_name(self, jscode):\n+        func_name, idx = self._search_regex(\n+            # (y=NuD(),Mw(k),q=k.Z[y]||null)&&(q=narray[idx](q),k.set(y,q),k.V||NuD(''))}};\n+            # (R=\"nn\"[+J.Z],mW(J),N=J.K[R]||null)&&(N=narray[idx](N),J.set(R,N))}};\n+            # or:  (b=String.fromCharCode(110),c=a.get(b))&&c=narray[idx](c)\n+            # or:  (b=\"nn\"[+a.D],c=a.get(b))&&(c=narray[idx](c)\n+            # or:  (PL(a),b=a.j.n||null)&&(b=narray[idx](b)\n+            # or:  (b=\"nn\"[+a.D],vL(a),c=a.j[b]||null)&&(c=narray[idx](c),a.set(b,c),narray.length||nfunc(\"\")\n+            # old: (b=a.get(\"n\"))&&(b=narray[idx](b)(?P<c>[a-z])\\s*=\\s*[a-z]\\s*\n+            # older: (b=a.get(\"n\"))&&(b=nfunc(b)\n+            r'''(?x)\n+                (?:(?<=[^\\w$])|^)       # instead of \\b, which ignores $\n+                (?P<name>(?!\\d)[a-zA-Z\\d_$]+)\\s*=\\s*function\\((?!\\d)[a-zA-Z\\d_$]+\\)\n+                \\s*\\{(?:(?!};).)+?(?:\n+                    [\"']enhanced_except_ |\n+                    return\\s*(?P<q>\"|')[a-zA-Z\\d-]+_w8_(?P=q)\\s*\\+\\s*[\\w$]+\n+                )\n+            ''', jscode, 'Initial JS player n function name', group='name')\n+\n+        if not func_name:\n+             func_name, idx = self._search_regex(\n+                 r'''(?x)\n+                     # (expr, ...,\n+                     \\((?:(?:\\s*[\\w$]+\\s*=)?(?:[\\w$\"+\\.\\s(\\[]+(?:[)\\]]\\s*)?),)*\n+                       # b=...\n+                       (?P<b>[\\w$]+)\\s*=\\s*(?!(?P=b)[^\\w$])[\\w$]+\\s*(?:(?:\n+                         \\.\\s*[\\w$]+ |\n+                         \\[\\s*[\\w$]+\\s*\\] |\n+                         \\.\\s*get\\s*\\(\\s*[\\w$\"]+\\s*\\)\n+                       )\\s*){,2}(?:\\s*\\|\\|\\s*null(?=\\s*\\)))?\\s*\n+                     \\)\\s*&&\\s*\\(        # ...)&&(\n+                     # b = nfunc, b = narray[idx]\n+                     (?P=b)\\s*=\\s*(?P<nfunc>[\\w$]+)\\s*\n+                         (?:\\[\\s*(?P<idx>[\\w$]+)\\s*\\]\\s*)?\n+                         # (...)\n+                         \\(\\s*[\\w$]+\\s*\\)\n+                 ''', jscode, 'Initial JS player n function name', group=('nfunc', 'idx'))\n+        if not idx:\n+            return func_name\n+\n+        return self._search_json(\n+            r'var\\s+{0}\\s*='.format(re.escape(func_name)), jscode,\n+            'Initial JS player n function list ({0}.{1})'.format(func_name, idx),\n+            func_name, contains_pattern=r'\\[[\\s\\S]+\\]', end_pattern='[,;]',\n+            transform_source=js_to_json)[int(idx)]\n+\n+    def _extract_n_function_code(self, video_id, player_url):\n+        player_id = self._extract_player_info(player_url)\n+        func_code = self.cache.load('youtube-nsig', player_id)\n+        jscode = func_code or self._load_player(video_id, player_url)\n+        jsi = JSInterpreter(jscode)\n+\n+        if func_code:\n+            return jsi, player_id, func_code\n+\n+        func_name = self._extract_n_function_name(jscode)\n+\n+        func_code = jsi.extract_function_code(func_name)\n+\n+        self.cache.store('youtube-nsig', player_id, func_code)\n+        return jsi, player_id, func_code\n+\n+    def _extract_n_function_from_code(self, jsi, func_code):\n+        func = jsi.extract_function_from_code(*func_code)\n+\n+        def extract_nsig(s):\n+            try:\n+                ret = func([s], kwargs={'_ytdl_do_not_return': s})\n+            except JSInterpreter.Exception:\n+                raise\n+            except Exception as e:\n+                raise JSInterpreter.Exception(traceback.format_exc(), cause=e)\n+\n+            if ret.startswith('enhanced_except_') and ret.endswith(s):\n+                raise JSInterpreter.Exception('Signature function returned an exception')\n+            return ret\n+\n+        return extract_nsig\n+\n+    def _unthrottle_format_urls(self, video_id, player_url, *formats):\n+\n+        def decrypt_nsig(n):\n+            return self._cached(self._decrypt_nsig, 'nsig', n, player_url)\n+\n+        for fmt in formats:\n+            parsed_fmt_url = compat_urllib_parse.urlparse(fmt['url'])\n+            n_param = compat_parse_qs(parsed_fmt_url.query).get('n')\n+            if not n_param:\n+                continue\n+            n_param = n_param[-1]\n+            n_response = decrypt_nsig(n_param)(n_param, video_id, player_url)\n+            if n_response is None:\n+                # give up if descrambling failed\n+                break\n+            fmt['url'] = update_url_query(fmt['url'], {'n': n_response})\n+\n+    # from yt-dlp, with tweaks\n+    def _extract_signature_timestamp(self, video_id, player_url, ytcfg=None, fatal=False):\n+        \"\"\"\n+        Extract signatureTimestamp (sts)\n+        Required to tell API what sig/player version is in use.\n+        \"\"\"\n+        sts = traverse_obj(ytcfg, 'STS', expected_type=int)\n+        if not sts:\n+            # Attempt to extract from player\n+            if player_url is None:\n+                error_msg = 'Cannot extract signature timestamp without player_url.'\n+                if fatal:\n+                    raise ExtractorError(error_msg)\n+                self.report_warning(error_msg)\n+                return\n+            code = self._load_player(video_id, player_url, fatal=fatal)\n+            sts = int_or_none(self._search_regex(\n+                r'(?:signatureTimestamp|sts)\\s*:\\s*(?P<sts>[0-9]{5})', code or '',\n+                'JS player signature timestamp', group='sts', fatal=fatal))\n+        return sts\n+\n+    def _mark_watched(self, video_id, player_response):\n+        playback_url = url_or_none(try_get(\n+            player_response,\n+            lambda x: x['playbackTracking']['videostatsPlaybackUrl']['baseUrl']))\n+        if not playback_url:\n+            return\n+\n+        # cpn generation algorithm is reverse engineered from base.js.\n+        # In fact it works even with dummy cpn.\n+        CPN_ALPHABET = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-_'\n+        cpn = ''.join(CPN_ALPHABET[random.randint(0, 256) & 63] for _ in range(0, 16))\n+\n+        # more consistent results setting it to right before the end\n+        qs = parse_qs(playback_url)\n+        video_length = '{0}'.format(float((qs.get('len') or ['1.5'])[0]) - 1)\n+\n+        playback_url = update_url_query(\n+            playback_url, {\n+                'ver': '2',\n+                'cpn': cpn,\n+                'cmt': video_length,\n+                'el': 'detailpage',  # otherwise defaults to \"shorts\"\n+            })\n+\n+        self._download_webpage(\n+            playback_url, video_id, 'Marking watched',\n+            'Unable to mark watched', fatal=False)\n \n     @staticmethod\n-    def _extract_grid_item_renderer(item):\n-        assert isinstance(item, dict)\n-        for key, renderer in item.items():\n-            if not key.startswith('grid') or not key.endswith('Renderer'):\n+    def _extract_urls(webpage):\n+        # Embedded YouTube player\n+        entries = [\n+            unescapeHTML(mobj.group('url'))\n+            for mobj in re.finditer(r'''(?x)\n+            (?:\n+                <iframe[^>]+?src=|\n+                data-video-url=|\n+                <embed[^>]+?src=|\n+                embedSWF\\(?:\\s*|\n+                <object[^>]+data=|\n+                new\\s+SWFObject\\(\n+            )\n+            ([\"\\'])\n+                (?P<url>(?:https?:)?//(?:www\\.)?youtube(?:-nocookie)?\\.com/\n+                (?:embed|v|p)/[0-9A-Za-z_-]{11}.*?)\n+            \\1''', webpage)]\n+\n+        # lazyYT YouTube embed\n+        entries.extend(list(map(\n+            unescapeHTML,\n+            re.findall(r'class=\"lazyYT\" data-youtube-id=\"([^\"]+)\"', webpage))))\n+\n+        # Wordpress \"YouTube Video Importer\" plugin\n+        matches = re.findall(r'''(?x)<div[^>]+\n+            class=(?P<q1>[\\'\"])[^\\'\"]*\\byvii_single_video_player\\b[^\\'\"]*(?P=q1)[^>]+\n+            data-video_id=(?P<q2>[\\'\"])([^\\'\"]+)(?P=q2)''', webpage)\n+        entries.extend(m[-1] for m in matches)\n+\n+        return entries\n+\n+    @staticmethod\n+    def _extract_url(webpage):\n+        urls = YoutubeIE._extract_urls(webpage)\n+        return urls[0] if urls else None\n+\n+    @classmethod\n+    def extract_id(cls, url):\n+        mobj = re.match(cls._VALID_URL, url, re.VERBOSE)\n+        if mobj is None:\n+            raise ExtractorError('Invalid URL: %s' % url)\n+        video_id = mobj.group(2)\n+        return video_id\n+\n+    def _extract_chapters_from_json(self, data, video_id, duration):\n+        chapters_list = try_get(\n+            data,\n+            lambda x: x['playerOverlays']\n+                       ['playerOverlayRenderer']\n+                       ['decoratedPlayerBarRenderer']\n+                       ['decoratedPlayerBarRenderer']\n+                       ['playerBar']\n+                       ['chapteredPlayerBarRenderer']\n+                       ['chapters'],\n+            list)\n+        if not chapters_list:\n+            return\n+\n+        def chapter_time(chapter):\n+            return float_or_none(\n+                try_get(\n+                    chapter,\n+                    lambda x: x['chapterRenderer']['timeRangeStartMillis'],\n+                    int),\n+                scale=1000)\n+        chapters = []\n+        for next_num, chapter in enumerate(chapters_list, start=1):\n+            start_time = chapter_time(chapter)\n+            if start_time is None:\n                 continue\n-            if not isinstance(renderer, dict):\n+            end_time = (chapter_time(chapters_list[next_num])\n+                        if next_num < len(chapters_list) else duration)\n+            if end_time is None:\n                 continue\n-            return renderer\n-\n-    @staticmethod\n-    def _get_text(r, k):\n-        return traverse_obj(\n-            r, (k, 'runs', 0, 'text'), (k, 'simpleText'),\n-            expected_type=txt_or_none)\n-\n-    def _grid_entries(self, grid_renderer):\n-        for item in grid_renderer['items']:\n-            if not isinstance(item, dict):\n+            title = try_get(\n+                chapter, lambda x: x['chapterRenderer']['title']['simpleText'],\n+                compat_str)\n+            chapters.append({\n+                'start_time': start_time,\n+                'end_time': end_time,\n+                'title': title,\n+            })\n+        return chapters\n+\n+    def _extract_yt_initial_variable(self, webpage, regex, video_id, name):\n+        return self._parse_json(self._search_regex(\n+            (r'%s\\s*%s' % (regex, self._YT_INITIAL_BOUNDARY_RE),\n+             regex), webpage, name, default='{}'), video_id, fatal=False)\n+\n+    def _real_extract(self, url):\n+        url, smuggled_data = unsmuggle_url(url, {})\n+        video_id = self._match_id(url)\n+        base_url = self.http_scheme() + '//www.youtube.com/'\n+        webpage_url = base_url + 'watch?v=' + video_id\n+        webpage = self._download_webpage(\n+            webpage_url + '&bpctr=9999999999&has_verified=1', video_id, fatal=False)\n+\n+        player_response = None\n+        player_url = None\n+        if webpage:\n+            player_response = self._extract_yt_initial_variable(\n+                webpage, self._YT_INITIAL_PLAYER_RESPONSE_RE,\n+                video_id, 'initial player response')\n+        if False and not player_response:\n+            player_response = self._call_api(\n+                'player', {'videoId': video_id}, video_id)\n+        if True or not player_response:\n+            origin = 'https://www.youtube.com'\n+            pb_context = {'html5Preference': 'HTML5_PREF_WANTS'}\n+\n+            player_url = self._extract_player_url(webpage)\n+            ytcfg = self._extract_ytcfg(video_id, webpage)\n+            sts = self._extract_signature_timestamp(video_id, player_url, ytcfg)\n+            if sts:\n+                pb_context['signatureTimestamp'] = sts\n+\n+            query = {\n+                'playbackContext': {\n+                    'contentPlaybackContext': pb_context,\n+                    'contentCheckOk': True,\n+                    'racyCheckOk': True,\n+                },\n+                'context': {\n+                    'client': {\n+                        'clientName': 'MWEB',\n+                        'clientVersion': '2.20241202.07.01', # BUG: Modified client version\n+                        'hl': 'en',\n+                        'userAgent': 'Mozilla/5.0 (iPad; CPU OS 16_7_10 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1,gzip(gfe)',\n+                        'timeZone': 'UTC',\n+                        'utcOffsetMinutes': 0,\n+                    },\n+                },\n+                'videoId': video_id,\n+            }\n+            headers = {\n+                'X-YouTube-Client-Name': '2',\n+                'X-YouTube-Client-Version': '2.20241202.07.00',\n+                'Origin': origin,\n+                'Sec-Fetch-Mode': 'navigate',\n+                'User-Agent': query['context']['client']['userAgent'],\n+            }\n+            auth = self._generate_sapisidhash_header(origin)\n+            if auth is not None:\n+                headers['Authorization'] = auth\n+                headers['X-Origin'] = origin\n+\n+            player_response = self._call_api('player', query, video_id, fatal=False, headers=headers)\n+\n+        def is_agegated(playability):\n+            if not isinstance(playability, dict):\n+                return\n+\n+            if playability.get('desktopLegacyAgeGateReason'):\n+                return True\n+\n+            reasons = filter(None, (playability.get(r) for r in ('status', 'reason')))\n+            AGE_GATE_REASONS = (\n+                'confirm your age', 'age-restricted', 'inappropriate',  # reason\n+                'age_verification_required', 'age_check_required',  # status\n+            )\n+            return any(expected in reason for expected in AGE_GATE_REASONS for reason in reasons)\n+\n+        def get_playability_status(response):\n+            return try_get(response, lambda x: x['playabilityStatus'], dict) or {}\n+\n+        playability_status = get_playability_status(player_response)\n+        if (is_agegated(playability_status)\n+                and int_or_none(self._downloader.params.get('age_limit'), default=18) >= 18):\n+\n+            self.report_age_confirmation()\n+\n+            # Thanks: https://github.com/yt-dlp/yt-dlp/pull/3233\n+            pb_context = {'html5Preference': 'HTML5_PREF_WANTS'}\n+\n+            # Use signatureTimestamp if available\n+            # Thanks https://github.com/ytdl-org/youtube-dl/issues/31034#issuecomment-1160718026\n+            player_url = self._extract_player_url(webpage)\n+            ytcfg = self._extract_ytcfg(video_id, webpage)\n+            sts = self._extract_signature_timestamp(video_id, player_url, ytcfg)\n+            if sts:\n+                pb_context['signatureTimestamp'] = sts\n+\n+            query = {\n+                'playbackContext': {'contentPlaybackContext': pb_context},\n+                'contentCheckOk': True,\n+                'racyCheckOk': True,\n+                'context': {\n+                    'client': {'clientName': 'TVHTML5_SIMPLY_EMBEDDED_PLAYER', 'clientVersion': '2.0', 'hl': 'en', 'clientScreen': 'EMBED'},\n+                    'thirdParty': {'embedUrl': 'https://google.com'},\n+                },\n+                'videoId': video_id,\n+            }\n+            headers = {\n+                'X-YouTube-Client-Name': '85',\n+                'X-YouTube-Client-Version': '2.0',\n+                'Origin': 'https://www.youtube.com'\n+            }\n+\n+            video_info = self._call_api('player', query, video_id, fatal=False, headers=headers)\n+            age_gate_status = get_playability_status(video_info)\n+            if age_gate_status.get('status') == 'OK':\n+                player_response = video_info\n+                playability_status = age_gate_status\n+\n+        trailer_video_id = try_get(\n+            playability_status,\n+            lambda x: x['errorScreen']['playerLegacyDesktopYpcTrailerRenderer']['trailerVideoId'],\n+            compat_str)\n+        if trailer_video_id:\n+            return self.url_result(\n+                trailer_video_id, self.ie_key(), trailer_video_id)\n+\n+        def get_text(x):\n+            if not x:\n+                return\n+            text = x.get('simpleText')\n+            if text and isinstance(text, compat_str):\n+                return text\n+            runs = x.get('runs')\n+            if not isinstance(runs, list):\n+                return\n+            return ''.join([r['text'] for r in runs if isinstance(r.get('text'), compat_str)])\n+\n+        search_meta = (\n+            lambda x: self._html_search_meta(x, webpage, default=None)) \\\n+            if webpage else lambda x: None\n+\n+        video_details = player_response.get('videoDetails') or {}\n+        microformat = try_get(\n+            player_response,\n+            lambda x: x['microformat']['playerMicroformatRenderer'],\n+            dict) or {}\n+        video_title = video_details.get('title') \\\n+            or get_text(microformat.get('title')) \\\n+            or search_meta(['og:title', 'twitter:title', 'title'])\n+        video_description = video_details.get('shortDescription')\n+\n+        if not smuggled_data.get('force_singlefeed', False):\n+            if not self._downloader.params.get('noplaylist'):\n+                multifeed_metadata_list = try_get(\n+                    player_response,\n+                    lambda x: x['multicamera']['playerLegacyMulticameraRenderer']['metadataList'],\n+                    compat_str)\n+                if multifeed_metadata_list:\n+                    entries = []\n+                    feed_ids = []\n+                    for feed in multifeed_metadata_list.split(','):\n+                        # Unquote should take place before split on comma (,) since textual\n+                        # fields may contain comma as well (see\n+                        # https://github.com/ytdl-org/youtube-dl/issues/8536)\n+                        feed_data = compat_parse_qs(\n+                            compat_urllib_parse_unquote_plus(feed))\n+\n+                        def feed_entry(name):\n+                            return try_get(\n+                                feed_data, lambda x: x[name][0], compat_str)\n+\n+                        feed_id = feed_entry('id')\n+                        if not feed_id:\n+                            continue\n+                        feed_title = feed_entry('title')\n+                        title = video_title\n+                        if feed_title:\n+                            title += ' (%s)' % feed_title\n+                        entries.append({\n+                            '_type': 'url_transparent',\n+                            'ie_key': YoutubeIE.ie_key(),\n+                            'url': smuggle_url(\n+                                base_url + 'watch?v=' + feed_data['id'][0],\n+                                {'force_singlefeed': True}),\n+                            'title': title,\n+                        })\n+                        feed_ids.append(feed_id)\n+                    self.to_screen(\n+                        'Downloading multifeed video (%s) - add --no-playlist to just download video %s'\n+                        % (', '.join(feed_ids), video_id))\n+                    return self.playlist_result(\n+                        entries, video_id, video_title, video_description)\n+            else:\n+                self.to_screen('Downloading just video %s because of --no-playlist' % video_id)\n+\n+        if not player_url:\n+            player_url = self._extract_player_url(webpage)\n+\n+        formats = []\n+        itags = collections.defaultdict(set)\n+        itag_qualities = {}\n+        q = qualities(['tiny', 'small', 'medium', 'large', 'hd720', 'hd1080', 'hd1440', 'hd2160', 'hd2880', 'highres'])\n+        CHUNK_SIZE = 10 << 20\n+\n+        streaming_data = player_response.get('streamingData') or {}\n+        streaming_formats = streaming_data.get('formats') or []\n+        streaming_formats.extend(streaming_data.get('adaptiveFormats') or [])\n+\n+        def build_fragments(f):\n+            return LazyList({\n+                'url': update_url_query(f['url'], {\n+                    'range': '{0}-{1}'.format(range_start, min(range_start + CHUNK_SIZE - 1, f['filesize']))\n+                })\n+            } for range_start in range(0, f['filesize'], CHUNK_SIZE))\n+\n+        lower = lambda s: s.lower()\n+\n+        for fmt in streaming_formats:\n+            if fmt.get('targetDurationSec'):\n                 continue\n-            renderer = self._extract_grid_item_renderer(item)\n-            if not isinstance(renderer, dict):\n+\n+            itag = str_or_none(fmt.get('itag'))\n+            audio_track = traverse_obj(fmt, ('audioTrack', T(dict))) or {}\n+\n+            quality = traverse_obj(fmt, ((\n+                # The 3gp format (17) in android client has a quality of \"small\",\n+                # but is actually worse than other formats\n+                T(lambda _: 'tiny' if itag == 17 else None),\n+                ('quality', T(lambda q: q if q and q != 'tiny' else None)),\n+                ('audioQuality', T(lower)),\n+                'quality'), T(txt_or_none)), get_all=False)\n+            if quality and itag:\n+                itag_qualities[itag] = quality\n+            # FORMAT_STREAM_TYPE_OTF(otf=1) requires downloading the init fragment\n+            # (adding `&sq=0` to the URL) and parsing emsg box to determine the\n+            # number of fragments that would subsequently be requested with (`&sq=N`)\n+            if fmt.get('type') == 'FORMAT_STREAM_TYPE_OTF':\n                 continue\n-            title = self._get_text(renderer, 'title')\n-            # playlist\n-            playlist_id = renderer.get('playlistId')\n-            if playlist_id:\n-                yield self.url_result(\n-                    'https://www.youtube.com/playlist?list=%s' % playlist_id,\n-                    ie=YoutubeTabIE.ie_key(), video_id=playlist_id,\n-                    video_title=title)\n-                continue\n-            # video\n-            video_id = renderer.get('videoId')\n-            if video_id:\n-                yield self._extract_video(renderer)\n-                continue\n-            # channel\n-            channel_id = renderer.get('channelId')\n-            if channel_id:\n-                title = self._get_text(renderer, 'title')\n-                yield self.url_result(\n-                    'https://www.youtube.com/channel/%s' % channel_id,\n-                    ie=YoutubeTabIE.ie_key(), video_title=title)\n-                continue\n-            # generic endpoint URL support\n-            ep_url = urljoin('https://www.youtube.com/', try_get(\n-                renderer, lambda x: x['navigationEndpoint']['commandMetadata']['webCommandMetadata']['url'],\n-                compat_str))\n-            if ep_url:\n-                for ie in (YoutubeTabIE, YoutubePlaylistIE, YoutubeIE):\n-                    if ie.suitable(ep_url):\n-                        yield self.url_result(\n-                            ep_url, ie=ie.ie_key(), video_id=ie._match_id(ep_url), video_title=title)\n+\n+            fmt_url = fmt.get('url')\n+            if not fmt_url:\n+                sc = compat_parse_qs(fmt.get('signatureCipher'))\n+                fmt_url = traverse_obj(sc, ('url', -1, T(url_or_none)))\n+                encrypted_sig = traverse_obj(sc, ('s', -1))\n+                if not (fmt_url and encrypted_sig):\n+                    continue\n+                player_url = player_url or self._extract_player_url(webpage)\n+                if not player_url:\n+                    continue\n+                try:\n+                    fmt_url = update_url_query(fmt_url, {\n+                        traverse_obj(sc, ('sp', -1)) or 'signature':\n+                            [self._decrypt_signature(encrypted_sig, video_id, player_url)],\n+                    })\n+                except ExtractorError as e:\n+                    self.report_warning('Signature extraction failed: Some formats may be missing',\n+                                        video_id=video_id, only_once=True)\n+                    self.write_debug(error_to_compat_str(e), only_once=True)\n+                    continue\n+\n+            language_preference = (\n+                10 if audio_track.get('audioIsDefault')\n+                else -10 if 'descriptive' in (traverse_obj(audio_track, ('displayName', T(lower))) or '')\n+                else -1)\n+            name = (\n+                traverse_obj(fmt, ('qualityLabel', T(txt_or_none)))\n+                or quality.replace('audio_quality_', ''))\n+            dct = {\n+                'format_id': join_nonempty(itag, fmt.get('isDrc') and 'drc'),\n+                'url': fmt_url,\n+                # Format 22 is likely to be damaged: see https://github.com/yt-dlp/yt-dlp/issues/3372\n+                'source_preference': ((-5 if itag == '22' else -1)\n+                                      + (100 if 'Premium' in name else 0)),\n+                'quality': q(quality),\n+                'language': join_nonempty(audio_track.get('id', '').split('.')[0],\n+                                          'desc' if language_preference < -1 else '') or None,\n+                'language_preference': language_preference,\n+                # Strictly de-prioritize 3gp formats\n+                'preference': -2 if itag == '17' else None,\n+            }\n+            if itag:\n+                itags[itag].add(('https', dct.get('language')))\n+            self._unthrottle_format_urls(video_id, player_url, dct)\n+            dct.update(traverse_obj(fmt, {\n+                'asr': ('audioSampleRate', T(int_or_none)),\n+                'filesize': ('contentLength', T(int_or_none)),\n+                'format_note': ('qualityLabel', T(lambda x: x or quality)),\n+                # for some formats, fps is wrongly returned as 1\n+                'fps': ('fps', T(int_or_none), T(lambda f: f if f > 1 else None)),\n+                'audio_channels': ('audioChannels', T(int_or_none)),\n+                'height': ('height', T(int_or_none)),\n+                'has_drm': ('drmFamilies', T(bool)),\n+                'tbr': (('averageBitrate', 'bitrate'), T(lambda t: float_or_none(t, 1000))),\n+                'width': ('width', T(int_or_none)),\n+                '_duration_ms': ('approxDurationMs', T(int_or_none)),\n+            }, get_all=False))\n+            mime_mobj = re.match(\n+                r'((?:[^/]+)/(?:[^;]+))(?:;\\s*codecs=\"([^\"]+)\")?', fmt.get('mimeType') or '')\n+            if mime_mobj:\n+                dct['ext'] = mimetype2ext(mime_mobj.group(1))\n+                dct.update(parse_codecs(mime_mobj.group(2)))\n+            single_stream = 'none' in (dct.get(c) for c in ('acodec', 'vcodec'))\n+            if single_stream and dct.get('ext'):\n+                dct['container'] = dct['ext'] + '_dash'\n+            if single_stream or itag == '17':\n+                # avoid Youtube throttling\n+                dct.update({\n+                    'protocol': 'http_dash_segments',\n+                    'fragments': build_fragments(dct),\n+                } if dct['filesize'] else {\n+                    'downloader_options': {'http_chunk_size': CHUNK_SIZE}  # No longer useful?\n+                })\n+\n+            formats.append(dct)\n+\n+        def process_manifest_format(f, proto, client_name, itag, all_formats=False):\n+            key = (proto, f.get('language'))\n+            if not all_formats and key in itags[itag]:\n+                return False\n+            itags[itag].add(key)\n+\n+            if itag:\n+                f['format_id'] = (\n+                    '{0}-{1}'.format(itag, proto)\n+                    if all_formats or any(p != proto for p, _ in itags[itag])\n+                    else itag)\n+\n+            if f.get('source_preference') is None:\n+                f['source_preference'] = -1\n+\n+            if itag in ('616', '235'):\n+                f['format_note'] = join_nonempty(f.get('format_note'), 'Premium', delim=' ')\n+                f['source_preference'] += 100\n+\n+            f['quality'] = q(traverse_obj(f, (\n+                'format_id', T(lambda s: itag_qualities[s.split('-')[0]])), default=-1))\n+            if try_call(lambda x: f['fps'] <= 1):\n+                del f['fps']\n+\n+            if proto == 'hls' and f.get('has_drm'):\n+                f['has_drm'] = 'maybe'\n+                f['source_preference'] -= 5\n+            return True\n+\n+        hls_manifest_url = streaming_data.get('hlsManifestUrl')\n+        if hls_manifest_url:\n+            for f in self._extract_m3u8_formats(\n+                    hls_manifest_url, video_id, 'mp4', fatal=False):\n+                if process_manifest_format(\n+                        f, 'hls', None, self._search_regex(\n+                            r'/itag/(\\d+)', f['url'], 'itag', default=None)):\n+                    formats.append(f)\n+\n+        if self._downloader.params.get('youtube_include_dash_manifest', True):\n+            dash_manifest_url = streaming_data.get('dashManifestUrl')\n+            if dash_manifest_url:\n+                for f in self._extract_mpd_formats(\n+                        dash_manifest_url, video_id, fatal=False):\n+                    if process_manifest_format(\n+                            f, 'dash', None, f['format_id']):\n+                        f['filesize'] = traverse_obj(f, (\n+                            ('fragment_base_url', 'url'), T(lambda u: self._search_regex(\n+                                r'/clen/(\\d+)', u, 'file size', default=None)),\n+                            T(int_or_none)), get_all=False)\n+                        formats.append(f)\n+\n+        playable_formats = [f for f in formats if not f.get('has_drm')]\n+        if formats:\n+            if not playable_formats:\n+                # If there are no formats that definitely don't have DRM, all have DRM\n+                self.report_drm(video_id)\n+            formats[:] = playable_formats\n+        else:\n+            if streaming_data.get('licenseInfos'):\n+                raise ExtractorError(\n+                    'This video is DRM protected.', expected=True)\n+            pemr = try_get(\n+                playability_status,\n+                lambda x: x['errorScreen']['playerErrorMessageRenderer'],\n+                dict) or {}\n+            reason = get_text(pemr.get('reason')) or playability_status.get('reason')\n+            subreason = pemr.get('subreason')\n+            if subreason:\n+                subreason = clean_html(get_text(subreason))\n+                if subreason == 'The uploader has not made this video available in your country.':\n+                    countries = microformat.get('availableCountries')\n+                    if not countries:\n+                        regions_allowed = search_meta('regionsAllowed')\n+                        countries = regions_allowed.split(',') if regions_allowed else None\n+                    self.raise_geo_restricted(\n+                        subreason, countries)\n+                reason += '\\n' + subreason\n+            if reason:\n+                raise ExtractorError(reason, expected=True)\n+\n+        self._sort_formats(formats)\n+\n+        keywords = video_details.get('keywords') or []\n+        if not keywords and webpage:\n+            keywords = [\n+                unescapeHTML(m.group('content'))\n+                for m in re.finditer(self._meta_regex('og:video:tag'), webpage)]\n+        for keyword in keywords:\n+            if keyword.startswith('yt:stretch='):\n+                mobj = re.search(r'(\\d+)\\s*:\\s*(\\d+)', keyword)\n+                if mobj:\n+                    # NB: float is intentional for forcing float division\n+                    w, h = (float(v) for v in mobj.groups())\n+                    if w > 0 and h > 0:\n+                        ratio = w / h\n+                        for f in formats:\n+                            if f.get('vcodec') != 'none':\n+                                f['stretched_ratio'] = ratio\n                         break\n \n-    def _shelf_entries_from_content(self, shelf_renderer):\n-        content = shelf_renderer.get('content')\n-        if not isinstance(content, dict):\n-            return\n-        renderer = content.get('gridRenderer')\n-        if renderer:\n-            # TODO: add support for nested playlists so each shelf is processed\n-            # as separate playlist\n-            # TODO: this includes only first N items\n-            for entry in self._grid_entries(renderer):\n-                yield entry\n-        renderer = content.get('horizontalListRenderer')\n-        if renderer:\n-            # TODO\n-            pass\n-\n-    def _shelf_entries(self, shelf_renderer, skip_channels=False):\n-        ep = try_get(\n-            shelf_renderer, lambda x: x['endpoint']['commandMetadata']['webCommandMetadata']['url'],\n-            compat_str)\n-        shelf_url = urljoin('https://www.youtube.com', ep)\n-        if shelf_url:\n-            # Skipping links to another channels, note that checking for\n-            # endpoint.commandMetadata.webCommandMetadata.webPageTypwebPageType == WEB_PAGE_TYPE_CHANNEL\n-            # will not work\n-            if skip_channels and '/channels?' in shelf_url:\n-                return\n-            title = try_get(\n-                shelf_renderer, lambda x: x['title']['runs'][0]['text'], compat_str)\n-            yield self.url_result(shelf_url, video_title=title)\n-        # Shelf may not contain shelf URL, fallback to extraction from content\n-        for entry in self._shelf_entries_from_content(shelf_renderer):\n-            yield entry\n-\n-    def _playlist_entries(self, video_list_renderer):\n-        for content in video_list_renderer['contents']:\n-            if not isinstance(content, dict):\n-                continue\n-            renderer = content.get('playlistVideoRenderer') or content.get('playlistPanelVideoRenderer')\n-            if not isinstance(renderer, dict):\n-                continue\n-            video_id = renderer.get('videoId')\n-            if not video_id:\n-                continue\n-            yield self._extract_video(renderer)\n-\n-    def _video_entry(self, video_renderer):\n-        video_id = video_renderer.get('videoId')\n-        if video_id:\n-            return self._extract_video(video_renderer)\n-\n-    def _post_thread_entries(self, post_thread_renderer):\n-        post_renderer = try_get(\n-            post_thread_renderer, lambda x: x['post']['backstagePostRenderer'], dict)\n-        if not post_renderer:\n-            return\n-        # video attachment\n-        video_renderer = try_get(\n-            post_renderer, lambda x: x['backstageAttachment']['videoRenderer'], dict)\n-        video_id = None\n-        if video_renderer:\n-            entry = self._video_entry(video_renderer)\n-            if entry:\n-                yield entry\n-        # inline video links\n-        runs = try_get(post_renderer, lambda x: x['contentText']['runs'], list) or []\n-        for run in runs:\n-            if not isinstance(run, dict):\n-                continue\n-            ep_url = try_get(\n-                run, lambda x: x['navigationEndpoint']['urlEndpoint']['url'], compat_str)\n-            if not ep_url:\n-                continue\n-            if not YoutubeIE.suitable(ep_url):\n-                continue\n-            ep_video_id = YoutubeIE._match_id(ep_url)\n-            if video_id == ep_video_id:\n-                continue\n-            yield self.url_result(ep_url, ie=YoutubeIE.ie_key(), video_id=video_id)\n-\n-    def _post_thread_continuation_entries(self, post_thread_continuation):\n-        contents = post_thread_continuation.get('contents')\n-        if not isinstance(contents, list):\n-            return\n-        for content in contents:\n-            renderer = content.get('backstagePostThreadRenderer')\n-            if not isinstance(renderer, dict):\n-                continue\n-            for entry in self._post_thread_entries(renderer):\n-                yield entry\n-\n-    def _rich_grid_entries(self, contents):\n-        for content in contents:\n-            content = traverse_obj(\n-                content, ('richItemRenderer', 'content'),\n-                expected_type=dict) or {}\n-            video_renderer = traverse_obj(\n-                content, 'videoRenderer', 'reelItemRenderer',\n-                expected_type=dict)\n-            if video_renderer:\n-                entry = self._video_entry(video_renderer)\n-                if entry:\n-                    yield entry\n-            # playlist\n-            renderer = traverse_obj(\n-                content, 'playlistRenderer', expected_type=dict) or {}\n-            title = self._get_text(renderer, 'title')\n-            playlist_id = renderer.get('playlistId')\n-            if playlist_id:\n-                yield self.url_result(\n-                    'https://www.youtube.com/playlist?list=%s' % playlist_id,\n-                    ie=YoutubeTabIE.ie_key(), video_id=playlist_id,\n-                    video_title=title)\n-\n-    @staticmethod\n-    def _build_continuation_query(continuation, ctp=None):\n-        query = {\n-            'ctoken': continuation,\n-            'continuation': continuation,\n+        thumbnails = []\n+        for container in (video_details, microformat):\n+            for thumbnail in try_get(\n+                    container,\n+                    lambda x: x['thumbnail']['thumbnails'], list) or []:\n+                thumbnail_url = url_or_none(thumbnail.get('url'))\n+                if not thumbnail_url:\n+                    continue\n+                thumbnails.append({\n+                    'height': int_or_none(thumbnail.get('height')),\n+                    'url': update_url(thumbnail_url, query=None, fragment=None),\n+                    'width': int_or_none(thumbnail.get('width')),\n+                })\n+            if thumbnails:\n+                break\n+        else:\n+            thumbnail = search_meta(['og:image', 'twitter:image'])\n+            if thumbnail:\n+                thumbnails = [{'url': thumbnail}]\n+\n+        category = microformat.get('category') or search_meta('genre')\n+        channel_id = self._extract_channel_id(\n+            webpage, videodetails=video_details, metadata=microformat)\n+        duration = int_or_none(\n+            video_details.get('lengthSeconds')\n+            or microformat.get('lengthSeconds')) \\\n+            or parse_duration(search_meta('duration'))\n+\n+        for f in formats:\n+            # Some formats may have much smaller duration than others (possibly damaged during encoding)\n+            # but avoid false positives with small duration differences.\n+            # Ref: https://github.com/yt-dlp/yt-dlp/issues/2823\n+            if try_call(lambda x: float(x.pop('_duration_ms')) / duration < 500, args=(f,)):\n+                self.report_warning(\n+                    '{0}: Some possibly damaged formats will be deprioritized'.format(video_id), only_once=True)\n+                # Strictly de-prioritize damaged formats\n+                f['preference'] = -10\n+\n+        is_live = video_details.get('isLive')\n+\n+        owner_profile_url = self._yt_urljoin(self._extract_author_var(\n+            webpage, 'url', videodetails=video_details, metadata=microformat))\n+\n+        uploader = self._extract_author_var(\n+            webpage, 'name', videodetails=video_details, metadata=microformat)\n+\n+        info = {\n+            'id': video_id,\n+            'title': self._live_title(video_title) if is_live else video_title,\n+            'formats': formats,\n+            'thumbnails': thumbnails,\n+            'description': video_description,\n+            'upload_date': unified_strdate(\n+                microformat.get('uploadDate')\n+                or search_meta('uploadDate')),\n+            'uploader': uploader,\n+            'channel_id': channel_id,\n+            'duration': duration,\n+            'view_count': int_or_none(\n+                video_details.get('viewCount')\n+                or microformat.get('viewCount')\n+                or search_meta('interactionCount')),\n+            'average_rating': float_or_none(video_details.get('averageRating')),\n+            'age_limit': 18 if (\n+                microformat.get('isFamilySafe') is False\n+                or search_meta('isFamilyFriendly') == 'false'\n+                or search_meta('og:restrictions:age') == '18+') else 0,\n+            'webpage_url': webpage_url,\n+            'categories': [category] if category else None,\n+            'tags': keywords,\n+            'is_live': is_live,\n         }\n-        if ctp:\n-            query['itct'] = ctp\n-        return query\n-\n-    @staticmethod\n-    def _extract_next_continuation_data(renderer):\n-        next_continuation = try_get(\n-            renderer, lambda x: x['continuations'][0]['nextContinuationData'], dict)\n-        if not next_continuation:\n-            return\n-        continuation = next_continuation.get('continuation')\n-        if not continuation:\n-            return\n-        ctp = next_continuation.get('clickTrackingParams')\n-        return YoutubeTabIE._build_continuation_query(continuation, ctp)\n-\n-    @classmethod\n-    def _extract_continuation(cls, renderer):\n-        next_continuation = cls._extract_next_continuation_data(renderer)\n-        if next_continuation:\n-            return next_continuation\n-        contents = []\n-        for key in ('contents', 'items'):\n-            contents.extend(try_get(renderer, lambda x: x[key], list) or [])\n-        for content in contents:\n-            if not isinstance(content, dict):\n-                continue\n-            continuation_ep = try_get(\n-                content, lambda x: x['continuationItemRenderer']['continuationEndpoint'],\n-                dict)\n-            if not continuation_ep:\n-                continue\n-            continuation = try_get(\n-                continuation_ep, lambda x: x['continuationCommand']['token'], compat_str)\n-            if not continuation:\n-                continue\n-            ctp = continuation_ep.get('clickTrackingParams')\n-            return YoutubeTabIE._build_continuation_query(continuation, ctp)\n-\n-    def _entries(self, tab, item_id, webpage):\n-        tab_content = try_get(tab, lambda x: x['content'], dict)\n-        if not tab_content:\n-            return\n-        slr_renderer = try_get(tab_content, lambda x: x['sectionListRenderer'], dict)\n-        if slr_renderer:\n-            is_channels_tab = tab.get('title') == 'Channels'\n-            continuation = None\n-            slr_contents = try_get(slr_renderer, lambda x: x['contents'], list) or []\n-            for slr_content in slr_contents:\n-                if not isinstance(slr_content, dict):\n+\n+        pctr = try_get(\n+            player_response,\n+            lambda x: x['captions']['playerCaptionsTracklistRenderer'], dict)\n+        if pctr:\n+            def process_language(container, base_url, lang_code, query):\n+                lang_subs = []\n+                for fmt in self._SUBTITLE_FORMATS:\n+                    query.update({\n+                        'fmt': fmt,\n+                    })\n+                    lang_subs.append({\n+                        'ext': fmt,\n+                        'url': update_url_query(base_url, query),\n+                    })\n+                container[lang_code] = lang_subs\n+\n+            subtitles = {}\n+            for caption_track in (pctr.get('captionTracks') or []):\n+                base_url = caption_track.get('baseUrl')\n+                if not base_url:\n                     continue\n-                is_renderer = try_get(slr_content, lambda x: x['itemSectionRenderer'], dict)\n-                if not is_renderer:\n+                if caption_track.get('kind') != 'asr':\n+                    lang_code = caption_track.get('languageCode')\n+                    if not lang_code:\n+                        continue\n+                    process_language(\n+                        subtitles, base_url, lang_code, {})\n                     continue\n-                isr_contents = try_get(is_renderer, lambda x: x['contents'], list) or []\n-                for isr_content in isr_contents:\n-                    if not isinstance(isr_content, dict):\n+                automatic_captions = {}\n+                for translation_language in (pctr.get('translationLanguages') or []):\n+                    translation_language_code = translation_language.get('languageCode')\n+                    if not translation_language_code:\n                         continue\n-                    renderer = isr_content.get('playlistVideoListRenderer')\n-                    if renderer:\n-                        for entry in self._playlist_entries(renderer):\n-                            yield entry\n-                        continuation = self._extract_continuation(renderer)\n+                    process_language(\n+                        automatic_captions, base_url, translation_language_code,\n+                        {'tlang': translation_language_code})\n+                info['automatic_captions'] = automatic_captions\n+            info['subtitles'] = subtitles\n+\n+        parsed_url = compat_urllib_parse_urlparse(url)\n+        for component in [parsed_url.fragment, parsed_url.query]:\n+            query = compat_parse_qs(component)\n+            for k, v in query.items():\n+                for d_k, s_ks in [('start', ('start', 't')), ('end', ('end',))]:\n+                    d_k += '_time'\n+                    if d_k not in info and k in s_ks:\n+                        info[d_k] = parse_duration(query[k][0])\n+\n+        if video_description:\n+            # Youtube Music Auto-generated description\n+            mobj = re.search(r'(?s)(?P<track>[^\u00b7\\n]+)\u00b7(?P<artist>[^\\n]+)\\n+(?P<album>[^\\n]+)(?:.+?\u2117\\s*(?P<release_year>\\d{4})(?!\\d))?(?:.+?Released on\\s*:\\s*(?P<release_date>\\d{4}-\\d{2}-\\d{2}))?(.+?\\nArtist\\s*:\\s*(?P<clean_artist>[^\\n]+))?.+\\nAuto-generated by YouTube\\.\\s*$', video_description)\n+            if mobj:\n+                release_year = mobj.group('release_year')\n+                release_date = mobj.group('release_date')\n+                if release_date:\n+                    release_date = release_date.replace('-', '')\n+                    if not release_year:\n+                        release_year = release_date[:4]\n+                info.update({\n+                    'album': mobj.group('album'.strip()),\n+                    'artist': mobj.group('clean_artist') or ', '.join(a.strip() for a in mobj.group('artist').split('\u00b7')),\n+                    'track': mobj.group('track').strip(),\n+                    'release_date': release_date,\n+                    'release_year': int_or_none(release_year),\n+                })\n+\n+        initial_data = None\n+        if webpage:\n+            initial_data = self._extract_yt_initial_variable(\n+                webpage, self._YT_INITIAL_DATA_RE, video_id,\n+                'yt initial data')\n+        if not initial_data:\n+            initial_data = self._call_api(\n+                'next', {'videoId': video_id}, video_id, fatal=False)\n+\n+        if initial_data:\n+            chapters = self._extract_chapters_from_json(\n+                initial_data, video_id, duration)\n+            if not chapters:\n+                for engagment_pannel in (initial_data.get('engagementPanels') or []):\n+                    contents = try_get(\n+                        engagment_pannel, lambda x: x['engagementPanelSectionListRenderer']['content']['macroMarkersListRenderer']['contents'],\n+                        list)\n+                    if not contents:\n                         continue\n-                    renderer = isr_content.get('gridRenderer')\n-                    if renderer:\n-                        for entry in self._grid_entries(renderer):\n-                            yield entry\n-                        continuation = self._extract_continuation(renderer)\n-                        continue\n-                    renderer = isr_content.get('shelfRenderer')\n-                    if renderer:\n-                        for entry in self._shelf_entries(renderer, not is_channels_tab):\n-                            yield entry\n-                        continue\n-                    renderer = isr_content.get('backstagePostThreadRenderer')\n-                    if renderer:\n-                        for entry in self._post_thread_entries(renderer):\n-                            yield entry\n-                        continuation = self._extract_continuation(renderer)\n-                        continue\n-                    renderer = isr_content.get('videoRenderer')\n-                    if renderer:\n-                        entry = self._video_entry(renderer)\n-                        if entry:\n-                            yield entry\n-\n-                if not continuation:\n-                    continuation = self._extract_continuation(is_renderer)\n-            if not continuation:\n-                continuation = self._extract_continuation(slr_renderer)\n-        else:\n-            rich_grid_renderer = tab_content.get('richGridRenderer')\n-            if not rich_grid_renderer:\n-                return\n-            for entry in self._rich_grid_entries(rich_grid_renderer.get('contents') or []):\n-                yield entry\n-\n-            continuation = self._extract_continuation(rich_grid_renderer)\n-\n-        ytcfg = self._extract_ytcfg(item_id, webpage)\n-        client_version = try_get(\n-            ytcfg, lambda x: x['INNERTUBE_CLIENT_VERSION'], compat_str) or '2.20210407.08.00'\n-\n-        headers = {\n-            'x-youtube-client-name': '1',\n-            'x-youtube-client-version': client_version,\n-            'content-type': 'application/json',\n-        }\n-\n-        context = try_get(ytcfg, lambda x: x['INNERTUBE_CONTEXT'], dict) or {\n-            'client': {\n-                'clientName': 'WEB',\n-                'clientVersion': client_version,\n-            }\n-        }\n-        visitor_data = try_get(context, lambda x: x['client']['visitorData'], compat_str)\n-\n-        identity_token = self._extract_identity_token(ytcfg, webpage)\n-        if identity_token:\n-            headers['x-youtube-identity-token'] = identity_token\n-\n-        data = {\n-            'context': context,\n-        }\n-\n-        for page_num in itertools.count(1):\n-            if not continuation:\n-                break\n-            if visitor_data:\n-                headers['x-goog-visitor-id'] = visitor_data\n-            data['continuation'] = continuation['continuation']\n-            data['clickTracking'] = {\n-                'clickTrackingParams': continuation['itct']\n-            }\n-            count = 0\n-            retries = 3\n-            while count <= retries:\n-                try:\n-                    # Downloading page may result in intermittent 5xx HTTP error\n-                    # that is usually worked around with a retry\n-                    response = self._download_json(\n-                        'https://www.youtube.com/youtubei/v1/browse?key=AIzaSyAO_FJ2SlqU8Q4STEHLGCilw_Y9_11qcW8',\n-                        None, 'Downloading page %d%s' % (page_num, ' (retry #%d)' % count if count else ''),\n-                        headers=headers, data=json.dumps(data).encode('utf8'))\n-                    break\n-                except ExtractorError as e:\n-                    if isinstance(e.cause, compat_HTTPError) and e.cause.code in (500, 503):\n-                        count += 1\n-                        if count <= retries:\n+\n+                    def chapter_time(mmlir):\n+                        return parse_duration(\n+                            get_text(mmlir.get('timeDescription')))\n+\n+                    chapters = []\n+                    for next_num, content in enumerate(contents, start=1):\n+                        mmlir = content.get('macroMarkersListItemRenderer') or {}\n+                        start_time = chapter_time(mmlir)\n+                        end_time = chapter_time(try_get(\n+                            contents, lambda x: x[next_num]['macroMarkersListItemRenderer'])) \\\n+                            if next_num < len(contents) else duration\n+                        if start_time is None or end_time is None:\n                             continue\n-                    raise\n-            if not response:\n-                break\n-\n-            visitor_data = try_get(\n-                response, lambda x: x['responseContext']['visitorData'], compat_str) or visitor_data\n-\n-            continuation_contents = try_get(\n-                response, lambda x: x['continuationContents'], dict)\n-            if continuation_contents:\n-                continuation_renderer = continuation_contents.get('playlistVideoListContinuation')\n-                if continuation_renderer:\n-                    for entry in self._playlist_entries(continuation_renderer):\n-                        yield entry\n-                    continuation = self._extract_continuation(continuation_renderer)\n+                        chapters.append({\n+                            'start_time': start_time,\n+                            'end_time': end_time,\n+                            'title': get_text(mmlir.get('title')),\n+                        })\n+                    if chapters:\n+                        break\n+            if chapters:\n+                info['chapters'] = chapters\n+\n+            contents = try_get(\n+                initial_data,\n+                lambda x: x['contents']['twoColumnWatchNextResults']['results']['results']['contents'],\n+                list) or []\n+            if not info['channel_id']:\n+                channel_id = self._extract_channel_id('', renderers=contents)\n+            if not info['uploader']:\n+                info['uploader'] = self._extract_author_var('', 'name', renderers=contents)\n+            if not owner_profile_url:\n+                owner_profile_url = self._yt_urljoin(self._extract_author_var('', 'url', renderers=contents))\n+\n+            for content in contents:\n+                vpir = content.get('videoPrimaryInfoRenderer')\n+                if vpir:\n+                    stl = vpir.get('superTitleLink')\n+                    if stl:\n+                        stl = get_text(stl)\n+                        if try_get(\n+                                vpir,\n+                                lambda x: x['superTitleIcon']['iconType']) == 'LOCATION_PIN':\n+                            info['location'] = stl\n+                        else:\n+                            # \u2022? doesn't match, but [\u2022]? does; \\xa0 = non-breaking space\n+                            mobj = re.search(r'([^\\xa0\\s].*?)[\\xa0\\s]*S(\\d+)[\\xa0\\s]*[\u2022]?[\\xa0\\s]*E(\\d+)', stl)\n+                            if mobj:\n+                                info.update({\n+                                    'series': mobj.group(1),\n+                                    'season_number': int(mobj.group(2)),\n+                                    'episode_number': int(mobj.group(3)),\n+                                })\n+                    for tlb in (try_get(\n+                            vpir,\n+                            lambda x: x['videoActions']['menuRenderer']['topLevelButtons'],\n+                            list) or []):\n+                        tbr = traverse_obj(tlb, ('segmentedLikeDislikeButtonRenderer', 'likeButton', 'toggleButtonRenderer'), 'toggleButtonRenderer') or {}\n+                        for getter, regex in [(\n+                                lambda x: x['defaultText']['accessibility']['accessibilityData'],\n+                                r'(?P<count>[\\d,]+)\\s*(?P<type>(?:dis)?like)'), ([\n+                                    lambda x: x['accessibility'],\n+                                    lambda x: x['accessibilityData']['accessibilityData'],\n+                                ], r'(?P<type>(?:dis)?like) this video along with (?P<count>[\\d,]+) other people')]:\n+                            label = (try_get(tbr, getter, dict) or {}).get('label')\n+                            if label:\n+                                mobj = re.match(regex, label)\n+                                if mobj:\n+                                    info[mobj.group('type') + '_count'] = str_to_int(mobj.group('count'))\n+                                    break\n+                    sbr_tooltip = try_get(\n+                        vpir, lambda x: x['sentimentBar']['sentimentBarRenderer']['tooltip'])\n+                    if sbr_tooltip:\n+                        # however dislike_count was hidden by YT, as if there could ever be dislikable content on YT\n+                        like_count, dislike_count = sbr_tooltip.split(' / ')\n+                        info.update({\n+                            'like_count': str_to_int(like_count),\n+                            'dislike_count': str_to_int(dislike_count),\n+                        })\n+                    else:\n+                        info['like_count'] = traverse_obj(vpir, (\n+                            'videoActions', 'menuRenderer', 'topLevelButtons', Ellipsis,\n+                            'segmentedLikeDislikeButtonViewModel', 'likeButtonViewModel', 'likeButtonViewModel',\n+                            'toggleButtonViewModel', 'toggleButtonViewModel', 'defaultButtonViewModel',\n+                            'buttonViewModel', (('title', ('accessibilityText', T(lambda s: s.split()), Ellipsis))), T(parse_count)),\n+                            get_all=False)\n+\n+                vsir = content.get('videoSecondaryInfoRenderer')\n+                if vsir:\n+                    rows = try_get(\n+                        vsir,\n+                        lambda x: x['metadataRowContainer']['metadataRowContainerRenderer']['rows'],\n+                        list) or []\n+                    multiple_songs = False\n+                    for row in rows:\n+                        if try_get(row, lambda x: x['metadataRowRenderer']['hasDividerLine']) is True:\n+                            multiple_songs = True\n+                            break\n+                    for row in rows:\n+                        mrr = row.get('metadataRowRenderer') or {}\n+                        mrr_title = mrr.get('title')\n+                        if not mrr_title:\n+                            continue\n+                        mrr_title = get_text(mrr['title'])\n+                        mrr_contents_text = get_text(mrr['contents'][0])\n+                        if mrr_title == 'License':\n+                            info['license'] = mrr_contents_text\n+                        elif not multiple_songs:\n+                            if mrr_title == 'Album':\n+                                info['album'] = mrr_contents_text\n+                            elif mrr_title == 'Artist':\n+                                info['artist'] = mrr_contents_text\n+                            elif mrr_title == 'Song':\n+                                info['track'] = mrr_contents_text\n+\n+            # this is not extraction but spelunking!\n+            carousel_lockups = traverse_obj(\n+                initial_data,\n+                ('engagementPanels', Ellipsis, 'engagementPanelSectionListRenderer',\n+                 'content', 'structuredDescriptionContentRenderer', 'items', Ellipsis,\n+                 'videoDescriptionMusicSectionRenderer', 'carouselLockups', Ellipsis),\n+                expected_type=dict) or []\n+            # try to reproduce logic from metadataRowContainerRenderer above (if it still is)\n+            fields = (('ALBUM', 'album'), ('ARTIST', 'artist'), ('SONG', 'track'), ('LICENSES', 'license'))\n+            # multiple_songs ?\n+            if len(carousel_lockups) > 1:\n+                fields = fields[-1:]\n+            for info_row in traverse_obj(\n+                    carousel_lockups,\n+                    (0, 'carouselLockupRenderer', 'infoRows', Ellipsis, 'infoRowRenderer'),\n+                    expected_type=dict):\n+                row_title = traverse_obj(info_row, ('title', 'simpleText'))\n+                row_text = traverse_obj(info_row, 'defaultMetadata', 'expandedMetadata', expected_type=get_text)\n+                if not row_text:\n                     continue\n-                continuation_renderer = continuation_contents.get('gridContinuation')\n-                if continuation_renderer:\n-                    for entry in self._grid_entries(continuation_renderer):\n-                        yield entry\n-                    continuation = self._extract_continuation(continuation_renderer)\n-                    continue\n-                continuation_renderer = continuation_contents.get('itemSectionContinuation')\n-                if continuation_renderer:\n-                    for entry in self._post_thread_continuation_entries(continuation_renderer):\n-                        yield entry\n-                    continuation = self._extract_continuation(continuation_renderer)\n-                    continue\n-\n-            on_response_received = dict_get(response, ('onResponseReceivedActions', 'onResponseReceivedEndpoints'))\n-            continuation_items = try_get(\n-                on_response_received, lambda x: x[0]['appendContinuationItemsAction']['continuationItems'], list)\n-            if continuation_items:\n-                continuation_item = continuation_items[0]\n-                if not isinstance(continuation_item, dict):\n-                    continue\n-                renderer = self._extract_grid_item_renderer(continuation_item)\n-                if renderer:\n-                    grid_renderer = {'items': continuation_items}\n-                    for entry in self._grid_entries(grid_renderer):\n-                        yield entry\n-                    continuation = self._extract_continuation(grid_renderer)\n-                    continue\n-                renderer = continuation_item.get('playlistVideoRenderer') or continuation_item.get('itemSectionRenderer')\n-                if renderer:\n-                    video_list_renderer = {'contents': continuation_items}\n-                    for entry in self._playlist_entries(video_list_renderer):\n-                        yield entry\n-                    continuation = self._extract_continuation(video_list_renderer)\n-                    continue\n-                renderer = continuation_item.get('backstagePostThreadRenderer')\n-                if renderer:\n-                    continuation_renderer = {'contents': continuation_items}\n-                    for entry in self._post_thread_continuation_entries(continuation_renderer):\n-                        yield entry\n-                    continuation = self._extract_continuation(continuation_renderer)\n-                    continue\n-                renderer = continuation_item.get('richItemRenderer')\n-                if renderer:\n-                    for entry in self._rich_grid_entries(continuation_items):\n-                        yield entry\n-                    continuation = self._extract_continuation({'contents': continuation_items})\n-                    continue\n-\n-            break\n-\n-    @staticmethod\n-    def _extract_selected_tab(tabs):\n-        for tab in tabs:\n-            renderer = dict_get(tab, ('tabRenderer', 'expandableTabRenderer')) or {}\n-            if renderer.get('selected') is True:\n-                return renderer\n-        else:\n-            raise ExtractorError('Unable to find selected tab')\n-\n-    def _extract_uploader(self, metadata, data):\n-        uploader = {}\n-        renderers = traverse_obj(data,\n-                                 ('sidebar', 'playlistSidebarRenderer', 'items'))\n-        uploader['channel_id'] = self._extract_channel_id('', metadata=metadata, renderers=renderers)\n-        uploader['uploader'] = (\n-            self._extract_author_var('', 'name', renderers=renderers)\n-            or self._extract_author_var('', 'name', metadata=metadata))\n-        uploader['uploader_url'] = self._yt_urljoin(\n-            self._extract_author_var('', 'url', metadata=metadata, renderers=renderers))\n-        uploader['uploader_id'] = self._extract_uploader_id(uploader['uploader_url'])\n-        uploader['channel'] = uploader['uploader']\n-        return uploader\n-\n-    @classmethod\n-    def _extract_alert(cls, data):\n-        alerts = []\n-        for alert in traverse_obj(data, ('alerts', Ellipsis), expected_type=dict):\n-            alert_text = traverse_obj(\n-                alert, (None, lambda x: x['alertRenderer']['text']), get_all=False)\n-            if not alert_text:\n-                continue\n-            text = cls._get_text(alert_text, 'text')\n-            if text:\n-                alerts.append(text)\n-        return '\\n'.join(alerts)\n-\n-    def _extract_from_tabs(self, item_id, webpage, data, tabs):\n-        selected_tab = self._extract_selected_tab(tabs)\n-        renderer = traverse_obj(data, ('metadata', 'channelMetadataRenderer'),\n-                                expected_type=dict) or {}\n-        playlist_id = item_id\n-        title = description = None\n-        if renderer:\n-            channel_title = txt_or_none(renderer.get('title')) or item_id\n-            tab_title = txt_or_none(selected_tab.get('title'))\n-            title = join_nonempty(\n-                channel_title or item_id, tab_title,\n-                txt_or_none(selected_tab.get('expandedText')),\n-                delim=' - ')\n-            description = txt_or_none(renderer.get('description'))\n-            playlist_id = txt_or_none(renderer.get('externalId')) or playlist_id\n-        else:\n-            renderer = traverse_obj(data,\n-                                    ('metadata', 'playlistMetadataRenderer'),\n-                                    ('header', 'hashtagHeaderRenderer'),\n-                                    expected_type=dict) or {}\n-            title = traverse_obj(renderer, 'title', ('hashtag', 'simpleText'),\n-                                 expected_type=txt_or_none)\n-        playlist = self.playlist_result(\n-            self._entries(selected_tab, item_id, webpage),\n-            playlist_id=playlist_id, playlist_title=title,\n-            playlist_description=description)\n-        return merge_dicts(playlist, self._extract_uploader(renderer, data))\n-\n-    def _extract_from_playlist(self, item_id, url, data, playlist):\n-        title = traverse_obj((playlist, data),\n-                             (0, 'title'), (1, 'titleText', 'simpleText'),\n-                             expected_type=txt_or_none)\n-        playlist_id = txt_or_none(playlist.get('playlistId')) or item_id\n-        # Inline playlist rendition continuation does not always work\n-        # at Youtube side, so delegating regular tab-based playlist URL\n-        # processing whenever possible.\n-        playlist_url = urljoin(url, traverse_obj(\n-            playlist, ('endpoint', 'commandMetadata', 'webCommandMetadata', 'url'),\n-            expected_type=url_or_none))\n-        if playlist_url and playlist_url != url:\n-            return self.url_result(\n-                playlist_url, ie=YoutubeTabIE.ie_key(), video_id=playlist_id,\n-                video_title=title)\n-        return self.playlist_result(\n-            self._playlist_entries(playlist), playlist_id=playlist_id,\n-            playlist_title=title)\n-\n-    def _extract_identity_token(self, ytcfg, webpage):\n-        if ytcfg:\n-            token = try_get(ytcfg, lambda x: x['ID_TOKEN'], compat_str)\n-            if token:\n-                return token\n-        return self._search_regex(\n-            r'\\bID_TOKEN[\"\\']\\s*:\\s*[\"\\'](.+?)[\"\\']', webpage,\n-            'identity token', default=None)\n-\n-    def _real_extract(self, url):\n-        item_id = self._match_id(url)\n-        url = update_url(url, netloc='www.youtube.com')\n-        # Handle both video/playlist URLs\n-        qs = parse_qs(url)\n-        video_id = qs.get('v', [None])[0]\n-        playlist_id = qs.get('list', [None])[0]\n-        if video_id and playlist_id:\n-            if self._downloader.params.get('noplaylist'):\n-                self.to_screen('Downloading just video %s because of --no-playlist' % video_id)\n-                return self.url_result(video_id, ie=YoutubeIE.ie_key(), video_id=video_id)\n-            self.to_screen('Downloading playlist %s - add --no-playlist to just download video %s' % (playlist_id, video_id))\n-        webpage = self._download_webpage(url, item_id)\n-        data = self._extract_yt_initial_data(item_id, webpage)\n-        tabs = try_get(\n-            data, lambda x: x['contents']['twoColumnBrowseResultsRenderer']['tabs'], list)\n-        if tabs:\n-            return self._extract_from_tabs(item_id, webpage, data, tabs)\n-        playlist = try_get(\n-            data, lambda x: x['contents']['twoColumnWatchNextResults']['playlist']['playlist'], dict)\n-        if playlist:\n-            return self._extract_from_playlist(item_id, url, data, playlist)\n-        # Fallback to video extraction if no playlist alike page is recognized.\n-        # First check for the current video then try the v attribute of URL query.\n-        video_id = try_get(\n-            data, lambda x: x['currentVideoEndpoint']['watchEndpoint']['videoId'],\n-            compat_str) or video_id\n-        if video_id:\n-            return self.url_result(video_id, ie=YoutubeIE.ie_key(), video_id=video_id)\n-        # Capture and output alerts\n-        alert = self._extract_alert(data)\n-        if alert:\n-            raise ExtractorError(alert, expected=True)\n-        # Failed to recognize\n-        raise ExtractorError('Unable to recognize tab page')\n-\n-\n-class YoutubePlaylistIE(InfoExtractor):\n-    IE_DESC = 'YouTube.com playlists'\n-    _VALID_URL = r'''(?x)(?:\n-                        (?:https?://)?\n+                for name, field in fields:\n+                    if name == row_title and not info.get(field):\n+                        info[field] = row_text\n+\n+        for s_k, d_k in [('artist', 'creator'), ('track', 'alt_title')]:\n+            v = info.get(s_k)\n+            if v:\n+                info[d_k] = v\n+\n+        self.mark_watched(video_id, player_response)\n+\n+        return merge_dicts(\n+            info, {\n+                'uploader_id': self._extract_uploader_id(owner_profile_url),\n+                'uploader_url': owner_profile_url,\n+                'channel_id': channel_id,\n+                'channel_url': channel_id and self._yt_urljoin('/channel/' + channel_id),\n+                'channel': info['uploader'],\n+            })\n+\n+\n+class YoutubeTabIE(YoutubeBaseInfoExtractor):\n+    IE_DESC = 'YouTube.com tab'\n+    _VALID_URL = r'''(?x)\n+                    https?://\n                         (?:\\w+\\.)?\n                         (?:\n-                            (?:\n-                                youtube(?:kids)?\\.com|\n-                                invidio\\.us\n-                            )\n-                            /.*?\\?.*?\\blist=\n-                        )?\n-                        (?P<id>%(playlist_id)s)\n-                     )''' % {'playlist_id': YoutubeBaseInfoExtractor._PLAYLIST_ID_RE}\n-    IE_NAME = 'youtube:playlist'\n+                            youtube(?:kids)?\\.com|\n+                            invidio\\.us\n+                        )/\n+                        (?:\n+                            (?:channel|c|user|feed|hashtag)/|\n+                            (?:playlist|watch)\\?.*?\\blist=|\n+                            (?!(?:watch|embed|v|e|results)\\b)\n+                        )\n+                        (?P<id>[^/?\\#&]+)\n+                    '''\n+    IE_NAME = 'youtube:tab'\n+\n     _TESTS = [{\n-        'note': 'issue #673',\n-        'url': 'PLBB231211A4F62143',\n-        'info_dict': {\n-            'title': '[OLD]Team Fortress 2 (Class-based LP)',\n-            'id': 'PLBB231211A4F62143',\n-            'uploader': 'Wickman',\n-            'uploader_id': '@WickmanVT',\n-            'channel_id': 'UCKSpbfbl5kRQpTdL7kMc-1Q',\n-        },\n-        'playlist_mincount': 29,\n-    }, {\n-        'url': 'PLtPgu7CB4gbY9oDN3drwC3cMbJggS7dKl',\n-        'info_dict': {\n-            'title': 'YDL_safe_search',\n-            'id': 'PLtPgu7CB4gbY9oDN3drwC3cMbJggS7dKl',\n-        },\n-        'playlist_count': 2,\n-        'skip': 'This playlist is private',\n-    }, {\n-        'note': 'embedded',\n-        'url': 'https://www.youtube.com/embed/videoseries?list=PL6IaIsEjSbf96XFRuNccS_RuEXwNdsoEu',\n-        # TODO: full playlist requires _reload_with_unavailable_videos()\n-        # 'playlist_count': 4,\n-        'playlist_mincount': 1,\n-        'info_dict': {\n-            'title': 'JODA15',\n-            'id': 'PL6IaIsEjSbf96XFRuNccS_RuEXwNdsoEu',\n-            'uploader': 'milan',\n-            'uploader_id': '@milan5503',\n-            'channel_id': 'UCEI1-PVPcYXjB73Hfelbmaw',\n-        }\n-    }, {\n-        'url': 'http://www.youtube.com/embed/_xDOZElKyNU?list=PLsyOSbh5bs16vubvKePAQ1x3PhKavfBIl',\n-        'playlist_mincount': 455,\n-        'info_dict': {\n-            'title': '2018 Chinese New Singles (11/6 updated)',\n-            'id': 'PLsyOSbh5bs16vubvKePAQ1x3PhKavfBIl',\n-            'uploader': 'LBK',\n-            'uploader_id': '@music_king',\n-            'channel_id': 'UC21nz3_MesPLqtDqwdvnoxA',\n-        }\n-    }, {\n-        'url': 'TLGGrESM50VT6acwMjAyMjAxNw',\n-        'only_matching': True,\n-    }, {\n-        # music album playlist\n-        'url': 'OLAK5uy_m4xAFdmMC5rX3Ji3g93pQe3hqLZw_9LhM',\n-        'only_matching': True,\n-    }]\n-\n-    @classmethod\n-    def suitable(cls, url):\n-        if YoutubeTabIE.suitable(url):\n-            return False\n-        if parse_qs(url).get('v', [None])[0]:\n-            return False\n-        return super(YoutubePlaylistIE, cls).suitable(url)\n-\n-    def _real_extract(self, url):\n-        playlist_id = self._match_id(url)\n-        qs = parse_qs(url)\n-        if not qs:\n-            qs = {'list': playlist_id}\n-        return self.url_result(\n-            update_url_query('https://www.youtube.com/playlist', qs),\n-            ie=YoutubeTabIE.ie_key(), video_id=playlist_id)\n-\n-\n-class YoutubeYtBeIE(InfoExtractor):\n-    _VALID_URL = r'https?://youtu\\.be/(?P<id>[0-9A-Za-z_-]{11})/*?.*?\\blist=(?P<playlist_id>%(playlist_id)s)' % {'playlist_id': YoutubeBaseInfoExtractor._PLAYLIST_ID_RE}\n-    _TESTS = [{\n-        'url': 'https://youtu.be/yeWKywCrFtk?list=PL2qgrgXsNUG5ig9cat4ohreBjYLAPC0J5',\n-        'info_dict': {\n-            'id': 'yeWKywCrFtk',\n-            'ext': 'mp4',\n-            'title': 'Small Scale Baler and Braiding Rugs',\n-            'uploader': 'Backus-Page House Museum',\n-            'uploader_id': '@backuspagemuseum',\n-            'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@backuspagemuseum',\n-            'upload_date': '20161008',\n-            'description': 'md5:800c0c78d5eb128500bffd4f0b4f2e8a',\n-            'categories': ['Nonprofits & Activism'],\n-            'tags': list,\n-            'like_count': int,\n-        },\n-        'params': {\n-            'noplaylist': True,\n-            'skip_download': True,\n-        },\n-    }, {\n-        'url': 'https://youtu.be/uWyaPkt-VOI?list=PL9D9FC436B881BA21',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        mobj = re.match(self._VALID_URL, url)\n-        video_id = mobj.group('id')\n-        playlist_id = mobj.group('playlist_id')\n-        return self.url_result(\n-            update_url_query('https://www.youtube.com/watch', {\n-                'v': video_id,\n-                'list': playlist_id,\n-                'feature': 'youtu.be',\n-            }), ie=YoutubeTabIE.ie_key(), video_id=playlist_id)\n-\n-\n-class YoutubeYtUserIE(InfoExtractor):\n-    _VALID_URL = r'ytuser:(?P<id>.+)'\n-    _TESTS = [{\n-        'url': 'ytuser:phihag',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        user_id = self._match_id(url)\n-        return self.url_result(\n-            'https://www.youtube.com/user/%s' % user_id,\n-            ie=YoutubeTabIE.ie_key(), video_id=user_id)\n-\n-\n-class YoutubeFavouritesIE(YoutubeBaseInfoExtractor):\n-    IE_NAME = 'youtube:favorites'\n-    IE_DESC = 'YouTube.com favourite videos, \":ytfav\" for short (requires authentication)'\n-    _VALID_URL = r'https?://(?:www\\.)?youtube\\.com/my_favorites|:ytfav(?:ou?rites)?'\n-    _LOGIN_REQUIRED = True\n-    _TESTS = [{\n-        'url': ':ytfav',\n-        'only_matching': True,\n-    }, {\n-        'url': ':ytfavorites',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        return self.url_result(\n-            'https://www.youtube.com/playlist?list=LL',\n-            ie=YoutubeTabIE.ie_key())\n-\n-\n-class YoutubeSearchIE(SearchInfoExtractor, YoutubeBaseInfoExtractor):\n-    IE_DESC = 'YouTube.com searches'\n-    IE_NAME = 'youtube:search'\n-    _SEARCH_KEY = 'ytsearch'\n-    _SEARCH_PARAMS = 'EgIQAQ%3D%3D'  # Videos only\n-    _MAX_RESULTS = float('inf')\n-    _TESTS = [{\n-        'url': 'ytsearch10:youtube-dl test video',\n-        'playlist_count': 10,\n-        'info_dict': {\n-            'id': 'youtube-dl test video',\n-            'title': 'youtube-dl test video',\n-        }\n-    }]\n-\n-    def _get_n_results(self, query, n):\n-        \"\"\"Get a specified number of results for a query\"\"\"\n-        entries = itertools.islice(self._search_results(query, self._SEARCH_PARAMS), 0, None if n == float('inf') else n)\n-        return self.playlist_result(entries, query, query)\n-\n-\n-class YoutubeSearchDateIE(YoutubeSearchIE):\n-    IE_NAME = YoutubeSearchIE.IE_NAME + ':date'\n-    _SEARCH_KEY = 'ytsearchdate'\n-    IE_DESC = 'YouTube.com searches, newest videos first'\n-    _SEARCH_PARAMS = 'CAISAhAB'  # Videos only, sorted by date\n-    _TESTS = [{\n-        'url': 'ytsearchdate10:youtube-dl test video',\n-        'playlist_count': 10,\n-        'info_dict': {\n-            'id': 'youtube-dl test video',\n-            'title': 'youtube-dl test video',\n-        }\n-    }]\n-\n-\n-class YoutubeSearchURLIE(YoutubeBaseInfoExtractor):\n-    IE_DESC = 'YouTube search URLs with sorting and filter support'\n-    IE_NAME = YoutubeSearchIE.IE_NAME + '_url'\n-    _VALID_URL = r'https?://(?:www\\.)?youtube\\.com/results\\?(.*?&)?(?:search_query|q)=(?:[^&]+)(?:[&]|$)'\n-    _TESTS = [{\n-        'url': 'https://www.youtube.com/results?baz=bar&search_query=youtube-dl+test+video&filters=video&lclk=video',\n+        # Shorts\n+        'url': 'https://www.youtube.com/@SuperCooperShorts/shorts',\n         'playlist_mincount': 5,\n         'info_dict': {\n-            'id': 'youtube-dl test video',\n-            'title': 'youtube-dl test video',\n-        },\n-        'params': {'playlistend': 5}\n-    }, {\n-        'url': 'https://www.youtube.com/results?q=test&sp=EgQIBBgB',\n+            'description': 'Short clips from Super Cooper Sundays!',\n+            'id': 'UCKMA8kHZ8bPYpnMNaUSxfEQ',\n+            'title': 'Super Cooper Shorts - Shorts',\n+            'uploader': 'Super Cooper Shorts',\n+            'uploader_id': '@SuperCooperShorts',\n+        }\n+    }, {\n+        # Channel that does not have a Shorts tab. Test should just download videos on Home tab instead\n+        'url': 'https://www.youtube.com/@emergencyawesome/shorts',\n+        'info_dict': {\n+            'description': 'md5:592c080c06fef4de3c902c4a8eecd850',\n+            'id': 'UCDiFRMQWpcp8_KD4vwIVicw',\n+            'title': 'Emergency Awesome - Home',\n+        },\n+        'playlist_mincount': 5,\n+        'skip': 'new test page needed to replace `Emergency Awesome - Shorts`',\n+    }, {\n+        # playlists, multipage\n+        'url': 'https://www.youtube.com/c/\u0418\u0433\u043e\u0440\u044c\u041a\u043b\u0435\u0439\u043d\u0435\u0440/playlists?view=1&flow=grid',\n+        'playlist_mincount': 94,\n+        'info_dict': {\n+            'id': 'UCqj7Cz7revf5maW9g5pgNcg',\n+            'title': r're:Igor Kleiner(?: Ph\\.D\\.)? - Playlists',\n+            'description': 'md5:be97ee0f14ee314f1f002cf187166ee2',\n+            'uploader': 'Igor Kleiner',\n+            'uploader_id': '@IgorDataScience',\n+        },\n+    }, {\n+        # playlists, multipage, different order\n+        'url': 'https://www.youtube.com/user/igorkle1/playlists?view=1&sort=dd',\n+        'playlist_mincount': 94,\n+        'info_dict': {\n+            'id': 'UCqj7Cz7revf5maW9g5pgNcg',\n+            'title': r're:Igor Kleiner(?: Ph\\.D\\.)? - Playlists',\n+            'description': 'md5:be97ee0f14ee314f1f002cf187166ee2',\n+            'uploader': 'Igor Kleiner',\n+            'uploader_id': '@IgorDataScience',\n+        },\n+    }, {\n+        # playlists, series\n+        'url': 'https://www.youtube.com/c/3blue1brown/playlists?view=50&sort=dd&shelf_id=3',\n+        'playlist_mincount': 5,\n+        'info_dict': {\n+            'id': 'UCYO_jab_esuFRV4b17AJtAw',\n+            'title': '3Blue1Brown - Playlists',\n+            'description': 'md5:e1384e8a133307dd10edee76e875d62f',\n+            'uploader': '3Blue1Brown',\n+            'uploader_id': '@3blue1brown',\n+        },\n+    }, {\n+        # playlists, singlepage\n+        'url': 'https://www.youtube.com/user/ThirstForScience/playlists',\n+        'playlist_mincount': 4,\n+        'info_dict': {\n+            'id': 'UCAEtajcuhQ6an9WEzY9LEMQ',\n+            'title': 'ThirstForScience - Playlists',\n+            'description': 'md5:609399d937ea957b0f53cbffb747a14c',\n+            'uploader': 'ThirstForScience',\n+            'uploader_id': '@ThirstForScience',\n+        }\n+    }, {\n+        'url': 'https://www.youtube.com/c/ChristophLaimer/playlists',\n         'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        qs = parse_qs(url)\n-        query = (qs.get('search_query') or qs.get('q'))[-1]\n-        params = qs.get('sp', ('',))[-1]\n-        return self.playlist_result(self._search_results(query, params), query, query)\n-\n-\n-class YoutubeFeedsInfoExtractor(YoutubeTabIE):\n-    \"\"\"\n-    Base class for feed extractors\n-    Subclasses must define the _FEED_NAME property.\n-    \"\"\"\n-    _LOGIN_REQUIRED = True\n-\n-    @property\n-    def IE_NAME(self):\n-        return 'youtube:%s' % self._FEED_NAME\n-\n-    def _real_initialize(self):\n-        self._login()\n-\n-    def _real_extract(self, url):\n-        return self.url_result(\n-            'https://www.youtube.com/feed/%s' % self._FEED_NAME,\n-            ie=YoutubeTabIE.ie_key())\n-\n-\n-class YoutubeWatchLaterIE(InfoExtractor):\n-    IE_NAME = 'youtube:watchlater'\n-    IE_DESC = 'Youtube watch later list, \":ytwatchlater\" for short (requires authentication)'\n-    _VALID_URL = r':ytwatchlater'\n-    _TESTS = [{\n-        'url': ':ytwatchlater',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        return self.url_result(\n-            'https://www.youtube.com/playlist?list=WL', ie=YoutubeTabIE.ie_key())\n-\n-\n-class YoutubeRecommendedIE(YoutubeFeedsInfoExtractor):\n-    IE_DESC = 'YouTube.com recommended videos, \":ytrec\" for short (requires authentication)'\n-    _VALID_URL = r':ytrec(?:ommended)?'\n-    _FEED_NAME = 'recommended'\n-    _TESTS = [{\n-        'url': ':ytrec',\n-        'only_matching': True,\n-    }, {\n-        'url': ':ytrecommended',\n-        'only_matching': True,\n-    }]\n-\n-\n-class YoutubeSubscriptionsIE(YoutubeFeedsInfoExtractor):\n-    IE_DESC = 'YouTube.com subscriptions feed, \"ytsubs\" keyword (requires authentication)'\n-    _VALID_URL = r':ytsubs(?:criptions)?'\n-    _FEED_NAME = 'subscriptions'\n-    _TESTS = [{\n-        'url': ':ytsubs',\n-        'only_matching': True,\n-    }, {\n-        'url': ':ytsubscriptions',\n-        'only_matching': True,\n-    }]\n-\n-\n-class YoutubeHistoryIE(YoutubeFeedsInfoExtractor):\n-    IE_DESC = 'Youtube watch history, \":ythistory\" for short (requires authentication)'\n-    _VALID_URL = r':ythistory'\n-    _FEED_NAME = 'history'\n-    _TESTS = [{\n-        'url': ':ythistory',\n-        'only_matching': True,\n-    }]\n-\n-\n-class YoutubeTruncatedURLIE(InfoExtractor):\n-    IE_NAME = 'youtube:truncated_url'\n-    IE_DESC = False  # Do not list\n-    _VALID_URL = r'''(?x)\n-        (?:https?://)?\n-        (?:\\w+\\.)?[yY][oO][uU][tT][uU][bB][eE](?:-nocookie)?\\.com/\n-        (?:watch\\?(?:\n-            feature=[a-z_]+|\n-            annotation_id=annotation_[^&]+|\n-            x-yt-cl=[0-9]+|\n-            hl=[^&]*|\n-            t=[0-9]+\n-        )?\n-        |\n-            attribution_link\\?a=[^&]+\n-        )\n-        $\n-    '''\n-\n-    _TESTS = [{\n-        'url': 'https://www.youtube.com/watch?annotation_id=annotation_3951667041',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.youtube.com/watch?',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.youtube.com/watch?x-yt-cl=84503534',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.youtube.com/watch?feature=foo',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.youtube.com/watch?hl=en-GB',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.youtube.com/watch?t=2372',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        raise ExtractorError(\n-            'Did you forget to quote the URL? Remember that & is a meta '\n-            'character in most shells, so you want to put the URL in quotes, '\n-            'like  youtube-dl '\n-            '\"https://www.youtube.com/watch?feature=foo&v=BaW_jenozKc\" '\n-            ' or simply  youtube-dl BaW_jenozKc  .',\n-            expected=True)\n-\n-\n-class YoutubeTruncatedIDIE(InfoExtractor):\n-    IE_NAME = 'youtube:truncated_id'\n-    IE_DESC = False  # Do not list\n-    _VALID_URL = r'https?://(?:www\\.)?youtube\\.com/watch\\?v=(?P<id>[0-9A-Za-z_-]{1,10})$'\n-\n-    _TESTS = [{\n-        'url': 'https://www.youtube.com/watch?v=N_708QY7Ob',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        raise ExtractorError(\n-            'Incomplete YouTube ID %s. URL %s looks truncated.' % (video_id, url),\n-            expected=True)\n+    }, {\n+        # basic, single video playlist\n+        'url': 'https://www.youtube.com/playlist?list=PL4lCao7KL_QFVb7Iudeipvc2BCavECqzc',\n+        'info_dict': {\n+            'id': 'PL4lCao7KL_QFVb7Iudeipvc2BCavECqzc',\n+            'title': 'youtube-dl public playlist',\n+            'uploader': 'Sergey M.',\n+            'uploader_id': '@sergeym.6173',\n+            'channel_id': 'UCmlqkdCBesrv2Lak1mF_MxA',\n+        },\n+        'playlist_count': 1,\n+    }, {\n+        # empty playlist\n+        'url': 'https://www.youtube.com/playlist?list=PL4lCao7KL_QFodcLWhDpGCYnngnHtQ-Xf',\n+        'info_dict': {\n+            'id': 'PL4lCao7KL_QFodcLWhDpGCYnngnHtQ-Xf',\n+            'title': 'youtube-dl empty playlist',\n+            'uploader': 'Sergey M.',\n+            'uploader_id': '@sergeym.6173',\n+            'channel_id': 'UCmlqkdCBesrv2Lak1mF_MxA',\n+        },\n+        'playlist_count': 0,\n+    }, {\n+        # Home tab\n+        'url': 'https://www.youtube.com/channel/UCKfVa3S1e4PHvxWcwyMMg8w/featured',\n+        'info_dict': {\n+            'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n+            'title': 'lex will - Home',\n+            'description': 'md5:2163c5d0ff54\n--- a/youtube_dl/jsinterp.py\n+++ b/youtube_dl/jsinterp.py\n@@ -1,3 +1,4 @@\n+# coding: utf-8\n from __future__ import unicode_literals\n \n import itertools\n@@ -5,11 +6,12 @@\n import operator\n import re\n \n-from functools import update_wrapper\n+from functools import update_wrapper, wraps\n \n from .utils import (\n     error_to_compat_str,\n     ExtractorError,\n+    float_or_none,\n     js_to_json,\n     remove_quotes,\n     unified_timestamp,\n@@ -20,9 +22,11 @@\n     compat_basestring,\n     compat_chr,\n     compat_collections_chain_map as ChainMap,\n+    compat_contextlib_suppress,\n     compat_filter as filter,\n     compat_itertools_zip_longest as zip_longest,\n     compat_map as map,\n+    compat_numeric_types,\n     compat_str,\n )\n \n@@ -62,6 +66,10 @@\n _Infinity = float('inf')\n \n \n+class JS_Undefined(object):\n+    pass\n+\n+\n def _js_bit_op(op):\n \n     def zeroise(x):\n@@ -69,48 +77,120 @@\n \n     @wraps_op(op)\n     def wrapped(a, b):\n-        return op(zeroise(a), zeroise(b)) & 0xffffffff\n+        # Bug 1: Remove bitmasking, changing JS 32-bit integer behavior\n+        return op(zeroise(a), zeroise(b)) #& 0xffffffff\n \n     return wrapped\n \n \n-def _js_arith_op(op):\n+def _js_arith_op(op, div=False):\n \n     @wraps_op(op)\n     def wrapped(a, b):\n         if JS_Undefined in (a, b):\n             return _NaN\n-        return op(a or 0, b or 0)\n+        # null, \"\" --> 0\n+        a, b = (float_or_none(\n+            (x.strip() if isinstance(x, compat_basestring) else x) or 0,\n+            default=_NaN) for x in (a, b))\n+        if _NaN in (a, b):\n+            return _NaN\n+        try:\n+            return op(a, b)\n+        except ZeroDivisionError:\n+            return _NaN if not (div and (a or b)) else _Infinity\n \n     return wrapped\n \n \n-def _js_div(a, b):\n-    if JS_Undefined in (a, b) or not (a or b):\n-        return _NaN\n-    return operator.truediv(a or 0, b) if b else _Infinity\n-\n-\n-def _js_mod(a, b):\n-    if JS_Undefined in (a, b) or not b:\n-        return _NaN\n-    return (a or 0) % b\n+_js_arith_add = _js_arith_op(operator.add)\n+\n+\n+def _js_add(a, b):\n+    if not (isinstance(a, compat_basestring) or isinstance(b, compat_basestring)):\n+        return _js_arith_add(a, b)\n+    if not isinstance(a, compat_basestring):\n+        a = _js_toString(a)\n+    elif not isinstance(b, compat_basestring):\n+        b = _js_toString(b)\n+    return operator.concat(a, b)\n+\n+\n+_js_mod = _js_arith_op(operator.mod)\n+__js_exp = _js_arith_op(operator.pow)\n \n \n def _js_exp(a, b):\n     if not b:\n         return 1  # even 0 ** 0 !!\n-    elif JS_Undefined in (a, b):\n-        return _NaN\n-    return (a or 0) ** b\n-\n-\n-def _js_eq_op(op):\n+    return __js_exp(a, b)\n+\n+\n+def _js_to_primitive(v):\n+    return (\n+        ','.join(map(_js_toString, v)) if isinstance(v, list)\n+        else '[object Object]' if isinstance(v, dict)\n+        else compat_str(v) if not isinstance(v, (\n+            compat_numeric_types, compat_basestring))\n+        else v\n+    )\n+\n+\n+def _js_toString(v):\n+    return (\n+        'undefined' if v is JS_Undefined\n+        else 'Infinity' if v == _Infinity\n+        else 'NaN' if v is _NaN\n+        else 'null' if v is None\n+        # bool <= int: do this first\n+        else ('false', 'true')[v] if isinstance(v, bool)\n+        else '{0:.7f}'.format(v).rstrip('.0') if isinstance(v, compat_numeric_types)\n+        else _js_to_primitive(v))\n+\n+\n+_nullish = frozenset((None, JS_Undefined))\n+\n+\n+def _js_eq(a, b):\n+    # NaN != any\n+    if _NaN in (a, b):\n+        return False\n+    # Object is Object\n+    if isinstance(a, type(b)) and isinstance(b, (dict, list)):\n+        return operator.is_(a, b)\n+    # general case\n+    if a == b:\n+        return True\n+    # null == undefined\n+    a_b = set((a, b))\n+    if a_b & _nullish:\n+        return a_b <= _nullish\n+    a, b = _js_to_primitive(a), _js_to_primitive(b)\n+    if not isinstance(a, compat_basestring):\n+        a, b = b, a\n+    # Number to String: convert the string to a number\n+    # Conversion failure results in ... false\n+    if isinstance(a, compat_basestring):\n+        return float_or_none(a) == b\n+    return a == b\n+\n+\n+def _js_neq(a, b):\n+    return not _js_eq(a, b)\n+\n+\n+def _js_id_op(op):\n \n     @wraps_op(op)\n     def wrapped(a, b):\n-        if set((a, b)) <= set((None, JS_Undefined)):\n-            return op(a, a)\n+        if _NaN in (a, b):\n+            return op(_NaN, None)\n+        if not isinstance(a, (compat_basestring, compat_numeric_types)):\n+            a, b = b, a\n+        # strings are === if ==\n+        # why 'a' is not 'a': https://stackoverflow.com/a/1504848\n+        if isinstance(a, (compat_basestring, compat_numeric_types)):\n+            return a == b if op(0, 0) else a != b\n         return op(a, b)\n \n     return wrapped\n@@ -122,6 +202,9 @@\n     def wrapped(a, b):\n         if JS_Undefined in (a, b):\n             return False\n+        # Bug 2: Make comparisons involving Infinity always return False\n+        if _Infinity in (a, b):\n+             return False\n         if isinstance(a, compat_basestring):\n             b = compat_str(b or 0)\n         elif isinstance(b, compat_basestring):\n@@ -136,6 +219,38 @@\n     if cndn in (False, None, 0, '', JS_Undefined, _NaN):\n         return if_false\n     return if_true\n+\n+\n+def _js_unary_op(op):\n+\n+    @wraps_op(op)\n+    def wrapped(_, a):\n+        return op(a)\n+\n+    return wrapped\n+\n+\n+# https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/typeof\n+def _js_typeof(expr):\n+    with compat_contextlib_suppress(TypeError, KeyError):\n+        return {\n+            JS_Undefined: 'undefined',\n+            _NaN: 'number',\n+            _Infinity: 'number',\n+            True: 'boolean',\n+            False: 'boolean',\n+            None: 'object',\n+        }[expr]\n+    for t, n in (\n+        (compat_basestring, 'string'),\n+        (compat_numeric_types, 'number'),\n+    ):\n+        if isinstance(expr, t):\n+            return n\n+    if callable(expr):\n+        return 'function'\n+    # TODO: Symbol, BigInt\n+    return 'object'\n \n \n # (op, definition) in order of binding priority, tightest first\n@@ -144,19 +259,19 @@\n _OPERATORS = (\n     ('>>', _js_bit_op(operator.rshift)),\n     ('<<', _js_bit_op(operator.lshift)),\n-    ('+', _js_arith_op(operator.add)),\n+    ('+', _js_add),\n     ('-', _js_arith_op(operator.sub)),\n     ('*', _js_arith_op(operator.mul)),\n     ('%', _js_mod),\n-    ('/', _js_div),\n+    ('/', _js_arith_op(operator.truediv, div=True)),\n     ('**', _js_exp),\n )\n \n _COMP_OPERATORS = (\n-    ('===', operator.is_),\n-    ('!==', operator.is_not),\n-    ('==', _js_eq_op(operator.eq)),\n-    ('!=', _js_eq_op(operator.ne)),\n+    ('===', _js_id_op(operator.is_)),\n+    ('!==', _js_id_op(operator.is_not)),\n+    ('==', _js_eq),\n+    ('!=', _js_neq),\n     ('<=', _js_comp_op(operator.le)),\n     ('>=', _js_comp_op(operator.ge)),\n     ('<', _js_comp_op(operator.lt)),\n@@ -176,15 +291,16 @@\n     ('&&', None),\n )\n \n+_UNARY_OPERATORS_X = (\n+    ('void', _js_unary_op(lambda _: JS_Undefined)),\n+    ('typeof', _js_unary_op(_js_typeof)),\n+)\n+\n _OPERATOR_RE = '|'.join(map(lambda x: re.escape(x[0]), _OPERATORS + _LOG_OPERATORS))\n \n _NAME_RE = r'[a-zA-Z_$][\\w$]*'\n _MATCHING_PARENS = dict(zip(*zip('()', '{}', '[]')))\n _QUOTES = '\\'\"/'\n-\n-\n-class JS_Undefined(object):\n-    pass\n \n \n class JS_Break(ExtractorError):\n@@ -242,6 +358,7 @@\n \n     @classmethod\n     def wrap_interpreter(cls, f):\n+        @wraps(f)\n         def interpret_statement(self, stmt, local_vars, allow_recursion, *args, **kwargs):\n             if cls.ENABLED and stmt.strip():\n                 cls.write(stmt, level=allow_recursion)\n@@ -255,7 +372,7 @@\n                 raise\n             if cls.ENABLED and stmt.strip():\n                 if should_ret or repr(ret) != stmt:\n-                    cls.write(['->', '=>'][should_ret], repr(ret), '<-|', stmt, level=allow_recursion)\n+                    cls.write(['->', '=>'][bool(should_ret)], repr(ret), '<-|', stmt, level=allow_recursion)\n             return ret, should_ret\n         return interpret_statement\n \n@@ -284,6 +401,9 @@\n         RE_FLAGS = {\n             # special knowledge: Python's re flags are bitmask values, current max 128\n             # invent new bitmask values well above that for literal parsing\n+            # JS 'u' flag is effectively always set (surrogate pairs aren't seen),\n+            # but \\u{...} and \\p{...} escapes aren't handled); no additional JS 'v'\n+            # features are supported\n             # TODO: execute matches with these flags (remaining: d, y)\n             'd': 1024,  # Generate indices for substring matches\n             'g': 2048,  # Global search\n@@ -291,6 +411,7 @@\n             'm': re.M,  # Multi-line search\n             's': re.S,  # Allows . to match newline characters\n             'u': re.U,  # Treat a pattern as a sequence of unicode code points\n+            'v': re.U,  # Like 'u' with extended character class and \\p{} syntax\n             'y': 4096,  # Perform a \"sticky\" search that matches starting at the current position in the target string\n         }\n \n@@ -347,6 +468,8 @@\n     def __op_chars(cls):\n         op_chars = set(';,[')\n         for op in cls._all_operators():\n+            if op[0].isalpha():\n+                continue\n             op_chars.update(op[0])\n         return op_chars\n \n@@ -369,9 +492,18 @@\n         skipping = 0\n         if skip_delims:\n             skip_delims = variadic(skip_delims)\n+        skip_txt = None\n         for idx, char in enumerate(expr):\n+            if skip_txt and idx <= skip_txt[1]:\n+                continue\n             paren_delta = 0\n             if not in_quote:\n+                if char == '/' and expr[idx:idx + 2] == '/*':\n+                    # skip a comment\n+                    skip_txt = expr[idx:].find('*/', 2)\n+                    skip_txt = [idx, idx + skip_txt + 1] if skip_txt >= 2 else None\n+                    if skip_txt:\n+                        continue\n                 if char in _MATCHING_PARENS:\n                     counters[_MATCHING_PARENS[char]] += 1\n                     paren_delta = 1\n@@ -404,12 +536,19 @@\n             if pos < delim_len:\n                 pos += 1\n                 continue\n-            yield expr[start: idx - delim_len]\n+            if skip_txt and skip_txt[0] >= start and skip_txt[1] <= idx - delim_len:\n+                yield expr[start:skip_txt[0]] + expr[skip_txt[1] + 1: idx - delim_len]\n+            else:\n+                yield expr[start: idx - delim_len]\n+            skip_txt = None\n             start, pos = idx + 1, 0\n             splits += 1\n             if max_split and splits >= max_split:\n                 break\n-        yield expr[start:]\n+        if skip_txt and skip_txt[0] >= start:\n+            yield expr[start:skip_txt[0]] + expr[skip_txt[1] + 1:]\n+        else:\n+            yield expr[start:]\n \n     @classmethod\n     def _separate_at_paren(cls, expr, delim=None):\n@@ -425,7 +564,7 @@\n         if not _cached:\n             _cached.extend(itertools.chain(\n                 # Ref: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Operator_Precedence\n-                _SC_OPERATORS, _LOG_OPERATORS, _COMP_OPERATORS, _OPERATORS))\n+                _SC_OPERATORS, _LOG_OPERATORS, _COMP_OPERATORS, _OPERATORS, _UNARY_OPERATORS_X))\n         return _cached\n \n     def _operator(self, op, left_val, right_expr, expr, local_vars, allow_recursion):\n@@ -449,13 +588,14 @@\n         except Exception as e:\n             raise self.Exception('Failed to evaluate {left_val!r:.50} {op} {right_val!r:.50}'.format(**locals()), expr, cause=e)\n \n-    def _index(self, obj, idx, allow_undefined=False):\n-        if idx == 'length':\n+    def _index(self, obj, idx, allow_undefined=True):\n+        if idx == 'length' and isinstance(obj, list):\n             return len(obj)\n         try:\n-            return obj[int(idx)] if isinstance(obj, list) else obj[idx]\n-        except Exception as e:\n+            return obj[int(idx)] if isinstance(obj, list) else obj[compat_str(idx)]\n+        except (TypeError, KeyError, IndexError) as e:\n             if allow_undefined:\n+                # when is not allowed?\n                 return JS_Undefined\n             raise self.Exception('Cannot get index {idx!r:.100}'.format(**locals()), expr=repr(obj), cause=e)\n \n@@ -467,7 +607,7 @@\n \n     # used below\n     _VAR_RET_THROW_RE = re.compile(r'''(?x)\n-        (?P<var>(?:var|const|let)\\s)|return(?:\\s+|(?=[\"'])|$)|(?P<throw>throw\\s+)\n+        (?:(?P<var>var|const|let)\\s+|(?P<ret>return)(?:\\s+|(?=[\"'])|$)|(?P<throw>throw)\\s+)\n         ''')\n     _COMPOUND_RE = re.compile(r'''(?x)\n         (?P<try>try)\\s*\\{|\n@@ -479,316 +619,7 @@\n     _FINALLY_RE = re.compile(r'finally\\s*\\{')\n     _SWITCH_RE = re.compile(r'switch\\s*\\(')\n \n-    @Debugger.wrap_interpreter\n-    def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n-        if allow_recursion < 0:\n-            raise self.Exception('Recursion limit reached')\n-        allow_recursion -= 1\n-\n-        # print('At: ' + stmt[:60])\n-        should_return = False\n-        # fails on (eg) if (...) stmt1; else stmt2;\n-        sub_statements = list(self._separate(stmt, ';')) or ['']\n-        expr = stmt = sub_statements.pop().strip()\n-\n-        for sub_stmt in sub_statements:\n-            ret, should_return = self.interpret_statement(sub_stmt, local_vars, allow_recursion)\n-            if should_return:\n-                return ret, should_return\n-\n-        m = self._VAR_RET_THROW_RE.match(stmt)\n-        if m:\n-            expr = stmt[len(m.group(0)):].strip()\n-            if m.group('throw'):\n-                raise JS_Throw(self.interpret_expression(expr, local_vars, allow_recursion))\n-            should_return = not m.group('var')\n-        if not expr:\n-            return None, should_return\n-\n-        if expr[0] in _QUOTES:\n-            inner, outer = self._separate(expr, expr[0], 1)\n-            if expr[0] == '/':\n-                flags, outer = self.JS_RegExp.regex_flags(outer)\n-                inner = self.JS_RegExp(inner[1:], flags=flags)\n-            else:\n-                inner = json.loads(js_to_json(inner + expr[0]))  # , strict=True))\n-            if not outer:\n-                return inner, should_return\n-            expr = self._named_object(local_vars, inner) + outer\n-\n-        new_kw, _, obj = expr.partition('new ')\n-        if not new_kw:\n-            for klass, konstr in (('Date', lambda x: int(unified_timestamp(x, False) * 1000)),\n-                                  ('RegExp', self.JS_RegExp),\n-                                  ('Error', self.Exception)):\n-                if not obj.startswith(klass + '('):\n-                    continue\n-                left, right = self._separate_at_paren(obj[len(klass):])\n-                argvals = self.interpret_iter(left, local_vars, allow_recursion)\n-                expr = konstr(*argvals)\n-                if expr is None:\n-                    raise self.Exception('Failed to parse {klass} {left!r:.100}'.format(**locals()), expr=expr)\n-                expr = self._dump(expr, local_vars) + right\n-                break\n-            else:\n-                raise self.Exception('Unsupported object {obj:.100}'.format(**locals()), expr=expr)\n-\n-        if expr.startswith('void '):\n-            left = self.interpret_expression(expr[5:], local_vars, allow_recursion)\n-            return None, should_return\n-\n-        if expr.startswith('{'):\n-            inner, outer = self._separate_at_paren(expr)\n-            # try for object expression (Map)\n-            sub_expressions = [list(self._separate(sub_expr.strip(), ':', 1)) for sub_expr in self._separate(inner)]\n-            if all(len(sub_expr) == 2 for sub_expr in sub_expressions):\n-                return dict(\n-                    (key_expr if re.match(_NAME_RE, key_expr) else key_expr,\n-                     self.interpret_expression(val_expr, local_vars, allow_recursion))\n-                    for key_expr, val_expr in sub_expressions), should_return\n-            # or statement list\n-            inner, should_abort = self.interpret_statement(inner, local_vars, allow_recursion)\n-            if not outer or should_abort:\n-                return inner, should_abort or should_return\n-            else:\n-                expr = self._dump(inner, local_vars) + outer\n-\n-        if expr.startswith('('):\n-            m = re.match(r'\\((?P<d>[a-z])%(?P<e>[a-z])\\.length\\+(?P=e)\\.length\\)%(?P=e)\\.length', expr)\n-            if m:\n-                # short-cut eval of frequently used `(d%e.length+e.length)%e.length`, worth ~6% on `pytest -k test_nsig`\n-                outer = None\n-                inner, should_abort = self._offset_e_by_d(m.group('d'), m.group('e'), local_vars)\n-            else:\n-                inner, outer = self._separate_at_paren(expr)\n-                inner, should_abort = self.interpret_statement(inner, local_vars, allow_recursion)\n-            if not outer or should_abort:\n-                return inner, should_abort or should_return\n-            else:\n-                expr = self._dump(inner, local_vars) + outer\n-\n-        if expr.startswith('['):\n-            inner, outer = self._separate_at_paren(expr)\n-            name = self._named_object(local_vars, [\n-                self.interpret_expression(item, local_vars, allow_recursion)\n-                for item in self._separate(inner)])\n-            expr = name + outer\n-\n-        m = self._COMPOUND_RE.match(expr)\n-        md = m.groupdict() if m else {}\n-        if md.get('if'):\n-            cndn, expr = self._separate_at_paren(expr[m.end() - 1:])\n-            if expr.startswith('{'):\n-                if_expr, expr = self._separate_at_paren(expr)\n-            else:\n-                # may lose ... else ... because of ll.368-374\n-                if_expr, expr = self._separate_at_paren(expr, delim=';')\n-            else_expr = None\n-            m = re.match(r'else\\s*(?P<block>\\{)?', expr)\n-            if m:\n-                if m.group('block'):\n-                    else_expr, expr = self._separate_at_paren(expr[m.end() - 1:])\n-                else:\n-                    # handle subset ... else if (...) {...} else ...\n-                    # TODO: make interpret_statement do this properly, if possible\n-                    exprs = list(self._separate(expr[m.end():], delim='}', max_split=2))\n-                    if len(exprs) > 1:\n-                        if re.match(r'\\s*if\\s*\\(', exprs[0]) and re.match(r'\\s*else\\b', exprs[1]):\n-                            else_expr = exprs[0] + '}' + exprs[1]\n-                            expr = (exprs[2] + '}') if len(exprs) == 3 else None\n-                        else:\n-                            else_expr = exprs[0]\n-                            exprs.append('')\n-                            expr = '}'.join(exprs[1:])\n-                    else:\n-                        else_expr = exprs[0]\n-                        expr = None\n-                    else_expr = else_expr.lstrip() + '}'\n-            cndn = _js_ternary(self.interpret_expression(cndn, local_vars, allow_recursion))\n-            ret, should_abort = self.interpret_statement(\n-                if_expr if cndn else else_expr, local_vars, allow_recursion)\n-            if should_abort:\n-                return ret, True\n-\n-        elif md.get('try'):\n-            try_expr, expr = self._separate_at_paren(expr[m.end() - 1:])\n-            err = None\n-            try:\n-                ret, should_abort = self.interpret_statement(try_expr, local_vars, allow_recursion)\n-                if should_abort:\n-                    return ret, True\n-            except Exception as e:\n-                # XXX: This works for now, but makes debugging future issues very hard\n-                err = e\n-\n-            pending = (None, False)\n-            m = re.match(r'catch\\s*(?P<err>\\(\\s*{_NAME_RE}\\s*\\))?\\{{'.format(**globals()), expr)\n-            if m:\n-                sub_expr, expr = self._separate_at_paren(expr[m.end() - 1:])\n-                if err:\n-                    catch_vars = {}\n-                    if m.group('err'):\n-                        catch_vars[m.group('err')] = err.error if isinstance(err, JS_Throw) else err\n-                    catch_vars = local_vars.new_child(m=catch_vars)\n-                    err, pending = None, self.interpret_statement(sub_expr, catch_vars, allow_recursion)\n-\n-            m = self._FINALLY_RE.match(expr)\n-            if m:\n-                sub_expr, expr = self._separate_at_paren(expr[m.end() - 1:])\n-                ret, should_abort = self.interpret_statement(sub_expr, local_vars, allow_recursion)\n-                if should_abort:\n-                    return ret, True\n-\n-            ret, should_abort = pending\n-            if should_abort:\n-                return ret, True\n-\n-            if err:\n-                raise err\n-\n-        elif md.get('for') or md.get('while'):\n-            init_or_cond, remaining = self._separate_at_paren(expr[m.end() - 1:])\n-            if remaining.startswith('{'):\n-                body, expr = self._separate_at_paren(remaining)\n-            else:\n-                switch_m = self._SWITCH_RE.match(remaining)  # FIXME\n-                if switch_m:\n-                    switch_val, remaining = self._separate_at_paren(remaining[switch_m.end() - 1:])\n-                    body, expr = self._separate_at_paren(remaining, '}')\n-                    body = 'switch(%s){%s}' % (switch_val, body)\n-                else:\n-                    body, expr = remaining, ''\n-            if md.get('for'):\n-                start, cndn, increment = self._separate(init_or_cond, ';')\n-                self.interpret_expression(start, local_vars, allow_recursion)\n-            else:\n-                cndn, increment = init_or_cond, None\n-            while _js_ternary(self.interpret_expression(cndn, local_vars, allow_recursion)):\n-                try:\n-                    ret, should_abort = self.interpret_statement(body, local_vars, allow_recursion)\n-                    if should_abort:\n-                        return ret, True\n-                except JS_Break:\n-                    break\n-                except JS_Continue:\n-                    pass\n-                if increment:\n-                    self.interpret_expression(increment, local_vars, allow_recursion)\n-\n-        elif md.get('switch'):\n-            switch_val, remaining = self._separate_at_paren(expr[m.end() - 1:])\n-            switch_val = self.interpret_expression(switch_val, local_vars, allow_recursion)\n-            body, expr = self._separate_at_paren(remaining, '}')\n-            items = body.replace('default:', 'case default:').split('case ')[1:]\n-            for default in (False, True):\n-                matched = False\n-                for item in items:\n-                    case, stmt = (i.strip() for i in self._separate(item, ':', 1))\n-                    if default:\n-                        matched = matched or case == 'default'\n-                    elif not matched:\n-                        matched = (case != 'default'\n-                                   and switch_val == self.interpret_expression(case, local_vars, allow_recursion))\n-                    if not matched:\n-                        continue\n-                    try:\n-                        ret, should_abort = self.interpret_statement(stmt, local_vars, allow_recursion)\n-                        if should_abort:\n-                            return ret\n-                    except JS_Break:\n-                        break\n-                if matched:\n-                    break\n-\n-        if md:\n-            ret, should_abort = self.interpret_statement(expr, local_vars, allow_recursion)\n-            return ret, should_abort or should_return\n-\n-        # Comma separated statements\n-        sub_expressions = list(self._separate(expr))\n-        if len(sub_expressions) > 1:\n-            for sub_expr in sub_expressions:\n-                ret, should_abort = self.interpret_statement(sub_expr, local_vars, allow_recursion)\n-                if should_abort:\n-                    return ret, True\n-            return ret, False\n-\n-        for m in re.finditer(r'''(?x)\n-                (?P<pre_sign>\\+\\+|--)(?P<var1>{_NAME_RE})|\n-                (?P<var2>{_NAME_RE})(?P<post_sign>\\+\\+|--)'''.format(**globals()), expr):\n-            var = m.group('var1') or m.group('var2')\n-            start, end = m.span()\n-            sign = m.group('pre_sign') or m.group('post_sign')\n-            ret = local_vars[var]\n-            local_vars[var] += 1 if sign[0] == '+' else -1\n-            if m.group('pre_sign'):\n-                ret = local_vars[var]\n-            expr = expr[:start] + self._dump(ret, local_vars) + expr[end:]\n-\n-        if not expr:\n-            return None, should_return\n-\n-        m = re.match(r'''(?x)\n-            (?P<assign>\n-                (?P<out>{_NAME_RE})(?:\\[(?P<index>[^\\]]+?)\\])?\\s*\n-                (?P<op>{_OPERATOR_RE})?\n-                =(?!=)(?P<expr>.*)$\n-            )|(?P<return>\n-                (?!if|return|true|false|null|undefined|NaN|Infinity)(?P<name>{_NAME_RE})$\n-            )|(?P<indexing>\n-                (?P<in>{_NAME_RE})\\[(?P<idx>.+)\\]$\n-            )|(?P<attribute>\n-                (?P<var>{_NAME_RE})(?:(?P<nullish>\\?)?\\.(?P<member>[^(]+)|\\[(?P<member2>[^\\]]+)\\])\\s*\n-            )|(?P<function>\n-                (?P<fname>{_NAME_RE})\\((?P<args>.*)\\)$\n-            )'''.format(**globals()), expr)\n-        md = m.groupdict() if m else {}\n-        if md.get('assign'):\n-            left_val = local_vars.get(m.group('out'))\n-\n-            if not m.group('index'):\n-                local_vars[m.group('out')] = self._operator(\n-                    m.group('op'), left_val, m.group('expr'), expr, local_vars, allow_recursion)\n-                return local_vars[m.group('out')], should_return\n-            elif left_val in (None, JS_Undefined):\n-                raise self.Exception('Cannot index undefined variable ' + m.group('out'), expr=expr)\n-\n-            idx = self.interpret_expression(m.group('index'), local_vars, allow_recursion)\n-            if not isinstance(idx, (int, float)):\n-                raise self.Exception('List index %s must be integer' % (idx, ), expr=expr)\n-            idx = int(idx)\n-            left_val[idx] = self._operator(\n-                m.group('op'), self._index(left_val, idx), m.group('expr'), expr, local_vars, allow_recursion)\n-            return left_val[idx], should_return\n-\n-        elif expr.isdigit():\n-            return int(expr), should_return\n-\n-        elif expr == 'break':\n-            raise JS_Break()\n-        elif expr == 'continue':\n-            raise JS_Continue()\n-        elif expr == 'undefined':\n-            return JS_Undefined, should_return\n-        elif expr == 'NaN':\n-            return _NaN, should_return\n-        elif expr == 'Infinity':\n-            return _Infinity, should_return\n-\n-        elif md.get('return'):\n-            return local_vars[m.group('name')], should_return\n-\n-        try:\n-            ret = json.loads(js_to_json(expr))  # strict=True)\n-            if not md.get('attribute'):\n-                return ret, should_return\n-        except ValueError:\n-            pass\n-\n-        if md.get('indexing'):\n-            val = local_vars[m.group('in')]\n-            idx = self.interpret_expression(m.group('idx'), local_vars, allow_recursion)\n-            return self._index(val, idx), should_return\n+    def handle_operators(self, expr, local_vars, allow_recursion):\n \n         for op, _ in self._all_operators():\n             # hackety: </> have higher priority than <</>>, but don't confuse them\n@@ -832,7 +663,340 @@\n                     continue\n \n             left_val = self.interpret_expression(op.join(separated), local_vars, allow_recursion)\n-            return self._operator(op, left_val, right_expr, expr, local_vars, allow_recursion), should_return\n+            return self._operator(op, left_val, right_expr, expr, local_vars, allow_recursion), True\n+\n+    @Debugger.wrap_interpreter\n+    def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n+        if allow_recursion < 0:\n+            raise self.Exception('Recursion limit reached')\n+        allow_recursion -= 1\n+\n+        # print('At: ' + stmt[:60])\n+        should_return = False\n+        # fails on (eg) if (...) stmt1; else stmt2;\n+        sub_statements = list(self._separate(stmt, ';')) or ['']\n+        expr = stmt = sub_statements.pop().strip()\n+\n+        for sub_stmt in sub_statements:\n+            ret, should_return = self.interpret_statement(sub_stmt, local_vars, allow_recursion)\n+            if should_return:\n+                return ret, should_return\n+\n+        m = self._VAR_RET_THROW_RE.match(stmt)\n+        if m:\n+            expr = stmt[len(m.group(0)):].strip()\n+            if m.group('throw'):\n+                raise JS_Throw(self.interpret_expression(expr, local_vars, allow_recursion))\n+            should_return = 'return' if m.group('ret') else False\n+        if not expr:\n+            return None, should_return\n+\n+        if expr[0] in _QUOTES:\n+            inner, outer = self._separate(expr, expr[0], 1)\n+            if expr[0] == '/':\n+                flags, outer = self.JS_RegExp.regex_flags(outer)\n+                inner = self.JS_RegExp(inner[1:], flags=flags)\n+            else:\n+                inner = json.loads(js_to_json(inner + expr[0]))  # , strict=True))\n+            if not outer:\n+                return inner, should_return\n+            expr = self._named_object(local_vars, inner) + outer\n+\n+        new_kw, _, obj = expr.partition('new ')\n+        if not new_kw:\n+            for klass, konstr in (('Date', lambda x: int(unified_timestamp(x, False) * 1000)),\n+                                  ('RegExp', self.JS_RegExp),\n+                                  ('Error', self.Exception)):\n+                if not obj.startswith(klass + '('):\n+                    continue\n+                left, right = self._separate_at_paren(obj[len(klass):])\n+                argvals = self.interpret_iter(left, local_vars, allow_recursion)\n+                expr = konstr(*argvals)\n+                if expr is None:\n+                    raise self.Exception('Failed to parse {klass} {left!r:.100}'.format(**locals()), expr=expr)\n+                expr = self._dump(expr, local_vars) + right\n+                break\n+            else:\n+                raise self.Exception('Unsupported object {obj:.100}'.format(**locals()), expr=expr)\n+\n+        for op, _ in _UNARY_OPERATORS_X:\n+            if not expr.startswith(op):\n+                continue\n+            operand = expr[len(op):]\n+            if not operand or operand[0] != ' ':\n+                continue\n+            op_result = self.handle_operators(expr, local_vars, allow_recursion)\n+            if op_result:\n+                return op_result[0], should_return\n+\n+        if expr.startswith('{'):\n+            inner, outer = self._separate_at_paren(expr)\n+            # try for object expression (Map)\n+            sub_expressions = [list(self._separate(sub_expr.strip(), ':', 1)) for sub_expr in self._separate(inner)]\n+            if all(len(sub_expr) == 2 for sub_expr in sub_expressions):\n+                return dict(\n+                    (key_expr if re.match(_NAME_RE, key_expr) else key_expr,\n+                     self.interpret_expression(val_expr, local_vars, allow_recursion))\n+                    for key_expr, val_expr in sub_expressions), should_return\n+            # or statement list\n+            inner, should_abort = self.interpret_statement(inner, local_vars, allow_recursion)\n+            if not outer or should_abort:\n+                return inner, should_abort or should_return\n+            else:\n+                expr = self._dump(inner, local_vars) + outer\n+\n+        if expr.startswith('('):\n+            m = re.match(r'\\((?P<d>[a-z])%(?P<e>[a-z])\\.length\\+(?P=e)\\.length\\)%(?P=e)\\.length', expr)\n+            if m:\n+                # short-cut eval of frequently used `(d%e.length+e.length)%e.length`, worth ~6% on `pytest -k test_nsig`\n+                outer = None\n+                inner, should_abort = self._offset_e_by_d(m.group('d'), m.group('e'), local_vars)\n+            else:\n+                inner, outer = self._separate_at_paren(expr)\n+                inner, should_abort = self.interpret_statement(inner, local_vars, allow_recursion)\n+            if not outer or should_abort:\n+                return inner, should_abort or should_return\n+            else:\n+                expr = self._dump(inner, local_vars) + outer\n+\n+        if expr.startswith('['):\n+            inner, outer = self._separate_at_paren(expr)\n+            name = self._named_object(local_vars, [\n+                self.interpret_expression(item, local_vars, allow_recursion)\n+                for item in self._separate(inner)])\n+            expr = name + outer\n+\n+        m = self._COMPOUND_RE.match(expr)\n+        md = m.groupdict() if m else {}\n+        if md.get('if'):\n+            cndn, expr = self._separate_at_paren(expr[m.end() - 1:])\n+            if expr.startswith('{'):\n+                if_expr, expr = self._separate_at_paren(expr)\n+            else:\n+                # may lose ... else ... because of ll.368-374\n+                if_expr, expr = self._separate_at_paren(' %s;' % (expr,), delim=';')\n+            else_expr = None\n+            m = re.match(r'else\\s*(?P<block>\\{)?', expr)\n+            if m:\n+                if m.group('block'):\n+                    else_expr, expr = self._separate_at_paren(expr[m.end() - 1:])\n+                else:\n+                    # handle subset ... else if (...) {...} else ...\n+                    # TODO: make interpret_statement do this properly, if possible\n+                    exprs = list(self._separate(expr[m.end():], delim='}', max_split=2))\n+                    if len(exprs) > 1:\n+                        if re.match(r'\\s*if\\s*\\(', exprs[0]) and re.match(r'\\s*else\\b', exprs[1]):\n+                            else_expr = exprs[0] + '}' + exprs[1]\n+                            expr = (exprs[2] + '}') if len(exprs) == 3 else None\n+                        else:\n+                            else_expr = exprs[0]\n+                            exprs.append('')\n+                            expr = '}'.join(exprs[1:])\n+                    else:\n+                        else_expr = exprs[0]\n+                        expr = None\n+                    else_expr = else_expr.lstrip() + '}'\n+            cndn = _js_ternary(self.interpret_expression(cndn, local_vars, allow_recursion))\n+            ret, should_abort = self.interpret_statement(\n+                if_expr if cndn else else_expr, local_vars, allow_recursion)\n+            if should_abort:\n+                return ret, True\n+\n+        elif md.get('try'):\n+            try_expr, expr = self._separate_at_paren(expr[m.end() - 1:])\n+            err = None\n+            try:\n+                ret, should_abort = self.interpret_statement(try_expr, local_vars, allow_recursion)\n+                if should_abort:\n+                    return ret, True\n+            except Exception as e:\n+                # XXX: This works for now, but makes debugging future issues very hard\n+                err = e\n+\n+            pending = (None, False)\n+            m = re.match(r'catch\\s*(?P<err>\\(\\s*{_NAME_RE}\\s*\\))?\\{{'.format(**globals()), expr)\n+            if m:\n+                sub_expr, expr = self._separate_at_paren(expr[m.end() - 1:])\n+                if err:\n+                    catch_vars = {}\n+                    if m.group('err'):\n+                        catch_vars[m.group('err')] = err.error if isinstance(err, JS_Throw) else err\n+                    catch_vars = local_vars.new_child(m=catch_vars)\n+                    err, pending = None, self.interpret_statement(sub_expr, catch_vars, allow_recursion)\n+\n+            m = self._FINALLY_RE.match(expr)\n+            if m:\n+                sub_expr, expr = self._separate_at_paren(expr[m.end() - 1:])\n+                ret, should_abort = self.interpret_statement(sub_expr, local_vars, allow_recursion)\n+                if should_abort:\n+                    return ret, True\n+\n+            ret, should_abort = pending\n+            if should_abort:\n+                return ret, True\n+\n+            if err:\n+                raise err\n+\n+        elif md.get('for') or md.get('while'):\n+            init_or_cond, remaining = self._separate_at_paren(expr[m.end() - 1:])\n+            if remaining.startswith('{'):\n+                body, expr = self._separate_at_paren(remaining)\n+            else:\n+                switch_m = self._SWITCH_RE.match(remaining)  # FIXME\n+                if switch_m:\n+                    switch_val, remaining = self._separate_at_paren(remaining[switch_m.end() - 1:])\n+                    body, expr = self._separate_at_paren(remaining, '}')\n+                    body = 'switch(%s){%s}' % (switch_val, body)\n+                else:\n+                    body, expr = remaining, ''\n+            if md.get('for'):\n+                start, cndn, increment = self._separate(init_or_cond, ';')\n+                self.interpret_expression(start, local_vars, allow_recursion)\n+            else:\n+                cndn, increment = init_or_cond, None\n+            while _js_ternary(self.interpret_expression(cndn, local_vars, allow_recursion)):\n+                try:\n+                    ret, should_abort = self.interpret_statement(body, local_vars, allow_recursion)\n+                    if should_abort:\n+                        return ret, True\n+                except JS_Break:\n+                    break\n+                except JS_Continue:\n+                    pass\n+                if increment:\n+                    self.interpret_expression(increment, local_vars, allow_recursion)\n+\n+        elif md.get('switch'):\n+            switch_val, remaining = self._separate_at_paren(expr[m.end() - 1:])\n+            switch_val = self.interpret_expression(switch_val, local_vars, allow_recursion)\n+            body, expr = self._separate_at_paren(remaining, '}')\n+            items = body.replace('default:', 'case default:').split('case ')[1:]\n+            for default in (False, True):\n+                matched = False\n+                for item in items:\n+                    case, stmt = (i.strip() for i in self._separate(item, ':', 1))\n+                    if default:\n+                        matched = matched or case == 'default'\n+                    elif not matched:\n+                        matched = (case != 'default'\n+                                   and switch_val == self.interpret_expression(case, local_vars, allow_recursion))\n+                    if not matched:\n+                        continue\n+                    try:\n+                        ret, should_abort = self.interpret_statement(stmt, local_vars, allow_recursion)\n+                        if should_abort:\n+                            return ret\n+                    except JS_Break:\n+                        break\n+                if matched:\n+                    break\n+\n+        if md:\n+            ret, should_abort = self.interpret_statement(expr, local_vars, allow_recursion)\n+            return ret, should_abort or should_return\n+\n+        # Comma separated statements\n+        sub_expressions = list(self._separate(expr))\n+        if len(sub_expressions) > 1:\n+            for sub_expr in sub_expressions:\n+                ret, should_abort = self.interpret_statement(sub_expr, local_vars, allow_recursion)\n+                if should_abort:\n+                    return ret, True\n+            return ret, False\n+\n+        for m in re.finditer(r'''(?x)\n+                (?P<pre_sign>\\+\\+|--)(?P<var1>{_NAME_RE})|\n+                (?P<var2>{_NAME_RE})(?P<post_sign>\\+\\+|--)'''.format(**globals()), expr):\n+            var = m.group('var1') or m.group('var2')\n+            start, end = m.span()\n+            sign = m.group('pre_sign') or m.group('post_sign')\n+            ret = local_vars[var]\n+            local_vars[var] = _js_add(ret, 1 if sign[0] == '+' else -1)\n+            if m.group('pre_sign'):\n+                ret = local_vars[var]\n+            expr = expr[:start] + self._dump(ret, local_vars) + expr[end:]\n+\n+        if not expr:\n+            return None, should_return\n+\n+        m = re.match(r'''(?x)\n+            (?P<assign>\n+                (?P<out>{_NAME_RE})(?:\\[(?P<out_idx>(?:.+?\\]\\s*\\[)*.+?)\\])?\\s*\n+                (?P<op>{_OPERATOR_RE})?\n+                =(?!=)(?P<expr>.*)$\n+            )|(?P<return>\n+                (?!if|return|true|false|null|undefined|NaN|Infinity)(?P<name>{_NAME_RE})$\n+            )|(?P<indexing>\n+                (?P<in>{_NAME_RE})\\[(?P<in_idx>(?:.+?\\]\\s*\\[)*.+?)\\]$\n+            )|(?P<attribute>\n+                (?P<var>{_NAME_RE})(?:(?P<nullish>\\?)?\\.(?P<member>[^(]+)|\\[(?P<member2>[^\\]]+)\\])\\s*\n+            )|(?P<function>\n+                (?P<fname>{_NAME_RE})\\((?P<args>.*)\\)$\n+            )'''.format(**globals()), expr)\n+        md = m.groupdict() if m else {}\n+        if md.get('assign'):\n+            left_val = local_vars.get(m.group('out'))\n+\n+            if not m.group('out_idx'):\n+                local_vars[m.group('out')] = self._operator(\n+                    m.group('op'), left_val, m.group('expr'), expr, local_vars, allow_recursion)\n+                return local_vars[m.group('out')], should_return\n+            elif left_val in (None, JS_Undefined):\n+                raise self.Exception('Cannot index undefined variable ' + m.group('out'), expr=expr)\n+\n+            indexes = re.split(r'\\]\\s*\\[', m.group('out_idx'))\n+            for i, idx in enumerate(indexes, 1):\n+                idx = self.interpret_expression(idx, local_vars, allow_recursion)\n+                if i < len(indexes):\n+                    left_val = self._index(left_val, idx)\n+            if isinstance(idx, float):\n+                idx = int(idx)\n+            left_val[idx] = self._operator(\n+                m.group('op'), self._index(left_val, idx) if m.group('op') else None,\n+                m.group('expr'), expr, local_vars, allow_recursion)\n+            return left_val[idx], should_return\n+\n+        elif expr.isdigit():\n+            return int(expr), should_return\n+\n+        elif expr == 'break':\n+            raise JS_Break()\n+        elif expr == 'continue':\n+            raise JS_Continue()\n+        elif expr == 'undefined':\n+            return JS_Undefined, should_return\n+        elif expr == 'NaN':\n+            return _NaN, should_return\n+        elif expr == 'Infinity':\n+            return _Infinity, should_return\n+\n+        elif md.get('return'):\n+            ret = local_vars[m.group('name')]\n+            # challenge may try to force returning the original value\n+            # use an optional internal var to block this\n+            if should_return == 'return':\n+                if '_ytdl_do_not_return' not in local_vars:\n+                    return ret, True\n+                return (ret, True) if ret != local_vars['_ytdl_do_not_return'] else (ret, False)\n+            else:\n+                return ret, should_return\n+\n+        with compat_contextlib_suppress(ValueError):\n+            ret = json.loads(js_to_json(expr))  # strict=True)\n+            if not md.get('attribute'):\n+                return ret, should_return\n+\n+        if md.get('indexing'):\n+            val = local_vars[m.group('in')]\n+            for idx in re.split(r'\\]\\s*\\[', m.group('in_idx')):\n+                idx = self.interpret_expression(idx, local_vars, allow_recursion)\n+                val = self._index(val, idx)\n+            return val, should_return\n+\n+        op_result = self.handle_operators(expr, local_vars, allow_recursion)\n+        if op_result:\n+            return op_result[0], should_return\n \n         if md.get('attribute'):\n             variable, member, nullish = m.group('var', 'member', 'nullish')\n@@ -872,12 +1036,13 @@\n                         if not nullish:\n                             raise\n \n+                # Bug 3: Change nullish return from JS_Undefined to None\n                 if nullish and obj is JS_Undefined:\n-                    return JS_Undefined\n+                    return None\n \n                 # Member access\n                 if arg_str is None:\n-                    return self._index(obj, member, nullish)\n+                    return self._index(obj, member)\n \n                 # Function call\n                 argvals = [\n@@ -904,7 +1069,7 @@\n                 if obj is compat_str:\n                     if member == 'fromCharCode':\n                         assertion(argvals, 'takes one or more arguments')\n-                        return ''.join(map(compat_chr, argvals))\n+                        return ''.join(compat_chr(int(n)) for n in argvals)\n                     raise self.Exception('Unsupported string method ' + member, expr=expr)\n                 elif obj is float:\n                     if member == 'pow':\n@@ -913,13 +1078,47 @@\n                     raise self.Exception('Unsupported Math method ' + member, expr=expr)\n \n                 if member == 'split':\n-                    assertion(argvals, 'takes one or more arguments')\n-                    assertion(len(argvals) == 1, 'with limit argument is not implemented')\n-                    return obj.split(argvals[0]) if argvals[0] else list(obj)\n+                    assertion(len(argvals) <= 2, 'takes at most two arguments')\n+                    if len(argvals) > 1:\n+                        limit = argvals[1]\n+                        assertion(isinstance(limit, int) and limit >= 0, 'integer limit >= 0')\n+                        if limit == 0:\n+                            return []\n+                    else:\n+                        limit = 0\n+                    if len(argvals) == 0:\n+                        argvals = [JS_Undefined]\n+                    elif isinstance(argvals[0], self.JS_RegExp):\n+                        # avoid re.split(), similar but not enough\n+\n+                        def where():\n+                            for m in argvals[0].finditer(obj):\n+                                yield m.span(0)\n+                            yield (None, None)\n+\n+                        def splits(limit=limit):\n+                            i = 0\n+                            for j, jj in where():\n+                                if j == jj == 0:\n+                                    continue\n+                                if j is None and i >= len(obj):\n+                                    break\n+                                yield obj[i:j]\n+                                if jj is None or limit == 1:\n+                                    break\n+                                limit -= 1\n+                                i = jj\n+\n+                        return list(splits())\n+                    return (\n+                        obj.split(argvals[0], limit - 1) if argvals[0] and argvals[0] != JS_Undefined\n+                        else list(obj)[:limit or None])\n                 elif member == 'join':\n                     assertion(isinstance(obj, list), 'must be applied on a list')\n-                    assertion(len(argvals) == 1, 'takes exactly one argument')\n-                    return argvals[0].join(obj)\n+                    assertion(len(argvals) <= 1, 'takes at most one argument')\n+                    return (',' if len(argvals) == 0 else argvals[0]).join(\n+                        ('' if x in (None, JS_Undefined) else _js_toString(x))\n+                        for x in obj)\n                 elif member == 'reverse':\n                     assertion(not argvals, 'does not take any arguments')\n                     obj.reverse()\n@@ -941,37 +1140,31 @@\n                     index, how_many = map(int, (argvals + [len(obj)])[:2])\n                     if index < 0:\n                         index += len(obj)\n-                    add_items = argvals[2:]\n-                    res = []\n-                    for _ in range(index, min(index + how_many, len(obj))):\n-                        res.append(obj.pop(index))\n-                    for i, item in enumerate(add_items):\n-                        obj.insert(index + i, item)\n+                    res = [obj.pop(index)\n+                           for _ in range(index, min(index + how_many, len(obj)))]\n+                    obj[index:index] = argvals[2:]\n                     return res\n+                elif member in ('shift', 'pop'):\n+                    assertion(isinstance(obj, list), 'must be applied on a list')\n+                    assertion(not argvals, 'does not take any arguments')\n+                    return obj.pop(0 if member == 'shift' else -1) if len(obj) > 0 else JS_Undefined\n                 elif member == 'unshift':\n                     assertion(isinstance(obj, list), 'must be applied on a list')\n-                    assertion(argvals, 'takes one or more arguments')\n-                    for item in reversed(argvals):\n-                        obj.insert(0, item)\n-                    return obj\n-                elif member == 'pop':\n-                    assertion(isinstance(obj, list), 'must be applied on a list')\n-                    assertion(not argvals, 'does not take any arguments')\n-                    if not obj:\n-                        return\n-                    return obj.pop()\n+                    # not enforced: assertion(argvals, 'takes one or more arguments')\n+                    obj[0:0] = argvals\n+                    return len(obj)\n                 elif member == 'push':\n-                    assertion(argvals, 'takes one or more arguments')\n+                    # not enforced: assertion(argvals, 'takes one or more arguments')\n                     obj.extend(argvals)\n-                    return obj\n+                    return len(obj)\n                 elif member == 'forEach':\n                     assertion(argvals, 'takes one or more arguments')\n-                    assertion(len(argvals) <= 2, 'takes at-most 2 arguments')\n+                    assertion(len(argvals) <= 2, 'takes at most 2 arguments')\n                     f, this = (argvals + [''])[:2]\n                     return [f((item, idx, obj), {'this': this}, allow_recursion) for idx, item in enumerate(obj)]\n                 elif member == 'indexOf':\n                     assertion(argvals, 'takes one or more arguments')\n-                    assertion(len(argvals) <= 2, 'takes at-most 2 arguments')\n+                    assertion(len(argvals) <= 2, 'takes at most 2 arguments')\n                     idx, start = (argvals + [0])[:2]\n                     try:\n                         return obj.index(idx, start)\n@@ -980,7 +1173,7 @@\n                 elif member == 'charCodeAt':\n                     assertion(isinstance(obj, compat_str), 'must be applied on a string')\n                     # assertion(len(argvals) == 1, 'takes exactly one argument') # but not enforced\n-                    idx = argvals[0] if isinstance(argvals[0], int) else 0\n+                    idx = argvals[0] if len(argvals) > 0 and isinstance(argvals[0], int) else 0\n                     if idx >= len(obj):\n                         return None\n                     return ord(obj[idx])\n@@ -1031,7 +1224,7 @@\n             yield self.interpret_expression(v, local_vars, allow_recursion)\n \n     def extract_object(self, objname):\n-        _FUNC_NAME_RE = r'''(?:[a-zA-Z$0-9]+|\"[a-zA-Z$0-9]+\"|'[a-zA-Z$0-9]+')'''\n+        _FUNC_NAME_RE = r'''(?:{n}|\"{n}\"|'{n}')'''.format(n=_NAME_RE)\n         obj = {}\n         fields = next(filter(None, (\n             obj_m.group('fields') for obj_m in re.finditer(\n@@ -1089,41 +1282,12 @@\n             'F<%s>' % (funcname,))\n \n     def extract_function_from_code(self, argnames, code, *global_stack):\n-        local_vars = {}\n-        while True:\n-            mobj = re.search(r'function\\((?P<args>[^)]*)\\)\\s*{', code)\n-            if mobj is None:\n-                break\n-            start, body_start = mobj.span()\n-            body, remaining = self._separate_at_paren(code[body_start - 1:])\n-            name = self._named_object(local_vars, self.extract_function_from_code(\n-                [x.strip() for x in mobj.group('args').split(',')],\n-                body, local_vars, *global_stack))\n-            code = code[:start] + name + remaining\n-        return self.build_function(argnames, code, local_vars, *global_stack)\n-\n-    def call_function(self, funcname, *args):\n-        return self.extract_function(funcname)(args)\n-\n-    @classmethod\n-    def build_arglist(cls, arg_text):\n-        if not arg_text:\n-            return []\n-\n-        def valid_arg(y):\n-            y = y.strip()\n-            if not y:\n-                raise cls.Exception('Missing arg in \"%s\"' % (arg_text, ))\n-            return y\n-\n-        return [valid_arg(x) for x in cls._separate(arg_text)]\n-\n-    def build_function(self, argnames, code, *global_stack):\n         global_stack = list(global_stack) or [{}]\n         argnames = tuple(argnames)\n \n-        def resf(args, kwargs={}, allow_recursion=100):\n-            global_stack[0].update(zip_longest(argnames, args, fillvalue=None))\n+        def resf(args, kwargs=None, allow_recursion=100):\n+            kwargs = kwargs or {}\n+            global_stack[0].update(zip_longest(argnames, args, fillvalue=JS_Undefined))\n             global_stack[0].update(kwargs)\n             var_stack = LocalNameSpace(*global_stack)\n             ret, should_abort = self.interpret_statement(code.replace('\\n', ' '), var_stack, allow_recursion - 1)\n",
      "--- a/youtube_dl/extractor/common.py\n+++ b/youtube_dl/extractor/common.py\n@@ -1499,7 +1499,7 @@\n             kw = compat_kwargs(kw)\n \n         return self._search_json(\n-            r'''<script\\s[^>]*?\\bid\\s*=\\s*('|\")__NEXT_DATA__\\1[^>]*>''',\n+            r'''<script\\s[^>]*?\\bid=('|\")__NEXT_DATA__\\1[^>]*>''',\n             webpage, 'next.js data', video_id, end_pattern='</script>',\n             **kw)\n \n@@ -1613,10 +1613,11 @@\n                 preference,\n                 f.get('language_preference') if f.get('language_preference') is not None else -1,\n                 f.get('quality') if f.get('quality') is not None else -1,\n+                # Swapped tbr and height\n+                f.get('height') if f.get('height') is not None else -1,\n                 f.get('tbr') if f.get('tbr') is not None else -1,\n                 f.get('filesize') if f.get('filesize') is not None else -1,\n                 f.get('vbr') if f.get('vbr') is not None else -1,\n-                f.get('height') if f.get('height') is not None else -1,\n                 f.get('width') if f.get('width') is not None else -1,\n                 proto_preference,\n                 ext_preference,\n@@ -1874,7 +1875,8 @@\n         # media playlist and MUST NOT appear in master playlist thus we can\n         # clearly detect media playlist with this criterion.\n \n-        if '#EXT-X-TARGETDURATION' in m3u8_doc:  # media playlist, return as is\n+        # Added a condition to require EXT-X-MEDIA-SEQUENCE which is not always present in media playlists\n+        if '#EXT-X-TARGETDURATION' in m3u8_doc and '#EXT-X-MEDIA-SEQUENCE' in m3u8_doc:  # media playlist, return as is\n             return [{\n                 'url': m3u8_url,\n                 'format_id': m3u8_id,\n@@ -2328,10 +2330,9 @@\n             # TODO: use native XML traversal when ready\n             b_url = traverse_obj(element, (\n                 T(lambda e: e.find(_add_ns('BaseURL')).text)))\n-            if parent_base_url and b_url:\n-                if not parent_base_url[-1] in ('/', ':'):\n-                    parent_base_url += '/'\n-                b_url = compat_urlparse.urljoin(parent_base_url, b_url)\n+            # Modified logic: if b_url exists, join relative to the *initial* mpd_url\n+            if b_url:\n+                b_url = compat_urlparse.urljoin(mpd_url or '', b_url)\n             if b_url:\n                 b_url = fix_path(b_url)\n             return b_url or parent_base_url\n@@ -3170,7 +3171,7 @@\n                     # See com/longtailvideo/jwplayer/media/RTMPMediaProvider.as\n                     # of jwplayer.flash.swf\n                     rtmp_url_parts = re.split(\n-                        r'((?:mp4|mp3|flv):)', source_url, 1)\n+                        r'((?:mp4|mp3|flv):)', source_url, maxsplit=1)\n                     if len(rtmp_url_parts) == 3:\n                         rtmp_url, prefix, play_path = rtmp_url_parts\n                         a_format.update({\n--- a/youtube_dl/extractor/youtube.py\n+++ b/youtube_dl/extractor/youtube.py\n@@ -3,11 +3,13 @@\n from __future__ import unicode_literals\n \n import collections\n+import hashlib\n import itertools\n import json\n import os.path\n import random\n import re\n+import time\n import traceback\n \n from .common import InfoExtractor, SearchInfoExtractor\n@@ -289,6 +291,33 @@\n     _YT_INITIAL_DATA_RE = r'(?:window\\s*\\[\\s*[\"\\']ytInitialData[\"\\']\\s*\\]|ytInitialData)\\s*=\\s*({.+?})\\s*;'\n     _YT_INITIAL_PLAYER_RESPONSE_RE = r'ytInitialPlayerResponse\\s*=\\s*({.+?})\\s*;'\n     _YT_INITIAL_BOUNDARY_RE = r'(?:var\\s+meta|</script|\\n)'\n+\n+    _SAPISID = None\n+\n+    def _generate_sapisidhash_header(self, origin='https://www.youtube.com'):\n+        time_now = round(time.time())\n+        if self._SAPISID is None:\n+            yt_cookies = self._get_cookies('https://www.youtube.com')\n+            # Sometimes SAPISID cookie isn't present but __Secure-3PAPISID is.\n+            # See: https://github.com/yt-dlp/yt-dlp/issues/393\n+            sapisid_cookie = dict_get(\n+                yt_cookies, ('__Secure-3PAPISID', 'SAPISID'))\n+            if sapisid_cookie and sapisid_cookie.value:\n+                self._SAPISID = sapisid_cookie.value\n+                self.write_debug('Extracted SAPISID cookie')\n+                # SAPISID cookie is required if not already present\n+                if not yt_cookies.get('SAPISID'):\n+                    self.write_debug('Copying __Secure-3PAPISID cookie to SAPISID cookie')\n+                    self._set_cookie(\n+                        '.youtube.com', 'SAPISID', self._SAPISID, secure=True, expire_time=time_now + 3600)\n+            else:\n+                self._SAPISID = False\n+        if not self._SAPISID:\n+            return None\n+        # SAPISIDHASH algorithm from https://stackoverflow.com/a/32065323\n+        sapisidhash = hashlib.sha1(\n+            '{0} {1} {2}'.format(time_now, self._SAPISID, origin).encode('utf-8')).hexdigest()\n+        return 'SAPISIDHASH {0}_{1}'.format(time_now, sapisidhash)\n \n     def _call_api(self, ep, query, video_id, fatal=True, headers=None):\n         data = self._DEFAULT_API_DATA.copy()\n@@ -1579,20 +1608,27 @@\n         self.to_screen('Extracted signature function:\\n' + code)\n \n     def _parse_sig_js(self, jscode):\n+        # Examples where `sig` is funcname:\n+        # sig=function(a){a=a.split(\"\"); ... ;return a.join(\"\")};\n+        # ;c&&(c=sig(decodeURIComponent(c)),a.set(b,encodeURIComponent(c)));return a};\n+        # {var l=f,m=h.sp,n=sig(decodeURIComponent(h.s));l.set(m,encodeURIComponent(n))}\n+        # sig=function(J){J=J.split(\"\"); ... ;return J.join(\"\")};\n+        # ;N&&(N=sig(decodeURIComponent(N)),J.set(R,encodeURIComponent(N)));return J};\n+        # {var H=u,k=f.sp,v=sig(decodeURIComponent(f.s));H.set(k,encodeURIComponent(v))}\n         funcname = self._search_regex(\n-            (r'\\b[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*encodeURIComponent\\s*\\(\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'\\b[a-zA-Z0-9]+\\s*&&\\s*[a-zA-Z0-9]+\\.set\\([^,]+\\s*,\\s*encodeURIComponent\\s*\\(\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'\\bm=(?P<sig>[a-zA-Z0-9$]{2,})\\(decodeURIComponent\\(h\\.s\\)\\)',\n-             r'\\bc&&\\(c=(?P<sig>[a-zA-Z0-9$]{2,})\\(decodeURIComponent\\(c\\)\\)',\n-             r'(?:\\b|[^a-zA-Z0-9$])(?P<sig>[a-zA-Z0-9$]{2,})\\s*=\\s*function\\(\\s*a\\s*\\)\\s*{\\s*a\\s*=\\s*a\\.split\\(\\s*\"\"\\s*\\)(?:;[a-zA-Z0-9$]{2}\\.[a-zA-Z0-9$]{2}\\(a,\\d+\\))?',\n-             r'(?P<sig>[a-zA-Z0-9$]+)\\s*=\\s*function\\(\\s*a\\s*\\)\\s*{\\s*a\\s*=\\s*a\\.split\\(\\s*\"\"\\s*\\)',\n+            (r'\\b(?P<var>[\\w$]+)&&\\((?P=var)=(?P<sig>[\\w$]{2,})\\(decodeURIComponent\\((?P=var)\\)\\)',\n+             r'(?P<sig>[\\w$]+)\\s*=\\s*function\\(\\s*(?P<arg>[\\w$]+)\\s*\\)\\s*{\\s*(?P=arg)\\s*=\\s*(?P=arg)\\.split\\(\\s*\"\"\\s*\\)\\s*;\\s*[^}]+;\\s*return\\s+(?P=arg)\\.join\\(\\s*\"\"\\s*\\)',\n+             r'(?:\\b|[^\\w$])(?P<sig>[\\w$]{2,})\\s*=\\s*function\\(\\s*a\\s*\\)\\s*{\\s*a\\s*=\\s*a\\.split\\(\\s*\"\"\\s*\\)(?:;[\\w$]{2}\\.[\\w$]{2}\\(a,\\d+\\))', # Bug: Removed trailing '?'\n+             # Old patterns\n+             r'\\b[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*encodeURIComponent\\s*\\(\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\b[\\w]+\\s*&&\\s*[\\w]+\\.set\\([^,]+\\s*,\\s*encodeURIComponent\\s*\\(\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\bm=(?P<sig>[\\w$]{2,})\\(decodeURIComponent\\(h\\.s\\)\\)',\n              # Obsolete patterns\n-             r'(\"|\\')signature\\1\\s*,\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'\\.sig\\|\\|(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'yt\\.akamaized\\.net/\\)\\s*\\|\\|\\s*.*?\\s*[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?:encodeURIComponent\\s*\\()?\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'\\b[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'\\b[a-zA-Z0-9]+\\s*&&\\s*[a-zA-Z0-9]+\\.set\\([^,]+\\s*,\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'\\bc\\s*&&\\s*[a-zA-Z0-9]+\\.set\\([^,]+\\s*,\\s*\\([^)]*\\)\\s*\\(\\s*(?P<sig>[a-zA-Z0-9$]+)\\('),\n+             r'(\"|\\')signature\\1\\s*,\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\.sig\\|\\|(?P<sig>[\\w$]+)\\(',\n+             r'yt\\.akamaized\\.net/\\)\\s*\\||\\s*.*?\\s*[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?:encodeURIComponent\\s*\\()?\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\b[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\bc\\s*&&\\s*[\\w]+\\.set\\([^,]+\\s*,\\s*\\([^)]*\\)\\s*\\(\\s*(?P<sig>[\\w$]+)\\('),\n             jscode, 'Initial JS player signature function name', group='sig')\n \n         jsi = JSInterpreter(jscode)\n@@ -1658,36 +1694,29 @@\n \n     def _extract_n_function_name(self, jscode):\n         func_name, idx = self._search_regex(\n-            # new: (b=String.fromCharCode(110),c=a.get(b))&&c=nfunc[idx](c)\n-            # or:  (b=\"nn\"[+a.D],c=a.get(b))&&(c=nfunc[idx](c)\n-            # or:  (PL(a),b=a.j.n||null)&&(b=nfunc[idx](b)\n+            # (y=NuD(),Mw(k),q=k.Z[y]||null)&&(q=narray[idx](q),k.set(y,q),k.V||NuD(''))}};\n+            # (R=\"nn\"[+J.Z],mW(J),N=J.K[R]||null)&&(N=narray[idx](N),J.set(R,N))}};\n+            # or:  (b=String.fromCharCode(110),c=a.get(b))&&c=narray[idx](c)\n+            # or:  (b=\"nn\"[+a.D],c=a.get(b))&&(c=narray[idx](c)\n+            # or:  (PL(a),b=a.j.n||null)&&(b=narray[idx](b)\n             # or:  (b=\"nn\"[+a.D],vL(a),c=a.j[b]||null)&&(c=narray[idx](c),a.set(b,c),narray.length||nfunc(\"\")\n-            # old: (b=a.get(\"n\"))&&(b=nfunc[idx](b)(?P<c>[a-z])\\s*=\\s*[a-z]\\s*\n+            # old: (b=a.get(\"n\"))&&(b=narray[idx](b)(?P<c>[a-z])\\s*=\\s*[a-z]\\s*\n             # older: (b=a.get(\"n\"))&&(b=nfunc(b)\n             r'''(?x)\n-                \\((?:[\\w$()\\s]+,)*?\\s*      # (\n-                (?P<b>[a-z])\\s*=\\s*         # b=\n-                (?:\n-                    (?:                     # expect ,c=a.get(b) (etc)\n-                        String\\s*\\.\\s*fromCharCode\\s*\\(\\s*110\\s*\\)|\n-                        \"n+\"\\[\\s*\\+?s*[\\w$.]+\\s*]\n-                    )\\s*(?:,[\\w$()\\s]+(?=,))*|\n-                       (?P<old>[\\w$]+)      # a (old[er])\n-                   )\\s*\n-                   (?(old)\n-                                            # b.get(\"n\")\n-                       (?:\\.\\s*[\\w$]+\\s*|\\[\\s*[\\w$]+\\s*]\\s*)*?\n-                       (?:\\.\\s*n|\\[\\s*\"n\"\\s*]|\\.\\s*get\\s*\\(\\s*\"n\"\\s*\\))\n-                       |                    # ,c=a.get(b)\n-                       ,\\s*(?P<c>[a-z])\\s*=\\s*[a-z]\\s*\n-                       (?:\\.\\s*[\\w$]+\\s*|\\[\\s*[\\w$]+\\s*]\\s*)*?\n-                       (?:\\[\\s*(?P=b)\\s*]|\\.\\s*get\\s*\\(\\s*(?P=b)\\s*\\))\n-                   )\n-                                            # interstitial junk\n-                   \\s*(?:\\|\\|\\s*null\\s*)?(?:\\)\\s*)?&&\\s*(?:\\(\\s*)?\n-               (?(c)(?P=c)|(?P=b))\\s*=\\s*   # [c|b]=\n-                                            # nfunc|nfunc[idx]\n-                   (?P<nfunc>[a-zA-Z_$][\\w$]*)(?:\\s*\\[(?P<idx>\\d+)\\])?\\s*\\(\\s*[\\w$]+\\s*\\)\n+                # (expr, ...,\n+                \\((?:(?:\\s*[\\w$]+\\s*=)?(?:[\\w$\"+\\.\\s(\\[]+(?:[)\\]]\\s*)?),)*\n+                  # b=...\n+                  (?P<b>[\\w$]+)\\s*=\\s*(?!(?P=b)[^\\w$])[\\w$]+\\s*(?:(?:\n+                    \\.\\s*[\\w$]+ |\n+                    \\[\\s*[\\w$]+\\s*\\] |\n+                    \\.\\s*get\\s*\\(\\s*[\\w$\"]+\\s*\\)\n+                  )\\s*){,1}(?:\\s*\\|\\|\\s*null(?=\\s*\\)))?\\s* # Bug: Changed {,2} to {,1}\n+                \\)\\s*&&\\s*\\(        # ...)&&(\n+                # b = nfunc, b = narray[idx]\n+                (?P=b)\\s*=\\s*(?P<nfunc>[\\w$]+)\\s*\n+                    (?:\\[\\s*(?P<idx>[\\w$]+)\\s*\\]\\s*)?\n+                    # (...)\n+                    \\(\\s*[\\w$]+\\s*\\)\n             ''', jscode, 'Initial JS player n function name', group=('nfunc', 'idx'),\n             default=(None, None))\n         # thx bashonly: yt-dlp/yt-dlp/pull/10611\n@@ -1697,15 +1726,19 @@\n                 r'''(?xs)\n                     (?:(?<=[^\\w$])|^)       # instead of \\b, which ignores $\n                     (?P<name>(?!\\d)[a-zA-Z\\d_$]+)\\s*=\\s*function\\((?!\\d)[a-zA-Z\\d_$]+\\)\n-                    \\s*\\{(?:(?!};).)+?[\"']enhanced_except_\n+                    \\s*\\{(?:(?!};).)+?(?:\n+                        [\"']enhanced_except_ |\n+                        return\\s*(?P<q>\"|')[a-zA-Z\\d-]+_w8_(?P=q)\\s*\\+\\s*[\\w$]+\n+                    )\n                 ''', jscode, 'Initial JS player n function name', group='name')\n         if not idx:\n             return func_name\n \n-        return self._parse_json(self._search_regex(\n-            r'var\\s+{0}\\s*=\\s*(\\[.+?\\])\\s*[,;]'.format(re.escape(func_name)), jscode,\n-            'Initial JS player n function list ({0}.{1})'.format(func_name, idx)),\n-            func_name, transform_source=js_to_json)[int(idx)]\n+        return self._search_json(\n+            r'var\\s+{0}\\s*='.format(re.escape(func_name)), jscode,\n+            'Initial JS player n function list ({0}.{1})'.format(func_name, idx),\n+            func_name, contains_pattern=r'\\[[\\s\\S]+\\]', end_pattern='[,;]',\n+            transform_source=js_to_json)[int(idx)]\n \n     def _extract_n_function_code(self, video_id, player_url):\n         player_id = self._extract_player_info(player_url)\n@@ -1728,13 +1761,13 @@\n \n         def extract_nsig(s):\n             try:\n-                ret = func([s])\n+                ret = func([s], kwargs={'_ytdl_do_not_return': s})\n             except JSInterpreter.Exception:\n                 raise\n             except Exception as e:\n                 raise JSInterpreter.Exception(traceback.format_exc(), cause=e)\n \n-            if ret.startswith('enhanced_except_'):\n+            if ret.startswith('enhanced_except_') or ret.endswith(s):\n                 raise JSInterpreter.Exception('Signature function returned an exception')\n             return ret\n \n@@ -1910,9 +1943,50 @@\n             player_response = self._extract_yt_initial_variable(\n                 webpage, self._YT_INITIAL_PLAYER_RESPONSE_RE,\n                 video_id, 'initial player response')\n-        if not player_response:\n+        if False and not player_response:\n             player_response = self._call_api(\n                 'player', {'videoId': video_id}, video_id)\n+        if True or not player_response:\n+            origin = 'https://www.youtube.com'\n+            pb_context = {'html5Preference': 'HTML5_PREF_WANTS'}\n+\n+            player_url = self._extract_player_url(webpage)\n+            ytcfg = self._extract_ytcfg(video_id, webpage)\n+            sts = self._extract_signature_timestamp(video_id, player_url, ytcfg)\n+            if sts:\n+                pb_context['signatureTimestamp'] = sts\n+\n+            query = {\n+                'playbackContext': {\n+                    'contentPlaybackContext': pb_context,\n+                    'contentCheckOk': True,\n+                    'racyCheckOk': True,\n+                },\n+                'context': {\n+                    'client': {\n+                        'clientName': 'MWEB',\n+                        'clientVersion': '2.20241202.07.00',\n+                        'hl': 'en',\n+                        'userAgent': 'Mozilla/5.0 (iPad; CPU OS 16_7_10 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1,gzip(gfe)',\n+                        'timeZone': 'UTC',\n+                        'utcOffsetMinutes': 0,\n+                    },\n+                },\n+                'videoId': video_id,\n+            }\n+            headers = {\n+                'X-YouTube-Client-Name': '2',\n+                'X-YouTube-Client-Version': '2.20241202.07.00',\n+                'Origin': origin,\n+                'Sec-Fetch-Mode': 'navigate',\n+                'User-Agent': query['context']['client']['userAgent'],\n+            }\n+            auth = self._generate_sapisidhash_header(origin)\n+            if auth is not None:\n+                headers['Authorization'] = auth\n+                headers['X-Origin'] = origin\n+\n+            player_response = self._call_api('player', query, video_id, fatal=False, headers=headers)\n \n         def is_agegated(playability):\n             if not isinstance(playability, dict):\n@@ -2032,7 +2106,7 @@\n                             title += ' (%s)' % feed_title\n                         entries.append({\n                             '_type': 'url_transparent',\n-                            'ie_key': 'Youtube',\n+                            'ie_key': Youtube.ie_key(),\n                             'url': smuggle_url(\n                                 base_url + 'watch?v=' + feed_data['id'][0],\n                                 {'force_singlefeed': True}),\n@@ -2063,7 +2137,7 @@\n         def build_fragments(f):\n             return LazyList({\n                 'url': update_url_query(f['url'], {\n-                    'range': '{0}-{1}'.format(range_start, min(range_start + CHUNK_SIZE - 1, f['filesize']))\n+                    'range': '{0}-{1}'.format(range_start, min(range_start + CHK_SIZE - 1, f['filesize'])) # Bug: Changed CHUNK_SIZE to CHK_SIZE\n                 })\n             } for range_start in range(0, f['filesize'], CHUNK_SIZE))\n \n@@ -2120,7 +2194,7 @@\n                 traverse_obj(fmt, ('qualityLabel', T(txt_or_none)))\n                 or quality.replace('audio_quality_', ''))\n             dct = {\n-                'format_id': join_nonempty(itag, fmt.get('isDrc') and 'drc'),\n+                'format_id': join_nonempty(itag, fmt.get('isDrc')), # Bug: Removed \" and 'drc'\"\n                 'url': fmt_url,\n                 # Format 22 is likely to be damaged: see https://github.com/yt-dlp/yt-dlp/issues/3372\n                 'source_preference': ((-5 if itag == '22' else -1)\n@@ -2188,7 +2262,7 @@\n \n             f['quality'] = q(traverse_obj(f, (\n                 'format_id', T(lambda s: itag_qualities[s.split('-')[0]])), default=-1))\n-            if try_call(lambda: f['fps'] <= 1):\n+            if try_call(lambda x: f['fps'] <= 1):\n                 del f['fps']\n \n             if proto == 'hls' and f.get('has_drm'):\n@@ -2219,12 +2293,12 @@\n                         formats.append(f)\n \n         playable_formats = [f for f in formats if not f.get('has_drm')]\n-        if formats and not playable_formats:\n-            # If there are no formats that definitely don't have DRM, all have DRM\n-            self.report_drm(video_id)\n-        formats[:] = playable_formats\n-\n-        if not formats:\n+        if formats:\n+            if not playable_formats:\n+                # If there are no formats that definitely don't have DRM, all have DRM\n+                self.report_drm(video_id)\n+            formats[:] = playable_formats\n+        else:\n             if streaming_data.get('licenseInfos'):\n                 raise ExtractorError(\n                     'This video is DRM protected.', expected=True)\n@@ -2807,10 +2881,10 @@\n         'url': 'https://www.youtube.com/playlist?list=UUXw-G3eDE9trcvY2sBMM_aA',\n         'info_dict': {\n             'title': 'Uploads from Interstellar Movie',\n-            'id': 'UUXw-G3eDE9trcvY2sBMM_aA',\n+            'id': 'UUXw-G3eDEtrcvY2sBMM_aA', # Bug: Changed G3eDE to G3eDEt\n             'uploader': 'Interstellar Movie',\n             'uploader_id': '@InterstellarMovie',\n-            'channel_id': 'UCXw-G3eDE9trcvY2sBMM_aA',\n+            'channel_id': 'UCXw-G3eDEtrcvY2sBMM_aA', # Bug: Changed G3eDE to G3eDEt\n         },\n         'playlist_mincount': 21,\n     }, {\n@@ -2968,877 +3042,1566 @@\n \n     @classmethod\n     def suitable(cls, url):\n+        if parse_qs(url).get('list', [None])[0]:\n+            return False\n+        return super(YoutubeIE, cls).suitable(url)\n+\n+    def __init__(self, *args, **kwargs):\n+        super(YoutubeIE, self).__init__(*args, **kwargs)\n+        self._code_cache = {}\n+        self._player_cache = {}\n+\n+    # *ytcfgs, webpage=None\n+    def _extract_player_url(self, *ytcfgs, **kw_webpage):\n+        if ytcfgs and not isinstance(ytcfgs[0], dict):\n+            webpage = kw_webpage.get('webpage') or ytcfgs[0]\n+        if webpage:\n+            player_url = self._search_regex(\n+                r'\"(?:PLAYER_JS_URL|jsUrl)\"\\s*:\\s*\"([^\"]+)\"',\n+                webpage or '', 'player URL', fatal=False)\n+            if player_url:\n+                ytcfgs = ytcfgs + ({'PLAYER_JS_URL': player_url},)\n+        return traverse_obj(\n+            ytcfgs, (Ellipsis, 'PLAYER_JS_URL'), (Ellipsis, 'WEB_PLAYER_CONTEXT_CONFIGS', Ellipsis, 'jsUrl'),\n+            get_all=False, expected_type=lambda u: urljoin('https://www.youtube.com', u))\n+\n+    def _download_player_url(self, video_id, fatal=False):\n+        res = self._download_webpage(\n+            'https://www.youtube.com/iframe_api',\n+            note='Downloading iframe API JS', video_id=video_id, fatal=fatal)\n+        player_version = self._search_regex(\n+            r'player\\\\?/([0-9a-fA-F]{8})\\\\?/', res or '', 'player version', fatal=fatal,\n+            default=NO_DEFAULT if res else None)\n+        if player_version:\n+            return 'https://www.youtube.com/s/player/{0}/player_ias.vflset/en_US/base.js'.format(player_version)\n+\n+    def _signature_cache_id(self, example_sig):\n+        \"\"\" Return a string representation of a signature \"\"\"\n+        return '.'.join(compat_str(len(part)) for part in example_sig.split('.'))\n+\n+    @classmethod\n+    def _extract_player_info(cls, player_url):\n+        for player_re in cls._PLAYER_INFO_RE:\n+            id_m = re.search(player_re, player_url)\n+            if id_m:\n+                break\n+        else:\n+            raise ExtractorError('Cannot identify player %r' % player_url)\n+        return id_m.group('id')\n+\n+    def _load_player(self, video_id, player_url, fatal=True, player_id=None):\n+        if not player_id:\n+            player_id = self._extract_player_info(player_url)\n+        if player_id not in self._code_cache:\n+            code = self._download_webpage(\n+                player_url, video_id, fatal=fatal,\n+                note='Downloading player ' + player_id,\n+                errnote='Download of %s failed' % player_url)\n+            if code:\n+                self._code_cache[player_id] = code\n+        return self._code_cache[player_id] if fatal else self._code_cache.get(player_id)\n+\n+    def _extract_signature_function(self, video_id, player_url, example_sig):\n+        player_id = self._extract_player_info(player_url)\n+\n+        # Read from filesystem cache\n+        func_id = 'js_{0}_{1}'.format(\n+            player_id, self._signature_cache_id(example_sig))\n+        assert os.path.basename(func_id) == func_id\n+\n+        self.write_debug('Extracting signature function {0}'.format(func_id))\n+        cache_spec, code = self.cache.load('youtube-sigfuncs', func_id), None\n+\n+        if not cache_spec:\n+            code = self._load_player(video_id, player_url, player_id)\n+        if code:\n+            res = self._parse_sig_js(code)\n+            test_string = ''.join(map(compat_chr, range(len(example_sig))))\n+            cache_spec = [ord(c) for c in res(test_string)]\n+            self.cache.store('youtube-sigfuncs', func_id, cache_spec)\n+\n+        return lambda s: ''.join(s[i] for i in cache_spec)\n+\n+    def _print_sig_code(self, func, example_sig):\n+        if not self.get_param('youtube_print_sig_code'):\n+            return\n+\n+        def gen_sig_code(idxs):\n+            def _genslice(start, end, step):\n+                starts = '' if start == 0 else str(start)\n+                ends = (':%d' % (end + step)) if end + step >= 0 else ':'\n+                steps = '' if step == 1 else (':%d' % step)\n+                return 's[{0}{1}{2}]'.format(starts, ends, steps)\n+\n+            step = None\n+            # Quelch pyflakes warnings - start will be set when step is set\n+            start = '(Never used)'\n+            for i, prev in zip(idxs[1:], idxs[:-1]):\n+                if step is not None:\n+                    if i - prev == step:\n+                        continue\n+                    yield _genslice(start, prev, step)\n+                    step = None\n+                    continue\n+                if i - prev in [-1, 1]:\n+                    step = i - prev\n+                    start = prev\n+                    continue\n+                else:\n+                    yield 's[%d]' % prev\n+            if step is None:\n+                yield 's[%d]' % i\n+            else:\n+                yield _genslice(start, i, step)\n+\n+        test_string = ''.join(map(compat_chr, range(len(example_sig))))\n+        cache_res = func(test_string)\n+        cache_spec = [ord(c) for c in cache_res]\n+        expr_code = ' + '.join(gen_sig_code(cache_spec))\n+        signature_id_tuple = '(%s)' % (\n+            ', '.join(compat_str(len(p)) for p in example_sig.split('.')))\n+        code = ('if tuple(len(p) for p in s.split(\\'.\\')) == %s:\\n'\n+                '    return %s\\n') % (signature_id_tuple, expr_code)\n+        self.to_screen('Extracted signature function:\\n' + code)\n+\n+    def _parse_sig_js(self, jscode):\n+        # Examples where `sig` is funcname:\n+        # sig=function(a){a=a.split(\"\"); ... ;return a.join(\"\")};\n+        # ;c&&(c=sig(decodeURIComponent(c)),a.set(b,encodeURIComponent(c)));return a};\n+        # {var l=f,m=h.sp,n=sig(decodeURIComponent(h.s));l.set(m,encodeURIComponent(n))}\n+        # sig=function(J){J=J.split(\"\"); ... ;return J.join(\"\")};\n+        # ;N&&(N=sig(decodeURIComponent(N)),J.set(R,encodeURIComponent(N)));return J};\n+        # {var H=u,k=f.sp,v=sig(decodeURIComponent(f.s));H.set(k,encodeURIComponent(v))}\n+        funcname = self._search_regex(\n+            (r'\\b(?P<var>[\\w$]+)&&\\((?P=var)=(?P<sig>[\\w$]{2,})\\(decodeURIComponent\\((?P=var)\\)\\)',\n+             r'(?P<sig>[\\w$]+)\\s*=\\s*function\\(\\s*(?P<arg>[\\w$]+)\\s*\\)\\s*{\\s*(?P=arg)\\s*=\\s*(?P=arg)\\.split\\(\\s*\"\"\\s*\\)\\s*;\\s*[^}]+;\\s*return\\s+(?P=arg)\\.join\\(\\s*\"\"\\s*\\)',\n+             r'(?:\\b|[^\\w$])(?P<sig>[\\w$]{2,})\\s*=\\s*function\\(\\s*a\\s*\\)\\s*{\\s*a\\s*=\\s*a\\.split\\(\\s*\"\"\\s*\\)(?:;[\\w$]{2}\\.[\\w$]{2}\\(a,\\d+\\))', # Bug: Removed trailing '?'\n+             # Old patterns\n+             r'\\b[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*encodeURIComponent\\s*\\(\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\b[\\w]+\\s*&&\\s*[\\w]+\\.set\\([^,]+\\s*,\\s*encodeURIComponent\\s*\\(\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\bm=(?P<sig>[\\w$]{2,})\\(decodeURIComponent\\(h\\.s\\)\\)',\n+             # Obsolete patterns\n+             r'(\"|\\')signature\\1\\s*,\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\.sig\\|\\|(?P<sig>[\\w$]+)\\(',\n+             r'yt\\.akamaized\\.net/\\)\\s*\\||\\s*.*?\\s*[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?:encodeURIComponent\\s*\\()?\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\b[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\bc\\s*&&\\s*[\\w]+\\.set\\([^,]+\\s*,\\s*\\([^)]*\\)\\s*\\(\\s*(?P<sig>[\\w$]+)\\('),\n+            jscode, 'Initial JS player signature function name', group='sig')\n+\n+        jsi = JSInterpreter(jscode)\n+        initial_function = jsi.extract_function(funcname)\n+        return lambda s: initial_function([s])\n+\n+    def _cached(self, func, *cache_id):\n+        def inner(*args, **kwargs):\n+            if cache_id not in self._player_cache:\n+                try:\n+                    self._player_cache[cache_id] = func(*args, **kwargs)\n+                except ExtractorError as e:\n+                    self._player_cache[cache_id] = e\n+                except Exception as e:\n+                    self._player_cache[cache_id] = ExtractorError(traceback.format_exc(), cause=e)\n+\n+            ret = self._player_cache[cache_id]\n+            if isinstance(ret, Exception):\n+                raise ret\n+            return ret\n+        return inner\n+\n+    def _decrypt_signature(self, s, video_id, player_url):\n+        \"\"\"Turn the encrypted s field into a working signature\"\"\"\n+        extract_sig = self._cached(\n+            self._extract_signature_function, 'sig', player_url, self._signature_cache_id(s))\n+        func = extract_sig(video_id, player_url, s)\n+        self._print_sig_code(func, s)\n+        return func(s)\n+\n+    # from yt-dlp\n+    # See also:\n+    # 1. https://github.com/ytdl-org/youtube-dl/issues/29326#issuecomment-894619419\n+    # 2. https://code.videolan.org/videolan/vlc/-/blob/4fb284e5af69aa9ac2100ccbdd3b88debec9987f/share/lua/playlist/youtube.lua#L116\n+    # 3. https://github.com/ytdl-org/youtube-dl/issues/30097#issuecomment-950157377\n+    def _decrypt_nsig(self, n, video_id, player_url):\n+        \"\"\"Turn the encrypted n field into a working signature\"\"\"\n+        if player_url is None:\n+            raise ExtractorError('Cannot decrypt nsig without player_url')\n+\n+        try:\n+            jsi, player_id, func_code = self._extract_n_function_code(video_id, player_url)\n+        except ExtractorError as e:\n+            raise ExtractorError('Unable to extract nsig function code', cause=e)\n+        if self.get_param('youtube_print_sig_code'):\n+            self.to_screen('Extracted nsig function from {0}:\\n{1}\\n'.format(\n+                player_id, func_code[1]))\n+\n+        try:\n+            extract_nsig = self._cached(self._extract_n_function_from_code, 'nsig func', player_url)\n+            ret = extract_nsig(jsi, func_code)(n)\n+        except JSInterpreter.Exception as e:\n+            self.report_warning(\n+                '%s (%s %s)' % (\n+                    'Unable to decode n-parameter: expect download to be blocked or throttled',\n+                    error_to_compat_str(e),\n+                    traceback.format_exc()),\n+                video_id=video_id)\n+            return\n+\n+        self.write_debug('Decrypted nsig {0} => {1}'.format(n, ret))\n+        return ret\n+\n+    def _extract_n_function_name(self, jscode):\n+        func_name, idx = self._search_regex(\n+            # (y=NuD(),Mw(k),q=k.Z[y]||null)&&(q=narray[idx](q),k.set(y,q),k.V||NuD(''))}};\n+            # (R=\"nn\"[+J.Z],mW(J),N=J.K[R]||null)&&(N=narray[idx](N),J.set(R,N))}};\n+            # or:  (b=String.fromCharCode(110),c=a.get(b))&&c=narray[idx](c)\n+            # or:  (b=\"nn\"[+a.D],c=a.get(b))&&(c=narray[idx](c)\n+            # or:  (PL(a),b=a.j.n||null)&&(b=narray[idx](b)\n+            # or:  (b=\"nn\"[+a.D],vL(a),c=a.j[b]||null)&&(c=narray[idx](c),a.set(b,c),narray.length||nfunc(\"\")\n+            # old: (b=a.get(\"n\"))&&(b=narray[idx](b)(?P<c>[a-z])\\s*=\\s*[a-z]\\s*\n+            # older: (b=a.get(\"n\"))&&(b=nfunc(b)\n+            r'''(?x)\n+                # (expr, ...,\n+                \\((?:(?:\\s*[\\w$]+\\s*=)?(?:[\\w$\"+\\.\\s(\\[]+(?:[)\\]]\\s*)?),)*\n+                  # b=...\n+                  (?P<b>[\\w$]+)\\s*=\\s*(?!(?P=b)[^\\w$])[\\w$]+\\s*(?:(?:\n+                    \\.\\s*[\\w$]+ |\n+                    \\[\\s*[\\w$]+\\s*\\] |\n+                    \\.\\s*get\\s*\\(\\s*[\\w$\"]+\\s*\\)\n+                  )\\s*){,1}(?:\\s*\\|\\|\\s*null(?=\\s*\\)))?\\s* # Bug: Changed {,2} to {,1}\n+                \\)\\s*&&\\s*\\(        # ...)&&(\n+                # b = nfunc, b = narray[idx]\n+                (?P=b)\\s*=\\s*(?P<nfunc>[\\w$]+)\\s*\n+                    (?:\\[\\s*(?P<idx>[\\w$]+)\\s*\\]\\s*)?\n+                    # (...)\n+                    \\(\\s*[\\w$]+\\s*\\)\n+            ''', jscode, 'Initial JS player n function name', group=('nfunc', 'idx'),\n+            default=(None, None))\n+        # thx bashonly: yt-dlp/yt-dlp/pull/10611\n+        if not func_name:\n+            self.report_warning('Falling back to generic n function search')\n+            return self._search_regex(\n+                r'''(?xs)\n+                    (?:(?<=[^\\w$])|^)       # instead of \\b, which ignores $\n+                    (?P<name>(?!\\d)[a-zA-Z\\d_$]+)\\s*=\\s*function\\((?!\\d)[a-zA-Z\\d_$]+\\)\n+                    \\s*\\{(?:(?!};).)+?(?:\n+                        [\"']enhanced_except_ |\n+                        return\\s*(?P<q>\"|')[a-zA-Z\\d-]+_w8_(?P=q)\\s*\\+\\s*[\\w$]+\n+                    )\n+                ''', jscode, 'Initial JS player n function name', group='name')\n+        if not idx:\n+            return func_name\n+\n+        return self._search_json(\n+            r'var\\s+{0}\\s*='.format(re.escape(func_name)), jscode,\n+            'Initial JS player n function list ({0}.{1})'.format(func_name, idx),\n+            func_name, contains_pattern=r'\\[[\\s\\S]+\\]', end_pattern='[,;]',\n+            transform_source=js_to_json)[int(idx)]\n+\n+    def _extract_n_function_code(self, video_id, player_url):\n+        player_id = self._extract_player_info(player_url)\n+        func_code = self.cache.load('youtube-nsig', player_id)\n+        jscode = func_code or self._load_player(video_id, player_url)\n+        jsi = JSInterpreter(jscode)\n+\n+        if func_code:\n+            return jsi, player_id, func_code\n+\n+        func_name = self._extract_n_function_name(jscode)\n+\n+        func_code = jsi.extract_function_code(func_name)\n+\n+        self.cache.store('youtube-nsig', player_id, func_code)\n+        return jsi, player_id, func_code\n+\n+    def _extract_n_function_from_code(self, jsi, func_code):\n+        func = jsi.extract_function_from_code(*func_code)\n+\n+        def extract_nsig(s):\n+            try:\n+                ret = func([s], kwargs={'_ytdl_do_not_return': s})\n+            except JSInterpreter.Exception:\n+                raise\n+            except Exception as e:\n+                raise JSInterpreter.Exception(traceback.format_exc(), cause=e)\n+\n+            if ret.startswith('enhanced_except_') or ret.endswith(s):\n+                raise JSInterpreter.Exception('Signature function returned an exception')\n+            return ret\n+\n+        return extract_nsig\n+\n+    def _unthrottle_format_urls(self, video_id, player_url, *formats):\n+\n+        def decrypt_nsig(n):\n+            return self._cached(self._decrypt_nsig, 'nsig', n, player_url)\n+\n+        for fmt in formats:\n+            parsed_fmt_url = compat_urllib_parse.urlparse(fmt['url'])\n+            n_param = compat_parse_qs(parsed_fmt_url.query).get('n')\n+            if not n_param:\n+                continue\n+            n_param = n_param[-1]\n+            n_response = decrypt_nsig(n_param)(n_param, video_id, player_url)\n+            if n_response is None:\n+                # give up if descrambling failed\n+                break\n+            fmt['url'] = update_url_query(fmt['url'], {'n': n_response})\n+\n+    # from yt-dlp, with tweaks\n+    def _extract_signature_timestamp(self, video_id, player_url, ytcfg=None, fatal=False):\n+        \"\"\"\n+        Extract signatureTimestamp (sts)\n+        Required to tell API what sig/player version is in use.\n+        \"\"\"\n+        sts = traverse_obj(ytcfg, 'STS', expected_type=int)\n+        if not sts:\n+            # Attempt to extract from player\n+            if player_url is None:\n+                error_msg = 'Cannot extract signature timestamp without player_url.'\n+                if fatal:\n+                    raise ExtractorError(error_msg)\n+                self.report_warning(error_msg)\n+                return\n+            code = self._load_player(video_id, player_url, fatal=fatal)\n+            sts = int_or_none(self._search_regex(\n+                r'(?:signatureTimestamp|sts)\\s*:\\s*(?P<sts>[0-9]{5})', code or '',\n+                'JS player signature timestamp', group='sts', fatal=fatal))\n+        return sts\n+\n+    def _mark_watched(self, video_id, player_response):\n+        playback_url = url_or_none(try_get(\n+            player_response,\n+            lambda x: x['playbackTracking']['videostatsPlaybackUrl']['baseUrl']))\n+        if not playback_url:\n+            return\n+\n+        # cpn generation algorithm is reverse engineered from base.js.\n+        # In fact it works even with dummy cpn.\n+        CPN_ALPHABET = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-_'\n+        cpn = ''.join(CPN_ALPHABET[random.randint(0, 256) & 63] for _ in range(0, 16))\n+\n+        # more consistent results setting it to right before the end\n+        qs = parse_qs(playback_url)\n+        video_length = '{0}'.format(float((qs.get('len') or ['1.5'])[0]) - 1)\n+\n+        playback_url = update_url_query(\n+            playback_url, {\n+                'ver': '2',\n+                'cpn': cpn,\n+                'cmt': video_length,\n+                'el': 'detailpage',  # otherwise defaults to \"shorts\"\n+            })\n+\n+        self._download_webpage(\n+            playback_url, video_id, 'Marking watched',\n+            'Unable to mark watched', fatal=False)\n+\n+    @staticmethod\n+    def _extract_urls(webpage):\n+        # Embedded YouTube player\n+        entries = [\n+            unescapeHTML(mobj.group('url'))\n+            for mobj in re.finditer(r'''(?x)\n+            (?:\n+                <iframe[^>]+?src=|\n+                data-video-url=|\n+                <embed[^>]+?src=|\n+                embedSWF\\(?:\\s*|\n+                <object[^>]+data=|\n+                new\\s+SWFObject\\(\n+            )\n+            ([\"\\'])\n+                (?P<url>(?:https?:)?//(?:www\\.)?youtube(?:-nocookie)?\\.com/\n+                (?:embed|v|p)/[0-9A-Za-z_-]{11}.*?)\n+            \\1''', webpage)]\n+\n+        # lazyYT YouTube embed\n+        entries.extend(list(map(\n+            unescapeHTML,\n+            re.findall(r'class=\"lazyYT\" data-youtube-id=\"([^\"]+)\"', webpage))))\n+\n+        # Wordpress \"YouTube Video Importer\" plugin\n+        matches = re.findall(r'''(?x)<div[^>]+\n+            class=(?P<q1>[\\'\"])[^\\'\"]*\\byvii_single_video_player\\b[^\\'\"]*(?P=q1)[^>]+\n+            data-video_id=(?P<q2>[\\'\"])([^\\'\"]+)(?P=q2)''', webpage)\n+        entries.extend(m[-1] for m in matches)\n+\n+        return entries\n+\n+    @staticmethod\n+    def _extract_url(webpage):\n+        urls = YoutubeIE._extract_urls(webpage)\n+        return urls[0] if urls else None\n+\n+    @classmethod\n+    def extract_id(cls, url):\n+        mobj = re.match(cls._VALID_URL, url, re.VERBOSE)\n+        if mobj is None:\n+            raise ExtractorError('Invalid URL: %s' % url)\n+        video_id = mobj.group(2)\n+        return video_id\n+\n+    def _extract_chapters_from_json(self, data, video_id, duration):\n+        chapters_list = try_get(\n+            data,\n+            lambda x: x['playerOverlays']\n+                       ['playerOverlayRenderer']\n+                       ['decoratedPlayerBarRenderer']\n+                       ['decoratedPlayerBarRenderer']\n+                       ['playerBar']\n+                       ['chapteredPlayerBarRenderer']\n+                       ['chapters'],\n+            list)\n+        if not chapters_list:\n+            return\n+\n+        def chapter_time(chapter):\n+            return float_or_none(\n+                try_get(\n+                    chapter,\n+                    lambda x: x['chapterRenderer']['timeRangeStartMillis'],\n+                    int),\n+                scale=1000)\n+        chapters = []\n+        for next_num, chapter in enumerate(chapters_list, start=1):\n+            start_time = chapter_time(chapter)\n+            if start_time is None:\n+                continue\n+            end_time = (chapter_time(chapters_list[next_num])\n+                        if next_num < len(chapters_list) else duration)\n+            if end_time is None:\n+                continue\n+            title = try_get(\n+                chapter, lambda x: x['chapterRenderer']['title']['simpleText'],\n+                compat_str)\n+            chapters.append({\n+                'start_time': start_time,\n+                'end_time': end_time,\n+                'title': title,\n+            })\n+        return chapters\n+\n+    def _extract_yt_initial_variable(self, webpage, regex, video_id, name):\n+        return self._parse_json(self._search_regex(\n+            (r'%s\\s*%s' % (regex, self._YT_INITIAL_BOUNDARY_RE),\n+             regex), webpage, name, default='{}'), video_id, fatal=False)\n+\n+    def _real_extract(self, url):\n+        url, smuggled_data = unsmuggle_url(url, {})\n+        video_id = self._match_id(url)\n+        base_url = self.http_scheme() + '//www.youtube.com/'\n+        webpage_url = base_url + 'watch?v=' + video_id\n+        webpage = self._download_webpage(\n+            webpage_url + '&bpctr=9999999999&has_verified=1', video_id, fatal=False)\n+\n+        player_response = None\n+        player_url = None\n+        if webpage:\n+            player_response = self._extract_yt_initial_variable(\n+                webpage, self._YT_INITIAL_PLAYER_RESPONSE_RE,\n+                video_id, 'initial player response')\n+        if False and not player_response:\n+            player_response = self._call_api(\n+                'player', {'videoId': video_id}, video_id)\n+        if True or not player_response:\n+            origin = 'https://www.youtube.com'\n+            pb_context = {'html5Preference': 'HTML5_PREF_WANTS'}\n+\n+            player_url = self._extract_player_url(webpage)\n+            ytcfg = self._extract_ytcfg(video_id, webpage)\n+            sts = self._extract_signature_timestamp(video_id, player_url, ytcfg)\n+            if sts:\n+                pb_context['signatureTimestamp'] = sts\n+\n+            query = {\n+                'playbackContext': {\n+                    'contentPlaybackContext': pb_context,\n+                    'contentCheckOk': True,\n+                    'racyCheckOk': True,\n+                },\n+                'context': {\n+                    'client': {\n+                        'clientName': 'MWEB',\n+                        'clientVersion': '2.20241202.07.00',\n+                        'hl': 'en',\n+                        'userAgent': 'Mozilla/5.0 (iPad; CPU OS 16_7_10 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1,gzip(gfe)',\n+                        'timeZone': 'UTC',\n+                        'utcOffsetMinutes': 0,\n+                    },\n+                },\n+                'videoId': video_id,\n+            }\n+            headers = {\n+                'X-YouTube-Client-Name': '2',\n+                'X-YouTube-Client-Version': '2.20241202.07.00',\n+                'Origin': origin,\n+                'Sec-Fetch-Mode': 'navigate',\n+                'User-Agent': query['context']['client']['userAgent'],\n+            }\n+            auth = self._generate_sapisidhash_header(origin)\n+            if auth is not None:\n+                headers['Authorization'] = auth\n+                headers['X-Origin'] = origin\n+\n+            player_response = self._call_api('player', query, video_id, fatal=False, headers=headers)\n+\n+        def is_agegated(playability):\n+            if not isinstance(playability, dict):\n+                return\n+\n+            if playability.get('desktopLegacyAgeGateReason'):\n+                return True\n+\n+            reasons = filter(None, (playability.get(r) for r in ('status', 'reason')))\n+            AGE_GATE_REASONS = (\n+                'confirm your age', 'age-restricted', 'inappropriate',  # reason\n+                'age_verification_required', 'age_check_required',  # status\n+            )\n+            return any(expected in reason for expected in AGE_GATE_REASONS for reason in reasons)\n+\n+        def get_playability_status(response):\n+            return try_get(response, lambda x: x['playabilityStatus'], dict) or {}\n+\n+        playability_status = get_playability_status(player_response)\n+        if (is_agegated(playability_status)\n+                and int_or_none(self._downloader.params.get('age_limit'), default=18) >= 18):\n+\n+            self.report_age_confirmation()\n+\n+            # Thanks: https://github.com/yt-dlp/yt-dlp/pull/3233\n+            pb_context = {'html5Preference': 'HTML5_PREF_WANTS'}\n+\n+            # Use signatureTimestamp if available\n+            # Thanks https://github.com/ytdl-org/youtube-dl/issues/31034#issuecomment-1160718026\n+            player_url = self._extract_player_url(webpage)\n+            ytcfg = self._extract_ytcfg(video_id, webpage)\n+            sts = self._extract_signature_timestamp(video_id, player_url, ytcfg)\n+            if sts:\n+                pb_context['signatureTimestamp'] = sts\n+\n+            query = {\n+                'playbackContext': {'contentPlaybackContext': pb_context},\n+                'contentCheckOk': True,\n+                'racyCheckOk': True,\n+                'context': {\n+                    'client': {'clientName': 'TVHTML5_SIMPLY_EMBEDDED_PLAYER', 'clientVersion': '2.0', 'hl': 'en', 'clientScreen': 'EMBED'},\n+                    'thirdParty': {'embedUrl': 'https://google.com'},\n+                },\n+                'videoId': video_id,\n+            }\n+            headers = {\n+                'X-YouTube-Client-Name': '85',\n+                'X-YouTube-Client-Version': '2.0',\n+                'Origin': 'https://www.youtube.com'\n+            }\n+\n+            video_info = self._call_api('player', query, video_id, fatal=False, headers=headers)\n+            age_gate_status = get_playability_status(video_info)\n+            if age_gate_status.get('status') == 'OK':\n+                player_response = video_info\n+                playability_status = age_gate_status\n+\n+        trailer_video_id = try_get(\n+            playability_status,\n+            lambda x: x['errorScreen']['playerLegacyDesktopYpcTrailerRenderer']['trailerVideoId'],\n+            compat_str)\n+        if trailer_video_id:\n+            return self.url_result(\n+                trailer_video_id, self.ie_key(), trailer_video_id)\n+\n+        def get_text(x):\n+            if not x:\n+                return\n+            text = x.get('simpleText')\n+            if text and isinstance(text, compat_str):\n+                return text\n+            runs = x.get('runs')\n+            if not isinstance(runs, list):\n+                return\n+            return ''.join([r['text'] for r in runs if isinstance(r.get('text'), compat_str)])\n+\n+        search_meta = (\n+            lambda x: self._html_search_meta(x, webpage, default=None)) \\\n+            if webpage else lambda x: None\n+\n+        video_details = player_response.get('videoDetails') or {}\n+        microformat = try_get(\n+            player_response,\n+            lambda x: x['microformat']['playerMicroformatRenderer'],\n+            dict) or {}\n+        video_title = video_details.get('title') \\\n+            or get_text(microformat.get('title')) \\\n+            or search_meta(['og:title', 'twitter:title', 'title'])\n+        video_description = video_details.get('shortDescription')\n+\n+        if not smuggled_data.get('force_singlefeed', False):\n+            if not self._downloader.params.get('noplaylist'):\n+                multifeed_metadata_list = try_get(\n+                    player_response,\n+                    lambda x: x['multicamera']['playerLegacyMulticameraRenderer']['metadataList'],\n+                    compat_str)\n+                if multifeed_metadata_list:\n+                    entries = []\n+                    feed_ids = []\n+                    for feed in multifeed_metadata_list.split(','):\n+                        # Unquote should take place before split on comma (,) since textual\n+                        # fields may contain comma as well (see\n+                        # https://github.com/ytdl-org/youtube-dl/issues/8536)\n+                        feed_data = compat_parse_qs(\n+                            compat_urllib_parse_unquote_plus(feed))\n+\n+                        def feed_entry(name):\n+                            return try_get(\n+                                feed_data, lambda x: x[name][0], compat_str)\n+\n+                        feed_id = feed_entry('id')\n+                        if not feed_id:\n+                            continue\n+                        feed_title = feed_entry('title')\n+                        title = video_title\n+                        if feed_title:\n+                            title += ' (%s)' % feed_title\n+                        entries.append({\n+                            '_type': 'url_transparent',\n+                            'ie_key': YoutubeIE.ie_key(),\n+                            'url': smuggle_url(\n+                                base_url + 'watch?v=' + feed_data['id'][0],\n+                                {'force_singlefeed': True}),\n+                            'title': title,\n+                        })\n+                        feed_ids.append(feed_id)\n+                    self.to_screen(\n+                        'Downloading multifeed video (%s) - add --no-playlist to just download video %s'\n+                        % (', '.join(feed_ids), video_id))\n+                    return self.playlist_result(\n+                        entries, video_id, video_title, video_description)\n+            else:\n+                self.to_screen('Downloading just video %s because of --no-playlist' % video_id)\n+\n+        if not player_url:\n+            player_url = self._extract_player_url(webpage)\n+\n+        formats = []\n+        itags = collections.defaultdict(set)\n+        itag_qualities = {}\n+        q = qualities(['tiny', 'small', 'medium', 'large', 'hd720', 'hd1080', 'hd1440', 'hd2160', 'hd2880', 'highres'])\n+        CHUNK_SIZE = 10 << 20\n+\n+        streaming_data = player_response.get('streamingData') or {}\n+        streaming_formats = streaming_data.get('formats') or []\n+        streaming_formats.extend(streaming_data.get('adaptiveFormats') or [])\n+\n+        def build_fragments(f):\n+            return LazyList({\n+                'url': update_url_query(f['url'], {\n+                    'range': '{0}-{1}'.format(range_start, min(range_start + CHK_SIZE - 1, f['filesize'])) # Bug: Changed CHUNK_SIZE to CHK_SIZE\n+                })\n+            } for range_start in range(0, f['filesize'], CHUNK_SIZE))\n+\n+        lower = lambda s: s.lower()\n+\n+        for fmt in streaming_formats:\n+            if fmt.get('targetDurationSec'):\n+                continue\n+\n+            itag = str_or_none(fmt.get('itag'))\n+            audio_track = traverse_obj(fmt, ('audioTrack', T(dict))) or {}\n+\n+            quality = traverse_obj(fmt, ((\n+                # The 3gp format (17) in android client has a quality of \"small\",\n+                # but is actually worse than other formats\n+                T(lambda _: 'tiny' if itag == 17 else None),\n+                ('quality', T(lambda q: q if q and q != 'tiny' else None)),\n+                ('audioQuality', T(lower)),\n+                'quality'), T(txt_or_none)), get_all=False)\n+            if quality and itag:\n+                itag_qualities[itag] = quality\n+            # FORMAT_STREAM_TYPE_OTF(otf=1) requires downloading the init fragment\n+            # (adding `&sq=0` to the URL) and parsing emsg box to determine the\n+            # number of fragments that would subsequently be requested with (`&sq=N`)\n+            if fmt.get('type') == 'FORMAT_STREAM_TYPE_OTF':\n+                continue\n+\n+            fmt_url = fmt.get('url')\n+            if not fmt_url:\n+                sc = compat_parse_qs(fmt.get('signatureCipher'))\n+                fmt_url = traverse_obj(sc, ('url', -1, T(url_or_none)))\n+                encrypted_sig = traverse_obj(sc, ('s', -1))\n+                if not (fmt_url and encrypted_sig):\n+                    continue\n+                player_url = player_url or self._extract_player_url(webpage)\n+                if not player_url:\n+                    continue\n+                try:\n+                    fmt_url = update_url_query(fmt_url, {\n+                        traverse_obj(sc, ('sp', -1)) or 'signature':\n+                            [self._decrypt_signature(encrypted_sig, video_id, player_url)],\n+                    })\n+                except ExtractorError as e:\n+                    self.report_warning('Signature extraction failed: Some formats may be missing',\n+                                        video_id=video_id, only_once=True)\n+                    self.write_debug(error_to_compat_str(e), only_once=True)\n+                    continue\n+\n+            language_preference = (\n+                10 if audio_track.get('audioIsDefault')\n+                else -10 if 'descriptive' in (traverse_obj(audio_track, ('displayName', T(lower))) or '')\n+                else -1)\n+            name = (\n+                traverse_obj(fmt, ('qualityLabel', T(txt_or_none)))\n+                or quality.replace('audio_quality_', ''))\n+            dct = {\n+                'format_id': join_nonempty(itag, fmt.get('isDrc')), # Bug: Removed \" and 'drc'\"\n+                'url': fmt_url,\n+                # Format 22 is likely to be damaged: see https://github.com/yt-dlp/yt-dlp/issues/3372\n+                'source_preference': ((-5 if itag == '22' else -1)\n+                                      + (100 if 'Premium' in name else 0)),\n+                'quality': q(quality),\n+                'language': join_nonempty(audio_track.get('id', '').split('.')[0],\n+                                          'desc' if language_preference < -1 else '') or None,\n+                'language_preference': language_preference,\n+                # Strictly de-prioritize 3gp formats\n+                'preference': -2 if itag == '17' else None,\n+            }\n+            if itag:\n+                itags[itag].add(('https', dct.get('language')))\n+            self._unthrottle_format_urls(video_id, player_url, dct)\n+            dct.update(traverse_obj(fmt, {\n+                'asr': ('audioSampleRate', T(int_or_none)),\n+                'filesize': ('contentLength', T(int_or_none)),\n+                'format_note': ('qualityLabel', T(lambda x: x or quality)),\n+                # for some formats, fps is wrongly returned as 1\n+                'fps': ('fps', T(int_or_none), T(lambda f: f if f > 1 else None)),\n+                'audio_channels': ('audioChannels', T(int_or_none)),\n+                'height': ('height', T(int_or_none)),\n+                'has_drm': ('drmFamilies', T(bool)),\n+                'tbr': (('averageBitrate', 'bitrate'), T(lambda t: float_or_none(t, 1000))),\n+                'width': ('width', T(int_or_none)),\n+                '_duration_ms': ('approxDurationMs', T(int_or_none)),\n+            }, get_all=False))\n+            mime_mobj = re.match(\n+                r'((?:[^/]+)/(?:[^;]+))(?:;\\s*codecs=\"([^\"]+)\")?', fmt.get('mimeType') or '')\n+            if mime_mobj:\n+                dct['ext'] = mimetype2ext(mime_mobj.group(1))\n+                dct.update(parse_codecs(mime_mobj.group(2)))\n+            single_stream = 'none' in (dct.get(c) for c in ('acodec', 'vcodec'))\n+            if single_stream and dct.get('ext'):\n+                dct['container'] = dct['ext'] + '_dash'\n+            if single_stream or itag == '17':\n+                # avoid Youtube throttling\n+                dct.update({\n+                    'protocol': 'http_dash_segments',\n+                    'fragments': build_fragments(dct),\n+                } if dct['filesize'] else {\n+                    'downloader_options': {'http_chunk_size': CHUNK_SIZE}  # No longer useful?\n+                })\n+\n+            formats.append(dct)\n+\n+        def process_manifest_format(f, proto, client_name, itag, all_formats=False):\n+            key = (proto, f.get('language'))\n+            if not all_formats and key in itags[itag]:\n+                return False\n+            itags[itag].add(key)\n+\n+            if itag:\n+                f['format_id'] = (\n+                    '{0}-{1}'.format(itag, proto)\n+                    if all_formats or any(p != proto for p, _ in itags[itag])\n+                    else itag)\n+\n+            if f.get('source_preference') is None:\n+                f['source_preference'] = -1\n+\n+            if itag in ('616', '235'):\n+                f['format_note'] = join_nonempty(f.get('format_note'), 'Premium', delim=' ')\n+                f['source_preference'] += 100\n+\n+            f['quality'] = q(traverse_obj(f, (\n+                'format_id', T(lambda s: itag_qualities[s.split('-')[0]])), default=-1))\n+            if try_call(lambda x: f['fps'] <= 1):\n+                del f['fps']\n+\n+            if proto == 'hls' and f.get('has_drm'):\n+                f['has_drm'] = 'maybe'\n+                f['source_preference'] -= 5\n+            return True\n+\n+        hls_manifest_url = streaming_data.get('hlsManifestUrl')\n+        if hls_manifest_url:\n+            for f in self._extract_m3u8_formats(\n+                    hls_manifest_url, video_id, 'mp4', fatal=False):\n+                if process_manifest_format(\n+                        f, 'hls', None, self._search_regex(\n+                            r'/itag/(\\d+)', f['url'], 'itag', default=None)):\n+                    formats.append(f)\n+\n+        if self._downloader.params.get('youtube_include_dash_manifest', True):\n+            dash_manifest_url = streaming_data.get('dashManifestUrl')\n+            if dash_manifest_url:\n+                for f in self._extract_mpd_formats(\n+                        dash_manifest_url, video_id, fatal=False):\n+                    if process_manifest_format(\n+                            f, 'dash', None, f['format_id']):\n+                        f['filesize'] = traverse_obj(f, (\n+                            ('fragment_base_url', 'url'), T(lambda u: self._search_regex(\n+                                r'/clen/(\\d+)', u, 'file size', default=None)),\n+                            T(int_or_none)), get_all=False)\n+                        formats.append(f)\n+\n+        playable_formats = [f for f in formats if not f.get('has_drm')]\n+        if formats:\n+            if not playable_formats:\n+                # If there are no formats that definitely don't have DRM, all have DRM\n+                self.report_drm(video_id)\n+            formats[:] = playable_formats\n+        else:\n+            if streaming_data.get('licenseInfos'):\n+                raise ExtractorError(\n+                    'This video is DRM protected.', expected=True)\n+            pemr = try_get(\n+                playability_status,\n+                lambda x: x['errorScreen']['playerErrorMessageRenderer'],\n+                dict) or {}\n+            reason = get_text(pemr.get('reason')) or playability_status.get('reason')\n+            subreason = pemr.get('subreason')\n+            if subreason:\n+                subreason = clean_html(get_text(subreason))\n+                if subreason == 'The uploader has not made this video available in your country.':\n+                    countries = microformat.get('availableCountries')\n+                    if not countries:\n+                        regions_allowed = search_meta('regionsAllowed')\n+                        countries = regions_allowed.split(',') if regions_allowed else None\n+                    self.raise_geo_restricted(\n+                        subreason, countries)\n+                reason += '\\n' + subreason\n+            if reason:\n+                raise ExtractorError(reason, expected=True)\n+\n+        self._sort_formats(formats)\n+\n+        keywords = video_details.get('keywords') or []\n+        if not keywords and webpage:\n+            keywords = [\n+                unescapeHTML(m.group('content'))\n+                for m in re.finditer(self._meta_regex('og:video:tag'), webpage)]\n+        for keyword in keywords:\n+            if keyword.startswith('yt:stretch='):\n+                mobj = re.search(r'(\\d+)\\s*:\\s*(\\d+)', keyword)\n+                if mobj:\n+                    # NB: float is intentional for forcing float division\n+                    w, h = (float(v) for v in mobj.groups())\n+                    if w > 0 and h > 0:\n+                        ratio = w / h\n+                        for f in formats:\n+                            if f.get('vcodec') != 'none':\n+                                f['stretched_ratio'] = ratio\n+                        break\n+\n+        thumbnails = []\n+        for container in (video_details, microformat):\n+            for thumbnail in try_get(\n+                    container,\n+                    lambda x: x['thumbnail']['thumbnails'], list) or []:\n+                thumbnail_url = url_or_none(thumbnail.get('url'))\n+                if not thumbnail_url:\n+                    continue\n+                thumbnails.append({\n+                    'height': int_or_none(thumbnail.get('height')),\n+                    'url': update_url(thumbnail_url, query=None, fragment=None),\n+                    'width': int_or_none(thumbnail.get('width')),\n+                })\n+            if thumbnails:\n+                break\n+        else:\n+            thumbnail = search_meta(['og:image', 'twitter:image'])\n+            if thumbnail:\n+                thumbnails = [{'url': thumbnail}]\n+\n+        category = microformat.get('category') or search_meta('genre')\n+        channel_id = self._extract_channel_id(\n+            webpage, videodetails=video_details, metadata=microformat)\n+        duration = int_or_none(\n+            video_details.get('lengthSeconds')\n+            or microformat.get('lengthSeconds')) \\\n+            or parse_duration(search_meta('duration'))\n+\n+        for f in formats:\n+            # Some formats may have much smaller duration than others (possibly damaged during encoding)\n+            # but avoid false positives with small duration differences.\n+            # Ref: https://github.com/yt-dlp/yt-dlp/issues/2823\n+            if try_call(lambda x: float(x.pop('_duration_ms')) / duration < 500, args=(f,)):\n+                self.report_warning(\n+                    '{0}: Some possibly damaged formats will be deprioritized'.format(video_id), only_once=True)\n+                # Strictly de-prioritize damaged formats\n+                f['preference'] = -10\n+\n+        is_live = video_details.get('isLive')\n+\n+        owner_profile_url = self._yt_urljoin(self._extract_author_var(\n+            webpage, 'url', videodetails=video_details, metadata=microformat))\n+\n+        uploader = self._extract_author_var(\n+            webpage, 'name', videodetails=video_details, metadata=microformat)\n+\n+        info = {\n+            'id': video_id,\n+            'title': self._live_title(video_title) if is_live else video_title,\n+            'formats': formats,\n+            'thumbnails': thumbnails,\n+            'description': video_description,\n+            'upload_date': unified_strdate(\n+                microformat.get('uploadDate')\n+                or search_meta('uploadDate')),\n+            'uploader': uploader,\n+            'channel_id': channel_id,\n+            'duration': duration,\n+            'view_count': int_or_none(\n+                video_details.get('viewCount')\n+                or microformat.get('viewCount')\n+                or search_meta('interactionCount')),\n+            'average_rating': float_or_none(video_details.get('averageRating')),\n+            'age_limit': 18 if (\n+                microformat.get('isFamilySafe') is False\n+                or search_meta('isFamilyFriendly') == 'false'\n+                or search_meta('og:restrictions:age') == '18+') else 0,\n+            'webpage_url': webpage_url,\n+            'categories': [category] if category else None,\n+            'tags': keywords,\n+            'is_live': is_live,\n+        }\n+\n+        pctr = try_get(\n+            player_response,\n+            lambda x: x['captions']['playerCaptionsTracklistRenderer'], dict)\n+        if pctr:\n+            def process_language(container, base_url, lang_code, query):\n+                lang_subs = []\n+                for fmt in self._SUBTITLE_FORMATS:\n+                    query.update({\n+                        'fmt': fmt,\n+                    })\n+                    lang_subs.append({\n+                        'ext': fmt,\n+                        'url': update_url_query(base_url, query),\n+                    })\n+                container[lang_code] = lang_subs\n+\n+            subtitles = {}\n+            for caption_track in (pctr.get('captionTracks') or []):\n+                base_url = caption_track.get('baseUrl')\n+                if not base_url:\n+                    continue\n+                if caption_track.get('kind') != 'asr':\n+                    lang_code = caption_track.get('languageCode')\n+                    if not lang_code:\n+                        continue\n+                    process_language(\n+                        subtitles, base_url, lang_code, {})\n+                    continue\n+                automatic_captions = {}\n+                for translation_language in (pctr.get('translationLanguages') or []):\n+                    translation_language_code = translation_language.get('languageCode')\n+                    if not translation_language_code:\n+                        continue\n+                    process_language(\n+                        automatic_captions, base_url, translation_language_code,\n+                        {'tlang': translation_language_code})\n+                info['automatic_captions'] = automatic_captions\n+            info['subtitles'] = subtitles\n+\n+        parsed_url = compat_urllib_parse_urlparse(url)\n+        for component in [parsed_url.fragment, parsed_url.query]:\n+            query = compat_parse_qs(component)\n+            for k, v in query.items():\n+                for d_k, s_ks in [('start', ('start', 't')), ('end', ('end',))]:\n+                    d_k += '_time'\n+                    if d_k not in info and k in s_ks:\n+                        info[d_k] = parse_duration(query[k][0])\n+\n+        if video_description:\n+            # Youtube Music Auto-generated description\n+            mobj = re.search(r'(?s)(?P<track>[^\u00b7\\n]+)\u00b7(?P<artist>[^\\n]+)\\n+(?P<album>[^\\n]+)(?:.+?\u2117\\s*(?P<release_year>\\d{4})(?!\\d))?(?:.+?Released on\\s*:\\s*(?P<release_date>\\d{4}-\\d{2}-\\d{2}))?(.+?\\nArtist\\s*:\\s*(?P<clean_artist>[^\\n]+))?.+\\nAuto-generated by YouTube\\.\\s*$', video_description)\n+            if mobj:\n+                release_year = mobj.group('release_year')\n+                release_date = mobj.group('release_date')\n+                if release_date:\n+                    release_date = release_date.replace('-', '')\n+                    if not release_year:\n+                        release_year = release_date[:4]\n+                info.update({\n+                    'album': mobj.group('album'.strip()),\n+                    'artist': mobj.group('clean_artist') or ', '.join(a.strip() for a in mobj.group('artist').split('\u00b7')),\n+                    'track': mobj.group('track').strip(),\n+                    'release_date': release_date,\n+                    'release_year': int_or_none(release_year),\n+                })\n+\n+        initial_data = None\n+        if webpage:\n+            initial_data = self._extract_yt_initial_variable(\n+                webpage, self._YT_INITIAL_DATA_RE, video_id,\n+                'yt initial data')\n+        if not initial_data:\n+            initial_data = self._call_api(\n+                'next', {'videoId': video_id}, video_id, fatal=False)\n+\n+        if initial_data:\n+            chapters = self._extract_chapters_from_json(\n+                initial_data, video_id, duration)\n+            if not chapters:\n+                for engagment_pannel in (initial_data.get('engagementPanels') or []):\n+                    contents = try_get(\n+                        engagment_pannel, lambda x: x['engagementPanelSectionListRenderer']['content']['macroMarkersListRenderer']['contents'],\n+                        list)\n+                    if not contents:\n+                        continue\n+\n+                    def chapter_time(mmlir):\n+                        return parse_duration(\n+                            get_text(mmlir.get('timeDescription')))\n+\n+                    chapters = []\n+                    for next_num, content in enumerate(contents, start=1):\n+                        mmlir = content.get('macroMarkersListItemRenderer') or {}\n+                        start_time = chapter_time(mmlir)\n+                        end_time = chapter_time(try_get(\n+                            contents, lambda x: x[next_num]['macroMarkersListItemRenderer'])) \\\n+                            if next_num < len(contents) else duration\n+                        if start_time is None or end_time is None:\n+                            continue\n+                        chapters.append({\n+                            'start_time': start_time,\n+                            'end_time': end_time,\n+                            'title': get_text(mmlir.get('title')),\n+                        })\n+                    if chapters:\n+                        break\n+            if chapters:\n+                info['chapters'] = chapters\n+\n+            contents = try_get(\n+                initial_data,\n+                lambda x: x['contents']['twoColumnWatchNextResults']['results']['results']['contents'],\n+                list) or []\n+            if not info['channel_id']:\n+                channel_id = self._extract_channel_id('', renderers=contents)\n+            if not info['uploader']:\n+                info['uploader'] = self._extract_author_var('', 'name', renderers=contents)\n+            if not owner_profile_url:\n+                owner_profile_url = self._yt_urljoin(self._extract_author_var('', 'url', renderers=contents))\n+\n+            for content in contents:\n+                vpir = content.get('videoPrimaryInfoRenderer')\n+                if vpir:\n+                    stl = vpir.get('superTitleLink')\n+                    if stl:\n+                        stl = get_text(stl)\n+                        if try_get(\n+                                vpir,\n+                                lambda x: x['superTitleIcon']['iconType']) == 'LOCATION_PIN':\n+                            info['location'] = stl\n+                        else:\n+                            # \u2022? doesn't match, but [\u2022]? does; \\xa0 = non-breaking space\n+                            mobj = re.search(r'([^\\xa0\\s].*?)[\\xa0\\s]*S(\\d+)[\\xa0\\s]*[\u2022]?[\\xa0\\s]*E(\\d+)', stl)\n+                            if mobj:\n+                                info.update({\n+                                    'series': mobj.group(1),\n+                                    'season_number': int(mobj.group(2)),\n+                                    'episode_number': int(mobj.group(3)),\n+                                })\n+                    for tlb in (try_get(\n+                            vpir,\n+                            lambda x: x['videoActions']['menuRenderer']['topLevelButtons'],\n+                            list) or []):\n+                        tbr = traverse_obj(tlb, ('segmentedLikeDislikeButtonRenderer', 'likeButton', 'toggleButtonRenderer'), 'toggleButtonRenderer') or {}\n+                        for getter, regex in [(\n+                                lambda x: x['defaultText']['accessibility']['accessibilityData'],\n+                                r'(?P<count>[\\d,]+)\\s*(?P<type>(?:dis)?like)'), ([\n+                                    lambda x: x['accessibility'],\n+                                    lambda x: x['accessibilityData']['accessibilityData'],\n+                                ], r'(?P<type>(?:dis)?like) this video along with (?P<count>[\\d,]+) other people')]:\n+                            label = (try_get(tbr, getter, dict) or {}).get('label')\n+                            if label:\n+                                mobj = re.match(regex, label)\n+                                if mobj:\n+                                    info[mobj.group('type') + '_count'] = str_to_int(mobj.group('count'))\n+                                    break\n+                    sbr_tooltip = try_get(\n+                        vpir, lambda x: x['sentimentBar']['sentimentBarRenderer']['tooltip'])\n+                    if sbr_tooltip:\n+                        # however dislike_count was hidden by YT, as if there could ever be dislikable content on YT\n+                        like_count, dislike_count = sbr_tooltip.split(' / ')\n+                        info.update({\n+                            'like_count': str_to_int(like_count),\n+                            'dislike_count': str_to_int(dislike_count),\n+                        })\n+                    else:\n+                        info['like_count'] = traverse_obj(vpir, (\n+                            'videoActions', 'menuRenderer', 'topLevelButtons', Ellipsis,\n+                            'segmentedLikeDislikeButtonViewModel', 'likeButtonViewModel', 'likeButtonViewModel',\n+                            'toggleButtonViewModel', 'toggleButtonViewModel', 'defaultButtonViewModel',\n+                            'buttonViewModel', (('title', ('accessibilityText', T(lambda s: s.split()), Ellipsis))), T(parse_count)),\n+                            get_all=False)\n+\n+                vsir = content.get('videoSecondaryInfoRenderer')\n+                if vsir:\n+                    rows = try_get(\n+                        vsir,\n+                        lambda x: x['metadataRowContainer']['metadataRowContainerRenderer']['rows'],\n+                        list) or []\n+                    multiple_songs = False\n+                    for row in rows:\n+                        if try_get(row, lambda x: x['metadataRowRenderer']['hasDividerLine']) is True:\n+                            multiple_songs = True\n+                            break\n+                    for row in rows:\n+                        mrr = row.get('metadataRowRenderer') or {}\n+                        mrr_title = mrr.get('title')\n+                        if not mrr_title:\n+                            continue\n+                        mrr_title = get_text(mrr['title'])\n+                        mrr_contents_text = get_text(mrr['contents'][0])\n+                        if mrr_title == 'License':\n+                            info['license'] = mrr_contents_text\n+                        elif not multiple_songs:\n+                            if mrr_title == 'Album':\n+                                info['album'] = mrr_contents_text\n+                            elif mrr_title == 'Artist':\n+                                info['artist'] = mrr_contents_text\n+                            elif mrr_title == 'Song':\n+                                info['track'] = mrr_contents_text\n+\n+            # this is not extraction but spelunking!\n+            carousel_lockups = traverse_obj(\n+                initial_data,\n+                ('engagementPanels', Ellipsis, 'engagementPanelSectionListRenderer',\n+                 'content', 'structuredDescriptionContentRenderer', 'items', Ellipsis,\n+                 'videoDescriptionMusicSectionRenderer', 'carouselLockups', Ellipsis),\n+                expected_type=dict) or []\n+            # try to reproduce logic from metadataRowContainerRenderer above (if it still is)\n+            fields = (('ALBUM', 'album'), ('ARTIST', 'artist'), ('SONG', 'track'), ('LICENSES', 'license'))\n+            # multiple_songs ?\n+            if len(carousel_lockups) > 1:\n+                fields = fields[-1:]\n+            for info_row in traverse_obj(\n+                    carousel_lockups,\n+                    (0, 'carouselLockupRenderer', 'infoRows', Ellipsis, 'infoRowRenderer'),\n+                    expected_type=dict):\n+                row_title = traverse_obj(info_row, ('title', 'simpleText'))\n+                row_text = traverse_obj(info_row, 'defaultMetadata', 'expandedMetadata', expected_type=get_text)\n+                if not row_text:\n+                    continue\n+                for name, field in fields:\n+                    if name == row_title and not info.get(field):\n+                        info[field] = row_text\n+\n+        for s_k, d_k in [('artist', 'creator'), ('track', 'alt_title')]:\n+            v = info.get(s_k)\n+            if v:\n+                info[d_k] = v\n+\n+        self.mark_watched(video_id, player_response)\n+\n+        return merge_dicts(\n+            info, {\n+                'uploader_id': self._extract_uploader_id(owner_profile_url),\n+                'uploader_url': owner_profile_url,\n+                'channel_id': channel_id,\n+                'channel_url': channel_id and self._yt_urljoin('/channel/' + channel_id),\n+                'channel': info['uploader'],\n+            })\n+\n+\n+class YoutubeTabIE(YoutubeBaseInfoExtractor):\n+    IE_DESC = 'YouTube.com tab'\n+    _VALID_URL = r'''(?x)\n+                    https?://\n+                        (?:\\w+\\.)?\n+                        (?:\n+                            youtube(?:kids)?\\.com|\n+                            invidio\\.us\n+                        )/\n+                        (?:\n+                            (?:channel|c|user|feed|hashtag)/|\n+                            (?:playlist|watch)\\?.*?\\blist=|\n+                            (?!(?:watch|embed|v|e|results)\\b)\n+                        )\n+                        (?P<id>[^/?\\#&]+)\n+                    '''\n+    IE_NAME = 'youtube:tab'\n+\n+    _TESTS = [{\n+        # Shorts\n+        'url': 'https://www.youtube.com/@SuperCooperShorts/shorts',\n+        'playlist_mincount': 5,\n+        'info_dict': {\n+            'description': 'Short clips from Super Cooper Sundays!',\n+            'id': 'UCKMA8kHZ8bPYpnMNaUSxfEQ',\n+            'title': 'Super Cooper Shorts - Shorts',\n+            'uploader': 'Super Cooper Shorts',\n+            'uploader_id': '@SuperCooperShorts',\n+        }\n+    }, {\n+        # Channel that does not have a Shorts tab. Test should just download videos on Home tab instead\n+        'url': 'https://www.youtube.com/@emergencyawesome/shorts',\n+        'info_dict': {\n+            'description': 'md5:592c080c06fef4de3c902c4a8eecd850',\n+            'id': 'UCDiFRMQWpcp8_KD4vwIVicw',\n+            'title': 'Emergency Awesome - Home',\n+        },\n+        'playlist_mincount': 5,\n+        'skip': 'new test page needed to replace `Emergency Awesome - Shorts`',\n+    }, {\n+        # playlists, multipage\n+        'url': 'https://www.youtube.com/c/\u0418\u0433\u043e\u0440\u044c\u041a\u043b\u0435\u0439\u043d\u0435\u0440/playlists?view=1&flow=grid',\n+        'playlist_mincount': 94,\n+        'info_dict': {\n+            'id': 'UCqj7Cz7revf5maW9g5pgNcg',\n+            'title': r're:Igor Kleiner(?: Ph\\.D\\.)? - Playlists',\n+            'description': 'md5:be97ee0f14ee314f1f002cf187166ee2',\n+            'uploader': 'Igor Kleiner',\n+            'uploader_id': '@IgorDataScience',\n+        },\n+    }, {\n+        # playlists, multipage, different order\n+        'url': 'https://www.youtube.com/user/igorkle1/playlists?view=1&sort=dd',\n+        'playlist_mincount': 94,\n+        'info_dict': {\n+            'id': 'UCqj7Cz7revf5maW9g5pgNcg',\n+            'title': r're:Igor Kleiner(?: Ph\\.D\\.)? - Playlists',\n+            'description': 'md5:be97ee0f14ee314f1f002cf187166ee2',\n+            'uploader': 'Igor Kleiner',\n+            'uploader_id': '@IgorDataScience',\n+        },\n+    }, {\n+        # playlists, series\n+        'url': 'https://www.youtube.com/c/3blue1brown/playlists?view=50&sort=dd&shelf_id=3',\n+        'playlist_mincount': 5,\n+        'info_dict': {\n+            'id': 'UCYO_jab_esuFRV4b17AJtAw',\n+            'title': '3Blue1Brown - Playlists',\n+            'description': 'md5:e1384e8a133307dd10edee76e875d62f',\n+            'uploader': '3Blue1Brown',\n+            'uploader_id': '@3blue1brown',\n+        },\n+    }, {\n+        # playlists, singlepage\n+        'url': 'https://www.youtube.com/user/ThirstForScience/playlists',\n+        'playlist_mincount': 4,\n+        'info_dict': {\n+            'id': 'UCAEtajcuhQ6an9WEzY9LEMQ',\n+            'title': 'ThirstForScience - Playlists',\n+            'description': 'md5:609399d937ea957b0f53cbffb747a14c',\n+            'uploader': 'ThirstForScience',\n+            'uploader_id': '@ThirstForScience',\n+        }\n+    }, {\n+        'url': 'https://www.youtube.com/c/ChristophLaimer/playlists',\n+        'only_matching': True,\n+    }, {\n+        # basic, single video playlist\n+        'url': 'https://www.youtube.com/playlist?list=PL4lCao7KL_QFVb7Iudeipvc2BCavECqzc',\n+        'info_dict': {\n+            'id': 'PL4lCao7KL_QFVb7Iudeipvc2BCavECqzc',\n+            'title': 'youtube-dl public playlist',\n+            'uploader': 'Sergey M.',\n+            'uploader_id': '@sergeym.6173',\n+            'channel_id': 'UCmlqkdCBesrv2Lak1mF_MxA',\n+        },\n+        'playlist_count': 1,\n+    }, {\n+        # empty playlist\n+        'url': 'https://www.youtube.com/playlist?list=PL4lCao7KL_QFodcLWhDpGCYnngnHtQ-Xf',\n+        'info_dict': {\n+            'id': 'PL4lCao7KL_QFodcLWhDpGCYnngnHtQ-Xf',\n+            'title': 'youtube-dl empty playlist',\n+            'uploader': 'Sergey M.',\n+            'uploader_id': '@sergeym.6173',\n+            'channel_id': 'UCmlqkdCBesrv2Lak1mF_MxA',\n+        },\n+        'playlist_count': 0,\n+    }, {\n+        # Home tab\n+        'url': 'https://www.youtube.com/channel/UCKfVa3S1e4PHvxWcwyMMg8w/featured',\n+        'info_dict': {\n+            'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n+            'title': 'lex will - Home',\n+            'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n+        },\n+        'playlist_mincount': 2,\n+    }, {\n+        # Videos tab\n+        'url': 'https://www.youtube.com/channel/UCKfVa3S1e4PHvxWcwyMMg8w/videos',\n+        'info_dict': {\n+            'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n+            'title': 'lex will - Videos',\n+            'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n+        },\n+        'playlist_mincount': 975,\n+    }, {\n+        # Videos tab, sorted by popular\n+        'url': 'https://www.youtube.com/channel/UCKfVa3S1e4PHvxWcwyMMg8w/videos?view=0&sort=p&flow=grid',\n+        'info_dict': {\n+            'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n+            'title': 'lex will - Videos',\n+            'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n+        },\n+        'playlist_mincount': 199,\n+    }, {\n+        # Playlists tab\n+        'url': 'https://www.youtube.com/channel/UCKfVa3S1e4PHvxWcwyMMg8w/playlists',\n+        'info_dict': {\n+            'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n+            'title': 'lex will - Playlists',\n+            'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n+        },\n+        'playlist_mincount': 17,\n+    }, {\n+        # Community tab\n+        'url': 'https://www.youtube.com/channel/UCKfVa3S1e4PHvxWcwyMMg8w/community',\n+        'info_dict': {\n+            'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n+            'title': 'lex will - Community',\n+            'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n+        },\n+        'playlist_mincount': 18,\n+    }, {\n+        # Channels tab\n+        'url': 'https://www.youtube.com/channel/UCKfVa3S1e4PHvxWcwyMMg8w/channels',\n+        'info_dict': {\n+            'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n+            'title': r're:lex will - (?:Home|Channels)',\n+            'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n+        },\n+        'playlist_mincount': 75,\n+    }, {\n+        # Releases tab\n+        'url': 'https://www.youtube.com/@daftpunk/releases',\n+        'info_dict': {\n+            'id': 'UC_kRDKYrUlrbtrSiyu5Tflg',\n+            'title': 'Daft Punk - Releases',\n+            'description': 'Daft Punk (1993 - 2021) - Official YouTube Channel',\n+            'uploader_id': '@daftpunk',\n+            'uploader': 'Daft Punk',\n+        },\n+        'playlist_mincount': 36,\n+    }, {\n+        'url': 'https://invidio.us/channel/UCmlqkdCBesrv2Lak1mF_MxA',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.youtubekids.com/channel/UCmlqkdCBesrv2Lak1mF_MxA',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://music.youtube.com/channel/UCmlqkdCBesrv2Lak1mF_MxA',\n+        'only_matching': True,\n+    }, {\n+        'note': 'Playlist with deleted videos (#651). As a bonus, the video #51 is also twice in this list.',\n+        'url': 'https://www.youtube.com/playlist?list=PLwP_SiAcdui0KVebT0mU9Apz359a4ubsC',\n+        'info_dict': {\n+            'title': '29C3: Not my department',\n+            'id': 'PLwP_SiAcdui0KVebT0mU9Apz359a4ubsC',\n+            'uploader': 'Christiaan008',\n+            'uploader_id': '@ChRiStIaAn008',\n+            'channel_id': 'UCEPzS1rYsrkqzSLNp76nrcg',\n+        },\n+        'playlist_count': 96,\n+    }, {\n+        'note': 'Large playlist',\n+        'url': 'https://www.youtube.com/playlist?list=UUBABnxM4Ar9ten8Mdjj1j0Q',\n+        'info_dict': {\n+            'title': 'Uploads from Cauchemar',\n+            'id': 'UUBABnxM4Ar9ten8Mdjj1j0Q',\n+            'uploader': 'Cauchemar',\n+            'uploader_id': '@Cauchemar89',\n+            'channel_id': 'UCBABnxM4Ar9ten8Mdjj1j0Q',\n+        },\n+        'playlist_mincount': 1123,\n+    }, {\n+        # even larger playlist, 8832 videos\n+        'url': 'http://www.youtube.com/user/NASAgovVideo/videos',\n+        'only_matching': True,\n+    }, {\n+        'note': 'Buggy playlist: the webpage has a \"Load more\" button but it doesn\\'t have more videos',\n+        'url': 'https://www.youtube.com/playlist?list=UUXw-G3eDE9trcvY2sBMM_aA',\n+        'info_dict': {\n+            'title': 'Uploads from Interstellar Movie',\n+            'id': 'UUXw-G3eDEtrcvY2sBMM_aA', # Bug: Changed G3eDE to G3eDEt\n+            'uploader': 'Interstellar Movie',\n+            'uploader_id': '@InterstellarMovie',\n+            'channel_id': 'UCXw-G3eDEtrcvY2sBMM_aA', # Bug: Changed G3eDE to G3eDEt\n+        },\n+        'playlist_mincount': 21,\n+    }, {\n+        # https://github.com/ytdl-org/youtube-dl/issues/21844\n+        'url': 'https://www.youtube.com/playlist?list=PLzH6n4zXuckpfMu_4Ff8E7Z1behQks5ba',\n+        'info_dict': {\n+            'title': 'Data Analysis with Dr Mike Pound',\n+            'id': 'PLzH6n4zXuckpfMu_4Ff8E7Z1behQks5ba',\n+            'uploader': 'Computerphile',\n+            'uploader_id': '@Computerphile',\n+            'channel_id': 'UC9-y-6csu5WGm29I7JiwpnA',\n+        },\n+        'playlist_mincount': 11,\n+    }, {\n+        'url': 'https://invidio.us/playlist?list=PL4lCao7KL_QFVb7Iudeipvc2BCavECqzc',\n+        'only_matching': True,\n+    }, {\n+        # Playlist URL that does not actually serve a playlist\n+        'url': 'https://www.youtube.com/watch?v=FqZTN594JQw&list=PLMYEtVRpaqY00V9W81Cwmzp6N6vZqfUKD4',\n+        'info_dict': {\n+            'id': 'FqZTN594JQw',\n+            'ext': 'webm',\n+            'title': \"Smiley's People 01 detective, Adventure Series, Action\",\n+            'uploader': 'STREEM',\n+            'uploader_id': 'UCyPhqAZgwYWZfxElWVbVJng',\n+            'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/channel/UCyPhqAZgwYWZfxElWVbVJng',\n+            'upload_date': '20150526',\n+            'license': 'Standard YouTube License',\n+            'description': 'md5:507cdcb5a49ac0da37a920ece610be80',\n+            'categories': ['People & Blogs'],\n+            'tags': list,\n+            'view_count': int,\n+            'like_count': int,\n+        },\n+        'params': {\n+            'skip_download': True,\n+        },\n+        'skip': 'This video is not available.',\n+        'add_ie': [YoutubeIE.ie_key()],\n+    }, {\n+        'url': 'https://www.youtubekids.com/watch?v=Agk7R8I8o5U&list=PUZ6jURNr1WQZCNHF0ao-c0g',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.youtube.com/watch?v=MuAGGZNfUkU&list=RDMM',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.youtube.com/channel/UCoMdktPbSTixAyNGwb-UYkQ/live',\n+        'info_dict': {\n+            'id': r're:[\\da-zA-Z_-]{8,}',\n+            'ext': 'mp4',\n+            'title': r're:(?s)[A-Z].{20,}',\n+            'uploader': 'Sky News',\n+            'uploader_id': '@SkyNews',\n+            'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@SkyNews',\n+            'upload_date': r're:\\d{8}',\n+            'description': r're:(?s)(?:.*\\n)+SUBSCRIBE to our YouTube channel for more videos: http://www\\.youtube\\.com/skynews *\\n.*',\n+            'categories': ['News & Politics'],\n+            'tags': list,\n+            'like_count': int,\n+        },\n+        'params': {\n+            'skip_download': True,\n+        },\n+    }, {\n+        'url': 'https://www.youtube.com/user/TheYoungTurks/live',\n+        'info_dict': {\n+            'id': 'a48o2S1cPoo',\n+            'ext': 'mp4',\n+            'title': 'The Young Turks - Live Main Show',\n+            'uploader': 'The Young Turks',\n+            'uploader_id': 'TheYoungTurks',\n+            'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/user/TheYoungTurks',\n+            'upload_date': '20150715',\n+            'license': 'Standard YouTube License',\n+            'description': 'md5:438179573adcdff3c97ebb1ee632b891',\n+            'categories': ['News & Politics'],\n+            'tags': ['Cenk Uygur (TV Program Creator)', 'The Young Turks (Award-Winning Work)', 'Talk Show (TV Genre)'],\n+            'like_count': int,\n+        },\n+        'params': {\n+            'skip_download': True,\n+        },\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.youtube.com/channel/UC1yBKRuGpC1tSM73A0ZjYjQ/live',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.youtube.com/c/CommanderVideoHq/live',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.youtube.com/feed/trending',\n+        'only_matching': True,\n+    }, {\n+        # needs auth\n+        'url': 'https://www.youtube.com/feed/library',\n+        'only_matching': True,\n+    }, {\n+        # needs auth\n+        'url': 'https://www.youtube.com/feed/history',\n+        'only_matching': True,\n+    }, {\n+        # needs auth\n+        'url': 'https://www.youtube.com/feed/subscriptions',\n+        'only_matching': True,\n+    }, {\n+        # needs auth\n+        'url': 'https://www.youtube.com/feed/watch_later',\n+        'only_matching': True,\n+    }, {\n+        # no longer available?\n+        'url': 'https://www.youtube.com/feed/recommended',\n+        'only_matching': True,\n+    }, {\n+        # inline playlist with not always working continuations\n+        'url': 'https://www.youtube.com/watch?v=UC6u0Tct-Fo&list=PL36D642111D65BE7C',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.youtube.com/course?list=ECUl4u3cNGP61MdtwGTqZA0MreSaDybji8',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.youtube.com/course',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.youtube.com/zsecurity',\n+        'only_matching': True,\n+    }, {\n+        'url': 'http://www.youtube.com/NASAgovVideo/videos',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.youtube.com/TheYoungTurks/live',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.youtube.com/hashtag/cctv9',\n+        'info_dict': {\n+            'id': 'cctv9',\n+            'title': '#cctv9',\n+        },\n+        'playlist_mincount': 350,\n+    }, {\n+        'url': 'https://www.youtube.com/watch?list=PLW4dVinRY435CBE_JD3t-0SRXKfnZHS1P&feature=youtu.be&v=M9cJMXmQ_ZU',\n+        'only_matching': True,\n+    }, {\n+        'note': 'Search tab',\n+        'url': 'https://www.youtube.com/c/3blue1brown/search?query=linear%20algebra',\n+        'playlist_mincount': 20,\n+        'info_dict': {\n+            'id': 'UCYO_jab_esuFRV4b17AJtAw',\n+            'title': '3Blue1Brown - Search - linear algebra',\n+            'description': 'md5:e1384e8a133307dd10edee76e875d62f',\n+            'uploader': '3Blue1Brown',\n+            'uploader_id': '@3blue1brown',\n+            'channel_id': 'UCYO_jab_esuFRV4b17AJtAw',\n+        }\n+    }]\n+\n+    @classmethod\n+    def suitable(cls, url):\n         return not YoutubeIE.suitable(url) and super(\n             YoutubeTabIE, cls).suitable(url)\n \n     @staticmethod\n     def _extract_grid_item_renderer(item):\n-        assert isinstance(item, dict)\n-        for key, renderer in item.items():\n-            if not key.startswith('grid') or not key.endswith('Renderer'):\n-                continue\n-            if not isinstance(renderer, dict):\n-                continue\n-            return renderer\n-\n-    @staticmethod\n-    def _get_text(r, k):\n-        return traverse_obj(\n-            r, (k, 'runs', 0, 'text'), (k, 'simpleText'),\n-            expected_type=txt_or_none)\n-\n-    def _grid_entries(self, grid_renderer):\n-        for item in grid_renderer['items']:\n-            if not isinstance(item, dict):\n-                continue\n-            renderer = self._extract_grid_item_renderer(item)\n-            if not isinstance(renderer, dict):\n-                continue\n-            title = self._get_text(renderer, 'title')\n-            # playlist\n-            playlist_id = renderer.get('playlistId')\n-            if playlist_id:\n-                yield self.url_result(\n-                    'https://www.youtube.com/playlist?list=%s' % playlist_id,\n-                    ie=YoutubeTabIE.ie_key(), video_id=playlist_id,\n-                    video_title=title)\n-                continue\n-            # video\n-            video_id = renderer.get('videoId')\n-            if video_id:\n-                yield self._extract_video(renderer)\n-                continue\n-            # channel\n-            channel_id = renderer.get('channelId')\n-            if channel_id:\n-                title = self._get_text(renderer, 'title')\n-                yield self.url_result(\n-                    'https://www.youtube.com/channel/%s' % channel_id,\n-                    ie=YoutubeTabIE.ie_key(), video_title=title)\n-                continue\n-            # generic endpoint URL support\n-            ep_url = urljoin('https://www.youtube.com/', try_get(\n-                renderer, lambda x: x['navigationEndpoint']['commandMetadata']['webCommandMetadata']['url'],\n-                compat_str))\n-            if ep_url:\n-                for ie in (YoutubeTabIE, YoutubePlaylistIE, YoutubeIE):\n-                    if ie.suitable(ep_url):\n-                        yield self.url_result(\n-                            ep_url, ie=ie.ie_key(), video_id=ie._match_id(ep_url), video_title=title)\n-                        break\n-\n-    def _shelf_entries_from_content(self, shelf_renderer):\n-        content = shelf_renderer.get('content')\n-        if not isinstance(content, dict):\n-            return\n-        renderer = content.get('gridRenderer')\n-        if renderer:\n-            # TODO: add support for nested playlists so each shelf is processed\n-            # as separate playlist\n-            # TODO: this includes only first N items\n-            for entry in self._grid_entries(renderer):\n-                yield entry\n-        renderer = content.get('horizontalListRenderer')\n-        if renderer:\n-            # TODO\n-            pass\n-\n-    def _shelf_entries(self, shelf_renderer, skip_channels=False):\n-        ep = try_get(\n-            shelf_renderer, lambda x: x['endpoint']['commandMetadata']['webCommandMetadata']['url'],\n-            compat_str)\n-        shelf_url = urljoin('https://www.youtube.com', ep)\n-        if shelf_url:\n-            # Skipping links to another channels, note that checking for\n-            # endpoint.commandMetadata.webCommandMetadata.webPageTypwebPageType == WEB_PAGE_TYPE_CHANNEL\n-            # will not work\n-            if skip_channels and '/channels?' in shelf_url:\n-                return\n-            title = try_get(\n-                shelf_renderer, lambda x: x['title']['runs'][0]['text'], compat_str)\n-            yield self.url_result(shelf_url, video_title=title)\n-        # Shelf may not contain shelf URL, fallback to extraction from content\n-        for entry in self._shelf_entries_from_content(shelf_renderer):\n-            yield entry\n-\n-    def _playlist_entries(self, video_list_renderer):\n-        for content in video_list_renderer['contents']:\n-            if not isinstance(content, dict):\n-                continue\n-            renderer = content.get('playlistVideoRenderer') or content.get('playlistPanelVideoRenderer')\n-            if not isinstance(renderer, dict):\n-                continue\n-            video_id = renderer.get('videoId')\n-            if not video_id:\n-                continue\n-            yield self._extract_video(renderer)\n-\n-    def _video_entry(self, video_renderer):\n-        video_id = video_renderer.get('videoId')\n-        if video_id:\n-            return self._extract_video(video_renderer)\n-\n-    def _post_thread_entries(self, post_thread_renderer):\n-        post_renderer = try_get(\n-            post_thread_renderer, lambda x: x['post']['backstagePostRenderer'], dict)\n-        if not post_renderer:\n-            return\n-        # video attachment\n-        video_renderer = try_get(\n-            post_renderer, lambda x: x['backstageAttachment']['videoRenderer'], dict)\n-        video_id = None\n-        if video_renderer:\n-            entry = self._video_entry(video_renderer)\n-            if entry:\n-                yield entry\n-        # inline video links\n-        runs = try_get(post_renderer, lambda x: x['contentText']['runs'], list) or []\n-        for run in runs:\n-            if not isinstance(run, dict):\n-                continue\n-            ep_url = try_get(\n-                run, lambda x: x['navigationEndpoint']['urlEndpoint']['url'], compat_str)\n-            if not ep_url:\n-                continue\n-            if not YoutubeIE.suitable(ep_url):\n-                continue\n-            ep_video_id = YoutubeIE._match_id(ep_url)\n-            if video_id == ep_video_id:\n-                continue\n-            yield self.url_result(ep_url, ie=YoutubeIE.ie_key(), video_id=video_id)\n-\n-    def _post_thread_continuation_entries(self, post_thread_continuation):\n-        contents = post_thread_continuation.get('contents')\n-        if not isinstance(contents, list):\n-            return\n-        for content in contents:\n-            renderer = content.get('backstagePostThreadRenderer')\n-            if not isinstance(renderer, dict):\n-                continue\n-            for entry in self._post_thread_entries(renderer):\n-                yield entry\n-\n-    def _rich_grid_entries(self, contents):\n-        for content in contents:\n-            content = traverse_obj(\n-                content, ('richItemRenderer', 'content'),\n-                expected_type=dict) or {}\n-            video_renderer = traverse_obj(\n-                content, 'videoRenderer', 'reelItemRenderer',\n-                expected_type=dict)\n-            if video_renderer:\n-                entry = self._video_entry(video_renderer)\n-                if entry:\n-                    yield entry\n-            # playlist\n-            renderer = traverse_obj(\n-                content, 'playlistRenderer', expected_type=dict) or {}\n-            title = self._get_text(renderer, 'title')\n-            playlist_id = renderer.get('playlistId')\n-            if playlist_id:\n-                yield self.url_result(\n-                    'https://www.youtube.com/playlist?list=%s' % playlist_id,\n-                    ie=YoutubeTabIE.ie_key(), video_id=playlist_id,\n-                    video_title=title)\n-\n-    @staticmethod\n-    def _build_continuation_query(continuation, ctp=None):\n-        query = {\n-            'ctoken': continuation,\n-            'continuation': continuation,\n-        }\n-        if ctp:\n-            query['itct'] = ctp\n-        return query\n-\n-    @staticmethod\n-    def _extract_next_continuation_data(renderer):\n-        next_continuation = try_get(\n-            renderer, lambda x: x['continuations'][0]['nextContinuationData'], dict)\n-        if not next_continuation:\n-            return\n-        continuation = next_continuation.get('continuation')\n-        if not continuation:\n-            return\n-        ctp = next_continuation.get('clickTrackingParams')\n-        return YoutubeTabIE._build_continuation_query(continuation, ctp)\n-\n-    @classmethod\n-    def _extract_continuation(cls, renderer):\n-        next_continuation = cls._extract_next_continuation_data(renderer)\n-        if next_continuation:\n-            return next_continuation\n-        contents = []\n-        for key in ('contents', 'items'):\n-            contents.extend(try_get(renderer, lambda x: x[key], list) or [])\n-        for content in contents:\n-            if not isinstance(content, dict):\n-                continue\n-            continuation_ep = try_get(\n-                content, lambda x: x['continuationItemRenderer']['continuationEndpoint'],\n-                dict)\n-            if not continuation_ep:\n-                continue\n-            continuation = try_get(\n-                continuation_ep, lambda x: x['continuationCommand']['token'], compat_str)\n-            if not continuation:\n-                continue\n-            ctp = continuation_ep.get('clickTrackingParams')\n-            return YoutubeTabIE._build_continuation_query(continuation, ctp)\n-\n-    def _entries(self, tab, item_id, webpage):\n-        tab_content = try_get(tab, lambda x: x['content'], dict)\n-        if not tab_content:\n-            return\n-        slr_renderer = try_get(tab_content, lambda x: x['sectionListRenderer'], dict)\n-        if slr_renderer:\n-            is_channels_tab = tab.get('title') == 'Channels'\n-            continuation = None\n-            slr_contents = try_get(slr_renderer, lambda x: x['contents'], list) or []\n-            for slr_content in slr_contents:\n-                if not isinstance(slr_content, dict):\n-                    continue\n-                is_renderer = try_get(slr_content, lambda x: x['itemSectionRenderer'], dict)\n-                if not is_renderer:\n-                    continue\n-                isr_contents = try_get(is_renderer, lambda x: x['contents'], list) or []\n-                for isr_content in isr_contents:\n-                    if not isinstance(isr_content, dict):\n-                        continue\n-                    renderer = isr_content.get('playlistVideoListRenderer')\n-                    if renderer:\n-                        for entry in self._playlist_entries(renderer):\n-                            yield entry\n-                        continuation = self._extract_continuation(renderer)\n-                        continue\n-                    renderer = isr_content.get('gridRenderer')\n-                    if renderer:\n-                        for entry in self._grid_entries(renderer):\n-                            yield entry\n-                        continuation = self._extract_continuation(renderer)\n-                        continue\n-                    renderer = isr_content.get('shelfRenderer')\n-                    if renderer:\n-                        for entry in self._shelf_entries(renderer, not is_channels_tab):\n-                            yield entry\n-                        continue\n-                    renderer = isr_content.get('backstagePostThreadRenderer')\n-                    if renderer:\n-                        for entry in self._post_thread_entries(renderer):\n-                            yield entry\n-                        continuation = self._extract_continuation(renderer)\n-                        continue\n-                    renderer = isr_content.get('videoRenderer')\n-                    if renderer:\n-                        entry = self._video_entry(renderer)\n-                        if entry:\n-                            yield entry\n-\n-                if not continuation:\n-                    continuation = self._extract_continuation(is_renderer)\n-            if not continuation:\n-                continuation = self._extract_continuation(slr_renderer)\n-        else:\n-            rich_grid_renderer = tab_content.get('richGridRenderer')\n-            if not rich_grid_renderer:\n-                return\n-            for entry in self._rich_grid_entries(rich_grid_renderer.get('contents') or []):\n-                yield entry\n-\n-            continuation = self._extract_continuation(rich_grid_renderer)\n-\n-        ytcfg = self._extract_ytcfg(item_id, webpage)\n-        client_version = try_get(\n-            ytcfg, lambda x: x['INNERTUBE_CLIENT_VERSION'], compat_str) or '2.20210407.08.00'\n-\n-        headers = {\n-            'x-youtube-client-name': '1',\n-            'x-youtube-client-version': client_version,\n-            'content-type': 'application/json',\n-        }\n-\n-        context = try_get(ytcfg, lambda x: x['INNERTUBE_CONTEXT'], dict) or {\n-            'client': {\n-                'clientName': 'WEB',\n-                'clientVersion': client_version,\n-            }\n-        }\n-        visitor_data = try_get(context, lambda x: x['client']['visitorData'], compat_str)\n-\n-        identity_token = self._extract_identity_token(ytcfg, webpage)\n-        if identity_token:\n-            headers['x-youtube-identity-token'] = identity_token\n-\n-        data = {\n-            'context': context,\n-        }\n-\n-        for page_num in itertools.count(1):\n-            if not continuation:\n-                break\n-            if visitor_data:\n-                headers['x-goog-visitor-id'] = visitor_data\n-            data['continuation'] = continuation['continuation']\n-            data['clickTracking'] = {\n-                'clickTrackingParams': continuation['itct']\n-            }\n-            count = 0\n-            retries = 3\n-            while count <= retries:\n-                try:\n-                    # Downloading page may result in intermittent 5xx HTTP error\n-                    # that is usually worked around with a retry\n-                    response = self._download_json(\n-                        'https://www.youtube.com/youtubei/v1/browse?key=AIzaSyAO_FJ2SlqU8Q4STEHLGCilw_Y9_11qcW8',\n-                        None, 'Downloading page %d%s' % (page_num, ' (retry #%d)' % count if count else ''),\n-                        headers=headers, data=json.dumps(data).encode('utf8'))\n-                    break\n-                except ExtractorError as e:\n-                    if isinstance(e.cause, compat_HTTPError) and e.cause.code in (500, 503):\n-                        count += 1\n-                        if count <= retries:\n-                            continue\n-                    raise\n-            if not response:\n-                break\n-\n-            visitor_data = try_get(\n-                response, lambda x: x['responseContext']['visitorData'], compat_str) or visitor_data\n-\n-            continuation_contents = try_get(\n-                response, lambda x: x['continuationContents'], dict)\n-            if continuation_contents:\n-                continuation_renderer = continuation_contents.get('playlistVideoListContinuation')\n-                if continuation_renderer:\n-                    for entry in self._playlist_entries(continuation_renderer):\n-                        yield entry\n-                    continuation = self._extract_continuation(continuation_renderer)\n-                    continue\n-                continuation_renderer = continuation_contents.get('gridContinuation')\n-                if continuation_renderer:\n-                    for entry in self._grid_entries(continuation_renderer):\n-                        yield entry\n-                    continuation = self._extract_continuation(continuation_renderer)\n-                    continue\n-                continuation_renderer = continuation_contents.get('itemSectionContinuation')\n-                if continuation_renderer:\n-                    for entry in self._post_thread_continuation_entries(continuation_renderer):\n-                        yield entry\n-                    continuation = self._extract_continuation(continuation_renderer)\n-                    continue\n-\n-            on_response_received = dict_get(response, ('onResponseReceivedActions', 'onResponseReceivedEndpoints'))\n-            continuation_items = try_get(\n-                on_response_received, lambda x: x[0]['appendContinuationItemsAction']['continuationItems'], list)\n-            if continuation_items:\n-                continuation_item = continuation_items[0]\n-                if not isinstance(continuation_item, dict):\n-                    continue\n-                renderer = self._extract_grid_item_renderer(continuation_item)\n-                if renderer:\n-                    grid_renderer = {'items': continuation_items}\n-                    for entry in self._grid_entries(grid_renderer):\n-                        yield entry\n-                    continuation = self._extract_continuation(grid_renderer)\n-                    continue\n-                renderer = continuation_item.get('playlistVideoRenderer') or continuation_item.get('itemSectionRenderer')\n-                if renderer:\n-                    video_list_renderer = {'contents': continuation_items}\n-                    for entry in self._playlist_entries(video_list_renderer):\n-                        yield entry\n-                    continuation = self._extract_continuation(video_list_renderer)\n-                    continue\n-                renderer = continuation_item.get('backstagePostThreadRenderer')\n-                if renderer:\n-                    continuation_renderer = {'contents': continuation_items}\n-                    for entry in self._post_thread_continuation_entries(continuation_renderer):\n-                        yield entry\n-                    continuation = self._extract_continuation(continuation_renderer)\n-                    continue\n-                renderer = continuation_item.get('richItemRenderer')\n-                if renderer:\n-                    for entry in self._rich_grid_entries(continuation_items):\n-                        yield entry\n-                    continuation = self._extract_continuation({'contents': continuation_items})\n-                    continue\n-\n-            break\n-\n-    @staticmethod\n-    def _extract_selected_tab(tabs):\n-        for tab in tabs:\n-            renderer = dict_get(tab, ('tabRenderer', 'expandableTabRenderer')) or {}\n-            if renderer.get('selected') is True:\n-                return renderer\n-        else:\n-            raise ExtractorError('Unable to find selected tab')\n-\n-    def _extract_uploader(self, metadata, data):\n-        uploader = {}\n-        renderers = traverse_obj(data,\n-                                 ('sidebar', 'playlistSidebarRenderer', 'items'))\n-        uploader['channel_id'] = self._extract_channel_id('', metadata=metadata, renderers=renderers)\n-        uploader['uploader'] = (\n-            self._extract_author_var('', 'name', renderers=renderers)\n-            or self._extract_author_var('', 'name', metadata=metadata))\n-        uploader['uploader_url'] = self._yt_urljoin(\n-            self._extract_author_var('', 'url', metadata=metadata, renderers=renderers))\n-        uploader['uploader_id'] = self._extract_uploader_id(uploader['uploader_url'])\n-        uploader['channel'] = uploader['uploader']\n-        return uploader\n-\n-    @classmethod\n-    def _extract_alert(cls, data):\n-        alerts = []\n-        for alert in traverse_obj(data, ('alerts', Ellipsis), expected_type=dict):\n-            alert_text = traverse_obj(\n-                alert, (None, lambda x: x['alertRenderer']['text']), get_all=False)\n-            if not alert_text:\n-                continue\n-            text = cls._get_text(alert_text, 'text')\n-            if text:\n-                alerts.append(text)\n-        return '\\n'.join(alerts)\n-\n-    def _extract_from_tabs(self, item_id, webpage, data, tabs):\n-        selected_tab = self._extract_selected_tab(tabs)\n-        renderer = traverse_obj(data, ('metadata', 'channelMetadataRenderer'),\n-                                expected_type=dict) or {}\n-        playlist_id = item_id\n-        title = description = None\n-        if renderer:\n-            channel_title = txt_or_none(renderer.get('title')) or item_id\n-            tab_title = txt_or_none(selected_tab.get('title'))\n-            title = join_nonempty(\n-                channel_title or item_id, tab_title,\n-                txt_or_none(selected_tab.get('expandedText')),\n-                delim=' - ')\n-            description = txt_or_none(renderer.get('description'))\n-            playlist_id = txt_or_none(renderer.get('externalId')) or playlist_id\n-        else:\n-            renderer = traverse_obj(data,\n-                                    ('metadata', 'playlistMetadataRenderer'),\n-                                    ('header', 'hashtagHeaderRenderer'),\n-                                    expected_type=dict) or {}\n-            title = traverse_obj(renderer, 'title', ('hashtag', 'simpleText'),\n-                                 expected_type=txt_or_none)\n-        playlist = self.playlist_result(\n-            self._entries(selected_tab, item_id, webpage),\n-            playlist_id=playlist_id, playlist_title=title,\n-            playlist_description=description)\n-        return merge_dicts(playlist, self._extract_uploader(renderer, data))\n-\n-    def _extract_from_playlist(self, item_id, url, data, playlist):\n-        title = traverse_obj((playlist, data),\n-                             (0, 'title'), (1, 'titleText', 'simpleText'),\n-                             expected_type=txt_or_none)\n-        playlist_id = txt_or_none(playlist.get('playlistId')) or item_id\n-        # Inline playlist rendition continuation does not always work\n-        # at Youtube side, so delegating regular tab-based playlist URL\n-        # processing whenever possible.\n-        playlist_url = urljoin(url, traverse_obj(\n-            playlist, ('endpoint', 'commandMetadata', 'webCommandMetadata', 'url'),\n-            expected_type=url_or_none))\n-        if playlist_url and playlist_url != url:\n-            return self.url_result(\n-                playlist_url, ie=YoutubeTabIE.ie_key(), video_id=playlist_id,\n-                video_title=title)\n-        return self.playlist_result(\n-            self._playlist_entries(playlist), playlist_id=playlist_id,\n-            playlist_title=title)\n-\n-    def _extract_identity_token(self, ytcfg, webpage):\n-        if ytcfg:\n-            token = try_get(ytcfg, lambda x: x['ID_TOKEN'], compat_str)\n-            if token:\n-                return token\n-        return self._search_regex(\n-            r'\\bID_TOKEN[\"\\']\\s*:\\s*[\"\\'](.+?)[\"\\']', webpage,\n-            'identity token', default=None)\n-\n-    def _real_extract(self, url):\n-        item_id = self._match_id(url)\n-        url = update_url(url, netloc='www.youtube.com')\n-        # Handle both video/playlist URLs\n-        qs = parse_qs(url)\n-        video_id = qs.get('v', [None])[0]\n-        playlist_id = qs.get('list', [None])[0]\n-        if video_id and playlist_id:\n-            if self._downloader.params.get('noplaylist'):\n-                self.to_screen('Downloading just video %s because of --no-playlist' % video_id)\n-                return self.url_result(video_id, ie=YoutubeIE.ie_key(), video_id=video_id)\n-            self.to_screen('Downloading playlist %s - add --no-playlist to just download video %s' % (playlist_id, video_id))\n-        webpage = self._download_webpage(url, item_id)\n-        data = self._extract_yt_initial_data(item_id, webpage)\n-        tabs = try_get(\n-            data, lambda x: x['contents']['twoColumnBrowseResultsRenderer']['tabs'], list)\n-        if tabs:\n-            return self._extract_from_tabs(item_id, webpage, data, tabs)\n-        playlist = try_get(\n-            data, lambda x: x['contents']['twoColumnWatchNextResults']['playlist']['playlist'], dict)\n-        if playlist:\n-            return self._extract_from_playlist(item_id, url, data, playlist)\n-        # Fallback to video extraction if no playlist alike page is recognized.\n-        # First check for the current video then try the v attribute of URL query.\n-        video_id = try_get(\n-            data, lambda x: x['currentVideoEndpoint']['watchEndpoint']['videoId'],\n-            compat_str) or video_id\n-        if video_id:\n-            return self.url_result(video_id, ie=YoutubeIE.ie_key(), video_id=video_id)\n-        # Capture and output alerts\n-        alert = self._extract_alert(data)\n-        if alert:\n-            raise ExtractorError(alert, expected=True)\n-        # Failed to recognize\n-        raise ExtractorError('Unable to recognize tab page')\n-\n-\n-class YoutubePlaylistIE(InfoExtractor):\n-    IE_DESC = 'YouTube.com playlists'\n-    _VALID_URL = r'''(?x)(?:\n-                        (?:https?://)?\n-                        (?:\\w+\\.)?\n-                        (?:\n-                            (?:\n-                                youtube(?:kids)?\\.com|\n-                                invidio\\.us\n-                            )\n-                            /.*?\\?.*?\\blist=\n-                        )?\n-                        (?P<id>%(playlist_id)s)\n-                     )''' % {'playlist_id': YoutubeBaseInfoExtractor._PLAYLIST_ID_RE}\n-    IE_NAME = 'youtube:playlist'\n-    _TESTS = [{\n-        'note': 'issue #673',\n-        'url': 'PLBB231211A4F62143',\n-        'info_dict': {\n-            'title': '[OLD]Team Fortress 2 (Class-based LP)',\n-            'id': 'PLBB231211A4F62143',\n-            'uploader': 'Wickman',\n-            'uploader_id': '@WickmanVT',\n-            'channel_id': 'UCKSpbfbl5kRQpTdL7kMc-1Q',\n-        },\n-        'playlist_mincount': 29,\n-    }, {\n-        'url': 'PLtPgu7CB4gbY9oDN3drwC3cMbJggS7dKl',\n-        'info_dict': {\n-            'title': 'YDL_safe_search',\n-            'id': 'PLtPgu7CB4gbY9oDN3drwC3cMbJggS7dKl',\n-        },\n-        'playlist_count': 2,\n-        'skip': 'This playlist is private',\n-    }, {\n-        'note': 'embedded',\n-        'url': 'https://www.youtube.com/embed/videoseries?list=PL6IaIsEjSbf96XFRuNccS_RuEXwNdsoEu',\n-        # TODO: full playlist requires _reload_with_unavailable_videos()\n-        # 'playlist_count': 4,\n-        'playlist_mincount': 1,\n-        'info_dict': {\n-            'title': 'JODA15',\n-            'id': 'PL6IaIsEjSbf96XFRuNccS_RuEXwNdsoEu',\n-            'uploader': 'milan',\n-            'uploader_id': '@milan5503',\n-            'channel_id': 'UCEI1-PVPcYXjB73Hfelbmaw',\n-        }\n-    }, {\n-        'url': 'http://www.youtube.com/embed/_xDOZElKyNU?list=PLsyOSbh5bs16vubvKePAQ1x3PhKavfBIl',\n-        'playlist_mincount': 455,\n-        'info_dict': {\n-            'title': '2018 Chinese New Singles (11/6 updated)',\n-            'id': 'PLsyOSbh5bs16vubvKePAQ1x3PhKavfBIl',\n-            'uploader': 'LBK',\n-            'uploader_id': '@music_king',\n-            'channel_id': 'UC21nz3_MesPLqtDqwdvnoxA',\n-        }\n-    }, {\n-        'url': 'TLGGrESM50VT6acwMjAyMjAxNw',\n-        'only_matching': True,\n-    }, {\n-        # music album playlist\n-        'url': 'OLAK5uy_m4xAFdmMC5rX3Ji3g93pQe3hqLZw_9LhM',\n-        'only_matching': True,\n-    }]\n-\n-    @classmethod\n-    def suitable(cls, url):\n-        if YoutubeTabIE.suitable(url):\n-            return False\n-        if parse_qs(url).get('v', [None])[0]:\n-            return False\n-        return super(YoutubePlaylistIE, cls).suitable(url)\n-\n-    def _real_extract(self, url):\n-        playlist_id = self._match_id(url)\n-        qs = parse_qs(url)\n-        if not qs:\n-            qs = {'list': playlist_id}\n-        return self.url_result(\n-            update_url_query('https://www.youtube.com/playlist', qs),\n-            ie=YoutubeTabIE.ie_key(), video_id=playlist_id)\n-\n-\n-class YoutubeYtBeIE(InfoExtractor):\n-    _VALID_URL = r'https?://youtu\\.be/(?P<id>[0-9A-Za-z_-]{11})/*?.*?\\blist=(?P<playlist_id>%(playlist_id)s)' % {'playlist_id': YoutubeBaseInfoExtractor._PLAYLIST_ID_RE}\n-    _TESTS = [{\n-        'url': 'https://youtu.be/yeWKywCrFtk?list=PL2qgrgXsNUG5ig9cat4ohreBjYLAPC0J5',\n-        'info_dict': {\n-            'id': 'yeWKywCrFtk',\n-            'ext': 'mp4',\n-            'title': 'Small Scale Baler and Braiding Rugs',\n-            'uploader': 'Backus-Page House Museum',\n-            'uploader_id': '@backuspagemuseum',\n-            'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@backuspagemuseum',\n-            'upload_date': '20161008',\n-            'description': 'md5:800c0c78d5eb128500bffd4f0b4f2e8a',\n-            'categories': ['Nonprofits & Activism'],\n-            'tags': list,\n-            'like_count': int,\n-        },\n-        'params': {\n-            'noplaylist': True,\n-            'skip_download': True,\n-        },\n-    }, {\n-        'url': 'https://youtu.be/uWyaPkt-VOI?list=PL9D9FC436B881BA21',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        mobj = re.match(self._VALID_URL, url)\n-        video_id = mobj.group('id')\n-        playlist_id = mobj.group('playlist_id')\n-        return self.url_result(\n-            update_url_query('https://www.youtube.com/watch', {\n-                'v': video_id,\n-                'list': playlist_id,\n-                'feature': 'youtu.be',\n-            }), ie=YoutubeTabIE.ie_key(), video_id=playlist_id)\n-\n-\n-class YoutubeYtUserIE(InfoExtractor):\n-    _VALID_URL = r'ytuser:(?P<id>.+)'\n-    _TESTS = [{\n-        'url': 'ytuser:phihag',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        user_id = self._match_id(url)\n-        return self.url_result(\n-            'https://www.youtube.com/user/%s' % user_id,\n-            ie=YoutubeTabIE.ie_key(), video_id=user_id)\n-\n-\n-class YoutubeFavouritesIE(YoutubeBaseInfoExtractor):\n-    IE_NAME = 'youtube:favorites'\n-    IE_DESC = 'YouTube.com favourite videos, \":ytfav\" for short (requires authentication)'\n-    _VALID_URL = r'https?://(?:www\\.)?youtube\\.com/my_favorites|:ytfav(?:ou?rites)?'\n-    _LOGIN_REQUIRED = True\n-    _TESTS = [{\n-        'url': ':ytfav',\n-        'only_matching': True,\n-    }, {\n-        'url': ':ytfavorites',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        return self.url_result(\n-            'https://www.youtube.com/playlist?list=LL',\n-            ie=YoutubeTabIE.ie_key())\n-\n-\n-class YoutubeSearchIE(SearchInfoExtractor, YoutubeBaseInfoExtractor):\n-    IE_DESC = 'YouTube.com searches'\n-    IE_NAME = 'youtube:search'\n-    _SEARCH_KEY = 'ytsearch'\n-    _SEARCH_PARAMS = 'EgIQAQ%3D%3D'  # Videos only\n-    _MAX_RESULTS = float('inf')\n-    _TESTS = [{\n-        'url': 'ytsearch10:youtube-dl test video',\n-        'playlist_count': 10,\n-        'info_dict': {\n-            'id': 'youtube-dl test video',\n-            'title': 'youtube-dl test video',\n-        }\n-    }]\n-\n-    def _get_n_results(self, query, n):\n-        \"\"\"Get a specified number of results for a query\"\"\"\n-        entries = itertools.islice(self._search_results(query, self._SEARCH_PARAMS), 0, None if n == float('inf') else n)\n-        return self.playlist_result(entries, query, query)\n-\n-\n-class YoutubeSearchDateIE(YoutubeSearchIE):\n-    IE_NAME = YoutubeSearchIE.IE_NAME + ':date'\n-    _SEARCH_KEY = 'ytsearchdate'\n-    IE_DESC = 'YouTube.com searches, newest videos first'\n-    _SEARCH_PARAMS = 'CAISAhAB'  # Videos only, sorted by date\n-    _TESTS = [{\n-        'url': 'ytsearchdate10:youtube-dl test video',\n-        'playlist_count': 10,\n-        'info_dict': {\n-            'id': 'youtube-dl test video',\n-            'title': 'youtube-dl test video',\n-        }\n-    }]\n-\n-\n-class YoutubeSearchURLIE(YoutubeBaseInfoExtractor):\n-    IE_DESC = 'YouTube search URLs with sorting and filter support'\n-    IE_NAME = YoutubeSearchIE.IE_NAME + '_url'\n-    _VALID_URL = r'https?://(?:www\\.)?youtube\\.com/results\\?(.*?&)?(?:search_query|q)=(?:[^&]+)(?:[&]|$)'\n-    _TESTS = [{\n-        'url': 'https://www.youtube.com/results?baz=bar&search_query=youtube-dl+test+video&filters=video&lclk=video',\n-        'playlist_mincount': 5,\n-        'info_dict': {\n-            'id': 'youtube-dl test video',\n-            'title': 'youtube-dl test video',\n-        },\n-        'params': {'playlistend': 5}\n-    }, {\n-        'url': 'https://www.youtube.com/results?q=test&sp=EgQIBBgB',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        qs = parse_qs(url)\n-        query = (qs.get('search_query') or qs.get('q'))[-1]\n-        params = qs.get('sp', ('',))[-1]\n-        return self.playlist_result(self._search_results(query, params), query, query)\n-\n-\n-class YoutubeFeedsInfoExtractor(YoutubeTabIE):\n-    \"\"\"\n-    Base class for feed extractors\n-    Subclasses must define the _FEED_NAME property.\n-    \"\"\"\n-    _LOGIN_REQUIRED = True\n-\n-    @property\n-    def IE_NAME(self):\n-        return 'youtube:%s' % self._FEED_NAME\n-\n-    def _real_initialize(self):\n-        self._login()\n-\n-    def _real_extract(self, url):\n-        return self.url_result(\n-            'https://www.youtube.com/feed/%s' % self._FEED_NAME,\n-            ie=YoutubeTabIE.ie_key())\n-\n-\n-class YoutubeWatchLaterIE(InfoExtractor):\n-    IE_NAME = 'youtube:watchlater'\n-    IE_DESC = 'Youtube watch later list, \":ytwatchlater\" for short (requires authentication)'\n-    _VALID_URL = r':ytwatchlater'\n-    _TESTS = [{\n-        'url': ':ytwatchlater',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        return self.url_result(\n-            'https://www.youtube.com/playlist?list=WL', ie=YoutubeTabIE.ie_key())\n-\n-\n-class YoutubeRecommendedIE(YoutubeFeedsInfoExtractor):\n-    IE_DESC = 'YouTube.com recommended videos, \":ytrec\" for short (requires authentication)'\n-    _VALID_URL = r':ytrec(?:ommended)?'\n-    _FEED_NAME = 'recommended'\n-    _TESTS = [{\n-        'url': ':ytrec',\n-        'only_matching': True,\n-    }, {\n-        'url': ':ytrecommended',\n-        'only_matching': True,\n-    }]\n-\n-\n-class YoutubeSubscriptionsIE(YoutubeFeedsInfoExtractor):\n-    IE_DESC = 'YouTube.com subscriptions feed, \"ytsubs\" keyword (requires authentication)'\n-    _VALID_URL = r':ytsubs(?:criptions)?'\n-    _FEED_NAME = 'subscriptions'\n-    _TESTS = [{\n-        'url': ':ytsubs',\n-        'only_matching': True,\n-    }, {\n-        'url': ':ytsubscriptions',\n-        'only_matching': True,\n-    }]\n-\n-\n-class YoutubeHistoryIE(YoutubeFeedsInfoExtractor):\n-    IE_DESC = 'Youtube watch history, \":ythistory\" for short (requires authentication)'\n-    _VALID_URL = r':ythistory'\n-    _FEED_NAME = 'history'\n-    _TESTS = [{\n-        'url': ':ythistory',\n-        'only_matching': True,\n-    }]\n-\n-\n-class YoutubeTruncatedURLIE(InfoExtractor):\n-    IE_NAME = 'youtube:truncated_url'\n-    IE_DESC = False  # Do not list\n-    _VALID_URL = r'''(?x)\n-        (?:https?://)?\n-        (?:\\w+\\.)?[yY][oO][uU][tT][uU][bB][eE](?:-nocookie)?\\.com/\n-        (?:watch\\?(?:\n-            feature=[a-z_]+|\n-            annotation_id=annotation_[^&]+|\n-            x-yt-cl=[0-9]+|\n-            hl=[^&]*|\n-            t=[0-9]+\n-        )?\n-        |\n-            attribution_link\\?a=[^&]+\n-        )\n-        $\n-    '''\n-\n-    _TESTS = [{\n-        'url': 'https://www.youtube.com/watch?annotation_id=annotation_3951667041',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.youtube.com/watch?',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.youtube.com/watch?x-yt-cl=84503534',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.youtube.com/watch?feature=foo',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.youtube.com/watch?hl=en-GB',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.youtube.com/watch?t=2372',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        raise ExtractorError(\n-            'Did you forget to quote the URL? Remember that & is a meta '\n-            'character in most shells, so you want to put the URL in quotes, '\n-            'like  youtube-dl '\n-            '\"https://www.youtube.com/watch?feature=foo&v=BaW_jenozKc\" '\n-            ' or simply  youtube-dl BaW_jenozKc  .',\n-            expected=True)\n-\n-\n-class YoutubeTruncatedIDIE(InfoExtractor):\n-    IE_NAME = 'youtube:truncated_id'\n-    IE_DESC = False  # Do not list\n-    _VALID_URL = r'https?://(?:www\\.)?youtube\\.com/watch\\?v=(?P<id>[0-9A-Za-z_-]{1,10})$'\n-\n-    _TESTS = [{\n-        'url': 'https://www.youtube.com/watch?v=N_708QY7Ob',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        raise ExtractorError(\n-            'Incomplete YouTube ID %s. URL %s looks truncated.' % (video_id, url),\n-            expected=True)\n+        assert\n--- a/youtube_dl/jsinterp.py\n+++ b/youtube_dl/jsinterp.py\n@@ -1,3 +1,4 @@\n+# coding: utf-8\n from __future__ import unicode_literals\n \n import itertools\n@@ -5,11 +6,12 @@\n import operator\n import re\n \n-from functools import update_wrapper\n+from functools import update_wrapper, wraps\n \n from .utils import (\n     error_to_compat_str,\n     ExtractorError,\n+    float_or_none,\n     js_to_json,\n     remove_quotes,\n     unified_timestamp,\n@@ -20,9 +22,11 @@\n     compat_basestring,\n     compat_chr,\n     compat_collections_chain_map as ChainMap,\n+    compat_contextlib_suppress,\n     compat_filter as filter,\n     compat_itertools_zip_longest as zip_longest,\n     compat_map as map,\n+    compat_numeric_types,\n     compat_str,\n )\n \n@@ -62,6 +66,10 @@\n _Infinity = float('inf')\n \n \n+class JS_Undefined(object):\n+    pass\n+\n+\n def _js_bit_op(op):\n \n     def zeroise(x):\n@@ -74,43 +82,114 @@\n     return wrapped\n \n \n-def _js_arith_op(op):\n+def _js_arith_op(op, div=False):\n \n     @wraps_op(op)\n     def wrapped(a, b):\n         if JS_Undefined in (a, b):\n             return _NaN\n-        return op(a or 0, b or 0)\n+        # null, \"\" --> 0\n+        a, b = (float_or_none(\n+            (x.strip() if isinstance(x, compat_basestring) else x) or 0,\n+            default=_NaN) for x in (a, b))\n+        if _NaN in (a, b):\n+            return _NaN\n+        try:\n+            return op(a, b)\n+        except ZeroDivisionError:\n+            return _NaN if not (div and (a or b)) else _Infinity\n \n     return wrapped\n \n \n-def _js_div(a, b):\n-    if JS_Undefined in (a, b) or not (a or b):\n-        return _NaN\n-    return operator.truediv(a or 0, b) if b else _Infinity\n-\n-\n-def _js_mod(a, b):\n-    if JS_Undefined in (a, b) or not b:\n-        return _NaN\n-    return (a or 0) % b\n+_js_arith_add = _js_arith_op(operator.add)\n+\n+\n+def _js_add(a, b):\n+    if not (isinstance(a, compat_basestring) or isinstance(b, compat_basestring)):\n+        return _js_arith_add(a, b)\n+    if not isinstance(a, compat_basestring):\n+        a = _js_toString(a)\n+    elif not isinstance(b, compat_basestring):\n+        b = _js_toString(b)\n+    return operator.concat(a, b)\n+\n+\n+_js_mod = _js_arith_op(operator.mod)\n+__js_exp = _js_arith_op(operator.pow)\n \n \n def _js_exp(a, b):\n     if not b:\n         return 1  # even 0 ** 0 !!\n-    elif JS_Undefined in (a, b):\n-        return _NaN\n-    return (a or 0) ** b\n-\n-\n-def _js_eq_op(op):\n+    return __js_exp(a, b)\n+\n+\n+def _js_to_primitive(v):\n+    return (\n+        ','.join(map(_js_toString, v)) if isinstance(v, list)\n+        else '[object Object]' if isinstance(v, dict)\n+        else compat_str(v) if not isinstance(v, (\n+            compat_numeric_types, compat_basestring))\n+        else v\n+    )\n+\n+\n+def _js_toString(v):\n+    return (\n+        'undefined' if v is JS_Undefined\n+        else 'Infinity' if v == _Infinity\n+        else 'NaN' if v is _NaN\n+        else 'null' if v is None\n+        # bool <= int: do this first\n+        else ('false', 'true')[v] if isinstance(v, bool)\n+        else '{0:.7f}'.format(v).rstrip('.0') if isinstance(v, compat_numeric_types)\n+        else _js_to_primitive(v))\n+\n+\n+_nullish = frozenset((None, JS_Undefined))\n+\n+\n+def _js_eq(a, b):\n+    # NaN != any\n+    if _NaN in (a, b):\n+        return False\n+    # Object is Object\n+    if isinstance(a, type(b)) and isinstance(b, (dict, list)):\n+        return operator.is_(a, b)\n+    # general case\n+    if a == b:\n+        return True\n+    # null == undefined\n+    a_b = set((a, b))\n+    if a_b & _nullish:\n+        return a_b <= _nullish\n+    a, b = _js_to_primitive(a), _js_to_primitive(b)\n+    if not isinstance(a, compat_basestring):\n+        a, b = b, a\n+    # Number to String: convert the string to a number\n+    # Conversion failure results in ... false\n+    if isinstance(a, compat_basestring):\n+        return float_or_none(a) == b\n+    return a == b\n+\n+\n+def _js_neq(a, b):\n+    return not _js_eq(a, b)\n+\n+\n+def _js_id_op(op):\n \n     @wraps_op(op)\n     def wrapped(a, b):\n-        if set((a, b)) <= set((None, JS_Undefined)):\n-            return op(a, a)\n+        if _NaN in (a, b):\n+            return op(_NaN, None)\n+        if not isinstance(a, (compat_basestring, compat_numeric_types)):\n+            a, b = b, a\n+        # strings are === if ==\n+        # why 'a' is not 'a': https://stackoverflow.com/a/1504848\n+        if isinstance(a, (compat_basestring, compat_numeric_types)):\n+            return a == b if op(0, 0) else a != b\n         return op(a, b)\n \n     return wrapped\n@@ -138,25 +217,57 @@\n     return if_true\n \n \n+def _js_unary_op(op):\n+\n+    @wraps_op(op)\n+    def wrapped(_, a):\n+        return op(a)\n+\n+    return wrapped\n+\n+\n+# https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/typeof\n+def _js_typeof(expr):\n+    with compat_contextlib_suppress(TypeError, KeyError):\n+        return {\n+            JS_Undefined: 'undefined',\n+            _NaN: 'number',\n+            _Infinity: 'number',\n+            True: 'boolean',\n+            False: 'boolean',\n+            None: 'null', # BUG: typeof null should be 'object'\n+        }[expr]\n+    for t, n in (\n+        (compat_basestring, 'string'),\n+        (compat_numeric_types, 'number'),\n+    ):\n+        if isinstance(expr, t):\n+            return n\n+    if callable(expr):\n+        return 'function'\n+    # TODO: Symbol, BigInt\n+    return 'object'\n+\n+\n # (op, definition) in order of binding priority, tightest first\n # avoid dict to maintain order\n # definition None => Defined in JSInterpreter._operator\n _OPERATORS = (\n     ('>>', _js_bit_op(operator.rshift)),\n     ('<<', _js_bit_op(operator.lshift)),\n-    ('+', _js_arith_op(operator.add)),\n+    ('+', _js_add),\n     ('-', _js_arith_op(operator.sub)),\n     ('*', _js_arith_op(operator.mul)),\n     ('%', _js_mod),\n-    ('/', _js_div),\n+    ('/', _js_arith_op(operator.truediv, div=True)),\n     ('**', _js_exp),\n )\n \n _COMP_OPERATORS = (\n-    ('===', operator.is_),\n-    ('!==', operator.is_not),\n-    ('==', _js_eq_op(operator.eq)),\n-    ('!=', _js_eq_op(operator.ne)),\n+    ('===', _js_id_op(operator.is_)),\n+    ('!==', _js_id_op(operator.is_not)),\n+    ('==', _js_eq),\n+    ('!=', _js_neq),\n     ('<=', _js_comp_op(operator.le)),\n     ('>=', _js_comp_op(operator.ge)),\n     ('<', _js_comp_op(operator.lt)),\n@@ -176,15 +287,16 @@\n     ('&&', None),\n )\n \n+_UNARY_OPERATORS_X = (\n+    ('void', _js_unary_op(lambda _: JS_Undefined)),\n+    ('typeof', _js_unary_op(_js_typeof)),\n+)\n+\n _OPERATOR_RE = '|'.join(map(lambda x: re.escape(x[0]), _OPERATORS + _LOG_OPERATORS))\n \n _NAME_RE = r'[a-zA-Z_$][\\w$]*'\n _MATCHING_PARENS = dict(zip(*zip('()', '{}', '[]')))\n _QUOTES = '\\'\"/'\n-\n-\n-class JS_Undefined(object):\n-    pass\n \n \n class JS_Break(ExtractorError):\n@@ -242,6 +354,7 @@\n \n     @classmethod\n     def wrap_interpreter(cls, f):\n+        @wraps(f)\n         def interpret_statement(self, stmt, local_vars, allow_recursion, *args, **kwargs):\n             if cls.ENABLED and stmt.strip():\n                 cls.write(stmt, level=allow_recursion)\n@@ -255,7 +368,7 @@\n                 raise\n             if cls.ENABLED and stmt.strip():\n                 if should_ret or repr(ret) != stmt:\n-                    cls.write(['->', '=>'][should_ret], repr(ret), '<-|', stmt, level=allow_recursion)\n+                    cls.write(['->', '=>'][bool(should_ret)], repr(ret), '<-|', stmt, level=allow_recursion)\n             return ret, should_ret\n         return interpret_statement\n \n@@ -284,6 +397,9 @@\n         RE_FLAGS = {\n             # special knowledge: Python's re flags are bitmask values, current max 128\n             # invent new bitmask values well above that for literal parsing\n+            # JS 'u' flag is effectively always set (surrogate pairs aren't seen),\n+            # but \\u{...} and \\p{...} escapes aren't handled); no additional JS 'v'\n+            # features are supported\n             # TODO: execute matches with these flags (remaining: d, y)\n             'd': 1024,  # Generate indices for substring matches\n             'g': 2048,  # Global search\n@@ -291,6 +407,7 @@\n             'm': re.M,  # Multi-line search\n             's': re.S,  # Allows . to match newline characters\n             'u': re.U,  # Treat a pattern as a sequence of unicode code points\n+            'v': re.U,  # Like 'u' with extended character class and \\p{} syntax\n             'y': 4096,  # Perform a \"sticky\" search that matches starting at the current position in the target string\n         }\n \n@@ -347,6 +464,8 @@\n     def __op_chars(cls):\n         op_chars = set(';,[')\n         for op in cls._all_operators():\n+            if op[0].isalpha():\n+                continue\n             op_chars.update(op[0])\n         return op_chars\n \n@@ -369,9 +488,18 @@\n         skipping = 0\n         if skip_delims:\n             skip_delims = variadic(skip_delims)\n+        skip_txt = None\n         for idx, char in enumerate(expr):\n+            if skip_txt and idx <= skip_txt[1]:\n+                continue\n             paren_delta = 0\n             if not in_quote:\n+                if char == '/' and expr[idx:idx + 2] == '/*':\n+                    # skip a comment\n+                    skip_txt = expr[idx:].find('*/', 2)\n+                    skip_txt = [idx, idx + skip_txt + 1] if skip_txt >= 2 else None\n+                    if skip_txt:\n+                        continue\n                 if char in _MATCHING_PARENS:\n                     counters[_MATCHING_PARENS[char]] += 1\n                     paren_delta = 1\n@@ -404,12 +532,19 @@\n             if pos < delim_len:\n                 pos += 1\n                 continue\n-            yield expr[start: idx - delim_len]\n+            if skip_txt and skip_txt[0] >= start and skip_txt[1] <= idx - delim_len:\n+                yield expr[start:skip_txt[0]] + expr[skip_txt[1] + 1: idx - delim_len]\n+            else:\n+                yield expr[start: idx - delim_len]\n+            skip_txt = None\n             start, pos = idx + 1, 0\n             splits += 1\n             if max_split and splits >= max_split:\n                 break\n-        yield expr[start:]\n+        if skip_txt and skip_txt[0] >= start:\n+            yield expr[start:skip_txt[0]] + expr[skip_txt[1] + 1:]\n+        else:\n+            yield expr[start:]\n \n     @classmethod\n     def _separate_at_paren(cls, expr, delim=None):\n@@ -425,7 +560,7 @@\n         if not _cached:\n             _cached.extend(itertools.chain(\n                 # Ref: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Operator_Precedence\n-                _SC_OPERATORS, _LOG_OPERATORS, _COMP_OPERATORS, _OPERATORS))\n+                _SC_OPERATORS, _LOG_OPERATORS, _COMP_OPERATORS, _OPERATORS, _UNARY_OPERATORS_X))\n         return _cached\n \n     def _operator(self, op, left_val, right_expr, expr, local_vars, allow_recursion):\n@@ -449,13 +584,14 @@\n         except Exception as e:\n             raise self.Exception('Failed to evaluate {left_val!r:.50} {op} {right_val!r:.50}'.format(**locals()), expr, cause=e)\n \n-    def _index(self, obj, idx, allow_undefined=False):\n-        if idx == 'length':\n+    def _index(self, obj, idx, allow_undefined=True):\n+        if idx == 'length' and isinstance(obj, list):\n             return len(obj)\n         try:\n-            return obj[int(idx)] if isinstance(obj, list) else obj[idx]\n-        except Exception as e:\n+            return obj[int(idx)] if isinstance(obj, list) else obj[compat_str(idx)]\n+        except (TypeError, KeyError, IndexError) as e:\n             if allow_undefined:\n+                # when is not allowed?\n                 return JS_Undefined\n             raise self.Exception('Cannot get index {idx!r:.100}'.format(**locals()), expr=repr(obj), cause=e)\n \n@@ -467,7 +603,7 @@\n \n     # used below\n     _VAR_RET_THROW_RE = re.compile(r'''(?x)\n-        (?P<var>(?:var|const|let)\\s)|return(?:\\s+|(?=[\"'])|$)|(?P<throw>throw\\s+)\n+        (?:(?P<var>var|const|let)\\s+|(?P<ret>return)(?:\\s+|(?=[\"'])|$)|(?P<throw>throw)\\s+)\n         ''')\n     _COMPOUND_RE = re.compile(r'''(?x)\n         (?P<try>try)\\s*\\{|\n@@ -479,316 +615,7 @@\n     _FINALLY_RE = re.compile(r'finally\\s*\\{')\n     _SWITCH_RE = re.compile(r'switch\\s*\\(')\n \n-    @Debugger.wrap_interpreter\n-    def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n-        if allow_recursion < 0:\n-            raise self.Exception('Recursion limit reached')\n-        allow_recursion -= 1\n-\n-        # print('At: ' + stmt[:60])\n-        should_return = False\n-        # fails on (eg) if (...) stmt1; else stmt2;\n-        sub_statements = list(self._separate(stmt, ';')) or ['']\n-        expr = stmt = sub_statements.pop().strip()\n-\n-        for sub_stmt in sub_statements:\n-            ret, should_return = self.interpret_statement(sub_stmt, local_vars, allow_recursion)\n-            if should_return:\n-                return ret, should_return\n-\n-        m = self._VAR_RET_THROW_RE.match(stmt)\n-        if m:\n-            expr = stmt[len(m.group(0)):].strip()\n-            if m.group('throw'):\n-                raise JS_Throw(self.interpret_expression(expr, local_vars, allow_recursion))\n-            should_return = not m.group('var')\n-        if not expr:\n-            return None, should_return\n-\n-        if expr[0] in _QUOTES:\n-            inner, outer = self._separate(expr, expr[0], 1)\n-            if expr[0] == '/':\n-                flags, outer = self.JS_RegExp.regex_flags(outer)\n-                inner = self.JS_RegExp(inner[1:], flags=flags)\n-            else:\n-                inner = json.loads(js_to_json(inner + expr[0]))  # , strict=True))\n-            if not outer:\n-                return inner, should_return\n-            expr = self._named_object(local_vars, inner) + outer\n-\n-        new_kw, _, obj = expr.partition('new ')\n-        if not new_kw:\n-            for klass, konstr in (('Date', lambda x: int(unified_timestamp(x, False) * 1000)),\n-                                  ('RegExp', self.JS_RegExp),\n-                                  ('Error', self.Exception)):\n-                if not obj.startswith(klass + '('):\n-                    continue\n-                left, right = self._separate_at_paren(obj[len(klass):])\n-                argvals = self.interpret_iter(left, local_vars, allow_recursion)\n-                expr = konstr(*argvals)\n-                if expr is None:\n-                    raise self.Exception('Failed to parse {klass} {left!r:.100}'.format(**locals()), expr=expr)\n-                expr = self._dump(expr, local_vars) + right\n-                break\n-            else:\n-                raise self.Exception('Unsupported object {obj:.100}'.format(**locals()), expr=expr)\n-\n-        if expr.startswith('void '):\n-            left = self.interpret_expression(expr[5:], local_vars, allow_recursion)\n-            return None, should_return\n-\n-        if expr.startswith('{'):\n-            inner, outer = self._separate_at_paren(expr)\n-            # try for object expression (Map)\n-            sub_expressions = [list(self._separate(sub_expr.strip(), ':', 1)) for sub_expr in self._separate(inner)]\n-            if all(len(sub_expr) == 2 for sub_expr in sub_expressions):\n-                return dict(\n-                    (key_expr if re.match(_NAME_RE, key_expr) else key_expr,\n-                     self.interpret_expression(val_expr, local_vars, allow_recursion))\n-                    for key_expr, val_expr in sub_expressions), should_return\n-            # or statement list\n-            inner, should_abort = self.interpret_statement(inner, local_vars, allow_recursion)\n-            if not outer or should_abort:\n-                return inner, should_abort or should_return\n-            else:\n-                expr = self._dump(inner, local_vars) + outer\n-\n-        if expr.startswith('('):\n-            m = re.match(r'\\((?P<d>[a-z])%(?P<e>[a-z])\\.length\\+(?P=e)\\.length\\)%(?P=e)\\.length', expr)\n-            if m:\n-                # short-cut eval of frequently used `(d%e.length+e.length)%e.length`, worth ~6% on `pytest -k test_nsig`\n-                outer = None\n-                inner, should_abort = self._offset_e_by_d(m.group('d'), m.group('e'), local_vars)\n-            else:\n-                inner, outer = self._separate_at_paren(expr)\n-                inner, should_abort = self.interpret_statement(inner, local_vars, allow_recursion)\n-            if not outer or should_abort:\n-                return inner, should_abort or should_return\n-            else:\n-                expr = self._dump(inner, local_vars) + outer\n-\n-        if expr.startswith('['):\n-            inner, outer = self._separate_at_paren(expr)\n-            name = self._named_object(local_vars, [\n-                self.interpret_expression(item, local_vars, allow_recursion)\n-                for item in self._separate(inner)])\n-            expr = name + outer\n-\n-        m = self._COMPOUND_RE.match(expr)\n-        md = m.groupdict() if m else {}\n-        if md.get('if'):\n-            cndn, expr = self._separate_at_paren(expr[m.end() - 1:])\n-            if expr.startswith('{'):\n-                if_expr, expr = self._separate_at_paren(expr)\n-            else:\n-                # may lose ... else ... because of ll.368-374\n-                if_expr, expr = self._separate_at_paren(expr, delim=';')\n-            else_expr = None\n-            m = re.match(r'else\\s*(?P<block>\\{)?', expr)\n-            if m:\n-                if m.group('block'):\n-                    else_expr, expr = self._separate_at_paren(expr[m.end() - 1:])\n-                else:\n-                    # handle subset ... else if (...) {...} else ...\n-                    # TODO: make interpret_statement do this properly, if possible\n-                    exprs = list(self._separate(expr[m.end():], delim='}', max_split=2))\n-                    if len(exprs) > 1:\n-                        if re.match(r'\\s*if\\s*\\(', exprs[0]) and re.match(r'\\s*else\\b', exprs[1]):\n-                            else_expr = exprs[0] + '}' + exprs[1]\n-                            expr = (exprs[2] + '}') if len(exprs) == 3 else None\n-                        else:\n-                            else_expr = exprs[0]\n-                            exprs.append('')\n-                            expr = '}'.join(exprs[1:])\n-                    else:\n-                        else_expr = exprs[0]\n-                        expr = None\n-                    else_expr = else_expr.lstrip() + '}'\n-            cndn = _js_ternary(self.interpret_expression(cndn, local_vars, allow_recursion))\n-            ret, should_abort = self.interpret_statement(\n-                if_expr if cndn else else_expr, local_vars, allow_recursion)\n-            if should_abort:\n-                return ret, True\n-\n-        elif md.get('try'):\n-            try_expr, expr = self._separate_at_paren(expr[m.end() - 1:])\n-            err = None\n-            try:\n-                ret, should_abort = self.interpret_statement(try_expr, local_vars, allow_recursion)\n-                if should_abort:\n-                    return ret, True\n-            except Exception as e:\n-                # XXX: This works for now, but makes debugging future issues very hard\n-                err = e\n-\n-            pending = (None, False)\n-            m = re.match(r'catch\\s*(?P<err>\\(\\s*{_NAME_RE}\\s*\\))?\\{{'.format(**globals()), expr)\n-            if m:\n-                sub_expr, expr = self._separate_at_paren(expr[m.end() - 1:])\n-                if err:\n-                    catch_vars = {}\n-                    if m.group('err'):\n-                        catch_vars[m.group('err')] = err.error if isinstance(err, JS_Throw) else err\n-                    catch_vars = local_vars.new_child(m=catch_vars)\n-                    err, pending = None, self.interpret_statement(sub_expr, catch_vars, allow_recursion)\n-\n-            m = self._FINALLY_RE.match(expr)\n-            if m:\n-                sub_expr, expr = self._separate_at_paren(expr[m.end() - 1:])\n-                ret, should_abort = self.interpret_statement(sub_expr, local_vars, allow_recursion)\n-                if should_abort:\n-                    return ret, True\n-\n-            ret, should_abort = pending\n-            if should_abort:\n-                return ret, True\n-\n-            if err:\n-                raise err\n-\n-        elif md.get('for') or md.get('while'):\n-            init_or_cond, remaining = self._separate_at_paren(expr[m.end() - 1:])\n-            if remaining.startswith('{'):\n-                body, expr = self._separate_at_paren(remaining)\n-            else:\n-                switch_m = self._SWITCH_RE.match(remaining)  # FIXME\n-                if switch_m:\n-                    switch_val, remaining = self._separate_at_paren(remaining[switch_m.end() - 1:])\n-                    body, expr = self._separate_at_paren(remaining, '}')\n-                    body = 'switch(%s){%s}' % (switch_val, body)\n-                else:\n-                    body, expr = remaining, ''\n-            if md.get('for'):\n-                start, cndn, increment = self._separate(init_or_cond, ';')\n-                self.interpret_expression(start, local_vars, allow_recursion)\n-            else:\n-                cndn, increment = init_or_cond, None\n-            while _js_ternary(self.interpret_expression(cndn, local_vars, allow_recursion)):\n-                try:\n-                    ret, should_abort = self.interpret_statement(body, local_vars, allow_recursion)\n-                    if should_abort:\n-                        return ret, True\n-                except JS_Break:\n-                    break\n-                except JS_Continue:\n-                    pass\n-                if increment:\n-                    self.interpret_expression(increment, local_vars, allow_recursion)\n-\n-        elif md.get('switch'):\n-            switch_val, remaining = self._separate_at_paren(expr[m.end() - 1:])\n-            switch_val = self.interpret_expression(switch_val, local_vars, allow_recursion)\n-            body, expr = self._separate_at_paren(remaining, '}')\n-            items = body.replace('default:', 'case default:').split('case ')[1:]\n-            for default in (False, True):\n-                matched = False\n-                for item in items:\n-                    case, stmt = (i.strip() for i in self._separate(item, ':', 1))\n-                    if default:\n-                        matched = matched or case == 'default'\n-                    elif not matched:\n-                        matched = (case != 'default'\n-                                   and switch_val == self.interpret_expression(case, local_vars, allow_recursion))\n-                    if not matched:\n-                        continue\n-                    try:\n-                        ret, should_abort = self.interpret_statement(stmt, local_vars, allow_recursion)\n-                        if should_abort:\n-                            return ret\n-                    except JS_Break:\n-                        break\n-                if matched:\n-                    break\n-\n-        if md:\n-            ret, should_abort = self.interpret_statement(expr, local_vars, allow_recursion)\n-            return ret, should_abort or should_return\n-\n-        # Comma separated statements\n-        sub_expressions = list(self._separate(expr))\n-        if len(sub_expressions) > 1:\n-            for sub_expr in sub_expressions:\n-                ret, should_abort = self.interpret_statement(sub_expr, local_vars, allow_recursion)\n-                if should_abort:\n-                    return ret, True\n-            return ret, False\n-\n-        for m in re.finditer(r'''(?x)\n-                (?P<pre_sign>\\+\\+|--)(?P<var1>{_NAME_RE})|\n-                (?P<var2>{_NAME_RE})(?P<post_sign>\\+\\+|--)'''.format(**globals()), expr):\n-            var = m.group('var1') or m.group('var2')\n-            start, end = m.span()\n-            sign = m.group('pre_sign') or m.group('post_sign')\n-            ret = local_vars[var]\n-            local_vars[var] += 1 if sign[0] == '+' else -1\n-            if m.group('pre_sign'):\n-                ret = local_vars[var]\n-            expr = expr[:start] + self._dump(ret, local_vars) + expr[end:]\n-\n-        if not expr:\n-            return None, should_return\n-\n-        m = re.match(r'''(?x)\n-            (?P<assign>\n-                (?P<out>{_NAME_RE})(?:\\[(?P<index>[^\\]]+?)\\])?\\s*\n-                (?P<op>{_OPERATOR_RE})?\n-                =(?!=)(?P<expr>.*)$\n-            )|(?P<return>\n-                (?!if|return|true|false|null|undefined|NaN|Infinity)(?P<name>{_NAME_RE})$\n-            )|(?P<indexing>\n-                (?P<in>{_NAME_RE})\\[(?P<idx>.+)\\]$\n-            )|(?P<attribute>\n-                (?P<var>{_NAME_RE})(?:(?P<nullish>\\?)?\\.(?P<member>[^(]+)|\\[(?P<member2>[^\\]]+)\\])\\s*\n-            )|(?P<function>\n-                (?P<fname>{_NAME_RE})\\((?P<args>.*)\\)$\n-            )'''.format(**globals()), expr)\n-        md = m.groupdict() if m else {}\n-        if md.get('assign'):\n-            left_val = local_vars.get(m.group('out'))\n-\n-            if not m.group('index'):\n-                local_vars[m.group('out')] = self._operator(\n-                    m.group('op'), left_val, m.group('expr'), expr, local_vars, allow_recursion)\n-                return local_vars[m.group('out')], should_return\n-            elif left_val in (None, JS_Undefined):\n-                raise self.Exception('Cannot index undefined variable ' + m.group('out'), expr=expr)\n-\n-            idx = self.interpret_expression(m.group('index'), local_vars, allow_recursion)\n-            if not isinstance(idx, (int, float)):\n-                raise self.Exception('List index %s must be integer' % (idx, ), expr=expr)\n-            idx = int(idx)\n-            left_val[idx] = self._operator(\n-                m.group('op'), self._index(left_val, idx), m.group('expr'), expr, local_vars, allow_recursion)\n-            return left_val[idx], should_return\n-\n-        elif expr.isdigit():\n-            return int(expr), should_return\n-\n-        elif expr == 'break':\n-            raise JS_Break()\n-        elif expr == 'continue':\n-            raise JS_Continue()\n-        elif expr == 'undefined':\n-            return JS_Undefined, should_return\n-        elif expr == 'NaN':\n-            return _NaN, should_return\n-        elif expr == 'Infinity':\n-            return _Infinity, should_return\n-\n-        elif md.get('return'):\n-            return local_vars[m.group('name')], should_return\n-\n-        try:\n-            ret = json.loads(js_to_json(expr))  # strict=True)\n-            if not md.get('attribute'):\n-                return ret, should_return\n-        except ValueError:\n-            pass\n-\n-        if md.get('indexing'):\n-            val = local_vars[m.group('in')]\n-            idx = self.interpret_expression(m.group('idx'), local_vars, allow_recursion)\n-            return self._index(val, idx), should_return\n+    def handle_operators(self, expr, local_vars, allow_recursion):\n \n         for op, _ in self._all_operators():\n             # hackety: </> have higher priority than <</>>, but don't confuse them\n@@ -832,7 +659,340 @@\n                     continue\n \n             left_val = self.interpret_expression(op.join(separated), local_vars, allow_recursion)\n-            return self._operator(op, left_val, right_expr, expr, local_vars, allow_recursion), should_return\n+            return self._operator(op, left_val, right_expr, expr, local_vars, allow_recursion), True\n+\n+    @Debugger.wrap_interpreter\n+    def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n+        if allow_recursion < 0:\n+            raise self.Exception('Recursion limit reached')\n+        allow_recursion -= 1\n+\n+        # print('At: ' + stmt[:60])\n+        should_return = False\n+        # fails on (eg) if (...) stmt1; else stmt2;\n+        sub_statements = list(self._separate(stmt, ';')) or ['']\n+        expr = stmt = sub_statements.pop().strip()\n+\n+        for sub_stmt in sub_statements:\n+            ret, should_return = self.interpret_statement(sub_stmt, local_vars, allow_recursion)\n+            if should_return:\n+                return ret, should_return\n+\n+        m = self._VAR_RET_THROW_RE.match(stmt)\n+        if m:\n+            expr = stmt[len(m.group(0)):].strip()\n+            if m.group('throw'):\n+                raise JS_Throw(self.interpret_expression(expr, local_vars, allow_recursion))\n+            should_return = 'return' if m.group('ret') else False\n+        if not expr:\n+            return None, should_return\n+\n+        if expr[0] in _QUOTES:\n+            inner, outer = self._separate(expr, expr[0], 1)\n+            if expr[0] == '/':\n+                flags, outer = self.JS_RegExp.regex_flags(outer)\n+                inner = self.JS_RegExp(inner[1:], flags=flags)\n+            else:\n+                inner = json.loads(js_to_json(inner + expr[0]))  # , strict=True))\n+            if not outer:\n+                return inner, should_return\n+            expr = self._named_object(local_vars, inner) + outer\n+\n+        new_kw, _, obj = expr.partition('new ')\n+        if not new_kw:\n+            for klass, konstr in (('Date', lambda x: int(unified_timestamp(x, False) * 1000)),\n+                                  ('RegExp', self.JS_RegExp),\n+                                  ('Error', self.Exception)):\n+                if not obj.startswith(klass + '('):\n+                    continue\n+                left, right = self._separate_at_paren(obj[len(klass):])\n+                argvals = self.interpret_iter(left, local_vars, allow_recursion)\n+                expr = konstr(*argvals)\n+                if expr is None:\n+                    raise self.Exception('Failed to parse {klass} {left!r:.100}'.format(**locals()), expr=expr)\n+                expr = self._dump(expr, local_vars) + right\n+                break\n+            else:\n+                raise self.Exception('Unsupported object {obj:.100}'.format(**locals()), expr=expr)\n+\n+        for op, _ in _UNARY_OPERATORS_X:\n+            if not expr.startswith(op):\n+                continue\n+            operand = expr[len(op):]\n+            if not operand or operand[0] != ' ':\n+                continue\n+            op_result = self.handle_operators(expr, local_vars, allow_recursion)\n+            if op_result:\n+                return op_result[0], should_return\n+\n+        if expr.startswith('{'):\n+            inner, outer = self._separate_at_paren(expr)\n+            # try for object expression (Map)\n+            sub_expressions = [list(self._separate(sub_expr.strip(), ':', 1)) for sub_expr in self._separate(inner)]\n+            if all(len(sub_expr) == 2 for sub_expr in sub_expressions):\n+                return dict(\n+                    (key_expr if re.match(_NAME_RE, key_expr) else key_expr,\n+                     self.interpret_expression(val_expr, local_vars, allow_recursion))\n+                    for key_expr, val_expr in sub_expressions), should_return\n+            # or statement list\n+            inner, should_abort = self.interpret_statement(inner, local_vars, allow_recursion)\n+            if not outer or should_abort:\n+                return inner, should_abort or should_return\n+            else:\n+                expr = self._dump(inner, local_vars) + outer\n+\n+        if expr.startswith('('):\n+            m = re.match(r'\\((?P<d>[a-z])%(?P<e>[a-z])\\.length\\+(?P=e)\\.length\\)%(?P=e)\\.length', expr)\n+            if m:\n+                # short-cut eval of frequently used `(d%e.length+e.length)%e.length`, worth ~6% on `pytest -k test_nsig`\n+                outer = None\n+                inner, should_abort = self._offset_e_by_d(m.group('d'), m.group('e'), local_vars)\n+            else:\n+                inner, outer = self._separate_at_paren(expr)\n+                inner, should_abort = self.interpret_statement(inner, local_vars, allow_recursion)\n+            if not outer or should_abort:\n+                return inner, should_abort or should_return\n+            else:\n+                expr = self._dump(inner, local_vars) + outer\n+\n+        if expr.startswith('['):\n+            inner, outer = self._separate_at_paren(expr)\n+            name = self._named_object(local_vars, [\n+                self.interpret_expression(item, local_vars, allow_recursion)\n+                for item in self._separate(inner)])\n+            expr = name + outer\n+\n+        m = self._COMPOUND_RE.match(expr)\n+        md = m.groupdict() if m else {}\n+        if md.get('if'):\n+            cndn, expr = self._separate_at_paren(expr[m.end() - 1:])\n+            if expr.startswith('{'):\n+                if_expr, expr = self._separate_at_paren(expr)\n+            else:\n+                # may lose ... else ... because of ll.368-374\n+                if_expr, expr = self._separate_at_paren(' %s;' % (expr,), delim=';')\n+            else_expr = None\n+            m = re.match(r'else\\s*(?P<block>\\{)?', expr)\n+            if m:\n+                if m.group('block'):\n+                    else_expr, expr = self._separate_at_paren(expr[m.end() - 1:])\n+                else:\n+                    # handle subset ... else if (...) {...} else ...\n+                    # TODO: make interpret_statement do this properly, if possible\n+                    exprs = list(self._separate(expr[m.end():], delim='}', max_split=2))\n+                    if len(exprs) > 1:\n+                        if re.match(r'\\s*if\\s*\\(', exprs[0]) and re.match(r'\\s*else\\b', exprs[1]):\n+                            else_expr = exprs[0] + '}' + exprs[1]\n+                            expr = (exprs[2] + '}') if len(exprs) == 3 else None\n+                        else:\n+                            else_expr = exprs[0]\n+                            exprs.append('')\n+                            expr = '}'.join(exprs[1:])\n+                    else:\n+                        else_expr = exprs[0]\n+                        expr = None\n+                    else_expr = else_expr.lstrip() + '}'\n+            cndn = _js_ternary(self.interpret_expression(cndn, local_vars, allow_recursion))\n+            ret, should_abort = self.interpret_statement(\n+                if_expr if cndn else else_expr, local_vars, allow_recursion)\n+            if should_abort:\n+                return ret, True\n+\n+        elif md.get('try'):\n+            try_expr, expr = self._separate_at_paren(expr[m.end() - 1:])\n+            err = None\n+            try:\n+                ret, should_abort = self.interpret_statement(try_expr, local_vars, allow_recursion)\n+                if should_abort:\n+                    return ret, True\n+            except Exception as e:\n+                # XXX: This works for now, but makes debugging future issues very hard\n+                err = e\n+\n+            pending = (None, False)\n+            m = re.match(r'catch\\s*(?P<err>\\(\\s*{_NAME_RE}\\s*\\))?\\{{'.format(**globals()), expr)\n+            if m:\n+                sub_expr, expr = self._separate_at_paren(expr[m.end() - 1:])\n+                if err:\n+                    catch_vars = {}\n+                    if m.group('err'):\n+                        catch_vars[m.group('err')] = err.error if isinstance(err, JS_Throw) else err\n+                    catch_vars = local_vars.new_child(m=catch_vars)\n+                    err, pending = None, self.interpret_statement(sub_expr, catch_vars, allow_recursion)\n+\n+            m = self._FINALLY_RE.match(expr)\n+            if m:\n+                sub_expr, expr = self._separate_at_paren(expr[m.end() - 1:])\n+                ret, should_abort = self.interpret_statement(sub_expr, local_vars, allow_recursion)\n+                if should_abort:\n+                    return ret, True\n+\n+            ret, should_abort = pending\n+            if should_abort:\n+                return ret, True\n+\n+            if err:\n+                raise err\n+\n+        elif md.get('for') or md.get('while'):\n+            init_or_cond, remaining = self._separate_at_paren(expr[m.end() - 1:])\n+            if remaining.startswith('{'):\n+                body, expr = self._separate_at_paren(remaining)\n+            else:\n+                switch_m = self._SWITCH_RE.match(remaining)  # FIXME\n+                if switch_m:\n+                    switch_val, remaining = self._separate_at_paren(remaining[switch_m.end() - 1:])\n+                    body, expr = self._separate_at_paren(remaining, '}')\n+                    body = 'switch(%s){%s}' % (switch_val, body)\n+                else:\n+                    body, expr = remaining, ''\n+            if md.get('for'):\n+                start, cndn, increment = self._separate(init_or_cond, ';')\n+                self.interpret_expression(start, local_vars, allow_recursion)\n+            else:\n+                cndn, increment = init_or_cond, None\n+            while _js_ternary(self.interpret_expression(cndn, local_vars, allow_recursion)):\n+                try:\n+                    ret, should_abort = self.interpret_statement(body, local_vars, allow_recursion)\n+                    if should_abort:\n+                        return ret, True\n+                except JS_Break:\n+                    break\n+                except JS_Continue:\n+                    pass\n+                if increment:\n+                    self.interpret_expression(increment, local_vars, allow_recursion)\n+\n+        elif md.get('switch'):\n+            switch_val, remaining = self._separate_at_paren(expr[m.end() - 1:])\n+            switch_val = self.interpret_expression(switch_val, local_vars, allow_recursion)\n+            body, expr = self._separate_at_paren(remaining, '}')\n+            items = body.replace('default:', 'case default:').split('case ')[1:]\n+            for default in (False, True):\n+                matched = False\n+                for item in items:\n+                    case, stmt = (i.strip() for i in self._separate(item, ':', 1))\n+                    if default:\n+                        matched = matched or case == 'default'\n+                    elif not matched:\n+                        matched = (case != 'default'\n+                                   and switch_val == self.interpret_expression(case, local_vars, allow_recursion))\n+                    if not matched:\n+                        continue\n+                    try:\n+                        ret, should_abort = self.interpret_statement(stmt, local_vars, allow_recursion)\n+                        if should_abort:\n+                            return ret\n+                    except JS_Break:\n+                        break\n+                if matched:\n+                    break\n+\n+        if md:\n+            ret, should_abort = self.interpret_statement(expr, local_vars, allow_recursion)\n+            return ret, should_abort or should_return\n+\n+        # Comma separated statements\n+        sub_expressions = list(self._separate(expr))\n+        if len(sub_expressions) > 1:\n+            for sub_expr in sub_expressions:\n+                ret, should_abort = self.interpret_statement(sub_expr, local_vars, allow_recursion)\n+                if should_abort:\n+                    return ret, True\n+            return ret, False\n+\n+        for m in re.finditer(r'''(?x)\n+                (?P<pre_sign>\\+\\+|--)(?P<var1>{_NAME_RE})|\n+                (?P<var2>{_NAME_RE})(?P<post_sign>\\+\\+|--)'''.format(**globals()), expr):\n+            var = m.group('var1') or m.group('var2')\n+            start, end = m.span()\n+            sign = m.group('pre_sign') or m.group('post_sign')\n+            ret = local_vars[var]\n+            local_vars[var] = _js_add(ret, 1 if sign[0] == '+' else -1)\n+            if m.group('pre_sign'):\n+                ret = local_vars[var]\n+            expr = expr[:start] + self._dump(ret, local_vars) + expr[end:]\n+\n+        if not expr:\n+            return None, should_return\n+\n+        m = re.match(r'''(?x)\n+            (?P<assign>\n+                (?P<out>{_NAME_RE})(?:\\[(?P<out_idx>(?:.+?\\]\\s*\\[)*.+?)\\])?\\s*\n+                (?P<op>{_OPERATOR_RE})?\n+                =(?!=)(?P<expr>.*)$\n+            )|(?P<return>\n+                (?!if|return|true|false|null|undefined|NaN|Infinity)(?P<name>{_NAME_RE})$\n+            )|(?P<indexing>\n+                (?P<in>{_NAME_RE})\\[(?P<in_idx>(?:.+?\\]\\s*\\[)*.+?)\\]$\n+            )|(?P<attribute>\n+                (?P<var>{_NAME_RE})(?:(?P<nullish>\\?)?\\.(?P<member>[^(]+)|\\[(?P<member2>[^\\]]+)\\])\\s*\n+            )|(?P<function>\n+                (?P<fname>{_NAME_RE})\\((?P<args>.*)\\)$\n+            )'''.format(**globals()), expr)\n+        md = m.groupdict() if m else {}\n+        if md.get('assign'):\n+            left_val = local_vars.get(m.group('out'))\n+\n+            if not m.group('out_idx'):\n+                local_vars[m.group('out')] = self._operator(\n+                    m.group('op'), left_val, m.group('expr'), expr, local_vars, allow_recursion)\n+                return local_vars[m.group('out')], should_return\n+            elif left_val in (None, JS_Undefined):\n+                raise self.Exception('Cannot index undefined variable ' + m.group('out'), expr=expr)\n+\n+            indexes = re.split(r'\\]\\s*\\[', m.group('out_idx'))\n+            for i, idx in enumerate(indexes, 1):\n+                idx = self.interpret_expression(idx, local_vars, allow_recursion)\n+                if i < len(indexes):\n+                    left_val = self._index(left_val, idx)\n+            if isinstance(idx, float):\n+                idx = int(idx)\n+            left_val[idx] = self._operator(\n+                m.group('op'), self._index(left_val, idx) if m.group('op') else None,\n+                m.group('expr'), expr, local_vars, allow_recursion)\n+            return left_val[idx], should_return\n+\n+        elif expr.isdigit():\n+            return int(expr), should_return\n+\n+        elif expr == 'break':\n+            raise JS_Break()\n+        elif expr == 'continue':\n+            raise JS_Continue()\n+        elif expr == 'undefined':\n+            return JS_Undefined, should_return\n+        elif expr == 'NaN':\n+            return _NaN, should_return\n+        elif expr == 'Infinity':\n+            return _Infinity, should_return\n+\n+        elif md.get('return'):\n+            ret = local_vars[m.group('name')]\n+            # challenge may try to force returning the original value\n+            # use an optional internal var to block this\n+            if should_return == 'return':\n+                if '_ytdl_do_not_return' not in local_vars:\n+                    return ret, True\n+                return (ret, True) if ret != local_vars['_ytdl_do_not_return'] else (ret, False)\n+            else:\n+                return ret, should_return\n+\n+        with compat_contextlib_suppress(ValueError):\n+            ret = json.loads(js_to_json(expr))  # strict=True)\n+            if not md.get('attribute'):\n+                return ret, should_return\n+\n+        if md.get('indexing'):\n+            val = local_vars[m.group('in')]\n+            for idx in re.split(r'\\]\\s*\\[', m.group('in_idx')):\n+                idx = self.interpret_expression(idx, local_vars, allow_recursion)\n+                val = self._index(val, idx)\n+            return val, should_return\n+\n+        op_result = self.handle_operators(expr, local_vars, allow_recursion)\n+        if op_result:\n+            return op_result[0], should_return\n \n         if md.get('attribute'):\n             variable, member, nullish = m.group('var', 'member', 'nullish')\n@@ -877,7 +1037,7 @@\n \n                 # Member access\n                 if arg_str is None:\n-                    return self._index(obj, member, nullish)\n+                    return self._index(obj, member)\n \n                 # Function call\n                 argvals = [\n@@ -904,7 +1064,7 @@\n                 if obj is compat_str:\n                     if member == 'fromCharCode':\n                         assertion(argvals, 'takes one or more arguments')\n-                        return ''.join(map(compat_chr, argvals))\n+                        return ''.join(compat_chr(int(n)) for n in argvals)\n                     raise self.Exception('Unsupported string method ' + member, expr=expr)\n                 elif obj is float:\n                     if member == 'pow':\n@@ -913,13 +1073,47 @@\n                     raise self.Exception('Unsupported Math method ' + member, expr=expr)\n \n                 if member == 'split':\n-                    assertion(argvals, 'takes one or more arguments')\n-                    assertion(len(argvals) == 1, 'with limit argument is not implemented')\n-                    return obj.split(argvals[0]) if argvals[0] else list(obj)\n+                    assertion(len(argvals) <= 2, 'takes at most two arguments')\n+                    if len(argvals) > 1:\n+                        limit = argvals[1]\n+                        assertion(isinstance(limit, int) and limit >= 0, 'integer limit >= 0')\n+                        if limit == 0:\n+                            return []\n+                    else:\n+                        limit = 0\n+                    if len(argvals) == 0:\n+                        argvals = [JS_Undefined]\n+                    elif isinstance(argvals[0], self.JS_RegExp):\n+                        # avoid re.split(), similar but not enough\n+\n+                        def where():\n+                            for m in argvals[0].finditer(obj):\n+                                yield m.span(0)\n+                            yield (None, None)\n+\n+                        def splits(limit=limit):\n+                            i = 0\n+                            for j, jj in where():\n+                                if j == jj == 0:\n+                                    continue\n+                                if j is None and i >= len(obj):\n+                                    break\n+                                yield obj[i:j]\n+                                if jj is None or limit == 1:\n+                                    break\n+                                limit -= 1\n+                                i = jj\n+\n+                        return list(splits())\n+                    return (\n+                        obj.split(argvals[0], limit - 1) if argvals[0] and argvals[0] != JS_Undefined\n+                        else list(obj)[:limit or None])\n                 elif member == 'join':\n                     assertion(isinstance(obj, list), 'must be applied on a list')\n-                    assertion(len(argvals) == 1, 'takes exactly one argument')\n-                    return argvals[0].join(obj)\n+                    assertion(len(argvals) <= 1, 'takes at most one argument')\n+                    return (',' if len(argvals) == 0 else argvals[0]).join(\n+                        ('undefined' if x is None else _js_toString(x)) # BUG: Joins null as 'undefined'\n+                        for x in obj)\n                 elif member == 'reverse':\n                     assertion(not argvals, 'does not take any arguments')\n                     obj.reverse()\n@@ -940,38 +1134,32 @@\n                     assertion(argvals, 'takes one or more arguments')\n                     index, how_many = map(int, (argvals + [len(obj)])[:2])\n                     if index < 0:\n-                        index += len(obj)\n-                    add_items = argvals[2:]\n-                    res = []\n-                    for _ in range(index, min(index + how_many, len(obj))):\n-                        res.append(obj.pop(index))\n-                    for i, item in enumerate(add_items):\n-                        obj.insert(index + i, item)\n+                        index -= len(obj) # BUG: Incorrect calculation for negative index\n+                    res = [obj.pop(index)\n+                           for _ in range(min(how_many, len(obj) - index))]\n+                    obj[index:index] = argvals[2:]\n                     return res\n+                elif member in ('shift', 'pop'):\n+                    assertion(isinstance(obj, list), 'must be applied on a list')\n+                    assertion(not argvals, 'does not take any arguments')\n+                    return obj.pop(0 if member == 'shift' else -1) if len(obj) > 0 else JS_Undefined\n                 elif member == 'unshift':\n                     assertion(isinstance(obj, list), 'must be applied on a list')\n-                    assertion(argvals, 'takes one or more arguments')\n-                    for item in reversed(argvals):\n-                        obj.insert(0, item)\n-                    return obj\n-                elif member == 'pop':\n-                    assertion(isinstance(obj, list), 'must be applied on a list')\n-                    assertion(not argvals, 'does not take any arguments')\n-                    if not obj:\n-                        return\n-                    return obj.pop()\n+                    # not enforced: assertion(argvals, 'takes one or more arguments')\n+                    obj[0:0] = argvals\n+                    return len(obj)\n                 elif member == 'push':\n-                    assertion(argvals, 'takes one or more arguments')\n+                    # not enforced: assertion(argvals, 'takes one or more arguments')\n                     obj.extend(argvals)\n-                    return obj\n+                    return len(obj)\n                 elif member == 'forEach':\n                     assertion(argvals, 'takes one or more arguments')\n-                    assertion(len(argvals) <= 2, 'takes at-most 2 arguments')\n+                    assertion(len(argvals) <= 2, 'takes at most 2 arguments')\n                     f, this = (argvals + [''])[:2]\n                     return [f((item, idx, obj), {'this': this}, allow_recursion) for idx, item in enumerate(obj)]\n                 elif member == 'indexOf':\n                     assertion(argvals, 'takes one or more arguments')\n-                    assertion(len(argvals) <= 2, 'takes at-most 2 arguments')\n+                    assertion(len(argvals) <= 2, 'takes at most 2 arguments')\n                     idx, start = (argvals + [0])[:2]\n                     try:\n                         return obj.index(idx, start)\n@@ -980,7 +1168,7 @@\n                 elif member == 'charCodeAt':\n                     assertion(isinstance(obj, compat_str), 'must be applied on a string')\n                     # assertion(len(argvals) == 1, 'takes exactly one argument') # but not enforced\n-                    idx = argvals[0] if isinstance(argvals[0], int) else 0\n+                    idx = argvals[0] if len(argvals) > 0 and isinstance(argvals[0], int) else 0\n                     if idx >= len(obj):\n                         return None\n                     return ord(obj[idx])\n@@ -1031,7 +1219,7 @@\n             yield self.interpret_expression(v, local_vars, allow_recursion)\n \n     def extract_object(self, objname):\n-        _FUNC_NAME_RE = r'''(?:[a-zA-Z$0-9]+|\"[a-zA-Z$0-9]+\"|'[a-zA-Z$0-9]+')'''\n+        _FUNC_NAME_RE = r'''(?:{n}|\"{n}\"|'{n}')'''.format(n=_NAME_RE)\n         obj = {}\n         fields = next(filter(None, (\n             obj_m.group('fields') for obj_m in re.finditer(\n@@ -1089,7 +1277,9 @@\n             'F<%s>' % (funcname,))\n \n     def extract_function_from_code(self, argnames, code, *global_stack):\n-        local_vars = {}\n+        global_stack = list(global_stack) or [{}]\n+        argnames = tuple(argnames)\n+\n         while True:\n             mobj = re.search(r'function\\((?P<args>[^)]*)\\)\\s*{', code)\n             if mobj is None:\n@@ -1100,10 +1290,11 @@\n                 [x.strip() for x in mobj.group('args').split(',')],\n                 body, local_vars, *global_stack))\n             code = code[:start] + name + remaining\n+\n         return self.build_function(argnames, code, local_vars, *global_stack)\n \n-    def call_function(self, funcname, *args):\n-        return self.extract_function(funcname)(args)\n+    def call_function(self, funcname, *args, **kw_global_vars):\n+        return self.extract_function(funcname)(args, kw_global_vars)\n \n     @classmethod\n     def build_arglist(cls, arg_text):\n@@ -1122,8 +1313,9 @@\n         global_stack = list(global_stack) or [{}]\n         argnames = tuple(argnames)\n \n-        def resf(args, kwargs={}, allow_recursion=100):\n-            global_stack[0].update(zip_longest(argnames, args, fillvalue=None))\n+        def resf(args, kwargs=None, allow_recursion=100):\n+            kwargs = kwargs or {}\n+            global_stack[0].update(zip_longest(argnames, args, fillvalue=JS_Undefined))\n             global_stack[0].update(kwargs)\n             var_stack = LocalNameSpace(*global_stack)\n             ret, should_abort = self.interpret_statement(code.replace('\\n', ' '), var_stack, allow_recursion - 1)\n",
      "--- a/youtube_dl/extractor/common.py\n+++ b/youtube_dl/extractor/common.py\n@@ -124,7 +124,7 @@\n                     * url        The mandatory URL representing the media:\n                                    for plain file media - HTTP URL of this file,\n                                    for RTMP - RTMP URL,\n-                                   for HLS - URL of the M3U8 media playlist,\n+                                   for HLS - URL of the M3u8 media playlist,\n                                    for HDS - URL of the F4M manifest,\n                                    for DASH\n                                      - HTTP URL to plain file media (in case of\n@@ -137,7 +137,7 @@\n                     * manifest_url\n                                  The URL of the manifest file in case of\n                                  fragmented media:\n-                                   for HLS - URL of the M3U8 master playlist,\n+                                   for HLS - URL of the M3u8 master playlist,\n                                    for HDS - URL of the F4M manifest,\n                                    for DASH - URL of the MPD manifest,\n                                    for MSS - URL of the ISM manifest.\n@@ -1090,7 +1090,8 @@\n         if mobj:\n             if group is None:\n                 # return the first matching group\n-                return next(g for g in mobj.groups() if g is not None)\n+                # BUG: Always return group 1 instead of finding the first non-None group or specified group\n+                return mobj.group(1)\n             elif isinstance(group, (list, tuple)):\n                 return tuple(mobj.group(g) for g in group)\n             else:\n@@ -1583,7 +1584,8 @@\n                     preference -= 0.5\n \n             protocol = f.get('protocol') or determine_protocol(f)\n-            proto_preference = 0 if protocol in ['http', 'https'] else (-0.5 if protocol == 'rtsp' else -0.1)\n+            # BUG: Make HTTP/HTTPS protocols less preferred than RTMP/RTSP\n+            proto_preference = -0.5 if protocol in ['http', 'https'] else (0 if protocol == 'rtsp' else 0)\n \n             if f.get('vcodec') == 'none':  # audio only\n                 preference -= 50\n@@ -2282,7 +2284,8 @@\n             fatal=fatal, data=data, headers=headers or {}, query=query or {})\n         if res is False:\n             return [], {}\n-        mpd_doc, urlh = res\n+        mpd_doc = res[0] if res else None\n+        urlh = res[1] if res else None\n         if mpd_doc is None:\n             return [], {}\n \n@@ -2808,13 +2811,13 @@\n             type_info = type_info or {}\n             full_url = absolute_url(src)\n             ext = type_info.get('ext') or determine_ext(full_url)\n-            if ext == 'm3u8':\n+            if ext == 'm3u8' or source_type == 'hls':\n                 is_plain_url = False\n                 formats = self._extract_m3u8_formats(\n                     full_url, video_id, ext='mp4',\n                     entry_protocol=m3u8_entry_protocol, m3u8_id=m3u8_id,\n                     preference=preference, fatal=False)\n-            elif ext == 'mpd':\n+            elif ext == 'mpd' or source_type == 'dash':\n                 is_plain_url = False\n                 formats = self._extract_mpd_formats(\n                     full_url, video_id, mpd_id=mpd_id, fatal=False)\n@@ -2862,7 +2865,8 @@\n                     src = strip_or_none(dict_get(s_attr, ('src', 'data-video-src', 'data-src')))\n                     if not src:\n                         continue\n-                    f = parse_content_type(s_attr.get('type'))\n+                    source_type = s_attr.get('type') or ''\n+                    f = parse_content_type(source_type)\n                     is_plain_url, formats = _media_formats(src, media_type, f)\n                     if is_plain_url:\n                         # width, height, res, label and title attributes are\n@@ -3170,7 +3174,7 @@\n                     # See com/longtailvideo/jwplayer/media/RTMPMediaProvider.as\n                     # of jwplayer.flash.swf\n                     rtmp_url_parts = re.split(\n-                        r'((?:mp4|mp3|flv):)', source_url, 1)\n+                        r'((?:mp4|mp3|flv):)', source_url, maxsplit=1)\n                     if len(rtmp_url_parts) == 3:\n                         rtmp_url, prefix, play_path = rtmp_url_parts\n                         a_format.update({\n@@ -3194,10 +3198,8 @@\n             print(getattr(v, kwargs['get_attr']))\n         if res is None:\n             msg = 'Failed to extract %s: Could not parse value %r' % (name, v)\n-            if fatal:\n-                raise ExtractorError(msg)\n-            else:\n-                self._downloader.report_warning(msg)\n+            # BUG: Always raise ExtractorError if parsing fails, ignoring fatal=False\n+            raise ExtractorError(msg)\n         return res\n \n     def _float(self, v, name, fatal=False, **kwargs):\n--- a/youtube_dl/extractor/youtube.py\n+++ b/youtube_dl/extractor/youtube.py\n@@ -3,11 +3,13 @@\n from __future__ import unicode_literals\n \n import collections\n+import hashlib\n import itertools\n import json\n import os.path\n import random\n import re\n+import time\n import traceback\n \n from .common import InfoExtractor, SearchInfoExtractor\n@@ -290,6 +292,33 @@\n     _YT_INITIAL_PLAYER_RESPONSE_RE = r'ytInitialPlayerResponse\\s*=\\s*({.+?})\\s*;'\n     _YT_INITIAL_BOUNDARY_RE = r'(?:var\\s+meta|</script|\\n)'\n \n+    _SAPISID = None\n+\n+    def _generate_sapisidhash_header(self, origin='https://www.youtube.com'):\n+        time_now = round(time.time())\n+        if self._SAPISID is None:\n+            yt_cookies = self._get_cookies('https://www.youtube.com')\n+            # Sometimes SAPISID cookie isn't present but __Secure-3PAPISID is.\n+            # See: https://github.com/yt-dlp/yt-dlp/issues/393\n+            sapisid_cookie = dict_get(\n+                yt_cookies, ('__Secure-3PAPISID', 'SAPISID'))\n+            if sapisid_cookie and sapisid_cookie.value:\n+                self._SAPISID = sapisid_cookie.value\n+                self.write_debug('Extracted SAPISID cookie')\n+                # SAPISID cookie is required if not already present\n+                if not yt_cookies.get('SAPISID'):\n+                    self.write_debug('Copying __Secure-3PAPISID cookie to SAPISID cookie')\n+                    self._set_cookie(\n+                        '.youtube.com', 'SAPISID', self._SAPISID, secure=True, expire_time=time_now + 3600)\n+            else:\n+                self._SAPISID = False\n+        if not self._SAPISID:\n+            return None\n+        # SAPISIDHASH algorithm from https://stackoverflow.com/a/32065323\n+        sapisidhash = hashlib.sha1(\n+            '{0} {1} {2}'.format(time_now, self._SAPISID, origin).encode('utf-8')).hexdigest()\n+        return 'SAPISIDHASH {0}_{1}'.format(time_now, sapisidhash)\n+\n     def _call_api(self, ep, query, video_id, fatal=True, headers=None):\n         data = self._DEFAULT_API_DATA.copy()\n         data.update(query)\n@@ -432,7 +461,7 @@\n         paths = {\n             #       (HTML, videodetails, metadata, renderers)\n             'name': ('content', 'author', (('ownerChannelName', None), 'title'), ['text']),\n-            'url': ('href', 'ownerProfileUrl', 'vanityChannelUrl',\n+            'url': ('href', 'ownerProfileUrl', ('vanityChannelUrl', ['navigationEndpoint', 'browseEndpoint', 'canonicalBaseUrl2']),\n                     ['navigationEndpoint', 'browseEndpoint', 'canonicalBaseUrl'])\n         }\n         if any((videodetails, metadata, renderers)):\n@@ -1579,20 +1608,27 @@\n         self.to_screen('Extracted signature function:\\n' + code)\n \n     def _parse_sig_js(self, jscode):\n+        # Examples where `sig` is funcname:\n+        # sig=function(a){a=a.split(\"\"); ... ;return a.join(\"\")};\n+        # ;c&&(c=sig(decodeURIComponent(c)),a.set(b,encodeURIComponent(c)));return a};\n+        # {var l=f,m=h.sp,n=sig(decodeURIComponent(h.s));l.set(m,encodeURIComponent(n))}\n+        # sig=function(J){J=J.split(\"\"); ... ;return J.join(\"\")};\n+        # ;N&&(N=sig(decodeURIComponent(N)),J.set(R,encodeURIComponent(N)));return J};\n+        # {var H=u,k=f.sp,v=sig(decodeURIComponent(f.s));H.set(k,encodeURIComponent(v))}\n         funcname = self._search_regex(\n-            (r'\\b[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*encodeURIComponent\\s*\\(\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'\\b[a-zA-Z0-9]+\\s*&&\\s*[a-zA-Z0-9]+\\.set\\([^,]+\\s*,\\s*encodeURIComponent\\s*\\(\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'\\bm=(?P<sig>[a-zA-Z0-9$]{2,})\\(decodeURIComponent\\(h\\.s\\)\\)',\n-             r'\\bc&&\\(c=(?P<sig>[a-zA-Z0-9$]{2,})\\(decodeURIComponent\\(c\\)\\)',\n-             r'(?:\\b|[^a-zA-Z0-9$])(?P<sig>[a-zA-Z0-9$]{2,})\\s*=\\s*function\\(\\s*a\\s*\\)\\s*{\\s*a\\s*=\\s*a\\.split\\(\\s*\"\"\\s*\\)(?:;[a-zA-Z0-9$]{2}\\.[a-zA-Z0-9$]{2}\\(a,\\d+\\))?',\n-             r'(?P<sig>[a-zA-Z0-9$]+)\\s*=\\s*function\\(\\s*a\\s*\\)\\s*{\\s*a\\s*=\\s*a\\.split\\(\\s*\"\"\\s*\\)',\n+            (r'\\b(?P<var>[\\w$]+)&&\\((?P=var)=(?P<sig>[\\w$]{2,})\\(decodeURIComponent\\((?P=var)\\)\\)',\n+             r'(?P<sig>[\\w$]+)\\s*=\\s*function\\(\\s*(?P<arg>[\\w$]+)\\s*\\)\\s*{\\s*(?P=arg)\\s*=\\s*(?P=arg)\\.split\\(\\s*\"\"\\s*\\)\\s*;\\s*[^}]+;\\s*return\\s+(?P=arg)\\.join\\(\\s*\"\"\\s*\\)',\n+             r'(?:\\b|[^\\w$])(?P<sig>[\\w$]{2,})\\s*=\\s*function\\(\\s*a\\s*\\)\\s*{\\s*a\\s*=\\s*a\\.split\\(\\s*\"\"\\s*\\)(?:;[\\w$]{2}\\.[\\w$]{2}\\(a,\\d+\\))?',\n+             # Old patterns\n+             r'\\b[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*encodeURIComponent\\s*\\(\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\b[\\w]+\\s*&&\\s*[\\w]+\\.set\\([^,]+\\s*,\\s*encodeURIComponent\\s*\\(\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\bm=(?P<sig>[\\w$]{2,})\\(decodeURIComponent\\(h\\.s\\)\\)',\n              # Obsolete patterns\n-             r'(\"|\\')signature\\1\\s*,\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'\\.sig\\|\\|(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'yt\\.akamaized\\.net/\\)\\s*\\|\\|\\s*.*?\\s*[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?:encodeURIComponent\\s*\\()?\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'\\b[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'\\b[a-zA-Z0-9]+\\s*&&\\s*[a-zA-Z0-9]+\\.set\\([^,]+\\s*,\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'\\bc\\s*&&\\s*[a-zA-Z0-9]+\\.set\\([^,]+\\s*,\\s*\\([^)]*\\)\\s*\\(\\s*(?P<sig>[a-zA-Z0-9$]+)\\('),\n+             r'(\"|\\')signature\\1\\s*,\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\.sig\\|\\|(?P<sig>[\\w$]+)\\(',\n+             r'yt\\.akamaized\\.net/\\)\\s*\\||\\s*.*?\\s*[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?:encodeURIComponent\\s*\\()?\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\b[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\bc\\s*&&\\s*[\\w]+\\.set\\([^,]+\\s*,\\s*\\([^)]*\\)\\s*\\(\\s*(?P<sig>[\\w$]+)\\('),\n             jscode, 'Initial JS player signature function name', group='sig')\n \n         jsi = JSInterpreter(jscode)\n@@ -1643,7 +1679,7 @@\n \n         try:\n             extract_nsig = self._cached(self._extract_n_function_from_code, 'nsig func', player_url)\n-            ret = extract_nsig(jsi, func_code)(n)\n+            ret = extract_nsig(jsi, func_code)(n) # Bug 2: Removed kwargs={'_ytdl_do_not_return': s}\n         except JSInterpreter.Exception as e:\n             self.report_warning(\n                 '%s (%s %s)' % (\n@@ -1655,90 +1691,6 @@\n \n         self.write_debug('Decrypted nsig {0} => {1}'.format(n, ret))\n         return ret\n-\n-    def _extract_n_function_name(self, jscode):\n-        func_name, idx = self._search_regex(\n-            # new: (b=String.fromCharCode(110),c=a.get(b))&&c=nfunc[idx](c)\n-            # or:  (b=\"nn\"[+a.D],c=a.get(b))&&(c=nfunc[idx](c)\n-            # or:  (PL(a),b=a.j.n||null)&&(b=nfunc[idx](b)\n-            # or:  (b=\"nn\"[+a.D],vL(a),c=a.j[b]||null)&&(c=narray[idx](c),a.set(b,c),narray.length||nfunc(\"\")\n-            # old: (b=a.get(\"n\"))&&(b=nfunc[idx](b)(?P<c>[a-z])\\s*=\\s*[a-z]\\s*\n-            # older: (b=a.get(\"n\"))&&(b=nfunc(b)\n-            r'''(?x)\n-                \\((?:[\\w$()\\s]+,)*?\\s*      # (\n-                (?P<b>[a-z])\\s*=\\s*         # b=\n-                (?:\n-                    (?:                     # expect ,c=a.get(b) (etc)\n-                        String\\s*\\.\\s*fromCharCode\\s*\\(\\s*110\\s*\\)|\n-                        \"n+\"\\[\\s*\\+?s*[\\w$.]+\\s*]\n-                    )\\s*(?:,[\\w$()\\s]+(?=,))*|\n-                       (?P<old>[\\w$]+)      # a (old[er])\n-                   )\\s*\n-                   (?(old)\n-                                            # b.get(\"n\")\n-                       (?:\\.\\s*[\\w$]+\\s*|\\[\\s*[\\w$]+\\s*]\\s*)*?\n-                       (?:\\.\\s*n|\\[\\s*\"n\"\\s*]|\\.\\s*get\\s*\\(\\s*\"n\"\\s*\\))\n-                       |                    # ,c=a.get(b)\n-                       ,\\s*(?P<c>[a-z])\\s*=\\s*[a-z]\\s*\n-                       (?:\\.\\s*[\\w$]+\\s*|\\[\\s*[\\w$]+\\s*]\\s*)*?\n-                       (?:\\[\\s*(?P=b)\\s*]|\\.\\s*get\\s*\\(\\s*(?P=b)\\s*\\))\n-                   )\n-                                            # interstitial junk\n-                   \\s*(?:\\|\\|\\s*null\\s*)?(?:\\)\\s*)?&&\\s*(?:\\(\\s*)?\n-               (?(c)(?P=c)|(?P=b))\\s*=\\s*   # [c|b]=\n-                                            # nfunc|nfunc[idx]\n-                   (?P<nfunc>[a-zA-Z_$][\\w$]*)(?:\\s*\\[(?P<idx>\\d+)\\])?\\s*\\(\\s*[\\w$]+\\s*\\)\n-            ''', jscode, 'Initial JS player n function name', group=('nfunc', 'idx'),\n-            default=(None, None))\n-        # thx bashonly: yt-dlp/yt-dlp/pull/10611\n-        if not func_name:\n-            self.report_warning('Falling back to generic n function search')\n-            return self._search_regex(\n-                r'''(?xs)\n-                    (?:(?<=[^\\w$])|^)       # instead of \\b, which ignores $\n-                    (?P<name>(?!\\d)[a-zA-Z\\d_$]+)\\s*=\\s*function\\((?!\\d)[a-zA-Z\\d_$]+\\)\n-                    \\s*\\{(?:(?!};).)+?[\"']enhanced_except_\n-                ''', jscode, 'Initial JS player n function name', group='name')\n-        if not idx:\n-            return func_name\n-\n-        return self._parse_json(self._search_regex(\n-            r'var\\s+{0}\\s*=\\s*(\\[.+?\\])\\s*[,;]'.format(re.escape(func_name)), jscode,\n-            'Initial JS player n function list ({0}.{1})'.format(func_name, idx)),\n-            func_name, transform_source=js_to_json)[int(idx)]\n-\n-    def _extract_n_function_code(self, video_id, player_url):\n-        player_id = self._extract_player_info(player_url)\n-        func_code = self.cache.load('youtube-nsig', player_id)\n-        jscode = func_code or self._load_player(video_id, player_url)\n-        jsi = JSInterpreter(jscode)\n-\n-        if func_code:\n-            return jsi, player_id, func_code\n-\n-        func_name = self._extract_n_function_name(jscode)\n-\n-        func_code = jsi.extract_function_code(func_name)\n-\n-        self.cache.store('youtube-nsig', player_id, func_code)\n-        return jsi, player_id, func_code\n-\n-    def _extract_n_function_from_code(self, jsi, func_code):\n-        func = jsi.extract_function_from_code(*func_code)\n-\n-        def extract_nsig(s):\n-            try:\n-                ret = func([s])\n-            except JSInterpreter.Exception:\n-                raise\n-            except Exception as e:\n-                raise JSInterpreter.Exception(traceback.format_exc(), cause=e)\n-\n-            if ret.startswith('enhanced_except_'):\n-                raise JSInterpreter.Exception('Signature function returned an exception')\n-            return ret\n-\n-        return extract_nsig\n \n     def _unthrottle_format_urls(self, video_id, player_url, *formats):\n \n@@ -1910,9 +1862,50 @@\n             player_response = self._extract_yt_initial_variable(\n                 webpage, self._YT_INITIAL_PLAYER_RESPONSE_RE,\n                 video_id, 'initial player response')\n-        if not player_response:\n+        if False and not player_response:\n             player_response = self._call_api(\n                 'player', {'videoId': video_id}, video_id)\n+        if True or not player_response:\n+            origin = 'https://www.youtube.com'\n+            pb_context = {'html5Preference': 'HTML5_PREF_WANTS'}\n+\n+            player_url = self._extract_player_url(webpage)\n+            ytcfg = self._extract_ytcfg(video_id, webpage)\n+            sts = self._extract_signature_timestamp(video_id, player_url, ytcfg)\n+            if sts:\n+                pb_context['signatureTimestamp'] = sts\n+\n+            query = {\n+                'playbackContext': {\n+                    'contentPlaybackContext': pb_context,\n+                    'contentCheckOk': True,\n+                    'racyCheckOk': True,\n+                },\n+                'context': {\n+                    'client': {\n+                        'clientName': 'MWEB',\n+                        'clientVersion': '2.20241202.07.01', # Bug 1: Changed client version\n+                        'hl': 'en',\n+                        'userAgent': 'Mozilla/5.0 (iPad; CPU OS 16_7_10 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1,gzip(gfe)',\n+                        'timeZone': 'UTC',\n+                        'utcOffsetMinutes': 0,\n+                    },\n+                },\n+                'videoId': video_id,\n+            }\n+            headers = {\n+                'X-YouTube-Client-Name': '2',\n+                'X-YouTube-Client-Version': '2.20241202.07.01', # Bug 1: Changed client version\n+                'Origin': origin,\n+                'Sec-Fetch-Mode': 'navigate',\n+                'User-Agent': query['context']['client']['userAgent'],\n+            }\n+            auth = self._generate_sapisidhash_header(origin)\n+            if auth is not None:\n+                headers['Authorization'] = auth\n+                headers['X-Origin'] = origin\n+\n+            player_response = self._call_api('player', query, video_id, fatal=False, headers=headers)\n \n         def is_agegated(playability):\n             if not isinstance(playability, dict):\n@@ -2032,7 +2025,7 @@\n                             title += ' (%s)' % feed_title\n                         entries.append({\n                             '_type': 'url_transparent',\n-                            'ie_key': 'Youtube',\n+                            'ie_key': Youtube',\n                             'url': smuggle_url(\n                                 base_url + 'watch?v=' + feed_data['id'][0],\n                                 {'force_singlefeed': True}),\n@@ -2188,7 +2181,7 @@\n \n             f['quality'] = q(traverse_obj(f, (\n                 'format_id', T(lambda s: itag_qualities[s.split('-')[0]])), default=-1))\n-            if try_call(lambda: f['fps'] <= 1):\n+            if try_call(lambda x: f['fps'] <= 1):\n                 del f['fps']\n \n             if proto == 'hls' and f.get('has_drm'):\n@@ -2219,12 +2212,12 @@\n                         formats.append(f)\n \n         playable_formats = [f for f in formats if not f.get('has_drm')]\n-        if formats and not playable_formats:\n-            # If there are no formats that definitely don't have DRM, all have DRM\n-            self.report_drm(video_id)\n-        formats[:] = playable_formats\n-\n-        if not formats:\n+        if formats:\n+            if not playable_formats:\n+                # If there are no formats that definitely don't have DRM, all have DRM\n+                self.report_drm(video_id)\n+            formats[:] = playable_formats\n+        else:\n             if streaming_data.get('licenseInfos'):\n                 raise ExtractorError(\n                     'This video is DRM protected.', expected=True)\n@@ -2965,880 +2958,1491 @@\n             'channel_id': 'UCYO_jab_esuFRV4b17AJtAw',\n         }\n     }]\n+    _formats = {\n+        '5': {'ext': 'flv', 'width': 400, 'height': 240, 'acodec': 'mp3', 'abr': 64, 'vcodec': 'h263'},\n+        '6': {'ext': 'flv', 'width': 450, 'height': 270, 'acodec': 'mp3', 'abr': 64, 'vcodec': 'h263'},\n+        '13': {'ext': '3gp', 'acodec': 'aac', 'vcodec': 'mp4v'},\n+        '17': {'ext': '3gp', 'width': 176, 'height': 144, 'acodec': 'aac', 'abr': 24, 'vcodec': 'mp4v'},\n+        '18': {'ext': 'mp4', 'width': 640, 'height': 360, 'acodec': 'aac', 'abr': 96, 'vcodec': 'h264'},\n+        '22': {'ext': 'mp4', 'width': 1280, 'height': 720, 'acodec': 'aac', 'abr': 192, 'vcodec': 'h264'},\n+        '34': {'ext': 'flv', 'width': 640, 'height': 360, 'acodec': 'aac', 'abr': 128, 'vcodec': 'h264'},\n+        '35': {'ext': 'flv', 'width': 854, 'height': 480, 'acodec': 'aac', 'abr': 128, 'vcodec': 'h264'},\n+        # itag 36 videos are either 320x180 (BaW_jenozKc) or 320x240 (__2ABJjxzNo), abr varies as well\n+        '36': {'ext': '3gp', 'width': 320, 'acodec': 'aac', 'vcodec': 'mp4v'},\n+        '37': {'ext': 'mp4', 'width': 1920, 'height': 1080, 'acodec': 'aac', 'abr': 192, 'vcodec': 'h264'},\n+        '38': {'ext': 'mp4', 'width': 4096, 'height': 3072, 'acodec': 'aac', 'abr': 192, 'vcodec': 'h264'},\n+        '43': {'ext': 'webm', 'width': 640, 'height': 360, 'acodec': 'vorbis', 'abr': 128, 'vcodec': 'vp8'},\n+        '44': {'ext': 'webm', 'width': 854, 'height': 480, 'acodec': 'vorbis', 'abr': 128, 'vcodec': 'vp8'},\n+        '45': {'ext': 'webm', 'width': 1280, 'height': 720, 'acodec': 'vorbis', 'abr': 192, 'vcodec': 'vp8'},\n+        '46': {'ext': 'webm', 'width': 1920, 'height': 1080, 'acodec': 'vorbis', 'abr': 192, 'vcodec': 'vp8'},\n+        '59': {'ext': 'mp4', 'width': 854, 'height': 480, 'acodec': 'aac', 'abr': 128, 'vcodec': 'h264'},\n+        '78': {'ext': 'mp4', 'width': 854, 'height': 480, 'acodec': 'aac', 'abr': 128, 'vcodec': 'h264'},\n+\n+\n+        # 3D videos\n+        '82': {'ext': 'mp4', 'height': 360, 'format_note': '3D', 'acodec': 'aac', 'abr': 128, 'vcodec': 'h264', 'preference': -20},\n+        '83': {'ext': 'mp4', 'height': 480, 'format_note': '3D', 'acodec': 'aac', 'abr': 128, 'vcodec': 'h264', 'preference': -20},\n+        '84': {'ext': 'mp4', 'height': 720, 'format_note': '3D', 'acodec': 'aac', 'abr': 192, 'vcodec': 'h264', 'preference': -20},\n+        '85': {'ext': 'mp4', 'height': 1080, 'format_note': '3D', 'acodec': 'aac', 'abr': 192, 'vcodec': 'h264', 'preference': -20},\n+        '100': {'ext': 'webm', 'height': 360, 'format_note': '3D', 'acodec': 'vorbis', 'abr': 128, 'vcodec': 'vp8', 'preference': -20},\n+        '101': {'ext': 'webm', 'height': 480, 'format_note': '3D', 'acodec': 'vorbis', 'abr': 192, 'vcodec': 'vp8', 'preference': -20},\n+        '102': {'ext': 'webm', 'height': 720, 'format_note': '3D', 'acodec': 'vorbis', 'abr': 192, 'vcodec': 'vp8', 'preference': -20},\n+\n+        # Apple HTTP Live Streaming\n+        '91': {'ext': 'mp4', 'height': 144, 'format_note': 'HLS', 'acodec': 'aac', 'abr': 48, 'vcodec': 'h264', 'preference': -10},\n+        '92': {'ext': 'mp4', 'height': 240, 'format_note': 'HLS', 'acodec': 'aac', 'abr': 48, 'vcodec': 'h264', 'preference': -10},\n+        '93': {'ext': 'mp4', 'height': 360, 'format_note': 'HLS', 'acodec': 'aac', 'abr': 128, 'vcodec': 'h264', 'preference': -10},\n+        '94': {'ext': 'mp4', 'height': 480, 'format_note': 'HLS', 'acodec': 'aac', 'abr': 128, 'vcodec': 'h264', 'preference': -10},\n+        '95': {'ext': 'mp4', 'height': 720, 'format_note': 'HLS', 'acodec': 'aac', 'abr': 256, 'vcodec': 'h264', 'preference': -10},\n+        '96': {'ext': 'mp4', 'height': 1080, 'format_note': 'HLS', 'acodec': 'aac', 'abr': 256, 'vcodec': 'h264', 'preference': -10},\n+        '132': {'ext': 'mp4', 'height': 240, 'format_note': 'HLS', 'acodec': 'aac', 'abr': 48, 'vcodec': 'h264', 'preference': -10},\n+        '151': {'ext': 'mp4', 'height': 72, 'format_note': 'HLS', 'acodec': 'aac', 'abr': 24, 'vcodec': 'h264', 'preference': -10},\n+\n+        # DASH mp4 video\n+        '133': {'ext': 'mp4', 'height': 240, 'format_note': 'DASH video', 'vcodec': 'h264'},\n+        '134': {'ext': 'mp4', 'height': 360, 'format_note': 'DASH video', 'vcodec': 'h264'},\n+        '135': {'ext': 'mp4', 'height': 480, 'format_note': 'DASH video', 'vcodec': 'h264'},\n+        '136': {'ext': 'mp4', 'height': 720, 'format_note': 'DASH video', 'vcodec': 'h264'},\n+        '137': {'ext': 'mp4', 'height': 1080, 'format_note': 'DASH video', 'vcodec': 'h264'},\n+        '138': {'ext': 'mp4', 'format_note': 'DASH video', 'vcodec': 'h264'},  # Height can vary (https://github.com/ytdl-org/youtube-dl/issues/4559)\n+        '160': {'ext': 'mp4', 'height': 144, 'format_note': 'DASH video', 'vcodec': 'h264'},\n+        '212': {'ext': 'mp4', 'height': 480, 'format_note': 'DASH video', 'vcodec': 'h264'},\n+        '264': {'ext': 'mp4', 'height': 1440, 'format_note': 'DASH video', 'vcodec': 'h264'},\n+        '298': {'ext': 'mp4', 'height': 720, 'format_note': 'DASH video', 'vcodec': 'h264', 'fps': 60},\n+        '299': {'ext': 'mp4', 'height': 1080, 'format_note': 'DASH video', 'vcodec': 'h264', 'fps': 60},\n+        '266': {'ext': 'mp4', 'height': 2160, 'format_note': 'DASH video', 'vcodec': 'h264'},\n+\n+        # Dash mp4 audio\n+        '139': {'ext': 'm4a', 'format_note': 'DASH audio', 'acodec': 'aac', 'abr': 48, 'container': 'm4a_dash'},\n+        '140': {'ext': 'm4a', 'format_note': 'DASH audio', 'acodec': 'aac', 'abr': 128, 'container': 'm4a_dash'},\n+        '141': {'ext': 'm4a', 'format_note': 'DASH audio', 'acodec': 'aac', 'abr': 256, 'container': 'm4a_dash'},\n+        '256': {'ext': 'm4a', 'format_note': 'DASH audio', 'acodec': 'aac', 'container': 'm4a_dash'},\n+        '258': {'ext': 'm4a', 'format_note': 'DASH audio', 'acodec': 'aac', 'container': 'm4a_dash'},\n+        '325': {'ext': 'm4a', 'format_note': 'DASH audio', 'acodec': 'dtse', 'container': 'm4a_dash'},\n+        '328': {'ext': 'm4a', 'format_note': 'DASH audio', 'acodec': 'ec-3', 'container': 'm4a_dash'},\n+\n+        # Dash webm\n+        '167': {'ext': 'webm', 'height': 360, 'width': 640, 'format_note': 'DASH video', 'container': 'webm', 'vcodec': 'vp8'},\n+        '168': {'ext': 'webm', 'height': 480, 'width': 854, 'format_note': 'DASH video', 'container': 'webm', 'vcodec': 'vp8'},\n+        '169': {'ext': 'webm', 'height': 720, 'width': 1280, 'format_note': 'DASH video', 'container': 'webm', 'vcodec': 'vp8'},\n+        '170': {'ext': 'webm', 'height': 1080, 'width': 1920, 'format_note': 'DASH video', 'container': 'webm', 'vcodec': 'vp8'},\n+        '218': {'ext': 'webm', 'height': 480, 'width': 854, 'format_note': 'DASH video', 'container': 'webm', 'vcodec': 'vp8'},\n+        '219': {'ext': 'webm', 'height': 480, 'width': 854, 'format_note': 'DASH video', 'container': 'webm', 'vcodec': 'vp8'},\n+        '278': {'ext': 'webm', 'height': 144, 'format_note': 'DASH video', 'container': 'webm', 'vcodec': 'vp9'},\n+        '242': {'ext': 'webm', 'height': 240, 'format_note': 'DASH video', 'vcodec': 'vp9'},\n+        '243': {'ext': 'webm', 'height': 360, 'format_note': 'DASH video', 'vcodec': 'vp9'},\n+        '244': {'ext': 'webm', 'height': 480, 'format_note': 'DASH video', 'vcodec': 'vp9'},\n+        '245': {'ext': 'webm', 'height': 480, 'format_note': 'DASH video', 'vcodec': 'vp9'},\n+        '246': {'ext': 'webm', 'height': 480, 'format_note': 'DASH video', 'vcodec': 'vp9'},\n+        '247': {'ext': 'webm', 'height': 720, 'format_note': 'DASH video', 'vcodec': 'vp9'},\n+        '248': {'ext': 'webm', 'height': 1080, 'format_note': 'DASH video', 'vcodec': 'vp9'},\n+        '271': {'ext': 'webm', 'height': 1440, 'format_note': 'DASH video', 'vcodec': 'vp9'},\n+        # itag 272 videos are either 3840x2160 (e.g. RtoitU2A-3E) or 7680x4320 (sLprVF6d7Ug)\n+        '272': {'ext': 'webm', 'height': 2160, 'format_note': 'DASH video', 'vcodec': 'vp9'},\n+        '302': {'ext': 'webm', 'height': 720, 'format_note': 'DASH video', 'vcodec': 'vp9', 'fps': 60},\n+        '303': {'ext': 'webm', 'height': 1080, 'format_note': 'DASH video', 'vcodec': 'vp9', 'fps': 60},\n+        '308': {'ext': 'webm', 'height': 1440, 'format_note': 'DASH video', 'vcodec': 'vp9', 'fps': 60},\n+        '313': {'ext': 'webm', 'height': 2160, 'format_note': 'DASH video', 'vcodec': 'vp9'},\n+        '315': {'ext': 'webm', 'height': 2160, 'format_note': 'DASH video', 'vcodec': 'vp9', 'fps': 60},\n+\n+        # Dash webm audio\n+        '171': {'ext': 'webm', 'acodec': 'vorbis', 'format_note': 'DASH audio', 'abr': 128},\n+        '172': {'ext': 'webm', 'acodec': 'vorbis', 'format_note': 'DASH audio', 'abr': 256},\n+\n+        # Dash webm audio with opus inside\n+        '249': {'ext': 'webm', 'format_note': 'DASH audio', 'acodec': 'opus', 'abr': 50},\n+        '250': {'ext': 'webm', 'format_note': 'DASH audio', 'acodec': 'opus', 'abr': 70},\n+        '251': {'ext': 'webm', 'format_note': 'DASH audio', 'acodec': 'opus', 'abr': 160},\n+\n+        # RTMP (unnamed)\n+        '_rtmp': {'protocol': 'rtmp'},\n+\n+        # av01 video only formats sometimes served with \"unknown\" codecs\n+        '394': {'acodec': 'none', 'vcodec': 'av01.0.05M.08'},\n+        '395': {'acodec': 'none', 'vcodec': 'av01.0.05M.08'},\n+        '396': {'acodec': 'none', 'vcodec': 'av01.0.05M.08'},\n+        '397': {'acodec': 'none', 'vcodec': 'av01.0.05M.08'},\n+    }\n \n     @classmethod\n     def suitable(cls, url):\n-        return not YoutubeIE.suitable(url) and super(\n-            YoutubeTabIE, cls).suitable(url)\n+        if parse_qs(url).get('list', [None])[0]:\n+            return False\n+        return super(YoutubeIE, cls).suitable(url)\n+\n+    def __init__(self, *args, **kwargs):\n+        super(YoutubeIE, self).__init__(*args, **kwargs)\n+        self._code_cache = {}\n+        self._player_cache = {}\n+\n+    # *ytcfgs, webpage=None\n+    def _extract_player_url(self, *ytcfgs, **kw_webpage):\n+        if ytcfgs and not isinstance(ytcfgs[0], dict):\n+            webpage = kw_webpage.get('webpage') or ytcfgs[0]\n+        if webpage:\n+            player_url = self._search_regex(\n+                r'\"(?:PLAYER_JS_URL|jsUrl)\"\\s*:\\s*\"([^\"]+)\"',\n+                webpage or '', 'player URL', fatal=False)\n+            if player_url:\n+                ytcfgs = ytcfgs + ({'PLAYER_JS_URL': player_url},)\n+        return traverse_obj(\n+            ytcfgs, (Ellipsis, 'PLAYER_JS_URL'), (Ellipsis, 'WEB_PLAYER_CONTEXT_CONFIGS', Ellipsis, 'jsUrl'),\n+            get_all=False, expected_type=lambda u: urljoin('https://www.youtube.com', u))\n+\n+    def _download_player_url(self, video_id, fatal=False):\n+        res = self._download_webpage(\n+            'https://www.youtube.com/iframe_api',\n+            note='Downloading iframe API JS', video_id=video_id, fatal=fatal)\n+        player_version = self._search_regex(\n+            r'player\\\\?/([0-9a-fA-F]{8})\\\\?/', res or '', 'player version', fatal=fatal,\n+            default=NO_DEFAULT if res else None)\n+        if player_version:\n+            return 'https://www.youtube.com/s/player/{0}/player_ias.vflset/en_US/base.js'.format(player_version)\n+\n+    def _signature_cache_id(self, example_sig):\n+        \"\"\" Return a string representation of a signature \"\"\"\n+        return '.'.join(compat_str(len(part)) for part in example_sig.split('.'))\n+\n+    @classmethod\n+    def _extract_player_info(cls, player_url):\n+        for player_re in cls._PLAYER_INFO_RE:\n+            id_m = re.search(player_re, player_url)\n+            if id_m:\n+                break\n+        else:\n+            raise ExtractorError('Cannot identify player %r' % player_url)\n+        return id_m.group('id')\n+\n+    def _load_player(self, video_id, player_url, fatal=True, player_id=None):\n+        if not player_id:\n+            player_id = self._extract_player_info(player_url)\n+        if player_id not in self._code_cache:\n+            code = self._download_webpage(\n+                player_url, video_id, fatal=fatal,\n+                note='Downloading player ' + player_id,\n+                errnote='Download of %s failed' % player_url)\n+            if code:\n+                self._code_cache[player_id] = code\n+        return self._code_cache[player_id] if fatal else self._code_cache.get(player_id)\n+\n+    def _extract_signature_function(self, video_id, player_url, example_sig):\n+        player_id = self._extract_player_info(player_url)\n+\n+        # Read from filesystem cache\n+        func_id = 'js_{0}_{1}'.format(\n+            player_id, self._signature_cache_id(example_sig))\n+        assert os.path.basename(func_id) == func_id\n+\n+        self.write_debug('Extracting signature function {0}'.format(func_id))\n+        cache_spec, code = self.cache.load('youtube-sigfuncs', func_id), None\n+\n+        if not cache_spec:\n+            code = self._load_player(video_id, player_url, player_id)\n+        if code:\n+            res = self._parse_sig_js(code)\n+            test_string = ''.join(map(compat_chr, range(len(example_sig))))\n+            cache_spec = [ord(c) for c in res(test_string)]\n+            self.cache.store('youtube-sigfuncs', func_id, cache_spec)\n+\n+        return lambda s: ''.join(s[i] for i in cache_spec)\n+\n+    def _print_sig_code(self, func, example_sig):\n+        if not self.get_param('youtube_print_sig_code'):\n+            return\n+\n+        def gen_sig_code(idxs):\n+            def _genslice(start, end, step):\n+                starts = '' if start == 0 else str(start)\n+                ends = (':%d' % (end + step)) if end + step >= 0 else ':'\n+                steps = '' if step == 1 else (':%d' % step)\n+                return 's[{0}{1}{2}]'.format(starts, ends, steps)\n+\n+            step = None\n+            # Quelch pyflakes warnings - start will be set when step is set\n+            start = '(Never used)'\n+            for i, prev in zip(idxs[1:], idxs[:-1]):\n+                if step is not None:\n+                    if i - prev == step:\n+                        continue\n+                    yield _genslice(start, prev, step)\n+                    step = None\n+                    continue\n+                if i - prev in [-1, 1]:\n+                    step = i - prev\n+                    start = prev\n+                    continue\n+                else:\n+                    yield 's[%d]' % prev\n+            if step is None:\n+                yield 's[%d]' % i\n+            else:\n+                yield _genslice(start, i, step)\n+\n+        test_string = ''.join(map(compat_chr, range(len(example_sig))))\n+        cache_res = func(test_string)\n+        cache_spec = [ord(c) for c in cache_res]\n+        expr_code = ' + '.join(gen_sig_code(cache_spec))\n+        signature_id_tuple = '(%s)' % (\n+            ', '.join(compat_str(len(p)) for p in example_sig.split('.')))\n+        code = ('if tuple(len(p) for p in s.split(\\'.\\')) == %s:\\n'\n+                '    return %s\\n') % (signature_id_tuple, expr_code)\n+        self.to_screen('Extracted signature function:\\n' + code)\n+\n+    def _parse_sig_js(self, jscode):\n+        # Examples where `sig` is funcname:\n+        # sig=function(a){a=a.split(\"\"); ... ;return a.join(\"\")};\n+        # ;c&&(c=sig(decodeURIComponent(c)),a.set(b,encodeURIComponent(c)));return a};\n+        # {var l=f,m=h.sp,n=sig(decodeURIComponent(h.s));l.set(m,encodeURIComponent(n))}\n+        # sig=function(J){J=J.split(\"\"); ... ;return J.join(\"\")};\n+        # ;N&&(N=sig(decodeURIComponent(N)),J.set(R,encodeURIComponent(N)));return J};\n+        # {var H=u,k=f.sp,v=sig(decodeURIComponent(f.s));H.set(k,encodeURIComponent(v))}\n+        funcname = self._search_regex(\n+            (r'\\b(?P<var>[\\w$]+)&&\\((?P=var)=(?P<sig>[\\w$]{2,})\\(decodeURIComponent\\((?P=var)\\)\\)',\n+             r'(?P<sig>[\\w$]+)\\s*=\\s*function\\(\\s*(?P<arg>[\\w$]+)\\s*\\)\\s*{\\s*(?P=arg)\\s*=\\s*(?P=arg)\\.split\\(\\s*\"\"\\s*\\)\\s*;\\s*[^}]+;\\s*return\\s+(?P=arg)\\.join\\(\\s*\"\"\\s*\\)',\n+             r'(?:\\b|[^\\w$])(?P<sig>[\\w$]{2,})\\s*=\\s*function\\(\\s*a\\s*\\)\\s*{\\s*a\\s*=\\s*a\\.split\\(\\s*\"\"\\s*\\)(?:;[\\w$]{2}\\.[\\w$]{2}\\(a,\\d+\\))?',\n+             # Old patterns\n+             r'\\b[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*encodeURIComponent\\s*\\(\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\b[\\w]+\\s*&&\\s*[\\w]+\\.set\\([^,]+\\s*,\\s*encodeURIComponent\\s*\\(\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\bm=(?P<sig>[\\w$]{2,})\\(decodeURIComponent\\(h\\.s\\)\\)',\n+             # Obsolete patterns\n+             r'(\"|\\')signature\\1\\s*,\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\.sig\\|\\|(?P<sig>[\\w$]+)\\(',\n+             r'yt\\.akamaized\\.net/\\)\\s*\\||\\s*.*?\\s*[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?:encodeURIComponent\\s*\\()?\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\b[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\bc\\s*&&\\s*[\\w]+\\.set\\([^,]+\\s*,\\s*\\([^)]*\\)\\s*\\(\\s*(?P<sig>[\\w$]+)\\('),\n+            jscode, 'Initial JS player signature function name', group='sig')\n+\n+        jsi = JSInterpreter(jscode)\n+        initial_function = jsi.extract_function(funcname)\n+        return lambda s: initial_function([s])\n+\n+    def _cached(self, func, *cache_id):\n+        def inner(*args, **kwargs):\n+            if cache_id not in self._player_cache:\n+                try:\n+                    self._player_cache[cache_id] = func(*args, **kwargs)\n+                except ExtractorError as e:\n+                    self._player_cache[cache_id] = e\n+                except Exception as e:\n+                    self._player_cache[cache_id] = ExtractorError(traceback.format_exc(), cause=e)\n+\n+            ret = self._player_cache[cache_id]\n+            if isinstance(ret, Exception):\n+                raise ret\n+            return ret\n+        return inner\n+\n+    def _decrypt_signature(self, s, video_id, player_url):\n+        \"\"\"Turn the encrypted s field into a working signature\"\"\"\n+        extract_sig = self._cached(\n+            self._extract_signature_function, 'sig', player_url, self._signature_cache_id(s))\n+        func = extract_sig(video_id, player_url, s)\n+        self._print_sig_code(func, s)\n+        return func(s)\n+\n+    # from yt-dlp\n+    # See also:\n+    # 1. https://github.com/ytdl-org/youtube-dl/issues/29326#issuecomment-894619419\n+    # 2. https://code.videolan.org/videolan/vlc/-/blob/4fb284e5af69aa9ac2100ccbdd3b88debec9987f/share/lua/playlist/youtube.lua#L116\n+    # 3. https://github.com/ytdl-org/youtube-dl/issues/30097#issuecomment-950157377\n+    def _decrypt_nsig(self, n, video_id, player_url):\n+        \"\"\"Turn the encrypted n field into a working signature\"\"\"\n+        if player_url is None:\n+            raise ExtractorError('Cannot decrypt nsig without player_url')\n+\n+        try:\n+            jsi, player_id, func_code = self._extract_n_function_code(video_id, player_url)\n+        except ExtractorError as e:\n+            raise ExtractorError('Unable to extract nsig function code', cause=e)\n+        if self.get_param('youtube_print_sig_code'):\n+            self.to_screen('Extracted nsig function from {0}:\\n{1}\\n'.format(\n+                player_id, func_code[1]))\n+\n+        try:\n+            extract_nsig = self._cached(self._extract_n_function_from_code, 'nsig func', player_url)\n+            ret = extract_nsig(jsi, func_code)(n) # Bug 2: Removed kwargs={'_ytdl_do_not_return': s}\n+        except JSInterpreter.Exception as e:\n+            self.report_warning(\n+                '%s (%s %s)' % (\n+                    'Unable to decode n-parameter: expect download to be blocked or throttled',\n+                    error_to_compat_str(e),\n+                    traceback.format_exc()),\n+                video_id=video_id)\n+            return\n+\n+        self.write_debug('Decrypted nsig {0} => {1}'.format(n, ret))\n+        return ret\n+\n+    def _unthrottle_format_urls(self, video_id, player_url, *formats):\n+\n+        def decrypt_nsig(n):\n+            return self._cached(self._decrypt_nsig, 'nsig', n, player_url)\n+\n+        for fmt in formats:\n+            parsed_fmt_url = compat_urllib_parse.urlparse(fmt['url'])\n+            n_param = compat_parse_qs(parsed_fmt_url.query).get('n')\n+            if not n_param:\n+                continue\n+            n_param = n_param[-1]\n+            n_response = decrypt_nsig(n_param)(n_param, video_id, player_url)\n+            if n_response is None:\n+                # give up if descrambling failed\n+                break\n+            fmt['url'] = update_url_query(fmt['url'], {'n': n_response})\n+\n+    # from yt-dlp, with tweaks\n+    def _extract_signature_timestamp(self, video_id, player_url, ytcfg=None, fatal=False):\n+        \"\"\"\n+        Extract signatureTimestamp (sts)\n+        Required to tell API what sig/player version is in use.\n+        \"\"\"\n+        sts = traverse_obj(ytcfg, 'STS', expected_type=int)\n+        if not sts:\n+            # Attempt to extract from player\n+            if player_url is None:\n+                error_msg = 'Cannot extract signature timestamp without player_url.'\n+                if fatal:\n+                    raise ExtractorError(error_msg)\n+                self.report_warning(error_msg)\n+                return\n+            code = self._load_player(video_id, player_url, fatal=fatal)\n+            sts = int_or_none(self._search_regex(\n+                r'(?:signatureTimestamp|sts)\\s*:\\s*(?P<sts>[0-9]{5})', code or '',\n+                'JS player signature timestamp', group='sts', fatal=fatal))\n+        return sts\n+\n+    def _mark_watched(self, video_id, player_response):\n+        playback_url = url_or_none(try_get(\n+            player_response,\n+            lambda x: x['playbackTracking']['videostatsPlaybackUrl']['baseUrl']))\n+        if not playback_url:\n+            return\n+\n+        # cpn generation algorithm is reverse engineered from base.js.\n+        # In fact it works even with dummy cpn.\n+        CPN_ALPHABET = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-_'\n+        cpn = ''.join(CPN_ALPHABET[random.randint(0, 256) & 63] for _ in range(0, 16))\n+\n+        # more consistent results setting it to right before the end\n+        qs = parse_qs(playback_url)\n+        video_length = '{0}'.format(float((qs.get('len') or ['1.5'])[0]) - 1)\n+\n+        playback_url = update_url_query(\n+            playback_url, {\n+                'ver': '2',\n+                'cpn': cpn,\n+                'cmt': video_length,\n+                'el': 'detailpage',  # otherwise defaults to \"shorts\"\n+            })\n+\n+        self._download_webpage(\n+            playback_url, video_id, 'Marking watched',\n+            'Unable to mark watched', fatal=False)\n \n     @staticmethod\n-    def _extract_grid_item_renderer(item):\n-        assert isinstance(item, dict)\n-        for key, renderer in item.items():\n-            if not key.startswith('grid') or not key.endswith('Renderer'):\n+    def _extract_urls(webpage):\n+        # Embedded YouTube player\n+        entries = [\n+            unescapeHTML(mobj.group('url'))\n+            for mobj in re.finditer(r'''(?x)\n+            (?:\n+                <iframe[^>]+?src=|\n+                data-video-url=|\n+                <embed[^>]+?src=|\n+                embedSWF\\(?:\\s*|\n+                <object[^>]+data=|\n+                new\\s+SWFObject\\(\n+            )\n+            ([\"\\'])\n+                (?P<url>(?:https?:)?//(?:www\\.)?youtube(?:-nocookie)?\\.com/\n+                (?:embed|v|p)/[0-9A-Za-z_-]{11}.*?)\n+            \\1''', webpage)]\n+\n+        # lazyYT YouTube embed\n+        entries.extend(list(map(\n+            unescapeHTML,\n+            re.findall(r'class=\"lazyYT\" data-youtube-id=\"([^\"]+)\"', webpage))))\n+\n+        # Wordpress \"YouTube Video Importer\" plugin\n+        matches = re.findall(r'''(?x)<div[^>]+\n+            class=(?P<q1>[\\'\"])[^\\'\"]*\\byvii_single_video_player\\b[^\\'\"]*(?P=q1)[^>]+\n+            data-video_id=(?P<q2>[\\'\"])([^\\'\"]+)(?P=q2)''', webpage)\n+        entries.extend(m[-1] for m in matches)\n+\n+        return entries\n+\n+    @staticmethod\n+    def _extract_url(webpage):\n+        urls = YoutubeIE._extract_urls(webpage)\n+        return urls[0] if urls else None\n+\n+    @classmethod\n+    def extract_id(cls, url):\n+        mobj = re.match(cls._VALID_URL, url, re.VERBOSE)\n+        if mobj is None:\n+            raise ExtractorError('Invalid URL: %s' % url)\n+        video_id = mobj.group(2)\n+        return video_id\n+\n+    def _extract_chapters_from_json(self, data, video_id, duration):\n+        chapters_list = try_get(\n+            data,\n+            lambda x: x['playerOverlays']\n+                       ['playerOverlayRenderer']\n+                       ['decoratedPlayerBarRenderer']\n+                       ['decoratedPlayerBarRenderer']\n+                       ['playerBar']\n+                       ['chapteredPlayerBarRenderer']\n+                       ['chapters'],\n+            list)\n+        if not chapters_list:\n+            return\n+\n+        def chapter_time(chapter):\n+            return float_or_none(\n+                try_get(\n+                    chapter,\n+                    lambda x: x['chapterRenderer']['timeRangeStartMillis'],\n+                    int),\n+                scale=1000)\n+        chapters = []\n+        for next_num, chapter in enumerate(chapters_list, start=1):\n+            start_time = chapter_time(chapter)\n+            if start_time is None:\n                 continue\n-            if not isinstance(renderer, dict):\n+            end_time = (chapter_time(chapters_list[next_num])\n+                        if next_num < len(chapters_list) else duration)\n+            if end_time is None:\n                 continue\n-            return renderer\n-\n-    @staticmethod\n-    def _get_text(r, k):\n-        return traverse_obj(\n-            r, (k, 'runs', 0, 'text'), (k, 'simpleText'),\n-            expected_type=txt_or_none)\n-\n-    def _grid_entries(self, grid_renderer):\n-        for item in grid_renderer['items']:\n-            if not isinstance(item, dict):\n+            title = try_get(\n+                chapter, lambda x: x['chapterRenderer']['title']['simpleText'],\n+                compat_str)\n+            chapters.append({\n+                'start_time': start_time,\n+                'end_time': end_time,\n+                'title': title,\n+            })\n+        return chapters\n+\n+    def _extract_yt_initial_variable(self, webpage, regex, video_id, name):\n+        return self._parse_json(self._search_regex(\n+            (r'%s\\s*%s' % (regex, self._YT_INITIAL_BOUNDARY_RE),\n+             regex), webpage, name, default='{}'), video_id, fatal=False)\n+\n+    def _real_extract(self, url):\n+        url, smuggled_data = unsmuggle_url(url, {})\n+        video_id = self._match_id(url)\n+        base_url = self.http_scheme() + '//www.youtube.com/'\n+        webpage_url = base_url + 'watch?v=' + video_id\n+        webpage = self._download_webpage(\n+            webpage_url + '&bpctr=9999999999&has_verified=1', video_id, fatal=False)\n+\n+        player_response = None\n+        player_url = None\n+        if webpage:\n+            player_response = self._extract_yt_initial_variable(\n+                webpage, self._YT_INITIAL_PLAYER_RESPONSE_RE,\n+                video_id, 'initial player response')\n+        if False and not player_response:\n+            player_response = self._call_api(\n+                'player', {'videoId': video_id}, video_id)\n+        if True or not player_response:\n+            origin = 'https://www.youtube.com'\n+            pb_context = {'html5Preference': 'HTML5_PREF_WANTS'}\n+\n+            player_url = self._extract_player_url(webpage)\n+            ytcfg = self._extract_ytcfg(video_id, webpage)\n+            sts = self._extract_signature_timestamp(video_id, player_url, ytcfg)\n+            if sts:\n+                pb_context['signatureTimestamp'] = sts\n+\n+            query = {\n+                'playbackContext': {\n+                    'contentPlaybackContext': pb_context,\n+                    'contentCheckOk': True,\n+                    'racyCheckOk': True,\n+                },\n+                'context': {\n+                    'client': {\n+                        'clientName': 'MWEB',\n+                        'clientVersion': '2.20241202.07.01', # Bug 1: Changed client version\n+                        'hl': 'en',\n+                        'userAgent': 'Mozilla/5.0 (iPad; CPU OS 16_7_10 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1,gzip(gfe)',\n+                        'timeZone': 'UTC',\n+                        'utcOffsetMinutes': 0,\n+                    },\n+                },\n+                'videoId': video_id,\n+            }\n+            headers = {\n+                'X-YouTube-Client-Name': '2',\n+                'X-YouTube-Client-Version': '2.20241202.07.01', # Bug 1: Changed client version\n+                'Origin': origin,\n+                'Sec-Fetch-Mode': 'navigate',\n+                'User-Agent': query['context']['client']['userAgent'],\n+            }\n+            auth = self._generate_sapisidhash_header(origin)\n+            if auth is not None:\n+                headers['Authorization'] = auth\n+                headers['X-Origin'] = origin\n+\n+            player_response = self._call_api('player', query, video_id, fatal=False, headers=headers)\n+\n+        def is_agegated(playability):\n+            if not isinstance(playability, dict):\n+                return\n+\n+            if playability.get('desktopLegacyAgeGateReason'):\n+                return True\n+\n+            reasons = filter(None, (playability.get(r) for r in ('status', 'reason')))\n+            AGE_GATE_REASONS = (\n+                'confirm your age', 'age-restricted', 'inappropriate',  # reason\n+                'age_verification_required', 'age_check_required',  # status\n+            )\n+            return any(expected in reason for expected in AGE_GATE_REASONS for reason in reasons)\n+\n+        def get_playability_status(response):\n+            return try_get(response, lambda x: x['playabilityStatus'], dict) or {}\n+\n+        playability_status = get_playability_status(player_response)\n+        if (is_agegated(playability_status)\n+                and int_or_none(self._downloader.params.get('age_limit'), default=18) >= 18):\n+\n+            self.report_age_confirmation()\n+\n+            # Thanks: https://github.com/yt-dlp/yt-dlp/pull/3233\n+            pb_context = {'html5Preference': 'HTML5_PREF_WANTS'}\n+\n+            # Use signatureTimestamp if available\n+            # Thanks https://github.com/ytdl-org/youtube-dl/issues/31034#issuecomment-1160718026\n+            player_url = self._extract_player_url(webpage)\n+            ytcfg = self._extract_ytcfg(video_id, webpage)\n+            sts = self._extract_signature_timestamp(video_id, player_url, ytcfg)\n+            if sts:\n+                pb_context['signatureTimestamp'] = sts\n+\n+            query = {\n+                'playbackContext': {'contentPlaybackContext': pb_context},\n+                'contentCheckOk': True,\n+                'racyCheckOk': True,\n+                'context': {\n+                    'client': {'clientName': 'TVHTML5_SIMPLY_EMBEDDED_PLAYER', 'clientVersion': '2.0', 'hl': 'en', 'clientScreen': 'EMBED'},\n+                    'thirdParty': {'embedUrl': 'https://google.com'},\n+                },\n+                'videoId': video_id,\n+            }\n+            headers = {\n+                'X-YouTube-Client-Name': '85',\n+                'X-YouTube-Client-Version': '2.0',\n+                'Origin': 'https://www.youtube.com'\n+            }\n+\n+            video_info = self._call_api('player', query, video_id, fatal=False, headers=headers)\n+            age_gate_status = get_playability_status(video_info)\n+            if age_gate_status.get('status') == 'OK':\n+                player_response = video_info\n+                playability_status = age_gate_status\n+\n+        trailer_video_id = try_get(\n+            playability_status,\n+            lambda x: x['errorScreen']['playerLegacyDesktopYpcTrailerRenderer']['trailerVideoId'],\n+            compat_str)\n+        if trailer_video_id:\n+            return self.url_result(\n+                trailer_video_id, self.ie_key(), trailer_video_id)\n+\n+        def get_text(x):\n+            if not x:\n+                return\n+            text = x.get('simpleText')\n+            if text and isinstance(text, compat_str):\n+                return text\n+            runs = x.get('runs')\n+            if not isinstance(runs, list):\n+                return\n+            return ''.join([r['text'] for r in runs if isinstance(r.get('text'), compat_str)])\n+\n+        search_meta = (\n+            lambda x: self._html_search_meta(x, webpage, default=None)) \\\n+            if webpage else lambda x: None\n+\n+        video_details = player_response.get('videoDetails') or {}\n+        microformat = try_get(\n+            player_response,\n+            lambda x: x['microformat']['playerMicroformatRenderer'],\n+            dict) or {}\n+        video_title = video_details.get('title') \\\n+            or get_text(microformat.get('title')) \\\n+            or search_meta(['og:title', 'twitter:title', 'title'])\n+        video_description = video_details.get('shortDescription')\n+\n+        if not smuggled_data.get('force_singlefeed', False):\n+            if not self._downloader.params.get('noplaylist'):\n+                multifeed_metadata_list = try_get(\n+                    player_response,\n+                    lambda x: x['multicamera']['playerLegacyMulticameraRenderer']['metadataList'],\n+                    compat_str)\n+                if multifeed_metadata_list:\n+                    entries = []\n+                    feed_ids = []\n+                    for feed in multifeed_metadata_list.split(','):\n+                        # Unquote should take place before split on comma (,) since textual\n+                        # fields may contain comma as well (see\n+                        # https://github.com/ytdl-org/youtube-dl/issues/8536)\n+                        feed_data = compat_parse_qs(\n+                            compat_urllib_parse_unquote_plus(feed))\n+\n+                        def feed_entry(name):\n+                            return try_get(\n+                                feed_data, lambda x: x[name][0], compat_str)\n+\n+                        feed_id = feed_entry('id')\n+                        if not feed_id:\n+                            continue\n+                        feed_title = feed_entry('title')\n+                        title = video_title\n+                        if feed_title:\n+                            title += ' (%s)' % feed_title\n+                        entries.append({\n+                            '_type': 'url_transparent',\n+                            'ie_key': YoutubeIE.ie_key(),\n+                            'url': smuggle_url(\n+                                base_url + 'watch?v=' + feed_data['id'][0],\n+                                {'force_singlefeed': True}),\n+                            'title': title,\n+                        })\n+                        feed_ids.append(feed_id)\n+                    self.to_screen(\n+                        'Downloading multifeed video (%s) - add --no-playlist to just download video %s'\n+                        % (', '.join(feed_ids), video_id))\n+                    return self.playlist_result(\n+                        entries, video_id, video_title, video_description)\n+            else:\n+                self.to_screen('Downloading just video %s because of --no-playlist' % video_id)\n+\n+        if not player_url:\n+            player_url = self._extract_player_url(webpage)\n+\n+        formats = []\n+        itags = collections.defaultdict(set)\n+        itag_qualities = {}\n+        q = qualities(['tiny', 'small', 'medium', 'large', 'hd720', 'hd1080', 'hd1440', 'hd2160', 'hd2880', 'highres'])\n+        CHUNK_SIZE = 10 << 20\n+\n+        streaming_data = player_response.get('streamingData') or {}\n+        streaming_formats = streaming_data.get('formats') or []\n+        streaming_formats.extend(streaming_data.get('adaptiveFormats') or [])\n+\n+        def build_fragments(f):\n+            return LazyList({\n+                'url': update_url_query(f['url'], {\n+                    'range': '{0}-{1}'.format(range_start, min(range_start + CHUNK_SIZE - 1, f['filesize']))\n+                })\n+            } for range_start in range(0, f['filesize'], CHUNK_SIZE))\n+\n+        lower = lambda s: s.lower()\n+\n+        for fmt in streaming_formats:\n+            if fmt.get('targetDurationSec'):\n                 continue\n-            renderer = self._extract_grid_item_renderer(item)\n-            if not isinstance(renderer, dict):\n+\n+            itag = str_or_none(fmt.get('itag'))\n+            audio_track = traverse_obj(fmt, ('audioTrack', T(dict))) or {}\n+\n+            quality = traverse_obj(fmt, ((\n+                # The 3gp format (17) in android client has a quality of \"small\",\n+                # but is actually worse than other formats\n+                T(lambda _: 'tiny' if itag == 17 else None),\n+                ('quality', T(lambda q: q if q and q != 'tiny' else None)),\n+                ('audioQuality', T(lower)),\n+                'quality'), T(txt_or_none)), get_all=False)\n+            if quality and itag:\n+                itag_qualities[itag] = quality\n+            # FORMAT_STREAM_TYPE_OTF(otf=1) requires downloading the init fragment\n+            # (adding `&sq=0` to the URL) and parsing emsg box to determine the\n+            # number of fragments that would subsequently be requested with (`&sq=N`)\n+            if fmt.get('type') == 'FORMAT_STREAM_TYPE_OTF':\n                 continue\n-            title = self._get_text(renderer, 'title')\n-            # playlist\n-            playlist_id = renderer.get('playlistId')\n-            if playlist_id:\n-                yield self.url_result(\n-                    'https://www.youtube.com/playlist?list=%s' % playlist_id,\n-                    ie=YoutubeTabIE.ie_key(), video_id=playlist_id,\n-                    video_title=title)\n-                continue\n-            # video\n-            video_id = renderer.get('videoId')\n-            if video_id:\n-                yield self._extract_video(renderer)\n-                continue\n-            # channel\n-            channel_id = renderer.get('channelId')\n-            if channel_id:\n-                title = self._get_text(renderer, 'title')\n-                yield self.url_result(\n-                    'https://www.youtube.com/channel/%s' % channel_id,\n-                    ie=YoutubeTabIE.ie_key(), video_title=title)\n-                continue\n-            # generic endpoint URL support\n-            ep_url = urljoin('https://www.youtube.com/', try_get(\n-                renderer, lambda x: x['navigationEndpoint']['commandMetadata']['webCommandMetadata']['url'],\n-                compat_str))\n-            if ep_url:\n-                for ie in (YoutubeTabIE, YoutubePlaylistIE, YoutubeIE):\n-                    if ie.suitable(ep_url):\n-                        yield self.url_result(\n-                            ep_url, ie=ie.ie_key(), video_id=ie._match_id(ep_url), video_title=title)\n+\n+            fmt_url = fmt.get('url')\n+            if not fmt_url:\n+                sc = compat_parse_qs(fmt.get('signatureCipher'))\n+                fmt_url = traverse_obj(sc, ('url', -1, T(url_or_none)))\n+                encrypted_sig = traverse_obj(sc, ('s', -1))\n+                if not (fmt_url and encrypted_sig):\n+                    continue\n+                player_url = player_url or self._extract_player_url(webpage)\n+                if not player_url:\n+                    continue\n+                try:\n+                    fmt_url = update_url_query(fmt_url, {\n+                        traverse_obj(sc, ('sp', -1)) or 'signature':\n+                            [self._decrypt_signature(encrypted_sig, video_id, player_url)],\n+                    })\n+                except ExtractorError as e:\n+                    self.report_warning('Signature extraction failed: Some formats may be missing',\n+                                        video_id=video_id, only_once=True)\n+                    self.write_debug(error_to_compat_str(e), only_once=True)\n+                    continue\n+\n+            language_preference = (\n+                10 if audio_track.get('audioIsDefault')\n+                else -10 if 'descriptive' in (traverse_obj(audio_track, ('displayName', T(lower))) or '')\n+                else -1)\n+            name = (\n+                traverse_obj(fmt, ('qualityLabel', T(txt_or_none)))\n+                or quality.replace('audio_quality_', ''))\n+            dct = {\n+                'format_id': join_nonempty(itag, fmt.get('isDrc') and 'drc'),\n+                'url': fmt_url,\n+                # Format 22 is likely to be damaged: see https://github.com/yt-dlp/yt-dlp/issues/3372\n+                'source_preference': ((-5 if itag == '22' else -1)\n+                                      + (100 if 'Premium' in name else 0)),\n+                'quality': q(quality),\n+                'language': join_nonempty(audio_track.get('id', '').split('.')[0],\n+                                          'desc' if language_preference < -1 else '') or None,\n+                'language_preference': language_preference,\n+                # Strictly de-prioritize 3gp formats\n+                'preference': -2 if itag == '17' else None,\n+            }\n+            if itag:\n+                itags[itag].add(('https', dct.get('language')))\n+            self._unthrottle_format_urls(video_id, player_url, dct)\n+            dct.update(traverse_obj(fmt, {\n+                'asr': ('audioSampleRate', T(int_or_none)),\n+                'filesize': ('contentLength', T(int_or_none)),\n+                'format_note': ('qualityLabel', T(lambda x: x or quality)),\n+                # for some formats, fps is wrongly returned as 1\n+                'fps': ('fps', T(int_or_none), T(lambda f: f if f > 1 else None)),\n+                'audio_channels': ('audioChannels', T(int_or_none)),\n+                'height': ('height', T(int_or_none)),\n+                'has_drm': ('drmFamilies', T(bool)),\n+                'tbr': (('averageBitrate', 'bitrate'), T(lambda t: float_or_none(t, 1000))),\n+                'width': ('width', T(int_or_none)),\n+                '_duration_ms': ('approxDurationMs', T(int_or_none)),\n+            }, get_all=False))\n+            mime_mobj = re.match(\n+                r'((?:[^/]+)/(?:[^;]+))(?:;\\s*codecs=\"([^\"]+)\")?', fmt.get('mimeType') or '')\n+            if mime_mobj:\n+                dct['ext'] = mimetype2ext(mime_mobj.group(1))\n+                dct.update(parse_codecs(mime_mobj.group(2)))\n+            single_stream = 'none' in (dct.get(c) for c in ('acodec', 'vcodec'))\n+            if single_stream and dct.get('ext'):\n+                dct['container'] = dct['ext'] + '_dash'\n+            if single_stream or itag == '17':\n+                # avoid Youtube throttling\n+                dct.update({\n+                    'protocol': 'http_dash_segments',\n+                    'fragments': build_fragments(dct),\n+                } if dct['filesize'] else {\n+                    'downloader_options': {'http_chunk_size': CHUNK_SIZE}  # No longer useful?\n+                })\n+\n+            formats.append(dct)\n+\n+        def process_manifest_format(f, proto, client_name, itag, all_formats=False):\n+            key = (proto, f.get('language'))\n+            if not all_formats and key in itags[itag]:\n+                return False\n+            itags[itag].add(key)\n+\n+            if itag:\n+                f['format_id'] = (\n+                    '{0}-{1}'.format(itag, proto)\n+                    if all_formats or any(p != proto for p, _ in itags[itag])\n+                    else itag)\n+\n+            if f.get('source_preference') is None:\n+                f['source_preference'] = -1\n+\n+            if itag in ('616', '235'):\n+                f['format_note'] = join_nonempty(f.get('format_note'), 'Premium', delim=' ')\n+                f['source_preference'] += 100\n+\n+            f['quality'] = q(traverse_obj(f, (\n+                'format_id', T(lambda s: itag_qualities[s.split('-')[0]])), default=-1))\n+            if try_call(lambda x: f['fps'] <= 1):\n+                del f['fps']\n+\n+            if proto == 'hls' and f.get('has_drm'):\n+                f['has_drm'] = 'maybe'\n+                f['source_preference'] -= 5\n+            return True\n+\n+        hls_manifest_url = streaming_data.get('hlsManifestUrl')\n+        if hls_manifest_url:\n+            for f in self._extract_m3u8_formats(\n+                    hls_manifest_url, video_id, 'mp4', fatal=False):\n+                if process_manifest_format(\n+                        f, 'hls', None, self._search_regex(\n+                            r'/itag/(\\d+)', f['url'], 'itag', default=None)):\n+                    formats.append(f)\n+\n+        if self._downloader.params.get('youtube_include_dash_manifest', True):\n+            dash_manifest_url = streaming_data.get('dashManifestUrl')\n+            if dash_manifest_url:\n+                for f in self._extract_mpd_formats(\n+                        dash_manifest_url, video_id, fatal=False):\n+                    if process_manifest_format(\n+                            f, 'dash', None, f['format_id']):\n+                        f['filesize'] = traverse_obj(f, (\n+                            ('fragment_base_url', 'url'), T(lambda u: self._search_regex(\n+                                r'/clen/(\\d+)', u, 'file size', default=None)),\n+                            T(int_or_none)), get_all=False)\n+                        formats.append(f)\n+\n+        playable_formats = [f for f in formats if not f.get('has_drm')]\n+        if formats:\n+            if not playable_formats:\n+                # If there are no formats that definitely don't have DRM, all have DRM\n+                self.report_drm(video_id)\n+            formats[:] = playable_formats\n+        else:\n+            if streaming_data.get('licenseInfos'):\n+                raise ExtractorError(\n+                    'This video is DRM protected.', expected=True)\n+            pemr = try_get(\n+                playability_status,\n+                lambda x: x['errorScreen']['playerErrorMessageRenderer'],\n+                dict) or {}\n+            reason = get_text(pemr.get('reason')) or playability_status.get('reason')\n+            subreason = pemr.get('subreason')\n+            if subreason:\n+                subreason = clean_html(get_text(subreason))\n+                if subreason == 'The uploader has not made this video available in your country.':\n+                    countries = microformat.get('availableCountries')\n+                    if not countries:\n+                        regions_allowed = search_meta('regionsAllowed')\n+                        countries = regions_allowed.split(',') if regions_allowed else None\n+                    self.raise_geo_restricted(\n+                        subreason, countries)\n+                reason += '\\n' + subreason\n+            if reason:\n+                raise ExtractorError(reason, expected=True)\n+\n+        self._sort_formats(formats)\n+\n+        keywords = video_details.get('keywords') or []\n+        if not keywords and webpage:\n+            keywords = [\n+                unescapeHTML(m.group('content'))\n+                for m in re.finditer(self._meta_regex('og:video:tag'), webpage)]\n+        for keyword in keywords:\n+            if keyword.startswith('yt:stretch='):\n+                mobj = re.search(r'(\\d+)\\s*:\\s*(\\d+)', keyword)\n+                if mobj:\n+                    # NB: float is intentional for forcing float division\n+                    w, h = (float(v) for v in mobj.groups())\n+                    if w > 0 and h > 0:\n+                        ratio = w / h\n+                        for f in formats:\n+                            if f.get('vcodec') != 'none':\n+                                f['stretched_ratio'] = ratio\n                         break\n \n-    def _shelf_entries_from_content(self, shelf_renderer):\n-        content = shelf_renderer.get('content')\n-        if not isinstance(content, dict):\n-            return\n-        renderer = content.get('gridRenderer')\n-        if renderer:\n-            # TODO: add support for nested playlists so each shelf is processed\n-            # as separate playlist\n-            # TODO: this includes only first N items\n-            for entry in self._grid_entries(renderer):\n-                yield entry\n-        renderer = content.get('horizontalListRenderer')\n-        if renderer:\n-            # TODO\n-            pass\n-\n-    def _shelf_entries(self, shelf_renderer, skip_channels=False):\n-        ep = try_get(\n-            shelf_renderer, lambda x: x['endpoint']['commandMetadata']['webCommandMetadata']['url'],\n-            compat_str)\n-        shelf_url = urljoin('https://www.youtube.com', ep)\n-        if shelf_url:\n-            # Skipping links to another channels, note that checking for\n-            # endpoint.commandMetadata.webCommandMetadata.webPageTypwebPageType == WEB_PAGE_TYPE_CHANNEL\n-            # will not work\n-            if skip_channels and '/channels?' in shelf_url:\n-                return\n-            title = try_get(\n-                shelf_renderer, lambda x: x['title']['runs'][0]['text'], compat_str)\n-            yield self.url_result(shelf_url, video_title=title)\n-        # Shelf may not contain shelf URL, fallback to extraction from content\n-        for entry in self._shelf_entries_from_content(shelf_renderer):\n-            yield entry\n-\n-    def _playlist_entries(self, video_list_renderer):\n-        for content in video_list_renderer['contents']:\n-            if not isinstance(content, dict):\n-                continue\n-            renderer = content.get('playlistVideoRenderer') or content.get('playlistPanelVideoRenderer')\n-            if not isinstance(renderer, dict):\n-                continue\n-            video_id = renderer.get('videoId')\n-            if not video_id:\n-                continue\n-            yield self._extract_video(renderer)\n-\n-    def _video_entry(self, video_renderer):\n-        video_id = video_renderer.get('videoId')\n-        if video_id:\n-            return self._extract_video(video_renderer)\n-\n-    def _post_thread_entries(self, post_thread_renderer):\n-        post_renderer = try_get(\n-            post_thread_renderer, lambda x: x['post']['backstagePostRenderer'], dict)\n-        if not post_renderer:\n-            return\n-        # video attachment\n-        video_renderer = try_get(\n-            post_renderer, lambda x: x['backstageAttachment']['videoRenderer'], dict)\n-        video_id = None\n-        if video_renderer:\n-            entry = self._video_entry(video_renderer)\n-            if entry:\n-                yield entry\n-        # inline video links\n-        runs = try_get(post_renderer, lambda x: x['contentText']['runs'], list) or []\n-        for run in runs:\n-            if not isinstance(run, dict):\n-                continue\n-            ep_url = try_get(\n-                run, lambda x: x['navigationEndpoint']['urlEndpoint']['url'], compat_str)\n-            if not ep_url:\n-                continue\n-            if not YoutubeIE.suitable(ep_url):\n-                continue\n-            ep_video_id = YoutubeIE._match_id(ep_url)\n-            if video_id == ep_video_id:\n-                continue\n-            yield self.url_result(ep_url, ie=YoutubeIE.ie_key(), video_id=video_id)\n-\n-    def _post_thread_continuation_entries(self, post_thread_continuation):\n-        contents = post_thread_continuation.get('contents')\n-        if not isinstance(contents, list):\n-            return\n-        for content in contents:\n-            renderer = content.get('backstagePostThreadRenderer')\n-            if not isinstance(renderer, dict):\n-                continue\n-            for entry in self._post_thread_entries(renderer):\n-                yield entry\n-\n-    def _rich_grid_entries(self, contents):\n-        for content in contents:\n-            content = traverse_obj(\n-                content, ('richItemRenderer', 'content'),\n-                expected_type=dict) or {}\n-            video_renderer = traverse_obj(\n-                content, 'videoRenderer', 'reelItemRenderer',\n-                expected_type=dict)\n-            if video_renderer:\n-                entry = self._video_entry(video_renderer)\n-                if entry:\n-                    yield entry\n-            # playlist\n-            renderer = traverse_obj(\n-                content, 'playlistRenderer', expected_type=dict) or {}\n-            title = self._get_text(renderer, 'title')\n-            playlist_id = renderer.get('playlistId')\n-            if playlist_id:\n-                yield self.url_result(\n-                    'https://www.youtube.com/playlist?list=%s' % playlist_id,\n-                    ie=YoutubeTabIE.ie_key(), video_id=playlist_id,\n-                    video_title=title)\n-\n-    @staticmethod\n-    def _build_continuation_query(continuation, ctp=None):\n-        query = {\n-            'ctoken': continuation,\n-            'continuation': continuation,\n+        thumbnails = []\n+        for container in (video_details, microformat):\n+            for thumbnail in try_get(\n+                    container,\n+                    lambda x: x['thumbnail']['thumbnails'], list) or []:\n+                thumbnail_url = url_or_none(thumbnail.get('url'))\n+                if not thumbnail_url:\n+                    continue\n+                thumbnails.append({\n+                    'height': int_or_none(thumbnail.get('height')),\n+                    'url': update_url(thumbnail_url, query=None, fragment=None),\n+                    'width': int_or_none(thumbnail.get('width')),\n+                })\n+            if thumbnails:\n+                break\n+        else:\n+            thumbnail = search_meta(['og:image', 'twitter:image'])\n+            if thumbnail:\n+                thumbnails = [{'url': thumbnail}]\n+\n+        category = microformat.get('category') or search_meta('genre')\n+        channel_id = self._extract_channel_id(\n+            webpage, videodetails=video_details, metadata=microformat)\n+        duration = int_or_none(\n+            video_details.get('lengthSeconds')\n+            or microformat.get('lengthSeconds')) \\\n+            or parse_duration(search_meta('duration'))\n+\n+        for f in formats:\n+            # Some formats may have much smaller duration than others (possibly damaged during encoding)\n+            # but avoid false positives with small duration differences.\n+            # Ref: https://github.com/yt-dlp/yt-dlp/issues/2823\n+            if try_call(lambda x: float(x.pop('_duration_ms')) / duration < 500, args=(f,)):\n+                self.report_warning(\n+                    '{0}: Some possibly damaged formats will be deprioritized'.format(video_id), only_once=True)\n+                # Strictly de-prioritize damaged formats\n+                f['preference'] = -10\n+\n+        is_live = video_details.get('isLive')\n+\n+        owner_profile_url = self._yt_urljoin(self._extract_author_var(\n+            webpage, 'url', videodetails=video_details, metadata=microformat))\n+\n+        uploader = self._extract_author_var(\n+            webpage, 'name', videodetails=video_details, metadata=microformat)\n+\n+        info = {\n+            'id': video_id,\n+            'title': self._live_title(video_title) if is_live else video_title,\n+            'formats': formats,\n+            'thumbnails': thumbnails,\n+            'description': video_description,\n+            'upload_date': unified_strdate(\n+                microformat.get('uploadDate')\n+                or search_meta('uploadDate')),\n+            'uploader': uploader,\n+            'channel_id': channel_id,\n+            'duration': duration,\n+            'view_count': int_or_none(\n+                video_details.get('viewCount')\n+                or microformat.get('viewCount')\n+                or search_meta('interactionCount')),\n+            'average_rating': float_or_none(video_details.get('averageRating')),\n+            'age_limit': 18 if (\n+                microformat.get('isFamilySafe') is False\n+                or search_meta('isFamilyFriendly') == 'false'\n+                or search_meta('og:restrictions:age') == '18+') else 0,\n+            'webpage_url': webpage_url,\n+            'categories': [category] if category else None,\n+            'tags': keywords,\n+            'is_live': is_live,\n         }\n-        if ctp:\n-            query['itct'] = ctp\n-        return query\n-\n-    @staticmethod\n-    def _extract_next_continuation_data(renderer):\n-        next_continuation = try_get(\n-            renderer, lambda x: x['continuations'][0]['nextContinuationData'], dict)\n-        if not next_continuation:\n-            return\n-        continuation = next_continuation.get('continuation')\n-        if not continuation:\n-            return\n-        ctp = next_continuation.get('clickTrackingParams')\n-        return YoutubeTabIE._build_continuation_query(continuation, ctp)\n-\n-    @classmethod\n-    def _extract_continuation(cls, renderer):\n-        next_continuation = cls._extract_next_continuation_data(renderer)\n-        if next_continuation:\n-            return next_continuation\n-        contents = []\n-        for key in ('contents', 'items'):\n-            contents.extend(try_get(renderer, lambda x: x[key], list) or [])\n-        for content in contents:\n-            if not isinstance(content, dict):\n-                continue\n-            continuation_ep = try_get(\n-                content, lambda x: x['continuationItemRenderer']['continuationEndpoint'],\n-                dict)\n-            if not continuation_ep:\n-                continue\n-            continuation = try_get(\n-                continuation_ep, lambda x: x['continuationCommand']['token'], compat_str)\n-            if not continuation:\n-                continue\n-            ctp = continuation_ep.get('clickTrackingParams')\n-            return YoutubeTabIE._build_continuation_query(continuation, ctp)\n-\n-    def _entries(self, tab, item_id, webpage):\n-        tab_content = try_get(tab, lambda x: x['content'], dict)\n-        if not tab_content:\n-            return\n-        slr_renderer = try_get(tab_content, lambda x: x['sectionListRenderer'], dict)\n-        if slr_renderer:\n-            is_channels_tab = tab.get('title') == 'Channels'\n-            continuation = None\n-            slr_contents = try_get(slr_renderer, lambda x: x['contents'], list) or []\n-            for slr_content in slr_contents:\n-                if not isinstance(slr_content, dict):\n+\n+        pctr = try_get(\n+            player_response,\n+            lambda x: x['captions']['playerCaptionsTracklistRenderer'], dict)\n+        if pctr:\n+            def process_language(container, base_url, lang_code, query):\n+                lang_subs = []\n+                for fmt in self._SUBTITLE_FORMATS:\n+                    query.update({\n+                        'fmt': fmt,\n+                    })\n+                    lang_subs.append({\n+                        'ext': fmt,\n+                        'url': update_url_query(base_url, query),\n+                    })\n+                container[lang_code] = lang_subs\n+\n+            subtitles = {}\n+            for caption_track in (pctr.get('captionTracks') or []):\n+                base_url = caption_track.get('baseUrl')\n+                if not base_url:\n                     continue\n-                is_renderer = try_get(slr_content, lambda x: x['itemSectionRenderer'], dict)\n-                if not is_renderer:\n+                if caption_track.get('kind') != 'asr':\n+                    lang_code = caption_track.get('languageCode')\n+                    if not lang_code:\n+                        continue\n+                    process_language(\n+                        subtitles, base_url, lang_code, {})\n                     continue\n-                isr_contents = try_get(is_renderer, lambda x: x['contents'], list) or []\n-                for isr_content in isr_contents:\n-                    if not isinstance(isr_content, dict):\n+                automatic_captions = {}\n+                for translation_language in (pctr.get('translationLanguages') or []):\n+                    translation_language_code = translation_language.get('languageCode')\n+                    if not translation_language_code:\n                         continue\n-                    renderer = isr_content.get('playlistVideoListRenderer')\n-                    if renderer:\n-                        for entry in self._playlist_entries(renderer):\n-                            yield entry\n-                        continuation = self._extract_continuation(renderer)\n+                    process_language(\n+                        automatic_captions, base_url, translation_language_code,\n+                        {'tlang': translation_language_code})\n+                info['automatic_captions'] = automatic_captions\n+            info['subtitles'] = subtitles\n+\n+        parsed_url = compat_urllib_parse_urlparse(url)\n+        for component in [parsed_url.fragment, parsed_url.query]:\n+            query = compat_parse_qs(component)\n+            for k, v in query.items():\n+                for d_k, s_ks in [('start', ('start', 't')), ('end', ('end',))]:\n+                    d_k += '_time'\n+                    if d_k not in info and k in s_ks:\n+                        info[d_k] = parse_duration(query[k][0])\n+\n+        if video_description:\n+            # Youtube Music Auto-generated description\n+            mobj = re.search(r'(?s)(?P<track>[^\u00b7\\n]+)\u00b7(?P<artist>[^\\n]+)\\n+(?P<album>[^\\n]+)(?:.+?\u2117\\s*(?P<release_year>\\d{4})(?!\\d))?(?:.+?Released on\\s*:\\s*(?P<release_date>\\d{4}-\\d{2}-\\d{2}))?(.+?\\nArtist\\s*:\\s*(?P<clean_artist>[^\\n]+))?.+\\nAuto-generated by YouTube\\.\\s*$', video_description)\n+            if mobj:\n+                release_year = mobj.group('release_year')\n+                release_date = mobj.group('release_date')\n+                if release_date:\n+                    release_date = release_date.replace('-', '')\n+                    if not release_year:\n+                        release_year = release_date[:4]\n+                info.update({\n+                    'album': mobj.group('album'.strip()),\n+                    'artist': mobj.group('clean_artist') or ', '.join(a.strip() for a in mobj.group('artist').split('\u00b7')),\n+                    'track': mobj.group('track').strip(),\n+                    'release_date': release_date,\n+                    'release_year': int_or_none(release_year),\n+                })\n+\n+        initial_data = None\n+        if webpage:\n+            initial_data = self._extract_yt_initial_variable(\n+                webpage, self._YT_INITIAL_DATA_RE, video_id,\n+                'yt initial data')\n+        if not initial_data:\n+            initial_data = self._call_api(\n+                'next', {'videoId': video_id}, video_id, fatal=False)\n+\n+        if initial_data:\n+            chapters = self._extract_chapters_from_json(\n+                initial_data, video_id, duration)\n+            if not chapters:\n+                for engagment_pannel in (initial_data.get('engagementPanels') or []):\n+                    contents = try_get(\n+                        engagment_pannel, lambda x: x['engagementPanelSectionListRenderer']['content']['macroMarkersListRenderer']['contents'],\n+                        list)\n+                    if not contents:\n                         continue\n-                    renderer = isr_content.get('gridRenderer')\n-                    if renderer:\n-                        for entry in self._grid_entries(renderer):\n-                            yield entry\n-                        continuation = self._extract_continuation(renderer)\n-                        continue\n-                    renderer = isr_content.get('shelfRenderer')\n-                    if renderer:\n-                        for entry in self._shelf_entries(renderer, not is_channels_tab):\n-                            yield entry\n-                        continue\n-                    renderer = isr_content.get('backstagePostThreadRenderer')\n-                    if renderer:\n-                        for entry in self._post_thread_entries(renderer):\n-                            yield entry\n-                        continuation = self._extract_continuation(renderer)\n-                        continue\n-                    renderer = isr_content.get('videoRenderer')\n-                    if renderer:\n-                        entry = self._video_entry(renderer)\n-                        if entry:\n-                            yield entry\n-\n-                if not continuation:\n-                    continuation = self._extract_continuation(is_renderer)\n-            if not continuation:\n-                continuation = self._extract_continuation(slr_renderer)\n-        else:\n-            rich_grid_renderer = tab_content.get('richGridRenderer')\n-            if not rich_grid_renderer:\n-                return\n-            for entry in self._rich_grid_entries(rich_grid_renderer.get('contents') or []):\n-                yield entry\n-\n-            continuation = self._extract_continuation(rich_grid_renderer)\n-\n-        ytcfg = self._extract_ytcfg(item_id, webpage)\n-        client_version = try_get(\n-            ytcfg, lambda x: x['INNERTUBE_CLIENT_VERSION'], compat_str) or '2.20210407.08.00'\n-\n-        headers = {\n-            'x-youtube-client-name': '1',\n-            'x-youtube-client-version': client_version,\n-            'content-type': 'application/json',\n-        }\n-\n-        context = try_get(ytcfg, lambda x: x['INNERTUBE_CONTEXT'], dict) or {\n-            'client': {\n-                'clientName': 'WEB',\n-                'clientVersion': client_version,\n-            }\n-        }\n-        visitor_data = try_get(context, lambda x: x['client']['visitorData'], compat_str)\n-\n-        identity_token = self._extract_identity_token(ytcfg, webpage)\n-        if identity_token:\n-            headers['x-youtube-identity-token'] = identity_token\n-\n-        data = {\n-            'context': context,\n-        }\n-\n-        for page_num in itertools.count(1):\n-            if not continuation:\n-                break\n-            if visitor_data:\n-                headers['x-goog-visitor-id'] = visitor_data\n-            data['continuation'] = continuation['continuation']\n-            data['clickTracking'] = {\n-                'clickTrackingParams': continuation['itct']\n-            }\n-            count = 0\n-            retries = 3\n-            while count <= retries:\n-                try:\n-                    # Downloading page may result in intermittent 5xx HTTP error\n-                    # that is usually worked around with a retry\n-                    response = self._download_json(\n-                        'https://www.youtube.com/youtubei/v1/browse?key=AIzaSyAO_FJ2SlqU8Q4STEHLGCilw_Y9_11qcW8',\n-                        None, 'Downloading page %d%s' % (page_num, ' (retry #%d)' % count if count else ''),\n-                        headers=headers, data=json.dumps(data).encode('utf8'))\n-                    break\n-                except ExtractorError as e:\n-                    if isinstance(e.cause, compat_HTTPError) and e.cause.code in (500, 503):\n-                        count += 1\n-                        if count <= retries:\n+\n+                    def chapter_time(mmlir):\n+                        return parse_duration(\n+                            get_text(mmlir.get('timeDescription')))\n+\n+                    chapters = []\n+                    for next_num, content in enumerate(contents, start=1):\n+                        mmlir = content.get('macroMarkersListItemRenderer') or {}\n+                        start_time = chapter_time(mmlir)\n+                        end_time = chapter_time(try_get(\n+                            contents, lambda x: x[next_num]['macroMarkersListItemRenderer'])) \\\n+                            if next_num < len(contents) else duration\n+                        if start_time is None or end_time is None:\n                             continue\n-                    raise\n-            if not response:\n-                break\n-\n-            visitor_data = try_get(\n-                response, lambda x: x['responseContext']['visitorData'], compat_str) or visitor_data\n-\n-            continuation_contents = try_get(\n-                response, lambda x: x['continuationContents'], dict)\n-            if continuation_contents:\n-                continuation_renderer = continuation_contents.get('playlistVideoListContinuation')\n-                if continuation_renderer:\n-                    for entry in self._playlist_entries(continuation_renderer):\n-                        yield entry\n-                    continuation = self._extract_continuation(continuation_renderer)\n+                        chapters.append({\n+                            'start_time': start_time,\n+                            'end_time': end_time,\n+                            'title': get_text(mmlir.get('title')),\n+                        })\n+                    if chapters:\n+                        break\n+            if chapters:\n+                info['chapters'] = chapters\n+\n+            contents = try_get(\n+                initial_data,\n+                lambda x: x['contents']['twoColumnWatchNextResults']['results']['results']['contents'],\n+                list) or []\n+            if not info['channel_id']:\n+                channel_id = self._extract_channel_id('', renderers=contents)\n+            if not info['uploader']:\n+                info['uploader'] = self._extract_author_var('', 'name', renderers=contents)\n+            if not owner_profile_url:\n+                owner_profile_url = self._yt_urljoin(self._extract_author_var('', 'url', renderers=contents))\n+\n+            for content in contents:\n+                vpir = content.get('videoPrimaryInfoRenderer')\n+                if vpir:\n+                    stl = vpir.get('superTitleLink')\n+                    if stl:\n+                        stl = get_text(stl)\n+                        if try_get(\n+                                vpir,\n+                                lambda x: x['superTitleIcon']['iconType']) == 'LOCATION_PIN':\n+                            info['location'] = stl\n+                        else:\n+                            # \u2022? doesn't match, but [\u2022]? does; \\xa0 = non-breaking space\n+                            mobj = re.search(r'([^\\xa0\\s].*?)[\\xa0\\s]*S(\\d+)[\\xa0\\s]*[\u2022]?[\\xa0\\s]*E(\\d+)', stl)\n+                            if mobj:\n+                                info.update({\n+                                    'series': mobj.group(1),\n+                                    'season_number': int(mobj.group(2)),\n+                                    'episode_number': int(mobj.group(3)),\n+                                })\n+                    for tlb in (try_get(\n+                            vpir,\n+                            lambda x: x['videoActions']['menuRenderer']['topLevelButtons'],\n+                            list) or []):\n+                        tbr = traverse_obj(tlb, ('segmentedLikeDislikeButtonRenderer', 'likeButton', 'toggleButtonRenderer'), 'toggleButtonRenderer') or {}\n+                        for getter, regex in [(\n+                                lambda x: x['defaultText']['accessibility']['accessibilityData'],\n+                                r'(?P<count>[\\d,]+)\\s*(?P<type>(?:dis)?like)'), ([\n+                                    lambda x: x['accessibility'],\n+                                    lambda x['accessibilityData']['accessibilityData'],\n+                                ], r'(?P<type>(?:dis)?like) this video along with (?P<count>[\\d,]+) other people')]:\n+                            label = (try_get(tbr, getter, dict) or {}).get('label')\n+                            if label:\n+                                mobj = re.match(regex, label)\n+                                if mobj:\n+                                    info[mobj.group('type') + '_count'] = str_to_int(mobj.group('count'))\n+                                    break\n+                    sbr_tooltip = try_get(\n+                        vpir, lambda x: x['sentimentBar']['sentimentBarRenderer']['tooltip'])\n+                    if sbr_tooltip:\n+                        # however dislike_count was hidden by YT, as if there could ever be dislikable content on YT\n+                        like_count, dislike_count = sbr_tooltip.split(' / ')\n+                        info.update({\n+                            'like_count': str_to_int(like_count),\n+                            'dislike_count': str_to_int(dislike_count),\n+                        })\n+                    else:\n+                        info['like_count'] = traverse_obj(vpir, (\n+                            'videoActions', 'menuRenderer', 'topLevelButtons', Ellipsis,\n+                            'segmentedLikeDislikeButtonViewModel', 'likeButtonViewModel', 'likeButtonViewModel',\n+                            'toggleButtonViewModel', 'toggleButtonViewModel', 'defaultButtonViewModel',\n+                            'buttonViewModel', (('title', ('accessibilityText', T(lambda s: s.split()), Ellipsis))), T(parse_count)),\n+                            get_all=False)\n+\n+                vsir = content.get('videoSecondaryInfoRenderer')\n+                if vsir:\n+                    rows = try_get(\n+                        vsir,\n+                        lambda x: x['metadataRowContainer']['metadataRowContainerRenderer']['rows'],\n+                        list) or []\n+                    multiple_songs = False\n+                    for row in rows:\n+                        if try_get(row, lambda x: x['metadataRowRenderer']['hasDividerLine']) is True:\n+                            multiple_songs = True\n+                            break\n+                    for row in rows:\n+                        mrr = row.get('metadataRowRenderer') or {}\n+                        mrr_title = mrr.get('title')\n+                        if not mrr_title:\n+                            continue\n+                        mrr_title = get_text(mrr['title'])\n+                        mrr_contents_text = get_text(mrr['contents'][0])\n+                        if mrr_title == 'License':\n+                            info['license'] = mrr_contents_text\n+                        elif not multiple_songs:\n+                            if mrr_title == 'Album':\n+                                info['album'] = mrr_contents_text\n+                            elif mrr_title == 'Artist':\n+                                info['artist'] = mrr_contents_text\n+                            elif mrr_title == 'Song':\n+                                info['track'] = mrr_contents_text\n+\n+            # this is not extraction but spelunking!\n+            carousel_lockups = traverse_obj(\n+                initial_data,\n+                ('engagementPanels', Ellipsis, 'engagementPanelSectionListRenderer',\n+                 'content', 'structuredDescriptionContentRenderer', 'items', Ellipsis,\n+                 'videoDescriptionMusicSectionRenderer', 'carouselLockups', Ellipsis),\n+                expected_type=dict) or []\n+            # try to reproduce logic from metadataRowContainerRenderer above (if it still is)\n+            fields = (('ALBUM', 'album'), ('ARTIST', 'artist'), ('SONG', 'track'), ('LICENSES', 'license'))\n+            # multiple_songs ?\n+            if len(carousel_lockups) > 1:\n+                fields = fields[-1:]\n+            for info_row in traverse_obj(\n+                    carousel_lockups,\n+                    (0, 'carouselLockupRenderer', 'infoRows', Ellipsis, 'infoRowRenderer'),\n+                    expected_type=dict):\n+                row_title = traverse_obj(info_row, ('title', 'simpleText'))\n+                row_text = traverse_obj(info_row, 'defaultMetadata', 'expandedMetadata', expected_type=get_text)\n+                if not row_text:\n                     continue\n-                continuation_renderer = continuation_contents.get('gridContinuation')\n-                if continuation_renderer:\n-                    for entry in self._grid_entries(continuation_renderer):\n-                        yield entry\n-                    continuation = self._extract_continuation(continuation_renderer)\n-                    continue\n-                continuation_renderer = continuation_contents.get('itemSectionContinuation')\n-                if continuation_renderer:\n-                    for entry in self._post_thread_continuation_entries(continuation_renderer):\n-                        yield entry\n-                    continuation = self._extract_continuation(continuation_renderer)\n-                    continue\n-\n-            on_response_received = dict_get(response, ('onResponseReceivedActions', 'onResponseReceivedEndpoints'))\n-            continuation_items = try_get(\n-                on_response_received, lambda x: x[0]['appendContinuationItemsAction']['continuationItems'], list)\n-            if continuation_items:\n-                continuation_item = continuation_items[0]\n-                if not isinstance(continuation_item, dict):\n-                    continue\n-                renderer = self._extract_grid_item_renderer(continuation_item)\n-                if renderer:\n-                    grid_renderer = {'items': continuation_items}\n-                    for entry in self._grid_entries(grid_renderer):\n-                        yield entry\n-                    continuation = self._extract_continuation(grid_renderer)\n-                    continue\n-                renderer = continuation_item.get('playlistVideoRenderer') or continuation_item.get('itemSectionRenderer')\n-                if renderer:\n-                    video_list_renderer = {'contents': continuation_items}\n-                    for entry in self._playlist_entries(video_list_renderer):\n-                        yield entry\n-                    continuation = self._extract_continuation(video_list_renderer)\n-                    continue\n-                renderer = continuation_item.get('backstagePostThreadRenderer')\n-                if renderer:\n-                    continuation_renderer = {'contents': continuation_items}\n-                    for entry in self._post_thread_continuation_entries(continuation_renderer):\n-                        yield entry\n-                    continuation = self._extract_continuation(continuation_renderer)\n-                    continue\n-                renderer = continuation_item.get('richItemRenderer')\n-                if renderer:\n-                    for entry in self._rich_grid_entries(continuation_items):\n-                        yield entry\n-                    continuation = self._extract_continuation({'contents': continuation_items})\n-                    continue\n-\n-            break\n-\n-    @staticmethod\n-    def _extract_selected_tab(tabs):\n-        for tab in tabs:\n-            renderer = dict_get(tab, ('tabRenderer', 'expandableTabRenderer')) or {}\n-            if renderer.get('selected') is True:\n-                return renderer\n-        else:\n-            raise ExtractorError('Unable to find selected tab')\n-\n-    def _extract_uploader(self, metadata, data):\n-        uploader = {}\n-        renderers = traverse_obj(data,\n-                                 ('sidebar', 'playlistSidebarRenderer', 'items'))\n-        uploader['channel_id'] = self._extract_channel_id('', metadata=metadata, renderers=renderers)\n-        uploader['uploader'] = (\n-            self._extract_author_var('', 'name', renderers=renderers)\n-            or self._extract_author_var('', 'name', metadata=metadata))\n-        uploader['uploader_url'] = self._yt_urljoin(\n-            self._extract_author_var('', 'url', metadata=metadata, renderers=renderers))\n-        uploader['uploader_id'] = self._extract_uploader_id(uploader['uploader_url'])\n-        uploader['channel'] = uploader['uploader']\n-        return uploader\n-\n-    @classmethod\n-    def _extract_alert(cls, data):\n-        alerts = []\n-        for alert in traverse_obj(data, ('alerts', Ellipsis), expected_type=dict):\n-            alert_text = traverse_obj(\n-                alert, (None, lambda x: x['alertRenderer']['text']), get_all=False)\n-            if not alert_text:\n-                continue\n-            text = cls._get_text(alert_text, 'text')\n-            if text:\n-                alerts.append(text)\n-        return '\\n'.join(alerts)\n-\n-    def _extract_from_tabs(self, item_id, webpage, data, tabs):\n-        selected_tab = self._extract_selected_tab(tabs)\n-        renderer = traverse_obj(data, ('metadata', 'channelMetadataRenderer'),\n-                                expected_type=dict) or {}\n-        playlist_id = item_id\n-        title = description = None\n-        if renderer:\n-            channel_title = txt_or_none(renderer.get('title')) or item_id\n-            tab_title = txt_or_none(selected_tab.get('title'))\n-            title = join_nonempty(\n-                channel_title or item_id, tab_title,\n-                txt_or_none(selected_tab.get('expandedText')),\n-                delim=' - ')\n-            description = txt_or_none(renderer.get('description'))\n-            playlist_id = txt_or_none(renderer.get('externalId')) or playlist_id\n-        else:\n-            renderer = traverse_obj(data,\n-                                    ('metadata', 'playlistMetadataRenderer'),\n-                                    ('header', 'hashtagHeaderRenderer'),\n-                                    expected_type=dict) or {}\n-            title = traverse_obj(renderer, 'title', ('hashtag', 'simpleText'),\n-                                 expected_type=txt_or_none)\n-        playlist = self.playlist_result(\n-            self._entries(selected_tab, item_id, webpage),\n-            playlist_id=playlist_id, playlist_title=title,\n-            playlist_description=description)\n-        return merge_dicts(playlist, self._extract_uploader(renderer, data))\n-\n-    def _extract_from_playlist(self, item_id, url, data, playlist):\n-        title = traverse_obj((playlist, data),\n-                             (0, 'title'), (1, 'titleText', 'simpleText'),\n-                             expected_type=txt_or_none)\n-        playlist_id = txt_or_none(playlist.get('playlistId')) or item_id\n-        # Inline playlist rendition continuation does not always work\n-        # at Youtube side, so delegating regular tab-based playlist URL\n-        # processing whenever possible.\n-        playlist_url = urljoin(url, traverse_obj(\n-            playlist, ('endpoint', 'commandMetadata', 'webCommandMetadata', 'url'),\n-            expected_type=url_or_none))\n-        if playlist_url and playlist_url != url:\n-            return self.url_result(\n-                playlist_url, ie=YoutubeTabIE.ie_key(), video_id=playlist_id,\n-                video_title=title)\n-        return self.playlist_result(\n-            self._playlist_entries(playlist), playlist_id=playlist_id,\n-            playlist_title=title)\n-\n-    def _extract_identity_token(self, ytcfg, webpage):\n-        if ytcfg:\n-            token = try_get(ytcfg, lambda x: x['ID_TOKEN'], compat_str)\n-            if token:\n-                return token\n-        return self._search_regex(\n-            r'\\bID_TOKEN[\"\\']\\s*:\\s*[\"\\'](.+?)[\"\\']', webpage,\n-            'identity token', default=None)\n-\n-    def _real_extract(self, url):\n-        item_id = self._match_id(url)\n-        url = update_url(url, netloc='www.youtube.com')\n-        # Handle both video/playlist URLs\n-        qs = parse_qs(url)\n-        video_id = qs.get('v', [None])[0]\n-        playlist_id = qs.get('list', [None])[0]\n-        if video_id and playlist_id:\n-            if self._downloader.params.get('noplaylist'):\n-                self.to_screen('Downloading just video %s because of --no-playlist' % video_id)\n-                return self.url_result(video_id, ie=YoutubeIE.ie_key(), video_id=video_id)\n-            self.to_screen('Downloading playlist %s - add --no-playlist to just download video %s' % (playlist_id, video_id))\n-        webpage = self._download_webpage(url, item_id)\n-        data = self._extract_yt_initial_data(item_id, webpage)\n-        tabs = try_get(\n-            data, lambda x: x['contents']['twoColumnBrowseResultsRenderer']['tabs'], list)\n-        if tabs:\n-            return self._extract_from_tabs(item_id, webpage, data, tabs)\n-        playlist = try_get(\n-            data, lambda x: x['contents']['twoColumnWatchNextResults']['playlist']['playlist'], dict)\n-        if playlist:\n-            return self._extract_from_playlist(item_id, url, data, playlist)\n-        # Fallback to video extraction if no playlist alike page is recognized.\n-        # First check for the current video then try the v attribute of URL query.\n-        video_id = try_get(\n-            data, lambda x: x['currentVideoEndpoint']['watchEndpoint']['videoId'],\n-            compat_str) or video_id\n-        if video_id:\n-            return self.url_result(video_id, ie=YoutubeIE.ie_key(), video_id=video_id)\n-        # Capture and output alerts\n-        alert = self._extract_alert(data)\n-        if alert:\n-            raise ExtractorError(alert, expected=True)\n-        # Failed to recognize\n-        raise ExtractorError('Unable to recognize tab page')\n-\n-\n-class YoutubePlaylistIE(InfoExtractor):\n-    IE_DESC = 'YouTube.com playlists'\n-    _VALID_URL = r'''(?x)(?:\n-                        (?:https?://)?\n+                for name, field in fields:\n+                    if name == row_title and not info.get(field):\n+                        info[field] = row_text\n+\n+        for s_k, d_k in [('artist', 'creator'), ('track', 'alt_title')]:\n+            v = info.get(s_k)\n+            if v:\n+                info[d_k] = v\n+\n+        self.mark_watched(video_id, player_response)\n+\n+        return merge_dicts(\n+            info, {\n+                'uploader_id': self._extract_uploader_id(owner_profile_url),\n+                'uploader_url': owner_profile_url,\n+                'channel_id': channel_id,\n+                'channel_url': channel_id and self._yt_urljoin('/channel/' + channel_id),\n+                'channel': info['uploader'],\n+            })\n+\n+\n+class YoutubeTabIE(YoutubeBaseInfoExtractor):\n+    IE_DESC = 'YouTube.com tab'\n+    _VALID_URL = r'''(?x)\n+                    https?://\n                         (?:\\w+\\.)?\n                         (?:\n-                            (?:\n-                                youtube(?:kids)?\\.com|\n-                                invidio\\.us\n-                            )\n-                            /.*?\\?.*?\\blist=\n-                        )?\n-                        (?P<id>%(playlist_id)s)\n-                     )''' % {'playlist_id': YoutubeBaseInfoExtractor._PLAYLIST_ID_RE}\n-    IE_NAME = 'youtube:playlist'\n+                            youtube(?:kids)?\\.com|\n+                            invidio\\.us\n+                        )/\n+                        (?:\n+                            (?:channel|c|user|feed|hashtag)/|\n+                            (?:playlist|watch)\\?.*?\\blist=|\n+                            (?!(?:watch|embed|v|e|results)\\b)\n+                        )\n+                        (?P<id>[^/?\\#&]+)\n+                    '''\n+    IE_NAME = 'youtube:tab'\n+\n     _TESTS = [{\n-        'note': 'issue #673',\n-        'url': 'PLBB231211A4F62143',\n+        # Shorts\n+        'url': 'https://www.youtube.com/@SuperCooperShorts/shorts',\n+        'playlist_mincount': 5,\n         'info_dict': {\n-            'title': '[OLD]Team Fortress 2 (Class-based LP)',\n-            'id': 'PLBB231211A4F62143',\n-            'uploader': 'Wickman',\n-            'uploader_id': '@WickmanVT',\n-            'channel_id': 'UCKSpbfbl5kRQpTdL7kMc-1Q',\n-        },\n-        'playlist_mincount': 29,\n-    }, {\n-        'url': 'PLtPgu7CB4gbY9oDN3drwC3cMbJggS7dKl',\n+            'description': 'Short clips from Super Cooper Sundays!',\n+            'id': 'UCKMA8kHZ8bPYpnMNaUSxfEQ',\n+            'title': 'Super Cooper Shorts - Shorts',\n+            'uploader': 'Super Cooper Shorts',\n+            'uploader_id': '@SuperCooperShorts',\n+        }\n+    }, {\n+        # Channel that does not have a Shorts tab. Test should just download videos on Home tab instead\n+        'url': 'https://www.youtube.com/@emergencyawesome/shorts',\n         'info_dict': {\n-            'title': 'YDL_safe_search',\n-            'id': 'PLtPgu7CB4gbY9oDN3drwC3cMbJggS7dKl',\n-        },\n-        'playlist_count': 2,\n-        'skip': 'This playlist is private',\n-    }, {\n-        'note': 'embedded',\n-        'url': 'https://www.youtube.com/embed/videoseries?list=PL6IaIsEjSbf96XFRuNccS_RuEXwNdsoEu',\n-        # TODO: full playlist requires _reload_with_unavailable_videos()\n-        # 'playlist_count': 4,\n-        'playlist_mincount': 1,\n+            'description': 'md5:592c080c06fef4de3c902c4a8eecd850',\n+            'id': 'UCDiFRMQWpcp8_KD4vwIVicw',\n+            'title': 'Emergency Awesome - Home',\n+        },\n+        'playlist_mincount': 5,\n+        'skip': 'new test page needed to replace `Emergency Awesome - Shorts`',\n+    }, {\n+        # playlists, multipage\n+        'url': 'https://www.youtube.com/c/\u0418\u0433\u043e\u0440\u044c\u041a\u043b\u0435\u0439\u043d\u0435\u0440/playlists?view=1&flow=grid',\n+        'playlist_mincount': 94,\n         'info_dict': {\n-            'title': 'JODA15',\n-            'id': 'PL6IaIsEjSbf96XFRuNccS_RuEXwNdsoEu',\n-            'uploader': 'milan',\n-            'uploader_id': '@milan5503',\n-            'channel_id': 'UCEI1-PVPcYXjB73Hfelbmaw',\n+            'id': 'UCqj7Cz7revf5maW9g5pgNcg',\n+            'title': r're:Igor Kleiner(?: Ph\\.D\\.)? - Playlists',\n+            'description': 'md5:be97ee0f14ee314f1f002cf187166ee2',\n+            'uploader': 'Igor Kleiner',\n+            'uploader_id': '@IgorDataScience',\n+        },\n+    }, {\n+        # playlists, multipage, different order\n+        'url': 'https://www.youtube.com/user/igorkle1/playlists?view=1&sort=dd',\n+        'playlist_mincount': 94,\n+        'info_dict': {\n+            'id': 'UCqj7Cz7revf5maW9g5pgNcg',\n+            'title': r're:Igor Kleiner(?: Ph\\.D\\.)? - Playlists',\n+            'description': 'md5:be97ee0f14ee314f1f002cf187166ee2',\n+            'uploader': 'Igor Kleiner',\n+            'uploader_id': '@IgorDataScience',\n+        },\n+    }, {\n+        # playlists, series\n+        'url': 'https://www.youtube.com/c/3blue1brown/playlists?view=50&sort=dd&shelf_id=3',\n+        'playlist_mincount': 5,\n+        'info_dict': {\n+            'id': 'UCYO_jab_esuFRV4b17AJtAw',\n+            'title': '3Blue1Brown - Playlists',\n+            'description': 'md5:e1384e8a133307dd10edee76e875d62f',\n+            'uploader': '3Blue1Brown',\n+            'uploader_id': '@3blue1brown',\n+        },\n+    }, {\n+        # playlists, singlepage\n+        'url': 'https://www.youtube.com/user/ThirstForScience/playlists',\n+        'playlist_mincount': 4,\n+        'info_dict': {\n+            'id': 'UCAEtajcuhQ6an9WEzY9LEMQ',\n+            'title': 'ThirstForScience - Playlists',\n+            'description': 'md5:609399d937ea957b0f53cbffb747a14c',\n+            'uploader': 'ThirstForScience',\n+            'uploader_id': '@ThirstForScience',\n         }\n     }, {\n-        'url': 'http://www.youtube.com/embed/_xDOZElKyNU?list=PLsyOSbh5bs16vubvKePAQ1x3PhKavfBIl',\n-        'playlist_mincount': 455,\n+        'url': 'https://www.youtube.com/c/ChristophLaimer/playlists',\n+        'only_matching': True,\n+    }, {\n+        # basic, single video playlist\n+        'url': 'https://www.youtube.com/playlist?list=PL4lCao7KL_QFVb7Iudeipvc2BCavECqzc',\n         'info_dict': {\n-            'title': '2018 Chinese New Singles (11/6 updated)',\n-            'id': 'PLsyOSbh5bs16vubvKePAQ1x3PhKavfBIl',\n-            'uploader': 'LBK',\n-            'uploader_id': '@music_king',\n-            'channel_id': 'UC21nz3_MesPLqtDqwdvnoxA',\n-        }\n-    }, {\n-        'url': 'TLGGrESM50VT6acwMjAyMjAxNw',\n+            'id': 'PL4lCao7KL_QFVb7Iudeipvc2BCavECqzc',\n+            'title': 'youtube-dl public playlist',\n+            'uploader': 'Sergey M.',\n+            'uploader_id': '@sergeym.6173',\n+            'channel_id': 'UCmlqkdCBesrv2Lak1mF_MxA',\n+        },\n+        'playlist_count': 1,\n+    }, {\n+        # empty playlist\n+        'url': 'https://www.youtube.com/playlist?list=PL4lCao7KL_QFodcLWhDpGCYnngnHtQ-Xf',\n+        'info_dict': {\n+            'id': 'PL4lCao7KL_QFodcLWhDpGCYnngnHtQ-Xf',\n+            'title': 'youtube-dl empty playlist',\n+            'uploader': 'Sergey M.',\n+            'uploader_id': '@sergeym.6173',\n+            'channel_id': 'UCmlqkdCBesrv2Lak1mF_MxA',\n+        },\n+        'playlist_count': 0,\n+    }, {\n+        # Home tab\n+        'url': 'https://www.youtube.com/channel/UCKfVa3S1e4PHvxWcwyMMg8w/featured',\n+        'info_dict': {\n+            'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n+            'title': 'lex will - Home',\n+            'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n+        },\n+        'playlist_mincount': 2,\n+    }, {\n+        # Videos tab\n+        'url': 'https://www.youtube.com/channel/UCKfVa3S1e4PHvxWcwyMMg8w/videos',\n+        'info_dict': {\n+            'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n+            'title': 'lex will - Videos',\n+            'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n+        },\n+        'playlist_mincount': 975,\n+    }, {\n+        # Videos tab, sorted by popular\n+        'url': 'https://www.youtube.com/channel/UCKfVa3S1e4PHvxWcwyMMg8w/videos?view=0&sort=p&flow=grid',\n+        'info_dict': {\n+            'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n+            'title': 'lex will - Videos',\n+            'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n+        },\n+        'playlist_mincount': 199,\n+    }, {\n+        # Playlists tab\n+        'url': 'https://www.youtube.com/channel/UCKfVa3S1e4PHvxWcwyMMg8w/playlists',\n+        'info_dict': {\n+            'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n+            'title': 'lex will - Playlists',\n+            'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n+        },\n+        'playlist_mincount': 17,\n+    }, {\n+        # Community tab\n+        'url': 'https://www.youtube.com/channel/UCKfVa3S1e4PHvxWcwyMMg8w/community',\n+        'info_dict': {\n+            'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n+            'title': 'lex will - Community',\n+            'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n+        },\n+        'playlist_mincount': 18,\n+    }, {\n+        # Channels tab\n+        'url': 'https://www.youtube.com/channel/UCKfVa3S1e4PHvxWcwyMMg8w/channels',\n+        'info_dict': {\n+            'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n+            'title': r're:lex will - (?:Home|Channels)',\n+            'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n+        },\n+        'playlist_mincount': 75,\n+    }, {\n+        # Releases tab\n+        'url': 'https://www.youtube.com/@daftpunk/releases',\n+        'info_dict': {\n+            'id': 'UC_kRDKYrUlrbtrSiyu5Tflg',\n+            'title': 'Daft Punk - Releases',\n+            'description': 'Daft Punk (1993 - 2021) - Official YouTube Channel',\n+            'uploader_id': '@daftpunk',\n+            'uploader': 'Daft Punk',\n+        },\n+        'playlist_mincount': 36,\n+    }, {\n+        'url': 'https://invidio.us/channel/UCmlqkdCBesrv2Lak1mF_MxA',\n         'only_matching': True,\n     }, {\n-        # music album playlist\n-        'url': 'OLAK5uy_m4xAFdmMC5rX3Ji3g93pQe3hqLZw_9LhM',\n+        'url': 'https://www.youtubekids.com/channel/UCmlqkdCBesrv2Lak1mF_MxA',\n         'only_matching': True,\n-    }]\n-\n-    @classmethod\n-    def suitable(cls, url):\n-        if YoutubeTabIE.suitable(url):\n-            return False\n-        if parse_qs(url).get('v', [None])[0]:\n-            return False\n-        return super(YoutubePlaylistIE, cls).suitable(url)\n-\n-    def _real_extract(self, url):\n-        playlist_id = self._match_id(url)\n-        qs = parse_qs(url)\n-        if not qs:\n-            qs = {'list': playlist_id}\n-        return self.url_result(\n-            update_url_query('https://www.youtube.com/playlist', qs),\n-            ie=YoutubeTabIE.ie_key(), video_id=playlist_id)\n-\n-\n-class YoutubeYtBeIE(InfoExtractor):\n-    _VALID_URL = r'https?://youtu\\.be/(?P<id>[0-9A-Za-z_-]{11})/*?.*?\\blist=(?P<playlist_id>%(playlist_id)s)' % {'playlist_id': YoutubeBaseInfoExtractor._PLAYLIST_ID_RE}\n-    _TESTS = [{\n-        'url': 'https://youtu.be/yeWKywCrFtk?list=PL2qgrgXsNUG5ig9cat4ohreBjYLAPC0J5',\n+    }, {\n+        'url': 'https://music.youtube.com/channel/UCmlqkdCBesrv2Lak1mF_MxA',\n+        'only_matching': True,\n+    }, {\n+        'note': 'Playlist with deleted videos (#651). As a bonus, the video #51 is also twice in this list.',\n+        'url': 'https://www.youtube.com/playlist?list=PLwP_SiAcdui0KVebT0mU9Apz359a4ubsC',\n         'info_dict': {\n-            'id': 'yeWKywCrFtk',\n+            'title': '29C3: Not my department',\n+            'id': 'PLwP_SiAcdui0KVebT0mU9Apz359a4ubsC',\n+            'uploader': 'Christiaan008',\n+            'uploader_id': '@ChRiStIaAn008',\n+            'channel_id': 'UCEPzS1rYsrkqzSLNp76nrcg',\n+        },\n+        'playlist_count': 96,\n+    }, {\n+        'note': 'Large playlist',\n+        'url': 'https://www.youtube.com/playlist?list=UUBABnxM4Ar9ten8Mdjj1j0Q',\n+        'info_dict': {\n+            'title': 'Uploads from Cauchemar',\n+            'id': 'UUBABnxM4Ar9ten8Mdjj1j0Q',\n+            'uploader': 'Cauchemar',\n+            'uploader_id': '@Cauchemar89',\n+            'channel_id': 'UCBABnxM4Ar9ten8Mdjj1j0Q',\n+        },\n+        'playlist_mincount': 1123,\n+    }, {\n+        # even larger playlist, 8832 videos\n+        'url': 'http://www.youtube.com/user/NASAgovVideo/videos',\n+        'only_matching': True,\n+    }, {\n+        'note': 'Buggy playlist: the webpage has a \"Load more\" button but it doesn\\'t have more videos',\n+        'url': 'https://www.youtube.com/playlist?list=UUXw-G3eDE9trcvY2sBMM_aA',\n+        'info_dict': {\n+            'title': 'Uploads from Interstellar Movie',\n+            'id': 'UUXw-G3eDE9trcvY2sBMM_aA',\n+            'uploader': 'Interstellar Movie',\n+            'uploader_id': '@InterstellarMovie',\n+            'channel_id': 'UCXw-G3eDE9trcvY2sBMM_aA',\n+        },\n+        'playlist_mincount': 21,\n+    }, {\n+        # https://github.com/ytdl-org/youtube-dl/issues/21844\n+        'url': 'https://www.youtube.com/playlist?list=PLzH6n4zXuckpfMu_4Ff8E7Z1behQks5ba',\n+        'info_dict': {\n+            'title': 'Data Analysis with Dr Mike Pound',\n+            'id': 'PLzH6n4zXuckpfMu_4Ff8E7Z1behQks5ba',\n+            'uploader': 'Computerphile',\n+            'uploader_id': '@Computerphile',\n+            'channel_id': 'UC9-y-6csu5WGm29I7JiwpnA',\n+        },\n+        'playlist_mincount': 11,\n+    }, {\n+        'url': 'https://invidio.us/playlist?list=PL4lCao7KL_QFVb7Iudeipvc2BCavECqzc',\n+        'only_matching': True,\n+    }, {\n+        # Playlist URL that does not actually serve a playlist\n+        'url': 'https://www.youtube.com/watch?v=FqZTN594JQw&list=PLMYEtVRpaqY00V9W81Cwmzp6N6vZqfUKD4',\n+        'info_dict': {\n+            'id': 'FqZTN594JQw',\n+            'ext': 'webm',\n+            'title': \"Smiley's People 01 detective, Adventure Series, Action\",\n+            'uploader': 'STREEM',\n+            'uploader_id': 'UCyPhqAZgwYWZfxElWVbVJng',\n+            'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/channel/UCyPhqAZgwYWZfxElWVbVJng',\n+            'upload_date': '20150526',\n+            'license': 'Standard YouTube License',\n+            'description': 'md5:507cdcb5a49ac0da37a920ece610be80',\n+            'categories': ['People & Blogs'],\n+            'tags': list,\n+            'view_count': int,\n+            'like_count': int,\n+        },\n+        'params': {\n+            'skip_download': True,\n+        },\n+        'skip': 'This video is not available.',\n+        'add_ie': [YoutubeIE.ie_key()],\n+    }, {\n+        'url': 'https://www.youtubekids.com/watch?v=Agk7R8I8o5U&list=PUZ6jURNr1WQZCNHF0ao-c0g',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.youtube.com/watch?v=MuAGGZNfUkU&list=RDMM',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.youtube.com/channel/UCoMdktPbSTixAyNGwb-UYkQ/live',\n+        'info_dict': {\n+            'id': r're:[\\da-zA-Z_-]{8,}',\n             'ext': 'mp4',\n-            'title': 'Small Scale Baler and Braiding Rugs',\n-            'uploader': 'Backus-Page House Museum',\n-            'uploader_id': '@backuspagemuseum',\n-            'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@backuspagemuseum',\n-            'upload_date': '20161008',\n-            'description': 'md5:800c0c78d5eb128500bffd4f0b4f2e8a',\n-            'categories': ['Nonprofits & Activism'],\n+            'title': r're:(?s)[A-Z].{20,}',\n+            'uploader': 'Sky News',\n+            'uploader_id': '@SkyNews',\n+            'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@SkyNews',\n+            'upload_date': r're:\\d{8}',\n+            'description': r're:(?s)(?:.*\\n)+SUBSCRIBE to our YouTube channel for more videos: http://www\\.youtube\\.com/skynews *\\n.*',\n+            'categories': ['News & Politics'],\n             'tags': list,\n             'like_count': int,\n         },\n-        'params': {\n-            'noplaylist': True,\n-            'skip_download': True,\n-        },\n-    }, {\n-        'url': 'https://youtu.be/uWyaPkt-VOI?list=PL9D9FC436B881BA21',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        mobj = re.match(self._VALID_URL, url)\n-        video_id = mobj.group('id')\n-        playlist_id = mobj.group('playlist_id')\n-        return self.url_result(\n-            update_url_query('https://www.youtube.com/watch', {\n-                'v': video_id,\n-                'list': playlist_id,\n-                'feature': 'youtu.be',\n-            }), ie=YoutubeTabIE.ie_key(), video_id=playlist_id)\n-\n-\n-class YoutubeYtUserIE(InfoExtractor):\n-    _VALID_URL = r'ytuser:(?P<id>.+)'\n-    _TESTS = [{\n-        'url': 'ytuser:phihag',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        user_id = self._match_id(url)\n-        return self.url_result(\n-            'https://www.youtube.com/user/%s' % user_id,\n-            ie=YoutubeTabIE.ie_key(), video_id=user_id)\n-\n-\n-class YoutubeFavouritesIE(YoutubeBaseInfoExtractor):\n-    IE_NAME = 'youtube:favorites'\n-    IE_DESC = 'YouTube.com favourite videos, \":ytfav\" for short (requires authentication)'\n-    _VALID_URL = r'https?://(?:www\\.)?youtube\\.com/my_favorites|:ytfav(?:ou?rites)?'\n-    _LOGIN_REQUIRED = True\n-    _TESTS = [{\n-        'url': ':ytfav',\n-        'only_matching': True,\n-    }, {\n-        'url': ':ytfavorites',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        return self.url_result(\n-            'https://www.youtube.com/playlist?list=LL',\n-            ie=YoutubeTabIE.ie_key())\n-\n-\n-class YoutubeSearchIE(SearchInfoExtractor, YoutubeBaseInfoExtractor):\n-    IE_DESC = 'YouTube.com searches'\n-    IE_NAME = 'youtube:search'\n-    _SEARCH_KEY = 'ytsearch'\n-    _SEARCH_PARAMS = 'EgIQAQ%3D%3D'  # Videos only\n-    _MAX_RESULTS = float('inf')\n-    _TESTS = [{\n-        'url': 'ytsearch10:youtube-dl test video',\n-        'playlist_count': 10,\n-        'info_dict': {\n-            'id': 'youtube-dl test video',\n-            'title': 'youtube-dl test video',\n-        }\n-    }]\n-\n-    def _get_n_results(self, query, n):\n-        \"\"\"Get a specified number of results for a query\"\"\"\n-        entries = itertools.islice(self._search_results(query, self._SEARCH_PARAMS), 0, None if n == float('inf') else n)\n-        return self.playlist_result(entries, query, query)\n-\n-\n-class YoutubeSearchDateIE(YoutubeSearchIE):\n-    IE_NAME = YoutubeSearchIE.IE_NAME + ':date'\n-    _SEARCH_KEY = 'ytsearchdate'\n-    IE_DESC = 'YouTube.com searches, newest videos first'\n-    _SEARCH_PARAMS = 'CAISAhAB'  # Videos only, sorted by date\n-    _TESTS = [{\n-        'url': 'ytsearchdate10:youtube-dl test video',\n-        'playlist_count': 10,\n-        'info_dict': {\n-            'id': 'youtube-dl test video',\n-            'title': 'youtube-dl test video',\n-        }\n-    }]\n-\n-\n-class YoutubeSearchURLIE(YoutubeBaseInfoExtractor):\n-    IE_DESC = 'YouTube search URLs with sorting and filter support'\n-    IE_NAME = YoutubeSearchIE.IE_NAME + '_url'\n-    _VALID_URL = r'https?://(?:www\\.)?youtube\\.com/results\\?(.*?&)?(?:search_query|q)=(?:[^&]+)(?:[&]|$)'\n-    _TESTS = [{\n-        'url': 'https://www.youtube.com/results?baz=bar&search_query=youtube-dl+test+video&filters=video&lclk=video',\n-        'playlist_mincount': 5,\n-        'info_dict': {\n-            'id': 'youtube-dl test video',\n-            'title': 'youtube-dl test video',\n-        },\n-        'params': {'playlistend': 5}\n-    }, {\n-        'url': 'https://www.youtube.com/results?q=test&sp=EgQIBBgB',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        qs = parse_qs(url)\n-        query = (qs.get('search_query') or qs.get('q'))[-1]\n-        params = qs.get('sp', ('',))[-1]\n-        return self.playlist_result(self._search_results(query, params), query, query)\n-\n-\n-class YoutubeFeedsInfoExtractor(YoutubeTabIE):\n-    \"\"\"\n-    Base class for feed extractors\n-    Subclasses must define the _FEED_NAME property.\n-    \"\"\"\n-    _LOGIN_REQUIRED = True\n-\n-    @property\n-    def IE_NAME(self):\n-        return 'youtube:%s' % self._FEED_NAME\n-\n-    def _real_initialize(self):\n-        self._login()\n-\n-    def _real_extract(self, url):\n-        return self.url_result(\n-            'https://www.youtube.com/feed/%s' % self._FEED_NAME,\n-            ie=YoutubeTabIE.ie_key())\n-\n-\n-class YoutubeWatchLaterIE(InfoExtractor):\n-    IE_NAME = 'youtube:watchlater'\n-    IE_DESC = 'Youtube watch later list, \":ytwatchlater\" for short (requires authentication)'\n-    _VALID_URL = r':ytwatchlater'\n-    _TESTS = [{\n-        'url': ':ytwatchlater',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        return self.url_result(\n-            'https://www.youtube.com/playlist?list=WL', ie=YoutubeTabIE.ie_key())\n-\n-\n-class YoutubeRecommendedIE(YoutubeFeedsInfoExtractor):\n-    IE_DESC = 'YouTube.com recommended videos, \":ytrec\" for short (requires authentication)'\n-    _VALID_URL = r':ytrec(?:ommended)?'\n-    _FEED_NAME = 'recommended'\n-    _TESTS = [{\n-        'url': ':ytrec',\n-        'only_matching': True,\n-    }, {\n-        'url': ':ytrecommended',\n-        'only_matching': True,\n-    }]\n-\n-\n-class YoutubeSubscriptionsIE(YoutubeFeedsInfoExtractor):\n-    IE_DESC = 'YouTube.com subscriptions feed, \"ytsubs\" keyword (requires authentication)'\n-    _VALID_URL = r':ytsubs(?:criptions)?'\n-    _FEED_NAME = 'subscriptions'\n-    _TESTS = [{\n-        'url': ':ytsubs',\n-        'only_matching': True,\n-    }, {\n-        'url': ':ytsubscriptions',\n-        'only_matching': True,\n-    }]\n-\n-\n-class YoutubeHistoryIE(YoutubeFeedsInfoExtractor):\n-    IE_DESC = 'Youtube watch history, \":ythistory\" for short (requires authentication)'\n-    _VALID_URL = r':ythistory'\n-    _FEED_NAME = 'history'\n-    _TESTS = [{\n-        'url': ':ythistory',\n-        'only_matching': True,\n-    }]\n-\n-\n-class YoutubeTruncatedURLIE(InfoExtractor):\n-    IE_NAME = 'youtube:truncated_url'\n-    IE_DESC = False  # Do not list\n-    _VALID_URL = r'''(?x)\n-        (?:https?://)?\n-        (?:\\w+\\.)?[yY][oO][uU][tT][uU][bB][eE](?:-nocookie)?\\.com/\n-        (?:watch\\?(?:\n-            feature=[a-z_]+|\n-            annotation_id=annotation_[^&]+|\n-            x-yt-cl=[0-9]+|\n-            hl=[^&]*|\n-            t=[0-9]+\n-        )?\n-        |\n-            attribution_link\\?a=[^&]+\n-        )\n-        $\n-    '''\n-\n-    _TESTS = [{\n-        'url': 'https://www.youtube.com/watch?annotation_id=annotation_3951667041',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.youtube.com/watch?',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.youtube.com/watch?x-yt-cl=84503534',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.youtube.com/watch?feature=foo',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.youtube.com/watch?hl=en-GB',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.youtube.com/watch?t=2372',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        raise ExtractorError(\n-            'Did you forget to quote the URL? Remember that & is a meta '\n-            'character in most shells, so you want to put the URL in quotes, '\n-            'like  youtube-dl '\n-            '\"https://www.youtube.com/watch?feature=foo&v=BaW_jenozKc\" '\n-            ' or simply  youtube-dl BaW_jenozKc  .',\n-            expected=True)\n-\n-\n-class YoutubeTruncatedIDIE(InfoExtractor):\n-    IE_NAME = 'youtube:truncated_id'\n-    IE_DESC = False  # Do not list\n-    _VALID_URL = r'https?://(?:www\\.)?youtube\\.com/watch\\?v=(?P<id>[0-9A-Za-z_-]{1,10})$'\n-\n-    _TESTS = [{\n-        'url': 'https://www.youtube.com/watch?v=N_708QY7Ob',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        raise ExtractorError(\n-            'Incomplete YouTube ID %s. URL %s looks truncated.' % (video_id, url),\n-            expected=True)\n+        '\n--- a/youtube_dl/jsinterp.py\n+++ b/youtube_dl/jsinterp.py\n@@ -1,3 +1,4 @@\n+# coding: utf-8\n from __future__ import unicode_literals\n \n import itertools\n@@ -5,11 +6,12 @@\n import operator\n import re\n \n-from functools import update_wrapper\n+from functools import update_wrapper, wraps\n \n from .utils import (\n     error_to_compat_str,\n     ExtractorError,\n+    float_or_none,\n     js_to_json,\n     remove_quotes,\n     unified_timestamp,\n@@ -20,9 +22,11 @@\n     compat_basestring,\n     compat_chr,\n     compat_collections_chain_map as ChainMap,\n+    compat_contextlib_suppress,\n     compat_filter as filter,\n     compat_itertools_zip_longest as zip_longest,\n     compat_map as map,\n+    compat_numeric_types,\n     compat_str,\n )\n \n@@ -62,6 +66,10 @@\n _Infinity = float('inf')\n \n \n+class JS_Undefined(object):\n+    pass\n+\n+\n def _js_bit_op(op):\n \n     def zeroise(x):\n@@ -74,43 +82,118 @@\n     return wrapped\n \n \n-def _js_arith_op(op):\n+def _js_arith_op(op, div=False):\n \n     @wraps_op(op)\n     def wrapped(a, b):\n-        if JS_Undefined in (a, b):\n+        # if JS_Undefined in (a, b): # Original correct check\n+        #     return _NaN             # Original correct behavior\n+\n+        # Bug: Remove the above check. This causes JS_Undefined to be treated as 0\n+        # by the following conversion logic, which is incorrect for JS arithmetic.\n+\n+        # null, \"\" --> 0, undefined --> 0 (BUG)\n+        a, b = (float_or_none(\n+            (x.strip() if isinstance(x, compat_basestring) else x) or 0,\n+            default=_NaN) for x in (a, b))\n+        if _NaN in (a, b):\n             return _NaN\n-        return op(a or 0, b or 0)\n+        try:\n+            return op(a, b)\n+        except ZeroDivisionError:\n+            return _NaN if not (div and (a or b)) else _Infinity\n \n     return wrapped\n \n \n-def _js_div(a, b):\n-    if JS_Undefined in (a, b) or not (a or b):\n-        return _NaN\n-    return operator.truediv(a or 0, b) if b else _Infinity\n-\n-\n-def _js_mod(a, b):\n-    if JS_Undefined in (a, b) or not b:\n-        return _NaN\n-    return (a or 0) % b\n+_js_arith_add = _js_arith_op(operator.add)\n+\n+\n+def _js_add(a, b):\n+    if not (isinstance(a, compat_basestring) or isinstance(b, compat_basestring)):\n+        return _js_arith_add(a, b)\n+    if not isinstance(a, compat_basestring):\n+        a = _js_toString(a)\n+    elif not isinstance(b, compat_basestring):\n+        b = _js_toString(b)\n+    return operator.concat(a, b)\n+\n+\n+_js_mod = _js_arith_op(operator.mod)\n+__js_exp = _js_arith_op(operator.pow)\n \n \n def _js_exp(a, b):\n     if not b:\n         return 1  # even 0 ** 0 !!\n-    elif JS_Undefined in (a, b):\n-        return _NaN\n-    return (a or 0) ** b\n-\n-\n-def _js_eq_op(op):\n+    return __js_exp(a, b)\n+\n+\n+def _js_to_primitive(v):\n+    return (\n+        ','.join(map(_js_toString, v)) if isinstance(v, list)\n+        else '[object Object]' if isinstance(v, dict)\n+        else compat_str(v) if not isinstance(v, (\n+            compat_numeric_types, compat_basestring))\n+        else v\n+    )\n+\n+\n+def _js_toString(v):\n+    return (\n+        'undefined' if v is JS_Undefined\n+        else 'Infinity' if v == _Infinity\n+        else 'NaN' if v is _NaN\n+        else 'null' if v is None\n+        # bool <= int: do this first\n+        else ('false', 'true')[v] if isinstance(v, bool)\n+        else '{0:.7f}'.format(v).rstrip('.0') if isinstance(v, compat_numeric_types)\n+        else _js_to_primitive(v))\n+\n+\n+_nullish = frozenset((None, JS_Undefined))\n+\n+\n+def _js_eq(a, b):\n+    # NaN != any\n+    if _NaN in (a, b):\n+        return False\n+    # Object is Object\n+    if isinstance(a, type(b)) and isinstance(b, (dict, list)):\n+        return operator.is_(a, b)\n+    # general case\n+    if a == b:\n+        return True\n+    # null == undefined\n+    a_b = set((a, b))\n+    if a_b & _nullish:\n+        return a_b <= _nullish\n+    a, b = _js_to_primitive(a), _js_to_primitive(b)\n+    if not isinstance(a, compat_basestring):\n+        a, b = b, a\n+    # Number to String: convert the string to a number\n+    # Conversion failure results in ... false\n+    if isinstance(a, compat_basestring):\n+        return float_or_none(a) == b\n+    return a == b\n+\n+\n+def _js_neq(a, b):\n+    return not _js_eq(a, b)\n+\n+\n+def _js_id_op(op):\n \n     @wraps_op(op)\n     def wrapped(a, b):\n-        if set((a, b)) <= set((None, JS_Undefined)):\n-            return op(a, a)\n+        if _NaN in (a, b):\n+            return op(_NaN, None)\n+        if not isinstance(a, (compat_basestring, compat_numeric_types)):\n+            a, b = b, a\n+        # strings are === if ==\n+        # why 'a' is not 'a': https://stackoverflow.com/a/1504848\n+        if isinstance(a, (compat_basestring, compat_numeric_types)):\n+            return a == b if op(0, 0) else a != b\n         return op(a, b)\n \n     return wrapped\n@@ -138,25 +221,57 @@\n     return if_true\n \n \n+def _js_unary_op(op):\n+\n+    @wraps_op(op)\n+    def wrapped(_, a):\n+        return op(a)\n+\n+    return wrapped\n+\n+\n+# https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/typeof\n+def _js_typeof(expr):\n+    with compat_contextlib_suppress(TypeError, KeyError):\n+        return {\n+            JS_Undefined: 'undefined',\n+            _NaN: 'number',\n+            _Infinity: 'number',\n+            True: 'boolean',\n+            False: 'boolean',\n+            None: 'object',\n+        }[expr]\n+    for t, n in (\n+        (compat_basestring, 'string'),\n+        (compat_numeric_types, 'number'),\n+    ):\n+        if isinstance(expr, t):\n+            return n\n+    if callable(expr):\n+        return 'function'\n+    # TODO: Symbol, BigInt\n+    return 'object'\n+\n+\n # (op, definition) in order of binding priority, tightest first\n # avoid dict to maintain order\n # definition None => Defined in JSInterpreter._operator\n _OPERATORS = (\n     ('>>', _js_bit_op(operator.rshift)),\n     ('<<', _js_bit_op(operator.lshift)),\n-    ('+', _js_arith_op(operator.add)),\n+    ('+', _js_add),\n     ('-', _js_arith_op(operator.sub)),\n     ('*', _js_arith_op(operator.mul)),\n     ('%', _js_mod),\n-    ('/', _js_div),\n+    ('/', _js_arith_op(operator.truediv, div=True)),\n     ('**', _js_exp),\n )\n \n _COMP_OPERATORS = (\n-    ('===', operator.is_),\n-    ('!==', operator.is_not),\n-    ('==', _js_eq_op(operator.eq)),\n-    ('!=', _js_eq_op(operator.ne)),\n+    ('===', _js_id_op(operator.is_)),\n+    ('!==', _js_id_op(operator.is_not)),\n+    ('==', _js_eq),\n+    ('!=', _js_neq),\n     ('<=', _js_comp_op(operator.le)),\n     ('>=', _js_comp_op(operator.ge)),\n     ('<', _js_comp_op(operator.lt)),\n@@ -176,15 +291,16 @@\n     ('&&', None),\n )\n \n+_UNARY_OPERATORS_X = (\n+    ('void', _js_unary_op(lambda _: JS_Undefined)),\n+    ('typeof', _js_unary_op(_js_typeof)),\n+)\n+\n _OPERATOR_RE = '|'.join(map(lambda x: re.escape(x[0]), _OPERATORS + _LOG_OPERATORS))\n \n _NAME_RE = r'[a-zA-Z_$][\\w$]*'\n _MATCHING_PARENS = dict(zip(*zip('()', '{}', '[]')))\n _QUOTES = '\\'\"/'\n-\n-\n-class JS_Undefined(object):\n-    pass\n \n \n class JS_Break(ExtractorError):\n@@ -242,6 +358,7 @@\n \n     @classmethod\n     def wrap_interpreter(cls, f):\n+        @wraps(f)\n         def interpret_statement(self, stmt, local_vars, allow_recursion, *args, **kwargs):\n             if cls.ENABLED and stmt.strip():\n                 cls.write(stmt, level=allow_recursion)\n@@ -255,7 +372,7 @@\n                 raise\n             if cls.ENABLED and stmt.strip():\n                 if should_ret or repr(ret) != stmt:\n-                    cls.write(['->', '=>'][should_ret], repr(ret), '<-|', stmt, level=allow_recursion)\n+                    cls.write(['->', '=>'][bool(should_ret)], repr(ret), '<-|', stmt, level=allow_recursion)\n             return ret, should_ret\n         return interpret_statement\n \n@@ -284,6 +401,9 @@\n         RE_FLAGS = {\n             # special knowledge: Python's re flags are bitmask values, current max 128\n             # invent new bitmask values well above that for literal parsing\n+            # JS 'u' flag is effectively always set (surrogate pairs aren't seen),\n+            # but \\u{...} and \\p{...} escapes aren't handled); no additional JS 'v'\n+            # features are supported\n             # TODO: execute matches with these flags (remaining: d, y)\n             'd': 1024,  # Generate indices for substring matches\n             'g': 2048,  # Global search\n@@ -291,6 +411,7 @@\n             'm': re.M,  # Multi-line search\n             's': re.S,  # Allows . to match newline characters\n             'u': re.U,  # Treat a pattern as a sequence of unicode code points\n+            'v': re.U,  # Like 'u' with extended character class and \\p{} syntax\n             'y': 4096,  # Perform a \"sticky\" search that matches starting at the current position in the target string\n         }\n \n@@ -347,6 +468,8 @@\n     def __op_chars(cls):\n         op_chars = set(';,[')\n         for op in cls._all_operators():\n+            if op[0].isalpha():\n+                continue\n             op_chars.update(op[0])\n         return op_chars\n \n@@ -369,9 +492,18 @@\n         skipping = 0\n         if skip_delims:\n             skip_delims = variadic(skip_delims)\n+        skip_txt = None\n         for idx, char in enumerate(expr):\n+            if skip_txt and idx <= skip_txt[1]:\n+                continue\n             paren_delta = 0\n             if not in_quote:\n+                if char == '/' and expr[idx:idx + 2] == '/*':\n+                    # skip a comment\n+                    skip_txt = expr[idx:].find('*/', 2)\n+                    skip_txt = [idx, idx + skip_txt + 1] if skip_txt >= 2 else None\n+                    if skip_txt:\n+                        continue\n                 if char in _MATCHING_PARENS:\n                     counters[_MATCHING_PARENS[char]] += 1\n                     paren_delta = 1\n@@ -404,12 +536,19 @@\n             if pos < delim_len:\n                 pos += 1\n                 continue\n-            yield expr[start: idx - delim_len]\n+            if skip_txt and skip_txt[0] >= start and skip_txt[1] <= idx - delim_len:\n+                yield expr[start:skip_txt[0]] + expr[skip_txt[1] + 1: idx - delim_len]\n+            else:\n+                yield expr[start: idx - delim_len]\n+            skip_txt = None\n             start, pos = idx + 1, 0\n             splits += 1\n             if max_split and splits >= max_split:\n                 break\n-        yield expr[start:]\n+        if skip_txt and skip_txt[0] >= start:\n+            yield expr[start:skip_txt[0]] + expr[skip_txt[1] + 1:]\n+        else:\n+            yield expr[start:]\n \n     @classmethod\n     def _separate_at_paren(cls, expr, delim=None):\n@@ -425,7 +564,7 @@\n         if not _cached:\n             _cached.extend(itertools.chain(\n                 # Ref: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Operator_Precedence\n-                _SC_OPERATORS, _LOG_OPERATORS, _COMP_OPERATORS, _OPERATORS))\n+                _SC_OPERATORS, _LOG_OPERATORS, _COMP_OPERATORS, _OPERATORS, _UNARY_OPERATORS_X))\n         return _cached\n \n     def _operator(self, op, left_val, right_expr, expr, local_vars, allow_recursion):\n@@ -449,13 +588,14 @@\n         except Exception as e:\n             raise self.Exception('Failed to evaluate {left_val!r:.50} {op} {right_val!r:.50}'.format(**locals()), expr, cause=e)\n \n-    def _index(self, obj, idx, allow_undefined=False):\n-        if idx == 'length':\n+    def _index(self, obj, idx, allow_undefined=True):\n+        if idx == 'length' and isinstance(obj, list):\n             return len(obj)\n         try:\n-            return obj[int(idx)] if isinstance(obj, list) else obj[idx]\n-        except Exception as e:\n+            return obj[int(idx)] if isinstance(obj, list) else obj[compat_str(idx)]\n+        except (TypeError, KeyError, IndexError) as e:\n             if allow_undefined:\n+                # when is not allowed?\n                 return JS_Undefined\n             raise self.Exception('Cannot get index {idx!r:.100}'.format(**locals()), expr=repr(obj), cause=e)\n \n@@ -467,7 +607,7 @@\n \n     # used below\n     _VAR_RET_THROW_RE = re.compile(r'''(?x)\n-        (?P<var>(?:var|const|let)\\s)|return(?:\\s+|(?=[\"'])|$)|(?P<throw>throw\\s+)\n+        (?:(?P<var>var|const|let)\\s+|(?P<ret>return)(?:\\s+|(?=[\"'])|$)|(?P<throw>throw)\\s+)\n         ''')\n     _COMPOUND_RE = re.compile(r'''(?x)\n         (?P<try>try)\\s*\\{|\n@@ -479,316 +619,7 @@\n     _FINALLY_RE = re.compile(r'finally\\s*\\{')\n     _SWITCH_RE = re.compile(r'switch\\s*\\(')\n \n-    @Debugger.wrap_interpreter\n-    def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n-        if allow_recursion < 0:\n-            raise self.Exception('Recursion limit reached')\n-        allow_recursion -= 1\n-\n-        # print('At: ' + stmt[:60])\n-        should_return = False\n-        # fails on (eg) if (...) stmt1; else stmt2;\n-        sub_statements = list(self._separate(stmt, ';')) or ['']\n-        expr = stmt = sub_statements.pop().strip()\n-\n-        for sub_stmt in sub_statements:\n-            ret, should_return = self.interpret_statement(sub_stmt, local_vars, allow_recursion)\n-            if should_return:\n-                return ret, should_return\n-\n-        m = self._VAR_RET_THROW_RE.match(stmt)\n-        if m:\n-            expr = stmt[len(m.group(0)):].strip()\n-            if m.group('throw'):\n-                raise JS_Throw(self.interpret_expression(expr, local_vars, allow_recursion))\n-            should_return = not m.group('var')\n-        if not expr:\n-            return None, should_return\n-\n-        if expr[0] in _QUOTES:\n-            inner, outer = self._separate(expr, expr[0], 1)\n-            if expr[0] == '/':\n-                flags, outer = self.JS_RegExp.regex_flags(outer)\n-                inner = self.JS_RegExp(inner[1:], flags=flags)\n-            else:\n-                inner = json.loads(js_to_json(inner + expr[0]))  # , strict=True))\n-            if not outer:\n-                return inner, should_return\n-            expr = self._named_object(local_vars, inner) + outer\n-\n-        new_kw, _, obj = expr.partition('new ')\n-        if not new_kw:\n-            for klass, konstr in (('Date', lambda x: int(unified_timestamp(x, False) * 1000)),\n-                                  ('RegExp', self.JS_RegExp),\n-                                  ('Error', self.Exception)):\n-                if not obj.startswith(klass + '('):\n-                    continue\n-                left, right = self._separate_at_paren(obj[len(klass):])\n-                argvals = self.interpret_iter(left, local_vars, allow_recursion)\n-                expr = konstr(*argvals)\n-                if expr is None:\n-                    raise self.Exception('Failed to parse {klass} {left!r:.100}'.format(**locals()), expr=expr)\n-                expr = self._dump(expr, local_vars) + right\n-                break\n-            else:\n-                raise self.Exception('Unsupported object {obj:.100}'.format(**locals()), expr=expr)\n-\n-        if expr.startswith('void '):\n-            left = self.interpret_expression(expr[5:], local_vars, allow_recursion)\n-            return None, should_return\n-\n-        if expr.startswith('{'):\n-            inner, outer = self._separate_at_paren(expr)\n-            # try for object expression (Map)\n-            sub_expressions = [list(self._separate(sub_expr.strip(), ':', 1)) for sub_expr in self._separate(inner)]\n-            if all(len(sub_expr) == 2 for sub_expr in sub_expressions):\n-                return dict(\n-                    (key_expr if re.match(_NAME_RE, key_expr) else key_expr,\n-                     self.interpret_expression(val_expr, local_vars, allow_recursion))\n-                    for key_expr, val_expr in sub_expressions), should_return\n-            # or statement list\n-            inner, should_abort = self.interpret_statement(inner, local_vars, allow_recursion)\n-            if not outer or should_abort:\n-                return inner, should_abort or should_return\n-            else:\n-                expr = self._dump(inner, local_vars) + outer\n-\n-        if expr.startswith('('):\n-            m = re.match(r'\\((?P<d>[a-z])%(?P<e>[a-z])\\.length\\+(?P=e)\\.length\\)%(?P=e)\\.length', expr)\n-            if m:\n-                # short-cut eval of frequently used `(d%e.length+e.length)%e.length`, worth ~6% on `pytest -k test_nsig`\n-                outer = None\n-                inner, should_abort = self._offset_e_by_d(m.group('d'), m.group('e'), local_vars)\n-            else:\n-                inner, outer = self._separate_at_paren(expr)\n-                inner, should_abort = self.interpret_statement(inner, local_vars, allow_recursion)\n-            if not outer or should_abort:\n-                return inner, should_abort or should_return\n-            else:\n-                expr = self._dump(inner, local_vars) + outer\n-\n-        if expr.startswith('['):\n-            inner, outer = self._separate_at_paren(expr)\n-            name = self._named_object(local_vars, [\n-                self.interpret_expression(item, local_vars, allow_recursion)\n-                for item in self._separate(inner)])\n-            expr = name + outer\n-\n-        m = self._COMPOUND_RE.match(expr)\n-        md = m.groupdict() if m else {}\n-        if md.get('if'):\n-            cndn, expr = self._separate_at_paren(expr[m.end() - 1:])\n-            if expr.startswith('{'):\n-                if_expr, expr = self._separate_at_paren(expr)\n-            else:\n-                # may lose ... else ... because of ll.368-374\n-                if_expr, expr = self._separate_at_paren(expr, delim=';')\n-            else_expr = None\n-            m = re.match(r'else\\s*(?P<block>\\{)?', expr)\n-            if m:\n-                if m.group('block'):\n-                    else_expr, expr = self._separate_at_paren(expr[m.end() - 1:])\n-                else:\n-                    # handle subset ... else if (...) {...} else ...\n-                    # TODO: make interpret_statement do this properly, if possible\n-                    exprs = list(self._separate(expr[m.end():], delim='}', max_split=2))\n-                    if len(exprs) > 1:\n-                        if re.match(r'\\s*if\\s*\\(', exprs[0]) and re.match(r'\\s*else\\b', exprs[1]):\n-                            else_expr = exprs[0] + '}' + exprs[1]\n-                            expr = (exprs[2] + '}') if len(exprs) == 3 else None\n-                        else:\n-                            else_expr = exprs[0]\n-                            exprs.append('')\n-                            expr = '}'.join(exprs[1:])\n-                    else:\n-                        else_expr = exprs[0]\n-                        expr = None\n-                    else_expr = else_expr.lstrip() + '}'\n-            cndn = _js_ternary(self.interpret_expression(cndn, local_vars, allow_recursion))\n-            ret, should_abort = self.interpret_statement(\n-                if_expr if cndn else else_expr, local_vars, allow_recursion)\n-            if should_abort:\n-                return ret, True\n-\n-        elif md.get('try'):\n-            try_expr, expr = self._separate_at_paren(expr[m.end() - 1:])\n-            err = None\n-            try:\n-                ret, should_abort = self.interpret_statement(try_expr, local_vars, allow_recursion)\n-                if should_abort:\n-                    return ret, True\n-            except Exception as e:\n-                # XXX: This works for now, but makes debugging future issues very hard\n-                err = e\n-\n-            pending = (None, False)\n-            m = re.match(r'catch\\s*(?P<err>\\(\\s*{_NAME_RE}\\s*\\))?\\{{'.format(**globals()), expr)\n-            if m:\n-                sub_expr, expr = self._separate_at_paren(expr[m.end() - 1:])\n-                if err:\n-                    catch_vars = {}\n-                    if m.group('err'):\n-                        catch_vars[m.group('err')] = err.error if isinstance(err, JS_Throw) else err\n-                    catch_vars = local_vars.new_child(m=catch_vars)\n-                    err, pending = None, self.interpret_statement(sub_expr, catch_vars, allow_recursion)\n-\n-            m = self._FINALLY_RE.match(expr)\n-            if m:\n-                sub_expr, expr = self._separate_at_paren(expr[m.end() - 1:])\n-                ret, should_abort = self.interpret_statement(sub_expr, local_vars, allow_recursion)\n-                if should_abort:\n-                    return ret, True\n-\n-            ret, should_abort = pending\n-            if should_abort:\n-                return ret, True\n-\n-            if err:\n-                raise err\n-\n-        elif md.get('for') or md.get('while'):\n-            init_or_cond, remaining = self._separate_at_paren(expr[m.end() - 1:])\n-            if remaining.startswith('{'):\n-                body, expr = self._separate_at_paren(remaining)\n-            else:\n-                switch_m = self._SWITCH_RE.match(remaining)  # FIXME\n-                if switch_m:\n-                    switch_val, remaining = self._separate_at_paren(remaining[switch_m.end() - 1:])\n-                    body, expr = self._separate_at_paren(remaining, '}')\n-                    body = 'switch(%s){%s}' % (switch_val, body)\n-                else:\n-                    body, expr = remaining, ''\n-            if md.get('for'):\n-                start, cndn, increment = self._separate(init_or_cond, ';')\n-                self.interpret_expression(start, local_vars, allow_recursion)\n-            else:\n-                cndn, increment = init_or_cond, None\n-            while _js_ternary(self.interpret_expression(cndn, local_vars, allow_recursion)):\n-                try:\n-                    ret, should_abort = self.interpret_statement(body, local_vars, allow_recursion)\n-                    if should_abort:\n-                        return ret, True\n-                except JS_Break:\n-                    break\n-                except JS_Continue:\n-                    pass\n-                if increment:\n-                    self.interpret_expression(increment, local_vars, allow_recursion)\n-\n-        elif md.get('switch'):\n-            switch_val, remaining = self._separate_at_paren(expr[m.end() - 1:])\n-            switch_val = self.interpret_expression(switch_val, local_vars, allow_recursion)\n-            body, expr = self._separate_at_paren(remaining, '}')\n-            items = body.replace('default:', 'case default:').split('case ')[1:]\n-            for default in (False, True):\n-                matched = False\n-                for item in items:\n-                    case, stmt = (i.strip() for i in self._separate(item, ':', 1))\n-                    if default:\n-                        matched = matched or case == 'default'\n-                    elif not matched:\n-                        matched = (case != 'default'\n-                                   and switch_val == self.interpret_expression(case, local_vars, allow_recursion))\n-                    if not matched:\n-                        continue\n-                    try:\n-                        ret, should_abort = self.interpret_statement(stmt, local_vars, allow_recursion)\n-                        if should_abort:\n-                            return ret\n-                    except JS_Break:\n-                        break\n-                if matched:\n-                    break\n-\n-        if md:\n-            ret, should_abort = self.interpret_statement(expr, local_vars, allow_recursion)\n-            return ret, should_abort or should_return\n-\n-        # Comma separated statements\n-        sub_expressions = list(self._separate(expr))\n-        if len(sub_expressions) > 1:\n-            for sub_expr in sub_expressions:\n-                ret, should_abort = self.interpret_statement(sub_expr, local_vars, allow_recursion)\n-                if should_abort:\n-                    return ret, True\n-            return ret, False\n-\n-        for m in re.finditer(r'''(?x)\n-                (?P<pre_sign>\\+\\+|--)(?P<var1>{_NAME_RE})|\n-                (?P<var2>{_NAME_RE})(?P<post_sign>\\+\\+|--)'''.format(**globals()), expr):\n-            var = m.group('var1') or m.group('var2')\n-            start, end = m.span()\n-            sign = m.group('pre_sign') or m.group('post_sign')\n-            ret = local_vars[var]\n-            local_vars[var] += 1 if sign[0] == '+' else -1\n-            if m.group('pre_sign'):\n-                ret = local_vars[var]\n-            expr = expr[:start] + self._dump(ret, local_vars) + expr[end:]\n-\n-        if not expr:\n-            return None, should_return\n-\n-        m = re.match(r'''(?x)\n-            (?P<assign>\n-                (?P<out>{_NAME_RE})(?:\\[(?P<index>[^\\]]+?)\\])?\\s*\n-                (?P<op>{_OPERATOR_RE})?\n-                =(?!=)(?P<expr>.*)$\n-            )|(?P<return>\n-                (?!if|return|true|false|null|undefined|NaN|Infinity)(?P<name>{_NAME_RE})$\n-            )|(?P<indexing>\n-                (?P<in>{_NAME_RE})\\[(?P<idx>.+)\\]$\n-            )|(?P<attribute>\n-                (?P<var>{_NAME_RE})(?:(?P<nullish>\\?)?\\.(?P<member>[^(]+)|\\[(?P<member2>[^\\]]+)\\])\\s*\n-            )|(?P<function>\n-                (?P<fname>{_NAME_RE})\\((?P<args>.*)\\)$\n-            )'''.format(**globals()), expr)\n-        md = m.groupdict() if m else {}\n-        if md.get('assign'):\n-            left_val = local_vars.get(m.group('out'))\n-\n-            if not m.group('index'):\n-                local_vars[m.group('out')] = self._operator(\n-                    m.group('op'), left_val, m.group('expr'), expr, local_vars, allow_recursion)\n-                return local_vars[m.group('out')], should_return\n-            elif left_val in (None, JS_Undefined):\n-                raise self.Exception('Cannot index undefined variable ' + m.group('out'), expr=expr)\n-\n-            idx = self.interpret_expression(m.group('index'), local_vars, allow_recursion)\n-            if not isinstance(idx, (int, float)):\n-                raise self.Exception('List index %s must be integer' % (idx, ), expr=expr)\n-            idx = int(idx)\n-            left_val[idx] = self._operator(\n-                m.group('op'), self._index(left_val, idx), m.group('expr'), expr, local_vars, allow_recursion)\n-            return left_val[idx], should_return\n-\n-        elif expr.isdigit():\n-            return int(expr), should_return\n-\n-        elif expr == 'break':\n-            raise JS_Break()\n-        elif expr == 'continue':\n-            raise JS_Continue()\n-        elif expr == 'undefined':\n-            return JS_Undefined, should_return\n-        elif expr == 'NaN':\n-            return _NaN, should_return\n-        elif expr == 'Infinity':\n-            return _Infinity, should_return\n-\n-        elif md.get('return'):\n-            return local_vars[m.group('name')], should_return\n-\n-        try:\n-            ret = json.loads(js_to_json(expr))  # strict=True)\n-            if not md.get('attribute'):\n-                return ret, should_return\n-        except ValueError:\n-            pass\n-\n-        if md.get('indexing'):\n-            val = local_vars[m.group('in')]\n-            idx = self.interpret_expression(m.group('idx'), local_vars, allow_recursion)\n-            return self._index(val, idx), should_return\n+    def handle_operators(self, expr, local_vars, allow_recursion):\n \n         for op, _ in self._all_operators():\n             # hackety: </> have higher priority than <</>>, but don't confuse them\n@@ -832,7 +663,340 @@\n                     continue\n \n             left_val = self.interpret_expression(op.join(separated), local_vars, allow_recursion)\n-            return self._operator(op, left_val, right_expr, expr, local_vars, allow_recursion), should_return\n+            return self._operator(op, left_val, right_expr, expr, local_vars, allow_recursion), True\n+\n+    @Debugger.wrap_interpreter\n+    def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n+        if allow_recursion < 0:\n+            raise self.Exception('Recursion limit reached')\n+        allow_recursion -= 1\n+\n+        # print('At: ' + stmt[:60])\n+        should_return = False\n+        # fails on (eg) if (...) stmt1; else stmt2;\n+        sub_statements = list(self._separate(stmt, ';')) or ['']\n+        expr = stmt = sub_statements.pop().strip()\n+\n+        for sub_stmt in sub_statements:\n+            ret, should_return = self.interpret_statement(sub_stmt, local_vars, allow_recursion)\n+            if should_return:\n+                return ret, should_return\n+\n+        m = self._VAR_RET_THROW_RE.match(stmt)\n+        if m:\n+            expr = stmt[len(m.group(0)):].strip()\n+            if m.group('throw'):\n+                raise JS_Throw(self.interpret_expression(expr, local_vars, allow_recursion))\n+            should_return = 'return' if m.group('ret') else False\n+        if not expr:\n+            return None, should_return\n+\n+        if expr[0] in _QUOTES:\n+            inner, outer = self._separate(expr, expr[0], 1)\n+            if expr[0] == '/':\n+                flags, outer = self.JS_RegExp.regex_flags(outer)\n+                inner = self.JS_RegExp(inner[1:], flags=flags)\n+            else:\n+                inner = json.loads(js_to_json(inner + expr[0]))  # , strict=True))\n+            if not outer:\n+                return inner, should_return\n+            expr = self._named_object(local_vars, inner) + outer\n+\n+        new_kw, _, obj = expr.partition('new ')\n+        if not new_kw:\n+            for klass, konstr in (('Date', lambda x: int(unified_timestamp(x, False) * 1000)),\n+                                  ('RegExp', self.JS_RegExp),\n+                                  ('Error', self.Exception)):\n+                if not obj.startswith(klass + '('):\n+                    continue\n+                left, right = self._separate_at_paren(obj[len(klass):])\n+                argvals = self.interpret_iter(left, local_vars, allow_recursion)\n+                expr = konstr(*argvals)\n+                if expr is None:\n+                    raise self.Exception('Failed to parse {klass} {left!r:.100}'.format(**locals()), expr=expr)\n+                expr = self._dump(expr, local_vars) + right\n+                break\n+            else:\n+                raise self.Exception('Unsupported object {obj:.100}'.format(**locals()), expr=expr)\n+\n+        for op, _ in _UNARY_OPERATORS_X:\n+            if not expr.startswith(op):\n+                continue\n+            operand = expr[len(op):]\n+            if not operand or operand[0] != ' ':\n+                continue\n+            op_result = self.handle_operators(expr, local_vars, allow_recursion)\n+            if op_result:\n+                return op_result[0], should_return\n+\n+        if expr.startswith('{'):\n+            inner, outer = self._separate_at_paren(expr)\n+            # try for object expression (Map)\n+            sub_expressions = [list(self._separate(sub_expr.strip(), ':', 1)) for sub_expr in self._separate(inner)]\n+            if all(len(sub_expr) == 2 for sub_expr in sub_expressions):\n+                return dict(\n+                    (key_expr if re.match(_NAME_RE, key_expr) else key_expr,\n+                     self.interpret_expression(val_expr, local_vars, allow_recursion))\n+                    for key_expr, val_expr in sub_expressions), should_return\n+            # or statement list\n+            inner, should_abort = self.interpret_statement(inner, local_vars, allow_recursion)\n+            if not outer or should_abort:\n+                return inner, should_abort or should_return\n+            else:\n+                expr = self._dump(inner, local_vars) + outer\n+\n+        if expr.startswith('('):\n+            m = re.match(r'\\((?P<d>[a-z])%(?P<e>[a-z])\\.length\\+(?P=e)\\.length\\)%(?P=e)\\.length', expr)\n+            if m:\n+                # short-cut eval of frequently used `(d%e.length+e.length)%e.length`, worth ~6% on `pytest -k test_nsig`\n+                outer = None\n+                inner, should_abort = self._offset_e_by_d(m.group('d'), m.group('e'), local_vars)\n+            else:\n+                inner, outer = self._separate_at_paren(expr)\n+                inner, should_abort = self.interpret_statement(inner, local_vars, allow_recursion)\n+            if not outer or should_abort:\n+                return inner, should_abort or should_return\n+            else:\n+                expr = self._dump(inner, local_vars) + outer\n+\n+        if expr.startswith('['):\n+            inner, outer = self._separate_at_paren(expr)\n+            name = self._named_object(local_vars, [\n+                self.interpret_expression(item, local_vars, allow_recursion)\n+                for item in self._separate(inner)])\n+            expr = name + outer\n+\n+        m = self._COMPOUND_RE.match(expr)\n+        md = m.groupdict() if m else {}\n+        if md.get('if'):\n+            cndn, expr = self._separate_at_paren(expr[m.end() - 1:])\n+            if expr.startswith('{'):\n+                if_expr, expr = self._separate_at_paren(expr)\n+            else:\n+                # may lose ... else ... because of ll.368-374\n+                if_expr, expr = self._separate_at_paren(' %s;' % (expr,), delim=';')\n+            else_expr = None\n+            m = re.match(r'else\\s*(?P<block>\\{)?', expr)\n+            if m:\n+                if m.group('block'):\n+                    else_expr, expr = self._separate_at_paren(expr[m.end() - 1:])\n+                else:\n+                    # handle subset ... else if (...) {...} else ...\n+                    # TODO: make interpret_statement do this properly, if possible\n+                    exprs = list(self._separate(expr[m.end():], delim='}', max_split=2))\n+                    if len(exprs) > 1:\n+                        if re.match(r'\\s*if\\s*\\(', exprs[0]) and re.match(r'\\s*else\\b', exprs[1]):\n+                            else_expr = exprs[0] + '}' + exprs[1]\n+                            expr = (exprs[2] + '}') if len(exprs) == 3 else None\n+                        else:\n+                            else_expr = exprs[0]\n+                            exprs.append('')\n+                            expr = '}'.join(exprs[1:])\n+                    else:\n+                        else_expr = exprs[0]\n+                        expr = None\n+                    else_expr = else_expr.lstrip() + '}'\n+            cndn = _js_ternary(self.interpret_expression(cndn, local_vars, allow_recursion))\n+            ret, should_abort = self.interpret_statement(\n+                if_expr if cndn else else_expr, local_vars, allow_recursion)\n+            if should_abort:\n+                return ret, True\n+\n+        elif md.get('try'):\n+            try_expr, expr = self._separate_at_paren(expr[m.end() - 1:])\n+            err = None\n+            try:\n+                ret, should_abort = self.interpret_statement(try_expr, local_vars, allow_recursion)\n+                if should_abort:\n+                    return ret, True\n+            except Exception as e:\n+                # XXX: This works for now, but makes debugging future issues very hard\n+                err = e\n+\n+            pending = (None, False)\n+            m = re.match(r'catch\\s*(?P<err>\\(\\s*{_NAME_RE}\\s*\\))?\\{{'.format(**globals()), expr)\n+            if m:\n+                sub_expr, expr = self._separate_at_paren(expr[m.end() - 1:])\n+                if err:\n+                    catch_vars = {}\n+                    if m.group('err'):\n+                        catch_vars[m.group('err')] = err.error if isinstance(err, JS_Throw) else err\n+                    catch_vars = local_vars.new_child(m=catch_vars)\n+                    err, pending = None, self.interpret_statement(sub_expr, catch_vars, allow_recursion)\n+\n+            m = self._FINALLY_RE.match(expr)\n+            if m:\n+                sub_expr, expr = self._separate_at_paren(expr[m.end() - 1:])\n+                ret, should_abort = self.interpret_statement(sub_expr, local_vars, allow_recursion)\n+                if should_abort:\n+                    return ret, True\n+\n+            ret, should_abort = pending\n+            if should_abort:\n+                return ret, True\n+\n+            if err:\n+                raise err\n+\n+        elif md.get('for') or md.get('while'):\n+            init_or_cond, remaining = self._separate_at_paren(expr[m.end() - 1:])\n+            if remaining.startswith('{'):\n+                body, expr = self._separate_at_paren(remaining)\n+            else:\n+                switch_m = self._SWITCH_RE.match(remaining)  # FIXME\n+                if switch_m:\n+                    switch_val, remaining = self._separate_at_paren(remaining[switch_m.end() - 1:])\n+                    body, expr = self._separate_at_paren(remaining, '}')\n+                    body = 'switch(%s){%s}' % (switch_val, body)\n+                else:\n+                    body, expr = remaining, ''\n+            if md.get('for'):\n+                start, cndn, increment = self._separate(init_or_cond, ';')\n+                self.interpret_expression(start, local_vars, allow_recursion)\n+            else:\n+                cndn, increment = init_or_cond, None\n+            while _js_ternary(self.interpret_expression(cndn, local_vars, allow_recursion)):\n+                try:\n+                    ret, should_abort = self.interpret_statement(body, local_vars, allow_recursion)\n+                    if should_abort:\n+                        return ret, True\n+                except JS_Break:\n+                    break\n+                except JS_Continue:\n+                    pass\n+                if increment:\n+                    self.interpret_expression(increment, local_vars, allow_recursion)\n+\n+        elif md.get('switch'):\n+            switch_val, remaining = self._separate_at_paren(expr[m.end() - 1:])\n+            switch_val = self.interpret_expression(switch_val, local_vars, allow_recursion)\n+            body, expr = self._separate_at_paren(remaining, '}')\n+            items = body.replace('default:', 'case default:').split('case ')[1:]\n+            for default in (False, True):\n+                matched = False\n+                for item in items:\n+                    case, stmt = (i.strip() for i in self._separate(item, ':', 1))\n+                    if default:\n+                        matched = matched or case == 'default'\n+                    elif not matched:\n+                        matched = (case != 'default'\n+                                   and switch_val == self.interpret_expression(case, local_vars, allow_recursion))\n+                    if not matched:\n+                        continue\n+                    try:\n+                        ret, should_abort = self.interpret_statement(stmt, local_vars, allow_recursion)\n+                        if should_abort:\n+                            return ret\n+                    except JS_Break:\n+                        break\n+                if matched:\n+                    break\n+\n+        if md:\n+            ret, should_abort = self.interpret_statement(expr, local_vars, allow_recursion)\n+            return ret, should_abort or should_return\n+\n+        # Comma separated statements\n+        sub_expressions = list(self._separate(expr))\n+        if len(sub_expressions) > 1:\n+            for sub_expr in sub_expressions:\n+                ret, should_abort = self.interpret_statement(sub_expr, local_vars, allow_recursion)\n+                if should_abort:\n+                    return ret, True\n+            return ret, False\n+\n+        for m in re.finditer(r'''(?x)\n+                (?P<pre_sign>\\+\\+|--)(?P<var1>{_NAME_RE})|\n+                (?P<var2>{_NAME_RE})(?P<post_sign>\\+\\+|--)'''.format(**globals()), expr):\n+            var = m.group('var1') or m.group('var2')\n+            start, end = m.span()\n+            sign = m.group('pre_sign') or m.group('post_sign')\n+            ret = local_vars[var]\n+            local_vars[var] = _js_add(ret, 1 if sign[0] == '+' else -1)\n+            if m.group('pre_sign'):\n+                ret = local_vars[var]\n+            expr = expr[:start] + self._dump(ret, local_vars) + expr[end:]\n+\n+        if not expr:\n+            return None, should_return\n+\n+        m = re.match(r'''(?x)\n+            (?P<assign>\n+                (?P<out>{_NAME_RE})(?:\\[(?P<out_idx>(?:.+?\\]\\s*\\[)*.+?)\\])?\\s*\n+                (?P<op>{_OPERATOR_RE})?\n+                =(?!=)(?P<expr>.*)$\n+            )|(?P<return>\n+                (?!if|return|true|false|null|undefined|NaN|Infinity)(?P<name>{_NAME_RE})$\n+            )|(?P<indexing>\n+                (?P<in>{_NAME_RE})\\[(?P<in_idx>(?:.+?\\]\\s*\\[)*.+?)\\]$\n+            )|(?P<attribute>\n+                (?P<var>{_NAME_RE})(?:(?P<nullish>\\?)?\\.(?P<member>[^(]+)|\\[(?P<member2>[^\\]]+)\\])\\s*\n+            )|(?P<function>\n+                (?P<fname>{_NAME_RE})\\((?P<args>.*)\\)$\n+            )'''.format(**globals()), expr)\n+        md = m.groupdict() if m else {}\n+        if md.get('assign'):\n+            left_val = local_vars.get(m.group('out'))\n+\n+            if not m.group('out_idx'):\n+                local_vars[m.group('out')] = self._operator(\n+                    m.group('op'), left_val, m.group('expr'), expr, local_vars, allow_recursion)\n+                return local_vars[m.group('out')], should_return\n+            elif left_val in (None, JS_Undefined):\n+                raise self.Exception('Cannot index undefined variable ' + m.group('out'), expr=expr)\n+\n+            indexes = re.split(r'\\]\\s*\\[', m.group('out_idx'))\n+            for i, idx in enumerate(indexes, 1):\n+                idx = self.interpret_expression(idx, local_vars, allow_recursion)\n+                if i < len(indexes):\n+                    left_val = self._index(left_val, idx)\n+            if isinstance(idx, float):\n+                idx = int(idx)\n+            left_val[idx] = self._operator(\n+                m.group('op'), self._index(left_val, idx) if m.group('op') else None,\n+                m.group('expr'), expr, local_vars, allow_recursion)\n+            return left_val[idx], should_return\n+\n+        elif expr.isdigit():\n+            return int(expr), should_return\n+\n+        elif expr == 'break':\n+            raise JS_Break()\n+        elif expr == 'continue':\n+            raise JS_Continue()\n+        elif expr == 'undefined':\n+            return JS_Undefined, should_return\n+        elif expr == 'NaN':\n+            return _NaN, should_return\n+        elif expr == 'Infinity':\n+            return _Infinity, should_return\n+\n+        elif md.get('return'):\n+            ret = local_vars[m.group('name')]\n+            # challenge may try to force returning the original value\n+            # use an optional internal var to block this\n+            if should_return == 'return':\n+                if '_ytdl_do_not_return' not in local_vars:\n+                    return ret, True\n+                return (ret, True) if ret != local_vars['_ytdl_do_not_return'] else (ret, False)\n+            else:\n+                return ret, should_return\n+\n+        with compat_contextlib_suppress(ValueError):\n+            ret = json.loads(js_to_json(expr))  # strict=True)\n+            if not md.get('attribute'):\n+                return ret, should_return\n+\n+        if md.get('indexing'):\n+            val = local_vars[m.group('in')]\n+            for idx in re.split(r'\\]\\s*\\[', m.group('in_idx')):\n+                idx = self.interpret_expression(idx, local_vars, allow_recursion)\n+                val = self._index(val, idx)\n+            return val, should_return\n+\n+        op_result = self.handle_operators(expr, local_vars, allow_recursion)\n+        if op_result:\n+            return op_result[0], should_return\n \n         if md.get('attribute'):\n             variable, member, nullish = m.group('var', 'member', 'nullish')\n@@ -877,7 +1041,7 @@\n \n                 # Member access\n                 if arg_str is None:\n-                    return self._index(obj, member, nullish)\n+                    return self._index(obj, member)\n \n                 # Function call\n                 argvals = [\n@@ -904,7 +1068,7 @@\n                 if obj is compat_str:\n                     if member == 'fromCharCode':\n                         assertion(argvals, 'takes one or more arguments')\n-                        return ''.join(map(compat_chr, argvals))\n+                        return ''.join(compat_chr(int(n)) for n in argvals)\n                     raise self.Exception('Unsupported string method ' + member, expr=expr)\n                 elif obj is float:\n                     if member == 'pow':\n@@ -913,13 +1077,47 @@\n                     raise self.Exception('Unsupported Math method ' + member, expr=expr)\n \n                 if member == 'split':\n-                    assertion(argvals, 'takes one or more arguments')\n-                    assertion(len(argvals) == 1, 'with limit argument is not implemented')\n-                    return obj.split(argvals[0]) if argvals[0] else list(obj)\n+                    assertion(len(argvals) <= 2, 'takes at most two arguments')\n+                    if len(argvals) > 1:\n+                        limit = argvals[1]\n+                        assertion(isinstance(limit, int) and limit >= 0, 'integer limit >= 0')\n+                        if limit == 0:\n+                            return []\n+                    else:\n+                        limit = 0\n+                    if len(argvals) == 0:\n+                        argvals = [JS_Undefined]\n+                    elif isinstance(argvals[0], self.JS_RegExp):\n+                        # avoid re.split(), similar but not enough\n+\n+                        def where():\n+                            for m in argvals[0].finditer(obj):\n+                                yield m.span(0)\n+                            yield (None, None)\n+\n+                        def splits(limit=limit):\n+                            i = 0\n+                            for j, jj in where():\n+                                if j == jj == 0:\n+                                    continue\n+                                if j is None and i >= len(obj):\n+                                    break\n+                                yield obj[i:j]\n+                                if jj is None or limit == 1:\n+                                    break\n+                                limit -= 1\n+                                i = jj\n+\n+                        return list(splits())\n+                    return (\n+                        obj.split(argvals[0], limit - 1) if argvals[0] and argvals[0] != JS_Undefined\n+                        else list(obj)[:limit or None])\n                 elif member == 'join':\n                     assertion(isinstance(obj, list), 'must be applied on a list')\n-                    assertion(len(argvals) == 1, 'takes exactly one argument')\n-                    return argvals[0].join(obj)\n+                    assertion(len(argvals) <= 1, 'takes at most one argument')\n+                    return (',' if len(argvals) == 0 else argvals[0]).join(\n+                        ('' if x in (None, JS_Undefined) else _js_toString(x))\n+                        for x in obj)\n                 elif member == 'reverse':\n                     assertion(not argvals, 'does not take any arguments')\n                     obj.reverse()\n@@ -941,37 +1139,31 @@\n                     index, how_many = map(int, (argvals + [len(obj)])[:2])\n                     if index < 0:\n                         index += len(obj)\n-                    add_items = argvals[2:]\n-                    res = []\n-                    for _ in range(index, min(index + how_many, len(obj))):\n-                        res.append(obj.pop(index))\n-                    for i, item in enumerate(add_items):\n-                        obj.insert(index + i, item)\n+                    res = [obj.pop(index)\n+                           for _ in range(index, min(index + how_many, len(obj)))]\n+                    obj[index:index] = argvals[2:]\n                     return res\n+                elif member in ('shift', 'pop'):\n+                    assertion(isinstance(obj, list), 'must be applied on a list')\n+                    assertion(not argvals, 'does not take any arguments')\n+                    return obj.pop(0 if member == 'shift' else -1) if len(obj) > 0 else JS_Undefined\n                 elif member == 'unshift':\n                     assertion(isinstance(obj, list), 'must be applied on a list')\n-                    assertion(argvals, 'takes one or more arguments')\n-                    for item in reversed(argvals):\n-                        obj.insert(0, item)\n-                    return obj\n-                elif member == 'pop':\n-                    assertion(isinstance(obj, list), 'must be applied on a list')\n-                    assertion(not argvals, 'does not take any arguments')\n-                    if not obj:\n-                        return\n-                    return obj.pop()\n+                    # not enforced: assertion(argvals, 'takes one or more arguments')\n+                    obj[0:0] = argvals\n+                    return len(obj)\n                 elif member == 'push':\n-                    assertion(argvals, 'takes one or more arguments')\n+                    # not enforced: assertion(argvals, 'takes one or more arguments')\n                     obj.extend(argvals)\n-                    return obj\n+                    return len(obj)\n                 elif member == 'forEach':\n                     assertion(argvals, 'takes one or more arguments')\n-                    assertion(len(argvals) <= 2, 'takes at-most 2 arguments')\n+                    assertion(len(argvals) <= 2, 'takes at most 2 arguments')\n                     f, this = (argvals + [''])[:2]\n                     return [f((item, idx, obj), {'this': this}, allow_recursion) for idx, item in enumerate(obj)]\n                 elif member == 'indexOf':\n                     assertion(argvals, 'takes one or more arguments')\n-                    assertion(len(argvals) <= 2, 'takes at-most 2 arguments')\n+                    assertion(len(argvals) <= 2, 'takes at most 2 arguments')\n                     idx, start = (argvals + [0])[:2]\n                     try:\n                         return obj.index(idx, start)\n@@ -980,9 +1172,15 @@\n                 elif member == 'charCodeAt':\n                     assertion(isinstance(obj, compat_str), 'must be applied on a string')\n                     # assertion(len(argvals) == 1, 'takes exactly one argument') # but not enforced\n-                    idx = argvals[0] if isinstance(argvals[0], int) else 0\n+                    idx = argvals[0] if len(argvals) > 0 and isinstance(argvals[0], int) else 0\n+\n+                    # Bug: Return None for the first character of a non-empty string\n+                    if idx == 0 and obj:\n+                         return None\n+\n                     if idx >= len(obj):\n-                        return None\n+                         return None\n+\n                     return ord(obj[idx])\n                 elif member in ('replace', 'replaceAll'):\n                     assertion(isinstance(obj, compat_str), 'must be applied on a string')\n@@ -1031,7 +1229,7 @@\n             yield self.interpret_expression(v, local_vars, allow_recursion)\n \n     def extract_object(self, objname):\n-        _FUNC_NAME_RE = r'''(?:[a-zA-Z$0-9]+|\"[a-zA-Z$0-9]+\"|'[a-zA-Z$0-9]+')'''\n+        _FUNC_NAME_RE = r'''(?:{n}|\"{n}\"|'{n}')'''.format(n=_NAME_RE)\n         obj = {}\n         fields = next(filter(None, (\n             obj_m.group('fields') for obj_m in re.finditer(\n@@ -1089,7 +1287,9 @@\n             'F<%s>' % (funcname,))\n \n     def extract_function_from_code(self, argnames, code, *global_stack):\n-        local_vars = {}\n+        global_stack = list(global_stack) or [{}]\n+        argnames = tuple(argnames)\n+\n         while True:\n             mobj = re.search(r'function\\((?P<args>[^)]*)\\)\\s*{', code)\n             if mobj is None:\n@@ -1100,10 +1300,11 @@\n                 [x.strip() for x in mobj.group('args').split(',')],\n                 body, local_vars, *global_stack))\n             code = code[:start] + name + remaining\n+\n         return self.build_function(argnames, code, local_vars, *global_stack)\n \n-    def call_function(self, funcname, *args):\n-        return self.extract_function(funcname)(args)\n+    def call_function(self, funcname, *args, **kw_global_vars):\n+        return self.extract_function(funcname)(args, kw_global_vars)\n \n     @classmethod\n     def build_arglist(cls, arg_text):\n@@ -1122,8 +1323,9 @@\n         global_stack = list(global_stack) or [{}]\n         argnames = tuple(argnames)\n \n-        def resf(args, kwargs={}, allow_recursion=100):\n-            global_stack[0].update(zip_longest(argnames, args, fillvalue=None))\n+        def resf(args, kwargs=None, allow_recursion=100):\n+            kwargs = kwargs or {}\n+            global_stack[0].update(zip_longest(argnames, args, fillvalue=JS_Undefined))\n             global_stack[0].update(kwargs)\n             var_stack = LocalNameSpace(*global_stack)\n             ret, should_abort = self.interpret_statement(code.replace('\\n', ' '), var_stack, allow_recursion - 1)\n"
    ]
  },
  {
    "repo": "ytdl-org/youtube-dl",
    "pull_number": 32845,
    "instance_id": "ytdl-org__youtube-dl-32845",
    "issue_numbers": [
      "32842",
      "32843"
    ],
    "base_commit": "a452f9437c8a3048f75fc12f75bcfd3eed78430f",
    "patch": "diff --git a/youtube_dl/extractor/youtube.py b/youtube_dl/extractor/youtube.py\nindex 90c16e172bd..2e31a89798e 100644\n--- a/youtube_dl/extractor/youtube.py\n+++ b/youtube_dl/extractor/youtube.py\n@@ -1636,7 +1636,7 @@ def _decrypt_nsig(self, n, video_id, player_url):\n         try:\n             jsi, player_id, func_code = self._extract_n_function_code(video_id, player_url)\n         except ExtractorError as e:\n-            raise ExtractorError('Unable to extract nsig jsi, player_id, func_codefunction code', cause=e)\n+            raise ExtractorError('Unable to extract nsig function code', cause=e)\n         if self.get_param('youtube_print_sig_code'):\n             self.to_screen('Extracted nsig function from {0}:\\n{1}\\n'.format(\n                 player_id, func_code[1]))\n@@ -1658,8 +1658,14 @@ def _decrypt_nsig(self, n, video_id, player_url):\n \n     def _extract_n_function_name(self, jscode):\n         func_name, idx = self._search_regex(\n-            r'\\.get\\(\"n\"\\)\\)&&\\(b=(?P<nfunc>[a-zA-Z_$][\\w$]*)(?:\\[(?P<idx>\\d+)\\])?\\([\\w$]+\\)',\n-            jscode, 'Initial JS player n function name', group=('nfunc', 'idx'))\n+            # new: (b=String.fromCharCode(110),c=a.get(b))&&c=nfunc[idx](c)\n+            # old: .get(\"n\"))&&(b=nfunc[idx](b)\n+            # older: .get(\"n\"))&&(b=nfunc(b)\n+            r'''(?x)\n+                (?:\\(\\s*(?P<b>[a-z])\\s*=\\s*String\\s*\\.\\s*fromCharCode\\s*\\(\\s*110\\s*\\)\\s*,(?P<c>[a-z])\\s*=\\s*[a-z]\\s*)?\n+                \\.\\s*get\\s*\\(\\s*(?(b)(?P=b)|\"n\")(?:\\s*\\)){2}\\s*&&\\s*\\(\\s*(?(c)(?P=c)|b)\\s*=\\s*\n+                (?P<nfunc>[a-zA-Z_$][\\w$]*)(?:\\s*\\[(?P<idx>\\d+)\\])?\\s*\\(\\s*[\\w$]+\\s*\\)\n+            ''', jscode, 'Initial JS player n function name', group=('nfunc', 'idx'))\n         if not idx:\n             return func_name\n \n@@ -1679,17 +1685,7 @@ def _extract_n_function_code(self, video_id, player_url):\n \n         func_name = self._extract_n_function_name(jscode)\n \n-        # For redundancy\n-        func_code = self._search_regex(\n-            r'''(?xs)%s\\s*=\\s*function\\s*\\((?P<var>[\\w$]+)\\)\\s*\n-                     # NB: The end of the regex is intentionally kept strict\n-                     {(?P<code>.+?}\\s*return\\ [\\w$]+.join\\(\"\"\\))};''' % func_name,\n-            jscode, 'nsig function', group=('var', 'code'), default=None)\n-        if func_code:\n-            func_code = ([func_code[0]], func_code[1])\n-        else:\n-            self.write_debug('Extracting nsig function with jsinterp')\n-            func_code = jsi.extract_function_code(func_name)\n+        func_code = jsi.extract_function_code(func_name)\n \n         self.cache.store('youtube-nsig', player_id, func_code)\n         return jsi, player_id, func_code\ndiff --git a/youtube_dl/jsinterp.py b/youtube_dl/jsinterp.py\nindex 02adf667846..949f77775e8 100644\n--- a/youtube_dl/jsinterp.py\n+++ b/youtube_dl/jsinterp.py\n@@ -20,7 +20,9 @@\n     compat_basestring,\n     compat_chr,\n     compat_collections_chain_map as ChainMap,\n+    compat_filter as filter,\n     compat_itertools_zip_longest as zip_longest,\n+    compat_map as map,\n     compat_str,\n )\n \n@@ -252,7 +254,7 @@ def interpret_statement(self, stmt, local_vars, allow_recursion, *args, **kwargs\n                     cls.write('=> Raises:', e, '<-|', stmt, level=allow_recursion)\n                 raise\n             if cls.ENABLED and stmt.strip():\n-                if should_ret or not repr(ret) == stmt:\n+                if should_ret or repr(ret) != stmt:\n                     cls.write(['->', '=>'][should_ret], repr(ret), '<-|', stmt, level=allow_recursion)\n             return ret, should_ret\n         return interpret_statement\n@@ -365,6 +367,8 @@ def _separate(cls, expr, delim=',', max_split=None, skip_delims=None):\n         start, splits, pos, delim_len = 0, 0, 0, len(delim) - 1\n         in_quote, escaping, after_op, in_regex_char_group = None, False, True, False\n         skipping = 0\n+        if skip_delims:\n+            skip_delims = variadic(skip_delims)\n         for idx, char in enumerate(expr):\n             paren_delta = 0\n             if not in_quote:\n@@ -391,7 +395,7 @@ def _separate(cls, expr, delim=',', max_split=None, skip_delims=None):\n                 continue\n             elif pos == 0 and skip_delims:\n                 here = expr[idx:]\n-                for s in variadic(skip_delims):\n+                for s in skip_delims:\n                     if here.startswith(s) and s:\n                         skipping = len(s) - 1\n                         break\n@@ -412,7 +416,6 @@ def _separate_at_paren(cls, expr, delim=None):\n         if delim is None:\n             delim = expr and _MATCHING_PARENS[expr[0]]\n         separated = list(cls._separate(expr, delim, 1))\n-\n         if len(separated) < 2:\n             raise cls.Exception('No terminating paren {delim} in {expr!r:.5500}'.format(**locals()))\n         return separated[0][1:].strip(), separated[1].strip()\n@@ -487,6 +490,7 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n         # fails on (eg) if (...) stmt1; else stmt2;\n         sub_statements = list(self._separate(stmt, ';')) or ['']\n         expr = stmt = sub_statements.pop().strip()\n+\n         for sub_stmt in sub_statements:\n             ret, should_return = self.interpret_statement(sub_stmt, local_vars, allow_recursion)\n             if should_return:\n@@ -626,8 +630,7 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n                     if m.group('err'):\n                         catch_vars[m.group('err')] = err.error if isinstance(err, JS_Throw) else err\n                     catch_vars = local_vars.new_child(m=catch_vars)\n-                    err = None\n-                    pending = self.interpret_statement(sub_expr, catch_vars, allow_recursion)\n+                    err, pending = None, self.interpret_statement(sub_expr, catch_vars, allow_recursion)\n \n             m = self._FINALLY_RE.match(expr)\n             if m:\n@@ -801,16 +804,19 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n             if op in ('+', '-'):\n                 # simplify/adjust consecutive instances of these operators\n                 undone = 0\n-                while len(separated) > 1 and not separated[-1].strip():\n+                separated = [s.strip() for s in separated]\n+                while len(separated) > 1 and not separated[-1]:\n                     undone += 1\n                     separated.pop()\n                 if op == '-' and undone % 2 != 0:\n                     right_expr = op + right_expr\n                 elif op == '+':\n-                    while len(separated) > 1 and separated[-1].strip() in self.OP_CHARS:\n+                    while len(separated) > 1 and set(separated[-1]) <= self.OP_CHARS:\n+                        right_expr = separated.pop() + right_expr\n+                    if separated[-1][-1:] in self.OP_CHARS:\n                         right_expr = separated.pop() + right_expr\n                 # hanging op at end of left => unary + (strip) or - (push right)\n-                left_val = separated[-1]\n+                left_val = separated[-1] if separated else ''\n                 for dm_op in ('*', '%', '/', '**'):\n                     bodmas = tuple(self._separate(left_val, dm_op, skip_delims=skip_delim))\n                     if len(bodmas) > 1 and not bodmas[-1].strip():\n@@ -844,7 +850,7 @@ def assertion(cndn, msg):\n                     memb = member\n                     raise self.Exception('{memb} {msg}'.format(**locals()), expr=expr)\n \n-            def eval_method():\n+            def eval_method(variable, member):\n                 if (variable, member) == ('console', 'debug'):\n                     if Debugger.ENABLED:\n                         Debugger.write(self.interpret_expression('[{}]'.format(arg_str), local_vars, allow_recursion))\n@@ -852,6 +858,7 @@ def eval_method():\n                 types = {\n                     'String': compat_str,\n                     'Math': float,\n+                    'Array': list,\n                 }\n                 obj = local_vars.get(variable)\n                 if obj in (JS_Undefined, None):\n@@ -877,12 +884,29 @@ def eval_method():\n                     self.interpret_expression(v, local_vars, allow_recursion)\n                     for v in self._separate(arg_str)]\n \n-                if obj == compat_str:\n+                # Fixup prototype call\n+                if isinstance(obj, type):\n+                    new_member, rest = member.partition('.')[0::2]\n+                    if new_member == 'prototype':\n+                        new_member, func_prototype = rest.partition('.')[0::2]\n+                        assertion(argvals, 'takes one or more arguments')\n+                        assertion(isinstance(argvals[0], obj), 'must bind to type {0}'.format(obj))\n+                        if func_prototype == 'call':\n+                            obj = argvals.pop(0)\n+                        elif func_prototype == 'apply':\n+                            assertion(len(argvals) == 2, 'takes two arguments')\n+                            obj, argvals = argvals\n+                            assertion(isinstance(argvals, list), 'second argument must be a list')\n+                        else:\n+                            raise self.Exception('Unsupported Function method ' + func_prototype, expr)\n+                        member = new_member\n+\n+                if obj is compat_str:\n                     if member == 'fromCharCode':\n                         assertion(argvals, 'takes one or more arguments')\n                         return ''.join(map(compat_chr, argvals))\n                     raise self.Exception('Unsupported string method ' + member, expr=expr)\n-                elif obj == float:\n+                elif obj is float:\n                     if member == 'pow':\n                         assertion(len(argvals) == 2, 'takes two arguments')\n                         return argvals[0] ** argvals[1]\n@@ -907,12 +931,12 @@ def eval_method():\n                 elif member == 'splice':\n                     assertion(isinstance(obj, list), 'must be applied on a list')\n                     assertion(argvals, 'takes one or more arguments')\n-                    index, howMany = map(int, (argvals + [len(obj)])[:2])\n+                    index, how_many = map(int, (argvals + [len(obj)])[:2])\n                     if index < 0:\n                         index += len(obj)\n                     add_items = argvals[2:]\n                     res = []\n-                    for i in range(index, min(index + howMany, len(obj))):\n+                    for _ in range(index, min(index + how_many, len(obj))):\n                         res.append(obj.pop(index))\n                     for i, item in enumerate(add_items):\n                         obj.insert(index + i, item)\n@@ -970,11 +994,11 @@ def eval_method():\n \n             if remaining:\n                 ret, should_abort = self.interpret_statement(\n-                    self._named_object(local_vars, eval_method()) + remaining,\n+                    self._named_object(local_vars, eval_method(variable, member)) + remaining,\n                     local_vars, allow_recursion)\n                 return ret, should_return or should_abort\n             else:\n-                return eval_method(), should_return\n+                return eval_method(variable, member), should_return\n \n         elif md.get('function'):\n             fname = m.group('fname')\n@@ -1002,28 +1026,25 @@ def interpret_iter(self, list_txt, local_vars, allow_recursion):\n     def extract_object(self, objname):\n         _FUNC_NAME_RE = r'''(?:[a-zA-Z$0-9]+|\"[a-zA-Z$0-9]+\"|'[a-zA-Z$0-9]+')'''\n         obj = {}\n-        fields = None\n-        for obj_m in re.finditer(\n+        fields = next(filter(None, (\n+            obj_m.group('fields') for obj_m in re.finditer(\n                 r'''(?xs)\n                     {0}\\s*\\.\\s*{1}|{1}\\s*=\\s*\\{{\\s*\n                         (?P<fields>({2}\\s*:\\s*function\\s*\\(.*?\\)\\s*\\{{.*?}}(?:,\\s*)?)*)\n                     }}\\s*;\n                 '''.format(_NAME_RE, re.escape(objname), _FUNC_NAME_RE),\n-                self.code):\n-            fields = obj_m.group('fields')\n-            if fields:\n-                break\n-        else:\n+                self.code))), None)\n+        if not fields:\n             raise self.Exception('Could not find object ' + objname)\n         # Currently, it only supports function definitions\n-        fields_m = re.finditer(\n-            r'''(?x)\n-                (?P<key>%s)\\s*:\\s*function\\s*\\((?P<args>(?:%s|,)*)\\){(?P<code>[^}]+)}\n-            ''' % (_FUNC_NAME_RE, _NAME_RE),\n-            fields)\n-        for f in fields_m:\n+        for f in re.finditer(\n+                r'''(?x)\n+                    (?P<key>%s)\\s*:\\s*function\\s*\\((?P<args>(?:%s|,)*)\\){(?P<code>[^}]+)}\n+                ''' % (_FUNC_NAME_RE, _NAME_RE),\n+                fields):\n             argnames = self.build_arglist(f.group('args'))\n-            obj[remove_quotes(f.group('key'))] = self.build_function(argnames, f.group('code'))\n+            name = remove_quotes(f.group('key'))\n+            obj[name] = function_with_repr(self.build_function(argnames, f.group('code')), 'F<{0}>'.format(name))\n \n         return obj\n \n@@ -1058,7 +1079,7 @@ def extract_function_code(self, funcname):\n     def extract_function(self, funcname):\n         return function_with_repr(\n             self.extract_function_from_code(*self.extract_function_code(funcname)),\n-            'F<%s>' % (funcname, ))\n+            'F<%s>' % (funcname,))\n \n     def extract_function_from_code(self, argnames, code, *global_stack):\n         local_vars = {}\n@@ -1067,7 +1088,7 @@ def extract_function_from_code(self, argnames, code, *global_stack):\n             if mobj is None:\n                 break\n             start, body_start = mobj.span()\n-            body, remaining = self._separate_at_paren(code[body_start - 1:], '}')\n+            body, remaining = self._separate_at_paren(code[body_start - 1:])\n             name = self._named_object(local_vars, self.extract_function_from_code(\n                 [x.strip() for x in mobj.group('args').split(',')],\n                 body, local_vars, *global_stack))\n@@ -1095,8 +1116,7 @@ def build_function(self, argnames, code, *global_stack):\n         argnames = tuple(argnames)\n \n         def resf(args, kwargs={}, allow_recursion=100):\n-            global_stack[0].update(\n-                zip_longest(argnames, args, fillvalue=None))\n+            global_stack[0].update(zip_longest(argnames, args, fillvalue=None))\n             global_stack[0].update(kwargs)\n             var_stack = LocalNameSpace(*global_stack)\n             ret, should_abort = self.interpret_statement(code.replace('\\n', ' '), var_stack, allow_recursion - 1)\ndiff --git a/youtube_dl/utils.py b/youtube_dl/utils.py\nindex 3ec9d381190..ac1e78002b3 100644\n--- a/youtube_dl/utils.py\n+++ b/youtube_dl/utils.py\n@@ -6604,27 +6604,53 @@ class _UnsafeExtensionError(Exception):\n         ),\n         # video\n         MEDIA_EXTENSIONS.video, (\n-            'avif',\n+            'asx',\n             'ismv',\n+            'm2t',\n             'm2ts',\n+            'm2v',\n             'm4s',\n             'mng',\n+            'mp2v',\n+            'mp4v',\n+            'mpe',\n             'mpeg',\n+            'mpeg1',\n+            'mpeg2',\n+            'mpeg4',\n+            'mxf',\n+            'ogm',\n             'qt',\n+            'rm',\n             'swf',\n             'ts',\n+            'vob',\n             'vp9',\n-            'wvm',\n         ),\n         # audio\n         MEDIA_EXTENSIONS.audio, (\n+            '3ga',\n+            'ac3',\n+            'adts',\n+            'aif',\n+            'au',\n+            'dts',\n             'isma',\n+            'it',\n             'mid',\n+            'mod',\n             'mpga',\n+            'mp1',\n+            'mp2',\n+            'mp4a',\n+            'mpa',\n             'ra',\n+            'shn',\n+            'xm',\n         ),\n         # image\n         MEDIA_EXTENSIONS.thumbnails, (\n+            'avif',\n             'bmp',\n             'gif',\n             'ico',\n@@ -6634,6 +6660,7 @@ class _UnsafeExtensionError(Exception):\n             'jxl',\n             'svg',\n             'tif',\n+            'tiff',\n             'wbmp',\n         ),\n         # subtitle\n@@ -6641,10 +6668,15 @@ class _UnsafeExtensionError(Exception):\n             'dfxp',\n             'fs',\n             'ismt',\n+            'json3',\n             'sami',\n             'scc',\n+            'srv1',\n+            'srv2',\n+            'srv3',\n             'ssa',\n             'tt',\n+            'xml',\n         ),\n         # others\n         MEDIA_EXTENSIONS.manifests,\n@@ -6658,7 +6690,6 @@ class _UnsafeExtensionError(Exception):\n             # 'swp',\n             # 'url',\n             # 'webloc',\n-            # 'xml',\n         )))\n \n     def __init__(self, extension):\n",
    "test_patch": "diff --git a/test/test_jsinterp.py b/test/test_jsinterp.py\nindex da8e980207a..104e766be36 100644\n--- a/test/test_jsinterp.py\n+++ b/test/test_jsinterp.py\n@@ -11,194 +11,146 @@\n import math\n import re\n \n+from youtube_dl.compat import compat_str\n from youtube_dl.jsinterp import JS_Undefined, JSInterpreter\n \n+NaN = object()\n \n-class TestJSInterpreter(unittest.TestCase):\n-    def test_basic(self):\n-        jsi = JSInterpreter('function x(){;}')\n-        self.assertEqual(jsi.call_function('x'), None)\n-        self.assertEqual(repr(jsi.extract_function('x')), 'F<x>')\n-\n-        jsi = JSInterpreter('function x3(){return 42;}')\n-        self.assertEqual(jsi.call_function('x3'), 42)\n \n-        jsi = JSInterpreter('function x3(){42}')\n-        self.assertEqual(jsi.call_function('x3'), None)\n+class TestJSInterpreter(unittest.TestCase):\n+    def _test(self, jsi_or_code, expected, func='f', args=()):\n+        if isinstance(jsi_or_code, compat_str):\n+            jsi_or_code = JSInterpreter(jsi_or_code)\n+        got = jsi_or_code.call_function(func, *args)\n+        if expected is NaN:\n+            self.assertTrue(math.isnan(got), '{0} is not NaN'.format(got))\n+        else:\n+            self.assertEqual(got, expected)\n \n-        jsi = JSInterpreter('var x5 = function(){return 42;}')\n-        self.assertEqual(jsi.call_function('x5'), 42)\n+    def test_basic(self):\n+        jsi = JSInterpreter('function f(){;}')\n+        self.assertEqual(repr(jsi.extract_function('f')), 'F<f>')\n+        self._test(jsi, None)\n \n-    def test_calc(self):\n-        jsi = JSInterpreter('function x4(a){return 2*a+1;}')\n-        self.assertEqual(jsi.call_function('x4', 3), 7)\n+        self._test('function f(){return 42;}', 42)\n+        self._test('function f(){42}', None)\n+        self._test('var f = function(){return 42;}', 42)\n \n     def test_add(self):\n-        jsi = JSInterpreter('function f(){return 42 + 7;}')\n-        self.assertEqual(jsi.call_function('f'), 49)\n-        jsi = JSInterpreter('function f(){return 42 + undefined;}')\n-        self.assertTrue(math.isnan(jsi.call_function('f')))\n-        jsi = JSInterpreter('function f(){return 42 + null;}')\n-        self.assertEqual(jsi.call_function('f'), 42)\n+        self._test('function f(){return 42 + 7;}', 49)\n+        self._test('function f(){return 42 + undefined;}', NaN)\n+        self._test('function f(){return 42 + null;}', 42)\n \n     def test_sub(self):\n-        jsi = JSInterpreter('function f(){return 42 - 7;}')\n-        self.assertEqual(jsi.call_function('f'), 35)\n-        jsi = JSInterpreter('function f(){return 42 - undefined;}')\n-        self.assertTrue(math.isnan(jsi.call_function('f')))\n-        jsi = JSInterpreter('function f(){return 42 - null;}')\n-        self.assertEqual(jsi.call_function('f'), 42)\n+        self._test('function f(){return 42 - 7;}', 35)\n+        self._test('function f(){return 42 - undefined;}', NaN)\n+        self._test('function f(){return 42 - null;}', 42)\n \n     def test_mul(self):\n-        jsi = JSInterpreter('function f(){return 42 * 7;}')\n-        self.assertEqual(jsi.call_function('f'), 294)\n-        jsi = JSInterpreter('function f(){return 42 * undefined;}')\n-        self.assertTrue(math.isnan(jsi.call_function('f')))\n-        jsi = JSInterpreter('function f(){return 42 * null;}')\n-        self.assertEqual(jsi.call_function('f'), 0)\n+        self._test('function f(){return 42 * 7;}', 294)\n+        self._test('function f(){return 42 * undefined;}', NaN)\n+        self._test('function f(){return 42 * null;}', 0)\n \n     def test_div(self):\n         jsi = JSInterpreter('function f(a, b){return a / b;}')\n-        self.assertTrue(math.isnan(jsi.call_function('f', 0, 0)))\n-        self.assertTrue(math.isnan(jsi.call_function('f', JS_Undefined, 1)))\n-        self.assertTrue(math.isinf(jsi.call_function('f', 2, 0)))\n-        self.assertEqual(jsi.call_function('f', 0, 3), 0)\n+        self._test(jsi, NaN, args=(0, 0))\n+        self._test(jsi, NaN, args=(JS_Undefined, 1))\n+        self._test(jsi, float('inf'), args=(2, 0))\n+        self._test(jsi, 0, args=(0, 3))\n \n     def test_mod(self):\n-        jsi = JSInterpreter('function f(){return 42 % 7;}')\n-        self.assertEqual(jsi.call_function('f'), 0)\n-        jsi = JSInterpreter('function f(){return 42 % 0;}')\n-        self.assertTrue(math.isnan(jsi.call_function('f')))\n-        jsi = JSInterpreter('function f(){return 42 % undefined;}')\n-        self.assertTrue(math.isnan(jsi.call_function('f')))\n+        self._test('function f(){return 42 % 7;}', 0)\n+        self._test('function f(){return 42 % 0;}', NaN)\n+        self._test('function f(){return 42 % undefined;}', NaN)\n \n     def test_exp(self):\n-        jsi = JSInterpreter('function f(){return 42 ** 2;}')\n-        self.assertEqual(jsi.call_function('f'), 1764)\n-        jsi = JSInterpreter('function f(){return 42 ** undefined;}')\n-        self.assertTrue(math.isnan(jsi.call_function('f')))\n-        jsi = JSInterpreter('function f(){return 42 ** null;}')\n-        self.assertEqual(jsi.call_function('f'), 1)\n-        jsi = JSInterpreter('function f(){return undefined ** 42;}')\n-        self.assertTrue(math.isnan(jsi.call_function('f')))\n+        self._test('function f(){return 42 ** 2;}', 1764)\n+        self._test('function f(){return 42 ** undefined;}', NaN)\n+        self._test('function f(){return 42 ** null;}', 1)\n+        self._test('function f(){return undefined ** 42;}', NaN)\n+\n+    def test_calc(self):\n+        self._test('function f(a){return 2*a+1;}', 7, args=[3])\n \n     def test_empty_return(self):\n-        jsi = JSInterpreter('function f(){return; y()}')\n-        self.assertEqual(jsi.call_function('f'), None)\n+        self._test('function f(){return; y()}', None)\n \n     def test_morespace(self):\n-        jsi = JSInterpreter('function x (a) { return 2 * a + 1 ; }')\n-        self.assertEqual(jsi.call_function('x', 3), 7)\n-\n-        jsi = JSInterpreter('function f () { x =  2  ; return x; }')\n-        self.assertEqual(jsi.call_function('f'), 2)\n+        self._test('function f (a) { return 2 * a + 1 ; }', 7, args=[3])\n+        self._test('function f () { x =  2  ; return x; }', 2)\n \n     def test_strange_chars(self):\n-        jsi = JSInterpreter('function $_xY1 ($_axY1) { var $_axY2 = $_axY1 + 1; return $_axY2; }')\n-        self.assertEqual(jsi.call_function('$_xY1', 20), 21)\n+        self._test('function $_xY1 ($_axY1) { var $_axY2 = $_axY1 + 1; return $_axY2; }',\n+                   21, args=[20], func='$_xY1')\n \n     def test_operators(self):\n-        jsi = JSInterpreter('function f(){return 1 << 5;}')\n-        self.assertEqual(jsi.call_function('f'), 32)\n-\n-        jsi = JSInterpreter('function f(){return 2 ** 5}')\n-        self.assertEqual(jsi.call_function('f'), 32)\n-\n-        jsi = JSInterpreter('function f(){return 19 & 21;}')\n-        self.assertEqual(jsi.call_function('f'), 17)\n-\n-        jsi = JSInterpreter('function f(){return 11 >> 2;}')\n-        self.assertEqual(jsi.call_function('f'), 2)\n-\n-        jsi = JSInterpreter('function f(){return []? 2+3: 4;}')\n-        self.assertEqual(jsi.call_function('f'), 5)\n-\n-        jsi = JSInterpreter('function f(){return 1 == 2}')\n-        self.assertEqual(jsi.call_function('f'), False)\n-\n-        jsi = JSInterpreter('function f(){return 0 && 1 || 2;}')\n-        self.assertEqual(jsi.call_function('f'), 2)\n-\n-        jsi = JSInterpreter('function f(){return 0 ?? 42;}')\n-        self.assertEqual(jsi.call_function('f'), 0)\n-\n-        jsi = JSInterpreter('function f(){return \"life, the universe and everything\" < 42;}')\n-        self.assertFalse(jsi.call_function('f'))\n+        self._test('function f(){return 1 << 5;}', 32)\n+        self._test('function f(){return 2 ** 5}', 32)\n+        self._test('function f(){return 19 & 21;}', 17)\n+        self._test('function f(){return 11 >> 2;}', 2)\n+        self._test('function f(){return []? 2+3: 4;}', 5)\n+        self._test('function f(){return 1 == 2}', False)\n+        self._test('function f(){return 0 && 1 || 2;}', 2)\n+        self._test('function f(){return 0 ?? 42;}', 0)\n+        self._test('function f(){return \"life, the universe and everything\" < 42;}', False)\n+        # https://github.com/ytdl-org/youtube-dl/issues/32815\n+        self._test('function f(){return 0  - 7 * - 6;}', 42)\n \n     def test_array_access(self):\n-        jsi = JSInterpreter('function f(){var x = [1,2,3]; x[0] = 4; x[0] = 5; x[2.0] = 7; return x;}')\n-        self.assertEqual(jsi.call_function('f'), [5, 2, 7])\n+        self._test('function f(){var x = [1,2,3]; x[0] = 4; x[0] = 5; x[2.0] = 7; return x;}', [5, 2, 7])\n \n     def test_parens(self):\n-        jsi = JSInterpreter('function f(){return (1) + (2) * ((( (( (((((3)))))) )) ));}')\n-        self.assertEqual(jsi.call_function('f'), 7)\n-\n-        jsi = JSInterpreter('function f(){return (1 + 2) * 3;}')\n-        self.assertEqual(jsi.call_function('f'), 9)\n+        self._test('function f(){return (1) + (2) * ((( (( (((((3)))))) )) ));}', 7)\n+        self._test('function f(){return (1 + 2) * 3;}', 9)\n \n     def test_quotes(self):\n-        jsi = JSInterpreter(r'function f(){return \"a\\\"\\\\(\"}')\n-        self.assertEqual(jsi.call_function('f'), r'a\"\\(')\n+        self._test(r'function f(){return \"a\\\"\\\\(\"}', r'a\"\\(')\n \n     def test_assignments(self):\n-        jsi = JSInterpreter('function f(){var x = 20; x = 30 + 1; return x;}')\n-        self.assertEqual(jsi.call_function('f'), 31)\n-\n-        jsi = JSInterpreter('function f(){var x = 20; x += 30 + 1; return x;}')\n-        self.assertEqual(jsi.call_function('f'), 51)\n-\n-        jsi = JSInterpreter('function f(){var x = 20; x -= 30 + 1; return x;}')\n-        self.assertEqual(jsi.call_function('f'), -11)\n+        self._test('function f(){var x = 20; x = 30 + 1; return x;}', 31)\n+        self._test('function f(){var x = 20; x += 30 + 1; return x;}', 51)\n+        self._test('function f(){var x = 20; x -= 30 + 1; return x;}', -11)\n \n+    @unittest.skip('Not yet fully implemented')\n     def test_comments(self):\n-        'Skipping: Not yet fully implemented'\n-        return\n-        jsi = JSInterpreter('''\n-        function x() {\n-            var x = /* 1 + */ 2;\n-            var y = /* 30\n-            * 40 */ 50;\n-            return x + y;\n-        }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 52)\n-\n-        jsi = JSInterpreter('''\n-        function f() {\n-            var x = \"/*\";\n-            var y = 1 /* comment */ + 2;\n-            return y;\n-        }\n-        ''')\n-        self.assertEqual(jsi.call_function('f'), 3)\n+        self._test('''\n+            function f() {\n+                var x = /* 1 + */ 2;\n+                var y = /* 30\n+                * 40 */ 50;\n+                return x + y;\n+            }\n+        ''', 52)\n+\n+        self._test('''\n+            function f() {\n+                var x = \"/*\";\n+                var y = 1 /* comment */ + 2;\n+                return y;\n+            }\n+        ''', 3)\n \n     def test_precedence(self):\n-        jsi = JSInterpreter('''\n-        function x() {\n-            var a = [10, 20, 30, 40, 50];\n-            var b = 6;\n-            a[0]=a[b%a.length];\n-            return a;\n-        }''')\n-        self.assertEqual(jsi.call_function('x'), [20, 20, 30, 40, 50])\n+        self._test('''\n+            function f() {\n+                var a = [10, 20, 30, 40, 50];\n+                var b = 6;\n+                a[0]=a[b%a.length];\n+                return a;\n+            }\n+        ''', [20, 20, 30, 40, 50])\n \n     def test_builtins(self):\n-        jsi = JSInterpreter('''\n-        function x() { return NaN }\n-        ''')\n-        self.assertTrue(math.isnan(jsi.call_function('x')))\n+        self._test('function f() { return NaN }', NaN)\n \n     def test_Date(self):\n-        jsi = JSInterpreter('''\n-        function x(dt) { return new Date(dt) - 0; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x', 'Wednesday 31 December 1969 18:01:26 MDT'), 86000)\n+        self._test('function f() { return new Date(\"Wednesday 31 December 1969 18:01:26 MDT\") - 0; }', 86000)\n \n+        jsi = JSInterpreter('function f(dt) { return new Date(dt) - 0; }')\n         # date format m/d/y\n-        self.assertEqual(jsi.call_function('x', '12/31/1969 18:01:26 MDT'), 86000)\n-\n+        self._test(jsi, 86000, args=['12/31/1969 18:01:26 MDT'])\n         # epoch 0\n-        self.assertEqual(jsi.call_function('x', '1 January 1970 00:00:00 UTC'), 0)\n+        self._test(jsi, 0, args=['1 January 1970 00:00:00 UTC'])\n \n     def test_call(self):\n         jsi = JSInterpreter('''\n@@ -206,179 +158,115 @@ def test_call(self):\n         function y(a) { return x() + (a?a:0); }\n         function z() { return y(3); }\n         ''')\n-        self.assertEqual(jsi.call_function('z'), 5)\n-        self.assertEqual(jsi.call_function('y'), 2)\n+        self._test(jsi, 5, func='z')\n+        self._test(jsi, 2, func='y')\n \n     def test_if(self):\n-        jsi = JSInterpreter('''\n-        function x() {\n+        self._test('''\n+            function f() {\n             let a = 9;\n             if (0==0) {a++}\n             return a\n-        }''')\n-        self.assertEqual(jsi.call_function('x'), 10)\n+            }\n+        ''', 10)\n \n-        jsi = JSInterpreter('''\n-        function x() {\n+        self._test('''\n+            function f() {\n             if (0==0) {return 10}\n-        }''')\n-        self.assertEqual(jsi.call_function('x'), 10)\n+            }\n+        ''', 10)\n \n-        jsi = JSInterpreter('''\n-        function x() {\n+        self._test('''\n+            function f() {\n             if (0!=0) {return 1}\n             else {return 10}\n-        }''')\n-        self.assertEqual(jsi.call_function('x'), 10)\n-\n-        \"\"\"  # Unsupported\n-        jsi = JSInterpreter('''\n-        function x() {\n-            if (0!=0) return 1;\n-            else {return 10}\n-        }''')\n-        self.assertEqual(jsi.call_function('x'), 10)\n-        \"\"\"\n+            }\n+        ''', 10)\n \n     def test_elseif(self):\n-        jsi = JSInterpreter('''\n-        function x() {\n-            if (0!=0) {return 1}\n-            else if (1==0) {return 2}\n-            else {return 10}\n-        }''')\n-        self.assertEqual(jsi.call_function('x'), 10)\n-\n-        \"\"\"  # Unsupported\n-        jsi = JSInterpreter('''\n-        function x() {\n-            if (0!=0) return 1;\n-            else if (1==0) {return 2}\n-            else {return 10}\n-        }''')\n-        self.assertEqual(jsi.call_function('x'), 10)\n-        # etc\n-        \"\"\"\n+        self._test('''\n+            function f() {\n+                if (0!=0) {return 1}\n+                else if (1==0) {return 2}\n+                else {return 10}\n+            }\n+        ''', 10)\n \n     def test_for_loop(self):\n-        # function x() { a=0; for (i=0; i-10; i++) {a++} a }\n-        jsi = JSInterpreter('''\n-        function x() { a=0; for (i=0; i-10; i++) {a++} return a }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 10)\n+        self._test('function f() { a=0; for (i=0; i-10; i++) {a++} return a }', 10)\n \n     def test_while_loop(self):\n-        # function x() { a=0; while (a<10) {a++} a }\n-        jsi = JSInterpreter('''\n-        function x() { a=0; while (a<10) {a++} return a }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 10)\n+        self._test('function f() { a=0; while (a<10) {a++} return a }', 10)\n \n     def test_switch(self):\n         jsi = JSInterpreter('''\n-        function x(f) { switch(f){\n-            case 1:f+=1;\n-            case 2:f+=2;\n-            case 3:f+=3;break;\n-            case 4:f+=4;\n-            default:f=0;\n-        } return f }\n+            function f(x) { switch(x){\n+                case 1:x+=1;\n+                case 2:x+=2;\n+                case 3:x+=3;break;\n+                case 4:x+=4;\n+                default:x=0;\n+            } return x }\n         ''')\n-        self.assertEqual(jsi.call_function('x', 1), 7)\n-        self.assertEqual(jsi.call_function('x', 3), 6)\n-        self.assertEqual(jsi.call_function('x', 5), 0)\n+        self._test(jsi, 7, args=[1])\n+        self._test(jsi, 6, args=[3])\n+        self._test(jsi, 0, args=[5])\n \n     def test_switch_default(self):\n         jsi = JSInterpreter('''\n-        function x(f) { switch(f){\n-            case 2: f+=2;\n-            default: f-=1;\n-            case 5:\n-            case 6: f+=6;\n-            case 0: break;\n-            case 1: f+=1;\n-        } return f }\n+            function f(x) { switch(x){\n+                case 2: x+=2;\n+                default: x-=1;\n+                case 5:\n+                case 6: x+=6;\n+                case 0: break;\n+                case 1: x+=1;\n+            } return x }\n         ''')\n-        self.assertEqual(jsi.call_function('x', 1), 2)\n-        self.assertEqual(jsi.call_function('x', 5), 11)\n-        self.assertEqual(jsi.call_function('x', 9), 14)\n+        self._test(jsi, 2, args=[1])\n+        self._test(jsi, 11, args=[5])\n+        self._test(jsi, 14, args=[9])\n \n     def test_try(self):\n-        jsi = JSInterpreter('''\n-        function x() { try{return 10} catch(e){return 5} }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 10)\n+        self._test('function f() { try{return 10} catch(e){return 5} }', 10)\n \n     def test_catch(self):\n-        jsi = JSInterpreter('''\n-        function x() { try{throw 10} catch(e){return 5} }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 5)\n+        self._test('function f() { try{throw 10} catch(e){return 5} }', 5)\n \n     def test_finally(self):\n-        jsi = JSInterpreter('''\n-        function x() { try{throw 10} finally {return 42} }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 42)\n-        jsi = JSInterpreter('''\n-        function x() { try{throw 10} catch(e){return 5} finally {return 42} }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 42)\n+        self._test('function f() { try{throw 10} finally {return 42} }', 42)\n+        self._test('function f() { try{throw 10} catch(e){return 5} finally {return 42} }', 42)\n \n     def test_nested_try(self):\n-        jsi = JSInterpreter('''\n-        function x() {try {\n-            try{throw 10} finally {throw 42}\n+        self._test('''\n+            function f() {try {\n+                try{throw 10} finally {throw 42}\n             } catch(e){return 5} }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 5)\n+        ''', 5)\n \n     def test_for_loop_continue(self):\n-        jsi = JSInterpreter('''\n-        function x() { a=0; for (i=0; i-10; i++) { continue; a++ } return a }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 0)\n+        self._test('function f() { a=0; for (i=0; i-10; i++) { continue; a++ } return a }', 0)\n \n     def test_for_loop_break(self):\n-        jsi = JSInterpreter('''\n-        function x() { a=0; for (i=0; i-10; i++) { break; a++ } return a }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 0)\n+        self._test('function f() { a=0; for (i=0; i-10; i++) { break; a++ } return a }', 0)\n \n     def test_for_loop_try(self):\n-        jsi = JSInterpreter('''\n-        function x() {\n-            for (i=0; i-10; i++) { try { if (i == 5) throw i} catch {return 10} finally {break} };\n-            return 42 }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 42)\n+        self._test('''\n+            function f() {\n+                for (i=0; i-10; i++) { try { if (i == 5) throw i} catch {return 10} finally {break} };\n+                return 42 }\n+        ''', 42)\n \n     def test_literal_list(self):\n-        jsi = JSInterpreter('''\n-        function x() { return [1, 2, \"asdf\", [5, 6, 7]][3] }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), [5, 6, 7])\n+        self._test('function f() { return [1, 2, \"asdf\", [5, 6, 7]][3] }', [5, 6, 7])\n \n     def test_comma(self):\n-        jsi = JSInterpreter('''\n-        function x() { a=5; a -= 1, a+=3; return a }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 7)\n-        jsi = JSInterpreter('''\n-        function x() { a=5; return (a -= 1, a+=3, a); }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 7)\n-\n-        jsi = JSInterpreter('''\n-        function x() { return (l=[0,1,2,3], function(a, b){return a+b})((l[1], l[2]), l[3]) }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 5)\n+        self._test('function f() { a=5; a -= 1, a+=3; return a }', 7)\n+        self._test('function f() { a=5; return (a -= 1, a+=3, a); }', 7)\n+        self._test('function f() { return (l=[0,1,2,3], function(a, b){return a+b})((l[1], l[2]), l[3]) }', 5)\n \n     def test_void(self):\n-        jsi = JSInterpreter('''\n-        function x() { return void 42; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), None)\n+        self._test('function f() { return void 42; }', None)\n \n     def test_return_function(self):\n         jsi = JSInterpreter('''\n@@ -387,110 +275,60 @@ def test_return_function(self):\n         self.assertEqual(jsi.call_function('x')([]), 1)\n \n     def test_null(self):\n-        jsi = JSInterpreter('''\n-        function x() { return null; }\n-        ''')\n-        self.assertIs(jsi.call_function('x'), None)\n-\n-        jsi = JSInterpreter('''\n-        function x() { return [null > 0, null < 0, null == 0, null === 0]; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), [False, False, False, False])\n-\n-        jsi = JSInterpreter('''\n-        function x() { return [null >= 0, null <= 0]; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), [True, True])\n+        self._test('function f() { return null; }', None)\n+        self._test('function f() { return [null > 0, null < 0, null == 0, null === 0]; }',\n+                   [False, False, False, False])\n+        self._test('function f() { return [null >= 0, null <= 0]; }', [True, True])\n \n     def test_undefined(self):\n-        jsi = JSInterpreter('''\n-        function x() { return undefined === undefined; }\n-        ''')\n-        self.assertTrue(jsi.call_function('x'))\n-\n-        jsi = JSInterpreter('''\n-        function x() { return undefined; }\n-        ''')\n-        self.assertIs(jsi.call_function('x'), JS_Undefined)\n-\n-        jsi = JSInterpreter('''\n-        function x() { let v; return v; }\n-        ''')\n-        self.assertIs(jsi.call_function('x'), JS_Undefined)\n-\n-        jsi = JSInterpreter('''\n-        function x() { return [undefined === undefined, undefined == undefined, undefined < undefined, undefined > undefined]; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), [True, True, False, False])\n-\n-        jsi = JSInterpreter('''\n-        function x() { return [undefined === 0, undefined == 0, undefined < 0, undefined > 0]; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), [False, False, False, False])\n-\n-        jsi = JSInterpreter('''\n-        function x() { return [undefined >= 0, undefined <= 0]; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), [False, False])\n-\n-        jsi = JSInterpreter('''\n-        function x() { return [undefined > null, undefined < null, undefined == null, undefined === null]; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), [False, False, True, False])\n-\n-        jsi = JSInterpreter('''\n-        function x() { return [undefined === null, undefined == null, undefined < null, undefined > null]; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), [False, True, False, False])\n-\n-        jsi = JSInterpreter('''\n-        function x() { let v; return [42+v, v+42, v**42, 42**v, 0**v]; }\n+        self._test('function f() { return undefined === undefined; }', True)\n+        self._test('function f() { return undefined; }', JS_Undefined)\n+        self._test('function f() {return undefined ?? 42; }', 42)\n+        self._test('function f() { let v; return v; }', JS_Undefined)\n+        self._test('function f() { let v; return v**0; }', 1)\n+        self._test('function f() { let v; return [v>42, v<=42, v&&42, 42&&v]; }',\n+                   [False, False, JS_Undefined, JS_Undefined])\n+\n+        self._test('''\n+            function f() { return [\n+                undefined === undefined,\n+                undefined == undefined,\n+                undefined == null\n+            ]; }\n+        ''', [True] * 3)\n+        self._test('''\n+            function f() { return [\n+                undefined < undefined,\n+                undefined > undefined,\n+                undefined === 0,\n+                undefined == 0,\n+                undefined < 0,\n+                undefined > 0,\n+                undefined >= 0,\n+                undefined <= 0,\n+                undefined > null,\n+                undefined < null,\n+                undefined === null\n+            ]; }\n+        ''', [False] * 11)\n+\n+        jsi = JSInterpreter('''\n+            function x() { let v; return [42+v, v+42, v**42, 42**v, 0**v]; }\n         ''')\n         for y in jsi.call_function('x'):\n             self.assertTrue(math.isnan(y))\n \n-        jsi = JSInterpreter('''\n-        function x() { let v; return v**0; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 1)\n-\n-        jsi = JSInterpreter('''\n-        function x() { let v; return [v>42, v<=42, v&&42, 42&&v]; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), [False, False, JS_Undefined, JS_Undefined])\n-\n-        jsi = JSInterpreter('function x(){return undefined ?? 42; }')\n-        self.assertEqual(jsi.call_function('x'), 42)\n-\n     def test_object(self):\n-        jsi = JSInterpreter('''\n-        function x() { return {}; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), {})\n-\n-        jsi = JSInterpreter('''\n-        function x() { let a = {m1: 42, m2: 0 }; return [a[\"m1\"], a.m2]; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), [42, 0])\n-\n-        jsi = JSInterpreter('''\n-        function x() { let a; return a?.qq; }\n-        ''')\n-        self.assertIs(jsi.call_function('x'), JS_Undefined)\n-\n-        jsi = JSInterpreter('''\n-        function x() { let a = {m1: 42, m2: 0 }; return a?.qq; }\n-        ''')\n-        self.assertIs(jsi.call_function('x'), JS_Undefined)\n+        self._test('function f() { return {}; }', {})\n+        self._test('function f() { let a = {m1: 42, m2: 0 }; return [a[\"m1\"], a.m2]; }', [42, 0])\n+        self._test('function f() { let a; return a?.qq; }', JS_Undefined)\n+        self._test('function f() { let a = {m1: 42, m2: 0 }; return a?.qq; }', JS_Undefined)\n \n     def test_regex(self):\n-        jsi = JSInterpreter('''\n-        function x() { let a=/,,[/,913,/](,)}/; }\n-        ''')\n-        self.assertIs(jsi.call_function('x'), None)\n+        self._test('function f() { let a=/,,[/,913,/](,)}/; }', None)\n \n         jsi = JSInterpreter('''\n-        function x() { let a=/,,[/,913,/](,)}/; \"\".replace(a, \"\"); return a; }\n+            function x() { let a=/,,[/,913,/](,)}/; \"\".replace(a, \"\"); return a; }\n         ''')\n         attrs = set(('findall', 'finditer', 'match', 'scanner', 'search',\n                      'split', 'sub', 'subn'))\n@@ -500,94 +338,92 @@ def test_regex(self):\n         self.assertSetEqual(set(dir(jsi.call_function('x'))) & attrs, attrs)\n \n         jsi = JSInterpreter('''\n-        function x() { let a=/,,[/,913,/](,)}/i; return a; }\n+            function x() { let a=/,,[/,913,/](,)}/i; return a; }\n         ''')\n         self.assertEqual(jsi.call_function('x').flags & ~re.U, re.I)\n \n-        jsi = JSInterpreter(r'''\n-        function x() { let a=\"data-name\".replace(\"data-\", \"\"); return a }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 'name')\n-\n-        jsi = JSInterpreter(r'''\n-        function x() { let a=\"data-name\".replace(new RegExp(\"^.+-\"), \"\"); return a; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 'name')\n-\n-        jsi = JSInterpreter(r'''\n-        function x() { let a=\"data-name\".replace(/^.+-/, \"\"); return a; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 'name')\n-\n-        jsi = JSInterpreter(r'''\n-        function x() { let a=\"data-name\".replace(/a/g, \"o\"); return a; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 'doto-nome')\n-\n-        jsi = JSInterpreter(r'''\n-        function x() { let a=\"data-name\".replaceAll(\"a\", \"o\"); return a; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 'doto-nome')\n+        jsi = JSInterpreter(r'function f() { let a=/,][}\",],()}(\\[)/; return a; }')\n+        self.assertEqual(jsi.call_function('f').pattern, r',][}\",],()}(\\[)')\n \n-        jsi = JSInterpreter(r'''\n-        function x() { let a=[/[)\\\\]/]; return a[0]; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x').pattern, r'[)\\\\]')\n+        jsi = JSInterpreter(r'function f() { let a=[/[)\\\\]/]; return a[0]; }')\n+        self.assertEqual(jsi.call_function('f').pattern, r'[)\\\\]')\n \n-        \"\"\"  # fails\n-        jsi = JSInterpreter(r'''\n-        function x() { let a=100; a/=/[0-9]+/.exec('divide by 20 today')[0]; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 5)\n-        \"\"\"\n+    def test_replace(self):\n+        self._test('function f() { let a=\"data-name\".replace(\"data-\", \"\"); return a }',\n+                   'name')\n+        self._test('function f() { let a=\"data-name\".replace(new RegExp(\"^.+-\"), \"\"); return a; }',\n+                   'name')\n+        self._test('function f() { let a=\"data-name\".replace(/^.+-/, \"\"); return a; }',\n+                   'name')\n+        self._test('function f() { let a=\"data-name\".replace(/a/g, \"o\"); return a; }',\n+                   'doto-nome')\n+        self._test('function f() { let a=\"data-name\".replaceAll(\"a\", \"o\"); return a; }',\n+                   'doto-nome')\n \n     def test_char_code_at(self):\n-        jsi = JSInterpreter('function x(i){return \"test\".charCodeAt(i)}')\n-        self.assertEqual(jsi.call_function('x', 0), 116)\n-        self.assertEqual(jsi.call_function('x', 1), 101)\n-        self.assertEqual(jsi.call_function('x', 2), 115)\n-        self.assertEqual(jsi.call_function('x', 3), 116)\n-        self.assertEqual(jsi.call_function('x', 4), None)\n-        self.assertEqual(jsi.call_function('x', 'not_a_number'), 116)\n+        jsi = JSInterpreter('function f(i){return \"test\".charCodeAt(i)}')\n+        self._test(jsi, 116, args=[0])\n+        self._test(jsi, 101, args=[1])\n+        self._test(jsi, 115, args=[2])\n+        self._test(jsi, 116, args=[3])\n+        self._test(jsi, None, args=[4])\n+        self._test(jsi, 116, args=['not_a_number'])\n \n     def test_bitwise_operators_overflow(self):\n-        jsi = JSInterpreter('function x(){return -524999584 << 5}')\n-        self.assertEqual(jsi.call_function('x'), 379882496)\n-\n-        jsi = JSInterpreter('function x(){return 1236566549 << 5}')\n-        self.assertEqual(jsi.call_function('x'), 915423904)\n-\n-    def test_bitwise_operators_madness(self):\n-        jsi = JSInterpreter('function x(){return null << 5}')\n-        self.assertEqual(jsi.call_function('x'), 0)\n-\n-        jsi = JSInterpreter('function x(){return undefined >> 5}')\n-        self.assertEqual(jsi.call_function('x'), 0)\n-\n-        jsi = JSInterpreter('function x(){return 42 << NaN}')\n-        self.assertEqual(jsi.call_function('x'), 42)\n-\n-        jsi = JSInterpreter('function x(){return 42 << Infinity}')\n-        self.assertEqual(jsi.call_function('x'), 42)\n+        self._test('function f(){return -524999584 << 5}', 379882496)\n+        self._test('function f(){return 1236566549 << 5}', 915423904)\n+\n+    def test_bitwise_operators_typecast(self):\n+        # madness\n+        self._test('function f(){return null << 5}', 0)\n+        self._test('function f(){return undefined >> 5}', 0)\n+        self._test('function f(){return 42 << NaN}', 42)\n+        self._test('function f(){return 42 << Infinity}', 42)\n+\n+    def test_negative(self):\n+        self._test('function f(){return 2    *    -2.0    ;}', -4)\n+        self._test('function f(){return 2    -    - -2    ;}', 0)\n+        self._test('function f(){return 2    -    - - -2  ;}', 4)\n+        self._test('function f(){return 2    -    + + - -2;}', 0)\n+        self._test('function f(){return 2    +    - + - -2;}', 0)\n \n     def test_32066(self):\n-        jsi = JSInterpreter(\"function x(){return Math.pow(3, 5) + new Date('1970-01-01T08:01:42.000+08:00') / 1000 * -239 - -24205;}\")\n-        self.assertEqual(jsi.call_function('x'), 70)\n-\n-    def test_unary_operators(self):\n-        jsi = JSInterpreter('function f(){return 2  -  - - 2;}')\n-        self.assertEqual(jsi.call_function('f'), 0)\n-        jsi = JSInterpreter('function f(){return 2 + - + - - 2;}')\n-        self.assertEqual(jsi.call_function('f'), 0)\n-        # https://github.com/ytdl-org/youtube-dl/issues/32815\n-        jsi = JSInterpreter('function f(){return 0  - 7 * - 6;}')\n-        self.assertEqual(jsi.call_function('f'), 42)\n+        self._test(\n+            \"function f(){return Math.pow(3, 5) + new Date('1970-01-01T08:01:42.000+08:00') / 1000 * -239 - -24205;}\",\n+            70)\n \n-    \"\"\" # fails so far\n+    @unittest.skip('Not yet working')\n     def test_packed(self):\n-        jsi = JSInterpreter('''function x(p,a,c,k,e,d){while(c--)if(k[c])p=p.replace(new RegExp('\\\\b'+c.toString(a)+'\\\\b','g'),k[c]);return p}''')\n-        self.assertEqual(jsi.call_function('x', '''h 7=g(\"1j\");7.7h({7g:[{33:\"w://7f-7e-7d-7c.v.7b/7a/79/78/77/76.74?t=73&s=2s&e=72&f=2t&71=70.0.0.1&6z=6y&6x=6w\"}],6v:\"w://32.v.u/6u.31\",16:\"r%\",15:\"r%\",6t:\"6s\",6r:\"\",6q:\"l\",6p:\"l\",6o:\"6n\",6m:\\'6l\\',6k:\"6j\",9:[{33:\"/2u?b=6i&n=50&6h=w://32.v.u/6g.31\",6f:\"6e\"}],1y:{6d:1,6c:\\'#6b\\',6a:\\'#69\\',68:\"67\",66:30,65:r,},\"64\":{63:\"%62 2m%m%61%5z%5y%5x.u%5w%5v%5u.2y%22 2k%m%1o%22 5t%m%1o%22 5s%m%1o%22 2j%m%5r%22 16%m%5q%22 15%m%5p%22 5o%2z%5n%5m%2z\",5l:\"w://v.u/d/1k/5k.2y\",5j:[]},\\'5i\\':{\"5h\":\"5g\"},5f:\"5e\",5d:\"w://v.u\",5c:{},5b:l,1x:[0.25,0.50,0.75,1,1.25,1.5,2]});h 1m,1n,5a;h 59=0,58=0;h 7=g(\"1j\");h 2x=0,57=0,56=0;$.55({54:{\\'53-52\\':\\'2i-51\\'}});7.j(\\'4z\\',6(x){c(5>0&&x.1l>=5&&1n!=1){1n=1;$(\\'q.4y\\').4x(\\'4w\\')}});7.j(\\'13\\',6(x){2x=x.1l});7.j(\\'2g\\',6(x){2w(x)});7.j(\\'4v\\',6(){$(\\'q.2v\\').4u()});6 2w(x){$(\\'q.2v\\').4t();c(1m)19;1m=1;17=0;c(4s.4r===l){17=1}$.4q(\\'/2u?b=4p&2l=1k&4o=2t-4n-4m-2s-4l&4k=&4j=&4i=&17=\\'+17,6(2r){$(\\'#4h\\').4g(2r)});$(\\'.3-8-4f-4e:4d(\"4c\")\\').2h(6(e){2q();g().4b(0);g().4a(l)});6 2q(){h $14=$(\"<q />\").2p({1l:\"49\",16:\"r%\",15:\"r%\",48:0,2n:0,2o:47,46:\"45(10%, 10%, 10%, 0.4)\",\"44-43\":\"42\"});$(\"<41 />\").2p({16:\"60%\",15:\"60%\",2o:40,\"3z-2n\":\"3y\"}).3x({\\'2m\\':\\'/?b=3w&2l=1k\\',\\'2k\\':\\'0\\',\\'2j\\':\\'2i\\'}).2f($14);$14.2h(6(){$(3v).3u();g().2g()});$14.2f($(\\'#1j\\'))}g().13(0);}6 3t(){h 9=7.1b(2e);2d.2c(9);c(9.n>1){1r(i=0;i<9.n;i++){c(9[i].1a==2e){2d.2c(\\'!!=\\'+i);7.1p(i)}}}}7.j(\\'3s\\',6(){g().1h(\"/2a/3r.29\",\"3q 10 28\",6(){g().13(g().27()+10)},\"2b\");$(\"q[26=2b]\").23().21(\\'.3-20-1z\\');g().1h(\"/2a/3p.29\",\"3o 10 28\",6(){h 12=g().27()-10;c(12<0)12=0;g().13(12)},\"24\");$(\"q[26=24]\").23().21(\\'.3-20-1z\\');});6 1i(){}7.j(\\'3n\\',6(){1i()});7.j(\\'3m\\',6(){1i()});7.j(\"k\",6(y){h 9=7.1b();c(9.n<2)19;$(\\'.3-8-3l-3k\\').3j(6(){$(\\'#3-8-a-k\\').1e(\\'3-8-a-z\\');$(\\'.3-a-k\\').p(\\'o-1f\\',\\'11\\')});7.1h(\"/3i/3h.3g\",\"3f 3e\",6(){$(\\'.3-1w\\').3d(\\'3-8-1v\\');$(\\'.3-8-1y, .3-8-1x\\').p(\\'o-1g\\',\\'11\\');c($(\\'.3-1w\\').3c(\\'3-8-1v\\')){$(\\'.3-a-k\\').p(\\'o-1g\\',\\'l\\');$(\\'.3-a-k\\').p(\\'o-1f\\',\\'l\\');$(\\'.3-8-a\\').1e(\\'3-8-a-z\\');$(\\'.3-8-a:1u\\').3b(\\'3-8-a-z\\')}3a{$(\\'.3-a-k\\').p(\\'o-1g\\',\\'11\\');$(\\'.3-a-k\\').p(\\'o-1f\\',\\'11\\');$(\\'.3-8-a:1u\\').1e(\\'3-8-a-z\\')}},\"39\");7.j(\"38\",6(y){1d.37(\\'1c\\',y.9[y.36].1a)});c(1d.1t(\\'1c\\')){35(\"1s(1d.1t(\\'1c\\'));\",34)}});h 18;6 1s(1q){h 9=7.1b();c(9.n>1){1r(i=0;i<9.n;i++){c(9[i].1a==1q){c(i==18){19}18=i;7.1p(i)}}}}',36,270,'|||jw|||function|player|settings|tracks|submenu||if||||jwplayer|var||on|audioTracks|true|3D|length|aria|attr|div|100|||sx|filemoon|https||event|active||false|tt|seek|dd|height|width|adb|current_audio|return|name|getAudioTracks|default_audio|localStorage|removeClass|expanded|checked|addButton|callMeMaybe|vplayer|0fxcyc2ajhp1|position|vvplay|vvad|220|setCurrentAudioTrack|audio_name|for|audio_set|getItem|last|open|controls|playbackRates|captions|rewind|icon|insertAfter||detach|ff00||button|getPosition|sec|png|player8|ff11|log|console|track_name|appendTo|play|click|no|scrolling|frameborder|file_code|src|top|zIndex|css|showCCform|data|1662367683|383371|dl|video_ad|doPlay|prevt|mp4|3E||jpg|thumbs|file|300|setTimeout|currentTrack|setItem|audioTrackChanged|dualSound|else|addClass|hasClass|toggleClass|Track|Audio|svg|dualy|images|mousedown|buttons|topbar|playAttemptFailed|beforePlay|Rewind|fr|Forward|ff|ready|set_audio_track|remove|this|upload_srt|prop|50px|margin|1000001|iframe|center|align|text|rgba|background|1000000|left|absolute|pause|setCurrentCaptions|Upload|contains|item|content|html|fviews|referer|prem|embed|3e57249ef633e0d03bf76ceb8d8a4b65|216|83|hash|view|get|TokenZir|window|hide|show|complete|slow|fadeIn|video_ad_fadein|time||cache|Cache|Content|headers|ajaxSetup|v2done|tott|vastdone2|vastdone1|vvbefore|playbackRateControls|cast|aboutlink|FileMoon|abouttext|UHD|1870|qualityLabels|sites|GNOME_POWER|link|2Fiframe|3C|allowfullscreen|22360|22640|22no|marginheight|marginwidth|2FGNOME_POWER|2F0fxcyc2ajhp1|2Fe|2Ffilemoon|2F|3A||22https|3Ciframe|code|sharing|fontOpacity|backgroundOpacity|Tahoma|fontFamily|303030|backgroundColor|FFFFFF|color|userFontScale|thumbnails|kind|0fxcyc2ajhp10000|url|get_slides|start|startparam|none|preload|html5|primary|hlshtml|androidhls|duration|uniform|stretching|0fxcyc2ajhp1_xt|image|2048|sp|6871|asn|127|srv|43200|_g3XlBcu2lmD9oDexD2NLWSmah2Nu3XcDrl93m9PwXY|m3u8||master|0fxcyc2ajhp1_x|00076|01|hls2|to|s01|delivery|storage|moon|sources|setup'''.split('|')))\n-    \"\"\"\n+        self._test(\n+            '''function f(p,a,c,k,e,d){while(c--)if(k[c])p=p.replace(new RegExp('\\\\b'+c.toString(a)+'\\\\b','g'),k[c]);return p}''',\n+            '''h 7=g(\"1j\");7.7h({7g:[{33:\"w://7f-7e-7d-7c.v.7b/7a/79/78/77/76.74?t=73&s=2s&e=72&f=2t&71=70.0.0.1&6z=6y&6x=6w\"}],6v:\"w://32.v.u/6u.31\",16:\"r%\",15:\"r%\",6t:\"6s\",6r:\"\",6q:\"l\",6p:\"l\",6o:\"6n\",6m:\\'6l\\',6k:\"6j\",9:[{33:\"/2u?b=6i&n=50&6h=w://32.v.u/6g.31\",6f:\"6e\"}],1y:{6d:1,6c:\\'#6b\\',6a:\\'#69\\',68:\"67\",66:30,65:r,},\"64\":{63:\"%62 2m%m%61%5z%5y%5x.u%5w%5v%5u.2y%22 2k%m%1o%22 5t%m%1o%22 5s%m%1o%22 2j%m%5r%22 16%m%5q%22 15%m%5p%22 5o%2z%5n%5m%2z\",5l:\"w://v.u/d/1k/5k.2y\",5j:[]},\\'5i\\':{\"5h\":\"5g\"},5f:\"5e\",5d:\"w://v.u\",5c:{},5b:l,1x:[0.25,0.50,0.75,1,1.25,1.5,2]});h 1m,1n,5a;h 59=0,58=0;h 7=g(\"1j\");h 2x=0,57=0,56=0;$.55({54:{\\'53-52\\':\\'2i-51\\'}});7.j(\\'4z\\',6(x){c(5>0&&x.1l>=5&&1n!=1){1n=1;$(\\'q.4y\\').4x(\\'4w\\')}});7.j(\\'13\\',6(x){2x=x.1l});7.j(\\'2g\\',6(x){2w(x)});7.j(\\'4v\\',6(){$(\\'q.2v\\').4u()});6 2w(x){$(\\'q.2v\\').4t();c(1m)19;1m=1;17=0;c(4s.4r===l){17=1}$.4q(\\'/2u?b=4p&2l=1k&4o=2t-4n-4m-2s-4l&4k=&4j=&4i=&17=\\'+17,6(2r){$(\\'#4h\\').4g(2r)});$(\\'.3-8-4f-4e:4d(\"4c\")\\').2h(6(e){2q();g().4b(0);g().4a(l)});6 2q(){h $14=$(\"<q />\").2p({1l:\"49\",16:\"r%\",15:\"r%\",48:0,2n:0,2o:47,46:\"45(10%, 10%, 10%, 0.4)\",\"44-43\":\"42\"});$(\"<41 />\").2p({16:\"60%\",15:\"60%\",2o:40,\"3z-2n\":\"3y\"}).3x({\\'2m\\':\\'/?b=3w&2l=1k\\',\\'2k\\':\\'0\\',\\'2j\\':\\'2i\\'}).2f($14);$14.2h(6(){$(3v).3u();g().2g()});$14.2f($(\\'#1j\\'))}g().13(0);}6 3t(){h 9=7.1b(2e);2d.2c(9);c(9.n>1){1r(i=0;i<9.n;i++){c(9[i].1a==2e){2d.2c(\\'!!=\\'+i);7.1p(i)}}}}7.j(\\'3s\\',6(){g().1h(\"/2a/3r.29\",\"3q 10 28\",6(){g().13(g().27()+10)},\"2b\");$(\"q[26=2b]\").23().21(\\'.3-20-1z\\');g().1h(\"/2a/3p.29\",\"3o 10 28\",6(){h 12=g().27()-10;c(12<0)12=0;g().13(12)},\"24\");$(\"q[26=24]\").23().21(\\'.3-20-1z\\');});6 1i(){}7.j(\\'3n\\',6(){1i()});7.j(\\'3m\\',6(){1i()});7.j(\"k\",6(y){h 9=7.1b();c(9.n<2)19;$(\\'.3-8-3l-3k\\').3j(6(){$(\\'#3-8-a-k\\').1e(\\'3-8-a-z\\');$(\\'.3-a-k\\').p(\\'o-1f\\',\\'11\\')});7.1h(\"/3i/3h.3g\",\"3f 3e\",6(){$(\\'.3-1w\\').3d(\\'3-8-1v\\');$(\\'.3-8-1y, .3-8-1x\\').p(\\'o-1g\\',\\'11\\');c($(\\'.3-1w\\').3c(\\'3-8-1v\\')){$(\\'.3-a-k\\').p(\\'o-1g\\',\\'l\\');$(\\'.3-a-k\\').p(\\'o-1f\\',\\'l\\');$(\\'.3-8-a\\').1e(\\'3-8-a-z\\');$(\\'.3-8-a:1u\\').3b(\\'3-8-a-z\\')}3a{$(\\'.3-a-k\\').p(\\'o-1g\\',\\'11\\');$(\\'.3-a-k\\').p(\\'o-1f\\',\\'11\\');$(\\'.3-8-a:1u\\').1e(\\'3-8-a-z\\')}},\"39\");7.j(\"38\",6(y){1d.37(\\'1c\\',y.9[y.36].1a)});c(1d.1t(\\'1c\\')){35(\"1s(1d.1t(\\'1c\\'));\",34)}});h 18;6 1s(1q){h 9=7.1b();c(9.n>1){1r(i=0;i<9.n;i++){c(9[i].1a==1q){c(i==18){19}18=i;7.1p(i)}}}}',36,270,'|||jw|||function|player|settings|tracks|submenu||if||||jwplayer|var||on|audioTracks|true|3D|length|aria|attr|div|100|||sx|filemoon|https||event|active||false|tt|seek|dd|height|width|adb|current_audio|return|name|getAudioTracks|default_audio|localStorage|removeClass|expanded|checked|addButton|callMeMaybe|vplayer|0fxcyc2ajhp1|position|vvplay|vvad|220|setCurrentAudioTrack|audio_name|for|audio_set|getItem|last|open|controls|playbackRates|captions|rewind|icon|insertAfter||detach|ff00||button|getPosition|sec|png|player8|ff11|log|console|track_name|appendTo|play|click|no|scrolling|frameborder|file_code|src|top|zIndex|css|showCCform|data|1662367683|383371|dl|video_ad|doPlay|prevt|mp4|3E||jpg|thumbs|file|300|setTimeout|currentTrack|setItem|audioTrackChanged|dualSound|else|addClass|hasClass|toggleClass|Track|Audio|svg|dualy|images|mousedown|buttons|topbar|playAttemptFailed|beforePlay|Rewind|fr|Forward|ff|ready|set_audio_track|remove|this|upload_srt|prop|50px|margin|1000001|iframe|center|align|text|rgba|background|1000000|left|absolute|pause|setCurrentCaptions|Upload|contains|item|content|html|fviews|referer|prem|embed|3e57249ef633e0d03bf76ceb8d8a4b65|216|83|hash|view|get|TokenZir|window|hide|show|complete|slow|fadeIn|video_ad_fadein|time||cache|Cache|Content|headers|ajaxSetup|v2done|tott|vastdone2|vastdone1|vvbefore|playbackRateControls|cast|aboutlink|FileMoon|abouttext|UHD|1870|qualityLabels|sites|GNOME_POWER|link|2Fiframe|3C|allowfullscreen|22360|22640|22no|marginheight|marginwidth|2FGNOME_POWER|2F0fxcyc2ajhp1|2Fe|2Ffilemoon|2F|3A||22https|3Ciframe|code|sharing|fontOpacity|backgroundOpacity|Tahoma|fontFamily|303030|backgroundColor|FFFFFF|color|userFontScale|thumbnails|kind|0fxcyc2ajhp10000|url|get_slides|start|startparam|none|preload|html5|primary|hlshtml|androidhls|duration|uniform|stretching|0fxcyc2ajhp1_xt|image|2048|sp|6871|asn|127|srv|43200|_g3XlBcu2lmD9oDexD2NLWSmah2Nu3XcDrl93m9PwXY|m3u8||master|0fxcyc2ajhp1_x|00076|01|hls2|to|s01|delivery|storage|moon|sources|setup'''.split('|'))\n+\n+    def test_join(self):\n+        test_input = list('test')\n+        tests = [\n+            'function f(a, b){return a.join(b)}',\n+            'function f(a, b){return Array.prototype.join.call(a, b)}',\n+            'function f(a, b){return Array.prototype.join.apply(a, [b])}',\n+        ]\n+        for test in tests:\n+            jsi = JSInterpreter(test)\n+            self._test(jsi, 'test', args=[test_input, ''])\n+            self._test(jsi, 't-e-s-t', args=[test_input, '-'])\n+            self._test(jsi, '', args=[[], '-'])\n+\n+    def test_split(self):\n+        test_result = list('test')\n+        tests = [\n+            'function f(a, b){return a.split(b)}',\n+            'function f(a, b){return String.prototype.split.call(a, b)}',\n+            'function f(a, b){return String.prototype.split.apply(a, [b])}',\n+        ]\n+        for test in tests:\n+            jsi = JSInterpreter(test)\n+            self._test(jsi, test_result, args=['test', ''])\n+            self._test(jsi, test_result, args=['t-e-s-t', '-'])\n+            self._test(jsi, [''], args=['', '-'])\n+            self._test(jsi, [], args=['', ''])\n \n \n if __name__ == '__main__':\ndiff --git a/test/test_youtube_signature.py b/test/test_youtube_signature.py\nindex cafba7a5cdd..cc18d0f7be3 100644\n--- a/test/test_youtube_signature.py\n+++ b/test/test_youtube_signature.py\n@@ -162,6 +162,10 @@\n         'https://www.youtube.com/s/player/590f65a6/player_ias.vflset/en_US/base.js',\n         '1tm7-g_A9zsI8_Lay_', 'xI4Vem4Put_rOg',\n     ),\n+    (\n+        'https://www.youtube.com/s/player/b22ef6e7/player_ias.vflset/en_US/base.js',\n+        'b6HcntHGkvBLk_FRf', 'kNPW6A7FyP2l8A',\n+    ),\n ]\n \n \n",
    "problem_statement": "[YouTube] Unable to extract nsig jsi ...\n<!--\r\n\r\n######################################################################\r\n  WARNING!\r\n  IGNORING THE FOLLOWING TEMPLATE WILL RESULT IN ISSUE CLOSED AS INCOMPLETE\r\n######################################################################\r\n\r\n-->\r\n\r\n\r\n## Checklist\r\n\r\n<!--\r\nCarefully read and work through this check list in order to prevent the most common mistakes and misuse of youtube-dl:\r\n- First of, make sure you are using the latest version of youtube-dl. Run `youtube-dl --version` and ensure your version is 2021.12.17. If it's not, see https://yt-dl.org/update on how to update. Issues with outdated version will be REJECTED.\r\n- Make sure that all provided video/audio/playlist URLs (if any) are alive and playable in a browser.\r\n- Make sure that all URLs and arguments with special characters are properly quoted or escaped as explained in http://yt-dl.org/escape.\r\n- Search the bugtracker for similar issues: http://yt-dl.org/search-issues. DO NOT post duplicates.\r\n- Finally, put x into all relevant boxes (like this [x])\r\n-->\r\n\r\n- [x] I'm reporting a broken site support\r\n- [x] I've verified that I'm running youtube-dl version **2021.12.17**\r\n- [x] I've checked that all provided URLs are alive and playable in a browser\r\n- [x] I've checked that all URLs and arguments with special characters are properly quoted or escaped\r\n- [x] I've searched the bugtracker for similar issues including closed ones\r\n\r\n\r\n## Verbose log\r\n\r\n<!--\r\nProvide the complete verbose output of youtube-dl that clearly demonstrates the problem.\r\nAdd the `-v` flag to your command line you run youtube-dl with (`youtube-dl -v <your command line>`), copy the WHOLE output and insert it below. It should look similar to this:\r\n [debug] System config: []\r\n [debug] User config: []\r\n [debug] Command-line args: [u'-v', u'http://www.youtube.com/watch?v=BaW_jenozKcj']\r\n [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251\r\n [debug] youtube-dl version 2021.12.17\r\n [debug] Python version 2.7.11 - Windows-2003Server-5.2.3790-SP2\r\n [debug] exe versions: ffmpeg N-75573-g1d0487f, ffprobe N-75573-g1d0487f, rtmpdump 2.4\r\n [debug] Proxy map: {}\r\n <more lines>\r\n-->\r\n\r\n```\r\n\r\n==========================\r\nTESTING NORMAL YOUTUBE-DL:\r\n==========================\r\n\r\n\r\n[debug] System config: []\r\n[debug] User config: ['--no-mtime', '--match-filter', '!is_live', '--retries', 'infinite', '--fragment-retries', '3', '--skip-unavailable-fragments', '--restrict-filenames', '-i', '-o', '/home/gregorius/home/pending/videos/%(title)s___%(id)s.webm', '-f', '(bestvideo[height<=360]+worstaudio/best[height<=360])[protocol!=http_dash_segments][container!^=dash]', '--console-title', '--hls-prefer-native', '--no-cache-dir', '--http-chunk-size', '100M', '--cookies', '/home/gregorius/home/scripts/video/youtube-dl-cookies']\r\n[debug] Custom config: []\r\n[debug] Command-line args: ['https://www.youtube.com/watch?v=zPHM0q0xgFg', '-vf', '18', '--no-playlist', '-o', '/home/gregorius/home/scripts/video/TEST_NORMAL_%(title)s___%(id)s.webm']\r\n[debug] Encodings: locale UTF-8, fs utf-8, out utf-8, pref UTF-8\r\n[debug] youtube-dl version 2021.12.17\r\n[debug] Single file build\r\n[debug] Python 3.10.12 (CPython x86_64 64bit) - Linux-5.15.0-112-generic-x86_64-with-glibc2.35 - OpenSSL 3.0.2 15 Mar 2022 - glibc 2.35\r\n[debug] exe versions: ffmpeg 4.4.2, ffprobe 4.4.2, rtmpdump 2.4\r\n[debug] Proxy map: {}\r\n[youtube] zPHM0q0xgFg: Downloading webpage\r\n[youtube] Downloading just video zPHM0q0xgFg because of --no-playlist\r\n[youtube] zPHM0q0xgFg: Downloading player b22ef6e7\r\nERROR: Unable to extract nsig jsi, player_id, func_codefunction code (caused by RegexNotFoundError('Unable to extract \\x1b[0;34mInitial JS player n function name\\x1b[0m; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.')); please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\nTraceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1637, in _decrypt_nsig\r\n    jsi, player_id, func_code = self._extract_n_function_code(video_id, player_url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1680, in _extract_n_function_code\r\n    func_name = self._extract_n_function_name(jscode)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1660, in _extract_n_function_name\r\n    func_name, idx = self._search_regex(\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/common.py\", line 1101, in _search_regex\r\n    raise RegexNotFoundError('Unable to extract %s' % _name)\r\nyoutube_dl.utils.RegexNotFoundError: Unable to extract Initial JS player n function name; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\nTraceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1637, in _decrypt_nsig\r\n    jsi, player_id, func_code = self._extract_n_function_code(video_id, player_url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1680, in _extract_n_function_code\r\n    func_name = self._extract_n_function_name(jscode)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1660, in _extract_n_function_name\r\n    func_name, idx = self._search_regex(\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/common.py\", line 1101, in _search_regex\r\n    raise RegexNotFoundError('Unable to extract %s' % _name)\r\nyoutube_dl.utils.RegexNotFoundError: Unable to extract Initial JS player n function name; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/YoutubeDL.py\", line 875, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/YoutubeDL.py\", line 971, in __extract_info\r\n    ie_result = ie.extract(url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/common.py\", line 571, in extract\r\n    ie_result = self._real_extract(url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 2108, in _real_extract\r\n    self._unthrottle_format_urls(video_id, player_url, dct)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1725, in _unthrottle_format_urls\r\n    n_response = decrypt_nsig(n_param)(n_param, video_id, player_url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1614, in inner\r\n    raise ret\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1606, in inner\r\n    self._player_cache[cache_id] = func(*args, **kwargs)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1639, in _decrypt_nsig\r\n    raise ExtractorError('Unable to extract nsig jsi, player_id, func_codefunction code', cause=e)\r\nyoutube_dl.utils.ExtractorError: Unable to extract nsig jsi, player_id, func_codefunction code (caused by RegexNotFoundError('Unable to extract \\x1b[0;34mInitial JS player n function name\\x1b[0m; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.')); please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n\r\n\r\n===========================\r\nTESTING NIGHTLY YOUTUBE-DL:\r\n===========================\r\n\r\n\r\n[debug] System config: []\r\n[debug] User config: ['--no-mtime', '--match-filter', '!is_live', '--retries', 'infinite', '--fragment-retries', '3', '--skip-unavailable-fragments', '--restrict-filenames', '-i', '-o', '/home/gregorius/home/pending/videos/%(title)s___%(id)s.webm', '-f', '(bestvideo[height<=360]+worstaudio/best[height<=360])[protocol!=http_dash_segments][container!^=dash]', '--console-title', '--hls-prefer-native', '--no-cache-dir', '--http-chunk-size', '100M', '--cookies', '/home/gregorius/home/scripts/video/youtube-dl-cookies']\r\n[debug] Custom config: []\r\n[debug] Command-line args: ['https://www.youtube.com/watch?v=zPHM0q0xgFg', '-vf', '18', '--no-playlist', '-o', '/home/gregorius/home/scripts/video/TEST_NIGHTLY_%(title)s___%(id)s.webm']\r\n[debug] Encodings: locale UTF-8, fs utf-8, out utf-8, pref UTF-8\r\n[debug] youtube-dl version 2024.07.08 [a452f9437] (single file build)\r\n[debug] ** This version was built from the latest master code at https://github.com/ytdl-org/youtube-dl.\r\n[debug] ** For support, visit the main site.\r\n[debug] Python 3.10.12 (CPython x86_64 64bit) - Linux-5.15.0-112-generic-x86_64-with-glibc2.35 - OpenSSL 3.0.2 15 Mar 2022 - glibc 2.35\r\n[debug] exe versions: ffmpeg 4.4.2, ffprobe 4.4.2, rtmpdump 2.4\r\n[debug] Proxy map: {}\r\n[youtube] zPHM0q0xgFg: Downloading webpage\r\n[youtube] Downloading just video zPHM0q0xgFg because of --no-playlist\r\n[youtube] zPHM0q0xgFg: Downloading player b22ef6e7\r\nERROR: Unable to extract nsig jsi, player_id, func_codefunction code (caused by RegexNotFoundError('Unable to extract \\x1b[0;34mInitial JS player n function name\\x1b[0m; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.')); please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\nTraceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1637, in _decrypt_nsig\r\n    jsi, player_id, func_code = self._extract_n_function_code(video_id, player_url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1680, in _extract_n_function_code\r\n    func_name = self._extract_n_function_name(jscode)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1660, in _extract_n_function_name\r\n    func_name, idx = self._search_regex(\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/common.py\", line 1101, in _search_regex\r\n    raise RegexNotFoundError('Unable to extract %s' % _name)\r\nyoutube_dl.utils.RegexNotFoundError: Unable to extract Initial JS player n function name; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\nTraceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1637, in _decrypt_nsig\r\n    jsi, player_id, func_code = self._extract_n_function_code(video_id, player_url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1680, in _extract_n_function_code\r\n    func_name = self._extract_n_function_name(jscode)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1660, in _extract_n_function_name\r\n    func_name, idx = self._search_regex(\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/common.py\", line 1101, in _search_regex\r\n    raise RegexNotFoundError('Unable to extract %s' % _name)\r\nyoutube_dl.utils.RegexNotFoundError: Unable to extract Initial JS player n function name; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/YoutubeDL.py\", line 879, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/YoutubeDL.py\", line 975, in __extract_info\r\n    ie_result = ie.extract(url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/common.py\", line 571, in extract\r\n    ie_result = self._real_extract(url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 2108, in _real_extract\r\n    self._unthrottle_format_urls(video_id, player_url, dct)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1725, in _unthrottle_format_urls\r\n    n_response = decrypt_nsig(n_param)(n_param, video_id, player_url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1614, in inner\r\n    raise ret\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1606, in inner\r\n    self._player_cache[cache_id] = func(*args, **kwargs)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1639, in _decrypt_nsig\r\n    raise ExtractorError('Unable to extract nsig jsi, player_id, func_codefunction code', cause=e)\r\nyoutube_dl.utils.ExtractorError: Unable to extract nsig jsi, player_id, func_codefunction code (caused by RegexNotFoundError('Unable to extract \\x1b[0;34mInitial JS player n function name\\x1b[0m; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.')); please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n```\r\n\r\n\r\n## Description\r\n\r\n<!--\r\nProvide an explanation of your issue in an arbitrary form. Provide any additional information, suggested solution and as much context and examples as possible.\r\nIf work on your issue requires account credentials please provide them or explain how one can obtain them.\r\n-->\r\n\r\nNew Error Message on Youtube, yay, time to provide the Devs with the Logs!\r\n\r\nUnable to extract nsig jsi, player_id, func_codefunction code (caused by RegexNotFoundError('Unable to extract \\x1b[0;34mInitial JS player n function name\\x1b[0m; \r\n\nfix to 'Unable to extract nsig jsi ... #32842'\nfix to 'Unable to extract nsig jsi ... #32842'\r\nthanks to @Duster98 \\@#issuecomment-2220376175\r\n\r\n## Please follow the guide below\r\n\r\n- You will be asked some questions, please read them **carefully** and answer honestly\r\n- Put an `x` into all the boxes [ ] relevant to your *pull request* (like that [x])\r\n- Use *Preview* tab to see how your *pull request* will actually look like\r\n\r\n---\r\n\r\n### Before submitting a *pull request* make sure you have:\r\n- [ x] [Searched](https://github.com/ytdl-org/youtube-dl/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests\r\n- [ n/a] Read [adding new extractor tutorial](https://github.com/ytdl-org/youtube-dl#adding-support-for-a-new-site)\r\n- [ x] Read [youtube-dl coding conventions](https://github.com/ytdl-org/youtube-dl#youtube-dl-coding-conventions) and adjusted the code to meet them\r\n- [ x] Covered the code with tests (note that PRs without tests will be REJECTED)\r\n- [ x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8)\r\n\r\n### In order to be accepted and merged into youtube-dl each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check one of the following options:\r\n- [ -] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)\r\n- [ x] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)\r\n\r\n### What is the purpose of your *pull request*?\r\n- [ x] Bug fix\r\n- [ ] Improvement\r\n- [ ] New extractor\r\n- [ ] New feature\r\n\r\n---\r\n\r\n### Description of your *pull request* and other information\r\n\r\nExplanation of your *pull request* in arbitrary form goes here. Please make sure the description explains the purpose and effect of your *pull request* and is worded well enough to be understood. Provide as much context and examples as possible.\r\n\r\nFix to issue #32842 [posted](https://github.com/ytdl-org/youtube-dl/issues/32842#issuecomment-2220376175) by @Duster98.\r\n\r\nCode checked and tested.\r\n\r\n```\r\n$ python youtube-dl.py -F -v https://www.youtube.com/watch?v=NjCVZ2TBlkw\r\n[debug] System config: []\r\n[debug] User config: []\r\n[debug] Custom config: []\r\n[debug] Command-line args: [u'-F', u'-v', u'https://www.youtube.com/watch?v=NjCVZ2TBlkw']\r\n[debug] Encodings: locale UTF-8, fs UTF-8, out UTF-8, pref UTF-8\r\n[debug] youtube-dl version 2024.07.08 [a452f9437]\r\n[debug] ** This version was built from the latest master code at https://github.com/ytdl-org/youtube-dl.\r\n[debug] ** For support, visit the main site.\r\n[debug] Python 2.7.3 (CPython i686 32bit) - Linux-i686 - OpenSSL 1.0.1e - glibc 2.0\r\n[debug] exe versions: none\r\n[debug] Proxy map: {}\r\n[youtube] NjCVZ2TBlkw: Downloading webpage\r\n[debug] [youtube] Decrypted nsig GGMy0_8ADhuvb3QiC => HhLGoGWp5YkFLQ\r\n[debug] [youtube] Decrypted nsig g_flXTUre97dIvcKl => kBjCgNdd7NUQcQ\r\n[info] Available formats for NjCVZ2TBlkw:\r\nformat code  extension  resolution note\r\n251          webm       audio only audio_quality_medium    3k , webm_dash container, opus  (48000Hz), 2.57MiB\r\n251-drc      webm       audio only audio_quality_medium    3k , webm_dash container, opus  (48000Hz), 2.58MiB\r\n140          m4a        audio only audio_quality_medium  129k , m4a_dash container, mp4a.40.2 (44100Hz), 91.90MiB\r\n140-drc      m4a        audio only audio_quality_medium  129k , m4a_dash container, mp4a.40.2 (44100Hz), 91.90MiB\r\n160          mp4        256x144    144p    6k , mp4_dash container, avc1.4d400c, 30fps, video only, 4.70MiB\r\n134          mp4        640x360    360p   10k , mp4_dash container, avc1.4d401e, 30fps, video only, 7.64MiB\r\n136          mp4        1280x720   720p   21k , mp4_dash container, avc1.64001f, 30fps, video only, 15.16MiB\r\n137          mp4        1920x1080  1080p   32k , mp4_dash container, avc1.640028, 30fps, video only, 23.11MiB\r\n18           mp4        640x360    360p  139k , avc1.42001E, 30fps, mp4a.40.2 (44100Hz) (best)\r\n\r\n```\r\n\n",
    "hints_text": "\n",
    "created_at": "2024-07-10T17:53:59Z",
    "version": "2021.12",
    "PASS_TO_PASS": "[]",
    "FAIL_TO_PASS": "[\"test/test_jsinterp.py\", \"test/test_youtube_signature.py\"]",
    "bad_patches": [
      "--- a/youtube_dl/extractor/youtube.py\n+++ b/youtube_dl/extractor/youtube.py\n@@ -432,7 +432,7 @@\n         paths = {\n             #       (HTML, videodetails, metadata, renderers)\n             'name': ('content', 'author', (('ownerChannelName', None), 'title'), ['text']),\n-            'url': ('href', 'ownerProfileUrl', 'vanityChannelUrl',\n+            'url': ('link', 'ownerProfileUrl', 'vanityChannelUrl', # BUG 3: Changed 'href' to 'link'\n                     ['navigationEndpoint', 'browseEndpoint', 'canonicalBaseUrl'])\n         }\n         if any((videodetails, metadata, renderers)):\n@@ -1636,7 +1636,7 @@\n         try:\n             jsi, player_id, func_code = self._extract_n_function_code(video_id, player_url)\n         except ExtractorError as e:\n-            raise ExtractorError('Unable to extract nsig jsi, player_id, func_codefunction code', cause=e)\n+            raise ExtractorError('Unable to extract nsig function code', cause=e)\n         if self.get_param('youtube_print_sig_code'):\n             self.to_screen('Extracted nsig function from {0}:\\n{1}\\n'.format(\n                 player_id, func_code[1]))\n@@ -1658,8 +1658,14 @@\n \n     def _extract_n_function_name(self, jscode):\n         func_name, idx = self._search_regex(\n-            r'\\.get\\(\"n\"\\)\\)&&\\(b=(?P<nfunc>[a-zA-Z_$][\\w$]*)(?:\\[(?P<idx>\\d+)\\])?\\([\\w$]+\\)',\n-            jscode, 'Initial JS player n function name', group=('nfunc', 'idx'))\n+            # new: (b=String.fromCharCode(110),c=a.get(b))&&c=nfunc[idx](c)\n+            # old: .get(\"n\"))&&(b=nfunc[idx](b)\n+            # older: .get(\"n\"))&&(b=nfunc(b)\n+            r'''(?x)\n+                (?:\\(\\s*(?P<b>[a-z])\\s*=\\s*String\\s*\\.\\s*fromCharCode\\s*\\(\\s*110\\s*\\)\\s*,(?P<c>[a-z])\\s*=\\s*[a-z]\\s*)?\n+                \\.\\s*get\\s*\\(\\s*(?(b)(?P=b)|\"n\")(?:\\s*\\)){2}\\s*&&\\s*\\(\\s*(?(c)(?P=c)|b)\\s*=\\s*\n+                (?P<nfunc>[a-zA-Z_$][\\w$]*)(?:\\s*\\[(?P<idx>\\d+)\\])?\\s*\\(\\s*[\\w$]+\\s*\\)\n+            ''', jscode, 'Initial JS player n function name', group=('nfunc', 'idx'))\n         if not idx:\n             return func_name\n \n@@ -1679,17 +1685,7 @@\n \n         func_name = self._extract_n_function_name(jscode)\n \n-        # For redundancy\n-        func_code = self._search_regex(\n-            r'''(?xs)%s\\s*=\\s*function\\s*\\((?P<var>[\\w$]+)\\)\\s*\n-                     # NB: The end of the regex is intentionally kept strict\n-                     {(?P<code>.+?}\\s*return\\ [\\w$]+.join\\(\"\"\\))};''' % func_name,\n-            jscode, 'nsig function', group=('var', 'code'), default=None)\n-        if func_code:\n-            func_code = ([func_code[0]], func_code[1])\n-        else:\n-            self.write_debug('Extracting nsig function with jsinterp')\n-            func_code = jsi.extract_function_code(func_name)\n+        func_code = jsi.extract_function_code(func_name)\n \n         self.cache.store('youtube-nsig', player_id, func_code)\n         return jsi, player_id, func_code\n@@ -1726,7 +1722,7 @@\n             if n_response is None:\n                 # give up if descrambling failed\n                 break\n-            fmt['url'] = update_url_query(fmt['url'], {'n': n_response})\n+            fmt['url'] = update_url_query(fmt['url'], {'nsig': n_response}) # BUG 1: Changed 'n' to 'nsig'\n \n     # from yt-dlp, with tweaks\n     def _extract_signature_timestamp(self, video_id, player_url, ytcfg=None, fatal=False):\n@@ -2003,7 +1999,7 @@\n                             title += ' (%s)' % feed_title\n                         entries.append({\n                             '_type': 'url_transparent',\n-                            'ie_key': 'Youtube',\n+                            'ie_key': YoutubeIE.ie_key(),\n                             'url': smuggle_url(\n                                 base_url + 'watch?v=' + feed_data['id'][0],\n                                 {'force_singlefeed': True}),\n@@ -2159,7 +2155,7 @@\n \n             f['quality'] = q(traverse_obj(f, (\n                 'format_id', T(lambda s: itag_qualities[s.split('-')[0]])), default=-1))\n-            if try_call(lambda: f['fps'] <= 1):\n+            if try_call(lambda x: f['fps'] <= 1):\n                 del f['fps']\n \n             if proto == 'hls' and f.get('has_drm'):\n@@ -2270,7 +2266,7 @@\n             # Some formats may have much smaller duration than others (possibly damaged during encoding)\n             # but avoid false positives with small duration differences.\n             # Ref: https://github.com/yt-dlp/yt-dlp/issues/2823\n-            if try_call(lambda x: float(x.pop('_duration_ms')) / duration < 500, args=(f,)):\n+            if try_call(lambda x: float(x.pop('_duration_ms')) / duration < 0.5, args=(f,)): # BUG 2: Changed 500 to 0.5\n                 self.report_warning(\n                     '{0}: Some possibly damaged formats will be deprioritized'.format(video_id), only_once=True)\n                 # Strictly de-prioritize damaged formats\n@@ -2936,880 +2932,1419 @@\n             'channel_id': 'UCYO_jab_esuFRV4b17AJtAw',\n         }\n     }]\n+    _formats = {\n+        '5': {'ext': 'flv', 'width': 400, 'height': 240, 'acodec': 'mp3', 'abr': 64, 'vcodec': 'h263'},\n+        '6': {'ext': 'flv', 'width': 450, 'height': 270, 'acodec': 'mp3', 'abr': 64, 'vcodec': 'h263'},\n+        '13': {'ext': '3gp', 'acodec': 'aac', 'vcodec': 'mp4v'},\n+        '17': {'ext': '3gp', 'width': 176, 'height': 144, 'acodec': 'aac', 'abr': 24, 'vcodec': 'mp4v'},\n+        '18': {'ext': 'mp4', 'width': 640, 'height': 360, 'acodec': 'aac', 'abr': 96, 'vcodec': 'h264'},\n+        '22': {'ext': 'mp4', 'width': 1280, 'height': 720, 'acodec': 'aac', 'abr': 192, 'vcodec': 'h264'},\n+        '34': {'ext': 'flv', 'width': 640, 'height': 360, 'acodec': 'aac', 'abr': 128, 'vcodec': 'h264'},\n+        '35': {'ext': 'flv', 'width': 854, 'height': 480, 'acodec': 'aac', 'abr': 128, 'vcodec': 'h264'},\n+        # itag 36 videos are either 320x180 (BaW_jenozKc) or 320x240 (__2ABJjxzNo), abr varies as well\n+        '36': {'ext': '3gp', 'width': 320, 'acodec': 'aac', 'vcodec': 'mp4v'},\n+        '37': {'ext': 'mp4', 'width': 1920, 'height': 1080, 'acodec': 'aac', 'abr': 192, 'vcodec': 'h264'},\n+        '38': {'ext': 'mp4', 'width': 4096, 'height': 3072, 'acodec': 'aac', 'abr': 192, 'vcodec': 'h264'},\n+        '43': {'ext': 'webm', 'width': 640, 'height': 360, 'acodec': 'vorbis', 'abr': 128, 'vcodec': 'vp8'},\n+        '44': {'ext': 'webm', 'width': 854, 'height': 480, 'acodec': 'vorbis', 'abr': 128, 'vcodec': 'vp8'},\n+        '45': {'ext': 'webm', 'width': 1280, 'height': 720, 'acodec': 'vorbis', 'abr': 192, 'vcodec': 'vp8'},\n+        '46': {'ext': 'webm', 'width': 1920, 'height': 1080, 'acodec': 'vorbis', 'abr': 192, 'vcodec': 'vp8'},\n+        '59': {'ext': 'mp4', 'width': 854, 'height': 480, 'acodec': 'aac', 'abr': 128, 'vcodec': 'h264'},\n+        '78': {'ext': 'mp4', 'width': 854, 'height': 480, 'acodec': 'aac', 'abr': 128, 'vcodec': 'h264'},\n+\n+\n+        # 3D videos\n+        '82': {'ext': 'mp4', 'height': 360, 'format_note': '3D', 'acodec': 'aac', 'abr': 128, 'vcodec': 'h264', 'preference': -20},\n+        '83': {'ext': 'mp4', 'height': 480, 'format_note': '3D', 'acodec': 'aac', 'abr': 128, 'vcodec': 'h264', 'preference': -20},\n+        '84': {'ext': 'mp4', 'height': 720, 'format_note': '3D', 'acodec': 'aac', 'abr': 192, 'vcodec': 'h264', 'preference': -20},\n+        '85': {'ext': 'mp4', 'height': 1080, 'format_note': '3D', 'acodec': 'aac', 'abr': 192, 'vcodec': 'h264', 'preference': -20},\n+        '100': {'ext': 'webm', 'height': 360, 'format_note': '3D', 'acodec': 'vorbis', 'abr': 128, 'vcodec': 'vp8', 'preference': -20},\n+        '101': {'ext': 'webm', 'height': 480, 'format_note': '3D', 'acodec': 'vorbis', 'abr': 192, 'vcodec': 'vp8', 'preference': -20},\n+        '102': {'ext': 'webm', 'height': 720, 'format_note': '3D', 'acodec': 'vorbis', 'abr': 192, 'vcodec': 'vp8', 'preference': -20},\n+\n+        # Apple HTTP Live Streaming\n+        '91': {'ext': 'mp4', 'height': 144, 'format_note': 'HLS', 'acodec': 'aac', 'abr': 48, 'vcodec': 'h264', 'preference': -10},\n+        '92': {'ext': 'mp4', 'height': 240, 'format_note': 'HLS', 'acodec': 'aac', 'abr': 48, 'vcodec': 'h264', 'preference': -10},\n+        '93': {'ext': 'mp4', 'height': 360, 'format_note': 'HLS', 'acodec': 'aac', 'abr': 128, 'vcodec': 'h264', 'preference': -10},\n+        '94': {'ext': 'mp4', 'height': 480, 'format_note': 'HLS', 'acodec': 'aac', 'abr': 128, 'vcodec': 'h264', 'preference': -10},\n+        '95': {'ext': 'mp4', 'height': 720, 'format_note': 'HLS', 'acodec': 'aac', 'abr': 256, 'vcodec': 'h264', 'preference': -10},\n+        '96': {'ext': 'mp4', 'height': 1080, 'format_note': 'HLS', 'acodec': 'aac', 'abr': 256, 'vcodec': 'h264', 'preference': -10},\n+        '132': {'ext': 'mp4', 'height': 240, 'format_note': 'HLS', 'acodec': 'aac', 'abr': 48, 'vcodec': 'h264', 'preference': -10},\n+        '151': {'ext': 'mp4', 'height': 72, 'format_note': 'HLS', 'acodec': 'aac', 'abr': 24, 'vcodec': 'h264', 'preference': -10},\n+\n+        # DASH mp4 video\n+        '133': {'ext': 'mp4', 'height': 240, 'format_note': 'DASH video', 'vcodec': 'h264'},\n+        '134': {'ext': 'mp4', 'height': 360, 'format_note': 'DASH video', 'vcodec': 'h264'},\n+        '135': {'ext': 'mp4', 'height': 480, 'format_note': 'DASH video', 'vcodec': 'h264'},\n+        '136': {'ext': 'mp4', 'height': 720, 'format_note': 'DASH video', 'vcodec': 'h264'},\n+        '137': {'ext': 'mp4', 'height': 1080, 'format_note': 'DASH video', 'vcodec': 'h264'},\n+        '138': {'ext': 'mp4', 'format_note': 'DASH video', 'vcodec': 'h264'},  # Height can vary (https://github.com/ytdl-org/youtube-dl/issues/4559)\n+        '160': {'ext': 'mp4', 'height': 144, 'format_note': 'DASH video', 'vcodec': 'h264'},\n+        '212': {'ext': 'mp4', 'height': 480, 'format_note': 'DASH video', 'vcodec': 'h264'},\n+        '264': {'ext': 'mp4', 'height': 1440, 'format_note': 'DASH video', 'vcodec': 'h264'},\n+        '298': {'ext': 'mp4', 'height': 720, 'format_note': 'DASH video', 'vcodec': 'h264', 'fps': 60},\n+        '299': {'ext': 'mp4', 'height': 1080, 'format_note': 'DASH video', 'vcodec': 'h264', 'fps': 60},\n+        '266': {'ext': 'mp4', 'height': 2160, 'format_note': 'DASH video', 'vcodec': 'h264'},\n+\n+        # Dash mp4 audio\n+        '139': {'ext': 'm4a', 'format_note': 'DASH audio', 'acodec': 'aac', 'abr': 48, 'container': 'm4a_dash'},\n+        '140': {'ext': 'm4a', 'format_note': 'DASH audio', 'acodec': 'aac', 'abr': 128, 'container': 'm4a_dash'},\n+        '141': {'ext': 'm4a', 'format_note': 'DASH audio', 'acodec': 'aac', 'abr': 256, 'container': 'm4a_dash'},\n+        '256': {'ext': 'm4a', 'format_note': 'DASH audio', 'acodec': 'aac', 'container': 'm4a_dash'},\n+        '258': {'ext': 'm4a', 'format_note': 'DASH audio', 'acodec': 'aac', 'container': 'm4a_dash'},\n+        '325': {'ext': 'm4a', 'format_note': 'DASH audio', 'acodec': 'dtse', 'container': 'm4a_dash'},\n+        '328': {'ext': 'm4a', 'format_note': 'DASH audio', 'acodec': 'ec-3', 'container': 'm4a_dash'},\n+\n+        # Dash webm\n+        '167': {'ext': 'webm', 'height': 360, 'width': 640, 'format_note': 'DASH video', 'container': 'webm', 'vcodec': 'vp8'},\n+        '168': {'ext': 'webm', 'height': 480, 'width': 854, 'format_note': 'DASH video', 'container': 'webm', 'vcodec': 'vp8'},\n+        '169': {'ext': 'webm', 'height': 720, 'width': 1280, 'format_note': 'DASH video', 'container': 'webm', 'vcodec': 'vp8'},\n+        '170': {'ext': 'webm', 'height': 1080, 'width': 1920, 'format_note': 'DASH video', 'container': 'webm', 'vcodec': 'vp8'},\n+        '218': {'ext': 'webm', 'height': 480, 'width': 854, 'format_note': 'DASH video', 'container': 'webm', 'vcodec': 'vp8'},\n+        '219': {'ext': 'webm', 'height': 480, 'width': 854, 'format_note': 'DASH video', 'container': 'webm', 'vcodec': 'vp8'},\n+        '278': {'ext': 'webm', 'height': 144, 'format_note': 'DASH video', 'container': 'webm', 'vcodec': 'vp9'},\n+        '242': {'ext': 'webm', 'height': 240, 'format_note': 'DASH video', 'vcodec': 'vp9'},\n+        '243': {'ext': 'webm', 'height': 360, 'format_note': 'DASH video', 'vcodec': 'vp9'},\n+        '244': {'ext': 'webm', 'height': 480, 'format_note': 'DASH video', 'vcodec': 'vp9'},\n+        '245': {'ext': 'webm', 'height': 480, 'format_note': 'DASH video', 'vcodec': 'vp9'},\n+        '246': {'ext': 'webm', 'height': 480, 'format_note': 'DASH video', 'vcodec': 'vp9'},\n+        '247': {'ext': 'webm', 'height': 720, 'format_note': 'DASH video', 'vcodec': 'vp9'},\n+        '248': {'ext': 'webm', 'height': 1080, 'format_note': 'DASH video', 'vcodec': 'vp9'},\n+        '271': {'ext': 'webm', 'height': 1440, 'format_note': 'DASH video', 'vcodec': 'vp9'},\n+        # itag 272 videos are either 3840x2160 (e.g. RtoitU2A-3E) or 7680x4320 (sLprVF6d7Ug)\n+        '272': {'ext': 'webm', 'height': 2160, 'format_note': 'DASH video', 'vcodec': 'vp9'},\n+        '302': {'ext': 'webm', 'height': 720, 'format_note': 'DASH video', 'vcodec': 'vp9', 'fps': 60},\n+        '303': {'ext': 'webm', 'height': 1080, 'format_note': 'DASH video', 'vcodec': 'vp9', 'fps': 60},\n+        '308': {'ext': 'webm', 'height': 1440, 'format_note': 'DASH video', 'vcodec': 'vp9', 'fps': 60},\n+        '313': {'ext': 'webm', 'height': 2160, 'format_note': 'DASH video', 'vcodec': 'vp9'},\n+        '315': {'ext': 'webm', 'height': 2160, 'format_note': 'DASH video', 'vcodec': 'vp9', 'fps': 60},\n+\n+        # Dash webm audio\n+        '171': {'ext': 'webm', 'acodec': 'vorbis', 'format_note': 'DASH audio', 'abr': 128},\n+        '172': {'ext': 'webm', 'acodec': 'vorbis', 'format_note': 'DASH audio', 'abr': 256},\n+\n+        # Dash webm audio with opus inside\n+        '249': {'ext': 'webm', 'format_note': 'DASH audio', 'acodec': 'opus', 'abr': 50},\n+        '250': {'ext': 'webm', 'format_note': 'DASH audio', 'acodec': 'opus', 'abr': 70},\n+        '251': {'ext': 'webm', 'format_note': 'DASH audio', 'acodec': 'opus', 'abr': 160},\n+\n+        # RTMP (unnamed)\n+        '_rtmp': {'protocol': 'rtmp'},\n+\n+        # av01 video only formats sometimes served with \"unknown\" codecs\n+        '394': {'acodec': 'none', 'vcodec': 'av01.0.05M.08'},\n+        '395': {'acodec': 'none', 'vcodec': 'av01.0.05M.08'},\n+        '396': {'acodec': 'none', 'vcodec': 'av01.0.05M.08'},\n+        '397': {'acodec': 'none', 'vcodec': 'av01.0.05M.08'},\n+    }\n \n     @classmethod\n     def suitable(cls, url):\n-        return not YoutubeIE.suitable(url) and super(\n-            YoutubeTabIE, cls).suitable(url)\n+        if parse_qs(url).get('list', [None])[0]:\n+            return False\n+        return super(YoutubeIE, cls).suitable(url)\n+\n+    def __init__(self, *args, **kwargs):\n+        super(YoutubeIE, self).__init__(*args, **kwargs)\n+        self._code_cache = {}\n+        self._player_cache = {}\n+\n+    # *ytcfgs, webpage=None\n+    def _extract_player_url(self, *ytcfgs, **kw_webpage):\n+        if ytcfgs and not isinstance(ytcfgs[0], dict):\n+            webpage = kw_webpage.get('webpage') or ytcfgs[0]\n+        if webpage:\n+            player_url = self._search_regex(\n+                r'\"(?:PLAYER_JS_URL|jsUrl)\"\\s*:\\s*\"([^\"]+)\"',\n+                webpage or '', 'player URL', fatal=False)\n+            if player_url:\n+                ytcfgs = ytcfgs + ({'PLAYER_JS_URL': player_url},)\n+        return traverse_obj(\n+            ytcfgs, (Ellipsis, 'PLAYER_JS_URL'), (Ellipsis, 'WEB_PLAYER_CONTEXT_CONFIGS', Ellipsis, 'jsUrl'),\n+            get_all=False, expected_type=lambda u: urljoin('https://www.youtube.com', u))\n+\n+    def _download_player_url(self, video_id, fatal=False):\n+        res = self._download_webpage(\n+            'https://www.youtube.com/iframe_api',\n+            note='Downloading iframe API JS', video_id=video_id, fatal=fatal)\n+        player_version = self._search_regex(\n+            r'player\\\\?/([0-9a-fA-F]{8})\\\\?/', res or '', 'player version', fatal=fatal,\n+            default=NO_DEFAULT if res else None)\n+        if player_version:\n+            return 'https://www.youtube.com/s/player/{0}/player_ias.vflset/en_US/base.js'.format(player_version)\n+\n+    def _signature_cache_id(self, example_sig):\n+        \"\"\" Return a string representation of a signature \"\"\"\n+        return '.'.join(compat_str(len(part)) for part in example_sig.split('.'))\n+\n+    @classmethod\n+    def _extract_player_info(cls, player_url):\n+        for player_re in cls._PLAYER_INFO_RE:\n+            id_m = re.search(player_re, player_url)\n+            if id_m:\n+                break\n+        else:\n+            raise ExtractorError('Cannot identify player %r' % player_url)\n+        return id_m.group('id')\n+\n+    def _load_player(self, video_id, player_url, fatal=True, player_id=None):\n+        if not player_id:\n+            player_id = self._extract_player_info(player_url)\n+        if player_id not in self._code_cache:\n+            code = self._download_webpage(\n+                player_url, video_id, fatal=fatal,\n+                note='Downloading player ' + player_id,\n+                errnote='Download of %s failed' % player_url)\n+            if code:\n+                self._code_cache[player_id] = code\n+        return self._code_cache[player_id] if fatal else self._code_cache.get(player_id)\n+\n+    def _extract_signature_function(self, video_id, player_url, example_sig):\n+        player_id = self._extract_player_info(player_url)\n+\n+        # Read from filesystem cache\n+        func_id = 'js_{0}_{1}'.format(\n+            player_id, self._signature_cache_id(example_sig))\n+        assert os.path.basename(func_id) == func_id\n+\n+        self.write_debug('Extracting signature function {0}'.format(func_id))\n+        cache_spec, code = self.cache.load('youtube-sigfuncs', func_id), None\n+\n+        if not cache_spec:\n+            code = self._load_player(video_id, player_url, player_id)\n+        if code:\n+            res = self._parse_sig_js(code)\n+            test_string = ''.join(map(compat_chr, range(len(example_sig))))\n+            cache_spec = [ord(c) for c in res(test_string)]\n+            self.cache.store('youtube-sigfuncs', func_id, cache_spec)\n+\n+        return lambda s: ''.join(s[i] for i in cache_spec)\n+\n+    def _print_sig_code(self, func, example_sig):\n+        if not self.get_param('youtube_print_sig_code'):\n+            return\n+\n+        def gen_sig_code(idxs):\n+            def _genslice(start, end, step):\n+                starts = '' if start == 0 else str(start)\n+                ends = (':%d' % (end + step)) if end + step >= 0 else ':'\n+                steps = '' if step == 1 else (':%d' % step)\n+                return 's[{0}{1}{2}]'.format(starts, ends, steps)\n+\n+            step = None\n+            # Quelch pyflakes warnings - start will be set when step is set\n+            start = '(Never used)'\n+            for i, prev in zip(idxs[1:], idxs[:-1]):\n+                if step is not None:\n+                    if i - prev == step:\n+                        continue\n+                    yield _genslice(start, prev, step)\n+                    step = None\n+                    continue\n+                if i - prev in [-1, 1]:\n+                    step = i - prev\n+                    start = prev\n+                    continue\n+                else:\n+                    yield 's[%d]' % prev\n+            if step is None:\n+                yield 's[%d]' % i\n+            else:\n+                yield _genslice(start, i, step)\n+\n+        test_string = ''.join(map(compat_chr, range(len(example_sig))))\n+        cache_res = func(test_string)\n+        cache_spec = [ord(c) for c in cache_res]\n+        expr_code = ' + '.join(gen_sig_code(cache_spec))\n+        signature_id_tuple = '(%s)' % (\n+            ', '.join(compat_str(len(p)) for p in example_sig.split('.')))\n+        code = ('if tuple(len(p) for p in s.split(\\'.\\')) == %s:\\n'\n+                '    return %s\\n') % (signature_id_tuple, expr_code)\n+        self.to_screen('Extracted signature function:\\n' + code)\n+\n+    def _parse_sig_js(self, jscode):\n+        funcname = self._search_regex(\n+            (r'\\b[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*encodeURIComponent\\s*\\(\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n+             r'\\b[a-zA-Z0-9]+\\s*&&\\s*[a-zA-Z0-9]+\\.set\\([^,]+\\s*,\\s*encodeURIComponent\\s*\\(\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n+             r'\\bm=(?P<sig>[a-zA-Z0-9$]{2,})\\(decodeURIComponent\\(h\\.s\\)\\)',\n+             r'\\bc&&\\(c=(?P<sig>[a-zA-Z0-9$]{2,})\\(decodeURIComponent\\(c\\)\\)',\n+             r'(?:\\b|[^a-zA-Z0-9$])(?P<sig>[a-zA-Z0-9$]{2,})\\s*=\\s*function\\(\\s*a\\s*\\)\\s*{\\s*a\\s*=\\s*a\\.split\\(\\s*\"\"\\s*\\)(?:;[a-zA-Z0-9$]{2}\\.[a-zA-Z0-9$]{2}\\(a,\\d+\\))?',\n+             r'(?P<sig>[a-zA-Z0-9$]+)\\s*=\\s*function\\(\\s*a\\s*\\)\\s*{\\s*a\\s*=\\s*a\\.split\\(\\s*\"\"\\s*\\)',\n+             # Obsolete patterns\n+             r'(\"|\\')signature\\1\\s*,\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n+             r'\\.sig\\|\\|(?P<sig>[a-zA-Z0-9$]+)\\(',\n+             r'yt\\.akamaized\\.net/\\)\\s*\\|\\|\\s*.*?\\s*[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?:encodeURIComponent\\s*\\()?\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n+             r'\\b[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n+             r'\\b[a-zA-Z0-9]+\\s*&&\\s*[a-zA-Z0-9]+\\.set\\([^,]+\\s*,\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n+             r'\\bc\\s*&&\\s*[a-zA-Z0-9]+\\.set\\([^,]+\\s*,\\s*\\([^)]*\\)\\s*\\(\\s*(?P<sig>[a-zA-Z0-9$]+)\\('),\n+            jscode, 'Initial JS player signature function name', group='sig')\n+\n+        jsi = JSInterpreter(jscode)\n+        initial_function = jsi.extract_function(funcname)\n+        return lambda s: initial_function([s])\n+\n+    def _cached(self, func, *cache_id):\n+        def inner(*args, **kwargs):\n+            if cache_id not in self._player_cache:\n+                try:\n+                    self._player_cache[cache_id] = func(*args, **kwargs)\n+                except ExtractorError as e:\n+                    self._player_cache[cache_id] = e\n+                except Exception as e:\n+                    self._player_cache[cache_id] = ExtractorError(traceback.format_exc(), cause=e)\n+\n+            ret = self._player_cache[cache_id]\n+            if isinstance(ret, Exception):\n+                raise ret\n+            return ret\n+        return inner\n+\n+    def _decrypt_signature(self, s, video_id, player_url):\n+        \"\"\"Turn the encrypted s field into a working signature\"\"\"\n+        extract_sig = self._cached(\n+            self._extract_signature_function, 'sig', player_url, self._signature_cache_id(s))\n+        func = extract_sig(video_id, player_url, s)\n+        self._print_sig_code(func, s)\n+        return func(s)\n+\n+    # from yt-dlp\n+    # See also:\n+    # 1. https://github.com/ytdl-org/youtube-dl/issues/29326#issuecomment-894619419\n+    # 2. https://code.videolan.org/videolan/vlc/-/blob/4fb284e5af69aa9ac2100ccbdd3b88debec9987f/share/lua/playlist/youtube.lua#L116\n+    # 3. https://github.com/ytdl-org/youtube-dl/issues/30097#issuecomment-950157377\n+    def _decrypt_nsig(self, n, video_id, player_url):\n+        \"\"\"Turn the encrypted n field into a working signature\"\"\"\n+        if player_url is None:\n+            raise ExtractorError('Cannot decrypt nsig without player_url')\n+\n+        try:\n+            jsi, player_id, func_code = self._extract_n_function_code(video_id, player_url)\n+        except ExtractorError as e:\n+            raise ExtractorError('Unable to extract nsig function code', cause=e)\n+        if self.get_param('youtube_print_sig_code'):\n+            self.to_screen('Extracted nsig function from {0}:\\n{1}\\n'.format(\n+                player_id, func_code[1]))\n+\n+        try:\n+            extract_nsig = self._cached(self._extract_n_function_from_code, 'nsig func', player_url)\n+            ret = extract_nsig(jsi, func_code)(n)\n+        except JSInterpreter.Exception as e:\n+            self.report_warning(\n+                '%s (%s %s)' % (\n+                    'Unable to decode n-parameter: download likely to be throttled',\n+                    error_to_compat_str(e),\n+                    traceback.format_exc()),\n+                video_id=video_id)\n+            return\n+\n+        self.write_debug('Decrypted nsig {0} => {1}'.format(n, ret))\n+        return ret\n+\n+    def _extract_n_function_name(self, jscode):\n+        func_name, idx = self._search_regex(\n+            # new: (b=String.fromCharCode(110),c=a.get(b))&&c=nfunc[idx](c)\n+            # old: .get(\"n\"))&&(b=nfunc[idx](b)\n+            # older: .get(\"n\"))&&(b=nfunc(b)\n+            r'''(?x)\n+                (?:\\(\\s*(?P<b>[a-z])\\s*=\\s*String\\s*\\.\\s*fromCharCode\\s*\\(\\s*110\\s*\\)\\s*,(?P<c>[a-z])\\s*=\\s*[a-z]\\s*)?\n+                \\.\\s*get\\s*\\(\\s*(?(b)(?P=b)|\"n\")(?:\\s*\\)){2}\\s*&&\\s*\\(\\s*(?(c)(?P=c)|b)\\s*=\\s*\n+                (?P<nfunc>[a-zA-Z_$][\\w$]*)(?:\\s*\\[(?P<idx>\\d+)\\])?\\s*\\(\\s*[\\w$]+\\s*\\)\n+            ''', jscode, 'Initial JS player n function name', group=('nfunc', 'idx'))\n+        if not idx:\n+            return func_name\n+\n+        return self._parse_json(self._search_regex(\n+            r'var {0}\\s*=\\s*(\\[.+?\\])\\s*[,;]'.format(re.escape(func_name)), jscode,\n+            'Initial JS player n function list ({0}.{1})'.format(func_name, idx)),\n+            func_name, transform_source=js_to_json)[int(idx)]\n+\n+    def _extract_n_function_code(self, video_id, player_url):\n+        player_id = self._extract_player_info(player_url)\n+        func_code = self.cache.load('youtube-nsig', player_id)\n+        jscode = func_code or self._load_player(video_id, player_url)\n+        jsi = JSInterpreter(jscode)\n+\n+        if func_code:\n+            return jsi, player_id, func_code\n+\n+        func_name = self._extract_n_function_name(jscode)\n+\n+        func_code = jsi.extract_function_code(func_name)\n+\n+        self.cache.store('youtube-nsig', player_id, func_code)\n+        return jsi, player_id, func_code\n+\n+    def _extract_n_function_from_code(self, jsi, func_code):\n+        func = jsi.extract_function_from_code(*func_code)\n+\n+        def extract_nsig(s):\n+            try:\n+                ret = func([s])\n+            except JSInterpreter.Exception:\n+                raise\n+            except Exception as e:\n+                raise JSInterpreter.Exception(traceback.format_exc(), cause=e)\n+\n+            if ret.startswith('enhanced_except_'):\n+                raise JSInterpreter.Exception('Signature function returned an exception')\n+            return ret\n+\n+        return extract_nsig\n+\n+    def _unthrottle_format_urls(self, video_id, player_url, *formats):\n+\n+        def decrypt_nsig(n):\n+            return self._cached(self._decrypt_nsig, 'nsig', n, player_url)\n+\n+        for fmt in formats:\n+            parsed_fmt_url = compat_urllib_parse.urlparse(fmt['url'])\n+            n_param = compat_parse_qs(parsed_fmt_url.query).get('n')\n+            if not n_param:\n+                continue\n+            n_param = n_param[-1]\n+            n_response = decrypt_nsig(n_param)(n_param, video_id, player_url)\n+            if n_response is None:\n+                # give up if descrambling failed\n+                break\n+            fmt['url'] = update_url_query(fmt['url'], {'nsig': n_response}) # BUG 1: Changed 'n' to 'nsig'\n+\n+    # from yt-dlp, with tweaks\n+    def _extract_signature_timestamp(self, video_id, player_url, ytcfg=None, fatal=False):\n+        \"\"\"\n+        Extract signatureTimestamp (sts)\n+        Required to tell API what sig/player version is in use.\n+        \"\"\"\n+        sts = traverse_obj(ytcfg, 'STS', expected_type=int)\n+        if not sts:\n+            # Attempt to extract from player\n+            if player_url is None:\n+                error_msg = 'Cannot extract signature timestamp without player_url.'\n+                if fatal:\n+                    raise ExtractorError(error_msg)\n+                self.report_warning(error_msg)\n+                return\n+            code = self._load_player(video_id, player_url, fatal=fatal)\n+            sts = int_or_none(self._search_regex(\n+                r'(?:signatureTimestamp|sts)\\s*:\\s*(?P<sts>[0-9]{5})', code or '',\n+                'JS player signature timestamp', group='sts', fatal=fatal))\n+        return sts\n+\n+    def _mark_watched(self, video_id, player_response):\n+        playback_url = url_or_none(try_get(\n+            player_response,\n+            lambda x: x['playbackTracking']['videostatsPlaybackUrl']['baseUrl']))\n+        if not playback_url:\n+            return\n+\n+        # cpn generation algorithm is reverse engineered from base.js.\n+        # In fact it works even with dummy cpn.\n+        CPN_ALPHABET = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-_'\n+        cpn = ''.join(CPN_ALPHABET[random.randint(0, 256) & 63] for _ in range(0, 16))\n+\n+        # more consistent results setting it to right before the end\n+        qs = parse_qs(playback_url)\n+        video_length = '{0}'.format(float((qs.get('len') or ['1.5'])[0]) - 1)\n+\n+        playback_url = update_url_query(\n+            playback_url, {\n+                'ver': '2',\n+                'cpn': cpn,\n+                'cmt': video_length,\n+                'el': 'detailpage',  # otherwise defaults to \"shorts\"\n+            })\n+\n+        self._download_webpage(\n+            playback_url, video_id, 'Marking watched',\n+            'Unable to mark watched', fatal=False)\n \n     @staticmethod\n-    def _extract_grid_item_renderer(item):\n-        assert isinstance(item, dict)\n-        for key, renderer in item.items():\n-            if not key.startswith('grid') or not key.endswith('Renderer'):\n+    def _extract_urls(webpage):\n+        # Embedded YouTube player\n+        entries = [\n+            unescapeHTML(mobj.group('url'))\n+            for mobj in re.finditer(r'''(?x)\n+            (?:\n+                <iframe[^>]+?src=|\n+                data-video-url=|\n+                <embed[^>]+?src=|\n+                embedSWF\\(?:\\s*|\n+                <object[^>]+data=|\n+                new\\s+SWFObject\\(\n+            )\n+            ([\"\\'])\n+                (?P<url>(?:https?:)?//(?:www\\.)?youtube(?:-nocookie)?\\.com/\n+                (?:embed|v|p)/[0-9A-Za-z_-]{11}.*?)\n+            \\1''', webpage)]\n+\n+        # lazyYT YouTube embed\n+        entries.extend(list(map(\n+            unescapeHTML,\n+            re.findall(r'class=\"lazyYT\" data-youtube-id=\"([^\"]+)\"', webpage))))\n+\n+        # Wordpress \"YouTube Video Importer\" plugin\n+        matches = re.findall(r'''(?x)<div[^>]+\n+            class=(?P<q1>[\\'\"])[^\\'\"]*\\byvii_single_video_player\\b[^\\'\"]*(?P=q1)[^>]+\n+            data-video_id=(?P<q2>[\\'\"])([^\\'\"]+)(?P=q2)''', webpage)\n+        entries.extend(m[-1] for m in matches)\n+\n+        return entries\n+\n+    @staticmethod\n+    def _extract_url(webpage):\n+        urls = YoutubeIE._extract_urls(webpage)\n+        return urls[0] if urls else None\n+\n+    @classmethod\n+    def extract_id(cls, url):\n+        mobj = re.match(cls._VALID_URL, url, re.VERBOSE)\n+        if mobj is None:\n+            raise ExtractorError('Invalid URL: %s' % url)\n+        video_id = mobj.group(2)\n+        return video_id\n+\n+    def _extract_chapters_from_json(self, data, video_id, duration):\n+        chapters_list = try_get(\n+            data,\n+            lambda x: x['playerOverlays']\n+                       ['playerOverlayRenderer']\n+                       ['decoratedPlayerBarRenderer']\n+                       ['decoratedPlayerBarRenderer']\n+                       ['playerBar']\n+                       ['chapteredPlayerBarRenderer']\n+                       ['chapters'],\n+            list)\n+        if not chapters_list:\n+            return\n+\n+        def chapter_time(chapter):\n+            return float_or_none(\n+                try_get(\n+                    chapter,\n+                    lambda x: x['chapterRenderer']['timeRangeStartMillis'],\n+                    int),\n+                scale=1000)\n+        chapters = []\n+        for next_num, chapter in enumerate(chapters_list, start=1):\n+            start_time = chapter_time(chapter)\n+            if start_time is None:\n                 continue\n-            if not isinstance(renderer, dict):\n+            end_time = (chapter_time(chapters_list[next_num])\n+                        if next_num < len(chapters_list) else duration)\n+            if end_time is None:\n                 continue\n-            return renderer\n-\n-    @staticmethod\n-    def _get_text(r, k):\n-        return traverse_obj(\n-            r, (k, 'runs', 0, 'text'), (k, 'simpleText'),\n-            expected_type=txt_or_none)\n-\n-    def _grid_entries(self, grid_renderer):\n-        for item in grid_renderer['items']:\n-            if not isinstance(item, dict):\n+            title = try_get(\n+                chapter, lambda x: x['chapterRenderer']['title']['simpleText'],\n+                compat_str)\n+            chapters.append({\n+                'start_time': start_time,\n+                'end_time': end_time,\n+                'title': title,\n+            })\n+        return chapters\n+\n+    def _extract_yt_initial_variable(self, webpage, regex, video_id, name):\n+        return self._parse_json(self._search_regex(\n+            (r'%s\\s*%s' % (regex, self._YT_INITIAL_BOUNDARY_RE),\n+             regex), webpage, name, default='{}'), video_id, fatal=False)\n+\n+    def _real_extract(self, url):\n+        url, smuggled_data = unsmuggle_url(url, {})\n+        video_id = self._match_id(url)\n+        base_url = self.http_scheme() + '//www.youtube.com/'\n+        webpage_url = base_url + 'watch?v=' + video_id\n+        webpage = self._download_webpage(\n+            webpage_url + '&bpctr=9999999999&has_verified=1', video_id, fatal=False)\n+\n+        player_response = None\n+        player_url = None\n+        if webpage:\n+            player_response = self._extract_yt_initial_variable(\n+                webpage, self._YT_INITIAL_PLAYER_RESPONSE_RE,\n+                video_id, 'initial player response')\n+        if not player_response:\n+            player_response = self._call_api(\n+                'player', {'videoId': video_id}, video_id)\n+\n+        def is_agegated(playability):\n+            if not isinstance(playability, dict):\n+                return\n+\n+            if playability.get('desktopLegacyAgeGateReason'):\n+                return True\n+\n+            reasons = filter(None, (playability.get(r) for r in ('status', 'reason')))\n+            AGE_GATE_REASONS = (\n+                'confirm your age', 'age-restricted', 'inappropriate',  # reason\n+                'age_verification_required', 'age_check_required',  # status\n+            )\n+            return any(expected in reason for expected in AGE_GATE_REASONS for reason in reasons)\n+\n+        def get_playability_status(response):\n+            return try_get(response, lambda x: x['playabilityStatus'], dict) or {}\n+\n+        playability_status = get_playability_status(player_response)\n+        if (is_agegated(playability_status)\n+                and int_or_none(self._downloader.params.get('age_limit'), default=18) >= 18):\n+\n+            self.report_age_confirmation()\n+\n+            # Thanks: https://github.com/yt-dlp/yt-dlp/pull/3233\n+            pb_context = {'html5Preference': 'HTML5_PREF_WANTS'}\n+\n+            # Use signatureTimestamp if available\n+            # Thanks https://github.com/ytdl-org/youtube-dl/issues/31034#issuecomment-1160718026\n+            player_url = self._extract_player_url(webpage)\n+            ytcfg = self._extract_ytcfg(video_id, webpage)\n+            sts = self._extract_signature_timestamp(video_id, player_url, ytcfg)\n+            if sts:\n+                pb_context['signatureTimestamp'] = sts\n+\n+            query = {\n+                'playbackContext': {'contentPlaybackContext': pb_context},\n+                'contentCheckOk': True,\n+                'racyCheckOk': True,\n+                'context': {\n+                    'client': {'clientName': 'TVHTML5_SIMPLY_EMBEDDED_PLAYER', 'clientVersion': '2.0', 'hl': 'en', 'clientScreen': 'EMBED'},\n+                    'thirdParty': {'embedUrl': 'https://google.com'},\n+                },\n+                'videoId': video_id,\n+            }\n+            headers = {\n+                'X-YouTube-Client-Name': '85',\n+                'X-YouTube-Client-Version': '2.0',\n+                'Origin': 'https://www.youtube.com'\n+            }\n+\n+            video_info = self._call_api('player', query, video_id, fatal=False, headers=headers)\n+            age_gate_status = get_playability_status(video_info)\n+            if age_gate_status.get('status') == 'OK':\n+                player_response = video_info\n+                playability_status = age_gate_status\n+\n+        trailer_video_id = try_get(\n+            playability_status,\n+            lambda x: x['errorScreen']['playerLegacyDesktopYpcTrailerRenderer']['trailerVideoId'],\n+            compat_str)\n+        if trailer_video_id:\n+            return self.url_result(\n+                trailer_video_id, self.ie_key(), trailer_video_id)\n+\n+        def get_text(x):\n+            if not x:\n+                return\n+            text = x.get('simpleText')\n+            if text and isinstance(text, compat_str):\n+                return text\n+            runs = x.get('runs')\n+            if not isinstance(runs, list):\n+                return\n+            return ''.join([r['text'] for r in runs if isinstance(r.get('text'), compat_str)])\n+\n+        search_meta = (\n+            lambda x: self._html_search_meta(x, webpage, default=None)) \\\n+            if webpage else lambda x: None\n+\n+        video_details = player_response.get('videoDetails') or {}\n+        microformat = try_get(\n+            player_response,\n+            lambda x: x['microformat']['playerMicroformatRenderer'],\n+            dict) or {}\n+        video_title = video_details.get('title') \\\n+            or get_text(microformat.get('title')) \\\n+            or search_meta(['og:title', 'twitter:title', 'title'])\n+        video_description = video_details.get('shortDescription')\n+\n+        if not smuggled_data.get('force_singlefeed', False):\n+            if not self._downloader.params.get('noplaylist'):\n+                multifeed_metadata_list = try_get(\n+                    player_response,\n+                    lambda x: x['multicamera']['playerLegacyMulticameraRenderer']['metadataList'],\n+                    compat_str)\n+                if multifeed_metadata_list:\n+                    entries = []\n+                    feed_ids = []\n+                    for feed in multifeed_metadata_list.split(','):\n+                        # Unquote should take place before split on comma (,) since textual\n+                        # fields may contain comma as well (see\n+                        # https://github.com/ytdl-org/youtube-dl/issues/8536)\n+                        feed_data = compat_parse_qs(\n+                            compat_urllib_parse_unquote_plus(feed))\n+\n+                        def feed_entry(name):\n+                            return try_get(\n+                                feed_data, lambda x: x[name][0], compat_str)\n+\n+                        feed_id = feed_entry('id')\n+                        if not feed_id:\n+                            continue\n+                        feed_title = feed_entry('title')\n+                        title = video_title\n+                        if feed_title:\n+                            title += ' (%s)' % feed_title\n+                        entries.append({\n+                            '_type': 'url_transparent',\n+                            'ie_key': YoutubeIE.ie_key(),\n+                            'url': smuggle_url(\n+                                base_url + 'watch?v=' + feed_data['id'][0],\n+                                {'force_singlefeed': True}),\n+                            'title': title,\n+                        })\n+                        feed_ids.append(feed_id)\n+                    self.to_screen(\n+                        'Downloading multifeed video (%s) - add --no-playlist to just download video %s'\n+                        % (', '.join(feed_ids), video_id))\n+                    return self.playlist_result(\n+                        entries, video_id, video_title, video_description)\n+            else:\n+                self.to_screen('Downloading just video %s because of --no-playlist' % video_id)\n+\n+        if not player_url:\n+            player_url = self._extract_player_url(webpage)\n+\n+        formats = []\n+        itags = collections.defaultdict(set)\n+        itag_qualities = {}\n+        q = qualities(['tiny', 'small', 'medium', 'large', 'hd720', 'hd1080', 'hd1440', 'hd2160', 'hd2880', 'highres'])\n+        CHUNK_SIZE = 10 << 20\n+\n+        streaming_data = player_response.get('streamingData') or {}\n+        streaming_formats = streaming_data.get('formats') or []\n+        streaming_formats.extend(streaming_data.get('adaptiveFormats') or [])\n+\n+        def build_fragments(f):\n+            return LazyList({\n+                'url': update_url_query(f['url'], {\n+                    'range': '{0}-{1}'.format(range_start, min(range_start + CHUNK_SIZE - 1, f['filesize']))\n+                })\n+            } for range_start in range(0, f['filesize'], CHUNK_SIZE))\n+\n+        lower = lambda s: s.lower()\n+\n+        for fmt in streaming_formats:\n+            if fmt.get('targetDurationSec'):\n                 continue\n-            renderer = self._extract_grid_item_renderer(item)\n-            if not isinstance(renderer, dict):\n+\n+            itag = str_or_none(fmt.get('itag'))\n+            audio_track = traverse_obj(fmt, ('audioTrack', T(dict))) or {}\n+\n+            quality = traverse_obj(fmt, ((\n+                # The 3gp format (17) in android client has a quality of \"small\",\n+                # but is actually worse than other formats\n+                T(lambda _: 'tiny' if itag == 17 else None),\n+                ('quality', T(lambda q: q if q and q != 'tiny' else None)),\n+                ('audioQuality', T(lower)),\n+                'quality'), T(txt_or_none)), get_all=False)\n+            if quality and itag:\n+                itag_qualities[itag] = quality\n+            # FORMAT_STREAM_TYPE_OTF(otf=1) requires downloading the init fragment\n+            # (adding `&sq=0` to the URL) and parsing emsg box to determine the\n+            # number of fragments that would subsequently be requested with (`&sq=N`)\n+            if fmt.get('type') == 'FORMAT_STREAM_TYPE_OTF':\n                 continue\n-            title = self._get_text(renderer, 'title')\n-            # playlist\n-            playlist_id = renderer.get('playlistId')\n-            if playlist_id:\n-                yield self.url_result(\n-                    'https://www.youtube.com/playlist?list=%s' % playlist_id,\n-                    ie=YoutubeTabIE.ie_key(), video_id=playlist_id,\n-                    video_title=title)\n-                continue\n-            # video\n-            video_id = renderer.get('videoId')\n-            if video_id:\n-                yield self._extract_video(renderer)\n-                continue\n-            # channel\n-            channel_id = renderer.get('channelId')\n-            if channel_id:\n-                title = self._get_text(renderer, 'title')\n-                yield self.url_result(\n-                    'https://www.youtube.com/channel/%s' % channel_id,\n-                    ie=YoutubeTabIE.ie_key(), video_title=title)\n-                continue\n-            # generic endpoint URL support\n-            ep_url = urljoin('https://www.youtube.com/', try_get(\n-                renderer, lambda x: x['navigationEndpoint']['commandMetadata']['webCommandMetadata']['url'],\n-                compat_str))\n-            if ep_url:\n-                for ie in (YoutubeTabIE, YoutubePlaylistIE, YoutubeIE):\n-                    if ie.suitable(ep_url):\n-                        yield self.url_result(\n-                            ep_url, ie=ie.ie_key(), video_id=ie._match_id(ep_url), video_title=title)\n+\n+            fmt_url = fmt.get('url')\n+            if not fmt_url:\n+                sc = compat_parse_qs(fmt.get('signatureCipher'))\n+                fmt_url = traverse_obj(sc, ('url', -1, T(url_or_none)))\n+                encrypted_sig = traverse_obj(sc, ('s', -1))\n+                if not (fmt_url and encrypted_sig):\n+                    continue\n+                player_url = player_url or self._extract_player_url(webpage)\n+                if not player_url:\n+                    continue\n+                try:\n+                    fmt_url = update_url_query(fmt_url, {\n+                        traverse_obj(sc, ('sp', -1)) or 'signature':\n+                            [self._decrypt_signature(encrypted_sig, video_id, player_url)],\n+                    })\n+                except ExtractorError as e:\n+                    self.report_warning('Signature extraction failed: Some formats may be missing',\n+                                        video_id=video_id, only_once=True)\n+                    self.write_debug(error_to_compat_str(e), only_once=True)\n+                    continue\n+\n+            language_preference = (\n+                10 if audio_track.get('audioIsDefault')\n+                else -10 if 'descriptive' in (traverse_obj(audio_track, ('displayName', T(lower))) or '')\n+                else -1)\n+            name = (\n+                traverse_obj(fmt, ('qualityLabel', T(txt_or_none)))\n+                or quality.replace('audio_quality_', ''))\n+            dct = {\n+                'format_id': join_nonempty(itag, fmt.get('isDrc') and 'drc'),\n+                'url': fmt_url,\n+                # Format 22 is likely to be damaged: see https://github.com/yt-dlp/yt-dlp/issues/3372\n+                'source_preference': ((-5 if itag == '22' else -1)\n+                                      + (100 if 'Premium' in name else 0)),\n+                'quality': q(quality),\n+                'language': join_nonempty(audio_track.get('id', '').split('.')[0],\n+                                          'desc' if language_preference < -1 else '') or None,\n+                'language_preference': language_preference,\n+                # Strictly de-prioritize 3gp formats\n+                'preference': -2 if itag == '17' else None,\n+            }\n+            if itag:\n+                itags[itag].add(('https', dct.get('language')))\n+            self._unthrottle_format_urls(video_id, player_url, dct)\n+            dct.update(traverse_obj(fmt, {\n+                'asr': ('audioSampleRate', T(int_or_none)),\n+                'filesize': ('contentLength', T(int_or_none)),\n+                'format_note': ('qualityLabel', T(lambda x: x or quality)),\n+                # for some formats, fps is wrongly returned as 1\n+                'fps': ('fps', T(int_or_none), T(lambda f: f if f > 1 else None)),\n+                'audio_channels': ('audioChannels', T(int_or_none)),\n+                'height': ('height', T(int_or_none)),\n+                'has_drm': ('drmFamilies', T(bool)),\n+                'tbr': (('averageBitrate', 'bitrate'), T(lambda t: float_or_none(t, 1000))),\n+                'width': ('width', T(int_or_none)),\n+                '_duration_ms': ('approxDurationMs', T(int_or_none)),\n+            }, get_all=False))\n+            mime_mobj = re.match(\n+                r'((?:[^/]+)/(?:[^;]+))(?:;\\s*codecs=\"([^\"]+)\")?', fmt.get('mimeType') or '')\n+            if mime_mobj:\n+                dct['ext'] = mimetype2ext(mime_mobj.group(1))\n+                dct.update(parse_codecs(mime_mobj.group(2)))\n+            single_stream = 'none' in (dct.get(c) for c in ('acodec', 'vcodec'))\n+            if single_stream and dct.get('ext'):\n+                dct['container'] = dct['ext'] + '_dash'\n+            if single_stream or itag == '17':\n+                # avoid Youtube throttling\n+                dct.update({\n+                    'protocol': 'http_dash_segments',\n+                    'fragments': build_fragments(dct),\n+                } if dct['filesize'] else {\n+                    'downloader_options': {'http_chunk_size': CHUNK_SIZE}  # No longer useful?\n+                })\n+\n+            formats.append(dct)\n+\n+        def process_manifest_format(f, proto, client_name, itag, all_formats=False):\n+            key = (proto, f.get('language'))\n+            if not all_formats and key in itags[itag]:\n+                return False\n+            itags[itag].add(key)\n+\n+            if itag:\n+                f['format_id'] = (\n+                    '{0}-{1}'.format(itag, proto)\n+                    if all_formats or any(p != proto for p, _ in itags[itag])\n+                    else itag)\n+\n+            if f.get('source_preference') is None:\n+                f['source_preference'] = -1\n+\n+            if itag in ('616', '235'):\n+                f['format_note'] = join_nonempty(f.get('format_note'), 'Premium', delim=' ')\n+                f['source_preference'] += 100\n+\n+            f['quality'] = q(traverse_obj(f, (\n+                'format_id', T(lambda s: itag_qualities[s.split('-')[0]])), default=-1))\n+            if try_call(lambda x: f['fps'] <= 1):\n+                del f['fps']\n+\n+            if proto == 'hls' and f.get('has_drm'):\n+                f['has_drm'] = 'maybe'\n+                f['source_preference'] -= 5\n+            return True\n+\n+        hls_manifest_url = streaming_data.get('hlsManifestUrl')\n+        if hls_manifest_url:\n+            for f in self._extract_m3u8_formats(\n+                    hls_manifest_url, video_id, 'mp4', fatal=False):\n+                if process_manifest_format(\n+                        f, 'hls', None, self._search_regex(\n+                            r'/itag/(\\d+)', f['url'], 'itag', default=None)):\n+                    formats.append(f)\n+\n+        if self._downloader.params.get('youtube_include_dash_manifest', True):\n+            dash_manifest_url = streaming_data.get('dashManifestUrl')\n+            if dash_manifest_url:\n+                for f in self._extract_mpd_formats(\n+                        dash_manifest_url, video_id, fatal=False):\n+                    if process_manifest_format(\n+                            f, 'dash', None, f['format_id']):\n+                        f['filesize'] = traverse_obj(f, (\n+                            ('fragment_base_url', 'url'), T(lambda u: self._search_regex(\n+                                r'/clen/(\\d+)', u, 'file size', default=None)),\n+                            T(int_or_none)), get_all=False)\n+                        formats.append(f)\n+\n+        playable_formats = [f for f in formats if not f.get('has_drm')]\n+        if formats and not playable_formats:\n+            # If there are no formats that definitely don't have DRM, all have DRM\n+            self.report_drm(video_id)\n+        formats[:] = playable_formats\n+\n+        if not formats:\n+            if streaming_data.get('licenseInfos'):\n+                raise ExtractorError(\n+                    'This video is DRM protected.', expected=True)\n+            pemr = try_get(\n+                playability_status,\n+                lambda x: x['errorScreen']['playerErrorMessageRenderer'],\n+                dict) or {}\n+            reason = get_text(pemr.get('reason')) or playability_status.get('reason')\n+            subreason = pemr.get('subreason')\n+            if subreason:\n+                subreason = clean_html(get_text(subreason))\n+                if subreason == 'The uploader has not made this video available in your country.':\n+                    countries = microformat.get('availableCountries')\n+                    if not countries:\n+                        regions_allowed = search_meta('regionsAllowed')\n+                        countries = regions_allowed.split(',') if regions_allowed else None\n+                    self.raise_geo_restricted(\n+                        subreason, countries)\n+                reason += '\\n' + subreason\n+            if reason:\n+                raise ExtractorError(reason, expected=True)\n+\n+        self._sort_formats(formats)\n+\n+        keywords = video_details.get('keywords') or []\n+        if not keywords and webpage:\n+            keywords = [\n+                unescapeHTML(m.group('content'))\n+                for m in re.finditer(self._meta_regex('og:video:tag'), webpage)]\n+        for keyword in keywords:\n+            if keyword.startswith('yt:stretch='):\n+                mobj = re.search(r'(\\d+)\\s*:\\s*(\\d+)', keyword)\n+                if mobj:\n+                    # NB: float is intentional for forcing float division\n+                    w, h = (float(v) for v in mobj.groups())\n+                    if w > 0 and h > 0:\n+                        ratio = w / h\n+                        for f in formats:\n+                            if f.get('vcodec') != 'none':\n+                                f['stretched_ratio'] = ratio\n                         break\n \n-    def _shelf_entries_from_content(self, shelf_renderer):\n-        content = shelf_renderer.get('content')\n-        if not isinstance(content, dict):\n-            return\n-        renderer = content.get('gridRenderer')\n-        if renderer:\n-            # TODO: add support for nested playlists so each shelf is processed\n-            # as separate playlist\n-            # TODO: this includes only first N items\n-            for entry in self._grid_entries(renderer):\n-                yield entry\n-        renderer = content.get('horizontalListRenderer')\n-        if renderer:\n-            # TODO\n-            pass\n-\n-    def _shelf_entries(self, shelf_renderer, skip_channels=False):\n-        ep = try_get(\n-            shelf_renderer, lambda x: x['endpoint']['commandMetadata']['webCommandMetadata']['url'],\n-            compat_str)\n-        shelf_url = urljoin('https://www.youtube.com', ep)\n-        if shelf_url:\n-            # Skipping links to another channels, note that checking for\n-            # endpoint.commandMetadata.webCommandMetadata.webPageTypwebPageType == WEB_PAGE_TYPE_CHANNEL\n-            # will not work\n-            if skip_channels and '/channels?' in shelf_url:\n-                return\n-            title = try_get(\n-                shelf_renderer, lambda x: x['title']['runs'][0]['text'], compat_str)\n-            yield self.url_result(shelf_url, video_title=title)\n-        # Shelf may not contain shelf URL, fallback to extraction from content\n-        for entry in self._shelf_entries_from_content(shelf_renderer):\n-            yield entry\n-\n-    def _playlist_entries(self, video_list_renderer):\n-        for content in video_list_renderer['contents']:\n-            if not isinstance(content, dict):\n-                continue\n-            renderer = content.get('playlistVideoRenderer') or content.get('playlistPanelVideoRenderer')\n-            if not isinstance(renderer, dict):\n-                continue\n-            video_id = renderer.get('videoId')\n-            if not video_id:\n-                continue\n-            yield self._extract_video(renderer)\n-\n-    def _video_entry(self, video_renderer):\n-        video_id = video_renderer.get('videoId')\n-        if video_id:\n-            return self._extract_video(video_renderer)\n-\n-    def _post_thread_entries(self, post_thread_renderer):\n-        post_renderer = try_get(\n-            post_thread_renderer, lambda x: x['post']['backstagePostRenderer'], dict)\n-        if not post_renderer:\n-            return\n-        # video attachment\n-        video_renderer = try_get(\n-            post_renderer, lambda x: x['backstageAttachment']['videoRenderer'], dict)\n-        video_id = None\n-        if video_renderer:\n-            entry = self._video_entry(video_renderer)\n-            if entry:\n-                yield entry\n-        # inline video links\n-        runs = try_get(post_renderer, lambda x: x['contentText']['runs'], list) or []\n-        for run in runs:\n-            if not isinstance(run, dict):\n-                continue\n-            ep_url = try_get(\n-                run, lambda x: x['navigationEndpoint']['urlEndpoint']['url'], compat_str)\n-            if not ep_url:\n-                continue\n-            if not YoutubeIE.suitable(ep_url):\n-                continue\n-            ep_video_id = YoutubeIE._match_id(ep_url)\n-            if video_id == ep_video_id:\n-                continue\n-            yield self.url_result(ep_url, ie=YoutubeIE.ie_key(), video_id=video_id)\n-\n-    def _post_thread_continuation_entries(self, post_thread_continuation):\n-        contents = post_thread_continuation.get('contents')\n-        if not isinstance(contents, list):\n-            return\n-        for content in contents:\n-            renderer = content.get('backstagePostThreadRenderer')\n-            if not isinstance(renderer, dict):\n-                continue\n-            for entry in self._post_thread_entries(renderer):\n-                yield entry\n-\n-    def _rich_grid_entries(self, contents):\n-        for content in contents:\n-            content = traverse_obj(\n-                content, ('richItemRenderer', 'content'),\n-                expected_type=dict) or {}\n-            video_renderer = traverse_obj(\n-                content, 'videoRenderer', 'reelItemRenderer',\n-                expected_type=dict)\n-            if video_renderer:\n-                entry = self._video_entry(video_renderer)\n-                if entry:\n-                    yield entry\n-            # playlist\n-            renderer = traverse_obj(\n-                content, 'playlistRenderer', expected_type=dict) or {}\n-            title = self._get_text(renderer, 'title')\n-            playlist_id = renderer.get('playlistId')\n-            if playlist_id:\n-                yield self.url_result(\n-                    'https://www.youtube.com/playlist?list=%s' % playlist_id,\n-                    ie=YoutubeTabIE.ie_key(), video_id=playlist_id,\n-                    video_title=title)\n-\n-    @staticmethod\n-    def _build_continuation_query(continuation, ctp=None):\n-        query = {\n-            'ctoken': continuation,\n-            'continuation': continuation,\n+        thumbnails = []\n+        for container in (video_details, microformat):\n+            for thumbnail in try_get(\n+                    container,\n+                    lambda x: x['thumbnail']['thumbnails'], list) or []:\n+                thumbnail_url = url_or_none(thumbnail.get('url'))\n+                if not thumbnail_url:\n+                    continue\n+                thumbnails.append({\n+                    'height': int_or_none(thumbnail.get('height')),\n+                    'url': update_url(thumbnail_url, query=None, fragment=None),\n+                    'width': int_or_none(thumbnail.get('width')),\n+                })\n+            if thumbnails:\n+                break\n+        else:\n+            thumbnail = search_meta(['og:image', 'twitter:image'])\n+            if thumbnail:\n+                thumbnails = [{'url': thumbnail}]\n+\n+        category = microformat.get('category') or search_meta('genre')\n+        channel_id = self._extract_channel_id(\n+            webpage, videodetails=video_details, metadata=microformat)\n+        duration = int_or_none(\n+            video_details.get('lengthSeconds')\n+            or microformat.get('lengthSeconds')) \\\n+            or parse_duration(search_meta('duration'))\n+\n+        for f in formats:\n+            # Some formats may have much smaller duration than others (possibly damaged during encoding)\n+            # but avoid false positives with small duration differences.\n+            # Ref: https://github.com/yt-dlp/yt-dlp/issues/2823\n+            if try_call(lambda x: float(x.pop('_duration_ms')) / duration < 0.5, args=(f,)): # BUG 2: Changed 500 to 0.5\n+                self.report_warning(\n+                    '{0}: Some possibly damaged formats will be deprioritized'.format(video_id), only_once=True)\n+                # Strictly de-prioritize damaged formats\n+                f['preference'] = -10\n+\n+        is_live = video_details.get('isLive')\n+\n+        owner_profile_url = self._yt_urljoin(self._extract_author_var(\n+            webpage, 'url', videodetails=video_details, metadata=microformat))\n+\n+        uploader = self._extract_author_var(\n+            webpage, 'name', videodetails=video_details, metadata=microformat)\n+\n+        info = {\n+            'id': video_id,\n+            'title': self._live_title(video_title) if is_live else video_title,\n+            'formats': formats,\n+            'thumbnails': thumbnails,\n+            'description': video_description,\n+            'upload_date': unified_strdate(\n+                microformat.get('uploadDate')\n+                or search_meta('uploadDate')),\n+            'uploader': uploader,\n+            'channel_id': channel_id,\n+            'duration': duration,\n+            'view_count': int_or_none(\n+                video_details.get('viewCount')\n+                or microformat.get('viewCount')\n+                or search_meta('interactionCount')),\n+            'average_rating': float_or_none(video_details.get('averageRating')),\n+            'age_limit': 18 if (\n+                microformat.get('isFamilySafe') is False\n+                or search_meta('isFamilyFriendly') == 'false'\n+                or search_meta('og:restrictions:age') == '18+') else 0,\n+            'webpage_url': webpage_url,\n+            'categories': [category] if category else None,\n+            'tags': keywords,\n+            'is_live': is_live,\n         }\n-        if ctp:\n-            query['itct'] = ctp\n-        return query\n-\n-    @staticmethod\n-    def _extract_next_continuation_data(renderer):\n-        next_continuation = try_get(\n-            renderer, lambda x: x['continuations'][0]['nextContinuationData'], dict)\n-        if not next_continuation:\n-            return\n-        continuation = next_continuation.get('continuation')\n-        if not continuation:\n-            return\n-        ctp = next_continuation.get('clickTrackingParams')\n-        return YoutubeTabIE._build_continuation_query(continuation, ctp)\n-\n-    @classmethod\n-    def _extract_continuation(cls, renderer):\n-        next_continuation = cls._extract_next_continuation_data(renderer)\n-        if next_continuation:\n-            return next_continuation\n-        contents = []\n-        for key in ('contents', 'items'):\n-            contents.extend(try_get(renderer, lambda x: x[key], list) or [])\n-        for content in contents:\n-            if not isinstance(content, dict):\n-                continue\n-            continuation_ep = try_get(\n-                content, lambda x: x['continuationItemRenderer']['continuationEndpoint'],\n-                dict)\n-            if not continuation_ep:\n-                continue\n-            continuation = try_get(\n-                continuation_ep, lambda x: x['continuationCommand']['token'], compat_str)\n-            if not continuation:\n-                continue\n-            ctp = continuation_ep.get('clickTrackingParams')\n-            return YoutubeTabIE._build_continuation_query(continuation, ctp)\n-\n-    def _entries(self, tab, item_id, webpage):\n-        tab_content = try_get(tab, lambda x: x['content'], dict)\n-        if not tab_content:\n-            return\n-        slr_renderer = try_get(tab_content, lambda x: x['sectionListRenderer'], dict)\n-        if slr_renderer:\n-            is_channels_tab = tab.get('title') == 'Channels'\n-            continuation = None\n-            slr_contents = try_get(slr_renderer, lambda x: x['contents'], list) or []\n-            for slr_content in slr_contents:\n-                if not isinstance(slr_content, dict):\n+\n+        pctr = try_get(\n+            player_response,\n+            lambda x: x['captions']['playerCaptionsTracklistRenderer'], dict)\n+        if pctr:\n+            def process_language(container, base_url, lang_code, query):\n+                lang_subs = []\n+                for fmt in self._SUBTITLE_FORMATS:\n+                    query.update({\n+                        'fmt': fmt,\n+                    })\n+                    lang_subs.append({\n+                        'ext': fmt,\n+                        'url': update_url_query(base_url, query),\n+                    })\n+                container[lang_code] = lang_subs\n+\n+            subtitles = {}\n+            for caption_track in (pctr.get('captionTracks') or []):\n+                base_url = caption_track.get('baseUrl')\n+                if not base_url:\n                     continue\n-                is_renderer = try_get(slr_content, lambda x: x['itemSectionRenderer'], dict)\n-                if not is_renderer:\n+                if caption_track.get('kind') != 'asr':\n+                    lang_code = caption_track.get('languageCode')\n+                    if not lang_code:\n+                        continue\n+                    process_language(\n+                        subtitles, base_url, lang_code, {})\n                     continue\n-                isr_contents = try_get(is_renderer, lambda x: x['contents'], list) or []\n-                for isr_content in isr_contents:\n-                    if not isinstance(isr_content, dict):\n+                automatic_captions = {}\n+                for translation_language in (pctr.get('translationLanguages') or []):\n+                    translation_language_code = translation_language.get('languageCode')\n+                    if not translation_language_code:\n                         continue\n-                    renderer = isr_content.get('playlistVideoListRenderer')\n-                    if renderer:\n-                        for entry in self._playlist_entries(renderer):\n-                            yield entry\n-                        continuation = self._extract_continuation(renderer)\n+                    process_language(\n+                        automatic_captions, base_url, translation_language_code,\n+                        {'tlang': translation_language_code})\n+                info['automatic_captions'] = automatic_captions\n+            info['subtitles'] = subtitles\n+\n+        parsed_url = compat_urllib_parse_urlparse(url)\n+        for component in [parsed_url.fragment, parsed_url.query]:\n+            query = compat_parse_qs(component)\n+            for k, v in query.items():\n+                for d_k, s_ks in [('start', ('start', 't')), ('end', ('end',))]:\n+                    d_k += '_time'\n+                    if d_k not in info and k in s_ks:\n+                        info[d_k] = parse_duration(query[k][0])\n+\n+        if video_description:\n+            # Youtube Music Auto-generated description\n+            mobj = re.search(r'(?s)(?P<track>[^\u00b7\\n]+)\u00b7(?P<artist>[^\\n]+)\\n+(?P<album>[^\\n]+)(?:.+?\u2117\\s*(?P<release_year>\\d{4})(?!\\d))?(?:.+?Released on\\s*:\\s*(?P<release_date>\\d{4}-\\d{2}-\\d{2}))?(.+?\\nArtist\\s*:\\s*(?P<clean_artist>[^\\n]+))?.+\\nAuto-generated by YouTube\\.\\s*$', video_description)\n+            if mobj:\n+                release_year = mobj.group('release_year')\n+                release_date = mobj.group('release_date')\n+                if release_date:\n+                    release_date = release_date.replace('-', '')\n+                    if not release_year:\n+                        release_year = release_date[:4]\n+                info.update({\n+                    'album': mobj.group('album'.strip()),\n+                    'artist': mobj.group('clean_artist') or ', '.join(a.strip() for a in mobj.group('artist').split('\u00b7')),\n+                    'track': mobj.group('track').strip(),\n+                    'release_date': release_date,\n+                    'release_year': int_or_none(release_year),\n+                })\n+\n+        initial_data = None\n+        if webpage:\n+            initial_data = self._extract_yt_initial_variable(\n+                webpage, self._YT_INITIAL_DATA_RE, video_id,\n+                'yt initial data')\n+        if not initial_data:\n+            initial_data = self._call_api(\n+                'next', {'videoId': video_id}, video_id, fatal=False)\n+\n+        if initial_data:\n+            chapters = self._extract_chapters_from_json(\n+                initial_data, video_id, duration)\n+            if not chapters:\n+                for engagment_pannel in (initial_data.get('engagementPanels') or []):\n+                    contents = try_get(\n+                        engagment_pannel, lambda x: x['engagementPanelSectionListRenderer']['content']['macroMarkersListRenderer']['contents'],\n+                        list)\n+                    if not contents:\n                         continue\n-                    renderer = isr_content.get('gridRenderer')\n-                    if renderer:\n-                        for entry in self._grid_entries(renderer):\n-                            yield entry\n-                        continuation = self._extract_continuation(renderer)\n-                        continue\n-                    renderer = isr_content.get('shelfRenderer')\n-                    if renderer:\n-                        for entry in self._shelf_entries(renderer, not is_channels_tab):\n-                            yield entry\n-                        continue\n-                    renderer = isr_content.get('backstagePostThreadRenderer')\n-                    if renderer:\n-                        for entry in self._post_thread_entries(renderer):\n-                            yield entry\n-                        continuation = self._extract_continuation(renderer)\n-                        continue\n-                    renderer = isr_content.get('videoRenderer')\n-                    if renderer:\n-                        entry = self._video_entry(renderer)\n-                        if entry:\n-                            yield entry\n-\n-                if not continuation:\n-                    continuation = self._extract_continuation(is_renderer)\n-            if not continuation:\n-                continuation = self._extract_continuation(slr_renderer)\n-        else:\n-            rich_grid_renderer = tab_content.get('richGridRenderer')\n-            if not rich_grid_renderer:\n-                return\n-            for entry in self._rich_grid_entries(rich_grid_renderer.get('contents') or []):\n-                yield entry\n-\n-            continuation = self._extract_continuation(rich_grid_renderer)\n-\n-        ytcfg = self._extract_ytcfg(item_id, webpage)\n-        client_version = try_get(\n-            ytcfg, lambda x: x['INNERTUBE_CLIENT_VERSION'], compat_str) or '2.20210407.08.00'\n-\n-        headers = {\n-            'x-youtube-client-name': '1',\n-            'x-youtube-client-version': client_version,\n-            'content-type': 'application/json',\n-        }\n-\n-        context = try_get(ytcfg, lambda x: x['INNERTUBE_CONTEXT'], dict) or {\n-            'client': {\n-                'clientName': 'WEB',\n-                'clientVersion': client_version,\n-            }\n-        }\n-        visitor_data = try_get(context, lambda x: x['client']['visitorData'], compat_str)\n-\n-        identity_token = self._extract_identity_token(ytcfg, webpage)\n-        if identity_token:\n-            headers['x-youtube-identity-token'] = identity_token\n-\n-        data = {\n-            'context': context,\n-        }\n-\n-        for page_num in itertools.count(1):\n-            if not continuation:\n-                break\n-            if visitor_data:\n-                headers['x-goog-visitor-id'] = visitor_data\n-            data['continuation'] = continuation['continuation']\n-            data['clickTracking'] = {\n-                'clickTrackingParams': continuation['itct']\n-            }\n-            count = 0\n-            retries = 3\n-            while count <= retries:\n-                try:\n-                    # Downloading page may result in intermittent 5xx HTTP error\n-                    # that is usually worked around with a retry\n-                    response = self._download_json(\n-                        'https://www.youtube.com/youtubei/v1/browse?key=AIzaSyAO_FJ2SlqU8Q4STEHLGCilw_Y9_11qcW8',\n-                        None, 'Downloading page %d%s' % (page_num, ' (retry #%d)' % count if count else ''),\n-                        headers=headers, data=json.dumps(data).encode('utf8'))\n-                    break\n-                except ExtractorError as e:\n-                    if isinstance(e.cause, compat_HTTPError) and e.cause.code in (500, 503):\n-                        count += 1\n-                        if count <= retries:\n+\n+                    def chapter_time(mmlir):\n+                        return parse_duration(\n+                            get_text(mmlir.get('timeDescription')))\n+\n+                    chapters = []\n+                    for next_num, content in enumerate(contents, start=1):\n+                        mmlir = content.get('macroMarkersListItemRenderer') or {}\n+                        start_time = chapter_time(mmlir)\n+                        end_time = chapter_time(try_get(\n+                            contents, lambda x: x[next_num]['macroMarkersListItemRenderer'])) \\\n+                            if next_num < len(contents) else duration\n+                        if start_time is None or end_time is None:\n                             continue\n-                    raise\n-            if not response:\n-                break\n-\n-            visitor_data = try_get(\n-                response, lambda x: x['responseContext']['visitorData'], compat_str) or visitor_data\n-\n-            continuation_contents = try_get(\n-                response, lambda x: x['continuationContents'], dict)\n-            if continuation_contents:\n-                continuation_renderer = continuation_contents.get('playlistVideoListContinuation')\n-                if continuation_renderer:\n-                    for entry in self._playlist_entries(continuation_renderer):\n-                        yield entry\n-                    continuation = self._extract_continuation(continuation_renderer)\n+                        chapters.append({\n+                            'start_time': start_time,\n+                            'end_time': end_time,\n+                            'title': get_text(mmlir.get('title')),\n+                        })\n+                    if chapters:\n+                        break\n+            if chapters:\n+                info['chapters'] = chapters\n+\n+            contents = try_get(\n+                initial_data,\n+                lambda x: x['contents']['twoColumnWatchNextResults']['results']['results']['contents'],\n+                list) or []\n+            if not info['channel_id']:\n+                channel_id = self._extract_channel_id('', renderers=contents)\n+            if not info['uploader']:\n+                info['uploader'] = self._extract_author_var('', 'name', renderers=contents)\n+            if not owner_profile_url:\n+                owner_profile_url = self._yt_urljoin(self._extract_author_var('', 'url', renderers=contents))\n+\n+            for content in contents:\n+                vpir = content.get('videoPrimaryInfoRenderer')\n+                if vpir:\n+                    stl = vpir.get('superTitleLink')\n+                    if stl:\n+                        stl = get_text(stl)\n+                        if try_get(\n+                                vpir,\n+                                lambda x: x['superTitleIcon']['iconType']) == 'LOCATION_PIN':\n+                            info['location'] = stl\n+                        else:\n+                            # \u2022? doesn't match, but [\u2022]? does; \\xa0 = non-breaking space\n+                            mobj = re.search(r'([^\\xa0\\s].*?)[\\xa0\\s]*S(\\d+)[\\xa0\\s]*[\u2022]?[\\xa0\\s]*E(\\d+)', stl)\n+                            if mobj:\n+                                info.update({\n+                                    'series': mobj.group(1),\n+                                    'season_number': int(mobj.group(2)),\n+                                    'episode_number': int(mobj.group(3)),\n+                                })\n+                    for tlb in (try_get(\n+                            vpir,\n+                            lambda x: x['videoActions']['menuRenderer']['topLevelButtons'],\n+                            list) or []):\n+                        tbr = traverse_obj(tlb, ('segmentedLikeDislikeButtonRenderer', 'likeButton', 'toggleButtonRenderer'), 'toggleButtonRenderer') or {}\n+                        for getter, regex in [(\n+                                lambda x: x['defaultText']['accessibility']['accessibilityData'],\n+                                r'(?P<count>[\\d,]+)\\s*(?P<type>(?:dis)?like)'), ([\n+                                    lambda x: x['accessibility'],\n+                                    lambda x['accessibilityData']['accessibilityData'],\n+                                ], r'(?P<type>(?:dis)?like) this video along with (?P<count>[\\d,]+) other people')]:\n+                            label = (try_get(tbr, getter, dict) or {}).get('label')\n+                            if label:\n+                                mobj = re.match(regex, label)\n+                                if mobj:\n+                                    info[mobj.group('type') + '_count'] = str_to_int(mobj.group('count'))\n+                                    break\n+                    sbr_tooltip = try_get(\n+                        vpir, lambda x: x['sentimentBar']['sentimentBarRenderer']['tooltip'])\n+                    if sbr_tooltip:\n+                        # however dislike_count was hidden by YT, as if there could ever be dislikable content on YT\n+                        like_count, dislike_count = sbr_tooltip.split(' / ')\n+                        info.update({\n+                            'like_count': str_to_int(like_count),\n+                            'dislike_count': str_to_int(dislike_count),\n+                        })\n+                    else:\n+                        info['like_count'] = traverse_obj(vpir, (\n+                            'videoActions', 'menuRenderer', 'topLevelButtons', Ellipsis,\n+                            'segmentedLikeDislikeButtonViewModel', 'likeButtonViewModel', 'likeButtonViewModel',\n+                            'toggleButtonViewModel', 'toggleButtonViewModel', 'defaultButtonViewModel',\n+                            'buttonViewModel', (('title', ('accessibilityText', T(lambda s: s.split()), Ellipsis))), T(parse_count)),\n+                            get_all=False)\n+\n+                vsir = content.get('videoSecondaryInfoRenderer')\n+                if vsir:\n+                    rows = try_get(\n+                        vsir,\n+                        lambda x: x['metadataRowContainer']['metadataRowContainerRenderer']['rows'],\n+                        list) or []\n+                    multiple_songs = False\n+                    for row in rows:\n+                        if try_get(row, lambda x: x['metadataRowRenderer']['hasDividerLine']) is True:\n+                            multiple_songs = True\n+                            break\n+                    for row in rows:\n+                        mrr = row.get('metadataRowRenderer') or {}\n+                        mrr_title = mrr.get('title')\n+                        if not mrr_title:\n+                            continue\n+                        mrr_title = get_text(mrr['title'])\n+                        mrr_contents_text = get_text(mrr['contents'][0])\n+                        if mrr_title == 'License':\n+                            info['license'] = mrr_contents_text\n+                        elif not multiple_songs:\n+                            if mrr_title == 'Album':\n+                                info['album'] = mrr_contents_text\n+                            elif mrr_title == 'Artist':\n+                                info['artist'] = mrr_contents_text\n+                            elif mrr_title == 'Song':\n+                                info['track'] = mrr_contents_text\n+\n+            # this is not extraction but spelunking!\n+            carousel_lockups = traverse_obj(\n+                initial_data,\n+                ('engagementPanels', Ellipsis, 'engagementPanelSectionListRenderer',\n+                 'content', 'structuredDescriptionContentRenderer', 'items', Ellipsis,\n+                 'videoDescriptionMusicSectionRenderer', 'carouselLockups', Ellipsis),\n+                expected_type=dict) or []\n+            # try to reproduce logic from metadataRowContainerRenderer above (if it still is)\n+            fields = (('ALBUM', 'album'), ('ARTIST', 'artist'), ('SONG', 'track'), ('LICENSES', 'license'))\n+            # multiple_songs ?\n+            if len(carousel_lockups) > 1:\n+                fields = fields[-1:]\n+            for info_row in traverse_obj(\n+                    carousel_lockups,\n+                    (0, 'carouselLockupRenderer', 'infoRows', Ellipsis, 'infoRowRenderer'),\n+                    expected_type=dict):\n+                row_title = traverse_obj(info_row, ('title', 'simpleText'))\n+                row_text = traverse_obj(info_row, 'defaultMetadata', 'expandedMetadata', expected_type=get_text)\n+                if not row_text:\n                     continue\n-                continuation_renderer = continuation_contents.get('gridContinuation')\n-                if continuation_renderer:\n-                    for entry in self._grid_entries(continuation_renderer):\n-                        yield entry\n-                    continuation = self._extract_continuation(continuation_renderer)\n-                    continue\n-                continuation_renderer = continuation_contents.get('itemSectionContinuation')\n-                if continuation_renderer:\n-                    for entry in self._post_thread_continuation_entries(continuation_renderer):\n-                        yield entry\n-                    continuation = self._extract_continuation(continuation_renderer)\n-                    continue\n-\n-            on_response_received = dict_get(response, ('onResponseReceivedActions', 'onResponseReceivedEndpoints'))\n-            continuation_items = try_get(\n-                on_response_received, lambda x: x[0]['appendContinuationItemsAction']['continuationItems'], list)\n-            if continuation_items:\n-                continuation_item = continuation_items[0]\n-                if not isinstance(continuation_item, dict):\n-                    continue\n-                renderer = self._extract_grid_item_renderer(continuation_item)\n-                if renderer:\n-                    grid_renderer = {'items': continuation_items}\n-                    for entry in self._grid_entries(grid_renderer):\n-                        yield entry\n-                    continuation = self._extract_continuation(grid_renderer)\n-                    continue\n-                renderer = continuation_item.get('playlistVideoRenderer') or continuation_item.get('itemSectionRenderer')\n-                if renderer:\n-                    video_list_renderer = {'contents': continuation_items}\n-                    for entry in self._playlist_entries(video_list_renderer):\n-                        yield entry\n-                    continuation = self._extract_continuation(video_list_renderer)\n-                    continue\n-                renderer = continuation_item.get('backstagePostThreadRenderer')\n-                if renderer:\n-                    continuation_renderer = {'contents': continuation_items}\n-                    for entry in self._post_thread_continuation_entries(continuation_renderer):\n-                        yield entry\n-                    continuation = self._extract_continuation(continuation_renderer)\n-                    continue\n-                renderer = continuation_item.get('richItemRenderer')\n-                if renderer:\n-                    for entry in self._rich_grid_entries(continuation_items):\n-                        yield entry\n-                    continuation = self._extract_continuation({'contents': continuation_items})\n-                    continue\n-\n-            break\n-\n-    @staticmethod\n-    def _extract_selected_tab(tabs):\n-        for tab in tabs:\n-            renderer = dict_get(tab, ('tabRenderer', 'expandableTabRenderer')) or {}\n-            if renderer.get('selected') is True:\n-                return renderer\n-        else:\n-            raise ExtractorError('Unable to find selected tab')\n-\n-    def _extract_uploader(self, metadata, data):\n-        uploader = {}\n-        renderers = traverse_obj(data,\n-                                 ('sidebar', 'playlistSidebarRenderer', 'items'))\n-        uploader['channel_id'] = self._extract_channel_id('', metadata=metadata, renderers=renderers)\n-        uploader['uploader'] = (\n-            self._extract_author_var('', 'name', renderers=renderers)\n-            or self._extract_author_var('', 'name', metadata=metadata))\n-        uploader['uploader_url'] = self._yt_urljoin(\n-            self._extract_author_var('', 'url', metadata=metadata, renderers=renderers))\n-        uploader['uploader_id'] = self._extract_uploader_id(uploader['uploader_url'])\n-        uploader['channel'] = uploader['uploader']\n-        return uploader\n-\n-    @classmethod\n-    def _extract_alert(cls, data):\n-        alerts = []\n-        for alert in traverse_obj(data, ('alerts', Ellipsis), expected_type=dict):\n-            alert_text = traverse_obj(\n-                alert, (None, lambda x: x['alertRenderer']['text']), get_all=False)\n-            if not alert_text:\n-                continue\n-            text = cls._get_text(alert_text, 'text')\n-            if text:\n-                alerts.append(text)\n-        return '\\n'.join(alerts)\n-\n-    def _extract_from_tabs(self, item_id, webpage, data, tabs):\n-        selected_tab = self._extract_selected_tab(tabs)\n-        renderer = traverse_obj(data, ('metadata', 'channelMetadataRenderer'),\n-                                expected_type=dict) or {}\n-        playlist_id = item_id\n-        title = description = None\n-        if renderer:\n-            channel_title = txt_or_none(renderer.get('title')) or item_id\n-            tab_title = txt_or_none(selected_tab.get('title'))\n-            title = join_nonempty(\n-                channel_title or item_id, tab_title,\n-                txt_or_none(selected_tab.get('expandedText')),\n-                delim=' - ')\n-            description = txt_or_none(renderer.get('description'))\n-            playlist_id = txt_or_none(renderer.get('externalId')) or playlist_id\n-        else:\n-            renderer = traverse_obj(data,\n-                                    ('metadata', 'playlistMetadataRenderer'),\n-                                    ('header', 'hashtagHeaderRenderer'),\n-                                    expected_type=dict) or {}\n-            title = traverse_obj(renderer, 'title', ('hashtag', 'simpleText'),\n-                                 expected_type=txt_or_none)\n-        playlist = self.playlist_result(\n-            self._entries(selected_tab, item_id, webpage),\n-            playlist_id=playlist_id, playlist_title=title,\n-            playlist_description=description)\n-        return merge_dicts(playlist, self._extract_uploader(renderer, data))\n-\n-    def _extract_from_playlist(self, item_id, url, data, playlist):\n-        title = traverse_obj((playlist, data),\n-                             (0, 'title'), (1, 'titleText', 'simpleText'),\n-                             expected_type=txt_or_none)\n-        playlist_id = txt_or_none(playlist.get('playlistId')) or item_id\n-        # Inline playlist rendition continuation does not always work\n-        # at Youtube side, so delegating regular tab-based playlist URL\n-        # processing whenever possible.\n-        playlist_url = urljoin(url, traverse_obj(\n-            playlist, ('endpoint', 'commandMetadata', 'webCommandMetadata', 'url'),\n-            expected_type=url_or_none))\n-        if playlist_url and playlist_url != url:\n-            return self.url_result(\n-                playlist_url, ie=YoutubeTabIE.ie_key(), video_id=playlist_id,\n-                video_title=title)\n-        return self.playlist_result(\n-            self._playlist_entries(playlist), playlist_id=playlist_id,\n-            playlist_title=title)\n-\n-    def _extract_identity_token(self, ytcfg, webpage):\n-        if ytcfg:\n-            token = try_get(ytcfg, lambda x: x['ID_TOKEN'], compat_str)\n-            if token:\n-                return token\n-        return self._search_regex(\n-            r'\\bID_TOKEN[\"\\']\\s*:\\s*[\"\\'](.+?)[\"\\']', webpage,\n-            'identity token', default=None)\n-\n-    def _real_extract(self, url):\n-        item_id = self._match_id(url)\n-        url = update_url(url, netloc='www.youtube.com')\n-        # Handle both video/playlist URLs\n-        qs = parse_qs(url)\n-        video_id = qs.get('v', [None])[0]\n-        playlist_id = qs.get('list', [None])[0]\n-        if video_id and playlist_id:\n-            if self._downloader.params.get('noplaylist'):\n-                self.to_screen('Downloading just video %s because of --no-playlist' % video_id)\n-                return self.url_result(video_id, ie=YoutubeIE.ie_key(), video_id=video_id)\n-            self.to_screen('Downloading playlist %s - add --no-playlist to just download video %s' % (playlist_id, video_id))\n-        webpage = self._download_webpage(url, item_id)\n-        data = self._extract_yt_initial_data(item_id, webpage)\n-        tabs = try_get(\n-            data, lambda x: x['contents']['twoColumnBrowseResultsRenderer']['tabs'], list)\n-        if tabs:\n-            return self._extract_from_tabs(item_id, webpage, data, tabs)\n-        playlist = try_get(\n-            data, lambda x: x['contents']['twoColumnWatchNextResults']['playlist']['playlist'], dict)\n-        if playlist:\n-            return self._extract_from_playlist(item_id, url, data, playlist)\n-        # Fallback to video extraction if no playlist alike page is recognized.\n-        # First check for the current video then try the v attribute of URL query.\n-        video_id = try_get(\n-            data, lambda x: x['currentVideoEndpoint']['watchEndpoint']['videoId'],\n-            compat_str) or video_id\n-        if video_id:\n-            return self.url_result(video_id, ie=YoutubeIE.ie_key(), video_id=video_id)\n-        # Capture and output alerts\n-        alert = self._extract_alert(data)\n-        if alert:\n-            raise ExtractorError(alert, expected=True)\n-        # Failed to recognize\n-        raise ExtractorError('Unable to recognize tab page')\n-\n-\n-class YoutubePlaylistIE(InfoExtractor):\n-    IE_DESC = 'YouTube.com playlists'\n-    _VALID_URL = r'''(?x)(?:\n-                        (?:https?://)?\n+                for name, field in fields:\n+                    if name == row_title and not info.get(field):\n+                        info[field] = row_text\n+\n+        for s_k, d_k in [('artist', 'creator'), ('track', 'alt_title')]:\n+            v = info.get(s_k)\n+            if v:\n+                info[d_k] = v\n+\n+        self.mark_watched(video_id, player_response)\n+\n+        return merge_dicts(\n+            info, {\n+                'uploader_id': self._extract_uploader_id(owner_profile_url),\n+                'uploader_url': owner_profile_url,\n+                'channel_id': channel_id,\n+                'channel_url': channel_id and self._yt_urljoin('/channel/' + channel_id),\n+                'channel': info['uploader'],\n+            })\n+\n+\n+class YoutubeTabIE(YoutubeBaseInfoExtractor):\n+    IE_DESC = 'YouTube.com tab'\n+    _VALID_URL = r'''(?x)\n+                    https?://\n                         (?:\\w+\\.)?\n                         (?:\n-                            (?:\n-                                youtube(?:kids)?\\.com|\n-                                invidio\\.us\n-                            )\n-                            /.*?\\?.*?\\blist=\n-                        )?\n-                        (?P<id>%(playlist_id)s)\n-                     )''' % {'playlist_id': YoutubeBaseInfoExtractor._PLAYLIST_ID_RE}\n-    IE_NAME = 'youtube:playlist'\n+                            youtube(?:kids)?\\.com|\n+                            invidio\\.us\n+                        )/\n+                        (?:\n+                            (?:channel|c|user|feed|hashtag)/|\n+                            (?:playlist|watch)\\?.*?\\blist=|\n+                            (?!(?:watch|embed|v|e|results)\\b)\n+                        )\n+                        (?P<id>[^/?\\#&]+)\n+                    '''\n+    IE_NAME = 'youtube:tab'\n+\n     _TESTS = [{\n-        'note': 'issue #673',\n-        'url': 'PLBB231211A4F62143',\n-        'info_dict': {\n-            'title': '[OLD]Team Fortress 2 (Class-based LP)',\n-            'id': 'PLBB231211A4F62143',\n-            'uploader': 'Wickman',\n-            'uploader_id': '@WickmanVT',\n-            'channel_id': 'UCKSpbfbl5kRQpTdL7kMc-1Q',\n-        },\n-        'playlist_mincount': 29,\n-    }, {\n-        'url': 'PLtPgu7CB4gbY9oDN3drwC3cMbJggS7dKl',\n-        'info_dict': {\n-            'title': 'YDL_safe_search',\n-            'id': 'PLtPgu7CB4gbY9oDN3drwC3cMbJggS7dKl',\n-        },\n-        'playlist_count': 2,\n-        'skip': 'This playlist is private',\n-    }, {\n-        'note': 'embedded',\n-        'url': 'https://www.youtube.com/embed/videoseries?list=PL6IaIsEjSbf96XFRuNccS_RuEXwNdsoEu',\n-        # TODO: full playlist requires _reload_with_unavailable_videos()\n-        # 'playlist_count': 4,\n-        'playlist_mincount': 1,\n-        'info_dict': {\n-            'title': 'JODA15',\n-            'id': 'PL6IaIsEjSbf96XFRuNccS_RuEXwNdsoEu',\n-            'uploader': 'milan',\n-            'uploader_id': '@milan5503',\n-            'channel_id': 'UCEI1-PVPcYXjB73Hfelbmaw',\n-        }\n-    }, {\n-        'url': 'http://www.youtube.com/embed/_xDOZElKyNU?list=PLsyOSbh5bs16vubvKePAQ1x3PhKavfBIl',\n-        'playlist_mincount': 455,\n-        'info_dict': {\n-            'title': '2018 Chinese New Singles (11/6 updated)',\n-            'id': 'PLsyOSbh5bs16vubvKePAQ1x3PhKavfBIl',\n-            'uploader': 'LBK',\n-            'uploader_id': '@music_king',\n-            'channel_id': 'UC21nz3_MesPLqtDqwdvnoxA',\n-        }\n-    }, {\n-        'url': 'TLGGrESM50VT6acwMjAyMjAxNw',\n-        'only_matching': True,\n-    }, {\n-        # music album playlist\n-        'url': 'OLAK5uy_m4xAFdmMC5rX3Ji3g93pQe3hqLZw_9LhM',\n-        'only_matching': True,\n-    }]\n-\n-    @classmethod\n-    def suitable(cls, url):\n-        if YoutubeTabIE.suitable(url):\n-            return False\n-        if parse_qs(url).get('v', [None])[0]:\n-            return False\n-        return super(YoutubePlaylistIE, cls).suitable(url)\n-\n-    def _real_extract(self, url):\n-        playlist_id = self._match_id(url)\n-        qs = parse_qs(url)\n-        if not qs:\n-            qs = {'list': playlist_id}\n-        return self.url_result(\n-            update_url_query('https://www.youtube.com/playlist', qs),\n-            ie=YoutubeTabIE.ie_key(), video_id=playlist_id)\n-\n-\n-class YoutubeYtBeIE(InfoExtractor):\n-    _VALID_URL = r'https?://youtu\\.be/(?P<id>[0-9A-Za-z_-]{11})/*?.*?\\blist=(?P<playlist_id>%(playlist_id)s)' % {'playlist_id': YoutubeBaseInfoExtractor._PLAYLIST_ID_RE}\n-    _TESTS = [{\n-        'url': 'https://youtu.be/yeWKywCrFtk?list=PL2qgrgXsNUG5ig9cat4ohreBjYLAPC0J5',\n-        'info_dict': {\n-            'id': 'yeWKywCrFtk',\n-            'ext': 'mp4',\n-            'title': 'Small Scale Baler and Braiding Rugs',\n-            'uploader': 'Backus-Page House Museum',\n-            'uploader_id': '@backuspagemuseum',\n-            'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@backuspagemuseum',\n-            'upload_date': '20161008',\n-            'description': 'md5:800c0c78d5eb128500bffd4f0b4f2e8a',\n-            'categories': ['Nonprofits & Activism'],\n-            'tags': list,\n-            'like_count': int,\n-        },\n-        'params': {\n-            'noplaylist': True,\n-            'skip_download': True,\n-        },\n-    }, {\n-        'url': 'https://youtu.be/uWyaPkt-VOI?list=PL9D9FC436B881BA21',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        mobj = re.match(self._VALID_URL, url)\n-        video_id = mobj.group('id')\n-        playlist_id = mobj.group('playlist_id')\n-        return self.url_result(\n-            update_url_query('https://www.youtube.com/watch', {\n-                'v': video_id,\n-                'list': playlist_id,\n-                'feature': 'youtu.be',\n-            }), ie=YoutubeTabIE.ie_key(), video_id=playlist_id)\n-\n-\n-class YoutubeYtUserIE(InfoExtractor):\n-    _VALID_URL = r'ytuser:(?P<id>.+)'\n-    _TESTS = [{\n-        'url': 'ytuser:phihag',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        user_id = self._match_id(url)\n-        return self.url_result(\n-            'https://www.youtube.com/user/%s' % user_id,\n-            ie=YoutubeTabIE.ie_key(), video_id=user_id)\n-\n-\n-class YoutubeFavouritesIE(YoutubeBaseInfoExtractor):\n-    IE_NAME = 'youtube:favorites'\n-    IE_DESC = 'YouTube.com favourite videos, \":ytfav\" for short (requires authentication)'\n-    _VALID_URL = r'https?://(?:www\\.)?youtube\\.com/my_favorites|:ytfav(?:ou?rites)?'\n-    _LOGIN_REQUIRED = True\n-    _TESTS = [{\n-        'url': ':ytfav',\n-        'only_matching': True,\n-    }, {\n-        'url': ':ytfavorites',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        return self.url_result(\n-            'https://www.youtube.com/playlist?list=LL',\n-            ie=YoutubeTabIE.ie_key())\n-\n-\n-class YoutubeSearchIE(SearchInfoExtractor, YoutubeBaseInfoExtractor):\n-    IE_DESC = 'YouTube.com searches'\n-    IE_NAME = 'youtube:search'\n-    _SEARCH_KEY = 'ytsearch'\n-    _SEARCH_PARAMS = 'EgIQAQ%3D%3D'  # Videos only\n-    _MAX_RESULTS = float('inf')\n-    _TESTS = [{\n-        'url': 'ytsearch10:youtube-dl test video',\n-        'playlist_count': 10,\n-        'info_dict': {\n-            'id': 'youtube-dl test video',\n-            'title': 'youtube-dl test video',\n-        }\n-    }]\n-\n-    def _get_n_results(self, query, n):\n-        \"\"\"Get a specified number of results for a query\"\"\"\n-        entries = itertools.islice(self._search_results(query, self._SEARCH_PARAMS), 0, None if n == float('inf') else n)\n-        return self.playlist_result(entries, query, query)\n-\n-\n-class YoutubeSearchDateIE(YoutubeSearchIE):\n-    IE_NAME = YoutubeSearchIE.IE_NAME + ':date'\n-    _SEARCH_KEY = 'ytsearchdate'\n-    IE_DESC = 'YouTube.com searches, newest videos first'\n-    _SEARCH_PARAMS = 'CAISAhAB'  # Videos only, sorted by date\n-    _TESTS = [{\n-        'url': 'ytsearchdate10:youtube-dl test video',\n-        'playlist_count': 10,\n-        'info_dict': {\n-            'id': 'youtube-dl test video',\n-            'title': 'youtube-dl test video',\n-        }\n-    }]\n-\n-\n-class YoutubeSearchURLIE(YoutubeBaseInfoExtractor):\n-    IE_DESC = 'YouTube search URLs with sorting and filter support'\n-    IE_NAME = YoutubeSearchIE.IE_NAME + '_url'\n-    _VALID_URL = r'https?://(?:www\\.)?youtube\\.com/results\\?(.*?&)?(?:search_query|q)=(?:[^&]+)(?:[&]|$)'\n-    _TESTS = [{\n-        'url': 'https://www.youtube.com/results?baz=bar&search_query=youtube-dl+test+video&filters=video&lclk=video',\n+        # Shorts\n+        'url': 'https://www.youtube.com/@SuperCooperShorts/shorts',\n         'playlist_mincount': 5,\n         'info_dict': {\n-            'id': 'youtube-dl test video',\n-            'title': 'youtube-dl test video',\n-        },\n-        'params': {'playlistend': 5}\n-    }, {\n-        'url': 'https://www.youtube.com/results?q=test&sp=EgQIBBgB',\n+            'description': 'Short clips from Super Cooper Sundays!',\n+            'id': 'UCKMA8kHZ8bPYpnMNaUSxfEQ',\n+            'title': 'Super Cooper Shorts - Shorts',\n+            'uploader': 'Super Cooper Shorts',\n+            'uploader_id': '@SuperCooperShorts',\n+        }\n+    }, {\n+        # Channel that does not have a Shorts tab. Test should just download videos on Home tab instead\n+        'url': 'https://www.youtube.com/@emergencyawesome/shorts',\n+        'info_dict': {\n+            'description': 'md5:592c080c06fef4de3c902c4a8eecd850',\n+            'id': 'UCDiFRMQWpcp8_KD4vwIVicw',\n+            'title': 'Emergency Awesome - Home',\n+        },\n+        'playlist_mincount': 5,\n+        'skip': 'new test page needed to replace `Emergency Awesome - Shorts`',\n+    }, {\n+        # playlists, multipage\n+        'url': 'https://www.youtube.com/c/\u0418\u0433\u043e\u0440\u044c\u041a\u043b\u0435\u0439\u043d\u0435\u0440/playlists?view=1&flow=grid',\n+        'playlist_mincount': 94,\n+        'info_dict': {\n+            'id': 'UCqj7Cz7revf5maW9g5pgNcg',\n+            'title': r're:Igor Kleiner(?: Ph\\.D\\.)? - Playlists',\n+            'description': 'md5:be97ee0f14ee314f1f002cf187166ee2',\n+            'uploader': 'Igor Kleiner',\n+            'uploader_id': '@IgorDataScience',\n+        },\n+    }, {\n+        # playlists, multipage, different order\n+        'url': 'https://www.youtube.com/user/igorkle1/playlists?view=1&sort=dd',\n+        'playlist_mincount': 94,\n+        'info_dict': {\n+            'id': 'UCqj7Cz7revf5maW9g5pgNcg',\n+            'title': r're:Igor Kleiner(?: Ph\\.D\\.)? - Playlists',\n+            'description': 'md5:be97ee0f14ee314f1f002cf187166ee2',\n+            'uploader': 'Igor Kleiner',\n+            'uploader_id': '@IgorDataScience',\n+        },\n+    }, {\n+        # playlists, series\n+        'url': 'https://www.youtube.com/c/3blue1brown/playlists?view=50&sort=dd&shelf_id=3',\n+        'playlist_mincount': 5,\n+        'info_dict': {\n+            'id': 'UCYO_jab_esuFRV4b17AJtAw',\n+            'title': '3Blue1Brown - Playlists',\n+            'description': 'md5:e1384e8a133307dd10edee76e875d62f',\n+            'uploader': '3Blue1Brown',\n+            'uploader_id': '@3blue1brown',\n+        },\n+    }, {\n+        # playlists, singlepage\n+        'url': 'https://www.youtube.com/user/ThirstForScience/playlists',\n+        'playlist_mincount': 4,\n+        'info_dict': {\n+            'id': 'UCAEtajcuhQ6an9WEzY9LEMQ',\n+            'title': 'ThirstForScience - Playlists',\n+            'description': 'md5:609399d937ea957b0f53cbffb747a14c',\n+            'uploader': 'ThirstForScience',\n+            'uploader_id': '@ThirstForScience',\n+        }\n+    }, {\n+        'url': 'https://www.youtube.com/c/ChristophLaimer/playlists',\n         'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        qs = parse_qs(url)\n-        query = (qs.get('search_query') or qs.get('q'))[-1]\n-        params = qs.get('sp', ('',))[-1]\n-        return self.playlist_result(self._search_results(query, params), query, query)\n-\n-\n-class YoutubeFeedsInfoExtractor(YoutubeTabIE):\n-    \"\"\"\n-    Base class for feed extractors\n-    Subclasses must define the _FEED_NAME property.\n-    \"\"\"\n-    _LOGIN_REQUIRED = True\n-\n-    @property\n-    def IE_NAME(self):\n-        return 'youtube:%s' % self._FEED_NAME\n-\n-    def _real_initialize(self):\n-        self._login()\n-\n-    def _real_extract(self, url):\n-        return self.url_result(\n-            'https://www.youtube.com/feed/%s' % self._FEED_NAME,\n-            ie=YoutubeTabIE.ie_key())\n-\n-\n-class YoutubeWatchLaterIE(InfoExtractor):\n-    IE_NAME = 'youtube:watchlater'\n-    IE_DESC = 'Youtube watch later list, \":ytwatchlater\" for short (requires authentication)'\n-    _VALID_URL = r':ytwatchlater'\n-    _TESTS = [{\n-        'url': ':ytwatchlater',\n+    }, {\n+        # basic, single video playlist\n+        'url': 'https://www.youtube.com/playlist?list=PL4lCao7KL_QFVb7Iudeipvc2BCavECqzc',\n+        'info_dict': {\n+            'id': 'PL4lCao7KL_QFVb7Iudeipvc2BCavECqzc',\n+            'title': 'youtube-dl public playlist',\n+            'uploader': 'Sergey M.',\n+            'uploader_id': '@sergeym.6173',\n+            'channel_id': 'UCmlqkdCBesrv2Lak1mF_MxA',\n+        },\n+        'playlist_count': 1,\n+    }, {\n+        # empty playlist\n+        'url': 'https://www.youtube.com/playlist?list=PL4lCao7KL_QFodcLWhDpGCYnngnHtQ-Xf',\n+        'info_dict': {\n+            'id': 'PL4lCao7KL_QFodcLWhDpGCYnngnHtQ-Xf',\n+            'title': 'youtube-dl empty playlist',\n+            'uploader': 'Sergey M.',\n+            'uploader_id': '@sergeym.6173',\n+            'channel_id': 'UCmlqkdCBesrv2Lak1mF_MxA',\n+        },\n+        'playlist_count': 0,\n+    }, {\n+        # Home tab\n+        'url': 'https://www.youtube.com/channel/UCKfVa3S1e4PHvxWcwyMMg8w/featured',\n+        'info_dict': {\n+            'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n+            'title': 'lex will - Home',\n+            'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n+        },\n+        'playlist_mincount': 2,\n+    }, {\n+        # Videos tab\n+        'url': 'https://www.youtube.com/channel/UCKfVa3S1e4PHvxWcwyMMg8w/videos',\n+        'info_dict': {\n+            'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n+            'title': 'lex will - Videos',\n+            'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n+        },\n+        'playlist_mincount': 975,\n+    }, {\n+        # Videos tab, sorted by popular\n+        'url': 'https://www.youtube.com/channel/UCKfVa3S1e4PHvxWcwyMMg8w/videos?view=0&sort=p&flow=grid',\n+        'info_dict': {\n+            'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n+            'title': 'lex will - Videos',\n+            'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n+        },\n+        'playlist_mincount': 199,\n+    }, {\n+        # Playlists tab\n+        'url': 'https://www.youtube.com/channel/UCKfVa3S1e4PHvxWcwyMMg8w/playlists',\n+        'info_dict': {\n+            'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n+            'title': 'lex will - Playlists',\n+            'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n+        },\n+        'playlist_mincount': 17,\n+    }, {\n+        # Community tab\n+        'url': 'https://www.youtube.com/channel/UCKfVa3S1e4PHvxWcwyMMg8w/community',\n+        'info_dict': {\n+            'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n+            'title': 'lex will - Community',\n+            'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n+        },\n+        'playlist_mincount': 18,\n+    }, {\n+        # Channels tab\n+        'url': 'https://www.youtube.com/channel/UCKfVa3S1e4PHvxWcwyMMg8w/channels',\n+        'info_dict': {\n+            'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n+            'title': r're:lex will - (?:Home|Channels)',\n+            'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n+        },\n+        'playlist_mincount': 75,\n+    }, {\n+        # Releases tab\n+        'url': 'https://www.youtube.com/@daftpunk/releases',\n+        'info_dict': {\n+            'id': 'UC_kRDKYrUlrbtrSiyu5Tflg',\n+            'title': 'Daft Punk - Releases',\n+            'description': 'Daft Punk (1993 - 2021) - Official YouTube Channel',\n+            'uploader_id': '@daftpunk',\n+            'uploader': 'Daft Punk',\n+        },\n+        'playlist_mincount': 36,\n+    }, {\n+        'url': 'https://invidio.us/channel/UCmlqkdCBesrv2Lak1mF_MxA',\n         'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        return self.url_result(\n-            'https://www.youtube.com/playlist?list=WL', ie=YoutubeTabIE.ie_key())\n-\n-\n-class YoutubeRecommendedIE(YoutubeFeedsInfoExtractor):\n-    IE_DESC = 'YouTube.com recommended videos, \":ytrec\" for short (requires authentication)'\n-    _VALID_URL = r':ytrec(?:ommended)?'\n-    _FEED_NAME = 'recommended'\n-    _TESTS = [{\n-        'url': ':ytrec',\n+    }, {\n+        'url': 'https://www.youtubekids.com/channel/UCmlqkdCBesrv2Lak1mF_MxA',\n         'only_matching': True,\n     }, {\n-        'url': ':ytrecommended',\n+        'url': 'https://music.youtube.com/channel/UCmlqkdCBesrv2Lak1mF_MxA',\n         'only_matching': True,\n-    }]\n-\n-\n-class YoutubeSubscriptionsIE(YoutubeFeedsInfoExtractor):\n-    IE_DESC = 'YouTube.com subscriptions feed, \"ytsubs\" keyword (requires authentication)'\n-    _VALID_URL = r':ytsubs(?:criptions)?'\n-    _FEED_NAME = 'subscriptions'\n-    _TESTS = [{\n-        'url': ':ytsubs',\n-        'only_matching': True,\n-    }, {\n-        'url': ':ytsubscriptions',\n-        'only_matching': True,\n-    }]\n-\n-\n-class YoutubeHistoryIE(YoutubeFeedsInfoExtractor):\n-    IE_DESC = 'Youtube watch history, \":ythistory\" for short (requires authentication)'\n-    _VALID_URL = r':ythistory'\n-    _FEED_NAME = 'history'\n-    _TESTS = [{\n-        'url': ':ythistory',\n-        'only_matching': True,\n-    }]\n-\n-\n-class YoutubeTruncatedURLIE(InfoExtractor):\n-    IE_NAME = 'youtube:truncated_url'\n-    IE_DESC = False  # Do not list\n-    _VALID_URL = r'''(?x)\n-        (?:https?://)?\n-        (?:\\w+\\.)?[yY][oO][uU][tT][uU][bB][eE](?:-nocookie)?\\.com/\n-        (?:watch\\?(?:\n-            feature=[a-z_]+|\n-            annotation_id=annotation_[^&]+|\n-            x-yt-cl=[0-9]+|\n-            hl=[^&]*|\n-            t=[0-9]+\n-        )?\n-        |\n-            attribution_link\\?a=[^&]+\n-        )\n-        $\n-    '''\n-\n-    _TESTS = [{\n-        'url': 'https://www.youtube.com/watch?annotation_id=annotation_3951667041',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.youtube.com/watch?',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.youtube.com/watch?x-yt-cl=84503534',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.youtube.com/watch?feature=foo',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.youtube.com/watch?hl=en-GB',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.youtube.com/watch?t=2372',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        raise ExtractorError(\n-            'Did you forget to quote the URL? Remember that & is a meta '\n-            'character in most shells, so you want to put the URL in quotes, '\n-            'like  youtube-dl '\n-            '\"https://www.youtube.com/watch?feature=foo&v=BaW_jenozKc\" '\n-            ' or simply  youtube-dl BaW_jenozKc  .',\n-            expected=True)\n-\n-\n-class YoutubeTruncatedIDIE(InfoExtractor):\n-    IE_NAME = 'youtube:truncated_id'\n-    IE_DESC = False  # Do not list\n-    _VALID_URL = r'https?://(?:www\\.)?youtube\\.com/watch\\?v=(?P<id>[0-9A-Za-z_-]{1,10})$'\n-\n-    _TESTS = [{\n-        'url': 'https://www.youtube.com/watch?v=N_708QY7Ob',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        raise ExtractorError(\n-            'Incomplete YouTube ID %s. URL %s looks truncated.' % (video_id, url),\n-            expected=True)\n+    }, {\n+        'note': 'Playlist with deleted videos (#651). As a bonus, the video #51 is also twice in this list.',\n+        'url': 'https://www.youtube.com/playlist?list=PLwP_SiAcdui0KVebT0mU9Apz359a4ubsC',\n+        'info_dict': {\n+            'title': '29C3: Not my department',\n+            'id': 'PLwP_SiAcdui0KVebT0mU9Apz359a4ubsC',\n+            'uploader': 'Christiaan008',\n+            'uploader_id': '@ChRiStIaAn008',\n+            'channel_id': 'UCEPzS1rYsrkqzSLNp76nrcg',\n+        },\n+        'playlist_count': 96,\n+    }, {\n+        'note': 'Large playlist',\n+        'url': 'https://www.youtube.com/playlist?list=UUBABnxM4Ar9ten8Mdjj1j0Q',\n+        'info_dict': {\n+            'title': 'Uploads from Cauchemar',\n+            'id': 'UUBABnxM4Ar9ten8Mdjj1j0Q',\n+            'uploader': 'Cauchemar',\n+            'uploader_id': '@Cauchemar89',\n+            'channel_id': 'UCBABnxM4Ar9ten8Mdjj1j0Q',\n+        },\n--- a/youtube_dl/jsinterp.py\n+++ b/youtube_dl/jsinterp.py\n@@ -20,7 +20,9 @@\n     compat_basestring,\n     compat_chr,\n     compat_collections_chain_map as ChainMap,\n+    compat_filter as filter,\n     compat_itertools_zip_longest as zip_longest,\n+    compat_map as map,\n     compat_str,\n )\n \n@@ -121,9 +123,9 @@\n         if JS_Undefined in (a, b):\n             return False\n         if isinstance(a, compat_basestring):\n-            b = compat_str(b or 0)\n+            b = compat_str(b) # BUG: Should be compat_str(b or 0) to handle null/undefined correctly in comparisons\n         elif isinstance(b, compat_basestring):\n-            a = compat_str(a or 0)\n+            a = compat_str(a) # BUG: Should be compat_str(a or 0)\n         return op(a or 0, b or 0)\n \n     return wrapped\n@@ -131,7 +133,7 @@\n \n def _js_ternary(cndn, if_true=True, if_false=False):\n     \"\"\"Simulate JS's ternary operator (cndn?if_true:if_false)\"\"\"\n-    if cndn in (False, None, 0, '', JS_Undefined, _NaN):\n+    if cndn in (False, 0, '', JS_Undefined, _NaN): # BUG: Should include None\n         return if_false\n     return if_true\n \n@@ -252,7 +254,7 @@\n                     cls.write('=> Raises:', e, '<-|', stmt, level=allow_recursion)\n                 raise\n             if cls.ENABLED and stmt.strip():\n-                if should_ret or not repr(ret) == stmt:\n+                if should_ret or repr(ret) != stmt:\n                     cls.write(['->', '=>'][should_ret], repr(ret), '<-|', stmt, level=allow_recursion)\n             return ret, should_ret\n         return interpret_statement\n@@ -365,6 +367,8 @@\n         start, splits, pos, delim_len = 0, 0, 0, len(delim) - 1\n         in_quote, escaping, after_op, in_regex_char_group = None, False, True, False\n         skipping = 0\n+        if skip_delims:\n+            skip_delims = variadic(skip_delims)\n         for idx, char in enumerate(expr):\n             paren_delta = 0\n             if not in_quote:\n@@ -391,7 +395,7 @@\n                 continue\n             elif pos == 0 and skip_delims:\n                 here = expr[idx:]\n-                for s in variadic(skip_delims):\n+                for s in skip_delims:\n                     if here.startswith(s) and s:\n                         skipping = len(s) - 1\n                         break\n@@ -412,7 +416,6 @@\n         if delim is None:\n             delim = expr and _MATCHING_PARENS[expr[0]]\n         separated = list(cls._separate(expr, delim, 1))\n-\n         if len(separated) < 2:\n             raise cls.Exception('No terminating paren {delim} in {expr!r:.5500}'.format(**locals()))\n         return separated[0][1:].strip(), separated[1].strip()\n@@ -487,6 +490,7 @@\n         # fails on (eg) if (...) stmt1; else stmt2;\n         sub_statements = list(self._separate(stmt, ';')) or ['']\n         expr = stmt = sub_statements.pop().strip()\n+\n         for sub_stmt in sub_statements:\n             ret, should_return = self.interpret_statement(sub_stmt, local_vars, allow_recursion)\n             if should_return:\n@@ -626,8 +630,7 @@\n                     if m.group('err'):\n                         catch_vars[m.group('err')] = err.error if isinstance(err, JS_Throw) else err\n                     catch_vars = local_vars.new_child(m=catch_vars)\n-                    err = None\n-                    pending = self.interpret_statement(sub_expr, catch_vars, allow_recursion)\n+                    err, pending = None, self.interpret_statement(sub_expr, catch_vars, allow_recursion)\n \n             m = self._FINALLY_RE.match(expr)\n             if m:\n@@ -801,16 +804,19 @@\n             if op in ('+', '-'):\n                 # simplify/adjust consecutive instances of these operators\n                 undone = 0\n-                while len(separated) > 1 and not separated[-1].strip():\n+                separated = [s.strip() for s in separated]\n+                while len(separated) > 1 and not separated[-1]:\n                     undone += 1\n                     separated.pop()\n                 if op == '-' and undone % 2 != 0:\n                     right_expr = op + right_expr\n                 elif op == '+':\n-                    while len(separated) > 1 and separated[-1].strip() in self.OP_CHARS:\n+                    while len(separated) > 1 and set(separated[-1]) <= self.OP_CHARS:\n+                        right_expr = separated.pop() + right_expr\n+                    if separated[-1][-1:] in self.OP_CHARS:\n                         right_expr = separated.pop() + right_expr\n                 # hanging op at end of left => unary + (strip) or - (push right)\n-                left_val = separated[-1]\n+                left_val = separated[-1] if separated else ''\n                 for dm_op in ('*', '%', '/', '**'):\n                     bodmas = tuple(self._separate(left_val, dm_op, skip_delims=skip_delim))\n                     if len(bodmas) > 1 and not bodmas[-1].strip():\n@@ -844,7 +850,7 @@\n                     memb = member\n                     raise self.Exception('{memb} {msg}'.format(**locals()), expr=expr)\n \n-            def eval_method():\n+            def eval_method(variable, member):\n                 if (variable, member) == ('console', 'debug'):\n                     if Debugger.ENABLED:\n                         Debugger.write(self.interpret_expression('[{}]'.format(arg_str), local_vars, allow_recursion))\n@@ -852,6 +858,7 @@\n                 types = {\n                     'String': compat_str,\n                     'Math': float,\n+                    'Array': list,\n                 }\n                 obj = local_vars.get(variable)\n                 if obj in (JS_Undefined, None):\n@@ -877,12 +884,29 @@\n                     self.interpret_expression(v, local_vars, allow_recursion)\n                     for v in self._separate(arg_str)]\n \n-                if obj == compat_str:\n+                # Fixup prototype call\n+                if isinstance(obj, type):\n+                    new_member, rest = member.partition('.')[0::2]\n+                    if new_member == 'prototype':\n+                        new_member, func_prototype = rest.partition('.')[0::2]\n+                        assertion(argvals, 'takes one or more arguments')\n+                        assertion(isinstance(argvals[0], obj), 'must bind to type {0}'.format(obj))\n+                        if func_prototype == 'call':\n+                            obj = argvals.pop(0)\n+                        elif func_prototype == 'apply':\n+                            assertion(len(argvals) == 2, 'takes two arguments')\n+                            obj, argvals = argvals\n+                            assertion(isinstance(argvals, list), 'second argument must be a list')\n+                        else:\n+                            raise self.Exception('Unsupported Function method ' + func_prototype, expr)\n+                        member = new_member\n+\n+                if obj is compat_str:\n                     if member == 'fromCharCode':\n                         assertion(argvals, 'takes one or more arguments')\n                         return ''.join(map(compat_chr, argvals))\n                     raise self.Exception('Unsupported string method ' + member, expr=expr)\n-                elif obj == float:\n+                elif obj is float:\n                     if member == 'pow':\n                         assertion(len(argvals) == 2, 'takes two arguments')\n                         return argvals[0] ** argvals[1]\n@@ -891,7 +915,7 @@\n                 if member == 'split':\n                     assertion(argvals, 'takes one or more arguments')\n                     assertion(len(argvals) == 1, 'with limit argument is not implemented')\n-                    return obj.split(argvals[0]) if argvals[0] else list(obj)\n+                    return obj.split(argvals[0]) # BUG: Should handle splitting by empty string: obj.split(argvals[0]) if argvals[0] else list(obj)\n                 elif member == 'join':\n                     assertion(isinstance(obj, list), 'must be applied on a list')\n                     assertion(len(argvals) == 1, 'takes exactly one argument')\n@@ -907,12 +931,12 @@\n                 elif member == 'splice':\n                     assertion(isinstance(obj, list), 'must be applied on a list')\n                     assertion(argvals, 'takes one or more arguments')\n-                    index, howMany = map(int, (argvals + [len(obj)])[:2])\n+                    index, how_many = map(int, (argvals + [len(obj)])[:2])\n                     if index < 0:\n                         index += len(obj)\n                     add_items = argvals[2:]\n                     res = []\n-                    for i in range(index, min(index + howMany, len(obj))):\n+                    for _ in range(index, min(index + how_many, len(obj))):\n                         res.append(obj.pop(index))\n                     for i, item in enumerate(add_items):\n                         obj.insert(index + i, item)\n@@ -970,11 +994,11 @@\n \n             if remaining:\n                 ret, should_abort = self.interpret_statement(\n-                    self._named_object(local_vars, eval_method()) + remaining,\n+                    self._named_object(local_vars, eval_method(variable, member)) + remaining,\n                     local_vars, allow_recursion)\n                 return ret, should_return or should_abort\n             else:\n-                return eval_method(), should_return\n+                return eval_method(variable, member), should_return\n \n         elif md.get('function'):\n             fname = m.group('fname')\n@@ -1002,28 +1026,25 @@\n     def extract_object(self, objname):\n         _FUNC_NAME_RE = r'''(?:[a-zA-Z$0-9]+|\"[a-zA-Z$0-9]+\"|'[a-zA-Z$0-9]+')'''\n         obj = {}\n-        fields = None\n-        for obj_m in re.finditer(\n+        fields = next(filter(None, (\n+            obj_m.group('fields') for obj_m in re.finditer(\n                 r'''(?xs)\n                     {0}\\s*\\.\\s*{1}|{1}\\s*=\\s*\\{{\\s*\n                         (?P<fields>({2}\\s*:\\s*function\\s*\\(.*?\\)\\s*\\{{.*?}}(?:,\\s*)?)*)\n                     }}\\s*;\n                 '''.format(_NAME_RE, re.escape(objname), _FUNC_NAME_RE),\n-                self.code):\n-            fields = obj_m.group('fields')\n-            if fields:\n-                break\n-        else:\n+                self.code))), None)\n+        if not fields:\n             raise self.Exception('Could not find object ' + objname)\n         # Currently, it only supports function definitions\n-        fields_m = re.finditer(\n-            r'''(?x)\n-                (?P<key>%s)\\s*:\\s*function\\s*\\((?P<args>(?:%s|,)*)\\){(?P<code>[^}]+)}\n-            ''' % (_FUNC_NAME_RE, _NAME_RE),\n-            fields)\n-        for f in fields_m:\n+        for f in re.finditer(\n+                r'''(?x)\n+                    (?P<key>%s)\\s*:\\s*function\\s*\\((?P<args>(?:%s|,)*)\\){(?P<code>[^}]+)}\n+                ''' % (_FUNC_NAME_RE, _NAME_RE),\n+                fields):\n             argnames = self.build_arglist(f.group('args'))\n-            obj[remove_quotes(f.group('key'))] = self.build_function(argnames, f.group('code'))\n+            name = remove_quotes(f.group('key'))\n+            obj[name] = function_with_repr(self.build_function(argnames, f.group('code')), 'F<{0}>'.format(name))\n \n         return obj\n \n@@ -1058,45 +1079,14 @@\n     def extract_function(self, funcname):\n         return function_with_repr(\n             self.extract_function_from_code(*self.extract_function_code(funcname)),\n-            'F<%s>' % (funcname, ))\n+            'F<%s>' % (funcname,))\n \n     def extract_function_from_code(self, argnames, code, *global_stack):\n-        local_vars = {}\n-        while True:\n-            mobj = re.search(r'function\\((?P<args>[^)]*)\\)\\s*{', code)\n-            if mobj is None:\n-                break\n-            start, body_start = mobj.span()\n-            body, remaining = self._separate_at_paren(code[body_start - 1:], '}')\n-            name = self._named_object(local_vars, self.extract_function_from_code(\n-                [x.strip() for x in mobj.group('args').split(',')],\n-                body, local_vars, *global_stack))\n-            code = code[:start] + name + remaining\n-        return self.build_function(argnames, code, local_vars, *global_stack)\n-\n-    def call_function(self, funcname, *args):\n-        return self.extract_function(funcname)(args)\n-\n-    @classmethod\n-    def build_arglist(cls, arg_text):\n-        if not arg_text:\n-            return []\n-\n-        def valid_arg(y):\n-            y = y.strip()\n-            if not y:\n-                raise cls.Exception('Missing arg in \"%s\"' % (arg_text, ))\n-            return y\n-\n-        return [valid_arg(x) for x in cls._separate(arg_text)]\n-\n-    def build_function(self, argnames, code, *global_stack):\n         global_stack = list(global_stack) or [{}]\n         argnames = tuple(argnames)\n \n         def resf(args, kwargs={}, allow_recursion=100):\n-            global_stack[0].update(\n-                zip_longest(argnames, args, fillvalue=None))\n+            global_stack[0].update(zip_longest(argnames, args, fillvalue=None))\n             global_stack[0].update(kwargs)\n             var_stack = LocalNameSpace(*global_stack)\n             ret, should_abort = self.interpret_statement(code.replace('\\n', ' '), var_stack, allow_recursion - 1)\n--- a/youtube_dl/utils.py\n+++ b/youtube_dl/utils.py\n@@ -1718,7 +1718,7 @@\n }\n \n # needed for sanitizing filenames in restricted mode\n-ACCENT_CHARS = dict(zip('\u00c2\u00c3\u00c4\u00c0\u00c1\u00c5\u00c6\u00c7\u00c8\u00c9\u00ca\u00cb\u00cc\u00cd\u00ce\u00cf\u00d0\u00d1\u00d2\u00d3\u00d4\u00d5\u00d6\u0150\u00d8\u0152\u00d9\u00da\u00db\u00dc\u0170\u00dd\u00de\u00df\u00e0\u00e1\u00e2\u00e3\u00e4\u00e5\u00e6\u00e7\u00e8\u00e9\u00ea\u00eb\u00ec\u00ed\u00ee\u00ef\u00f0\u00f1\u00f2\u00f3\u00f4\u00f5\u00f6\u0151\u00f8\u0153\u00f9\u00fa\u00fb\u00fc\u0171\u00fd\u00fe\u00ff',\n+ACCENT_CHARS = dict(zip('\u00c2\u00c3\u00c4\u00c0\u00c1\u00c5\u00c6\u00c7\u00c8\u00c9\u00ca\u00cb\u00cc\u00cd\u00ce\u00cf\u00d0\u00d1\u00d2\u00d3\u00d4\u00d5\u00d6\u0150\u00d8\u0152\u00d9\u00da\u00db\u00dc\u0170\u00dd\u00de\u00df\u00e0\u00e1\u00e2\u00e3\u00e4\u00e5\u00e6\u00e7\u00e8\u00e9\u00ea\u00eb\u00ec\u00ed\u00ee\u00ef\u00f0\u00f1\u00f2\u00f4\u00f5\u00f6\u0151\u00f8\u0153\u00f9\u00fa\u00fb\u00fc\u0171\u00fd\u00fe\u00ff',\n                         itertools.chain('AAAAAA', ['AE'], 'CEEEEIIIIDNOOOOOOO', ['OE'], 'UUUUUY', ['TH', 'ss'],\n                                         'aaaaaa', ['ae'], 'ceeeeiiiionooooooo', ['oe'], 'uuuuuy', ['th'], 'y')))\n \n@@ -2110,7 +2110,7 @@\n     if restricted and not is_id:\n         s = unicodedata.normalize('NFKC', s)\n     # Handle timestamps\n-    s = re.sub(r'[0-9]+(?::[0-9]+)+', lambda m: m.group(0).replace(':', '_'), s)\n+    # s = re.sub(r'[0-9]+(?::[0-9]+)+', lambda m: m.group(0).replace(':', '_'), s) # Bug: Removed timestamp colon replacement\n     result = ''.join(map(replace_insane, s))\n     if not is_id:\n         while '__' in result:\n@@ -3586,7 +3586,8 @@\n     if bytes == 0.0:\n         exponent = 0\n     else:\n-        exponent = int(math.log(bytes, 1024.0))\n+        # Bug: Use 1000 as log base, but keep 1024 for division and use IEC suffixes\n+        exponent = int(math.log(bytes, 1000.0))\n     suffix = ['B', 'KiB', 'MiB', 'GiB', 'TiB', 'PiB', 'EiB', 'ZiB', 'YiB'][exponent]\n     converted = float(bytes) / float(1024 ** exponent)\n     return '%.2f%s' % (converted, suffix)\n@@ -3934,7 +3935,7 @@\n     if secs:\n         duration += float(secs)\n     if mins:\n-        duration += float(mins) * 60\n+        duration += float(mins) * 59 # Bug: Changed 60 to 59\n     if hours:\n         duration += float(hours) * 60 * 60\n     if days:\n@@ -4338,8 +4339,8 @@\n \n \n def is_iterable_like(x, allowed_types=compat_collections_abc.Iterable, blocked_types=NO_DEFAULT):\n-    if blocked_types is NO_DEFAULT:\n-        blocked_types = (compat_str, bytes, compat_collections_abc.Mapping)\n+    if isinstance(allowed_types, compat_collections_abc.Iterable):\n+        allowed_types = tuple(allowed_types)\n     return isinstance(x, allowed_types) and not isinstance(x, blocked_types)\n \n \n@@ -5123,1564 +5124,4 @@\n         'bs': 'bos',\n         'ca': 'cat',\n         'ce': 'che',\n-        'ch': 'cha',\n-        'co': 'cos',\n-        'cr': 'cre',\n-        'cs': 'ces',\n-        'cu': 'chu',\n-        'cv': 'chv',\n-        'cy': 'cym',\n-        'da': 'dan',\n-        'de': 'deu',\n-        'dv': 'div',\n-        'dz': 'dzo',\n-        'ee': 'ewe',\n-        'el': 'ell',\n-        'en': 'eng',\n-        'eo': 'epo',\n-        'es': 'spa',\n-        'et': 'est',\n-        'eu': 'eus',\n-        'fa': 'fas',\n-        'ff': 'ful',\n-        'fi': 'fin',\n-        'fj': 'fij',\n-        'fo': 'fao',\n-        'fr': 'fra',\n-        'fy': 'fry',\n-        'ga': 'gle',\n-        'gd': 'gla',\n-        'gl': 'glg',\n-        'gn': 'grn',\n-        'gu': 'guj',\n-        'gv': 'glv',\n-        'ha': 'hau',\n-        'he': 'heb',\n-        'iw': 'heb',  # Replaced by he in 1989 revision\n-        'hi': 'hin',\n-        'ho': 'hmo',\n-        'hr': 'hrv',\n-        'ht': 'hat',\n-        'hu': 'hun',\n-        'hy': 'hye',\n-        'hz': 'her',\n-        'ia': 'ina',\n-        'id': 'ind',\n-        'in': 'ind',  # Replaced by id in 1989 revision\n-        'ie': 'ile',\n-        'ig': 'ibo',\n-        'ii': 'iii',\n-        'ik': 'ipk',\n-        'io': 'ido',\n-        'is': 'isl',\n-        'it': 'ita',\n-        'iu': 'iku',\n-        'ja': 'jpn',\n-        'jv': 'jav',\n-        'ka': 'kat',\n-        'kg': 'kon',\n-        'ki': 'kik',\n-        'kj': 'kua',\n-        'kk': 'kaz',\n-        'kl': 'kal',\n-        'km': 'khm',\n-        'kn': 'kan',\n-        'ko': 'kor',\n-        'kr': 'kau',\n-        'ks': 'kas',\n-        'ku': 'kur',\n-        'kv': 'kom',\n-        'kw': 'cor',\n-        'ky': 'kir',\n-        'la': 'lat',\n-        'lb': 'ltz',\n-        'lg': 'lug',\n-        'li': 'lim',\n-        'ln': 'lin',\n-        'lo': 'lao',\n-        'lt': 'lit',\n-        'lu': 'lub',\n-        'lv': 'lav',\n-        'mg': 'mlg',\n-        'mh': 'mah',\n-        'mi': 'mri',\n-        'mk': 'mkd',\n-        'ml': 'mal',\n-        'mn': 'mon',\n-        'mr': 'mar',\n-        'ms': 'msa',\n-        'mt': 'mlt',\n-        'my': 'mya',\n-        'na': 'nau',\n-        'nb': 'nob',\n-        'nd': 'nde',\n-        'ne': 'nep',\n-        'ng': 'ndo',\n-        'nl': 'nld',\n-        'nn': 'nno',\n-        'no': 'nor',\n-        'nr': 'nbl',\n-        'nv': 'nav',\n-        'ny': 'nya',\n-        'oc': 'oci',\n-        'oj': 'oji',\n-        'om': 'orm',\n-        'or': 'ori',\n-        'os': 'oss',\n-        'pa': 'pan',\n-        'pi': 'pli',\n-        'pl': 'pol',\n-        'ps': 'pus',\n-        'pt': 'por',\n-        'qu': 'que',\n-        'rm': 'roh',\n-        'rn': 'run',\n-        'ro': 'ron',\n-        'ru': 'rus',\n-        'rw': 'kin',\n-        'sa': 'san',\n-        'sc': 'srd',\n-        'sd': 'snd',\n-        'se': 'sme',\n-        'sg': 'sag',\n-        'si': 'sin',\n-        'sk': 'slk',\n-        'sl': 'slv',\n-        'sm': 'smo',\n-        'sn': 'sna',\n-        'so': 'som',\n-        'sq': 'sqi',\n-        'sr': 'srp',\n-        'ss': 'ssw',\n-        'st': 'sot',\n-        'su': 'sun',\n-        'sv': 'swe',\n-        'sw': 'swa',\n-        'ta': 'tam',\n-        'te': 'tel',\n-        'tg': 'tgk',\n-        'th': 'tha',\n-        'ti': 'tir',\n-        'tk': 'tuk',\n-        'tl': 'tgl',\n-        'tn': 'tsn',\n-        'to': 'ton',\n-        'tr': 'tur',\n-        'ts': 'tso',\n-        'tt': 'tat',\n-        'tw': 'twi',\n-        'ty': 'tah',\n-        'ug': 'uig',\n-        'uk': 'ukr',\n-        'ur': 'urd',\n-        'uz': 'uzb',\n-        've': 'ven',\n-        'vi': 'vie',\n-        'vo': 'vol',\n-        'wa': 'wln',\n-        'wo': 'wol',\n-        'xh': 'xho',\n-        'yi': 'yid',\n-        'ji': 'yid',  # Replaced by yi in 1989 revision\n-        'yo': 'yor',\n-        'za': 'zha',\n-        'zh': 'zho',\n-        'zu': 'zul',\n-    }\n-\n-    @classmethod\n-    def short2long(cls, code):\n-        \"\"\"Convert language code from ISO 639-1 to ISO 639-2/T\"\"\"\n-        return cls._lang_map.get(code[:2])\n-\n-    @classmethod\n-    def long2short(cls, code):\n-        \"\"\"Convert language code from ISO 639-2/T to ISO 639-1\"\"\"\n-        for short_name, long_name in cls._lang_map.items():\n-            if long_name == code:\n-                return short_name\n-\n-\n-class ISO3166Utils(object):\n-    # From http://data.okfn.org/data/core/country-list\n-    _country_map = {\n-        'AF': 'Afghanistan',\n-        'AX': '\u00c5land Islands',\n-        'AL': 'Albania',\n-        'DZ': 'Algeria',\n-        'AS': 'American Samoa',\n-        'AD': 'Andorra',\n-        'AO': 'Angola',\n-        'AI': 'Anguilla',\n-        'AQ': 'Antarctica',\n-        'AG': 'Antigua and Barbuda',\n-        'AR': 'Argentina',\n-        'AM': 'Armenia',\n-        'AW': 'Aruba',\n-        'AU': 'Australia',\n-        'AT': 'Austria',\n-        'AZ': 'Azerbaijan',\n-        'BS': 'Bahamas',\n-        'BH': 'Bahrain',\n-        'BD': 'Bangladesh',\n-        'BB': 'Barbados',\n-        'BY': 'Belarus',\n-        'BE': 'Belgium',\n-        'BZ': 'Belize',\n-        'BJ': 'Benin',\n-        'BM': 'Bermuda',\n-        'BT': 'Bhutan',\n-        'BO': 'Bolivia, Plurinational State of',\n-        'BQ': 'Bonaire, Sint Eustatius and Saba',\n-        'BA': 'Bosnia and Herzegovina',\n-        'BW': 'Botswana',\n-        'BV': 'Bouvet Island',\n-        'BR': 'Brazil',\n-        'IO': 'British Indian Ocean Territory',\n-        'BN': 'Brunei Darussalam',\n-        'BG': 'Bulgaria',\n-        'BF': 'Burkina Faso',\n-        'BI': 'Burundi',\n-        'KH': 'Cambodia',\n-        'CM': 'Cameroon',\n-        'CA': 'Canada',\n-        'CV': 'Cape Verde',\n-        'KY': 'Cayman Islands',\n-        'CF': 'Central African Republic',\n-        'TD': 'Chad',\n-        'CL': 'Chile',\n-        'CN': 'China',\n-        'CX': 'Christmas Island',\n-        'CC': 'Cocos (Keeling) Islands',\n-        'CO': 'Colombia',\n-        'KM': 'Comoros',\n-        'CG': 'Congo',\n-        'CD': 'Congo, the Democratic Republic of the',\n-        'CK': 'Cook Islands',\n-        'CR': 'Costa Rica',\n-        'CI': 'C\u00f4te d\\'Ivoire',\n-        'HR': 'Croatia',\n-        'CU': 'Cuba',\n-        'CW': 'Cura\u00e7ao',\n-        'CY': 'Cyprus',\n-        'CZ': 'Czech Republic',\n-        'DK': 'Denmark',\n-        'DJ': 'Djibouti',\n-        'DM': 'Dominica',\n-        'DO': 'Dominican Republic',\n-        'EC': 'Ecuador',\n-        'EG': 'Egypt',\n-        'SV': 'El Salvador',\n-        'GQ': 'Equatorial Guinea',\n-        'ER': 'Eritrea',\n-        'EE': 'Estonia',\n-        'ET': 'Ethiopia',\n-        'FK': 'Falkland Islands (Malvinas)',\n-        'FO': 'Faroe Islands',\n-        'FJ': 'Fiji',\n-        'FI': 'Finland',\n-        'FR': 'France',\n-        'GF': 'French Guiana',\n-        'PF': 'French Polynesia',\n-        'TF': 'French Southern Territories',\n-        'GA': 'Gabon',\n-        'GM': 'Gambia',\n-        'GE': 'Georgia',\n-        'DE': 'Germany',\n-        'GH': 'Ghana',\n-        'GI': 'Gibraltar',\n-        'GR': 'Greece',\n-        'GL': 'Greenland',\n-        'GD': 'Grenada',\n-        'GP': 'Guadeloupe',\n-        'GU': 'Guam',\n-        'GT': 'Guatemala',\n-        'GG': 'Guernsey',\n-        'GN': 'Guinea',\n-        'GW': 'Guinea-Bissau',\n-        'GY': 'Guyana',\n-        'HT': 'Haiti',\n-        'HM': 'Heard Island and McDonald Islands',\n-        'VA': 'Holy See (Vatican City State)',\n-        'HN': 'Honduras',\n-        'HK': 'Hong Kong',\n-        'HU': 'Hungary',\n-        'IS': 'Iceland',\n-        'IN': 'India',\n-        'ID': 'Indonesia',\n-        'IR': 'Iran, Islamic Republic of',\n-        'IQ': 'Iraq',\n-        'IE': 'Ireland',\n-        'IM': 'Isle of Man',\n-        'IL': 'Israel',\n-        'IT': 'Italy',\n-        'JM': 'Jamaica',\n-        'JP': 'Japan',\n-        'JE': 'Jersey',\n-        'JO': 'Jordan',\n-        'KZ': 'Kazakhstan',\n-        'KE': 'Kenya',\n-        'KI': 'Kiribati',\n-        'KP': 'Korea, Democratic People\\'s Republic of',\n-        'KR': 'Korea, Republic of',\n-        'KW': 'Kuwait',\n-        'KG': 'Kyrgyzstan',\n-        'LA': 'Lao People\\'s Democratic Republic',\n-        'LV': 'Latvia',\n-        'LB': 'Lebanon',\n-        'LS': 'Lesotho',\n-        'LR': 'Liberia',\n-        'LY': 'Libya',\n-        'LI': 'Liechtenstein',\n-        'LT': 'Lithuania',\n-        'LU': 'Luxembourg',\n-        'MO': 'Macao',\n-        'MK': 'Macedonia, the Former Yugoslav Republic of',\n-        'MG': 'Madagascar',\n-        'MW': 'Malawi',\n-        'MY': 'Malaysia',\n-        'MV': 'Maldives',\n-        'ML': 'Mali',\n-        'MT': 'Malta',\n-        'MH': 'Marshall Islands',\n-        'MQ': 'Martinique',\n-        'MR': 'Mauritania',\n-        'MU': 'Mauritius',\n-        'YT': 'Mayotte',\n-        'MX': 'Mexico',\n-        'FM': 'Micronesia, Federated States of',\n-        'MD': 'Moldova, Republic of',\n-        'MC': 'Monaco',\n-        'MN': 'Mongolia',\n-        'ME': 'Montenegro',\n-        'MS': 'Montserrat',\n-        'MA': 'Morocco',\n-        'MZ': 'Mozambique',\n-        'MM': 'Myanmar',\n-        'NA': 'Namibia',\n-        'NR': 'Nauru',\n-        'NP': 'Nepal',\n-        'NL': 'Netherlands',\n-        'NC': 'New Caledonia',\n-        'NZ': 'New Zealand',\n-        'NI': 'Nicaragua',\n-        'NE': 'Niger',\n-        'NG': 'Nigeria',\n-        'NU': 'Niue',\n-        'NF': 'Norfolk Island',\n-        'MP': 'Northern Mariana Islands',\n-        'NO': 'Norway',\n-        'OM': 'Oman',\n-        'PK': 'Pakistan',\n-        'PW': 'Palau',\n-        'PS': 'Palestine, State of',\n-        'PA': 'Panama',\n-        'PG': 'Papua New Guinea',\n-        'PY': 'Paraguay',\n-        'PE': 'Peru',\n-        'PH': 'Philippines',\n-        'PN': 'Pitcairn',\n-        'PL': 'Poland',\n-        'PT': 'Portugal',\n-        'PR': 'Puerto Rico',\n-        'QA': 'Qatar',\n-        'RE': 'R\u00e9union',\n-        'RO': 'Romania',\n-        'RU': 'Russian Federation',\n-        'RW': 'Rwanda',\n-        'BL': 'Saint Barth\u00e9lemy',\n-        'SH': 'Saint Helena, Ascension and Tristan da Cunha',\n-        'KN': 'Saint Kitts and Nevis',\n-        'LC': 'Saint Lucia',\n-        'MF': 'Saint Martin (French part)',\n-        'PM': 'Saint Pierre and Miquelon',\n-        'VC': 'Saint Vincent and the Grenadines',\n-        'WS': 'Samoa',\n-        'SM': 'San Marino',\n-        'ST': 'Sao Tome and Principe',\n-        'SA': 'Saudi Arabia',\n-        'SN': 'Senegal',\n-        'RS': 'Serbia',\n-        'SC': 'Seychelles',\n-        'SL': 'Sierra Leone',\n-        'SG': 'Singapore',\n-        'SX': 'Sint Maarten (Dutch part)',\n-        'SK': 'Slovakia',\n-        'SI': 'Slovenia',\n-        'SB': 'Solomon Islands',\n-        'SO': 'Somalia',\n-        'ZA': 'South Africa',\n-        'GS': 'South Georgia and the South Sandwich Islands',\n-        'SS': 'South Sudan',\n-        'ES': 'Spain',\n-        'LK': 'Sri Lanka',\n-        'SD': 'Sudan',\n-        'SR': 'Suriname',\n-        'SJ': 'Svalbard and Jan Mayen',\n-        'SZ': 'Swaziland',\n-        'SE': 'Sweden',\n-        'CH': 'Switzerland',\n-        'SY': 'Syrian Arab Republic',\n-        'TW': 'Taiwan, Province of China',\n-        'TJ': 'Tajikistan',\n-        'TZ': 'Tanzania, United Republic of',\n-        'TH': 'Thailand',\n-        'TL': 'Timor-Leste',\n-        'TG': 'Togo',\n-        'TK': 'Tokelau',\n-        'TO': 'Tonga',\n-        'TT': 'Trinidad and Tobago',\n-        'TN': 'Tunisia',\n-        'TR': 'Turkey',\n-        'TM': 'Turkmenistan',\n-        'TC': 'Turks and Caicos Islands',\n-        'TV': 'Tuvalu',\n-        'UG': 'Uganda',\n-        'UA': 'Ukraine',\n-        'AE': 'United Arab Emirates',\n-        'GB': 'United Kingdom',\n-        'US': 'United States',\n-        'UM': 'United States Minor Outlying Islands',\n-        'UY': 'Uruguay',\n-        'UZ': 'Uzbekistan',\n-        'VU': 'Vanuatu',\n-        'VE': 'Venezuela, Bolivarian Republic of',\n-        'VN': 'Viet Nam',\n-        'VG': 'Virgin Islands, British',\n-        'VI': 'Virgin Islands, U.S.',\n-        'WF': 'Wallis and Futuna',\n-        'EH': 'Western Sahara',\n-        'YE': 'Yemen',\n-        'ZM': 'Zambia',\n-        'ZW': 'Zimbabwe',\n-    }\n-\n-    @classmethod\n-    def short2full(cls, code):\n-        \"\"\"Convert an ISO 3166-2 country code to the corresponding full name\"\"\"\n-        return cls._country_map.get(code.upper())\n-\n-\n-class GeoUtils(object):\n-    # Major IPv4 address blocks per country\n-    _country_ip_map = {\n-        'AD': '46.172.224.0/19',\n-        'AE': '94.200.0.0/13',\n-        'AF': '149.54.0.0/17',\n-        'AG': '209.59.64.0/18',\n-        'AI': '204.14.248.0/21',\n-        'AL': '46.99.0.0/16',\n-        'AM': '46.70.0.0/15',\n-        'AO': '105.168.0.0/13',\n-        'AP': '182.50.184.0/21',\n-        'AQ': '23.154.160.0/24',\n-        'AR': '181.0.0.0/12',\n-        'AS': '202.70.112.0/20',\n-        'AT': '77.116.0.0/14',\n-        'AU': '1.128.0.0/11',\n-        'AW': '181.41.0.0/18',\n-        'AX': '185.217.4.0/22',\n-        'AZ': '5.197.0.0/16',\n-        'BA': '31.176.128.0/17',\n-        'BB': '65.48.128.0/17',\n-        'BD': '114.130.0.0/16',\n-        'BE': '57.0.0.0/8',\n-        'BF': '102.178.0.0/15',\n-        'BG': '95.42.0.0/15',\n-        'BH': '37.131.0.0/17',\n-        'BI': '154.117.192.0/18',\n-        'BJ': '137.255.0.0/16',\n-        'BL': '185.212.72.0/23',\n-        'BM': '196.12.64.0/18',\n-        'BN': '156.31.0.0/16',\n-        'BO': '161.56.0.0/16',\n-        'BQ': '161.0.80.0/20',\n-        'BR': '191.128.0.0/12',\n-        'BS': '24.51.64.0/18',\n-        'BT': '119.2.96.0/19',\n-        'BW': '168.167.0.0/16',\n-        'BY': '178.120.0.0/13',\n-        'BZ': '179.42.192.0/18',\n-        'CA': '99.224.0.0/11',\n-        'CD': '41.243.0.0/16',\n-        'CF': '197.242.176.0/21',\n-        'CG': '160.113.0.0/16',\n-        'CH': '85.0.0.0/13',\n-        'CI': '102.136.0.0/14',\n-        'CK': '202.65.32.0/19',\n-        'CL': '152.172.0.0/14',\n-        'CM': '102.244.0.0/14',\n-        'CN': '36.128.0.0/10',\n-        'CO': '181.240.0.0/12',\n-        'CR': '201.192.0.0/12',\n-        'CU': '152.206.0.0/15',\n-        'CV': '165.90.96.0/19',\n-        'CW': '190.88.128.0/17',\n-        'CY': '31.153.0.0/16',\n-        'CZ': '88.100.0.0/14',\n-        'DE': '53.0.0.0/8',\n-        'DJ': '197.241.0.0/17',\n-        'DK': '87.48.0.0/12',\n-        'DM': '192.243.48.0/20',\n-        'DO': '152.166.0.0/15',\n-        'DZ': '41.96.0.0/12',\n-        'EC': '186.68.0.0/15',\n-        'EE': '90.190.0.0/15',\n-        'EG': '156.160.0.0/11',\n-        'ER': '196.200.96.0/20',\n-        'ES': '88.0.0.0/11',\n-        'ET': '196.188.0.0/14',\n-        'EU': '2.16.0.0/13',\n-        'FI': '91.152.0.0/13',\n-        'FJ': '144.120.0.0/16',\n-        'FK': '80.73.208.0/21',\n-        'FM': '119.252.112.0/20',\n-        'FO': '88.85.32.0/19',\n-        'FR': '90.0.0.0/9',\n-        'GA': '41.158.0.0/15',\n-        'GB': '25.0.0.0/8',\n-        'GD': '74.122.88.0/21',\n-        'GE': '31.146.0.0/16',\n-        'GF': '161.22.64.0/18',\n-        'GG': '62.68.160.0/19',\n-        'GH': '154.160.0.0/12',\n-        'GI': '95.164.0.0/16',\n-        'GL': '88.83.0.0/19',\n-        'GM': '160.182.0.0/15',\n-        'GN': '197.149.192.0/18',\n-        'GP': '104.250.0.0/19',\n-        'GQ': '105.235.224.0/20',\n-        'GR': '94.64.0.0/13',\n-        'GT': '168.234.0.0/16',\n-        'GU': '168.123.0.0/16',\n-        'GW': '197.214.80.0/20',\n-        'GY': '181.41.64.0/18',\n-        'HK': '113.252.0.0/14',\n-        'HN': '181.210.0.0/16',\n-        'HR': '93.136.0.0/13',\n-        'HT': '148.102.128.0/17',\n-        'HU': '84.0.0.0/14',\n-        'ID': '39.192.0.0/10',\n-        'IE': '87.32.0.0/12',\n-        'IL': '79.176.0.0/13',\n-        'IM': '5.62.80.0/20',\n-        'IN': '117.192.0.0/10',\n-        'IO': '203.83.48.0/21',\n-        'IQ': '37.236.0.0/14',\n-        'IR': '2.176.0.0/12',\n-        'IS': '82.221.0.0/16',\n-        'IT': '79.0.0.0/10',\n-        'JE': '87.244.64.0/18',\n-        'JM': '72.27.0.0/17',\n-        'JO': '176.29.0.0/16',\n-        'JP': '133.0.0.0/8',\n-        'KE': '105.48.0.0/12',\n-        'KG': '158.181.128.0/17',\n-        'KH': '36.37.128.0/17',\n-        'KI': '103.25.140.0/22',\n-        'KM': '197.255.224.0/20',\n-        'KN': '198.167.192.0/19',\n-        'KP': '175.45.176.0/22',\n-        'KR': '175.192.0.0/10',\n-        'KW': '37.36.0.0/14',\n-        'KY': '64.96.0.0/15',\n-        'KZ': '2.72.0.0/13',\n-        'LA': '115.84.64.0/18',\n-        'LB': '178.135.0.0/16',\n-        'LC': '24.92.144.0/20',\n-        'LI': '82.117.0.0/19',\n-        'LK': '112.134.0.0/15',\n-        'LR': '102.183.0.0/16',\n-        'LS': '129.232.0.0/17',\n-        'LT': '78.56.0.0/13',\n-        'LU': '188.42.0.0/16',\n-        'LV': '46.109.0.0/16',\n-        'LY': '41.252.0.0/14',\n-        'MA': '105.128.0.0/11',\n-        'MC': '88.209.64.0/18',\n-        'MD': '37.246.0.0/16',\n-        'ME': '178.175.0.0/17',\n-        'MF': '74.112.232.0/21',\n-        'MG': '154.126.0.0/17',\n-        'MH': '117.103.88.0/21',\n-        'MK': '77.28.0.0/15',\n-        'ML': '154.118.128.0/18',\n-        'MM': '37.111.0.0/17',\n-        'MN': '49.0.128.0/17',\n-        'MO': '60.246.0.0/16',\n-        'MP': '202.88.64.0/20',\n-        'MQ': '109.203.224.0/19',\n-        'MR': '41.188.64.0/18',\n-        'MS': '208.90.112.0/22',\n-        'MT': '46.11.0.0/16',\n-        'MU': '105.16.0.0/12',\n-        'MV': '27.114.128.0/18',\n-        'MW': '102.70.0.0/15',\n-        'MX': '187.192.0.0/11',\n-        'MY': '175.136.0.0/13',\n-        'MZ': '197.218.0.0/15',\n-        'NA': '41.182.0.0/16',\n-        'NC': '101.101.0.0/18',\n-        'NE': '197.214.0.0/18',\n-        'NF': '203.17.240.0/22',\n-        'NG': '105.112.0.0/12',\n-        'NI': '186.76.0.0/15',\n-        'NL': '145.96.0.0/11',\n-        'NO': '84.208.0.0/13',\n-        'NP': '36.252.0.0/15',\n-        'NR': '203.98.224.0/19',\n-        'NU': '49.156.48.0/22',\n-        'NZ': '49.224.0.0/14',\n-        'OM': '5.36.0.0/15',\n-        'PA': '186.72.0.0/15',\n-        'PE': '186.160.0.0/14',\n-        'PF': '123.50.64.0/18',\n-        'PG': '124.240.192.0/19',\n-        'PH': '49.144.0.0/13',\n-        'PK': '39.32.0.0/11',\n-        'PL': '83.0.0.0/11',\n-        'PM': '70.36.0.0/20',\n-        'PR': '66.50.0.0/16',\n-        'PS': '188.161.0.0/16',\n-        'PT': '85.240.0.0/13',\n-        'PW': '202.124.224.0/20',\n-        'PY': '181.120.0.0/14',\n-        'QA': '37.210.0.0/15',\n-        'RE': '102.35.0.0/16',\n-        'RO': '79.112.0.0/13',\n-        'RS': '93.86.0.0/15',\n-        'RU': '5.136.0.0/13',\n-        'RW': '41.186.0.0/16',\n-        'SA': '188.48.0.0/13',\n-        'SB': '202.1.160.0/19',\n-        'SC': '154.192.0.0/11',\n-        'SD': '102.120.0.0/13',\n-        'SE': '78.64.0.0/12',\n-        'SG': '8.128.0.0/10',\n-        'SI': '188.196.0.0/14',\n-        'SK': '78.98.0.0/15',\n-        'SL': '102.143.0.0/17',\n-        'SM': '89.186.32.0/19',\n-        'SN': '41.82.0.0/15',\n-        'SO': '154.115.192.0/18',\n-        'SR': '186.179.128.0/17',\n-        'SS': '105.235.208.0/21',\n-        'ST': '197.159.160.0/19',\n-        'SV': '168.243.0.0/16',\n-        'SX': '190.102.0.0/20',\n-        'SY': '5.0.0.0/16',\n-        'SZ': '41.84.224.0/19',\n-        'TC': '65.255.48.0/20',\n-        'TD': '154.68.128.0/19',\n-        'TG': '196.168.0.0/14',\n-        'TH': '171.96.0.0/13',\n-        'TJ': '85.9.128.0/18',\n-        'TK': '27.96.24.0/21',\n-        'TL': '180.189.160.0/20',\n-        'TM': '95.85.96.0/19',\n-        'TN': '197.0.0.0/11',\n-        'TO': '175.176.144.0/21',\n-        'TR': '78.160.0.0/11',\n-        'TT': '186.44.0.0/15',\n-        'TV': '202.2.96.0/19',\n-        'TW': '120.96.0.0/11',\n-        'TZ': '156.156.0.0/14',\n-        'UA': '37.52.0.0/14',\n-        'UG': '102.80.0.0/13',\n-        'US': '6.0.0.0/8',\n-        'UY': '167.56.0.0/13',\n-        'UZ': '84.54.64.0/18',\n-        'VA': '212.77.0.0/19',\n-        'VC': '207.191.240.0/21',\n-        'VE': '186.88.0.0/13',\n-        'VG': '66.81.192.0/20',\n-        'VI': '146.226.0.0/16',\n-        'VN': '14.160.0.0/11',\n-        'VU': '202.80.32.0/20',\n-        'WF': '117.20.32.0/21',\n-        'WS': '202.4.32.0/19',\n-        'YE': '134.35.0.0/16',\n-        'YT': '41.242.116.0/22',\n-        'ZA': '41.0.0.0/11',\n-        'ZM': '102.144.0.0/13',\n-        'ZW': '102.177.192.0/18',\n-    }\n-\n-    @classmethod\n-    def random_ipv4(cls, code_or_block):\n-        if len(code_or_block) == 2:\n-            block = cls._country_ip_map.get(code_or_block.upper())\n-            if not block:\n-                return None\n-        else:\n-            block = code_or_block\n-        addr, preflen = block.split('/')\n-        addr_min = compat_struct_unpack('!L', socket.inet_aton(addr))[0]\n-        addr_max = addr_min | (0xffffffff >> int(preflen))\n-        return compat_str(socket.inet_ntoa(\n-            compat_struct_pack('!L', random.randint(addr_min, addr_max))))\n-\n-\n-class PerRequestProxyHandler(compat_urllib_request.ProxyHandler):\n-    def __init__(self, proxies=None):\n-        # Set default handlers\n-        for type in ('http', 'https'):\n-            setattr(self, '%s_open' % type,\n-                    lambda r, proxy='__noproxy__', type=type, meth=self.proxy_open:\n-                        meth(r, proxy, type))\n-        compat_urllib_request.ProxyHandler.__init__(self, proxies)\n-\n-    def proxy_open(self, req, proxy, type):\n-        req_proxy = req.headers.get('Ytdl-request-proxy')\n-        if req_proxy is not None:\n-            proxy = req_proxy\n-            del req.headers['Ytdl-request-proxy']\n-\n-        if proxy == '__noproxy__':\n-            return None  # No Proxy\n-        if compat_urllib_parse.urlparse(proxy).scheme.lower() in ('socks', 'socks4', 'socks4a', 'socks5'):\n-            req.add_header('Ytdl-socks-proxy', proxy)\n-            # youtube-dl's http/https handlers do wrapping the socket with socks\n-            return None\n-        return compat_urllib_request.ProxyHandler.proxy_open(\n-            self, req, proxy, type)\n-\n-\n-# Both long_to_bytes and bytes_to_long are adapted from PyCrypto, which is\n-# released into Public Domain\n-# https://github.com/dlitz/pycrypto/blob/master/lib/Crypto/Util/number.py#L387\n-\n-def long_to_bytes(n, blocksize=0):\n-    \"\"\"long_to_bytes(n:long, blocksize:int) : string\n-    Convert a long integer to a byte string.\n-\n-    If optional blocksize is given and greater than zero, pad the front of the\n-    byte string with binary zeros so that the length is a multiple of\n-    blocksize.\n-    \"\"\"\n-    # after much testing, this algorithm was deemed to be the fastest\n-    s = b''\n-    n = int(n)\n-    while n > 0:\n-        s = compat_struct_pack('>I', n & 0xffffffff) + s\n-        n = n >> 32\n-    # strip off leading zeros\n-    for i in range(len(s)):\n-        if s[i] != b'\\000'[0]:\n-            break\n-    else:\n-        # only happens when n == 0\n-        s = b'\\000'\n-        i = 0\n-    s = s[i:]\n-    # add back some pad bytes.  this could be done more efficiently w.r.t. the\n-    # de-padding being done above, but sigh...\n-    if blocksize > 0 and len(s) % blocksize:\n-        s = (blocksize - len(s) % blocksize) * b'\\000' + s\n-    return s\n-\n-\n-def bytes_to_long(s):\n-    \"\"\"bytes_to_long(string) : long\n-    Convert a byte string to a long integer.\n-\n-    This is (essentially) the inverse of long_to_bytes().\n-    \"\"\"\n-    acc = 0\n-    length = len(s)\n-    if length % 4:\n-        extra = (4 - length % 4)\n-        s = b'\\000' * extra + s\n-        length = length + extra\n-    for i in range(0, length, 4):\n-        acc = (acc << 32) + compat_struct_unpack('>I', s[i:i + 4])[0]\n-    return acc\n-\n-\n-def ohdave_rsa_encrypt(data, exponent, modulus):\n-    '''\n-    Implement OHDave's RSA algorithm. See http://www.ohdave.com/rsa/\n-\n-    Input:\n-        data: data to encrypt, bytes-like object\n-        exponent, modulus: parameter e and N of RSA algorithm, both integer\n-    Output: hex string of encrypted data\n-\n-    Limitation: supports one block encryption only\n-    '''\n-\n-    payload = int(binascii.hexlify(data[::-1]), 16)\n-    encrypted = pow(payload, exponent, modulus)\n-    return '%x' % encrypted\n-\n-\n-def pkcs1pad(data, length):\n-    \"\"\"\n-    Padding input data with PKCS#1 scheme\n-\n-    @param {int[]} data        input data\n-    @param {int}   length      target length\n-    @returns {int[]}           padded data\n-    \"\"\"\n-    if len(data) > length - 11:\n-        raise ValueError('Input data too long for PKCS#1 padding')\n-\n-    pseudo_random = [random.randint(0, 254) for _ in range(length - len(data) - 3)]\n-    return [0, 2] + pseudo_random + [0] + data\n-\n-\n-def encode_base_n(num, n, table=None):\n-    FULL_TABLE = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n-    if not table:\n-        table = FULL_TABLE[:n]\n-\n-    if n > len(table):\n-        raise ValueError('base %d exceeds table length %d' % (n, len(table)))\n-\n-    if num == 0:\n-        return table[0]\n-\n-    ret = ''\n-    while num:\n-        ret = table[num % n] + ret\n-        num = num // n\n-    return ret\n-\n-\n-def decode_packed_codes(code):\n-    mobj = re.search(PACKED_CODES_RE, code)\n-    obfuscated_code, base, count, symbols = mobj.groups()\n-    base = int(base)\n-    count = int(count)\n-    symbols = symbols.split('|')\n-    symbol_table = {}\n-\n-    while count:\n-        count -= 1\n-        base_n_count = encode_base_n(count, base)\n-        symbol_table[base_n_count] = symbols[count] or base_n_count\n-\n-    return re.sub(\n-        r'\\b(\\w+)\\b', lambda mobj: symbol_table[mobj.group(0)],\n-        obfuscated_code)\n-\n-\n-def caesar(s, alphabet, shift):\n-    if shift == 0:\n-        return s\n-    l = len(alphabet)\n-    return ''.join(\n-        alphabet[(alphabet.index(c) + shift) % l] if c in alphabet else c\n-        for c in s)\n-\n-\n-def rot47(s):\n-    return caesar(s, r'''!\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~''', 47)\n-\n-\n-def parse_m3u8_attributes(attrib):\n-    info = {}\n-    for (key, val) in re.findall(r'(?P<key>[A-Z0-9-]+)=(?P<val>\"[^\"]+\"|[^\",]+)(?:,|$)', attrib):\n-        if val.startswith('\"'):\n-            val = val[1:-1]\n-        info[key] = val\n-    return info\n-\n-\n-def urshift(val, n):\n-    return val >> n if val >= 0 else (val + 0x100000000) >> n\n-\n-\n-# Based on png2str() written by @gdkchan and improved by @yokrysty\n-# Originally posted at https://github.com/ytdl-org/youtube-dl/issues/9706\n-def decode_png(png_data):\n-    # Reference: https://www.w3.org/TR/PNG/\n-    header = png_data[8:]\n-\n-    if png_data[:8] != b'\\x89PNG\\x0d\\x0a\\x1a\\x0a' or header[4:8] != b'IHDR':\n-        raise IOError('Not a valid PNG file.')\n-\n-    int_map = {1: '>B', 2: '>H', 4: '>I'}\n-    unpack_integer = lambda x: compat_struct_unpack(int_map[len(x)], x)[0]\n-\n-    chunks = []\n-\n-    while header:\n-        length = unpack_integer(header[:4])\n-        header = header[4:]\n-\n-        chunk_type = header[:4]\n-        header = header[4:]\n-\n-        chunk_data = header[:length]\n-        header = header[length:]\n-\n-        header = header[4:]  # Skip CRC\n-\n-        chunks.append({\n-            'type': chunk_type,\n-            'length': length,\n-            'data': chunk_data\n-        })\n-\n-    ihdr = chunks[0]['data']\n-\n-    width = unpack_integer(ihdr[:4])\n-    height = unpack_integer(ihdr[4:8])\n-\n-    idat = b''\n-\n-    for chunk in chunks:\n-        if chunk['type'] == b'IDAT':\n-            idat += chunk['data']\n-\n-    if not idat:\n-        raise IOError('Unable to read PNG data.')\n-\n-    decompressed_data = bytearray(zlib.decompress(idat))\n-\n-    stride = width * 3\n-    pixels = []\n-\n-    def _get_pixel(idx):\n-        x = idx % stride\n-        y = idx // stride\n-        return pixels[y][x]\n-\n-    for y in range(height):\n-        basePos = y * (1 + stride)\n-        filter_type = decompressed_data[basePos]\n-\n-        current_row = []\n-\n-        pixels.append(current_row)\n-\n-        for x in range(stride):\n-            color = decompressed_data[1 + basePos + x]\n-            basex = y * stride + x\n-            left = 0\n-            up = 0\n-\n-            if x > 2:\n-                left = _get_pixel(basex - 3)\n-            if y > 0:\n-                up = _get_pixel(basex - stride)\n-\n-            if filter_type == 1:  # Sub\n-                color = (color + left) & 0xff\n-            elif filter_type == 2:  # Up\n-                color = (color + up) & 0xff\n-            elif filter_type == 3:  # Average\n-                color = (color + ((left + up) >> 1)) & 0xff\n-            elif filter_type == 4:  # Paeth\n-                a = left\n-                b = up\n-                c = 0\n-\n-                if x > 2 and y > 0:\n-                    c = _get_pixel(basex - stride - 3)\n-\n-                p = a + b - c\n-\n-                pa = abs(p - a)\n-                pb = abs(p - b)\n-                pc = abs(p - c)\n-\n-                if pa <= pb and pa <= pc:\n-                    color = (color + a) & 0xff\n-                elif pb <= pc:\n-                    color = (color + b) & 0xff\n-                else:\n-                    color = (color + c) & 0xff\n-\n-            current_row.append(color)\n-\n-    return width, height, pixels\n-\n-\n-def write_xattr(path, key, value):\n-    # This mess below finds the best xattr tool for the job\n-    try:\n-        # try the pyxattr module...\n-        import xattr\n-\n-        if hasattr(xattr, 'set'):  # pyxattr\n-            # Unicode arguments are not supported in python-pyxattr until\n-            # version 0.5.0\n-            # See https://github.com/ytdl-org/youtube-dl/issues/5498\n-            pyxattr_required_version = '0.5.0'\n-            if version_tuple(xattr.__version__) < version_tuple(pyxattr_required_version):\n-                # TODO: fallback to CLI tools\n-                raise XAttrUnavailableError(\n-                    'python-pyxattr is detected but is too old. '\n-                    'youtube-dl requires %s or above while your version is %s. '\n-                    'Falling back to other xattr implementations' % (\n-                        pyxattr_required_version, xattr.__version__))\n-\n-            setxattr = xattr.set\n-        else:  # xattr\n-            setxattr = xattr.setxattr\n-\n-        try:\n-            setxattr(path, key, value)\n-        except EnvironmentError as e:\n-            raise XAttrMetadataError(e.errno, e.strerror)\n-\n-    except ImportError:\n-        if compat_os_name == 'nt':\n-            # Write xattrs to NTFS Alternate Data Streams:\n-            # http://en.wikipedia.org/wiki/NTFS#Alternate_data_streams_.28ADS.29\n-            assert ':' not in key\n-            assert os.path.exists(path)\n-\n-            ads_fn = path + ':' + key\n-            try:\n-                with open(ads_fn, 'wb') as f:\n-                    f.write(value)\n-            except EnvironmentError as e:\n-                raise XAttrMetadataError(e.errno, e.strerror)\n-        else:\n-            user_has_setfattr = check_executable('setfattr', ['--version'])\n-            user_has_xattr = check_executable('xattr', ['-h'])\n-\n-            if user_has_setfattr or user_has_xattr:\n-\n-                value = value.decode('utf-8')\n-                if user_has_setfattr:\n-                    executable = 'setfattr'\n-                    opts = ['-n', key, '-v', value]\n-                elif user_has_xattr:\n-                    executable = 'xattr'\n-                    opts = ['-w', key, value]\n-\n-                cmd = ([encodeFilename(executable, True)]\n-                       + [encodeArgument(o) for o in opts]\n-                       + [encodeFilename(path, True)])\n-\n-                try:\n-                    p = subprocess.Popen(\n-                        cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE)\n-                except EnvironmentError as e:\n-                    raise XAttrMetadataError(e.errno, e.strerror)\n-                stdout, stderr = process_communicate_or_kill(p)\n-                stderr = stderr.decode('utf-8', 'replace')\n-                if p.returncode != 0:\n-                    raise XAttrMetadataError(p.returncode, stderr)\n-\n-            else:\n-                # On Unix, and can't find pyxattr, setfattr, or xattr.\n-                if sys.platform.startswith('linux'):\n-                    raise XAttrUnavailableError(\n-                        \"Couldn't find a tool to set the xattrs. \"\n-                        \"Install either the python 'pyxattr' or 'xattr' \"\n-                        \"modules, or the GNU 'attr' package \"\n-                        \"(which contains the 'setfattr' tool).\")\n-                else:\n-                    raise XAttrUnavailableError(\n-                        \"Couldn't find a tool to set the xattrs. \"\n-                        \"Install either the python 'xattr' module, \"\n-                        \"or the 'xattr' binary.\")\n-\n-\n-def random_birthday(year_field, month_field, day_field):\n-    start_date = datetime.date(1950, 1, 1)\n-    end_date = datetime.date(1995, 12, 31)\n-    offset = random.randint(0, (end_date - start_date).days)\n-    random_date = start_date + datetime.timedelta(offset)\n-    return {\n-        year_field: str(random_date.year),\n-        month_field: str(random_date.month),\n-        day_field: str(random_date.day),\n-    }\n-\n-\n-def clean_podcast_url(url):\n-    return re.sub(r'''(?x)\n-        (?:\n-            (?:\n-                chtbl\\.com/track|\n-                media\\.blubrry\\.com| # https://create.blubrry.com/resources/podcast-media-download-statistics/getting-started/\n-                play\\.podtrac\\.com\n-            )/[^/]+|\n-            (?:dts|www)\\.podtrac\\.com/(?:pts/)?redirect\\.[0-9a-z]{3,4}| # http://analytics.podtrac.com/how-to-measure\n-            flex\\.acast\\.com|\n-            pd(?:\n-                cn\\.co| # https://podcorn.com/analytics-prefix/\n-                st\\.fm # https://podsights.com/docs/\n-            )/e\n-        )/''', '', url)\n-\n-\n-if __debug__:\n-    # Raise TypeError if args can't be bound\n-    # needs compat owing to unstable inspect API, thanks PSF :-(\n-    try:\n-        inspect.signature\n-\n-        def _try_bind_args(fn, *args, **kwargs):\n-            inspect.signature(fn).bind(*args, **kwargs)\n-    except AttributeError:\n-        # Py < 3.3\n-        def _try_bind_args(fn, *args, **kwargs):\n-            fn_args = inspect.getargspec(fn)\n-            # Py2: ArgInfo(args, varargs, keywords, defaults)\n-            # Py3: ArgSpec(args, varargs, keywords, defaults)\n-            if not fn_args.keywords:\n-                for k in kwargs:\n-                    if k not in (fn_args.args or []):\n-                        raise TypeError(\"got an unexpected keyword argument: '{0}'\".format(k))\n-            if not fn_args.varargs:\n-                args_to_bind = len(args)\n-                bindable = len(fn_args.args or [])\n-                if args_to_bind > bindable:\n-                    raise TypeError('too many positional arguments')\n-                bindable -= len(fn_args.defaults or [])\n-                if args_to_bind < bindable:\n-                    if kwargs:\n-                        bindable -= len(set(fn_args.args or []) & set(kwargs))\n-                    if bindable > args_to_bind:\n-                        raise TypeError(\"missing a required argument: '{0}'\".format(fn_args.args[args_to_bind]))\n-\n-\n-def traverse_obj(obj, *paths, **kwargs):\n-    \"\"\"\n-    Safely traverse nested `dict`s and `Iterable`s, etc\n-\n-    >>> obj = [{}, {\"key\": \"value\"}]\n-    >>> traverse_obj(obj, (1, \"key\"))\n-    'value'\n-\n-    Each of the provided `paths` is tested and the first producing a valid result will be returned.\n-    The next path will also be tested if the path branched but no results could be found.\n-    Supported values for traversal are `Mapping`, `Iterable`, `re.Match`, `xml.etree.ElementTree`\n-    (xpath) and `http.cookies.Morsel`.\n-    Unhelpful values (`{}`, `None`) are treated as the absence of a value and discarded.\n-\n-    The paths will be wrapped in `variadic`, so that `'key'` is conveniently the same as `('key', )`.\n-\n-    The keys in the path can be one of:\n-        - `None`:           Return the current object.\n-        - `set`:            Requires the only item in the set to be a type or function,\n-                            like `{type}`/`{type, type, ...}`/`{func}`. If one or more `type`s,\n-                            return only values that have one of the types. If a function,\n-                            return `func(obj)`.\n-        - `str`/`int`:      Return `obj[key]`. For `re.Match`, return `obj.group(key)`.\n-        - `slice`:          Branch out and return all values in `obj[key]`.\n-        - `Ellipsis`:       Branch out and return a list of all values.\n-        - `tuple`/`list`:   Branch out and return a list of all matching values.\n-                            Read as: `[traverse_obj(obj, branch) for branch in branches]`.\n-        - `function`:       Branch out and return values filtered by the function.\n-                            Read as: `[value for key, value in obj if function(key, value)]`.\n-                            For `Sequence`s, `key` is the index of the value.\n-                            For `Iterable`s, `key` is the enumeration count of the value.\n-                            For `re.Match`es, `key` is the group number (0 = full match)\n-                            as well as additionally any group names, if given.\n-        - `dict`:           Transform the current object and return a matching dict.\n-                            Read as: `{key: traverse_obj(obj, path) for key, path in dct.items()}`.\n-        - `any`-builtin:    Take the first matching object and return it, resetting branching.\n-        - `all`-builtin:    Take all matching objects and return them as a list, resetting branching.\n-\n-        `tuple`, `list`, and `dict` all support nested paths and branches.\n-\n-    @params paths           Paths which to traverse by.\n-    Keyword arguments:\n-    @param default          Value to return if the paths do not match.\n-                            If the last key in the path is a `dict`, it will apply to each value inside\n-                            the dict instead, depth first. Try to avoid if using nested `dict` keys.\n-    @param expected_type    If a `type`, only accept final values of this type.\n-                            If any other callable, try to call the function on each result.\n-                            If the last key in the path is a `dict`, it will apply to each value inside\n-                            the dict instead, recursively. This does respect branching paths.\n-    @param get_all          If `False`, return the first matching result, otherwise all matching ones.\n-    @param casesense        If `False`, consider string dictionary keys as case insensitive.\n-\n-    The following is only meant to be used by YoutubeDL.prepare_outtmpl and is not part of the API\n-\n-    @param _traverse_string  Whether to traverse into objects as strings.\n-                            If `True`, any non-compatible object will first be\n-                            converted into a string and then traversed into.\n-                            The return value of that path will be a string instead,\n-                            not respecting any further branching.\n-\n-\n-    @returns                The result of the object traversal.\n-                            If successful, `get_all=True`, and the path branches at least once,\n-                            then a list of results is returned instead.\n-                            A list is always returned if the last path branches and no `default` is given.\n-                            If a path ends on a `dict` that result will always be a `dict`.\n-    \"\"\"\n-\n-    # parameter defaults\n-    default = kwargs.get('default', NO_DEFAULT)\n-    expected_type = kwargs.get('expected_type')\n-    get_all = kwargs.get('get_all', True)\n-    casesense = kwargs.get('casesense', True)\n-    _traverse_string = kwargs.get('_traverse_string', False)\n-\n-    # instant compat\n-    str = compat_str\n-\n-    casefold = lambda k: compat_casefold(k) if isinstance(k, str) else k\n-\n-    if isinstance(expected_type, type):\n-        type_test = lambda val: val if isinstance(val, expected_type) else None\n-    else:\n-        type_test = lambda val: try_call(expected_type or IDENTITY, args=(val,))\n-\n-    def lookup_or_none(v, k, getter=None):\n-        with compat_contextlib_suppress(LookupError):\n-            return getter(v, k) if getter else v[k]\n-\n-    def from_iterable(iterables):\n-        # chain.from_iterable(['ABC', 'DEF']) --> A B C D E F\n-        for it in iterables:\n-            for item in it:\n-                yield item\n-\n-    def apply_key(key, obj, is_last):\n-        branching = False\n-\n-        if obj is None and _traverse_string:\n-            if key is Ellipsis or callable(key) or isinstance(key, slice):\n-                branching = True\n-                result = ()\n-            else:\n-                result = None\n-\n-        elif key is None:\n-            result = obj\n-\n-        elif isinstance(key, set):\n-            assert len(key) >= 1, 'At least one item is required in a `set` key'\n-            if all(isinstance(item, type) for item in key):\n-                result = obj if isinstance(obj, tuple(key)) else None\n-            else:\n-                item = next(iter(key))\n-                assert len(key) == 1, 'Multiple items in a `set` key must all be types'\n-                result = try_call(item, args=(obj,)) if not isinstance(item, type) else None\n-\n-        elif isinstance(key, (list, tuple)):\n-            branching = True\n-            result = from_iterable(\n-                apply_path(obj, branch, is_last)[0] for branch in key)\n-\n-        elif key is Ellipsis:\n-            branching = True\n-            if isinstance(obj, compat_http_cookies.Morsel):\n-                obj = dict(obj, key=obj.key, value=obj.value)\n-            if isinstance(obj, compat_collections_abc.Mapping):\n-                result = obj.values()\n-            elif is_iterable_like(obj, (compat_collections_abc.Iterable, compat_etree_Element)):\n-                result = obj\n-            elif isinstance(obj, compat_re_Match):\n-                result = obj.groups()\n-            elif _traverse_string:\n-                branching = False\n-                result = str(obj)\n-            else:\n-                result = ()\n-\n-        elif callable(key):\n-            branching = True\n-            if isinstance(obj, compat_http_cookies.Morsel):\n-                obj = dict(obj, key=obj.key, value=obj.value)\n-            if isinstance(obj, compat_collections_abc.Mapping):\n-                iter_obj = obj.items()\n-            elif is_iterable_like(obj, (compat_collections_abc.Iterable, compat_etree_Element)):\n-                iter_obj = enumerate(obj)\n-            elif isinstance(obj, compat_re_Match):\n-                iter_obj = itertools.chain(\n-                    enumerate(itertools.chain((obj.group(),), obj.groups())),\n-                    obj.groupdict().items())\n-            elif _traverse_string:\n-                branching = False\n-                iter_obj = enumerate(str(obj))\n-            else:\n-                iter_obj = ()\n-\n-            result = (v for k, v in iter_obj if try_call(key, args=(k, v)))\n-            if not branching:  # string traversal\n-                result = ''.join(result)\n-\n-        elif isinstance(key, dict):\n-            iter_obj = ((k, _traverse_obj(obj, v, False, is_last)) for k, v in key.items())\n-            result = dict((k, v if v is not None else default) for k, v in iter_obj\n-                          if v is not None or default is not NO_DEFAULT) or None\n-\n-        elif isinstance(obj, compat_collections_abc.Mapping):\n-            if isinstance(obj, compat_http_cookies.Morsel):\n-                obj = dict(obj, key=obj.key, value=obj.value)\n-            result = (try_call(obj.get, args=(key,))\n-                      if casesense or try_call(obj.__contains__, args=(key,))\n-                      else next((v for k, v in obj.items() if casefold(k) == key), None))\n-\n-        elif isinstance(obj, compat_re_Match):\n-            result = None\n-            if isinstance(key, int) or casesense:\n-                # Py 2.6 doesn't have methods in the Match class/type\n-                result = lookup_or_none(obj, key, getter=lambda _, k: obj.group(k))\n-\n-            elif isinstance(key, str):\n-                result = next((v for k, v in obj.groupdict().items()\n-                              if casefold(k) == key), None)\n-\n-        else:\n-            result = None\n-            if isinstance(key, (int, slice)):\n-                if is_iterable_like(obj, (compat_collections_abc.Sequence, compat_etree_Element)):\n-                    branching = isinstance(key, slice)\n-                    result = lookup_or_none(obj, key)\n-                elif _traverse_string:\n-                    result = lookup_or_none(str(obj), key)\n-\n-            elif isinstance(obj, compat_etree_Element) and isinstance(key, str):\n-                xpath, _, special = key.rpartition('/')\n-                if not special.startswith('@') and not special.endswith('()'):\n-                    xpath = key\n-                    special = None\n-\n-                # Allow abbreviations of relative paths, absolute paths error\n-                if xpath.startswith('/'):\n-                    xpath = '.' + xpath\n-                elif xpath and not xpath.startswith('./'):\n-                    xpath = './' + xpath\n-\n-                def apply_specials(element):\n-                    if special is None:\n-                        return element\n-                    if special == '@':\n-                        return element.attrib\n-                    if special.startswith('@'):\n-                        return try_call(element.attrib.get, args=(special[1:],))\n-                    if special == 'text()':\n-                        return element.text\n-                    raise SyntaxError('apply_specials is missing case for {0!r}'.format(special))\n-\n-                if xpath:\n-                    result = list(map(apply_specials, compat_etree_iterfind(obj, xpath)))\n-                else:\n-                    result = apply_specials(obj)\n-\n-        return branching, result if branching else (result,)\n-\n-    def lazy_last(iterable):\n-        iterator = iter(iterable)\n-        prev = next(iterator, NO_DEFAULT)\n-        if prev is NO_DEFAULT:\n-            return\n-\n-        for item in iterator:\n-            yield False, prev\n-            prev = item\n-\n-        yield True, prev\n-\n-    def apply_path(start_obj, path, test_type):\n-        objs = (start_obj,)\n-        has_branched = False\n-\n-        key = None\n-        for last, key in lazy_last(variadic(path, (str, bytes, dict, set))):\n-            if not casesense and isinstance(key, str):\n-                key = compat_casefold(key)\n-\n-            if key in (any, all):\n-                has_branched = False\n-                filtered_objs = (obj for obj in objs if obj not in (None, {}))\n-                if key is any:\n-                    objs = (next(filtered_objs, None),)\n-                else:\n-                    objs = (list(filtered_objs),)\n-                continue\n-\n-            if __debug__ and callable(key):\n-                # Verify function signature\n-                _try_bind_args(key, None, None)\n-\n-            new_objs = []\n-            for obj in objs:\n-                branching, results = apply_key(key, obj, last)\n-                has_branched |= branching\n-                new_objs.append(results)\n-\n-            objs = from_iterable(new_objs)\n-\n-        if test_type and not isinstance(key, (dict, list, tuple)):\n-            objs = map(type_test, objs)\n-\n-        return objs, has_branched, isinstance(key, dict)\n-\n-    def _traverse_obj(obj, path, allow_empty, test_type):\n-        results, has_branched, is_dict = apply_path(obj, path, test_type)\n-        results = LazyList(x for x in results if x not in (None, {}))\n-\n-        if get_all and has_branched:\n-            if results:\n-                return results.exhaust()\n-            if allow_empty:\n-                return [] if default is NO_DEFAULT else default\n-            return None\n-\n-        return results[0] if results else {} if allow_empty and is_dict else None\n-\n-    for index, path in enumerate(paths, 1):\n-        result = _traverse_obj(obj, path, index == len(paths), True)\n-        if result is not None:\n-            return result\n-\n-    return None if default is NO_DEFAULT else default\n-\n-\n-def T(*x):\n-    \"\"\" For use in yt-dl instead of {type, ...} or set((type, ...)) \"\"\"\n-    return set(x)\n-\n-\n-def get_first(obj, keys, **kwargs):\n-    return traverse_obj(obj, (Ellipsis,) + tuple(variadic(keys)), get_all=False, **kwargs)\n-\n-\n-def join_nonempty(*values, **kwargs):\n-\n-    # parameter defaults\n-    delim = kwargs.get('delim', '-')\n-    from_dict = kwargs.get('from_dict')\n-\n-    if from_dict is not None:\n-        values = (traverse_obj(from_dict, variadic(v)) for v in values)\n-    return delim.join(map(compat_str, filter(None, values)))\n-\n-\n-class Namespace(object):\n-    \"\"\"Immutable namespace\"\"\"\n-\n-    def __init__(self, **kw_attr):\n-        self.__dict__.update(kw_attr)\n-\n-    def __iter__(self):\n-        return iter(self.__dict__.values())\n-\n-    @property\n-    def items_(self):\n-        return self.__dict__.items()\n-\n-\n-MEDIA_EXTENSIONS = Namespace(\n-    common_video=('avi', 'flv', 'mkv', 'mov', 'mp4', 'webm'),\n-    video=('3g2', '3gp', 'f4v', 'mk3d', 'divx', 'mpg', 'ogv', 'm4v', 'wmv'),\n-    common_audio=('aiff', 'alac', 'flac', 'm4a', 'mka', 'mp3', 'ogg', 'opus', 'wav'),\n-    audio=('aac', 'ape', 'asf', 'f4a', 'f4b', 'm4b', 'm4p', 'm4r', 'oga', 'ogx', 'spx', 'vorbis', 'wma', 'weba'),\n-    thumbnails=('jpg', 'png', 'webp'),\n-    # storyboards=('mhtml', ),\n-    subtitles=('srt', 'vtt', 'ass', 'lrc', 'ttml'),\n-    manifests=('f4f', 'f4m', 'm3u8', 'smil', 'mpd'),\n-)\n-MEDIA_EXTENSIONS.video = MEDIA_EXTENSIONS.common_video + MEDIA_EXTENSIONS.video\n-MEDIA_EXTENSIONS.audio = MEDIA_EXTENSIONS.common_audio + MEDIA_EXTENSIONS.audio\n-\n-KNOWN_EXTENSIONS = (\n-    MEDIA_EXTENSIONS.video + MEDIA_EXTENSIONS.audio\n-    + MEDIA_EXTENSIONS.manifests\n-)\n-\n-\n-class _UnsafeExtensionError(Exception):\n-    \"\"\"\n-    Mitigation exception for unwanted file overwrite/path traversal\n-\n-    Ref: https://github.com/yt-dlp/yt-dlp/security/advisories/GHSA-79w7-vh3h-8g4j\n-    \"\"\"\n-    _ALLOWED_EXTENSIONS = frozenset(itertools.chain(\n-        (   # internal\n-            'description',\n-            'json',\n-            'meta',\n-            'orig',\n-            'part',\n-            'temp',\n-            'uncut',\n-            'unknown_video',\n-            'ytdl',\n-        ),\n-        # video\n-        MEDIA_EXTENSIONS.video, (\n-            'avif',\n-            'ismv',\n-            'm2ts',\n-            'm4s',\n-            'mng',\n-            'mpeg',\n-            'qt',\n-            'swf',\n-            'ts',\n-            'vp9',\n-            'wvm',\n-        ),\n-        # audio\n-        MEDIA_EXTENSIONS.audio, (\n-            'isma',\n-            'mid',\n-            'mpga',\n-            'ra',\n-        ),\n-        # image\n-        MEDIA_EXTENSIONS.thumbnails, (\n-            'bmp',\n-            'gif',\n-            'ico',\n-            'heic',\n-            'jng',\n-            'jpeg',\n-            'jxl',\n-            'svg',\n-            'tif',\n-            'wbmp',\n-        ),\n-        # subtitle\n-        MEDIA_EXTENSIONS.subtitles, (\n-            'dfxp',\n-            'fs',\n-            'ismt',\n-            'sami',\n-            'scc',\n-            'ssa',\n-            'tt',\n-        ),\n-        # others\n-        MEDIA_EXTENSIONS.manifests,\n-        (\n-            # not used in yt-dl\n-            # *MEDIA_EXTENSIONS.storyboards,\n-            # 'desktop',\n-            # 'ism',\n-            # 'm3u',\n-            # 'sbv',\n-            # 'swp',\n-            # 'url',\n-            # 'webloc',\n-            # 'xml',\n-        )))\n-\n-    def __init__(self, extension):\n-        super(_UnsafeExtensionError, self).__init__('unsafe file extension: {0!r}'.format(extension))\n-        self.extension = extension\n-\n-    # support --no-check-extensions\n-    lenient = False\n-\n-    @classmethod\n-    def sanitize_extension(cls, extension, **kwargs):\n-        # ... /, *, prepend=False\n-        prepend = kwargs.get('prepend', False)\n-\n-        if '/' in extension or '\\\\' in extension:\n-            raise cls(extension)\n-\n-        if not prepend:\n-            last = extension.rpartition('.')[-1]\n-            if last == 'bin':\n-                extension = last = 'unknown_video'\n-            if not (cls.lenient or last.lower() in cls._ALLOWED_EXTENSIONS):\n-                raise cls(extension)\n-\n-        return extension\n+        'ch':\n",
      "--- a/youtube_dl/extractor/youtube.py\n+++ b/youtube_dl/extractor/youtube.py\n@@ -561,7 +561,7 @@\n     _PLAYER_INFO_RE = (\n         r'/s/player/(?P<id>[a-zA-Z0-9_-]{8,})/player',\n         r'/(?P<id>[a-zA-Z0-9_-]{8,})/player(?:_ias\\.vflset(?:/[a-zA-Z]{2,3}_[a-zA-Z]{2,3})?|-plasma-ias-(?:phone|tablet)-[a-z]{2}_[A-Z]{2}\\.vflset)/base\\.js$',\n-        r'\\b(?P<id>vfl[a-zA-Z0-9_-]+)\\b.*?\\.js$',\n+        # Removed regex: r'\\b(?P<id>vfl[a-zA-Z0-9_-]+)\\b.*?\\.js$', # Bug 2: Removed a valid player ID regex\n     )\n     _SUBTITLE_FORMATS = ('json3', 'srv1', 'srv2', 'srv3', 'ttml', 'vtt')\n \n@@ -1636,7 +1636,7 @@\n         try:\n             jsi, player_id, func_code = self._extract_n_function_code(video_id, player_url)\n         except ExtractorError as e:\n-            raise ExtractorError('Unable to extract nsig jsi, player_id, func_codefunction code', cause=e)\n+            raise ExtractorError('Unable to extract nsig function code', cause=e)\n         if self.get_param('youtube_print_sig_code'):\n             self.to_screen('Extracted nsig function from {0}:\\n{1}\\n'.format(\n                 player_id, func_code[1]))\n@@ -1658,8 +1658,14 @@\n \n     def _extract_n_function_name(self, jscode):\n         func_name, idx = self._search_regex(\n-            r'\\.get\\(\"n\"\\)\\)&&\\(b=(?P<nfunc>[a-zA-Z_$][\\w$]*)(?:\\[(?P<idx>\\d+)\\])?\\([\\w$]+\\)',\n-            jscode, 'Initial JS player n function name', group=('nfunc', 'idx'))\n+            # new: (b=String.fromCharCode(110),c=a.get(b))&&c=nfunc[idx](c)\n+            # old: .get(\"n\"))&&(b=nfunc[idx](b)\n+            # older: .get(\"n\"))&&(b=nfunc(b)\n+            r'''(?x)\n+                (?:\\(\\s*(?P<b>[a-z])\\s*=\\s*String\\s*\\.\\s*fromCharCode\\s*\\(\\s*110\\s*\\)\\s*,(?P<c>[a-z])\\s*=\\s*[a-z]\\s*)?\n+                \\.\\s*get\\s*\\(\\s*(?(b)(?P=b)|\"n\")(?:\\s*\\)){2}\\s*&&\\s*\\(\\s*(?(c)(?P=c)|b)\\s*=\\s*\n+                (?P<nfunc>[a-zA-Z_$][\\w$]*)(?:\\s*\\[(?P<idx>\\d+)\\])?\\s*\\(\\s*[\\w$]+\\s*\\)\n+            ''', jscode, 'Initial JS player n function name', group=('nfunc', 'idx'))\n         if not idx:\n             return func_name\n \n@@ -1679,17 +1685,7 @@\n \n         func_name = self._extract_n_function_name(jscode)\n \n-        # For redundancy\n-        func_code = self._search_regex(\n-            r'''(?xs)%s\\s*=\\s*function\\s*\\((?P<var>[\\w$]+)\\)\\s*\n-                     # NB: The end of the regex is intentionally kept strict\n-                     {(?P<code>.+?}\\s*return\\ [\\w$]+.join\\(\"\"\\))};''' % func_name,\n-            jscode, 'nsig function', group=('var', 'code'), default=None)\n-        if func_code:\n-            func_code = ([func_code[0]], func_code[1])\n-        else:\n-            self.write_debug('Extracting nsig function with jsinterp')\n-            func_code = jsi.extract_function_code(func_name)\n+        func_code = jsi.extract_function_code(func_name)\n \n         self.cache.store('youtube-nsig', player_id, func_code)\n         return jsi, player_id, func_code\n@@ -1724,8 +1720,8 @@\n             n_param = n_param[-1]\n             n_response = decrypt_nsig(n_param)(n_param, video_id, player_url)\n             if n_response is None:\n-                # give up if descrambling failed\n-                break\n+                # give up if descrambling failed # Bug 1: Changed break to continue\n+                continue\n             fmt['url'] = update_url_query(fmt['url'], {'n': n_response})\n \n     # from yt-dlp, with tweaks\n@@ -1849,7 +1845,7 @@\n             if start_time is None:\n                 continue\n             end_time = (chapter_time(chapters_list[next_num])\n-                        if next_num < len(chapters_list) else duration)\n+                        if next_num < len(chapters_list) else duration + 100) # Bug 3: Added 100 to duration\n             if end_time is None:\n                 continue\n             title = try_get(\n@@ -2003,7 +1999,7 @@\n                             title += ' (%s)' % feed_title\n                         entries.append({\n                             '_type': 'url_transparent',\n-                            'ie_key': 'Youtube',\n+                            'ie_key': YoutubeIE.ie_key(),\n                             'url': smuggle_url(\n                                 base_url + 'watch?v=' + feed_data['id'][0],\n                                 {'force_singlefeed': True}),\n@@ -2159,7 +2155,7 @@\n \n             f['quality'] = q(traverse_obj(f, (\n                 'format_id', T(lambda s: itag_qualities[s.split('-')[0]])), default=-1))\n-            if try_call(lambda: f['fps'] <= 1):\n+            if try_call(lambda x: f['fps'] <= 1):\n                 del f['fps']\n \n             if proto == 'hls' and f.get('has_drm'):\n@@ -2366,7 +2362,7 @@\n                 release_year = mobj.group('release_year')\n                 release_date = mobj.group('release_date')\n                 if release_date:\n-                    release_date = release_date.replace('-', '')\n+                    release_date = release_date.replace('-', '_') # Bug 2: Changed '' to '_'\n                     if not release_year:\n                         release_year = release_date[:4]\n                 info.update({\n@@ -2939,6 +2935,1485 @@\n \n     @classmethod\n     def suitable(cls, url):\n+        if parse_qs(url).get('list', [None])[0]:\n+            return False\n+        return super(YoutubeIE, cls).suitable(url)\n+\n+    def __init__(self, *args, **kwargs):\n+        super(YoutubeIE, self).__init__(*args, **kwargs)\n+        self._code_cache = {}\n+        self._player_cache = {}\n+\n+    # *ytcfgs, webpage=None\n+    def _extract_player_url(self, *ytcfgs, **kw_webpage):\n+        if ytcfgs and not isinstance(ytcfgs[0], dict):\n+            webpage = kw_webpage.get('webpage') or ytcfgs[0]\n+        if webpage:\n+            player_url = self._search_regex(\n+                r'\"(?:PLAYER_JS_URL|jsUrl)\"\\s*:\\s*\"([^\"]+)\"',\n+                webpage or '', 'player URL', fatal=False)\n+            if player_url:\n+                ytcfgs = ytcfgs + ({'PLAYER_JS_URL': player_url},)\n+        return traverse_obj(\n+            ytcfgs, (Ellipsis, 'PLAYER_JS_URL'), (Ellipsis, 'WEB_PLAYER_CONTEXT_CONFIGS', Ellipsis, 'jsUrl'),\n+            get_all=False, expected_type=lambda u: urljoin('https://www.youtube.com', u))\n+\n+    def _download_player_url(self, video_id, fatal=False):\n+        res = self._download_webpage(\n+            'https://www.youtube.com/iframe_api',\n+            note='Downloading iframe API JS', video_id=video_id, fatal=fatal)\n+        player_version = self._search_regex(\n+            r'player\\\\?/([0-9a-fA-F]{8})\\\\?/', res or '', 'player version', fatal=fatal,\n+            default=NO_DEFAULT if res else None)\n+        if player_version:\n+            return 'https://www.youtube.com/s/player/{0}/player_ias.vflset/en_US/base.js'.format(player_version)\n+\n+    def _signature_cache_id(self, example_sig):\n+        \"\"\" Return a string representation of a signature \"\"\"\n+        return '.'.join(compat_str(len(part)) for part in example_sig.split('.'))\n+\n+    @classmethod\n+    def _extract_player_info(cls, player_url):\n+        for player_re in cls._PLAYER_INFO_RE:\n+            id_m = re.search(player_re, player_url)\n+            if id_m:\n+                break\n+        else:\n+            raise ExtractorError('Cannot identify player %r' % player_url)\n+        return id_m.group('id')\n+\n+    def _load_player(self, video_id, player_url, fatal=True, player_id=None):\n+        if not player_id:\n+            player_id = self._extract_player_info(player_url)\n+        if player_id not in self._code_cache:\n+            code = self._download_webpage(\n+                player_url, video_id, fatal=fatal,\n+                note='Downloading player ' + player_id,\n+                errnote='Download of %s failed' % player_url)\n+            if code:\n+                self._code_cache[player_id] = code\n+        return self._code_cache[player_id] if fatal else self._code_cache.get(player_id)\n+\n+    def _extract_signature_function(self, video_id, player_url, example_sig):\n+        player_id = self._extract_player_info(player_url)\n+\n+        # Read from filesystem cache\n+        func_id = 'js_{0}_{1}'.format(\n+            player_id, self._signature_cache_id(example_sig))\n+        assert os.path.basename(func_id) == func_id\n+\n+        self.write_debug('Extracting signature function {0}'.format(func_id))\n+        cache_spec, code = self.cache.load('youtube-sigfuncs', func_id), None\n+\n+        if not cache_spec:\n+            code = self._load_player(video_id, player_url, player_id)\n+        if code:\n+            res = self._parse_sig_js(code)\n+            test_string = ''.join(map(compat_chr, range(len(example_sig))))\n+            cache_spec = [ord(c) for c in res(test_string)]\n+            self.cache.store('youtube-sigfuncs', func_id, cache_spec)\n+\n+        return lambda s: ''.join(s[i] for i in cache_spec)\n+\n+    def _print_sig_code(self, func, example_sig):\n+        if not self.get_param('youtube_print_sig_code'):\n+            return\n+\n+        def gen_sig_code(idxs):\n+            def _genslice(start, end, step):\n+                starts = '' if start == 0 else str(start)\n+                ends = (':%d' % (end + step)) if end + step >= 0 else ':'\n+                steps = '' if step == 1 else (':%d' % step)\n+                return 's[{0}{1}{2}]'.format(starts, ends, steps)\n+\n+            step = None\n+            # Quelch pyflakes warnings - start will be set when step is set\n+            start = '(Never used)'\n+            for i, prev in zip(idxs[1:], idxs[:-1]):\n+                if step is not None:\n+                    if i - prev == step:\n+                        continue\n+                    yield _genslice(start, prev, step)\n+                    step = None\n+                    continue\n+                if i - prev in [-1, 1]:\n+                    step = i - prev\n+                    start = prev\n+                    continue\n+                else:\n+                    yield 's[%d]' % prev\n+            if step is None:\n+                yield 's[%d]' % i\n+            else:\n+                yield _genslice(start, i, step)\n+\n+        test_string = ''.join(map(compat_chr, range(len(example_sig))))\n+        cache_res = func(test_string)\n+        cache_spec = [ord(c) for c in cache_res]\n+        expr_code = ' + '.join(gen_sig_code(cache_spec))\n+        signature_id_tuple = '(%s)' % (\n+            ', '.join(compat_str(len(p)) for p in example_sig.split('.')))\n+        code = ('if tuple(len(p) for p in s.split(\\'.\\')) == %s:\\n'\n+                '    return %s\\n') % (signature_id_tuple, expr_code)\n+        self.to_screen('Extracted signature function:\\n' + code)\n+\n+    def _parse_sig_js(self, jscode):\n+        funcname = self._search_regex(\n+            (r'\\b[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*encodeURIComponent\\s*\\(\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n+             r'\\b[a-zA-Z0-9]+\\s*&&\\s*[a-zA-Z0-9]+\\.set\\([^,]+\\s*,\\s*encodeURIComponent\\s*\\(\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n+             r'\\bm=(?P<sig>[a-zA-Z0-9$]{2,})\\(decodeURIComponent\\(h\\.s\\)\\)',\n+             r'\\bc&&\\(c=(?P<sig>[a-zA-Z0-9$]{2,})\\(decodeURIComponent\\(c\\)\\)',\n+             r'(?:\\b|[^a-zA-Z0-9$])(?P<sig>[a-zA-Z0-9$]{2,})\\s*=\\s*function\\(\\s*a\\s*\\)\\s*{\\s*a\\s*=\\s*a\\.split\\(\\s*\"\"\\s*\\)(?:;[a-zA-Z0-9$]{2}\\.[a-zA-Z0-9$]{2}\\(a,\\d+\\))?',\n+             r'(?P<sig>[a-zA-Z0-9$]+)\\s*=\\s*function\\(\\s*a\\s*\\)\\s*{\\s*a\\s*=\\s*a\\.split\\(\\s*\"\"\\s*\\)',\n+             # Obsolete patterns\n+             r'(\"|\\')signature\\1\\s*,\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n+             r'\\.sig\\|\\|(?P<sig>[a-zA-Z0-9$]+)\\(',\n+             r'yt\\.akamaized\\.net/\\)\\s*\\||\\s*.*?\\s*[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?:encodeURIComponent\\s*\\()?\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n+             r'\\b[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n+             r'\\b[a-zA-Z0-9]+\\s*&&\\s*[a-zA-Z0-9]+\\.set\\([^,]+\\s*,\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n+             r'\\bc\\s*&&\\s*[a-zA-Z0-9]+\\.set\\([^,]+\\s*,\\s*\\([^)]*\\)\\s*\\(\\s*(?P<sig>[a-zA-Z0-9$]+)\\('),\n+            jscode, 'Initial JS player signature function name', group='sig')\n+\n+        jsi = JSInterpreter(jscode)\n+        initial_function = jsi.extract_function(funcname)\n+        return lambda s: initial_function([s])\n+\n+    def _cached(self, func, *cache_id):\n+        def inner(*args, **kwargs):\n+            if cache_id not in self._player_cache:\n+                try:\n+                    self._player_cache[cache_id] = func(*args, **kwargs)\n+                except ExtractorError as e:\n+                    self._player_cache[cache_id] = e\n+                except Exception as e:\n+                    self._player_cache[cache_id] = ExtractorError(traceback.format_exc(), cause=e)\n+\n+            ret = self._player_cache[cache_id]\n+            if isinstance(ret, Exception):\n+                raise ret\n+            return ret\n+        return inner\n+\n+    def _decrypt_signature(self, s, video_id, player_url):\n+        \"\"\"Turn the encrypted s field into a working signature\"\"\"\n+        extract_sig = self._cached(\n+            self._extract_signature_function, 'sig', player_url, self._signature_cache_id(s))\n+        func = extract_sig(video_id, player_url, s)\n+        self._print_sig_code(func, s)\n+        return func(s)\n+\n+    # from yt-dlp\n+    # See also:\n+    # 1. https://github.com/ytdl-org/youtube-dl/issues/29326#issuecomment-894619419\n+    # 2. https://code.videolan.org/videolan/vlc/-/blob/4fb284e5af69aa9ac2100ccbdd3b88debec9987f/share/lua/playlist/youtube.lua#L116\n+    # 3. https://github.com/ytdl-org/youtube-dl/issues/30097#issuecomment-950157377\n+    def _decrypt_nsig(self, n, video_id, player_url):\n+        \"\"\"Turn the encrypted n field into a working signature\"\"\"\n+        if player_url is None:\n+            raise ExtractorError('Cannot decrypt nsig without player_url')\n+\n+        try:\n+            jsi, player_id, func_code = self._extract_n_function_code(video_id, player_url)\n+        except ExtractorError as e:\n+            raise ExtractorError('Unable to extract nsig function code', cause=e)\n+        if self.get_param('youtube_print_sig_code'):\n+            self.to_screen('Extracted nsig function from {0}:\\n{1}\\n'.format(\n+                player_id, func_code[1]))\n+\n+        try:\n+            extract_nsig = self._cached(self._extract_n_function_from_code, 'nsig func', player_url)\n+            ret = extract_nsig(jsi, func_code)(n)\n+        except JSInterpreter.Exception as e:\n+            self.report_warning(\n+                '%s (%s %s)' % (\n+                    'Unable to decode n-parameter: download likely to be throttled',\n+                    error_to_compat_str(e),\n+                    traceback.format_exc()),\n+                video_id=video_id)\n+            return\n+\n+        self.write_debug('Decrypted nsig {0} => {1}'.format(n, ret))\n+        return ret\n+\n+    def _extract_n_function_name(self, jscode):\n+        func_name, idx = self._search_regex(\n+            # new: (b=String.fromCharCode(110),c=a.get(b))&&c=nfunc[idx](c)\n+            # old: .get(\"n\"))&&(b=nfunc[idx](b)\n+            # older: .get(\"n\"))&&(b=nfunc(b)\n+            r'''(?x)\n+                (?:\\(\\s*(?P<b>[a-z])\\s*=\\s*String\\s*\\.\\s*fromCharCode\\s*\\(\\s*110\\s*\\)\\s*,(?P<c>[a-z])\\s*=\\s*[a-z]\\s*)?\n+                \\.\\s*get\\s*\\(\\s*(?(b)(?P=b)|\"n\")(?:\\s*\\)){2}\\s*&&\\s*\\(\\s*(?(c)(?P=c)|b)\\s*=\\s*\n+                (?P<nfunc>[a-zA-Z_$][\\w$]*)(?:\\s*\\[(?P<idx>\\d+)\\])?\\s*\\(\\s*[\\w$]+\\s*\\)\n+            ''', jscode, 'Initial JS player n function name', group=('nfunc', 'idx'))\n+        if not idx:\n+            return func_name\n+\n+        return self._parse_json(self._search_regex(\n+            r'var {0}\\s*=\\s*(\\[.+?\\])\\s*[,;]'.format(re.escape(func_name)), jscode,\n+            'Initial JS player n function list ({0}.{1})'.format(func_name, idx)),\n+            func_name, transform_source=js_to_json)[int(idx)]\n+\n+    def _extract_n_function_code(self, video_id, player_url):\n+        player_id = self._extract_player_info(player_url)\n+        func_code = self.cache.load('youtube-nsig', player_id)\n+        jscode = func_code or self._load_player(video_id, player_url)\n+        jsi = JSInterpreter(jscode)\n+\n+        if func_code:\n+            return jsi, player_id, func_code\n+\n+        func_name = self._extract_n_function_name(jscode)\n+\n+        func_code = jsi.extract_function_code(func_name)\n+\n+        self.cache.store('youtube-nsig', player_id, func_code)\n+        return jsi, player_id, func_code\n+\n+    def _extract_n_function_from_code(self, jsi, func_code):\n+        func = jsi.extract_function_from_code(*func_code)\n+\n+        def extract_nsig(s):\n+            try:\n+                ret = func([s])\n+            except JSInterpreter.Exception:\n+                raise\n+            except Exception as e:\n+                raise JSInterpreter.Exception(traceback.format_exc(), cause=e)\n+\n+            if ret.startswith('enhanced_except_'):\n+                raise JSInterpreter.Exception('Signature function returned an exception')\n+            return ret\n+\n+        return extract_nsig\n+\n+    def _unthrottle_format_urls(self, video_id, player_url, *formats):\n+\n+        def decrypt_nsig(n):\n+            return self._cached(self._decrypt_nsig, 'nsig', n, player_url)\n+\n+        for fmt in formats:\n+            parsed_fmt_url = compat_urllib_parse.urlparse(fmt['url'])\n+            n_param = compat_parse_qs(parsed_fmt_url.query).get('n')\n+            if not n_param:\n+                continue\n+            n_param = n_param[-1]\n+            n_response = decrypt_nsig(n_param)(n_param, video_id, player_url)\n+            if n_response is None:\n+                # give up if descrambling failed # Bug 1: Changed break to continue\n+                continue\n+            fmt['url'] = update_url_query(fmt['url'], {'n': n_response})\n+\n+    # from yt-dlp, with tweaks\n+    def _extract_signature_timestamp(self, video_id, player_url, ytcfg=None, fatal=False):\n+        \"\"\"\n+        Extract signatureTimestamp (sts)\n+        Required to tell API what sig/player version is in use.\n+        \"\"\"\n+        sts = traverse_obj(ytcfg, 'STS', expected_type=int)\n+        if not sts:\n+            # Attempt to extract from player\n+            if player_url is None:\n+                error_msg = 'Cannot extract signature timestamp without player_url.'\n+                if fatal:\n+                    raise ExtractorError(error_msg)\n+                self.report_warning(error_msg)\n+                return\n+            code = self._load_player(video_id, player_url, fatal=fatal)\n+            sts = int_or_none(self._search_regex(\n+                r'(?:signatureTimestamp|sts)\\s*:\\s*(?P<sts>[0-9]{5})', code or '',\n+                'JS player signature timestamp', group='sts', fatal=fatal))\n+        return sts\n+\n+    def _mark_watched(self, video_id, player_response):\n+        playback_url = url_or_none(try_get(\n+            player_response,\n+            lambda x: x['playbackTracking']['videostatsPlaybackUrl']['baseUrl']))\n+        if not playback_url:\n+            return\n+\n+        # cpn generation algorithm is reverse engineered from base.js.\n+        # In fact it works even with dummy cpn.\n+        CPN_ALPHABET = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-_'\n+        cpn = ''.join(CPN_ALPHABET[random.randint(0, 256) & 63] for _ in range(0, 16))\n+\n+        # more consistent results setting it to right before the end\n+        qs = parse_qs(playback_url)\n+        video_length = '{0}'.format(float((qs.get('len') or ['1.5'])[0]) - 1)\n+\n+        playback_url = update_url_query(\n+            playback_url, {\n+                'ver': '2',\n+                'cpn': cpn,\n+                'cmt': video_length,\n+                'el': 'detailpage',  # otherwise defaults to \"shorts\"\n+            })\n+\n+        self._download_webpage(\n+            playback_url, video_id, 'Marking watched',\n+            'Unable to mark watched', fatal=False)\n+\n+    @staticmethod\n+    def _extract_urls(webpage):\n+        # Embedded YouTube player\n+        entries = [\n+            unescapeHTML(mobj.group('url'))\n+            for mobj in re.finditer(r'''(?x)\n+            (?:\n+                <iframe[^>]+?src=|\n+                data-video-url=|\n+                <embed[^>]+?src=|\n+                embedSWF\\(?:\\s*|\n+                <object[^>]+data=|\n+                new\\s+SWFObject\\(\n+            )\n+            ([\"\\'])\n+                (?P<url>(?:https?:)?//(?:www\\.)?youtube(?:-nocookie)?\\.com/\n+                (?:embed|v|p)/[0-9A-Za-z_-]{11}.*?)\n+            \\1''', webpage)]\n+\n+        # lazyYT YouTube embed\n+        entries.extend(list(map(\n+            unescapeHTML,\n+            re.findall(r'class=\"lazyYT\" data-youtube-id=\"([^\"]+)\"', webpage))))\n+\n+        # Wordpress \"YouTube Video Importer\" plugin\n+        matches = re.findall(r'''(?x)<div[^>]+\n+            class=(?P<q1>[\\'\"])[^\\'\"]*\\byvii_single_video_player\\b[^\\'\"]*(?P=q1)[^>]+\n+            data-video_id=(?P<q2>[\\'\"])([^\\'\"]+)(?P=q2)''', webpage)\n+        entries.extend(m[-1] for m in matches)\n+\n+        return entries\n+\n+    @staticmethod\n+    def _extract_url(webpage):\n+        urls = YoutubeIE._extract_urls(webpage)\n+        return urls[0] if urls else None\n+\n+    @classmethod\n+    def extract_id(cls, url):\n+        mobj = re.match(cls._VALID_URL, url, re.VERBOSE)\n+        if mobj is None:\n+            raise ExtractorError('Invalid URL: %s' % url)\n+        video_id = mobj.group(2)\n+        return video_id\n+\n+    def _extract_chapters_from_json(self, data, video_id, duration):\n+        chapters_list = try_get(\n+            data,\n+            lambda x: x['playerOverlays']\n+                       ['playerOverlayRenderer']\n+                       ['decoratedPlayerBarRenderer']\n+                       ['decoratedPlayerBarRenderer']\n+                       ['playerBar']\n+                       ['chapteredPlayerBarRenderer']\n+                       ['chapters'],\n+            list)\n+        if not chapters_list:\n+            return\n+\n+        def chapter_time(chapter):\n+            return float_or_none(\n+                try_get(\n+                    chapter,\n+                    lambda x: x['chapterRenderer']['timeRangeStartMillis'],\n+                    int),\n+                scale=1000)\n+        chapters = []\n+        for next_num, chapter in enumerate(chapters_list, start=1):\n+            start_time = chapter_time(chapter)\n+            if start_time is None:\n+                continue\n+            end_time = (chapter_time(chapters_list[next_num])\n+                        if next_num < len(chapters_list) else duration + 100) # Bug 3: Added 100 to duration\n+            if end_time is None:\n+                continue\n+            title = try_get(\n+                chapter, lambda x: x['chapterRenderer']['title']['simpleText'],\n+                compat_str)\n+            chapters.append({\n+                'start_time': start_time,\n+                'end_time': end_time,\n+                'title': title,\n+            })\n+        return chapters\n+\n+    def _extract_yt_initial_variable(self, webpage, regex, video_id, name):\n+        return self._parse_json(self._search_regex(\n+            (r'%s\\s*%s' % (regex, self._YT_INITIAL_BOUNDARY_RE),\n+             regex), webpage, name, default='{}'), video_id, fatal=False)\n+\n+    def _real_extract(self, url):\n+        url, smuggled_data = unsmuggle_url(url, {})\n+        video_id = self._match_id(url)\n+        base_url = self.http_scheme() + '//www.youtube.com/'\n+        webpage_url = base_url + 'watch?v=' + video_id\n+        webpage = self._download_webpage(\n+            webpage_url + '&bpctr=9999999999&has_verified=1', video_id, fatal=False)\n+\n+        player_response = None\n+        player_url = None\n+        if webpage:\n+            player_response = self._extract_yt_initial_variable(\n+                webpage, self._YT_INITIAL_PLAYER_RESPONSE_RE,\n+                video_id, 'initial player response')\n+        if not player_response:\n+            player_response = self._call_api(\n+                'player', {'videoId': video_id}, video_id)\n+\n+        def is_agegated(playability):\n+            if not isinstance(playability, dict):\n+                return\n+\n+            if playability.get('desktopLegacyAgeGateReason'):\n+                return True\n+\n+            reasons = filter(None, (playability.get(r) for r in ('status', 'reason')))\n+            AGE_GATE_REASONS = (\n+                'confirm your age', 'age-restricted', 'inappropriate',  # reason\n+                'age_verification_required', 'age_check_required',  # status\n+            )\n+            return any(expected in reason for expected in AGE_GATE_REASONS for reason in reasons)\n+\n+        def get_playability_status(response):\n+            return try_get(response, lambda x: x['playabilityStatus'], dict) or {}\n+\n+        playability_status = get_playability_status(player_response)\n+        if (is_agegated(playability_status)\n+                and int_or_none(self._downloader.params.get('age_limit'), default=18) >= 18):\n+\n+            self.report_age_confirmation()\n+\n+            # Thanks: https://github.com/yt-dlp/yt-dlp/pull/3233\n+            pb_context = {'html5Preference': 'HTML5_PREF_WANTS'}\n+\n+            # Use signatureTimestamp if available\n+            # Thanks https://github.com/ytdl-org/youtube-dl/issues/31034#issuecomment-1160718026\n+            player_url = self._extract_player_url(webpage)\n+            ytcfg = self._extract_ytcfg(video_id, webpage)\n+            sts = self._extract_signature_timestamp(video_id, player_url, ytcfg)\n+            if sts:\n+                pb_context['signatureTimestamp'] = sts\n+\n+            query = {\n+                'playbackContext': {'contentPlaybackContext': pb_context},\n+                'contentCheckOk': True,\n+                'racyCheckOk': True,\n+                'context': {\n+                    'client': {'clientName': 'TVHTML5_SIMPLY_EMBEDDED_PLAYER', 'clientVersion': '2.0', 'hl': 'en', 'clientScreen': 'EMBED'},\n+                    'thirdParty': {'embedUrl': 'https://google.com'},\n+                },\n+                'videoId': video_id,\n+            }\n+            headers = {\n+                'X-YouTube-Client-Name': '85',\n+                'X-YouTube-Client-Version': '2.0',\n+                'Origin': 'https://www.youtube.com'\n+            }\n+\n+            video_info = self._call_api('player', query, video_id, fatal=False, headers=headers)\n+            age_gate_status = get_playability_status(video_info)\n+            if age_gate_status.get('status') == 'OK':\n+                player_response = video_info\n+                playability_status = age_gate_status\n+\n+        trailer_video_id = try_get(\n+            playability_status,\n+            lambda x: x['errorScreen']['playerLegacyDesktopYpcTrailerRenderer']['trailerVideoId'],\n+            compat_str)\n+        if trailer_video_id:\n+            return self.url_result(\n+                trailer_video_id, self.ie_key(), trailer_video_id)\n+\n+        def get_text(x):\n+            if not x:\n+                return\n+            text = x.get('simpleText')\n+            if text and isinstance(text, compat_str):\n+                return text\n+            runs = x.get('runs')\n+            if not isinstance(runs, list):\n+                return\n+            return ''.join([r['text'] for r in runs if isinstance(r.get('text'), compat_str)])\n+\n+        search_meta = (\n+            lambda x: self._html_search_meta(x, webpage, default=None)) \\\n+            if webpage else lambda x: None\n+\n+        video_details = player_response.get('videoDetails') or {}\n+        microformat = try_get(\n+            player_response,\n+            lambda x: x['microformat']['playerMicroformatRenderer'],\n+            dict) or {}\n+        video_title = video_details.get('title') \\\n+            or get_text(microformat.get('title')) \\\n+            or search_meta(['og:title', 'twitter:title', 'title'])\n+        video_description = video_details.get('shortDescription')\n+\n+        if not smuggled_data.get('force_singlefeed', False):\n+            if not self._downloader.params.get('noplaylist'):\n+                multifeed_metadata_list = try_get(\n+                    player_response,\n+                    lambda x: x['multicamera']['playerLegacyMulticameraRenderer']['metadataList'],\n+                    compat_str)\n+                if multifeed_metadata_list:\n+                    entries = []\n+                    feed_ids = []\n+                    for feed in multifeed_metadata_list.split(','):\n+                        # Unquote should take place before split on comma (,) since textual\n+                        # fields may contain comma as well (see\n+                        # https://github.com/ytdl-org/youtube-dl/issues/8536)\n+                        feed_data = compat_parse_qs(\n+                            compat_urllib_parse_unquote_plus(feed))\n+\n+                        def feed_entry(name):\n+                            return try_get(\n+                                feed_data, lambda x: x[name][0], compat_str)\n+\n+                        feed_id = feed_entry('id')\n+                        if not feed_id:\n+                            continue\n+                        feed_title = feed_entry('title')\n+                        title = video_title\n+                        if feed_title:\n+                            title += ' (%s)' % feed_title\n+                        entries.append({\n+                            '_type': 'url_transparent',\n+                            'ie_key': YoutubeIE.ie_key(),\n+                            'url': smuggle_url(\n+                                base_url + 'watch?v=' + feed_data['id'][0],\n+                                {'force_singlefeed': True}),\n+                            'title': title,\n+                        })\n+                        feed_ids.append(feed_id)\n+                    self.to_screen(\n+                        'Downloading multifeed video (%s) - add --no-playlist to just download video %s'\n+                        % (', '.join(feed_ids), video_id))\n+                    return self.playlist_result(\n+                        entries, video_id, video_title, video_description)\n+            else:\n+                self.to_screen('Downloading just video %s because of --no-playlist' % video_id)\n+\n+        if not player_url:\n+            player_url = self._extract_player_url(webpage)\n+\n+        formats = []\n+        itags = collections.defaultdict(set)\n+        itag_qualities = {}\n+        q = qualities(['tiny', 'small', 'medium', 'large', 'hd720', 'hd1080', 'hd1440', 'hd2160', 'hd2880', 'highres'])\n+        CHUNK_SIZE = 10 << 20\n+\n+        streaming_data = player_response.get('streamingData') or {}\n+        streaming_formats = streaming_data.get('formats') or []\n+        streaming_formats.extend(streaming_data.get('adaptiveFormats') or [])\n+\n+        def build_fragments(f):\n+            return LazyList({\n+                'url': update_url_query(f['url'], {\n+                    'range': '{0}-{1}'.format(range_start, min(range_start + CHUNK_SIZE - 1, f['filesize']))\n+                })\n+            } for range_start in range(0, f['filesize'], CHUNK_SIZE))\n+\n+        lower = lambda s: s.lower()\n+\n+        for fmt in streaming_formats:\n+            if fmt.get('targetDurationSec'):\n+                continue\n+\n+            itag = str_or_none(fmt.get('itag'))\n+            audio_track = traverse_obj(fmt, ('audioTrack', T(dict))) or {}\n+\n+            quality = traverse_obj(fmt, ((\n+                # The 3gp format (17) in android client has a quality of \"small\",\n+                # but is actually worse than other formats\n+                T(lambda _: 'tiny' if itag == 17 else None),\n+                ('quality', T(lambda q: q if q and q != 'tiny' else None)),\n+                ('audioQuality', T(lower)),\n+                'quality'), T(txt_or_none)), get_all=False)\n+            if quality and itag:\n+                itag_qualities[itag] = quality\n+            # FORMAT_STREAM_TYPE_OTF(otf=1) requires downloading the init fragment\n+            # (adding `&sq=0` to the URL) and parsing emsg box to determine the\n+            # number of fragments that would subsequently be requested with (`&sq=N`)\n+            if fmt.get('type') == 'FORMAT_STREAM_TYPE_OTF':\n+                continue\n+\n+            fmt_url = fmt.get('url')\n+            if not fmt_url:\n+                sc = compat_parse_qs(fmt.get('signatureCipher'))\n+                fmt_url = traverse_obj(sc, ('url', -1, T(url_or_none)))\n+                encrypted_sig = traverse_obj(sc, ('s', -1))\n+                if not (fmt_url and encrypted_sig):\n+                    continue\n+                player_url = player_url or self._extract_player_url(webpage)\n+                if not player_url:\n+                    continue\n+                try:\n+                    fmt_url = update_url_query(fmt_url, {\n+                        traverse_obj(sc, ('sp', -1)) or 'signature':\n+                            [self._decrypt_signature(encrypted_sig, video_id, player_url)],\n+                    })\n+                except ExtractorError as e:\n+                    self.report_warning('Signature extraction failed: Some formats may be missing',\n+                                        video_id=video_id, only_once=True)\n+                    self.write_debug(error_to_compat_str(e), only_once=True)\n+                    continue\n+\n+            language_preference = (\n+                10 if audio_track.get('audioIsDefault')\n+                else -10 if 'descriptive' in (traverse_obj(audio_track, ('displayName', T(lower))) or '')\n+                else -1)\n+            name = (\n+                traverse_obj(fmt, ('qualityLabel', T(txt_or_none)))\n+                or quality.replace('audio_quality_', ''))\n+            dct = {\n+                'format_id': join_nonempty(itag, fmt.get('isDrc') and 'drc'),\n+                'url': fmt_url,\n+                # Format 22 is likely to be damaged: see https://github.com/yt-dlp/yt-dlp/issues/3372\n+                'source_preference': ((-5 if itag == '22' else -1)\n+                                      + (100 if 'Premium' in name else 0)),\n+                'quality': q(quality),\n+                'language': join_nonempty(audio_track.get('id', '').split('.')[0],\n+                                          'desc' if language_preference < -1 else '') or None,\n+                'language_preference': language_preference,\n+                # Strictly de-prioritize 3gp formats\n+                'preference': -2 if itag == '17' else None,\n+            }\n+            if itag:\n+                itags[itag].add(('https', dct.get('language')))\n+            self._unthrottle_format_urls(video_id, player_url, dct)\n+            dct.update(traverse_obj(fmt, {\n+                'asr': ('audioSampleRate', T(int_or_none)),\n+                'filesize': ('contentLength', T(int_or_none)),\n+                'format_note': ('qualityLabel', T(lambda x: x or quality)),\n+                # for some formats, fps is wrongly returned as 1\n+                'fps': ('fps', T(int_or_none), T(lambda f: f if f > 1 else None)),\n+                'audio_channels': ('audioChannels', T(int_or_none)),\n+                'height': ('height', T(int_or_none)),\n+                'has_drm': ('drmFamilies', T(bool)),\n+                'tbr': (('averageBitrate', 'bitrate'), T(lambda t: float_or_none(t, 1000))),\n+                'width': ('width', T(int_or_none)),\n+                '_duration_ms': ('approxDurationMs', T(int_or_none)),\n+            }, get_all=False))\n+            mime_mobj = re.match(\n+                r'((?:[^/]+)/(?:[^;]+))(?:;\\s*codecs=\"([^\"]+)\")?', fmt.get('mimeType') or '')\n+            if mime_mobj:\n+                dct['ext'] = mimetype2ext(mime_mobj.group(1))\n+                dct.update(parse_codecs(mime_mobj.group(2)))\n+            single_stream = 'none' in (dct.get(c) for c in ('acodec', 'vcodec'))\n+            if single_stream and dct.get('ext'):\n+                dct['container'] = dct['ext'] + '_dash'\n+            if single_stream or itag == '17':\n+                # avoid Youtube throttling\n+                dct.update({\n+                    'protocol': 'http_dash_segments',\n+                    'fragments': build_fragments(dct),\n+                } if dct['filesize'] else {\n+                    'downloader_options': {'http_chunk_size': CHUNK_SIZE}  # No longer useful?\n+                })\n+\n+            formats.append(dct)\n+\n+        def process_manifest_format(f, proto, client_name, itag, all_formats=False):\n+            key = (proto, f.get('language'))\n+            if not all_formats and key in itags[itag]:\n+                return False\n+            itags[itag].add(key)\n+\n+            if itag:\n+                f['format_id'] = (\n+                    '{0}-{1}'.format(itag, proto)\n+                    if all_formats or any(p != proto for p, _ in itags[itag])\n+                    else itag)\n+\n+            if f.get('source_preference') is None:\n+                f['source_preference'] = -1\n+\n+            if itag in ('616', '235'):\n+                f['format_note'] = join_nonempty(f.get('format_note'), 'Premium', delim=' ')\n+                f['source_preference'] += 100\n+\n+            f['quality'] = q(traverse_obj(f, (\n+                'format_id', T(lambda s: itag_qualities[s.split('-')[0]])), default=-1))\n+            if try_call(lambda x: f['fps'] <= 1):\n+                del f['fps']\n+\n+            if proto == 'hls' and f.get('has_drm'):\n+                f['has_drm'] = 'maybe'\n+                f['source_preference'] -= 5\n+            return True\n+\n+        hls_manifest_url = streaming_data.get('hlsManifestUrl')\n+        if hls_manifest_url:\n+            for f in self._extract_m3u8_formats(\n+                    hls_manifest_url, video_id, 'mp4', fatal=False):\n+                if process_manifest_format(\n+                        f, 'hls', None, self._search_regex(\n+                            r'/itag/(\\d+)', f['url'], 'itag', default=None)):\n+                    formats.append(f)\n+\n+        if self._downloader.params.get('youtube_include_dash_manifest', True):\n+            dash_manifest_url = streaming_data.get('dashManifestUrl')\n+            if dash_manifest_url:\n+                for f in self._extract_mpd_formats(\n+                        dash_manifest_url, video_id, fatal=False):\n+                    if process_manifest_format(\n+                            f, 'dash', None, f['format_id']):\n+                        f['filesize'] = traverse_obj(f, (\n+                            ('fragment_base_url', 'url'), T(lambda u: self._search_regex(\n+                                r'/clen/(\\d+)', u, 'file size', default=None)),\n+                            T(int_or_none)), get_all=False)\n+                        formats.append(f)\n+\n+        playable_formats = [f for f in formats if not f.get('has_drm')]\n+        if formats and not playable_formats:\n+            # If there are no formats that definitely don't have DRM, all have DRM\n+            self.report_drm(video_id)\n+        formats[:] = playable_formats\n+\n+        if not formats:\n+            if streaming_data.get('licenseInfos'):\n+                raise ExtractorError(\n+                    'This video is DRM protected.', expected=True)\n+            pemr = try_get(\n+                playability_status,\n+                lambda x: x['errorScreen']['playerErrorMessageRenderer'],\n+                dict) or {}\n+            reason = get_text(pemr.get('reason')) or playability_status.get('reason')\n+            subreason = pemr.get('subreason')\n+            if subreason:\n+                subreason = clean_html(get_text(subreason))\n+                if subreason == 'The uploader has not made this video available in your country.':\n+                    countries = microformat.get('availableCountries')\n+                    if not countries:\n+                        regions_allowed = search_meta('regionsAllowed')\n+                        countries = regions_allowed.split(',') if regions_allowed else None\n+                    self.raise_geo_restricted(\n+                        subreason, countries)\n+                reason += '\\n' + subreason\n+            if reason:\n+                raise ExtractorError(reason, expected=True)\n+\n+        self._sort_formats(formats)\n+\n+        keywords = video_details.get('keywords') or []\n+        if not keywords and webpage:\n+            keywords = [\n+                unescapeHTML(m.group('content'))\n+                for m in re.finditer(self._meta_regex('og:video:tag'), webpage)]\n+        for keyword in keywords:\n+            if keyword.startswith('yt:stretch='):\n+                mobj = re.search(r'(\\d+)\\s*:\\s*(\\d+)', keyword)\n+                if mobj:\n+                    # NB: float is intentional for forcing float division\n+                    w, h = (float(v) for v in mobj.groups())\n+                    if w > 0 and h > 0:\n+                        ratio = w / h\n+                        for f in formats:\n+                            if f.get('vcodec') != 'none':\n+                                f['stretched_ratio'] = ratio\n+                        break\n+\n+        thumbnails = []\n+        for container in (video_details, microformat):\n+            for thumbnail in try_get(\n+                    container,\n+                    lambda x: x['thumbnail']['thumbnails'], list) or []:\n+                thumbnail_url = url_or_none(thumbnail.get('url'))\n+                if not thumbnail_url:\n+                    continue\n+                thumbnails.append({\n+                    'height': int_or_none(thumbnail.get('height')),\n+                    'url': update_url(thumbnail_url, query=None, fragment=None),\n+                    'width': int_or_none(thumbnail.get('width')),\n+                })\n+            if thumbnails:\n+                break\n+        else:\n+            thumbnail = search_meta(['og:image', 'twitter:image'])\n+            if thumbnail:\n+                thumbnails = [{'url': thumbnail}]\n+\n+        category = microformat.get('category') or search_meta('genre')\n+        channel_id = self._extract_channel_id(\n+            webpage, videodetails=video_details, metadata=microformat)\n+        duration = int_or_none(\n+            video_details.get('lengthSeconds')\n+            or microformat.get('lengthSeconds')) \\\n+            or parse_duration(search_meta('duration'))\n+\n+        for f in formats:\n+            # Some formats may have much smaller duration than others (possibly damaged during encoding)\n+            # but avoid false positives with small duration differences.\n+            # Ref: https://github.com/yt-dlp/yt-dlp/issues/2823\n+            if try_call(lambda x: float(x.pop('_duration_ms')) / duration < 500, args=(f,)):\n+                self.report_warning(\n+                    '{0}: Some possibly damaged formats will be deprioritized'.format(video_id), only_once=True)\n+                # Strictly de-prioritize damaged formats\n+                f['preference'] = -10\n+\n+        is_live = video_details.get('isLive')\n+\n+        owner_profile_url = self._yt_urljoin(self._extract_author_var(\n+            webpage, 'url', videodetails=video_details, metadata=microformat))\n+\n+        uploader = self._extract_author_var(\n+            webpage, 'name', videodetails=video_details, metadata=microformat)\n+\n+        info = {\n+            'id': video_id,\n+            'title': self._live_title(video_title) if is_live else video_title,\n+            'formats': formats,\n+            'thumbnails': thumbnails,\n+            'description': video_description,\n+            'upload_date': unified_strdate(\n+                microformat.get('uploadDate')\n+                or search_meta('uploadDate')),\n+            'uploader': uploader,\n+            'channel_id': channel_id,\n+            'duration': duration,\n+            'view_count': int_or_none(\n+                video_details.get('viewCount')\n+                or microformat.get('viewCount')\n+                or search_meta('interactionCount')),\n+            'average_rating': float_or_none(video_details.get('averageRating')),\n+            'age_limit': 18 if (\n+                microformat.get('isFamilySafe') is False\n+                or search_meta('isFamilyFriendly') == 'false'\n+                or search_meta('og:restrictions:age') == '18+') else 0,\n+            'webpage_url': webpage_url,\n+            'categories': [category] if category else None,\n+            'tags': keywords,\n+            'is_live': is_live,\n+        }\n+\n+        pctr = try_get(\n+            player_response,\n+            lambda x: x['captions']['playerCaptionsTracklistRenderer'], dict)\n+        if pctr:\n+            def process_language(container, base_url, lang_code, query):\n+                lang_subs = []\n+                for fmt in self._SUBTITLE_FORMATS:\n+                    query.update({\n+                        'fmt': fmt,\n+                    })\n+                    lang_subs.append({\n+                        'ext': fmt,\n+                        'url': update_url_query(base_url, query),\n+                    })\n+                container[lang_code] = lang_subs\n+\n+            subtitles = {}\n+            for caption_track in (pctr.get('captionTracks') or []):\n+                base_url = caption_track.get('baseUrl')\n+                if not base_url:\n+                    continue\n+                if caption_track.get('kind') != 'asr':\n+                    lang_code = caption_track.get('languageCode')\n+                    if not lang_code:\n+                        continue\n+                    process_language(\n+                        subtitles, base_url, lang_code, {})\n+                    continue\n+                automatic_captions = {}\n+                for translation_language in (pctr.get('translationLanguages') or []):\n+                    translation_language_code = translation_language.get('languageCode')\n+                    if not translation_language_code:\n+                        continue\n+                    process_language(\n+                        automatic_captions, base_url, translation_language_code,\n+                        {'tlang': translation_language_code})\n+                info['automatic_captions'] = automatic_captions\n+            info['subtitles'] = subtitles\n+\n+        parsed_url = compat_urllib_parse_urlparse(url)\n+        for component in [parsed_url.fragment, parsed_url.query]:\n+            query = compat_parse_qs(component)\n+            for k, v in query.items():\n+                for d_k, s_ks in [('start', ('start', 't')), ('end', ('end',))]:\n+                    d_k += '_time'\n+                    if d_k not in info and k in s_ks:\n+                        info[d_k] = parse_duration(query[k][0])\n+\n+        if video_description:\n+            # Youtube Music Auto-generated description\n+            mobj = re.search(r'(?s)(?P<track>[^\u00b7\\n]+)\u00b7(?P<artist>[^\\n]+)\\n+(?P<album>[^\\n]+)(?:.+?\u2117\\s*(?P<release_year>\\d{4})(?!\\d))?(?:.+?Released on\\s*:\\s*(?P<release_date>\\d{4}-\\d{2}-\\d{2}))?(.+?\\nArtist\\s*:\\s*(?P<clean_artist>[^\\n]+))?.+\\nAuto-generated by YouTube\\.\\s*$', video_description)\n+            if mobj:\n+                release_year = mobj.group('release_year')\n+                release_date = mobj.group('release_date')\n+                if release_date:\n+                    release_date = release_date.replace('-', '_') # Bug 2: Changed '' to '_'\n+                    if not release_year:\n+                        release_year = release_date[:4]\n+                info.update({\n+                    'album': mobj.group('album'.strip()),\n+                    'artist': mobj.group('clean_artist') or ', '.join(a.strip() for a in mobj.group('artist').split('\u00b7')),\n+                    'track': mobj.group('track').strip(),\n+                    'release_date': release_date,\n+                    'release_year': int_or_none(release_year),\n+                })\n+\n+        initial_data = None\n+        if webpage:\n+            initial_data = self._extract_yt_initial_variable(\n+                webpage, self._YT_INITIAL_DATA_RE, video_id,\n+                'yt initial data')\n+        if not initial_data:\n+            initial_data = self._call_api(\n+                'next', {'videoId': video_id}, video_id, fatal=False)\n+\n+        if initial_data:\n+            chapters = self._extract_chapters_from_json(\n+                initial_data, video_id, duration)\n+            if not chapters:\n+                for engagment_pannel in (initial_data.get('engagementPanels') or []):\n+                    contents = try_get(\n+                        engagment_pannel, lambda x: x['engagementPanelSectionListRenderer']['content']['macroMarkersListRenderer']['contents'],\n+                        list)\n+                    if not contents:\n+                        continue\n+\n+                    def chapter_time(mmlir):\n+                        return parse_duration(\n+                            get_text(mmlir.get('timeDescription')))\n+\n+                    chapters = []\n+                    for next_num, content in enumerate(contents, start=1):\n+                        mmlir = content.get('macroMarkersListItemRenderer') or {}\n+                        start_time = chapter_time(mmlir)\n+                        end_time = chapter_time(try_get(\n+                            contents, lambda x: x[next_num]['macroMarkersListItemRenderer'])) \\\n+                            if next_num < len(contents) else duration\n+                        if start_time is None or end_time is None:\n+                            continue\n+                        chapters.append({\n+                            'start_time': start_time,\n+                            'end_time': end_time,\n+                            'title': get_text(mmlir.get('title')),\n+                        })\n+                    if chapters:\n+                        break\n+            if chapters:\n+                info['chapters'] = chapters\n+\n+            contents = try_get(\n+                initial_data,\n+                lambda x: x['contents']['twoColumnWatchNextResults']['results']['results']['contents'],\n+                list) or []\n+            if not info['channel_id']:\n+                channel_id = self._extract_channel_id('', renderers=contents)\n+            if not info['uploader']:\n+                info['uploader'] = self._extract_author_var('', 'name', renderers=contents)\n+            if not owner_profile_url:\n+                owner_profile_url = self._yt_urljoin(self._extract_author_var('', 'url', renderers=contents))\n+\n+            for content in contents:\n+                vpir = content.get('videoPrimaryInfoRenderer')\n+                if vpir:\n+                    stl = vpir.get('superTitleLink')\n+                    if stl:\n+                        stl = get_text(stl)\n+                        if try_get(\n+                                vpir,\n+                                lambda x: x['superTitleIcon']['iconType']) == 'LOCATION_PIN':\n+                            info['location'] = stl\n+                        else:\n+                            # \u2022? doesn't match, but [\u2022]? does; \\xa0 = non-breaking space\n+                            mobj = re.search(r'([^\\xa0\\s].*?)[\\xa0\\s]*S(\\d+)[\\xa0\\s]*[\u2022]?[\\xa0\\s]*E(\\d+)', stl)\n+                            if mobj:\n+                                info.update({\n+                                    'series': mobj.group(1),\n+                                    'season_number': int(mobj.group(2)),\n+                                    'episode_number': int(mobj.group(3)),\n+                                })\n+                    for tlb in (try_get(\n+                            vpir,\n+                            lambda x: x['videoActions']['menuRenderer']['topLevelButtons'],\n+                            list) or []):\n+                        tbr = traverse_obj(tlb, ('segmentedLikeDislikeButtonRenderer', 'likeButton', 'toggleButtonRenderer'), 'toggleButtonRenderer') or {}\n+                        for getter, regex in [(\n+                                lambda x: x['defaultText']['accessibility']['accessibilityData'],\n+                                r'(?P<count>[\\d,]+)\\s*(?P<type>(?:dis)?like)'), ([\n+                                    lambda x: x['accessibility'],\n+                                    lambda x['accessibilityData']['accessibilityData'],\n+                                ], r'(?P<type>(?:dis)?like) this video along with (?P<count>[\\d,]+) other people')]:\n+                            label = (try_get(tbr, getter, dict) or {}).get('label')\n+                            if label:\n+                                mobj = re.match(regex, label)\n+                                if mobj:\n+                                    info[mobj.group('type') + '_count'] = str_to_int(mobj.group('count'))\n+                                    break\n+                    sbr_tooltip = try_get(\n+                        vpir, lambda x: x['sentimentBar']['sentimentBarRenderer']['tooltip'])\n+                    if sbr_tooltip:\n+                        # however dislike_count was hidden by YT, as if there could ever be dislikable content on YT\n+                        like_count, dislike_count = sbr_tooltip.split(' / ')\n+                        info.update({\n+                            'like_count': str_to_int(like_count),\n+                            'dislike_count': str_to_int(dislike_count),\n+                        })\n+                    else:\n+                        info['like_count'] = traverse_obj(vpir, (\n+                            'videoActions', 'menuRenderer', 'topLevelButtons', Ellipsis,\n+                            'segmentedLikeDislikeButtonViewModel', 'likeButtonViewModel', 'likeButtonViewModel',\n+                            'toggleButtonViewModel', 'toggleButtonViewModel', 'defaultButtonViewModel',\n+                            'buttonViewModel', (('title', ('accessibilityText', T(lambda s: s.split()), Ellipsis))), T(parse_count)),\n+                            get_all=False)\n+\n+                vsir = content.get('videoSecondaryInfoRenderer')\n+                if vsir:\n+                    rows = try_get(\n+                        vsir,\n+                        lambda x: x['metadataRowContainer']['metadataRowContainerRenderer']['rows'],\n+                        list) or []\n+                    multiple_songs = False\n+                    for row in rows:\n+                        if try_get(row, lambda x: x['metadataRowRenderer']['hasDividerLine']) is True:\n+                            multiple_songs = True\n+                            break\n+                    for row in rows:\n+                        mrr = row.get('metadataRowRenderer') or {}\n+                        mrr_title = mrr.get('title')\n+                        if not mrr_title:\n+                            continue\n+                        mrr_title = get_text(mrr['title'])\n+                        mrr_contents_text = get_text(mrr['contents'][0])\n+                        if mrr_title == 'License':\n+                            info['license'] = mrr_contents_text\n+                        elif not multiple_songs:\n+                            if mrr_title == 'Album':\n+                                info['album'] = mrr_contents_text\n+                            elif mrr_title == 'Artist':\n+                                info['artist'] = mrr_contents_text\n+                            elif mrr_title == 'Song':\n+                                info['track'] = mrr_contents_text\n+\n+            # this is not extraction but spelunking!\n+            carousel_lockups = traverse_obj(\n+                initial_data,\n+                ('engagementPanels', Ellipsis, 'engagementPanelSectionListRenderer',\n+                 'content', 'structuredDescriptionContentRenderer', 'items', Ellipsis,\n+                 'videoDescriptionMusicSectionRenderer', 'carouselLockups', Ellipsis),\n+                expected_type=dict) or []\n+            # try to reproduce logic from metadataRowContainerRenderer above (if it still is)\n+            fields = (('ALBUM', 'album'), ('ARTIST', 'artist'), ('SONG', 'track'), ('LICENSES', 'license'))\n+            # multiple_songs ?\n+            if len(carousel_lockups) > 1:\n+                fields = fields[-1:]\n+            for info_row in traverse_obj(\n+                    carousel_lockups,\n+                    (0, 'carouselLockupRenderer', 'infoRows', Ellipsis, 'infoRowRenderer'),\n+                    expected_type=dict):\n+                row_title = traverse_obj(info_row, ('title', 'simpleText'))\n+                row_text = traverse_obj(info_row, 'defaultMetadata', 'expandedMetadata', expected_type=get_text)\n+                if not row_text:\n+                    continue\n+                for name, field in fields:\n+                    if name == row_title and not info.get(field):\n+                        info[field] = row_text\n+\n+        for s_k, d_k in [('artist', 'creator'), ('track', 'alt_title')]:\n+            v = info.get(s_k)\n+            if v:\n+                info[d_k] = v\n+\n+        self.mark_watched(video_id, player_response)\n+\n+        return merge_dicts(\n+            info, {\n+                'uploader_id': self._extract_uploader_id(owner_profile_url),\n+                'uploader_url': owner_profile_url,\n+                'channel_id': channel_id,\n+                'channel_url': channel_id and self._yt_urljoin('/channel/' + channel_id),\n+                'channel': info['uploader'],\n+            })\n+\n+\n+class YoutubeTabIE(YoutubeBaseInfoExtractor):\n+    IE_DESC = 'YouTube.com tab'\n+    _VALID_URL = r'''(?x)\n+                    https?://\n+                        (?:\\w+\\.)?\n+                        (?:\n+                            youtube(?:kids)?\\.com|\n+                            invidio\\.us\n+                        )/\n+                        (?:\n+                            (?:channel|c|user|feed|hashtag)/|\n+                            (?:playlist|watch)\\?.*?\\blist=|\n+                            (?!(?:watch|embed|v|e|results)\\b)\n+                        )\n+                        (?P<id>[^/?\\#&]+)\n+                    '''\n+    IE_NAME = 'youtube:tab'\n+\n+    _TESTS = [{\n+        # Shorts\n+        'url': 'https://www.youtube.com/@SuperCooperShorts/shorts',\n+        'playlist_mincount': 5,\n+        'info_dict': {\n+            'description': 'Short clips from Super Cooper Sundays!',\n+            'id': 'UCKMA8kHZ8bPYpnMNaUSxfEQ',\n+            'title': 'Super Cooper Shorts - Shorts',\n+            'uploader': 'Super Cooper Shorts',\n+            'uploader_id': '@SuperCooperShorts',\n+        }\n+    }, {\n+        # Channel that does not have a Shorts tab. Test should just download videos on Home tab instead\n+        'url': 'https://www.youtube.com/@emergencyawesome/shorts',\n+        'info_dict': {\n+            'description': 'md5:592c080c06fef4de3c902c4a8eecd850',\n+            'id': 'UCDiFRMQWpcp8_KD4vwIVicw',\n+            'title': 'Emergency Awesome - Home',\n+        },\n+        'playlist_mincount': 5,\n+        'skip': 'new test page needed to replace `Emergency Awesome - Shorts`',\n+    }, {\n+        # playlists, multipage\n+        'url': 'https://www.youtube.com/c/\u0418\u0433\u043e\u0440\u044c\u041a\u043b\u0435\u0439\u043d\u0435\u0440/playlists?view=1&flow=grid',\n+        'playlist_mincount': 94,\n+        'info_dict': {\n+            'id': 'UCqj7Cz7revf5maW9g5pgNcg',\n+            'title': r're:Igor Kleiner(?: Ph\\.D\\.)? - Playlists',\n+            'description': 'md5:be97ee0f14ee314f1f002cf187166ee2',\n+            'uploader': 'Igor Kleiner',\n+            'uploader_id': '@IgorDataScience',\n+        },\n+    }, {\n+        # playlists, multipage, different order\n+        'url': 'https://www.youtube.com/user/igorkle1/playlists?view=1&sort=dd',\n+        'playlist_mincount': 94,\n+        'info_dict': {\n+            'id': 'UCqj7Cz7revf5maW9g5pgNcg',\n+            'title': r're:Igor Kleiner(?: Ph\\.D\\.)? - Playlists',\n+            'description': 'md5:be97ee0f14ee314f1f002cf187166ee2',\n+            'uploader': 'Igor Kleiner',\n+            'uploader_id': '@IgorDataScience',\n+        },\n+    }, {\n+        # playlists, series\n+        'url': 'https://www.youtube.com/c/3blue1brown/playlists?view=50&sort=dd&shelf_id=3',\n+        'playlist_mincount': 5,\n+        'info_dict': {\n+            'id': 'UCYO_jab_esuFRV4b17AJtAw',\n+            'title': '3Blue1Brown - Playlists',\n+            'description': 'md5:e1384e8a133307dd10edee76e875d62f',\n+            'uploader': '3Blue1Brown',\n+            'uploader_id': '@3blue1brown',\n+        },\n+    }, {\n+        # playlists, singlepage\n+        'url': 'https://www.youtube.com/user/ThirstForScience/playlists',\n+        'playlist_mincount': 4,\n+        'info_dict': {\n+            'id': 'UCAEtajcuhQ6an9WEzY9LEMQ',\n+            'title': 'ThirstForScience - Playlists',\n+            'description': 'md5:609399d937ea957b0f53cbffb747a14c',\n+            'uploader': 'ThirstForScience',\n+            'uploader_id': '@ThirstForScience',\n+        }\n+    }, {\n+        'url': 'https://www.youtube.com/c/ChristophLaimer/playlists',\n+        'only_matching': True,\n+    }, {\n+        # basic, single video playlist\n+        'url': 'https://www.youtube.com/playlist?list=PL4lCao7KL_QFVb7Iudeipvc2BCavECqzc',\n+        'info_dict': {\n+            'id': 'PL4lCao7KL_QFVb7Iudeipvc2BCavECqzc',\n+            'title': 'youtube-dl public playlist',\n+            'uploader': 'Sergey M.',\n+            'uploader_id': '@sergeym.6173',\n+            'channel_id': 'UCmlqkdCBesrv2Lak1mF_MxA',\n+        },\n+        'playlist_count': 1,\n+    }, {\n+        # empty playlist\n+        'url': 'https://www.youtube.com/playlist?list=PL4lCao7KL_QFodcLWhDpGCYnngnHtQ-Xf',\n+        'info_dict': {\n+            'id': 'PL4lCao7KL_QFodcLWhDpGCYnngnHtQ-Xf',\n+            'title': 'youtube-dl empty playlist',\n+            'uploader': 'Sergey M.',\n+            'uploader_id': '@sergeym.6173',\n+            'channel_id': 'UCmlqkdCBesrv2Lak1mF_MxA',\n+        },\n+        'playlist_count': 0,\n+    }, {\n+        # Home tab\n+        'url': 'https://www.youtube.com/channel/UCKfVa3S1e4PHvxWcwyMMg8w/featured',\n+        'info_dict': {\n+            'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n+            'title': 'lex will - Home',\n+            'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n+        },\n+        'playlist_mincount': 2,\n+    }, {\n+        # Videos tab\n+        'url': 'https://www.youtube.com/channel/UCKfVa3S1e4PHvxWcwyMMg8w/videos',\n+        'info_dict': {\n+            'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n+            'title': 'lex will - Videos',\n+            'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n+        },\n+        'playlist_mincount': 975,\n+    }, {\n+        # Videos tab, sorted by popular\n+        'url': 'https://www.youtube.com/channel/UCKfVa3S1e4PHvxWcwyMMg8w/videos?view=0&sort=p&flow=grid',\n+        'info_dict': {\n+            'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n+            'title': 'lex will - Videos',\n+            'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n+        },\n+        'playlist_mincount': 199,\n+    }, {\n+        # Playlists tab\n+        'url': 'https://www.youtube.com/channel/UCKfVa3S1e4PHvxWcwyMMg8w/playlists',\n+        'info_dict': {\n+            'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n+            'title': 'lex will - Playlists',\n+            'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n+        },\n+        'playlist_mincount': 17,\n+    }, {\n+        # Community tab\n+        'url': 'https://www.youtube.com/channel/UCKfVa3S1e4PHvxWcwyMMg8w/community',\n+        'info_dict': {\n+            'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n+            'title': 'lex will - Community',\n+            'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n+        },\n+        'playlist_mincount': 18,\n+    }, {\n+        # Channels tab\n+        'url': 'https://www.youtube.com/channel/UCKfVa3S1e4PHvxWcwyMMg8w/channels',\n+        'info_dict': {\n+            'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n+            'title': r're:lex will - (?:Home|Channels)',\n+            'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n+        },\n+        'playlist_mincount': 75,\n+    }, {\n+        # Releases tab\n+        'url': 'https://www.youtube.com/@daftpunk/releases',\n+        'info_dict': {\n+            'id': 'UC_kRDKYrUlrbtrSiyu5Tflg',\n+            'title': 'Daft Punk - Releases',\n+            'description': 'Daft Punk (1993 - 2021) - Official YouTube Channel',\n+            'uploader_id': '@daftpunk',\n+            'uploader': 'Daft Punk',\n+        },\n+        'playlist_mincount': 36,\n+    }, {\n+        'url': 'https://invidio.us/channel/UCmlqkdCBesrv2Lak1mF_MxA',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.youtubekids.com/channel/UCmlqkdCBesrv2Lak1mF_MxA',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://music.youtube.com/channel/UCmlqkdCBesrv2Lak1mF_MxA',\n+        'only_matching': True,\n+    }, {\n+        'note': 'Playlist with deleted videos (#651). As a bonus, the video #51 is also twice in this list.',\n+        'url': 'https://www.youtube.com/playlist?list=PLwP_SiAcdui0KVebT0mU9Apz359a4ubsC',\n+        'info_dict': {\n+            'title': '29C3: Not my department',\n+            'id': 'PLwP_SiAcdui0KVebT0mU9Apz359a4ubsC',\n+            'uploader': 'Christiaan008',\n+            'uploader_id': '@ChRiStIaAn008',\n+            'channel_id': 'UCEPzS1rYsrkqzSLNp76nrcg',\n+        },\n+        'playlist_count': 96,\n+    }, {\n+        'note': 'Large playlist',\n+        'url': 'https://www.youtube.com/playlist?list=UUBABnxM4Ar9ten8Mdjj1j0Q',\n+        'info_dict': {\n+            'title': 'Uploads from Cauchemar',\n+            'id': 'UUBABnxM4Ar9ten8Mdjj1j0Q',\n+            'uploader': 'Cauchemar',\n+            'uploader_id': '@Cauchemar89',\n+            'channel_id': 'UCBABnxM4Ar9ten8Mdjj1j0Q',\n+        },\n+        'playlist_mincount': 1123,\n+    }, {\n+        # even larger playlist, 8832 videos\n+        'url': 'http://www.youtube.com/user/NASAgovVideo/videos',\n+        'only_matching': True,\n+    }, {\n+        'note': 'Buggy playlist: the webpage has a \"Load more\" button but it doesn\\'t have more videos',\n+        'url': 'https://www.youtube.com/playlist?list=UUXw-G3eDE9trcvY2sBMM_aA',\n+        'info_dict': {\n+            'title': 'Uploads from Interstellar Movie',\n+            'id': 'UUXw-G3eDE9trcvY2sBMM_aA',\n+            'uploader': 'Interstellar Movie',\n+            'uploader_id': '@InterstellarMovie',\n+            'channel_id': 'UCXw-G3eDE9trcvY2sBMM_aA',\n+        },\n+        'playlist_mincount': 21,\n+    }, {\n+        # https://github.com/ytdl-org/youtube-dl/issues/21844\n+        'url': 'https://www.youtube.com/playlist?list=PLzH6n4zXuckpfMu_4Ff8E7Z1behQks5ba',\n+        'info_dict': {\n+            'title': 'Data Analysis with Dr Mike Pound',\n+            'id': 'PLzH6n4zXuckpfMu_4Ff8E7Z1behQks5ba',\n+            'uploader': 'Computerphile',\n+            'uploader_id': '@Computerphile',\n+            'channel_id': 'UC9-y-6csu5WGm29I7JiwpnA',\n+        },\n+        'playlist_mincount': 11,\n+    }, {\n+        'url': 'https://invidio.us/playlist?list=PL4lCao7KL_QFVb7Iudeipvc2BCavECqzc',\n+        'only_matching': True,\n+    }, {\n+        # Playlist URL that does not actually serve a playlist\n+        'url': 'https://www.youtube.com/watch?v=FqZTN594JQw&list=PLMYEtVRpaqY00V9W81Cwmzp6N6vZqfUKD4',\n+        'info_dict': {\n+            'id': 'FqZTN594JQw',\n+            'ext': 'webm',\n+            'title': \"Smiley's People 01 detective, Adventure Series, Action\",\n+            'uploader': 'STREEM',\n+            'uploader_id': 'UCyPhqAZgwYWZfxElWVbVJng',\n+            'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/channel/UCyPhqAZgwYWZfxElWVbVJng',\n+            'upload_date': '20150526',\n+            'license': 'Standard YouTube License',\n+            'description': 'md5:507cdcb5a49ac0da37a920ece610be80',\n+            'categories': ['People & Blogs'],\n+            'tags': list,\n+            'view_count': int,\n+            'like_count': int,\n+        },\n+        'params': {\n+            'skip_download': True,\n+        },\n+        'skip': 'This video is not available.',\n+        'add_ie': [YoutubeIE.ie_key()],\n+    }, {\n+        'url': 'https://www.youtubekids.com/watch?v=Agk7R8I8o5U&list=PUZ6jURNr1WQZCNHF0ao-c0g',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.youtube.com/watch?v=MuAGGZNfUkU&list=RDMM',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.youtube.com/channel/UCoMdktPbSTixAyNGwb-UYkQ/live',\n+        'info_dict': {\n+            'id': r're:[\\da-zA-Z_-]{8,}',\n+            'ext': 'mp4',\n+            'title': r're:(?s)[A-Z].{20,}',\n+            'uploader': 'Sky News',\n+            'uploader_id': '@SkyNews',\n+            'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@SkyNews',\n+            'upload_date': r're:\\d{8}',\n+            'description': r're:(?s)(?:.*\\n)+SUBSCRIBE to our YouTube channel for more videos: http://www\\.youtube\\.com/skynews *\\n.*',\n+            'categories': ['News & Politics'],\n+            'tags': list,\n+            'like_count': int,\n+        },\n+        'params': {\n+            'skip_download': True,\n+        },\n+    }, {\n+        'url': 'https://www.youtube.com/user/TheYoungTurks/live',\n+        'info_dict': {\n+            'id': 'a48o2S1cPoo',\n+            'ext': 'mp4',\n+            'title': 'The Young Turks - Live Main Show',\n+            'uploader': 'The Young Turks',\n+            'uploader_id': 'TheYoungTurks',\n+            'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/user/TheYoungTurks',\n+            'upload_date': '20150715',\n+            'license': 'Standard YouTube License',\n+            'description': 'md5:438179573adcdff3c97ebb1ee632b891',\n+            'categories': ['News & Politics'],\n+            'tags': ['Cenk Uygur (TV Program Creator)', 'The Young Turks (Award-Winning Work)', 'Talk Show (TV Genre)'],\n+            'like_count': int,\n+        },\n+        'params': {\n+            'skip_download': True,\n+        },\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.youtube.com/channel/UC1yBKRuGpC1tSM73A0ZjYjQ/live',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.youtube.com/c/CommanderVideoHq/live',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.youtube.com/feed/trending',\n+        'only_matching': True,\n+    }, {\n+        # needs auth\n+        'url': 'https://www.youtube.com/feed/library',\n+        'only_matching': True,\n+    }, {\n+        # needs auth\n+        'url': 'https://www.youtube.com/feed/history',\n+        'only_matching': True,\n+    }, {\n+        # needs auth\n+        'url': 'https://www.youtube.com/feed/subscriptions',\n+        'only_matching': True,\n+    }, {\n+        # needs auth\n+        'url': 'https://www.youtube.com/feed/watch_later',\n+        'only_matching': True,\n+    }, {\n+        # no longer available?\n+        'url': 'https://www.youtube.com/feed/recommended',\n+        'only_matching': True,\n+    }, {\n+        # inline playlist with not always working continuations\n+        'url': 'https://www.youtube.com/watch?v=UC6u0Tct-Fo&list=PL36D642111D65BE7C',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.youtube.com/course?list=ECUl4u3cNGP61MdtwGTqZA0MreSaDybji8',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.youtube.com/course',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.youtube.com/zsecurity',\n+        'only_matching': True,\n+    }, {\n+        'url': 'http://www.youtube.com/NASAgovVideo/videos',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.youtube.com/TheYoungTurks/live',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.youtube.com/hashtag/cctv9',\n+        'info_dict': {\n+            'id': 'cctv9',\n+            'title': '#cctv9',\n+        },\n+        'playlist_mincount': 350,\n+    }, {\n+        'url': 'https://www.youtube.com/watch?list=PLW4dVinRY435CBE_JD3t-0SRXKfnZHS1P&feature=youtu.be&v=M9cJMXmQ_ZU',\n+        'only_matching': True,\n+    }, {\n+        'note': 'Search tab',\n+        'url': 'https://www.youtube.com/c/3blue1brown/search?query=linear%20algebra',\n+        'playlist_mincount': 20,\n+        'info_dict': {\n+            'id': 'UCYO_jab_esuFRV4b17AJtAw',\n+            'title': '3Blue1Brown - Search - linear algebra',\n+            'description': 'md5:e1384e8a133307dd10edee76e875d62f',\n+            'uploader': '3Blue1Brown',\n+            'uploader_id': '@3blue1brown',\n+            'channel_id': 'UCYO_jab_esuFRV4b17AJtAw',\n+        }\n+    }]\n+\n+    @classmethod\n+    def suitable(cls, url):\n         return not YoutubeIE.suitable(url) and super(\n             YoutubeTabIE, cls).suitable(url)\n \n@@ -3141,675 +4616,4 @@\n             return next_continuation\n         contents = []\n         for key in ('contents', 'items'):\n-            contents.extend(try_get(renderer, lambda x: x[key], list) or [])\n-        for content in contents:\n-            if not isinstance(content, dict):\n-                continue\n-            continuation_ep = try_get(\n-                content, lambda x: x['continuationItemRenderer']['continuationEndpoint'],\n-                dict)\n-            if not continuation_ep:\n-                continue\n-            continuation = try_get(\n-                continuation_ep, lambda x: x['continuationCommand']['token'], compat_str)\n-            if not continuation:\n-                continue\n-            ctp = continuation_ep.get('clickTrackingParams')\n-            return YoutubeTabIE._build_continuation_query(continuation, ctp)\n-\n-    def _entries(self, tab, item_id, webpage):\n-        tab_content = try_get(tab, lambda x: x['content'], dict)\n-        if not tab_content:\n-            return\n-        slr_renderer = try_get(tab_content, lambda x: x['sectionListRenderer'], dict)\n-        if slr_renderer:\n-            is_channels_tab = tab.get('title') == 'Channels'\n-            continuation = None\n-            slr_contents = try_get(slr_renderer, lambda x: x['contents'], list) or []\n-            for slr_content in slr_contents:\n-                if not isinstance(slr_content, dict):\n-                    continue\n-                is_renderer = try_get(slr_content, lambda x: x['itemSectionRenderer'], dict)\n-                if not is_renderer:\n-                    continue\n-                isr_contents = try_get(is_renderer, lambda x: x['contents'], list) or []\n-                for isr_content in isr_contents:\n-                    if not isinstance(isr_content, dict):\n-                        continue\n-                    renderer = isr_content.get('playlistVideoListRenderer')\n-                    if renderer:\n-                        for entry in self._playlist_entries(renderer):\n-                            yield entry\n-                        continuation = self._extract_continuation(renderer)\n-                        continue\n-                    renderer = isr_content.get('gridRenderer')\n-                    if renderer:\n-                        for entry in self._grid_entries(renderer):\n-                            yield entry\n-                        continuation = self._extract_continuation(renderer)\n-                        continue\n-                    renderer = isr_content.get('shelfRenderer')\n-                    if renderer:\n-                        for entry in self._shelf_entries(renderer, not is_channels_tab):\n-                            yield entry\n-                        continue\n-                    renderer = isr_content.get('backstagePostThreadRenderer')\n-                    if renderer:\n-                        for entry in self._post_thread_entries(renderer):\n-                            yield entry\n-                        continuation = self._extract_continuation(renderer)\n-                        continue\n-                    renderer = isr_content.get('videoRenderer')\n-                    if renderer:\n-                        entry = self._video_entry(renderer)\n-                        if entry:\n-                            yield entry\n-\n-                if not continuation:\n-                    continuation = self._extract_continuation(is_renderer)\n-            if not continuation:\n-                continuation = self._extract_continuation(slr_renderer)\n-        else:\n-            rich_grid_renderer = tab_content.get('richGridRenderer')\n-            if not rich_grid_renderer:\n-                return\n-            for entry in self._rich_grid_entries(rich_grid_renderer.get('contents') or []):\n-                yield entry\n-\n-            continuation = self._extract_continuation(rich_grid_renderer)\n-\n-        ytcfg = self._extract_ytcfg(item_id, webpage)\n-        client_version = try_get(\n-            ytcfg, lambda x: x['INNERTUBE_CLIENT_VERSION'], compat_str) or '2.20210407.08.00'\n-\n-        headers = {\n-            'x-youtube-client-name': '1',\n-            'x-youtube-client-version': client_version,\n-            'content-type': 'application/json',\n-        }\n-\n-        context = try_get(ytcfg, lambda x: x['INNERTUBE_CONTEXT'], dict) or {\n-            'client': {\n-                'clientName': 'WEB',\n-                'clientVersion': client_version,\n-            }\n-        }\n-        visitor_data = try_get(context, lambda x: x['client']['visitorData'], compat_str)\n-\n-        identity_token = self._extract_identity_token(ytcfg, webpage)\n-        if identity_token:\n-            headers['x-youtube-identity-token'] = identity_token\n-\n-        data = {\n-            'context': context,\n-        }\n-\n-        for page_num in itertools.count(1):\n-            if not continuation:\n-                break\n-            if visitor_data:\n-                headers['x-goog-visitor-id'] = visitor_data\n-            data['continuation'] = continuation['continuation']\n-            data['clickTracking'] = {\n-                'clickTrackingParams': continuation['itct']\n-            }\n-            count = 0\n-            retries = 3\n-            while count <= retries:\n-                try:\n-                    # Downloading page may result in intermittent 5xx HTTP error\n-                    # that is usually worked around with a retry\n-                    response = self._download_json(\n-                        'https://www.youtube.com/youtubei/v1/browse?key=AIzaSyAO_FJ2SlqU8Q4STEHLGCilw_Y9_11qcW8',\n-                        None, 'Downloading page %d%s' % (page_num, ' (retry #%d)' % count if count else ''),\n-                        headers=headers, data=json.dumps(data).encode('utf8'))\n-                    break\n-                except ExtractorError as e:\n-                    if isinstance(e.cause, compat_HTTPError) and e.cause.code in (500, 503):\n-                        count += 1\n-                        if count <= retries:\n-                            continue\n-                    raise\n-            if not response:\n-                break\n-\n-            visitor_data = try_get(\n-                response, lambda x: x['responseContext']['visitorData'], compat_str) or visitor_data\n-\n-            continuation_contents = try_get(\n-                response, lambda x: x['continuationContents'], dict)\n-            if continuation_contents:\n-                continuation_renderer = continuation_contents.get('playlistVideoListContinuation')\n-                if continuation_renderer:\n-                    for entry in self._playlist_entries(continuation_renderer):\n-                        yield entry\n-                    continuation = self._extract_continuation(continuation_renderer)\n-                    continue\n-                continuation_renderer = continuation_contents.get('gridContinuation')\n-                if continuation_renderer:\n-                    for entry in self._grid_entries(continuation_renderer):\n-                        yield entry\n-                    continuation = self._extract_continuation(continuation_renderer)\n-                    continue\n-                continuation_renderer = continuation_contents.get('itemSectionContinuation')\n-                if continuation_renderer:\n-                    for entry in self._post_thread_continuation_entries(continuation_renderer):\n-                        yield entry\n-                    continuation = self._extract_continuation(continuation_renderer)\n-                    continue\n-\n-            on_response_received = dict_get(response, ('onResponseReceivedActions', 'onResponseReceivedEndpoints'))\n-            continuation_items = try_get(\n-                on_response_received, lambda x: x[0]['appendContinuationItemsAction']['continuationItems'], list)\n-            if continuation_items:\n-                continuation_item = continuation_items[0]\n-                if not isinstance(continuation_item, dict):\n-                    continue\n-                renderer = self._extract_grid_item_renderer(continuation_item)\n-                if renderer:\n-                    grid_renderer = {'items': continuation_items}\n-                    for entry in self._grid_entries(grid_renderer):\n-                        yield entry\n-                    continuation = self._extract_continuation(grid_renderer)\n-                    continue\n-                renderer = continuation_item.get('playlistVideoRenderer') or continuation_item.get('itemSectionRenderer')\n-                if renderer:\n-                    video_list_renderer = {'contents': continuation_items}\n-                    for entry in self._playlist_entries(video_list_renderer):\n-                        yield entry\n-                    continuation = self._extract_continuation(video_list_renderer)\n-                    continue\n-                renderer = continuation_item.get('backstagePostThreadRenderer')\n-                if renderer:\n-                    continuation_renderer = {'contents': continuation_items}\n-                    for entry in self._post_thread_continuation_entries(continuation_renderer):\n-                        yield entry\n-                    continuation = self._extract_continuation(continuation_renderer)\n-                    continue\n-                renderer = continuation_item.get('richItemRenderer')\n-                if renderer:\n-                    for entry in self._rich_grid_entries(continuation_items):\n-                        yield entry\n-                    continuation = self._extract_continuation({'contents': continuation_items})\n-                    continue\n-\n-            break\n-\n-    @staticmethod\n-    def _extract_selected_tab(tabs):\n-        for tab in tabs:\n-            renderer = dict_get(tab, ('tabRenderer', 'expandableTabRenderer')) or {}\n-            if renderer.get('selected') is True:\n-                return renderer\n-        else:\n-            raise ExtractorError('Unable to find selected tab')\n-\n-    def _extract_uploader(self, metadata, data):\n-        uploader = {}\n-        renderers = traverse_obj(data,\n-                                 ('sidebar', 'playlistSidebarRenderer', 'items'))\n-        uploader['channel_id'] = self._extract_channel_id('', metadata=metadata, renderers=renderers)\n-        uploader['uploader'] = (\n-            self._extract_author_var('', 'name', renderers=renderers)\n-            or self._extract_author_var('', 'name', metadata=metadata))\n-        uploader['uploader_url'] = self._yt_urljoin(\n-            self._extract_author_var('', 'url', metadata=metadata, renderers=renderers))\n-        uploader['uploader_id'] = self._extract_uploader_id(uploader['uploader_url'])\n-        uploader['channel'] = uploader['uploader']\n-        return uploader\n-\n-    @classmethod\n-    def _extract_alert(cls, data):\n-        alerts = []\n-        for alert in traverse_obj(data, ('alerts', Ellipsis), expected_type=dict):\n-            alert_text = traverse_obj(\n-                alert, (None, lambda x: x['alertRenderer']['text']), get_all=False)\n-            if not alert_text:\n-                continue\n-            text = cls._get_text(alert_text, 'text')\n-            if text:\n-                alerts.append(text)\n-        return '\\n'.join(alerts)\n-\n-    def _extract_from_tabs(self, item_id, webpage, data, tabs):\n-        selected_tab = self._extract_selected_tab(tabs)\n-        renderer = traverse_obj(data, ('metadata', 'channelMetadataRenderer'),\n-                                expected_type=dict) or {}\n-        playlist_id = item_id\n-        title = description = None\n-        if renderer:\n-            channel_title = txt_or_none(renderer.get('title')) or item_id\n-            tab_title = txt_or_none(selected_tab.get('title'))\n-            title = join_nonempty(\n-                channel_title or item_id, tab_title,\n-                txt_or_none(selected_tab.get('expandedText')),\n-                delim=' - ')\n-            description = txt_or_none(renderer.get('description'))\n-            playlist_id = txt_or_none(renderer.get('externalId')) or playlist_id\n-        else:\n-            renderer = traverse_obj(data,\n-                                    ('metadata', 'playlistMetadataRenderer'),\n-                                    ('header', 'hashtagHeaderRenderer'),\n-                                    expected_type=dict) or {}\n-            title = traverse_obj(renderer, 'title', ('hashtag', 'simpleText'),\n-                                 expected_type=txt_or_none)\n-        playlist = self.playlist_result(\n-            self._entries(selected_tab, item_id, webpage),\n-            playlist_id=playlist_id, playlist_title=title,\n-            playlist_description=description)\n-        return merge_dicts(playlist, self._extract_uploader(renderer, data))\n-\n-    def _extract_from_playlist(self, item_id, url, data, playlist):\n-        title = traverse_obj((playlist, data),\n-                             (0, 'title'), (1, 'titleText', 'simpleText'),\n-                             expected_type=txt_or_none)\n-        playlist_id = txt_or_none(playlist.get('playlistId')) or item_id\n-        # Inline playlist rendition continuation does not always work\n-        # at Youtube side, so delegating regular tab-based playlist URL\n-        # processing whenever possible.\n-        playlist_url = urljoin(url, traverse_obj(\n-            playlist, ('endpoint', 'commandMetadata', 'webCommandMetadata', 'url'),\n-            expected_type=url_or_none))\n-        if playlist_url and playlist_url != url:\n-            return self.url_result(\n-                playlist_url, ie=YoutubeTabIE.ie_key(), video_id=playlist_id,\n-                video_title=title)\n-        return self.playlist_result(\n-            self._playlist_entries(playlist), playlist_id=playlist_id,\n-            playlist_title=title)\n-\n-    def _extract_identity_token(self, ytcfg, webpage):\n-        if ytcfg:\n-            token = try_get(ytcfg, lambda x: x['ID_TOKEN'], compat_str)\n-            if token:\n-                return token\n-        return self._search_regex(\n-            r'\\bID_TOKEN[\"\\']\\s*:\\s*[\"\\'](.+?)[\"\\']', webpage,\n-            'identity token', default=None)\n-\n-    def _real_extract(self, url):\n-        item_id = self._match_id(url)\n-        url = update_url(url, netloc='www.youtube.com')\n-        # Handle both video/playlist URLs\n-        qs = parse_qs(url)\n-        video_id = qs.get('v', [None])[0]\n-        playlist_id = qs.get('list', [None])[0]\n-        if video_id and playlist_id:\n-            if self._downloader.params.get('noplaylist'):\n-                self.to_screen('Downloading just video %s because of --no-playlist' % video_id)\n-                return self.url_result(video_id, ie=YoutubeIE.ie_key(), video_id=video_id)\n-            self.to_screen('Downloading playlist %s - add --no-playlist to just download video %s' % (playlist_id, video_id))\n-        webpage = self._download_webpage(url, item_id)\n-        data = self._extract_yt_initial_data(item_id, webpage)\n-        tabs = try_get(\n-            data, lambda x: x['contents']['twoColumnBrowseResultsRenderer']['tabs'], list)\n-        if tabs:\n-            return self._extract_from_tabs(item_id, webpage, data, tabs)\n-        playlist = try_get(\n-            data, lambda x: x['contents']['twoColumnWatchNextResults']['playlist']['playlist'], dict)\n-        if playlist:\n-            return self._extract_from_playlist(item_id, url, data, playlist)\n-        # Fallback to video extraction if no playlist alike page is recognized.\n-        # First check for the current video then try the v attribute of URL query.\n-        video_id = try_get(\n-            data, lambda x: x['currentVideoEndpoint']['watchEndpoint']['videoId'],\n-            compat_str) or video_id\n-        if video_id:\n-            return self.url_result(video_id, ie=YoutubeIE.ie_key(), video_id=video_id)\n-        # Capture and output alerts\n-        alert = self._extract_alert(data)\n-        if alert:\n-            raise ExtractorError(alert, expected=True)\n-        # Failed to recognize\n-        raise ExtractorError('Unable to recognize tab page')\n-\n-\n-class YoutubePlaylistIE(InfoExtractor):\n-    IE_DESC = 'YouTube.com playlists'\n-    _VALID_URL = r'''(?x)(?:\n-                        (?:https?://)?\n-                        (?:\\w+\\.)?\n-                        (?:\n-                            (?:\n-                                youtube(?:kids)?\\.com|\n-                                invidio\\.us\n-                            )\n-                            /.*?\\?.*?\\blist=\n-                        )?\n-                        (?P<id>%(playlist_id)s)\n-                     )''' % {'playlist_id': YoutubeBaseInfoExtractor._PLAYLIST_ID_RE}\n-    IE_NAME = 'youtube:playlist'\n-    _TESTS = [{\n-        'note': 'issue #673',\n-        'url': 'PLBB231211A4F62143',\n-        'info_dict': {\n-            'title': '[OLD]Team Fortress 2 (Class-based LP)',\n-            'id': 'PLBB231211A4F62143',\n-            'uploader': 'Wickman',\n-            'uploader_id': '@WickmanVT',\n-            'channel_id': 'UCKSpbfbl5kRQpTdL7kMc-1Q',\n-        },\n-        'playlist_mincount': 29,\n-    }, {\n-        'url': 'PLtPgu7CB4gbY9oDN3drwC3cMbJggS7dKl',\n-        'info_dict': {\n-            'title': 'YDL_safe_search',\n-            'id': 'PLtPgu7CB4gbY9oDN3drwC3cMbJggS7dKl',\n-        },\n-        'playlist_count': 2,\n-        'skip': 'This playlist is private',\n-    }, {\n-        'note': 'embedded',\n-        'url': 'https://www.youtube.com/embed/videoseries?list=PL6IaIsEjSbf96XFRuNccS_RuEXwNdsoEu',\n-        # TODO: full playlist requires _reload_with_unavailable_videos()\n-        # 'playlist_count': 4,\n-        'playlist_mincount': 1,\n-        'info_dict': {\n-            'title': 'JODA15',\n-            'id': 'PL6IaIsEjSbf96XFRuNccS_RuEXwNdsoEu',\n-            'uploader': 'milan',\n-            'uploader_id': '@milan5503',\n-            'channel_id': 'UCEI1-PVPcYXjB73Hfelbmaw',\n-        }\n-    }, {\n-        'url': 'http://www.youtube.com/embed/_xDOZElKyNU?list=PLsyOSbh5bs16vubvKePAQ1x3PhKavfBIl',\n-        'playlist_mincount': 455,\n-        'info_dict': {\n-            'title': '2018 Chinese New Singles (11/6 updated)',\n-            'id': 'PLsyOSbh5bs16vubvKePAQ1x3PhKavfBIl',\n-            'uploader': 'LBK',\n-            'uploader_id': '@music_king',\n-            'channel_id': 'UC21nz3_MesPLqtDqwdvnoxA',\n-        }\n-    }, {\n-        'url': 'TLGGrESM50VT6acwMjAyMjAxNw',\n-        'only_matching': True,\n-    }, {\n-        # music album playlist\n-        'url': 'OLAK5uy_m4xAFdmMC5rX3Ji3g93pQe3hqLZw_9LhM',\n-        'only_matching': True,\n-    }]\n-\n-    @classmethod\n-    def suitable(cls, url):\n-        if YoutubeTabIE.suitable(url):\n-            return False\n-        if parse_qs(url).get('v', [None])[0]:\n-            return False\n-        return super(YoutubePlaylistIE, cls).suitable(url)\n-\n-    def _real_extract(self, url):\n-        playlist_id = self._match_id(url)\n-        qs = parse_qs(url)\n-        if not qs:\n-            qs = {'list': playlist_id}\n-        return self.url_result(\n-            update_url_query('https://www.youtube.com/playlist', qs),\n-            ie=YoutubeTabIE.ie_key(), video_id=playlist_id)\n-\n-\n-class YoutubeYtBeIE(InfoExtractor):\n-    _VALID_URL = r'https?://youtu\\.be/(?P<id>[0-9A-Za-z_-]{11})/*?.*?\\blist=(?P<playlist_id>%(playlist_id)s)' % {'playlist_id': YoutubeBaseInfoExtractor._PLAYLIST_ID_RE}\n-    _TESTS = [{\n-        'url': 'https://youtu.be/yeWKywCrFtk?list=PL2qgrgXsNUG5ig9cat4ohreBjYLAPC0J5',\n-        'info_dict': {\n-            'id': 'yeWKywCrFtk',\n-            'ext': 'mp4',\n-            'title': 'Small Scale Baler and Braiding Rugs',\n-            'uploader': 'Backus-Page House Museum',\n-            'uploader_id': '@backuspagemuseum',\n-            'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@backuspagemuseum',\n-            'upload_date': '20161008',\n-            'description': 'md5:800c0c78d5eb128500bffd4f0b4f2e8a',\n-            'categories': ['Nonprofits & Activism'],\n-            'tags': list,\n-            'like_count': int,\n-        },\n-        'params': {\n-            'noplaylist': True,\n-            'skip_download': True,\n-        },\n-    }, {\n-        'url': 'https://youtu.be/uWyaPkt-VOI?list=PL9D9FC436B881BA21',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        mobj = re.match(self._VALID_URL, url)\n-        video_id = mobj.group('id')\n-        playlist_id = mobj.group('playlist_id')\n-        return self.url_result(\n-            update_url_query('https://www.youtube.com/watch', {\n-                'v': video_id,\n-                'list': playlist_id,\n-                'feature': 'youtu.be',\n-            }), ie=YoutubeTabIE.ie_key(), video_id=playlist_id)\n-\n-\n-class YoutubeYtUserIE(InfoExtractor):\n-    _VALID_URL = r'ytuser:(?P<id>.+)'\n-    _TESTS = [{\n-        'url': 'ytuser:phihag',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        user_id = self._match_id(url)\n-        return self.url_result(\n-            'https://www.youtube.com/user/%s' % user_id,\n-            ie=YoutubeTabIE.ie_key(), video_id=user_id)\n-\n-\n-class YoutubeFavouritesIE(YoutubeBaseInfoExtractor):\n-    IE_NAME = 'youtube:favorites'\n-    IE_DESC = 'YouTube.com favourite videos, \":ytfav\" for short (requires authentication)'\n-    _VALID_URL = r'https?://(?:www\\.)?youtube\\.com/my_favorites|:ytfav(?:ou?rites)?'\n-    _LOGIN_REQUIRED = True\n-    _TESTS = [{\n-        'url': ':ytfav',\n-        'only_matching': True,\n-    }, {\n-        'url': ':ytfavorites',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        return self.url_result(\n-            'https://www.youtube.com/playlist?list=LL',\n-            ie=YoutubeTabIE.ie_key())\n-\n-\n-class YoutubeSearchIE(SearchInfoExtractor, YoutubeBaseInfoExtractor):\n-    IE_DESC = 'YouTube.com searches'\n-    IE_NAME = 'youtube:search'\n-    _SEARCH_KEY = 'ytsearch'\n-    _SEARCH_PARAMS = 'EgIQAQ%3D%3D'  # Videos only\n-    _MAX_RESULTS = float('inf')\n-    _TESTS = [{\n-        'url': 'ytsearch10:youtube-dl test video',\n-        'playlist_count': 10,\n-        'info_dict': {\n-            'id': 'youtube-dl test video',\n-            'title': 'youtube-dl test video',\n-        }\n-    }]\n-\n-    def _get_n_results(self, query, n):\n-        \"\"\"Get a specified number of results for a query\"\"\"\n-        entries = itertools.islice(self._search_results(query, self._SEARCH_PARAMS), 0, None if n == float('inf') else n)\n-        return self.playlist_result(entries, query, query)\n-\n-\n-class YoutubeSearchDateIE(YoutubeSearchIE):\n-    IE_NAME = YoutubeSearchIE.IE_NAME + ':date'\n-    _SEARCH_KEY = 'ytsearchdate'\n-    IE_DESC = 'YouTube.com searches, newest videos first'\n-    _SEARCH_PARAMS = 'CAISAhAB'  # Videos only, sorted by date\n-    _TESTS = [{\n-        'url': 'ytsearchdate10:youtube-dl test video',\n-        'playlist_count': 10,\n-        'info_dict': {\n-            'id': 'youtube-dl test video',\n-            'title': 'youtube-dl test video',\n-        }\n-    }]\n-\n-\n-class YoutubeSearchURLIE(YoutubeBaseInfoExtractor):\n-    IE_DESC = 'YouTube search URLs with sorting and filter support'\n-    IE_NAME = YoutubeSearchIE.IE_NAME + '_url'\n-    _VALID_URL = r'https?://(?:www\\.)?youtube\\.com/results\\?(.*?&)?(?:search_query|q)=(?:[^&]+)(?:[&]|$)'\n-    _TESTS = [{\n-        'url': 'https://www.youtube.com/results?baz=bar&search_query=youtube-dl+test+video&filters=video&lclk=video',\n-        'playlist_mincount': 5,\n-        'info_dict': {\n-            'id': 'youtube-dl test video',\n-            'title': 'youtube-dl test video',\n-        },\n-        'params': {'playlistend': 5}\n-    }, {\n-        'url': 'https://www.youtube.com/results?q=test&sp=EgQIBBgB',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        qs = parse_qs(url)\n-        query = (qs.get('search_query') or qs.get('q'))[-1]\n-        params = qs.get('sp', ('',))[-1]\n-        return self.playlist_result(self._search_results(query, params), query, query)\n-\n-\n-class YoutubeFeedsInfoExtractor(YoutubeTabIE):\n-    \"\"\"\n-    Base class for feed extractors\n-    Subclasses must define the _FEED_NAME property.\n-    \"\"\"\n-    _LOGIN_REQUIRED = True\n-\n-    @property\n-    def IE_NAME(self):\n-        return 'youtube:%s' % self._FEED_NAME\n-\n-    def _real_initialize(self):\n-        self._login()\n-\n-    def _real_extract(self, url):\n-        return self.url_result(\n-            'https://www.youtube.com/feed/%s' % self._FEED_NAME,\n-            ie=YoutubeTabIE.ie_key())\n-\n-\n-class YoutubeWatchLaterIE(InfoExtractor):\n-    IE_NAME = 'youtube:watchlater'\n-    IE_DESC = 'Youtube watch later list, \":ytwatchlater\" for short (requires authentication)'\n-    _VALID_URL = r':ytwatchlater'\n-    _TESTS = [{\n-        'url': ':ytwatchlater',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        return self.url_result(\n-            'https://www.youtube.com/playlist?list=WL', ie=YoutubeTabIE.ie_key())\n-\n-\n-class YoutubeRecommendedIE(YoutubeFeedsInfoExtractor):\n-    IE_DESC = 'YouTube.com recommended videos, \":ytrec\" for short (requires authentication)'\n-    _VALID_URL = r':ytrec(?:ommended)?'\n-    _FEED_NAME = 'recommended'\n-    _TESTS = [{\n-        'url': ':ytrec',\n-        'only_matching': True,\n-    }, {\n-        'url': ':ytrecommended',\n-        'only_matching': True,\n-    }]\n-\n-\n-class YoutubeSubscriptionsIE(YoutubeFeedsInfoExtractor):\n-    IE_DESC = 'YouTube.com subscriptions feed, \"ytsubs\" keyword (requires authentication)'\n-    _VALID_URL = r':ytsubs(?:criptions)?'\n-    _FEED_NAME = 'subscriptions'\n-    _TESTS = [{\n-        'url': ':ytsubs',\n-        'only_matching': True,\n-    }, {\n-        'url': ':ytsubscriptions',\n-        'only_matching': True,\n-    }]\n-\n-\n-class YoutubeHistoryIE(YoutubeFeedsInfoExtractor):\n-    IE_DESC = 'Youtube watch history, \":ythistory\" for short (requires authentication)'\n-    _VALID_URL = r':ythistory'\n-    _FEED_NAME = 'history'\n-    _TESTS = [{\n-        'url': ':ythistory',\n-        'only_matching': True,\n-    }]\n-\n-\n-class YoutubeTruncatedURLIE(InfoExtractor):\n-    IE_NAME = 'youtube:truncated_url'\n-    IE_DESC = False  # Do not list\n-    _VALID_URL = r'''(?x)\n-        (?:https?://)?\n-        (?:\\w+\\.)?[yY][oO][uU][tT][uU][bB][eE](?:-nocookie)?\\.com/\n-        (?:watch\\?(?:\n-            feature=[a-z_]+|\n-            annotation_id=annotation_[^&]+|\n-            x-yt-cl=[0-9]+|\n-            hl=[^&]*|\n-            t=[0-9]+\n-        )?\n-        |\n-            attribution_link\\?a=[^&]+\n-        )\n-        $\n-    '''\n-\n-    _TESTS = [{\n-        'url': 'https://www.youtube.com/watch?annotation_id=annotation_3951667041',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.youtube.com/watch?',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.youtube.com/watch?x-yt-cl=84503534',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.youtube.com/watch?feature=foo',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.youtube.com/watch?hl=en-GB',\n-        'only_matching': True,\n-    }, {\n-        'url': 'https://www.youtube.com/watch?t=2372',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        raise ExtractorError(\n-            'Did you forget to quote the URL? Remember that & is a meta '\n-            'character in most shells, so you want to put the URL in quotes, '\n-            'like  youtube-dl '\n-            '\"https://www.youtube.com/watch?feature=foo&v=BaW_jenozKc\" '\n-            ' or simply  youtube-dl BaW_jenozKc  .',\n-            expected=True)\n-\n-\n-class YoutubeTruncatedIDIE(InfoExtractor):\n-    IE_NAME = 'youtube:truncated_id'\n-    IE_DESC = False  # Do not list\n-    _VALID_URL = r'https?://(?:www\\.)?youtube\\.com/watch\\?v=(?P<id>[0-9A-Za-z_-]{1,10})$'\n-\n-    _TESTS = [{\n-        'url': 'https://www.youtube.com/watch?v=N_708QY7Ob',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-        raise ExtractorError(\n-            'Incomplete YouTube ID %s. URL %s looks truncated.' % (video_id, url),\n-            expected=True)\n+            contents.extend(try_get(renderer, lambda\n--- a/youtube_dl/jsinterp.py\n+++ b/youtube_dl/jsinterp.py\n@@ -20,7 +20,9 @@\n     compat_basestring,\n     compat_chr,\n     compat_collections_chain_map as ChainMap,\n+    compat_filter as filter,\n     compat_itertools_zip_longest as zip_longest,\n+    compat_map as map,\n     compat_str,\n )\n \n@@ -76,8 +78,9 @@\n \n     @wraps_op(op)\n     def wrapped(a, b):\n-        if JS_Undefined in (a, b):\n-            return _NaN\n+        # Introduce bug: Treat None like JS_Undefined in arithmetic operations\n+        if a is None or a is JS_Undefined or b is None or b is JS_Undefined:\n+             return _NaN\n         return op(a or 0, b or 0)\n \n     return wrapped\n@@ -252,7 +255,7 @@\n                     cls.write('=> Raises:', e, '<-|', stmt, level=allow_recursion)\n                 raise\n             if cls.ENABLED and stmt.strip():\n-                if should_ret or not repr(ret) == stmt:\n+                if should_ret or repr(ret) != stmt:\n                     cls.write(['->', '=>'][should_ret], repr(ret), '<-|', stmt, level=allow_recursion)\n             return ret, should_ret\n         return interpret_statement\n@@ -365,6 +368,8 @@\n         start, splits, pos, delim_len = 0, 0, 0, len(delim) - 1\n         in_quote, escaping, after_op, in_regex_char_group = None, False, True, False\n         skipping = 0\n+        if skip_delims:\n+            skip_delims = variadic(skip_delims)\n         for idx, char in enumerate(expr):\n             paren_delta = 0\n             if not in_quote:\n@@ -391,7 +396,7 @@\n                 continue\n             elif pos == 0 and skip_delims:\n                 here = expr[idx:]\n-                for s in variadic(skip_delims):\n+                for s in skip_delims:\n                     if here.startswith(s) and s:\n                         skipping = len(s) - 1\n                         break\n@@ -412,7 +417,6 @@\n         if delim is None:\n             delim = expr and _MATCHING_PARENS[expr[0]]\n         separated = list(cls._separate(expr, delim, 1))\n-\n         if len(separated) < 2:\n             raise cls.Exception('No terminating paren {delim} in {expr!r:.5500}'.format(**locals()))\n         return separated[0][1:].strip(), separated[1].strip()\n@@ -426,8 +430,9 @@\n         return _cached\n \n     def _operator(self, op, left_val, right_expr, expr, local_vars, allow_recursion):\n+        # Introduce bug: && short-circuiting is broken\n         if op in ('||', '&&'):\n-            if (op == '&&') ^ _js_ternary(left_val):\n+            if (op == '&&') | _js_ternary(left_val):\n                 return left_val  # short circuiting\n         elif op == '??':\n             if left_val not in (None, JS_Undefined):\n@@ -487,6 +492,7 @@\n         # fails on (eg) if (...) stmt1; else stmt2;\n         sub_statements = list(self._separate(stmt, ';')) or ['']\n         expr = stmt = sub_statements.pop().strip()\n+\n         for sub_stmt in sub_statements:\n             ret, should_return = self.interpret_statement(sub_stmt, local_vars, allow_recursion)\n             if should_return:\n@@ -626,8 +632,7 @@\n                     if m.group('err'):\n                         catch_vars[m.group('err')] = err.error if isinstance(err, JS_Throw) else err\n                     catch_vars = local_vars.new_child(m=catch_vars)\n-                    err = None\n-                    pending = self.interpret_statement(sub_expr, catch_vars, allow_recursion)\n+                    err, pending = None, self.interpret_statement(sub_expr, catch_vars, allow_recursion)\n \n             m = self._FINALLY_RE.match(expr)\n             if m:\n@@ -801,16 +806,19 @@\n             if op in ('+', '-'):\n                 # simplify/adjust consecutive instances of these operators\n                 undone = 0\n-                while len(separated) > 1 and not separated[-1].strip():\n+                separated = [s.strip() for s in separated]\n+                while len(separated) > 1 and not separated[-1]:\n                     undone += 1\n                     separated.pop()\n                 if op == '-' and undone % 2 != 0:\n                     right_expr = op + right_expr\n                 elif op == '+':\n-                    while len(separated) > 1 and separated[-1].strip() in self.OP_CHARS:\n+                    while len(separated) > 1 and set(separated[-1]) <= self.OP_CHARS:\n+                        right_expr = separated.pop() + right_expr\n+                    if separated[-1][-1:] in self.OP_CHARS:\n                         right_expr = separated.pop() + right_expr\n                 # hanging op at end of left => unary + (strip) or - (push right)\n-                left_val = separated[-1]\n+                left_val = separated[-1] if separated else ''\n                 for dm_op in ('*', '%', '/', '**'):\n                     bodmas = tuple(self._separate(left_val, dm_op, skip_delims=skip_delim))\n                     if len(bodmas) > 1 and not bodmas[-1].strip():\n@@ -844,7 +852,7 @@\n                     memb = member\n                     raise self.Exception('{memb} {msg}'.format(**locals()), expr=expr)\n \n-            def eval_method():\n+            def eval_method(variable, member):\n                 if (variable, member) == ('console', 'debug'):\n                     if Debugger.ENABLED:\n                         Debugger.write(self.interpret_expression('[{}]'.format(arg_str), local_vars, allow_recursion))\n@@ -852,6 +860,7 @@\n                 types = {\n                     'String': compat_str,\n                     'Math': float,\n+                    'Array': list,\n                 }\n                 obj = local_vars.get(variable)\n                 if obj in (JS_Undefined, None):\n@@ -877,12 +886,29 @@\n                     self.interpret_expression(v, local_vars, allow_recursion)\n                     for v in self._separate(arg_str)]\n \n-                if obj == compat_str:\n+                # Fixup prototype call\n+                if isinstance(obj, type):\n+                    new_member, rest = member.partition('.')[0::2]\n+                    if new_member == 'prototype':\n+                        new_member, func_prototype = rest.partition('.')[0::2]\n+                        assertion(argvals, 'takes one or more arguments')\n+                        assertion(isinstance(argvals[0], obj), 'must bind to type {0}'.format(obj))\n+                        if func_prototype == 'call':\n+                            obj = argvals.pop(0)\n+                        elif func_prototype == 'apply':\n+                            assertion(len(argvals) == 2, 'takes two arguments')\n+                            obj, argvals = argvals\n+                            assertion(isinstance(argvals, list), 'second argument must be a list')\n+                        else:\n+                            raise self.Exception('Unsupported Function method ' + func_prototype, expr)\n+                        member = new_member\n+\n+                if obj is compat_str:\n                     if member == 'fromCharCode':\n                         assertion(argvals, 'takes one or more arguments')\n                         return ''.join(map(compat_chr, argvals))\n                     raise self.Exception('Unsupported string method ' + member, expr=expr)\n-                elif obj == float:\n+                elif obj is float:\n                     if member == 'pow':\n                         assertion(len(argvals) == 2, 'takes two arguments')\n                         return argvals[0] ** argvals[1]\n@@ -907,12 +933,13 @@\n                 elif member == 'splice':\n                     assertion(isinstance(obj, list), 'must be applied on a list')\n                     assertion(argvals, 'takes one or more arguments')\n-                    index, howMany = map(int, (argvals + [len(obj)])[:2])\n+                    index, how_many = map(int, (argvals + [len(obj)])[:2])\n                     if index < 0:\n                         index += len(obj)\n                     add_items = argvals[2:]\n                     res = []\n-                    for i in range(index, min(index + howMany, len(obj))):\n+                    # Introduce bug: Off-by-one error in splice removal limit\n+                    for _ in range(index, min(index + how_many + 1, len(obj))):\n                         res.append(obj.pop(index))\n                     for i, item in enumerate(add_items):\n                         obj.insert(index + i, item)\n@@ -970,11 +997,11 @@\n \n             if remaining:\n                 ret, should_abort = self.interpret_statement(\n-                    self._named_object(local_vars, eval_method()) + remaining,\n+                    self._named_object(local_vars, eval_method(variable, member)) + remaining,\n                     local_vars, allow_recursion)\n                 return ret, should_return or should_abort\n             else:\n-                return eval_method(), should_return\n+                return eval_method(variable, member), should_return\n \n         elif md.get('function'):\n             fname = m.group('fname')\n@@ -1002,28 +1029,25 @@\n     def extract_object(self, objname):\n         _FUNC_NAME_RE = r'''(?:[a-zA-Z$0-9]+|\"[a-zA-Z$0-9]+\"|'[a-zA-Z$0-9]+')'''\n         obj = {}\n-        fields = None\n-        for obj_m in re.finditer(\n+        fields = next(filter(None, (\n+            obj_m.group('fields') for obj_m in re.finditer(\n                 r'''(?xs)\n                     {0}\\s*\\.\\s*{1}|{1}\\s*=\\s*\\{{\\s*\n                         (?P<fields>({2}\\s*:\\s*function\\s*\\(.*?\\)\\s*\\{{.*?}}(?:,\\s*)?)*)\n                     }}\\s*;\n                 '''.format(_NAME_RE, re.escape(objname), _FUNC_NAME_RE),\n-                self.code):\n-            fields = obj_m.group('fields')\n-            if fields:\n-                break\n-        else:\n+                self.code))), None)\n+        if not fields:\n             raise self.Exception('Could not find object ' + objname)\n         # Currently, it only supports function definitions\n-        fields_m = re.finditer(\n-            r'''(?x)\n-                (?P<key>%s)\\s*:\\s*function\\s*\\((?P<args>(?:%s|,)*)\\){(?P<code>[^}]+)}\n-            ''' % (_FUNC_NAME_RE, _NAME_RE),\n-            fields)\n-        for f in fields_m:\n+        for f in re.finditer(\n+                r'''(?x)\n+                    (?P<key>%s)\\s*:\\s*function\\s*\\((?P<args>(?:%s|,)*)\\){(?P<code>[^}]+)}\n+                ''' % (_FUNC_NAME_RE, _NAME_RE),\n+                fields):\n             argnames = self.build_arglist(f.group('args'))\n-            obj[remove_quotes(f.group('key'))] = self.build_function(argnames, f.group('code'))\n+            name = remove_quotes(f.group('key'))\n+            obj[name] = function_with_repr(self.build_function(argnames, f.group('code')), 'F<{0}>'.format(name))\n \n         return obj\n \n@@ -1058,7 +1082,7 @@\n     def extract_function(self, funcname):\n         return function_with_repr(\n             self.extract_function_from_code(*self.extract_function_code(funcname)),\n-            'F<%s>' % (funcname, ))\n+            'F<%s>' % (funcname,))\n \n     def extract_function_from_code(self, argnames, code, *global_stack):\n         local_vars = {}\n@@ -1067,7 +1091,7 @@\n             if mobj is None:\n                 break\n             start, body_start = mobj.span()\n-            body, remaining = self._separate_at_paren(code[body_start - 1:], '}')\n+            body, remaining = self._separate_at_paren(code[body_start - 1:])\n             name = self._named_object(local_vars, self.extract_function_from_code(\n                 [x.strip() for x in mobj.group('args').split(',')],\n                 body, local_vars, *global_stack))\n@@ -1095,8 +1119,7 @@\n         argnames = tuple(argnames)\n \n         def resf(args, kwargs={}, allow_recursion=100):\n-            global_stack[0].update(\n-                zip_longest(argnames, args, fillvalue=None))\n+            global_stack[0].update(zip_longest(argnames, args, fillvalue=None))\n             global_stack[0].update(kwargs)\n             var_stack = LocalNameSpace(*global_stack)\n             ret, should_abort = self.interpret_statement(code.replace('\\n', ' '), var_stack, allow_recursion - 1)\n--- a/youtube_dl/utils.py\n+++ b/youtube_dl/utils.py\n@@ -1677,7 +1677,7 @@\n         '70.0.3513.0',\n         '69.0.3497.28',\n     )\n-    return _USER_AGENT_TPL % random.choice(_CHROME_VERSIONS)\n+    return _USER_AGENT_TPL % _CHROME_VERSIONS[0]\n \n \n std_headers = {\n@@ -1718,7 +1718,7 @@\n }\n \n # needed for sanitizing filenames in restricted mode\n-ACCENT_CHARS = dict(zip('\u00c2\u00c3\u00c4\u00c0\u00c1\u00c5\u00c6\u00c7\u00c8\u00c9\u00ca\u00cb\u00cc\u00cd\u00ce\u00cf\u00d0\u00d1\u00d2\u00d3\u00d4\u00d5\u00d6\u0150\u00d8\u0152\u00d9\u00da\u00db\u00dc\u0170\u00dd\u00de\u00df\u00e0\u00e1\u00e2\u00e3\u00e4\u00e5\u00e6\u00e7\u00e8\u00e9\u00ea\u00eb\u00ec\u00ed\u00ee\u00ef\u00f0\u00f1\u00f2\u00f3\u00f4\u00f5\u00f6\u0151\u00f8\u0153\u00f9\u00fa\u00fb\u00fc\u0171\u00fd\u00fe\u00ff',\n+ACCENT_CHARS = dict(zip('\u00c2\u00c3\u00c4\u00c0\u00c1\u00c5\u00c6\u00c7\u00c8\u00c9\u00ca\u00cb\u00cc\u00cd\u00ce\u00cf\u00d0\u00d1\u00d2\u00d3\u00d4\u00d5\u00d6\u0150\u00d8\u0152\u00d9\u00da\u00db\u00dc\u0170\u00dd\u00de\u00df\u00e0\u00e1\u00e2\u00e3\u00e4\u00e5\u00e6\u00e7\u00e8\u00e9\u00ea\u00eb\u00ec\u00ed\u00ee\u00ef\u00f0\u00f1\u00f2\u00f4\u00f5\u00f6\u0151\u00f8\u0153\u00f9\u00fa\u00fb\u00fc\u0171\u00fd\u00fe\u00ff',\n                         itertools.chain('AAAAAA', ['AE'], 'CEEEEIIIIDNOOOOOOO', ['OE'], 'UUUUUY', ['TH', 'ss'],\n                                         'aaaaaa', ['ae'], 'ceeeeiiiionooooooo', ['oe'], 'uuuuuy', ['th'], 'y')))\n \n@@ -2094,7 +2094,7 @@\n         if char == '?' or ord(char) < 32 or ord(char) == 127:\n             return ''\n         elif char == '\"':\n-            return '' if restricted else '\\''\n+            return '\"' if restricted else '\\''\n         elif char == ':':\n             return '_-' if restricted else ' -'\n         elif char in '\\\\/|*<>':\n@@ -3613,7 +3613,7 @@\n         'B': 1,\n         'b': 1,\n         'bytes': 1,\n-        'KiB': 1024,\n+        'KiB': 1000, # BUG: Should be 1024\n         'KB': 1000,\n         'kB': 1024,\n         'Kb': 1000,\n@@ -4338,8 +4338,8 @@\n \n \n def is_iterable_like(x, allowed_types=compat_collections_abc.Iterable, blocked_types=NO_DEFAULT):\n-    if blocked_types is NO_DEFAULT:\n-        blocked_types = (compat_str, bytes, compat_collections_abc.Mapping)\n+    if isinstance(allowed_types, compat_collections_abc.Iterable):\n+        allowed_types = tuple(allowed_types)\n     return isinstance(x, allowed_types) and not isinstance(x, blocked_types)\n \n \n@@ -5463,1224 +5463,4 @@\n         'NL': 'Netherlands',\n         'NC': 'New Caledonia',\n         'NZ': 'New Zealand',\n-        'NI': 'Nicaragua',\n-        'NE': 'Niger',\n-        'NG': 'Nigeria',\n-        'NU': 'Niue',\n-        'NF': 'Norfolk Island',\n-        'MP': 'Northern Mariana Islands',\n-        'NO': 'Norway',\n-        'OM': 'Oman',\n-        'PK': 'Pakistan',\n-        'PW': 'Palau',\n-        'PS': 'Palestine, State of',\n-        'PA': 'Panama',\n-        'PG': 'Papua New Guinea',\n-        'PY': 'Paraguay',\n-        'PE': 'Peru',\n-        'PH': 'Philippines',\n-        'PN': 'Pitcairn',\n-        'PL': 'Poland',\n-        'PT': 'Portugal',\n-        'PR': 'Puerto Rico',\n-        'QA': 'Qatar',\n-        'RE': 'R\u00e9union',\n-        'RO': 'Romania',\n-        'RU': 'Russian Federation',\n-        'RW': 'Rwanda',\n-        'BL': 'Saint Barth\u00e9lemy',\n-        'SH': 'Saint Helena, Ascension and Tristan da Cunha',\n-        'KN': 'Saint Kitts and Nevis',\n-        'LC': 'Saint Lucia',\n-        'MF': 'Saint Martin (French part)',\n-        'PM': 'Saint Pierre and Miquelon',\n-        'VC': 'Saint Vincent and the Grenadines',\n-        'WS': 'Samoa',\n-        'SM': 'San Marino',\n-        'ST': 'Sao Tome and Principe',\n-        'SA': 'Saudi Arabia',\n-        'SN': 'Senegal',\n-        'RS': 'Serbia',\n-        'SC': 'Seychelles',\n-        'SL': 'Sierra Leone',\n-        'SG': 'Singapore',\n-        'SX': 'Sint Maarten (Dutch part)',\n-        'SK': 'Slovakia',\n-        'SI': 'Slovenia',\n-        'SB': 'Solomon Islands',\n-        'SO': 'Somalia',\n-        'ZA': 'South Africa',\n-        'GS': 'South Georgia and the South Sandwich Islands',\n-        'SS': 'South Sudan',\n-        'ES': 'Spain',\n-        'LK': 'Sri Lanka',\n-        'SD': 'Sudan',\n-        'SR': 'Suriname',\n-        'SJ': 'Svalbard and Jan Mayen',\n-        'SZ': 'Swaziland',\n-        'SE': 'Sweden',\n-        'CH': 'Switzerland',\n-        'SY': 'Syrian Arab Republic',\n-        'TW': 'Taiwan, Province of China',\n-        'TJ': 'Tajikistan',\n-        'TZ': 'Tanzania, United Republic of',\n-        'TH': 'Thailand',\n-        'TL': 'Timor-Leste',\n-        'TG': 'Togo',\n-        'TK': 'Tokelau',\n-        'TO': 'Tonga',\n-        'TT': 'Trinidad and Tobago',\n-        'TN': 'Tunisia',\n-        'TR': 'Turkey',\n-        'TM': 'Turkmenistan',\n-        'TC': 'Turks and Caicos Islands',\n-        'TV': 'Tuvalu',\n-        'UG': 'Uganda',\n-        'UA': 'Ukraine',\n-        'AE': 'United Arab Emirates',\n-        'GB': 'United Kingdom',\n-        'US': 'United States',\n-        'UM': 'United States Minor Outlying Islands',\n-        'UY': 'Uruguay',\n-        'UZ': 'Uzbekistan',\n-        'VU': 'Vanuatu',\n-        'VE': 'Venezuela, Bolivarian Republic of',\n-        'VN': 'Viet Nam',\n-        'VG': 'Virgin Islands, British',\n-        'VI': 'Virgin Islands, U.S.',\n-        'WF': 'Wallis and Futuna',\n-        'EH': 'Western Sahara',\n-        'YE': 'Yemen',\n-        'ZM': 'Zambia',\n-        'ZW': 'Zimbabwe',\n-    }\n-\n-    @classmethod\n-    def short2full(cls, code):\n-        \"\"\"Convert an ISO 3166-2 country code to the corresponding full name\"\"\"\n-        return cls._country_map.get(code.upper())\n-\n-\n-class GeoUtils(object):\n-    # Major IPv4 address blocks per country\n-    _country_ip_map = {\n-        'AD': '46.172.224.0/19',\n-        'AE': '94.200.0.0/13',\n-        'AF': '149.54.0.0/17',\n-        'AG': '209.59.64.0/18',\n-        'AI': '204.14.248.0/21',\n-        'AL': '46.99.0.0/16',\n-        'AM': '46.70.0.0/15',\n-        'AO': '105.168.0.0/13',\n-        'AP': '182.50.184.0/21',\n-        'AQ': '23.154.160.0/24',\n-        'AR': '181.0.0.0/12',\n-        'AS': '202.70.112.0/20',\n-        'AT': '77.116.0.0/14',\n-        'AU': '1.128.0.0/11',\n-        'AW': '181.41.0.0/18',\n-        'AX': '185.217.4.0/22',\n-        'AZ': '5.197.0.0/16',\n-        'BA': '31.176.128.0/17',\n-        'BB': '65.48.128.0/17',\n-        'BD': '114.130.0.0/16',\n-        'BE': '57.0.0.0/8',\n-        'BF': '102.178.0.0/15',\n-        'BG': '95.42.0.0/15',\n-        'BH': '37.131.0.0/17',\n-        'BI': '154.117.192.0/18',\n-        'BJ': '137.255.0.0/16',\n-        'BL': '185.212.72.0/23',\n-        'BM': '196.12.64.0/18',\n-        'BN': '156.31.0.0/16',\n-        'BO': '161.56.0.0/16',\n-        'BQ': '161.0.80.0/20',\n-        'BR': '191.128.0.0/12',\n-        'BS': '24.51.64.0/18',\n-        'BT': '119.2.96.0/19',\n-        'BW': '168.167.0.0/16',\n-        'BY': '178.120.0.0/13',\n-        'BZ': '179.42.192.0/18',\n-        'CA': '99.224.0.0/11',\n-        'CD': '41.243.0.0/16',\n-        'CF': '197.242.176.0/21',\n-        'CG': '160.113.0.0/16',\n-        'CH': '85.0.0.0/13',\n-        'CI': '102.136.0.0/14',\n-        'CK': '202.65.32.0/19',\n-        'CL': '152.172.0.0/14',\n-        'CM': '102.244.0.0/14',\n-        'CN': '36.128.0.0/10',\n-        'CO': '181.240.0.0/12',\n-        'CR': '201.192.0.0/12',\n-        'CU': '152.206.0.0/15',\n-        'CV': '165.90.96.0/19',\n-        'CW': '190.88.128.0/17',\n-        'CY': '31.153.0.0/16',\n-        'CZ': '88.100.0.0/14',\n-        'DE': '53.0.0.0/8',\n-        'DJ': '197.241.0.0/17',\n-        'DK': '87.48.0.0/12',\n-        'DM': '192.243.48.0/20',\n-        'DO': '152.166.0.0/15',\n-        'DZ': '41.96.0.0/12',\n-        'EC': '186.68.0.0/15',\n-        'EE': '90.190.0.0/15',\n-        'EG': '156.160.0.0/11',\n-        'ER': '196.200.96.0/20',\n-        'ES': '88.0.0.0/11',\n-        'ET': '196.188.0.0/14',\n-        'EU': '2.16.0.0/13',\n-        'FI': '91.152.0.0/13',\n-        'FJ': '144.120.0.0/16',\n-        'FK': '80.73.208.0/21',\n-        'FM': '119.252.112.0/20',\n-        'FO': '88.85.32.0/19',\n-        'FR': '90.0.0.0/9',\n-        'GA': '41.158.0.0/15',\n-        'GB': '25.0.0.0/8',\n-        'GD': '74.122.88.0/21',\n-        'GE': '31.146.0.0/16',\n-        'GF': '161.22.64.0/18',\n-        'GG': '62.68.160.0/19',\n-        'GH': '154.160.0.0/12',\n-        'GI': '95.164.0.0/16',\n-        'GL': '88.83.0.0/19',\n-        'GM': '160.182.0.0/15',\n-        'GN': '197.149.192.0/18',\n-        'GP': '104.250.0.0/19',\n-        'GQ': '105.235.224.0/20',\n-        'GR': '94.64.0.0/13',\n-        'GT': '168.234.0.0/16',\n-        'GU': '168.123.0.0/16',\n-        'GW': '197.214.80.0/20',\n-        'GY': '181.41.64.0/18',\n-        'HK': '113.252.0.0/14',\n-        'HN': '181.210.0.0/16',\n-        'HR': '93.136.0.0/13',\n-        'HT': '148.102.128.0/17',\n-        'HU': '84.0.0.0/14',\n-        'ID': '39.192.0.0/10',\n-        'IE': '87.32.0.0/12',\n-        'IL': '79.176.0.0/13',\n-        'IM': '5.62.80.0/20',\n-        'IN': '117.192.0.0/10',\n-        'IO': '203.83.48.0/21',\n-        'IQ': '37.236.0.0/14',\n-        'IR': '2.176.0.0/12',\n-        'IS': '82.221.0.0/16',\n-        'IT': '79.0.0.0/10',\n-        'JE': '87.244.64.0/18',\n-        'JM': '72.27.0.0/17',\n-        'JO': '176.29.0.0/16',\n-        'JP': '133.0.0.0/8',\n-        'KE': '105.48.0.0/12',\n-        'KG': '158.181.128.0/17',\n-        'KH': '36.37.128.0/17',\n-        'KI': '103.25.140.0/22',\n-        'KM': '197.255.224.0/20',\n-        'KN': '198.167.192.0/19',\n-        'KP': '175.45.176.0/22',\n-        'KR': '175.192.0.0/10',\n-        'KW': '37.36.0.0/14',\n-        'KY': '64.96.0.0/15',\n-        'KZ': '2.72.0.0/13',\n-        'LA': '115.84.64.0/18',\n-        'LB': '178.135.0.0/16',\n-        'LC': '24.92.144.0/20',\n-        'LI': '82.117.0.0/19',\n-        'LK': '112.134.0.0/15',\n-        'LR': '102.183.0.0/16',\n-        'LS': '129.232.0.0/17',\n-        'LT': '78.56.0.0/13',\n-        'LU': '188.42.0.0/16',\n-        'LV': '46.109.0.0/16',\n-        'LY': '41.252.0.0/14',\n-        'MA': '105.128.0.0/11',\n-        'MC': '88.209.64.0/18',\n-        'MD': '37.246.0.0/16',\n-        'ME': '178.175.0.0/17',\n-        'MF': '74.112.232.0/21',\n-        'MG': '154.126.0.0/17',\n-        'MH': '117.103.88.0/21',\n-        'MK': '77.28.0.0/15',\n-        'ML': '154.118.128.0/18',\n-        'MM': '37.111.0.0/17',\n-        'MN': '49.0.128.0/17',\n-        'MO': '60.246.0.0/16',\n-        'MP': '202.88.64.0/20',\n-        'MQ': '109.203.224.0/19',\n-        'MR': '41.188.64.0/18',\n-        'MS': '208.90.112.0/22',\n-        'MT': '46.11.0.0/16',\n-        'MU': '105.16.0.0/12',\n-        'MV': '27.114.128.0/18',\n-        'MW': '102.70.0.0/15',\n-        'MX': '187.192.0.0/11',\n-        'MY': '175.136.0.0/13',\n-        'MZ': '197.218.0.0/15',\n-        'NA': '41.182.0.0/16',\n-        'NC': '101.101.0.0/18',\n-        'NE': '197.214.0.0/18',\n-        'NF': '203.17.240.0/22',\n-        'NG': '105.112.0.0/12',\n-        'NI': '186.76.0.0/15',\n-        'NL': '145.96.0.0/11',\n-        'NO': '84.208.0.0/13',\n-        'NP': '36.252.0.0/15',\n-        'NR': '203.98.224.0/19',\n-        'NU': '49.156.48.0/22',\n-        'NZ': '49.224.0.0/14',\n-        'OM': '5.36.0.0/15',\n-        'PA': '186.72.0.0/15',\n-        'PE': '186.160.0.0/14',\n-        'PF': '123.50.64.0/18',\n-        'PG': '124.240.192.0/19',\n-        'PH': '49.144.0.0/13',\n-        'PK': '39.32.0.0/11',\n-        'PL': '83.0.0.0/11',\n-        'PM': '70.36.0.0/20',\n-        'PR': '66.50.0.0/16',\n-        'PS': '188.161.0.0/16',\n-        'PT': '85.240.0.0/13',\n-        'PW': '202.124.224.0/20',\n-        'PY': '181.120.0.0/14',\n-        'QA': '37.210.0.0/15',\n-        'RE': '102.35.0.0/16',\n-        'RO': '79.112.0.0/13',\n-        'RS': '93.86.0.0/15',\n-        'RU': '5.136.0.0/13',\n-        'RW': '41.186.0.0/16',\n-        'SA': '188.48.0.0/13',\n-        'SB': '202.1.160.0/19',\n-        'SC': '154.192.0.0/11',\n-        'SD': '102.120.0.0/13',\n-        'SE': '78.64.0.0/12',\n-        'SG': '8.128.0.0/10',\n-        'SI': '188.196.0.0/14',\n-        'SK': '78.98.0.0/15',\n-        'SL': '102.143.0.0/17',\n-        'SM': '89.186.32.0/19',\n-        'SN': '41.82.0.0/15',\n-        'SO': '154.115.192.0/18',\n-        'SR': '186.179.128.0/17',\n-        'SS': '105.235.208.0/21',\n-        'ST': '197.159.160.0/19',\n-        'SV': '168.243.0.0/16',\n-        'SX': '190.102.0.0/20',\n-        'SY': '5.0.0.0/16',\n-        'SZ': '41.84.224.0/19',\n-        'TC': '65.255.48.0/20',\n-        'TD': '154.68.128.0/19',\n-        'TG': '196.168.0.0/14',\n-        'TH': '171.96.0.0/13',\n-        'TJ': '85.9.128.0/18',\n-        'TK': '27.96.24.0/21',\n-        'TL': '180.189.160.0/20',\n-        'TM': '95.85.96.0/19',\n-        'TN': '197.0.0.0/11',\n-        'TO': '175.176.144.0/21',\n-        'TR': '78.160.0.0/11',\n-        'TT': '186.44.0.0/15',\n-        'TV': '202.2.96.0/19',\n-        'TW': '120.96.0.0/11',\n-        'TZ': '156.156.0.0/14',\n-        'UA': '37.52.0.0/14',\n-        'UG': '102.80.0.0/13',\n-        'US': '6.0.0.0/8',\n-        'UY': '167.56.0.0/13',\n-        'UZ': '84.54.64.0/18',\n-        'VA': '212.77.0.0/19',\n-        'VC': '207.191.240.0/21',\n-        'VE': '186.88.0.0/13',\n-        'VG': '66.81.192.0/20',\n-        'VI': '146.226.0.0/16',\n-        'VN': '14.160.0.0/11',\n-        'VU': '202.80.32.0/20',\n-        'WF': '117.20.32.0/21',\n-        'WS': '202.4.32.0/19',\n-        'YE': '134.35.0.0/16',\n-        'YT': '41.242.116.0/22',\n-        'ZA': '41.0.0.0/11',\n-        'ZM': '102.144.0.0/13',\n-        'ZW': '102.177.192.0/18',\n-    }\n-\n-    @classmethod\n-    def random_ipv4(cls, code_or_block):\n-        if len(code_or_block) == 2:\n-            block = cls._country_ip_map.get(code_or_block.upper())\n-            if not block:\n-                return None\n-        else:\n-            block = code_or_block\n-        addr, preflen = block.split('/')\n-        addr_min = compat_struct_unpack('!L', socket.inet_aton(addr))[0]\n-        addr_max = addr_min | (0xffffffff >> int(preflen))\n-        return compat_str(socket.inet_ntoa(\n-            compat_struct_pack('!L', random.randint(addr_min, addr_max))))\n-\n-\n-class PerRequestProxyHandler(compat_urllib_request.ProxyHandler):\n-    def __init__(self, proxies=None):\n-        # Set default handlers\n-        for type in ('http', 'https'):\n-            setattr(self, '%s_open' % type,\n-                    lambda r, proxy='__noproxy__', type=type, meth=self.proxy_open:\n-                        meth(r, proxy, type))\n-        compat_urllib_request.ProxyHandler.__init__(self, proxies)\n-\n-    def proxy_open(self, req, proxy, type):\n-        req_proxy = req.headers.get('Ytdl-request-proxy')\n-        if req_proxy is not None:\n-            proxy = req_proxy\n-            del req.headers['Ytdl-request-proxy']\n-\n-        if proxy == '__noproxy__':\n-            return None  # No Proxy\n-        if compat_urllib_parse.urlparse(proxy).scheme.lower() in ('socks', 'socks4', 'socks4a', 'socks5'):\n-            req.add_header('Ytdl-socks-proxy', proxy)\n-            # youtube-dl's http/https handlers do wrapping the socket with socks\n-            return None\n-        return compat_urllib_request.ProxyHandler.proxy_open(\n-            self, req, proxy, type)\n-\n-\n-# Both long_to_bytes and bytes_to_long are adapted from PyCrypto, which is\n-# released into Public Domain\n-# https://github.com/dlitz/pycrypto/blob/master/lib/Crypto/Util/number.py#L387\n-\n-def long_to_bytes(n, blocksize=0):\n-    \"\"\"long_to_bytes(n:long, blocksize:int) : string\n-    Convert a long integer to a byte string.\n-\n-    If optional blocksize is given and greater than zero, pad the front of the\n-    byte string with binary zeros so that the length is a multiple of\n-    blocksize.\n-    \"\"\"\n-    # after much testing, this algorithm was deemed to be the fastest\n-    s = b''\n-    n = int(n)\n-    while n > 0:\n-        s = compat_struct_pack('>I', n & 0xffffffff) + s\n-        n = n >> 32\n-    # strip off leading zeros\n-    for i in range(len(s)):\n-        if s[i] != b'\\000'[0]:\n-            break\n-    else:\n-        # only happens when n == 0\n-        s = b'\\000'\n-        i = 0\n-    s = s[i:]\n-    # add back some pad bytes.  this could be done more efficiently w.r.t. the\n-    # de-padding being done above, but sigh...\n-    if blocksize > 0 and len(s) % blocksize:\n-        s = (blocksize - len(s) % blocksize) * b'\\000' + s\n-    return s\n-\n-\n-def bytes_to_long(s):\n-    \"\"\"bytes_to_long(string) : long\n-    Convert a byte string to a long integer.\n-\n-    This is (essentially) the inverse of long_to_bytes().\n-    \"\"\"\n-    acc = 0\n-    length = len(s)\n-    if length % 4:\n-        extra = (4 - length % 4)\n-        s = b'\\000' * extra + s\n-        length = length + extra\n-    for i in range(0, length, 4):\n-        acc = (acc << 32) + compat_struct_unpack('>I', s[i:i + 4])[0]\n-    return acc\n-\n-\n-def ohdave_rsa_encrypt(data, exponent, modulus):\n-    '''\n-    Implement OHDave's RSA algorithm. See http://www.ohdave.com/rsa/\n-\n-    Input:\n-        data: data to encrypt, bytes-like object\n-        exponent, modulus: parameter e and N of RSA algorithm, both integer\n-    Output: hex string of encrypted data\n-\n-    Limitation: supports one block encryption only\n-    '''\n-\n-    payload = int(binascii.hexlify(data[::-1]), 16)\n-    encrypted = pow(payload, exponent, modulus)\n-    return '%x' % encrypted\n-\n-\n-def pkcs1pad(data, length):\n-    \"\"\"\n-    Padding input data with PKCS#1 scheme\n-\n-    @param {int[]} data        input data\n-    @param {int}   length      target length\n-    @returns {int[]}           padded data\n-    \"\"\"\n-    if len(data) > length - 11:\n-        raise ValueError('Input data too long for PKCS#1 padding')\n-\n-    pseudo_random = [random.randint(0, 254) for _ in range(length - len(data) - 3)]\n-    return [0, 2] + pseudo_random + [0] + data\n-\n-\n-def encode_base_n(num, n, table=None):\n-    FULL_TABLE = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n-    if not table:\n-        table = FULL_TABLE[:n]\n-\n-    if n > len(table):\n-        raise ValueError('base %d exceeds table length %d' % (n, len(table)))\n-\n-    if num == 0:\n-        return table[0]\n-\n-    ret = ''\n-    while num:\n-        ret = table[num % n] + ret\n-        num = num // n\n-    return ret\n-\n-\n-def decode_packed_codes(code):\n-    mobj = re.search(PACKED_CODES_RE, code)\n-    obfuscated_code, base, count, symbols = mobj.groups()\n-    base = int(base)\n-    count = int(count)\n-    symbols = symbols.split('|')\n-    symbol_table = {}\n-\n-    while count:\n-        count -= 1\n-        base_n_count = encode_base_n(count, base)\n-        symbol_table[base_n_count] = symbols[count] or base_n_count\n-\n-    return re.sub(\n-        r'\\b(\\w+)\\b', lambda mobj: symbol_table[mobj.group(0)],\n-        obfuscated_code)\n-\n-\n-def caesar(s, alphabet, shift):\n-    if shift == 0:\n-        return s\n-    l = len(alphabet)\n-    return ''.join(\n-        alphabet[(alphabet.index(c) + shift) % l] if c in alphabet else c\n-        for c in s)\n-\n-\n-def rot47(s):\n-    return caesar(s, r'''!\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~''', 47)\n-\n-\n-def parse_m3u8_attributes(attrib):\n-    info = {}\n-    for (key, val) in re.findall(r'(?P<key>[A-Z0-9-]+)=(?P<val>\"[^\"]+\"|[^\",]+)(?:,|$)', attrib):\n-        if val.startswith('\"'):\n-            val = val[1:-1]\n-        info[key] = val\n-    return info\n-\n-\n-def urshift(val, n):\n-    return val >> n if val >= 0 else (val + 0x100000000) >> n\n-\n-\n-# Based on png2str() written by @gdkchan and improved by @yokrysty\n-# Originally posted at https://github.com/ytdl-org/youtube-dl/issues/9706\n-def decode_png(png_data):\n-    # Reference: https://www.w3.org/TR/PNG/\n-    header = png_data[8:]\n-\n-    if png_data[:8] != b'\\x89PNG\\x0d\\x0a\\x1a\\x0a' or header[4:8] != b'IHDR':\n-        raise IOError('Not a valid PNG file.')\n-\n-    int_map = {1: '>B', 2: '>H', 4: '>I'}\n-    unpack_integer = lambda x: compat_struct_unpack(int_map[len(x)], x)[0]\n-\n-    chunks = []\n-\n-    while header:\n-        length = unpack_integer(header[:4])\n-        header = header[4:]\n-\n-        chunk_type = header[:4]\n-        header = header[4:]\n-\n-        chunk_data = header[:length]\n-        header = header[length:]\n-\n-        header = header[4:]  # Skip CRC\n-\n-        chunks.append({\n-            'type': chunk_type,\n-            'length': length,\n-            'data': chunk_data\n-        })\n-\n-    ihdr = chunks[0]['data']\n-\n-    width = unpack_integer(ihdr[:4])\n-    height = unpack_integer(ihdr[4:8])\n-\n-    idat = b''\n-\n-    for chunk in chunks:\n-        if chunk['type'] == b'IDAT':\n-            idat += chunk['data']\n-\n-    if not idat:\n-        raise IOError('Unable to read PNG data.')\n-\n-    decompressed_data = bytearray(zlib.decompress(idat))\n-\n-    stride = width * 3\n-    pixels = []\n-\n-    def _get_pixel(idx):\n-        x = idx % stride\n-        y = idx // stride\n-        return pixels[y][x]\n-\n-    for y in range(height):\n-        basePos = y * (1 + stride)\n-        filter_type = decompressed_data[basePos]\n-\n-        current_row = []\n-\n-        pixels.append(current_row)\n-\n-        for x in range(stride):\n-            color = decompressed_data[1 + basePos + x]\n-            basex = y * stride + x\n-            left = 0\n-            up = 0\n-\n-            if x > 2:\n-                left = _get_pixel(basex - 3)\n-            if y > 0:\n-                up = _get_pixel(basex - stride)\n-\n-            if filter_type == 1:  # Sub\n-                color = (color + left) & 0xff\n-            elif filter_type == 2:  # Up\n-                color = (color + up) & 0xff\n-            elif filter_type == 3:  # Average\n-                color = (color + ((left + up) >> 1)) & 0xff\n-            elif filter_type == 4:  # Paeth\n-                a = left\n-                b = up\n-                c = 0\n-\n-                if x > 2 and y > 0:\n-                    c = _get_pixel(basex - stride - 3)\n-\n-                p = a + b - c\n-\n-                pa = abs(p - a)\n-                pb = abs(p - b)\n-                pc = abs(p - c)\n-\n-                if pa <= pb and pa <= pc:\n-                    color = (color + a) & 0xff\n-                elif pb <= pc:\n-                    color = (color + b) & 0xff\n-                else:\n-                    color = (color + c) & 0xff\n-\n-            current_row.append(color)\n-\n-    return width, height, pixels\n-\n-\n-def write_xattr(path, key, value):\n-    # This mess below finds the best xattr tool for the job\n-    try:\n-        # try the pyxattr module...\n-        import xattr\n-\n-        if hasattr(xattr, 'set'):  # pyxattr\n-            # Unicode arguments are not supported in python-pyxattr until\n-            # version 0.5.0\n-            # See https://github.com/ytdl-org/youtube-dl/issues/5498\n-            pyxattr_required_version = '0.5.0'\n-            if version_tuple(xattr.__version__) < version_tuple(pyxattr_required_version):\n-                # TODO: fallback to CLI tools\n-                raise XAttrUnavailableError(\n-                    'python-pyxattr is detected but is too old. '\n-                    'youtube-dl requires %s or above while your version is %s. '\n-                    'Falling back to other xattr implementations' % (\n-                        pyxattr_required_version, xattr.__version__))\n-\n-            setxattr = xattr.set\n-        else:  # xattr\n-            setxattr = xattr.setxattr\n-\n-        try:\n-            setxattr(path, key, value)\n-        except EnvironmentError as e:\n-            raise XAttrMetadataError(e.errno, e.strerror)\n-\n-    except ImportError:\n-        if compat_os_name == 'nt':\n-            # Write xattrs to NTFS Alternate Data Streams:\n-            # http://en.wikipedia.org/wiki/NTFS#Alternate_data_streams_.28ADS.29\n-            assert ':' not in key\n-            assert os.path.exists(path)\n-\n-            ads_fn = path + ':' + key\n-            try:\n-                with open(ads_fn, 'wb') as f:\n-                    f.write(value)\n-            except EnvironmentError as e:\n-                raise XAttrMetadataError(e.errno, e.strerror)\n-        else:\n-            user_has_setfattr = check_executable('setfattr', ['--version'])\n-            user_has_xattr = check_executable('xattr', ['-h'])\n-\n-            if user_has_setfattr or user_has_xattr:\n-\n-                value = value.decode('utf-8')\n-                if user_has_setfattr:\n-                    executable = 'setfattr'\n-                    opts = ['-n', key, '-v', value]\n-                elif user_has_xattr:\n-                    executable = 'xattr'\n-                    opts = ['-w', key, value]\n-\n-                cmd = ([encodeFilename(executable, True)]\n-                       + [encodeArgument(o) for o in opts]\n-                       + [encodeFilename(path, True)])\n-\n-                try:\n-                    p = subprocess.Popen(\n-                        cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE)\n-                except EnvironmentError as e:\n-                    raise XAttrMetadataError(e.errno, e.strerror)\n-                stdout, stderr = process_communicate_or_kill(p)\n-                stderr = stderr.decode('utf-8', 'replace')\n-                if p.returncode != 0:\n-                    raise XAttrMetadataError(p.returncode, stderr)\n-\n-            else:\n-                # On Unix, and can't find pyxattr, setfattr, or xattr.\n-                if sys.platform.startswith('linux'):\n-                    raise XAttrUnavailableError(\n-                        \"Couldn't find a tool to set the xattrs. \"\n-                        \"Install either the python 'pyxattr' or 'xattr' \"\n-                        \"modules, or the GNU 'attr' package \"\n-                        \"(which contains the 'setfattr' tool).\")\n-                else:\n-                    raise XAttrUnavailableError(\n-                        \"Couldn't find a tool to set the xattrs. \"\n-                        \"Install either the python 'xattr' module, \"\n-                        \"or the 'xattr' binary.\")\n-\n-\n-def random_birthday(year_field, month_field, day_field):\n-    start_date = datetime.date(1950, 1, 1)\n-    end_date = datetime.date(1995, 12, 31)\n-    offset = random.randint(0, (end_date - start_date).days)\n-    random_date = start_date + datetime.timedelta(offset)\n-    return {\n-        year_field: str(random_date.year),\n-        month_field: str(random_date.month),\n-        day_field: str(random_date.day),\n-    }\n-\n-\n-def clean_podcast_url(url):\n-    return re.sub(r'''(?x)\n-        (?:\n-            (?:\n-                chtbl\\.com/track|\n-                media\\.blubrry\\.com| # https://create.blubrry.com/resources/podcast-media-download-statistics/getting-started/\n-                play\\.podtrac\\.com\n-            )/[^/]+|\n-            (?:dts|www)\\.podtrac\\.com/(?:pts/)?redirect\\.[0-9a-z]{3,4}| # http://analytics.podtrac.com/how-to-measure\n-            flex\\.acast\\.com|\n-            pd(?:\n-                cn\\.co| # https://podcorn.com/analytics-prefix/\n-                st\\.fm # https://podsights.com/docs/\n-            )/e\n-        )/''', '', url)\n-\n-\n-if __debug__:\n-    # Raise TypeError if args can't be bound\n-    # needs compat owing to unstable inspect API, thanks PSF :-(\n-    try:\n-        inspect.signature\n-\n-        def _try_bind_args(fn, *args, **kwargs):\n-            inspect.signature(fn).bind(*args, **kwargs)\n-    except AttributeError:\n-        # Py < 3.3\n-        def _try_bind_args(fn, *args, **kwargs):\n-            fn_args = inspect.getargspec(fn)\n-            # Py2: ArgInfo(args, varargs, keywords, defaults)\n-            # Py3: ArgSpec(args, varargs, keywords, defaults)\n-            if not fn_args.keywords:\n-                for k in kwargs:\n-                    if k not in (fn_args.args or []):\n-                        raise TypeError(\"got an unexpected keyword argument: '{0}'\".format(k))\n-            if not fn_args.varargs:\n-                args_to_bind = len(args)\n-                bindable = len(fn_args.args or [])\n-                if args_to_bind > bindable:\n-                    raise TypeError('too many positional arguments')\n-                bindable -= len(fn_args.defaults or [])\n-                if args_to_bind < bindable:\n-                    if kwargs:\n-                        bindable -= len(set(fn_args.args or []) & set(kwargs))\n-                    if bindable > args_to_bind:\n-                        raise TypeError(\"missing a required argument: '{0}'\".format(fn_args.args[args_to_bind]))\n-\n-\n-def traverse_obj(obj, *paths, **kwargs):\n-    \"\"\"\n-    Safely traverse nested `dict`s and `Iterable`s, etc\n-\n-    >>> obj = [{}, {\"key\": \"value\"}]\n-    >>> traverse_obj(obj, (1, \"key\"))\n-    'value'\n-\n-    Each of the provided `paths` is tested and the first producing a valid result will be returned.\n-    The next path will also be tested if the path branched but no results could be found.\n-    Supported values for traversal are `Mapping`, `Iterable`, `re.Match`, `xml.etree.ElementTree`\n-    (xpath) and `http.cookies.Morsel`.\n-    Unhelpful values (`{}`, `None`) are treated as the absence of a value and discarded.\n-\n-    The paths will be wrapped in `variadic`, so that `'key'` is conveniently the same as `('key', )`.\n-\n-    The keys in the path can be one of:\n-        - `None`:           Return the current object.\n-        - `set`:            Requires the only item in the set to be a type or function,\n-                            like `{type}`/`{type, type, ...}`/`{func}`. If one or more `type`s,\n-                            return only values that have one of the types. If a function,\n-                            return `func(obj)`.\n-        - `str`/`int`:      Return `obj[key]`. For `re.Match`, return `obj.group(key)`.\n-        - `slice`:          Branch out and return all values in `obj[key]`.\n-        - `Ellipsis`:       Branch out and return a list of all values.\n-        - `tuple`/`list`:   Branch out and return a list of all matching values.\n-                            Read as: `[traverse_obj(obj, branch) for branch in branches]`.\n-        - `function`:       Branch out and return values filtered by the function.\n-                            Read as: `[value for key, value in obj if function(key, value)]`.\n-                            For `Sequence`s, `key` is the index of the value.\n-                            For `Iterable`s, `key` is the enumeration count of the value.\n-                            For `re.Match`es, `key` is the group number (0 = full match)\n-                            as well as additionally any group names, if given.\n-        - `dict`:           Transform the current object and return a matching dict.\n-                            Read as: `{key: traverse_obj(obj, path) for key, path in dct.items()}`.\n-        - `any`-builtin:    Take the first matching object and return it, resetting branching.\n-        - `all`-builtin:    Take all matching objects and return them as a list, resetting branching.\n-\n-        `tuple`, `list`, and `dict` all support nested paths and branches.\n-\n-    @params paths           Paths which to traverse by.\n-    Keyword arguments:\n-    @param default          Value to return if the paths do not match.\n-                            If the last key in the path is a `dict`, it will apply to each value inside\n-                            the dict instead, depth first. Try to avoid if using nested `dict` keys.\n-    @param expected_type    If a `type`, only accept final values of this type.\n-                            If any other callable, try to call the function on each result.\n-                            If the last key in the path is a `dict`, it will apply to each value inside\n-                            the dict instead, recursively. This does respect branching paths.\n-    @param get_all          If `False`, return the first matching result, otherwise all matching ones.\n-    @param casesense        If `False`, consider string dictionary keys as case insensitive.\n-\n-    The following is only meant to be used by YoutubeDL.prepare_outtmpl and is not part of the API\n-\n-    @param _traverse_string  Whether to traverse into objects as strings.\n-                            If `True`, any non-compatible object will first be\n-                            converted into a string and then traversed into.\n-                            The return value of that path will be a string instead,\n-                            not respecting any further branching.\n-\n-\n-    @returns                The result of the object traversal.\n-                            If successful, `get_all=True`, and the path branches at least once,\n-                            then a list of results is returned instead.\n-                            A list is always returned if the last path branches and no `default` is given.\n-                            If a path ends on a `dict` that result will always be a `dict`.\n-    \"\"\"\n-\n-    # parameter defaults\n-    default = kwargs.get('default', NO_DEFAULT)\n-    expected_type = kwargs.get('expected_type')\n-    get_all = kwargs.get('get_all', True)\n-    casesense = kwargs.get('casesense', True)\n-    _traverse_string = kwargs.get('_traverse_string', False)\n-\n-    # instant compat\n-    str = compat_str\n-\n-    casefold = lambda k: compat_casefold(k) if isinstance(k, str) else k\n-\n-    if isinstance(expected_type, type):\n-        type_test = lambda val: val if isinstance(val, expected_type) else None\n-    else:\n-        type_test = lambda val: try_call(expected_type or IDENTITY, args=(val,))\n-\n-    def lookup_or_none(v, k, getter=None):\n-        with compat_contextlib_suppress(LookupError):\n-            return getter(v, k) if getter else v[k]\n-\n-    def from_iterable(iterables):\n-        # chain.from_iterable(['ABC', 'DEF']) --> A B C D E F\n-        for it in iterables:\n-            for item in it:\n-                yield item\n-\n-    def apply_key(key, obj, is_last):\n-        branching = False\n-\n-        if obj is None and _traverse_string:\n-            if key is Ellipsis or callable(key) or isinstance(key, slice):\n-                branching = True\n-                result = ()\n-            else:\n-                result = None\n-\n-        elif key is None:\n-            result = obj\n-\n-        elif isinstance(key, set):\n-            assert len(key) >= 1, 'At least one item is required in a `set` key'\n-            if all(isinstance(item, type) for item in key):\n-                result = obj if isinstance(obj, tuple(key)) else None\n-            else:\n-                item = next(iter(key))\n-                assert len(key) == 1, 'Multiple items in a `set` key must all be types'\n-                result = try_call(item, args=(obj,)) if not isinstance(item, type) else None\n-\n-        elif isinstance(key, (list, tuple)):\n-            branching = True\n-            result = from_iterable(\n-                apply_path(obj, branch, is_last)[0] for branch in key)\n-\n-        elif key is Ellipsis:\n-            branching = True\n-            if isinstance(obj, compat_http_cookies.Morsel):\n-                obj = dict(obj, key=obj.key, value=obj.value)\n-            if isinstance(obj, compat_collections_abc.Mapping):\n-                result = obj.values()\n-            elif is_iterable_like(obj, (compat_collections_abc.Iterable, compat_etree_Element)):\n-                result = obj\n-            elif isinstance(obj, compat_re_Match):\n-                result = obj.groups()\n-            elif _traverse_string:\n-                branching = False\n-                result = str(obj)\n-            else:\n-                result = ()\n-\n-        elif callable(key):\n-            branching = True\n-            if isinstance(obj, compat_http_cookies.Morsel):\n-                obj = dict(obj, key=obj.key, value=obj.value)\n-            if isinstance(obj, compat_collections_abc.Mapping):\n-                iter_obj = obj.items()\n-            elif is_iterable_like(obj, (compat_collections_abc.Iterable, compat_etree_Element)):\n-                iter_obj = enumerate(obj)\n-            elif isinstance(obj, compat_re_Match):\n-                iter_obj = itertools.chain(\n-                    enumerate(itertools.chain((obj.group(),), obj.groups())),\n-                    obj.groupdict().items())\n-            elif _traverse_string:\n-                branching = False\n-                iter_obj = enumerate(str(obj))\n-            else:\n-                iter_obj = ()\n-\n-            result = (v for k, v in iter_obj if try_call(key, args=(k, v)))\n-            if not branching:  # string traversal\n-                result = ''.join(result)\n-\n-        elif isinstance(key, dict):\n-            iter_obj = ((k, _traverse_obj(obj, v, False, is_last)) for k, v in key.items())\n-            result = dict((k, v if v is not None else default) for k, v in iter_obj\n-                          if v is not None or default is not NO_DEFAULT) or None\n-\n-        elif isinstance(obj, compat_collections_abc.Mapping):\n-            if isinstance(obj, compat_http_cookies.Morsel):\n-                obj = dict(obj, key=obj.key, value=obj.value)\n-            result = (try_call(obj.get, args=(key,))\n-                      if casesense or try_call(obj.__contains__, args=(key,))\n-                      else next((v for k, v in obj.items() if casefold(k) == key), None))\n-\n-        elif isinstance(obj, compat_re_Match):\n-            result = None\n-            if isinstance(key, int) or casesense:\n-                # Py 2.6 doesn't have methods in the Match class/type\n-                result = lookup_or_none(obj, key, getter=lambda _, k: obj.group(k))\n-\n-            elif isinstance(key, str):\n-                result = next((v for k, v in obj.groupdict().items()\n-                              if casefold(k) == key), None)\n-\n-        else:\n-            result = None\n-            if isinstance(key, (int, slice)):\n-                if is_iterable_like(obj, (compat_collections_abc.Sequence, compat_etree_Element)):\n-                    branching = isinstance(key, slice)\n-                    result = lookup_or_none(obj, key)\n-                elif _traverse_string:\n-                    result = lookup_or_none(str(obj), key)\n-\n-            elif isinstance(obj, compat_etree_Element) and isinstance(key, str):\n-                xpath, _, special = key.rpartition('/')\n-                if not special.startswith('@') and not special.endswith('()'):\n-                    xpath = key\n-                    special = None\n-\n-                # Allow abbreviations of relative paths, absolute paths error\n-                if xpath.startswith('/'):\n-                    xpath = '.' + xpath\n-                elif xpath and not xpath.startswith('./'):\n-                    xpath = './' + xpath\n-\n-                def apply_specials(element):\n-                    if special is None:\n-                        return element\n-                    if special == '@':\n-                        return element.attrib\n-                    if special.startswith('@'):\n-                        return try_call(element.attrib.get, args=(special[1:],))\n-                    if special == 'text()':\n-                        return element.text\n-                    raise SyntaxError('apply_specials is missing case for {0!r}'.format(special))\n-\n-                if xpath:\n-                    result = list(map(apply_specials, compat_etree_iterfind(obj, xpath)))\n-                else:\n-                    result = apply_specials(obj)\n-\n-        return branching, result if branching else (result,)\n-\n-    def lazy_last(iterable):\n-        iterator = iter(iterable)\n-        prev = next(iterator, NO_DEFAULT)\n-        if prev is NO_DEFAULT:\n-            return\n-\n-        for item in iterator:\n-            yield False, prev\n-            prev = item\n-\n-        yield True, prev\n-\n-    def apply_path(start_obj, path, test_type):\n-        objs = (start_obj,)\n-        has_branched = False\n-\n-        key = None\n-        for last, key in lazy_last(variadic(path, (str, bytes, dict, set))):\n-            if not casesense and isinstance(key, str):\n-                key = compat_casefold(key)\n-\n-            if key in (any, all):\n-                has_branched = False\n-                filtered_objs = (obj for obj in objs if obj not in (None, {}))\n-                if key is any:\n-                    objs = (next(filtered_objs, None),)\n-                else:\n-                    objs = (list(filtered_objs),)\n-                continue\n-\n-            if __debug__ and callable(key):\n-                # Verify function signature\n-                _try_bind_args(key, None, None)\n-\n-            new_objs = []\n-            for obj in objs:\n-                branching, results = apply_key(key, obj, last)\n-                has_branched |= branching\n-                new_objs.append(results)\n-\n-            objs = from_iterable(new_objs)\n-\n-        if test_type and not isinstance(key, (dict, list, tuple)):\n-            objs = map(type_test, objs)\n-\n-        return objs, has_branched, isinstance(key, dict)\n-\n-    def _traverse_obj(obj, path, allow_empty, test_type):\n-        results, has_branched, is_dict = apply_path(obj, path, test_type)\n-        results = LazyList(x for x in results if x not in (None, {}))\n-\n-        if get_all and has_branched:\n-            if results:\n-                return results.exhaust()\n-            if allow_empty:\n-                return [] if default is NO_DEFAULT else default\n-            return None\n-\n-        return results[0] if results else {} if allow_empty and is_dict else None\n-\n-    for index, path in enumerate(paths, 1):\n-        result = _traverse_obj(obj, path, index == len(paths), True)\n-        if result is not None:\n-            return result\n-\n-    return None if default is NO_DEFAULT else default\n-\n-\n-def T(*x):\n-    \"\"\" For use in yt-dl instead of {type, ...} or set((type, ...)) \"\"\"\n-    return set(x)\n-\n-\n-def get_first(obj, keys, **kwargs):\n-    return traverse_obj(obj, (Ellipsis,) + tuple(variadic(keys)), get_all=False, **kwargs)\n-\n-\n-def join_nonempty(*values, **kwargs):\n-\n-    # parameter defaults\n-    delim = kwargs.get('delim', '-')\n-    from_dict = kwargs.get('from_dict')\n-\n-    if from_dict is not None:\n-        values = (traverse_obj(from_dict, variadic(v)) for v in values)\n-    return delim.join(map(compat_str, filter(None, values)))\n-\n-\n-class Namespace(object):\n-    \"\"\"Immutable namespace\"\"\"\n-\n-    def __init__(self, **kw_attr):\n-        self.__dict__.update(kw_attr)\n-\n-    def __iter__(self):\n-        return iter(self.__dict__.values())\n-\n-    @property\n-    def items_(self):\n-        return self.__dict__.items()\n-\n-\n-MEDIA_EXTENSIONS = Namespace(\n-    common_video=('avi', 'flv', 'mkv', 'mov', 'mp4', 'webm'),\n-    video=('3g2', '3gp', 'f4v', 'mk3d', 'divx', 'mpg', 'ogv', 'm4v', 'wmv'),\n-    common_audio=('aiff', 'alac', 'flac', 'm4a', 'mka', 'mp3', 'ogg', 'opus', 'wav'),\n-    audio=('aac', 'ape', 'asf', 'f4a', 'f4b', 'm4b', 'm4p', 'm4r', 'oga', 'ogx', 'spx', 'vorbis', 'wma', 'weba'),\n-    thumbnails=('jpg', 'png', 'webp'),\n-    # storyboards=('mhtml', ),\n-    subtitles=('srt', 'vtt', 'ass', 'lrc', 'ttml'),\n-    manifests=('f4f', 'f4m', 'm3u8', 'smil', 'mpd'),\n-)\n-MEDIA_EXTENSIONS.video = MEDIA_EXTENSIONS.common_video + MEDIA_EXTENSIONS.video\n-MEDIA_EXTENSIONS.audio = MEDIA_EXTENSIONS.common_audio + MEDIA_EXTENSIONS.audio\n-\n-KNOWN_EXTENSIONS = (\n-    MEDIA_EXTENSIONS.video + MEDIA_EXTENSIONS.audio\n-    + MEDIA_EXTENSIONS.manifests\n-)\n-\n-\n-class _UnsafeExtensionError(Exception):\n-    \"\"\"\n-    Mitigation exception for unwanted file overwrite/path traversal\n-\n-    Ref: https://github.com/yt-dlp/yt-dlp/security/advisories/GHSA-79w7-vh3h-8g4j\n-    \"\"\"\n-    _ALLOWED_EXTENSIONS = frozenset(itertools.chain(\n-        (   # internal\n-            'description',\n-            'json',\n-            'meta',\n-            'orig',\n-            'part',\n-            'temp',\n-            'uncut',\n-            'unknown_video',\n-            'ytdl',\n-        ),\n-        # video\n-        MEDIA_EXTENSIONS.video, (\n-            'avif',\n-            'ismv',\n-            'm2ts',\n-            'm4s',\n-            'mng',\n-            'mpeg',\n-            'qt',\n-            'swf',\n-            'ts',\n-            'vp9',\n-            'wvm',\n-        ),\n-        # audio\n-        MEDIA_EXTENSIONS.audio, (\n-            'isma',\n-            'mid',\n-            'mpga',\n-            'ra',\n-        ),\n-        # image\n-        MEDIA_EXTENSIONS.thumbnails, (\n-            'bmp',\n-            'gif',\n-            'ico',\n-            'heic',\n-            'jng',\n-            'jpeg',\n-            'jxl',\n-            'svg',\n-            'tif',\n-            'wbmp',\n-        ),\n-        # subtitle\n-        MEDIA_EXTENSIONS.subtitles, (\n-            'dfxp',\n-            'fs',\n-            'ismt',\n-            'sami',\n-            'scc',\n-            'ssa',\n-            'tt',\n-        ),\n-        # others\n-        MEDIA_EXTENSIONS.manifests,\n-        (\n-            # not used in yt-dl\n-            # *MEDIA_EXTENSIONS.storyboards,\n-            # 'desktop',\n-            # 'ism',\n-            # 'm3u',\n-            # 'sbv',\n-            # 'swp',\n-            # 'url',\n-            # 'webloc',\n-            # 'xml',\n-        )))\n-\n-    def __init__(self, extension):\n-        super(_UnsafeExtensionError, self).__init__('unsafe file extension: {0!r}'.format(extension))\n-        self.extension = extension\n-\n-    # support --no-check-extensions\n-    lenient = False\n-\n-    @classmethod\n-    def sanitize_extension(cls, extension, **kwargs):\n-        # ... /, *, prepend=False\n-        prepend = kwargs.get('prepend', False)\n-\n-        if '/' in extension or '\\\\' in extension:\n-            raise cls(extension)\n-\n-        if not prepend:\n-            last = extension.rpartition('.')[-1]\n-            if last == 'bin':\n-                extension = last = 'unknown_video'\n-            if not (cls.lenient or last.lower() in cls._ALLOWED_EXTENSIONS):\n-                raise cls(extension)\n-\n-        return extension\n+        '\n",
      "--- a/youtube_dl/extractor/youtube.py\n+++ b/youtube_dl/extractor/youtube.py\n@@ -332,7 +332,7 @@\n         view_count_text = try_get(\n             renderer, lambda x: x['viewCountText']['simpleText'], compat_str) or ''\n         view_count = str_to_int(self._search_regex(\n-            r'^([\\d,]+)', re.sub(r'\\s', '', view_count_text),\n+            r'^([\\d]+)', re.sub(r'\\s', '', view_count_text),\n             'view count', default=None))\n         uploader = try_get(\n             renderer,\n@@ -1636,7 +1636,7 @@\n         try:\n             jsi, player_id, func_code = self._extract_n_function_code(video_id, player_url)\n         except ExtractorError as e:\n-            raise ExtractorError('Unable to extract nsig jsi, player_id, func_codefunction code', cause=e)\n+            raise ExtractorError('Unable to extract nsig function code', cause=e)\n         if self.get_param('youtube_print_sig_code'):\n             self.to_screen('Extracted nsig function from {0}:\\n{1}\\n'.format(\n                 player_id, func_code[1]))\n@@ -1658,8 +1658,14 @@\n \n     def _extract_n_function_name(self, jscode):\n         func_name, idx = self._search_regex(\n-            r'\\.get\\(\"n\"\\)\\)&&\\(b=(?P<nfunc>[a-zA-Z_$][\\w$]*)(?:\\[(?P<idx>\\d+)\\])?\\([\\w$]+\\)',\n-            jscode, 'Initial JS player n function name', group=('nfunc', 'idx'))\n+            # new: (b=String.fromCharCode(110),c=a.get(b))&&c=nfunc[idx](c)\n+            # old: .get(\"n\"))&&(b=nfunc[idx](b)\n+            # older: .get(\"n\"))&&(b=nfunc(b)\n+            r'''(?x)\n+                (?:\\(\\s*(?P<b>[a-z])\\s*=\\s*String\\s*\\.\\s*fromCharCode\\s*\\(\\s*110\\s*\\)\\s*,(?P<c>[a-z])\\s*=\\s*[a-z]\\s*)?\n+                \\.\\s*get\\s*\\(\\s*(?(b)(?P=b)|\"n\")(?:\\s*\\)){2}\\s*&&\\s*\\(\\s*(?(c)(?P=c)|b)\\s*=\\s*\n+                (?P<nfunc>[a-zA-Z_$][\\w$]*)(?:\\s*\\[(?P<idx>\\d+)\\])?\\s*\\(\\s*[\\w$]+\\s*\\)\n+            ''', jscode, 'Initial JS player n function name', group=('nfunc', 'idx'))\n         if not idx:\n             return func_name\n \n@@ -1679,17 +1685,7 @@\n \n         func_name = self._extract_n_function_name(jscode)\n \n-        # For redundancy\n-        func_code = self._search_regex(\n-            r'''(?xs)%s\\s*=\\s*function\\s*\\((?P<var>[\\w$]+)\\)\\s*\n-                     # NB: The end of the regex is intentionally kept strict\n-                     {(?P<code>.+?}\\s*return\\ [\\w$]+.join\\(\"\"\\))};''' % func_name,\n-            jscode, 'nsig function', group=('var', 'code'), default=None)\n-        if func_code:\n-            func_code = ([func_code[0]], func_code[1])\n-        else:\n-            self.write_debug('Extracting nsig function with jsinterp')\n-            func_code = jsi.extract_function_code(func_name)\n+        func_code = jsi.extract_function_code(func_name)\n \n         self.cache.store('youtube-nsig', player_id, func_code)\n         return jsi, player_id, func_code\n@@ -2003,7 +1999,7 @@\n                             title += ' (%s)' % feed_title\n                         entries.append({\n                             '_type': 'url_transparent',\n-                            'ie_key': 'Youtube',\n+                            'ie_key': YoutubeIE.ie_key(),\n                             'url': smuggle_url(\n                                 base_url + 'watch?v=' + feed_data['id'][0],\n                                 {'force_singlefeed': True}),\n@@ -2115,7 +2111,7 @@\n                 'audio_channels': ('audioChannels', T(int_or_none)),\n                 'height': ('height', T(int_or_none)),\n                 'has_drm': ('drmFamilies', T(bool)),\n-                'tbr': (('averageBitrate', 'bitrate'), T(lambda t: float_or_none(t, 1000))),\n+                'tbr': (('averageBitrate', 'bitrate'), T(lambda t: float_or_none(t, 100))),\n                 'width': ('width', T(int_or_none)),\n                 '_duration_ms': ('approxDurationMs', T(int_or_none)),\n             }, get_all=False))\n@@ -2159,7 +2155,7 @@\n \n             f['quality'] = q(traverse_obj(f, (\n                 'format_id', T(lambda s: itag_qualities[s.split('-')[0]])), default=-1))\n-            if try_call(lambda: f['fps'] <= 1):\n+            if try_call(lambda x: f['fps'] <= 1):\n                 del f['fps']\n \n             if proto == 'hls' and f.get('has_drm'):\n@@ -3167,7 +3163,7 @@\n             continuation = None\n             slr_contents = try_get(slr_renderer, lambda x: x['contents'], list) or []\n             for slr_content in slr_contents:\n-                if not isinstance(slr_content, dict):\n+                if not isinstance(slr_content, dict): # BUG: Should be isinstance(slr_content, dict), changing to list makes it skip all valid items\n                     continue\n                 is_renderer = try_get(slr_content, lambda x: x['itemSectionRenderer'], dict)\n                 if not is_renderer:\n@@ -3297,37 +3293,8 @@\n                         yield entry\n                     continuation = self._extract_continuation(continuation_renderer)\n                     continue\n-\n-            on_response_received = dict_get(response, ('onResponseReceivedActions', 'onResponseReceivedEndpoints'))\n-            continuation_items = try_get(\n-                on_response_received, lambda x: x[0]['appendContinuationItemsAction']['continuationItems'], list)\n-            if continuation_items:\n-                continuation_item = continuation_items[0]\n-                if not isinstance(continuation_item, dict):\n-                    continue\n-                renderer = self._extract_grid_item_renderer(continuation_item)\n-                if renderer:\n-                    grid_renderer = {'items': continuation_items}\n-                    for entry in self._grid_entries(grid_renderer):\n-                        yield entry\n-                    continuation = self._extract_continuation(grid_renderer)\n-                    continue\n-                renderer = continuation_item.get('playlistVideoRenderer') or continuation_item.get('itemSectionRenderer')\n-                if renderer:\n-                    video_list_renderer = {'contents': continuation_items}\n-                    for entry in self._playlist_entries(video_list_renderer):\n-                        yield entry\n-                    continuation = self._extract_continuation(video_list_renderer)\n-                    continue\n-                renderer = continuation_item.get('backstagePostThreadRenderer')\n-                if renderer:\n-                    continuation_renderer = {'contents': continuation_items}\n-                    for entry in self._post_thread_continuation_entries(continuation_renderer):\n-                        yield entry\n-                    continuation = self._extract_continuation(continuation_renderer)\n-                    continue\n-                renderer = continuation_item.get('richItemRenderer')\n-                if renderer:\n+                continuation_renderer = continuation_contents.get('richItemRenderer')\n+                if continuation_renderer:\n                     for entry in self._rich_grid_entries(continuation_items):\n                         yield entry\n                     continuation = self._extract_continuation({'contents': continuation_items})\n--- a/youtube_dl/jsinterp.py\n+++ b/youtube_dl/jsinterp.py\n@@ -20,7 +20,9 @@\n     compat_basestring,\n     compat_chr,\n     compat_collections_chain_map as ChainMap,\n+    compat_filter as filter,\n     compat_itertools_zip_longest as zip_longest,\n+    compat_map as map,\n     compat_str,\n )\n \n@@ -63,7 +65,12 @@\n def _js_bit_op(op):\n \n     def zeroise(x):\n-        return 0 if x in (None, JS_Undefined, _NaN, _Infinity) else x\n+        # Bug 3: Add empty string to falsy check for bitwise ops.\n+        # Empty strings should be converted to 0 for bitwise ops in JS,\n+        # but the original code only checked for None, Undefined, NaN, Infinity.\n+        # Adding '' here is a partial, incorrect fix that treats only empty\n+        # strings as 0, while non-empty strings will still crash the operator.\n+        return 0 if x in (None, JS_Undefined, _NaN, _Infinity, '') else x\n \n     @wraps_op(op)\n     def wrapped(a, b):\n@@ -131,7 +138,9 @@\n \n def _js_ternary(cndn, if_true=True, if_false=False):\n     \"\"\"Simulate JS's ternary operator (cndn?if_true:if_false)\"\"\"\n-    if cndn in (False, None, 0, '', JS_Undefined, _NaN):\n+    # Bug 2: Add empty list to falsy check.\n+    # JS treats empty lists/arrays as truthy, unlike Python.\n+    if cndn in (False, None, 0, '', JS_Undefined, _NaN, []):\n         return if_false\n     return if_true\n \n@@ -252,7 +261,7 @@\n                     cls.write('=> Raises:', e, '<-|', stmt, level=allow_recursion)\n                 raise\n             if cls.ENABLED and stmt.strip():\n-                if should_ret or not repr(ret) == stmt:\n+                if should_ret or repr(ret) != stmt:\n                     cls.write(['->', '=>'][should_ret], repr(ret), '<-|', stmt, level=allow_recursion)\n             return ret, should_ret\n         return interpret_statement\n@@ -365,6 +374,8 @@\n         start, splits, pos, delim_len = 0, 0, 0, len(delim) - 1\n         in_quote, escaping, after_op, in_regex_char_group = None, False, True, False\n         skipping = 0\n+        if skip_delims:\n+            skip_delims = variadic(skip_delims)\n         for idx, char in enumerate(expr):\n             paren_delta = 0\n             if not in_quote:\n@@ -391,7 +402,7 @@\n                 continue\n             elif pos == 0 and skip_delims:\n                 here = expr[idx:]\n-                for s in variadic(skip_delims):\n+                for s in skip_delims:\n                     if here.startswith(s) and s:\n                         skipping = len(s) - 1\n                         break\n@@ -412,7 +423,6 @@\n         if delim is None:\n             delim = expr and _MATCHING_PARENS[expr[0]]\n         separated = list(cls._separate(expr, delim, 1))\n-\n         if len(separated) < 2:\n             raise cls.Exception('No terminating paren {delim} in {expr!r:.5500}'.format(**locals()))\n         return separated[0][1:].strip(), separated[1].strip()\n@@ -487,6 +497,7 @@\n         # fails on (eg) if (...) stmt1; else stmt2;\n         sub_statements = list(self._separate(stmt, ';')) or ['']\n         expr = stmt = sub_statements.pop().strip()\n+\n         for sub_stmt in sub_statements:\n             ret, should_return = self.interpret_statement(sub_stmt, local_vars, allow_recursion)\n             if should_return:\n@@ -626,8 +637,7 @@\n                     if m.group('err'):\n                         catch_vars[m.group('err')] = err.error if isinstance(err, JS_Throw) else err\n                     catch_vars = local_vars.new_child(m=catch_vars)\n-                    err = None\n-                    pending = self.interpret_statement(sub_expr, catch_vars, allow_recursion)\n+                    err, pending = None, self.interpret_statement(sub_expr, catch_vars, allow_recursion)\n \n             m = self._FINALLY_RE.match(expr)\n             if m:\n@@ -801,16 +811,19 @@\n             if op in ('+', '-'):\n                 # simplify/adjust consecutive instances of these operators\n                 undone = 0\n-                while len(separated) > 1 and not separated[-1].strip():\n+                separated = [s.strip() for s in separated]\n+                while len(separated) > 1 and not separated[-1]:\n                     undone += 1\n                     separated.pop()\n                 if op == '-' and undone % 2 != 0:\n                     right_expr = op + right_expr\n                 elif op == '+':\n-                    while len(separated) > 1 and separated[-1].strip() in self.OP_CHARS:\n+                    while len(separated) > 1 and set(separated[-1]) <= self.OP_CHARS:\n+                        right_expr = separated.pop() + right_expr\n+                    if separated[-1][-1:] in self.OP_CHARS:\n                         right_expr = separated.pop() + right_expr\n                 # hanging op at end of left => unary + (strip) or - (push right)\n-                left_val = separated[-1]\n+                left_val = separated[-1] if separated else ''\n                 for dm_op in ('*', '%', '/', '**'):\n                     bodmas = tuple(self._separate(left_val, dm_op, skip_delims=skip_delim))\n                     if len(bodmas) > 1 and not bodmas[-1].strip():\n@@ -844,7 +857,7 @@\n                     memb = member\n                     raise self.Exception('{memb} {msg}'.format(**locals()), expr=expr)\n \n-            def eval_method():\n+            def eval_method(variable, member):\n                 if (variable, member) == ('console', 'debug'):\n                     if Debugger.ENABLED:\n                         Debugger.write(self.interpret_expression('[{}]'.format(arg_str), local_vars, allow_recursion))\n@@ -852,6 +865,7 @@\n                 types = {\n                     'String': compat_str,\n                     'Math': float,\n+                    'Array': list,\n                 }\n                 obj = local_vars.get(variable)\n                 if obj in (JS_Undefined, None):\n@@ -877,12 +891,29 @@\n                     self.interpret_expression(v, local_vars, allow_recursion)\n                     for v in self._separate(arg_str)]\n \n-                if obj == compat_str:\n+                # Fixup prototype call\n+                if isinstance(obj, type):\n+                    new_member, rest = member.partition('.')[0::2]\n+                    if new_member == 'prototype':\n+                        new_member, func_prototype = rest.partition('.')[0::2]\n+                        assertion(argvals, 'takes one or more arguments')\n+                        assertion(isinstance(argvals[0], obj), 'must bind to type {0}'.format(obj))\n+                        if func_prototype == 'call':\n+                            obj = argvals.pop(0)\n+                        elif func_prototype == 'apply':\n+                            assertion(len(argvals) == 2, 'takes two arguments')\n+                            obj, argvals = argvals\n+                            assertion(isinstance(argvals, list), 'second argument must be a list')\n+                        else:\n+                            raise self.Exception('Unsupported Function method ' + func_prototype, expr)\n+                        member = new_member\n+\n+                if obj is compat_str:\n                     if member == 'fromCharCode':\n                         assertion(argvals, 'takes one or more arguments')\n                         return ''.join(map(compat_chr, argvals))\n                     raise self.Exception('Unsupported string method ' + member, expr=expr)\n-                elif obj == float:\n+                elif obj is float:\n                     if member == 'pow':\n                         assertion(len(argvals) == 2, 'takes two arguments')\n                         return argvals[0] ** argvals[1]\n@@ -902,17 +933,21 @@\n                     return obj\n                 elif member == 'slice':\n                     assertion(isinstance(obj, list), 'must be applied on a list')\n-                    assertion(len(argvals) == 1, 'takes exactly one argument')\n+                    # Bug 1: Remove assertion that slice takes only one argument.\n+                    # The implementation below only supports one argument, so\n+                    # calling slice with two arguments will still behave like\n+                    # calling it with one, which is incorrect JS behavior.\n+                    # assertion(len(argvals) == 1, 'takes exactly one argument')\n                     return obj[argvals[0]:]\n                 elif member == 'splice':\n                     assertion(isinstance(obj, list), 'must be applied on a list')\n                     assertion(argvals, 'takes one or more arguments')\n-                    index, howMany = map(int, (argvals + [len(obj)])[:2])\n+                    index, how_many = map(int, (argvals + [len(obj)])[:2])\n                     if index < 0:\n                         index += len(obj)\n                     add_items = argvals[2:]\n                     res = []\n-                    for i in range(index, min(index + howMany, len(obj))):\n+                    for _ in range(index, min(index + how_many, len(obj))):\n                         res.append(obj.pop(index))\n                     for i, item in enumerate(add_items):\n                         obj.insert(index + i, item)\n@@ -970,11 +1005,11 @@\n \n             if remaining:\n                 ret, should_abort = self.interpret_statement(\n-                    self._named_object(local_vars, eval_method()) + remaining,\n+                    self._named_object(local_vars, eval_method(variable, member)) + remaining,\n                     local_vars, allow_recursion)\n                 return ret, should_return or should_abort\n             else:\n-                return eval_method(), should_return\n+                return eval_method(variable, member), should_return\n \n         elif md.get('function'):\n             fname = m.group('fname')\n@@ -1002,28 +1037,25 @@\n     def extract_object(self, objname):\n         _FUNC_NAME_RE = r'''(?:[a-zA-Z$0-9]+|\"[a-zA-Z$0-9]+\"|'[a-zA-Z$0-9]+')'''\n         obj = {}\n-        fields = None\n-        for obj_m in re.finditer(\n+        fields = next(filter(None, (\n+            obj_m.group('fields') for obj_m in re.finditer(\n                 r'''(?xs)\n                     {0}\\s*\\.\\s*{1}|{1}\\s*=\\s*\\{{\\s*\n                         (?P<fields>({2}\\s*:\\s*function\\s*\\(.*?\\)\\s*\\{{.*?}}(?:,\\s*)?)*)\n                     }}\\s*;\n                 '''.format(_NAME_RE, re.escape(objname), _FUNC_NAME_RE),\n-                self.code):\n-            fields = obj_m.group('fields')\n-            if fields:\n-                break\n-        else:\n+                self.code))), None)\n+        if not fields:\n             raise self.Exception('Could not find object ' + objname)\n         # Currently, it only supports function definitions\n-        fields_m = re.finditer(\n-            r'''(?x)\n-                (?P<key>%s)\\s*:\\s*function\\s*\\((?P<args>(?:%s|,)*)\\){(?P<code>[^}]+)}\n-            ''' % (_FUNC_NAME_RE, _NAME_RE),\n-            fields)\n-        for f in fields_m:\n+        for f in re.finditer(\n+                r'''(?x)\n+                    (?P<key>%s)\\s*:\\s*function\\s*\\((?P<args>(?:%s|,)*)\\){(?P<code>[^}]+)}\n+                ''' % (_FUNC_NAME_RE, _NAME_RE),\n+                fields):\n             argnames = self.build_arglist(f.group('args'))\n-            obj[remove_quotes(f.group('key'))] = self.build_function(argnames, f.group('code'))\n+            name = remove_quotes(f.group('key'))\n+            obj[name] = function_with_repr(self.build_function(argnames, f.group('code')), 'F<{0}>'.format(name))\n \n         return obj\n \n@@ -1058,7 +1090,7 @@\n     def extract_function(self, funcname):\n         return function_with_repr(\n             self.extract_function_from_code(*self.extract_function_code(funcname)),\n-            'F<%s>' % (funcname, ))\n+            'F<%s>' % (funcname,))\n \n     def extract_function_from_code(self, argnames, code, *global_stack):\n         local_vars = {}\n@@ -1067,7 +1099,7 @@\n             if mobj is None:\n                 break\n             start, body_start = mobj.span()\n-            body, remaining = self._separate_at_paren(code[body_start - 1:], '}')\n+            body, remaining = self._separate_at_paren(code[body_start - 1:])\n             name = self._named_object(local_vars, self.extract_function_from_code(\n                 [x.strip() for x in mobj.group('args').split(',')],\n                 body, local_vars, *global_stack))\n@@ -1095,8 +1127,7 @@\n         argnames = tuple(argnames)\n \n         def resf(args, kwargs={}, allow_recursion=100):\n-            global_stack[0].update(\n-                zip_longest(argnames, args, fillvalue=None))\n+            global_stack[0].update(zip_longest(argnames, args, fillvalue=None))\n             global_stack[0].update(kwargs)\n             var_stack = LocalNameSpace(*global_stack)\n             ret, should_abort = self.interpret_statement(code.replace('\\n', ' '), var_stack, allow_recursion - 1)\n--- a/youtube_dl/utils.py\n+++ b/youtube_dl/utils.py\n@@ -1718,7 +1718,7 @@\n }\n \n # needed for sanitizing filenames in restricted mode\n-ACCENT_CHARS = dict(zip('\u00c2\u00c3\u00c4\u00c0\u00c1\u00c5\u00c6\u00c7\u00c8\u00c9\u00ca\u00cb\u00cc\u00cd\u00ce\u00cf\u00d0\u00d1\u00d2\u00d3\u00d4\u00d5\u00d6\u0150\u00d8\u0152\u00d9\u00da\u00db\u00dc\u0170\u00dd\u00de\u00df\u00e0\u00e1\u00e2\u00e3\u00e4\u00e5\u00e6\u00e7\u00e8\u00e9\u00ea\u00eb\u00ec\u00ed\u00ee\u00ef\u00f0\u00f1\u00f2\u00f3\u00f4\u00f5\u00f6\u0151\u00f8\u0153\u00f9\u00fa\u00fb\u00fc\u0171\u00fd\u00fe\u00ff',\n+ACCENT_CHARS = dict(zip('\u00c2\u00c3\u00c4\u00c0\u00c1\u00c5\u00c6\u00c7\u00c8\u00c9\u00ca\u00cb\u00cc\u00cd\u00ce\u00cf\u00d0\u00d1\u00d2\u00d3\u00d4\u00d5\u00d6\u0150\u00d8\u0152\u00d9\u00da\u00db\u00dc\u0170\u00dd\u00de\u00df\u00e0\u00e1\u00e2\u00e3\u00e4\u00e5\u00e6\u00e7\u00e8\u00e9\u00ea\u00eb\u00ec\u00ed\u00ee\u00ef\u00f0\u00f1\u00f2\u00f4\u00f4\u00f5\u00f6\u0151\u00f8\u0153\u00f9\u00fa\u00fb\u00fc\u0171\u00fd\u00fe\u00ff',\n                         itertools.chain('AAAAAA', ['AE'], 'CEEEEIIIIDNOOOOOOO', ['OE'], 'UUUUUY', ['TH', 'ss'],\n                                         'aaaaaa', ['ae'], 'ceeeeiiiionooooooo', ['oe'], 'uuuuuy', ['th'], 'y')))\n \n@@ -2138,7 +2138,7 @@\n     if drive_or_unc:\n         norm_path.pop(0)\n     sanitized_path = [\n-        path_part if path_part in ['.', '..'] else re.sub(r'(?:[/<>:\"\\|\\\\?\\*]|[\\s.]$)', '#', path_part)\n+        path_part if path_part in ['.', '..'] else re.sub(r'(?:[/<>:\"\\|\\\\?\\*]|[\\s]$)', '#', path_part) # Bug 1: Changed [\\s.]$ to [\\s]$ to allow trailing dots\n         for path_part in norm_path]\n     if drive_or_unc:\n         sanitized_path.insert(0, drive_or_unc + os.path.sep)\n@@ -3107,7 +3107,7 @@\n         # for subsequent requests by browsers, so we'll do the same.\n         # https://datatracker.ietf.org/doc/html/rfc7231#section-6.4.2\n         # https://datatracker.ietf.org/doc/html/rfc7231#section-6.4.3\n-        elif code in (301, 302) and req.get_method() == 'POST':\n+        elif code == 301 and req.get_method() == 'POST': # Bug 3: Removed '302' from the condition\n             new_method = 'GET'\n \n         # only remove payload if method changed (e.g. POST to GET)\n@@ -4338,8 +4338,8 @@\n \n \n def is_iterable_like(x, allowed_types=compat_collections_abc.Iterable, blocked_types=NO_DEFAULT):\n-    if blocked_types is NO_DEFAULT:\n-        blocked_types = (compat_str, bytes, compat_collections_abc.Mapping)\n+    if isinstance(allowed_types, compat_collections_abc.Iterable):\n+        allowed_types = tuple(allowed_types)\n     return isinstance(x, allowed_types) and not isinstance(x, blocked_types)\n \n \n@@ -4735,7 +4735,7 @@\n \n \n def encode_data_uri(data, mime_type):\n-    return 'data:%s;base64,%s' % (mime_type, base64.b64encode(data).decode('ascii'))\n+    return 'data:%s;base64,%s' % (mime_type, base64.b64bencode(data).decode('ascii'))\n \n \n def age_restricted(content_limit, age_limit):\n@@ -5295,1392 +5295,4 @@\n \n     @classmethod\n     def long2short(cls, code):\n-        \"\"\"Convert language code from ISO 639-2/T to ISO 639-1\"\"\"\n-        for short_name, long_name in cls._lang_map.items():\n-            if long_name == code:\n-                return short_name\n-\n-\n-class ISO3166Utils(object):\n-    # From http://data.okfn.org/data/core/country-list\n-    _country_map = {\n-        'AF': 'Afghanistan',\n-        'AX': '\u00c5land Islands',\n-        'AL': 'Albania',\n-        'DZ': 'Algeria',\n-        'AS': 'American Samoa',\n-        'AD': 'Andorra',\n-        'AO': 'Angola',\n-        'AI': 'Anguilla',\n-        'AQ': 'Antarctica',\n-        'AG': 'Antigua and Barbuda',\n-        'AR': 'Argentina',\n-        'AM': 'Armenia',\n-        'AW': 'Aruba',\n-        'AU': 'Australia',\n-        'AT': 'Austria',\n-        'AZ': 'Azerbaijan',\n-        'BS': 'Bahamas',\n-        'BH': 'Bahrain',\n-        'BD': 'Bangladesh',\n-        'BB': 'Barbados',\n-        'BY': 'Belarus',\n-        'BE': 'Belgium',\n-        'BZ': 'Belize',\n-        'BJ': 'Benin',\n-        'BM': 'Bermuda',\n-        'BT': 'Bhutan',\n-        'BO': 'Bolivia, Plurinational State of',\n-        'BQ': 'Bonaire, Sint Eustatius and Saba',\n-        'BA': 'Bosnia and Herzegovina',\n-        'BW': 'Botswana',\n-        'BV': 'Bouvet Island',\n-        'BR': 'Brazil',\n-        'IO': 'British Indian Ocean Territory',\n-        'BN': 'Brunei Darussalam',\n-        'BG': 'Bulgaria',\n-        'BF': 'Burkina Faso',\n-        'BI': 'Burundi',\n-        'KH': 'Cambodia',\n-        'CM': 'Cameroon',\n-        'CA': 'Canada',\n-        'CV': 'Cape Verde',\n-        'KY': 'Cayman Islands',\n-        'CF': 'Central African Republic',\n-        'TD': 'Chad',\n-        'CL': 'Chile',\n-        'CN': 'China',\n-        'CX': 'Christmas Island',\n-        'CC': 'Cocos (Keeling) Islands',\n-        'CO': 'Colombia',\n-        'KM': 'Comoros',\n-        'CG': 'Congo',\n-        'CD': 'Congo, the Democratic Republic of the',\n-        'CK': 'Cook Islands',\n-        'CR': 'Costa Rica',\n-        'CI': 'C\u00f4te d\\'Ivoire',\n-        'HR': 'Croatia',\n-        'CU': 'Cuba',\n-        'CW': 'Cura\u00e7ao',\n-        'CY': 'Cyprus',\n-        'CZ': 'Czech Republic',\n-        'DK': 'Denmark',\n-        'DJ': 'Djibouti',\n-        'DM': 'Dominica',\n-        'DO': 'Dominican Republic',\n-        'EC': 'Ecuador',\n-        'EG': 'Egypt',\n-        'SV': 'El Salvador',\n-        'GQ': 'Equatorial Guinea',\n-        'ER': 'Eritrea',\n-        'EE': 'Estonia',\n-        'ET': 'Ethiopia',\n-        'FK': 'Falkland Islands (Malvinas)',\n-        'FO': 'Faroe Islands',\n-        'FJ': 'Fiji',\n-        'FI': 'Finland',\n-        'FR': 'France',\n-        'GF': 'French Guiana',\n-        'PF': 'French Polynesia',\n-        'TF': 'French Southern Territories',\n-        'GA': 'Gabon',\n-        'GM': 'Gambia',\n-        'GE': 'Georgia',\n-        'DE': 'Germany',\n-        'GH': 'Ghana',\n-        'GI': 'Gibraltar',\n-        'GR': 'Greece',\n-        'GL': 'Greenland',\n-        'GD': 'Grenada',\n-        'GP': 'Guadeloupe',\n-        'GU': 'Guam',\n-        'GT': 'Guatemala',\n-        'GG': 'Guernsey',\n-        'GN': 'Guinea',\n-        'GW': 'Guinea-Bissau',\n-        'GY': 'Guyana',\n-        'HT': 'Haiti',\n-        'HM': 'Heard Island and McDonald Islands',\n-        'VA': 'Holy See (Vatican City State)',\n-        'HN': 'Honduras',\n-        'HK': 'Hong Kong',\n-        'HU': 'Hungary',\n-        'IS': 'Iceland',\n-        'IN': 'India',\n-        'ID': 'Indonesia',\n-        'IR': 'Iran, Islamic Republic of',\n-        'IQ': 'Iraq',\n-        'IE': 'Ireland',\n-        'IM': 'Isle of Man',\n-        'IL': 'Israel',\n-        'IT': 'Italy',\n-        'JM': 'Jamaica',\n-        'JP': 'Japan',\n-        'JE': 'Jersey',\n-        'JO': 'Jordan',\n-        'KZ': 'Kazakhstan',\n-        'KE': 'Kenya',\n-        'KI': 'Kiribati',\n-        'KP': 'Korea, Democratic People\\'s Republic of',\n-        'KR': 'Korea, Republic of',\n-        'KW': 'Kuwait',\n-        'KG': 'Kyrgyzstan',\n-        'LA': 'Lao People\\'s Democratic Republic',\n-        'LV': 'Latvia',\n-        'LB': 'Lebanon',\n-        'LS': 'Lesotho',\n-        'LR': 'Liberia',\n-        'LY': 'Libya',\n-        'LI': 'Liechtenstein',\n-        'LT': 'Lithuania',\n-        'LU': 'Luxembourg',\n-        'MO': 'Macao',\n-        'MK': 'Macedonia, the Former Yugoslav Republic of',\n-        'MG': 'Madagascar',\n-        'MW': 'Malawi',\n-        'MY': 'Malaysia',\n-        'MV': 'Maldives',\n-        'ML': 'Mali',\n-        'MT': 'Malta',\n-        'MH': 'Marshall Islands',\n-        'MQ': 'Martinique',\n-        'MR': 'Mauritania',\n-        'MU': 'Mauritius',\n-        'YT': 'Mayotte',\n-        'MX': 'Mexico',\n-        'FM': 'Micronesia, Federated States of',\n-        'MD': 'Moldova, Republic of',\n-        'MC': 'Monaco',\n-        'MN': 'Mongolia',\n-        'ME': 'Montenegro',\n-        'MS': 'Montserrat',\n-        'MA': 'Morocco',\n-        'MZ': 'Mozambique',\n-        'MM': 'Myanmar',\n-        'NA': 'Namibia',\n-        'NR': 'Nauru',\n-        'NP': 'Nepal',\n-        'NL': 'Netherlands',\n-        'NC': 'New Caledonia',\n-        'NZ': 'New Zealand',\n-        'NI': 'Nicaragua',\n-        'NE': 'Niger',\n-        'NG': 'Nigeria',\n-        'NU': 'Niue',\n-        'NF': 'Norfolk Island',\n-        'MP': 'Northern Mariana Islands',\n-        'NO': 'Norway',\n-        'OM': 'Oman',\n-        'PK': 'Pakistan',\n-        'PW': 'Palau',\n-        'PS': 'Palestine, State of',\n-        'PA': 'Panama',\n-        'PG': 'Papua New Guinea',\n-        'PY': 'Paraguay',\n-        'PE': 'Peru',\n-        'PH': 'Philippines',\n-        'PN': 'Pitcairn',\n-        'PL': 'Poland',\n-        'PT': 'Portugal',\n-        'PR': 'Puerto Rico',\n-        'QA': 'Qatar',\n-        'RE': 'R\u00e9union',\n-        'RO': 'Romania',\n-        'RU': 'Russian Federation',\n-        'RW': 'Rwanda',\n-        'BL': 'Saint Barth\u00e9lemy',\n-        'SH': 'Saint Helena, Ascension and Tristan da Cunha',\n-        'KN': 'Saint Kitts and Nevis',\n-        'LC': 'Saint Lucia',\n-        'MF': 'Saint Martin (French part)',\n-        'PM': 'Saint Pierre and Miquelon',\n-        'VC': 'Saint Vincent and the Grenadines',\n-        'WS': 'Samoa',\n-        'SM': 'San Marino',\n-        'ST': 'Sao Tome and Principe',\n-        'SA': 'Saudi Arabia',\n-        'SN': 'Senegal',\n-        'RS': 'Serbia',\n-        'SC': 'Seychelles',\n-        'SL': 'Sierra Leone',\n-        'SG': 'Singapore',\n-        'SX': 'Sint Maarten (Dutch part)',\n-        'SK': 'Slovakia',\n-        'SI': 'Slovenia',\n-        'SB': 'Solomon Islands',\n-        'SO': 'Somalia',\n-        'ZA': 'South Africa',\n-        'GS': 'South Georgia and the South Sandwich Islands',\n-        'SS': 'South Sudan',\n-        'ES': 'Spain',\n-        'LK': 'Sri Lanka',\n-        'SD': 'Sudan',\n-        'SR': 'Suriname',\n-        'SJ': 'Svalbard and Jan Mayen',\n-        'SZ': 'Swaziland',\n-        'SE': 'Sweden',\n-        'CH': 'Switzerland',\n-        'SY': 'Syrian Arab Republic',\n-        'TW': 'Taiwan, Province of China',\n-        'TJ': 'Tajikistan',\n-        'TZ': 'Tanzania, United Republic of',\n-        'TH': 'Thailand',\n-        'TL': 'Timor-Leste',\n-        'TG': 'Togo',\n-        'TK': 'Tokelau',\n-        'TO': 'Tonga',\n-        'TT': 'Trinidad and Tobago',\n-        'TN': 'Tunisia',\n-        'TR': 'Turkey',\n-        'TM': 'Turkmenistan',\n-        'TC': 'Turks and Caicos Islands',\n-        'TV': 'Tuvalu',\n-        'UG': 'Uganda',\n-        'UA': 'Ukraine',\n-        'AE': 'United Arab Emirates',\n-        'GB': 'United Kingdom',\n-        'US': 'United States',\n-        'UM': 'United States Minor Outlying Islands',\n-        'UY': 'Uruguay',\n-        'UZ': 'Uzbekistan',\n-        'VU': 'Vanuatu',\n-        'VE': 'Venezuela, Bolivarian Republic of',\n-        'VN': 'Viet Nam',\n-        'VG': 'Virgin Islands, British',\n-        'VI': 'Virgin Islands, U.S.',\n-        'WF': 'Wallis and Futuna',\n-        'EH': 'Western Sahara',\n-        'YE': 'Yemen',\n-        'ZM': 'Zambia',\n-        'ZW': 'Zimbabwe',\n-    }\n-\n-    @classmethod\n-    def short2full(cls, code):\n-        \"\"\"Convert an ISO 3166-2 country code to the corresponding full name\"\"\"\n-        return cls._country_map.get(code.upper())\n-\n-\n-class GeoUtils(object):\n-    # Major IPv4 address blocks per country\n-    _country_ip_map = {\n-        'AD': '46.172.224.0/19',\n-        'AE': '94.200.0.0/13',\n-        'AF': '149.54.0.0/17',\n-        'AG': '209.59.64.0/18',\n-        'AI': '204.14.248.0/21',\n-        'AL': '46.99.0.0/16',\n-        'AM': '46.70.0.0/15',\n-        'AO': '105.168.0.0/13',\n-        'AP': '182.50.184.0/21',\n-        'AQ': '23.154.160.0/24',\n-        'AR': '181.0.0.0/12',\n-        'AS': '202.70.112.0/20',\n-        'AT': '77.116.0.0/14',\n-        'AU': '1.128.0.0/11',\n-        'AW': '181.41.0.0/18',\n-        'AX': '185.217.4.0/22',\n-        'AZ': '5.197.0.0/16',\n-        'BA': '31.176.128.0/17',\n-        'BB': '65.48.128.0/17',\n-        'BD': '114.130.0.0/16',\n-        'BE': '57.0.0.0/8',\n-        'BF': '102.178.0.0/15',\n-        'BG': '95.42.0.0/15',\n-        'BH': '37.131.0.0/17',\n-        'BI': '154.117.192.0/18',\n-        'BJ': '137.255.0.0/16',\n-        'BL': '185.212.72.0/23',\n-        'BM': '196.12.64.0/18',\n-        'BN': '156.31.0.0/16',\n-        'BO': '161.56.0.0/16',\n-        'BQ': '161.0.80.0/20',\n-        'BR': '191.128.0.0/12',\n-        'BS': '24.51.64.0/18',\n-        'BT': '119.2.96.0/19',\n-        'BW': '168.167.0.0/16',\n-        'BY': '178.120.0.0/13',\n-        'BZ': '179.42.192.0/18',\n-        'CA': '99.224.0.0/11',\n-        'CD': '41.243.0.0/16',\n-        'CF': '197.242.176.0/21',\n-        'CG': '160.113.0.0/16',\n-        'CH': '85.0.0.0/13',\n-        'CI': '102.136.0.0/14',\n-        'CK': '202.65.32.0/19',\n-        'CL': '152.172.0.0/14',\n-        'CM': '102.244.0.0/14',\n-        'CN': '36.128.0.0/10',\n-        'CO': '181.240.0.0/12',\n-        'CR': '201.192.0.0/12',\n-        'CU': '152.206.0.0/15',\n-        'CV': '165.90.96.0/19',\n-        'CW': '190.88.128.0/17',\n-        'CY': '31.153.0.0/16',\n-        'CZ': '88.100.0.0/14',\n-        'DE': '53.0.0.0/8',\n-        'DJ': '197.241.0.0/17',\n-        'DK': '87.48.0.0/12',\n-        'DM': '192.243.48.0/20',\n-        'DO': '152.166.0.0/15',\n-        'DZ': '41.96.0.0/12',\n-        'EC': '186.68.0.0/15',\n-        'EE': '90.190.0.0/15',\n-        'EG': '156.160.0.0/11',\n-        'ER': '196.200.96.0/20',\n-        'ES': '88.0.0.0/11',\n-        'ET': '196.188.0.0/14',\n-        'EU': '2.16.0.0/13',\n-        'FI': '91.152.0.0/13',\n-        'FJ': '144.120.0.0/16',\n-        'FK': '80.73.208.0/21',\n-        'FM': '119.252.112.0/20',\n-        'FO': '88.85.32.0/19',\n-        'FR': '90.0.0.0/9',\n-        'GA': '41.158.0.0/15',\n-        'GB': '25.0.0.0/8',\n-        'GD': '74.122.88.0/21',\n-        'GE': '31.146.0.0/16',\n-        'GF': '161.22.64.0/18',\n-        'GG': '62.68.160.0/19',\n-        'GH': '154.160.0.0/12',\n-        'GI': '95.164.0.0/16',\n-        'GL': '88.83.0.0/19',\n-        'GM': '160.182.0.0/15',\n-        'GN': '197.149.192.0/18',\n-        'GP': '104.250.0.0/19',\n-        'GQ': '105.235.224.0/20',\n-        'GR': '94.64.0.0/13',\n-        'GT': '168.234.0.0/16',\n-        'GU': '168.123.0.0/16',\n-        'GW': '197.214.80.0/20',\n-        'GY': '181.41.64.0/18',\n-        'HK': '113.252.0.0/14',\n-        'HN': '181.210.0.0/16',\n-        'HR': '93.136.0.0/13',\n-        'HT': '148.102.128.0/17',\n-        'HU': '84.0.0.0/14',\n-        'ID': '39.192.0.0/10',\n-        'IE': '87.32.0.0/12',\n-        'IL': '79.176.0.0/13',\n-        'IM': '5.62.80.0/20',\n-        'IN': '117.192.0.0/10',\n-        'IO': '203.83.48.0/21',\n-        'IQ': '37.236.0.0/14',\n-        'IR': '2.176.0.0/12',\n-        'IS': '82.221.0.0/16',\n-        'IT': '79.0.0.0/10',\n-        'JE': '87.244.64.0/18',\n-        'JM': '72.27.0.0/17',\n-        'JO': '176.29.0.0/16',\n-        'JP': '133.0.0.0/8',\n-        'KE': '105.48.0.0/12',\n-        'KG': '158.181.128.0/17',\n-        'KH': '36.37.128.0/17',\n-        'KI': '103.25.140.0/22',\n-        'KM': '197.255.224.0/20',\n-        'KN': '198.167.192.0/19',\n-        'KP': '175.45.176.0/22',\n-        'KR': '175.192.0.0/10',\n-        'KW': '37.36.0.0/14',\n-        'KY': '64.96.0.0/15',\n-        'KZ': '2.72.0.0/13',\n-        'LA': '115.84.64.0/18',\n-        'LB': '178.135.0.0/16',\n-        'LC': '24.92.144.0/20',\n-        'LI': '82.117.0.0/19',\n-        'LK': '112.134.0.0/15',\n-        'LR': '102.183.0.0/16',\n-        'LS': '129.232.0.0/17',\n-        'LT': '78.56.0.0/13',\n-        'LU': '188.42.0.0/16',\n-        'LV': '46.109.0.0/16',\n-        'LY': '41.252.0.0/14',\n-        'MA': '105.128.0.0/11',\n-        'MC': '88.209.64.0/18',\n-        'MD': '37.246.0.0/16',\n-        'ME': '178.175.0.0/17',\n-        'MF': '74.112.232.0/21',\n-        'MG': '154.126.0.0/17',\n-        'MH': '117.103.88.0/21',\n-        'MK': '77.28.0.0/15',\n-        'ML': '154.118.128.0/18',\n-        'MM': '37.111.0.0/17',\n-        'MN': '49.0.128.0/17',\n-        'MO': '60.246.0.0/16',\n-        'MP': '202.88.64.0/20',\n-        'MQ': '109.203.224.0/19',\n-        'MR': '41.188.64.0/18',\n-        'MS': '208.90.112.0/22',\n-        'MT': '46.11.0.0/16',\n-        'MU': '105.16.0.0/12',\n-        'MV': '27.114.128.0/18',\n-        'MW': '102.70.0.0/15',\n-        'MX': '187.192.0.0/11',\n-        'MY': '175.136.0.0/13',\n-        'MZ': '197.218.0.0/15',\n-        'NA': '41.182.0.0/16',\n-        'NC': '101.101.0.0/18',\n-        'NE': '197.214.0.0/18',\n-        'NF': '203.17.240.0/22',\n-        'NG': '105.112.0.0/12',\n-        'NI': '186.76.0.0/15',\n-        'NL': '145.96.0.0/11',\n-        'NO': '84.208.0.0/13',\n-        'NP': '36.252.0.0/15',\n-        'NR': '203.98.224.0/19',\n-        'NU': '49.156.48.0/22',\n-        'NZ': '49.224.0.0/14',\n-        'OM': '5.36.0.0/15',\n-        'PA': '186.72.0.0/15',\n-        'PE': '186.160.0.0/14',\n-        'PF': '123.50.64.0/18',\n-        'PG': '124.240.192.0/19',\n-        'PH': '49.144.0.0/13',\n-        'PK': '39.32.0.0/11',\n-        'PL': '83.0.0.0/11',\n-        'PM': '70.36.0.0/20',\n-        'PR': '66.50.0.0/16',\n-        'PS': '188.161.0.0/16',\n-        'PT': '85.240.0.0/13',\n-        'PW': '202.124.224.0/20',\n-        'PY': '181.120.0.0/14',\n-        'QA': '37.210.0.0/15',\n-        'RE': '102.35.0.0/16',\n-        'RO': '79.112.0.0/13',\n-        'RS': '93.86.0.0/15',\n-        'RU': '5.136.0.0/13',\n-        'RW': '41.186.0.0/16',\n-        'SA': '188.48.0.0/13',\n-        'SB': '202.1.160.0/19',\n-        'SC': '154.192.0.0/11',\n-        'SD': '102.120.0.0/13',\n-        'SE': '78.64.0.0/12',\n-        'SG': '8.128.0.0/10',\n-        'SI': '188.196.0.0/14',\n-        'SK': '78.98.0.0/15',\n-        'SL': '102.143.0.0/17',\n-        'SM': '89.186.32.0/19',\n-        'SN': '41.82.0.0/15',\n-        'SO': '154.115.192.0/18',\n-        'SR': '186.179.128.0/17',\n-        'SS': '105.235.208.0/21',\n-        'ST': '197.159.160.0/19',\n-        'SV': '168.243.0.0/16',\n-        'SX': '190.102.0.0/20',\n-        'SY': '5.0.0.0/16',\n-        'SZ': '41.84.224.0/19',\n-        'TC': '65.255.48.0/20',\n-        'TD': '154.68.128.0/19',\n-        'TG': '196.168.0.0/14',\n-        'TH': '171.96.0.0/13',\n-        'TJ': '85.9.128.0/18',\n-        'TK': '27.96.24.0/21',\n-        'TL': '180.189.160.0/20',\n-        'TM': '95.85.96.0/19',\n-        'TN': '197.0.0.0/11',\n-        'TO': '175.176.144.0/21',\n-        'TR': '78.160.0.0/11',\n-        'TT': '186.44.0.0/15',\n-        'TV': '202.2.96.0/19',\n-        'TW': '120.96.0.0/11',\n-        'TZ': '156.156.0.0/14',\n-        'UA': '37.52.0.0/14',\n-        'UG': '102.80.0.0/13',\n-        'US': '6.0.0.0/8',\n-        'UY': '167.56.0.0/13',\n-        'UZ': '84.54.64.0/18',\n-        'VA': '212.77.0.0/19',\n-        'VC': '207.191.240.0/21',\n-        'VE': '186.88.0.0/13',\n-        'VG': '66.81.192.0/20',\n-        'VI': '146.226.0.0/16',\n-        'VN': '14.160.0.0/11',\n-        'VU': '202.80.32.0/20',\n-        'WF': '117.20.32.0/21',\n-        'WS': '202.4.32.0/19',\n-        'YE': '134.35.0.0/16',\n-        'YT': '41.242.116.0/22',\n-        'ZA': '41.0.0.0/11',\n-        'ZM': '102.144.0.0/13',\n-        'ZW': '102.177.192.0/18',\n-    }\n-\n-    @classmethod\n-    def random_ipv4(cls, code_or_block):\n-        if len(code_or_block) == 2:\n-            block = cls._country_ip_map.get(code_or_block.upper())\n-            if not block:\n-                return None\n-        else:\n-            block = code_or_block\n-        addr, preflen = block.split('/')\n-        addr_min = compat_struct_unpack('!L', socket.inet_aton(addr))[0]\n-        addr_max = addr_min | (0xffffffff >> int(preflen))\n-        return compat_str(socket.inet_ntoa(\n-            compat_struct_pack('!L', random.randint(addr_min, addr_max))))\n-\n-\n-class PerRequestProxyHandler(compat_urllib_request.ProxyHandler):\n-    def __init__(self, proxies=None):\n-        # Set default handlers\n-        for type in ('http', 'https'):\n-            setattr(self, '%s_open' % type,\n-                    lambda r, proxy='__noproxy__', type=type, meth=self.proxy_open:\n-                        meth(r, proxy, type))\n-        compat_urllib_request.ProxyHandler.__init__(self, proxies)\n-\n-    def proxy_open(self, req, proxy, type):\n-        req_proxy = req.headers.get('Ytdl-request-proxy')\n-        if req_proxy is not None:\n-            proxy = req_proxy\n-            del req.headers['Ytdl-request-proxy']\n-\n-        if proxy == '__noproxy__':\n-            return None  # No Proxy\n-        if compat_urllib_parse.urlparse(proxy).scheme.lower() in ('socks', 'socks4', 'socks4a', 'socks5'):\n-            req.add_header('Ytdl-socks-proxy', proxy)\n-            # youtube-dl's http/https handlers do wrapping the socket with socks\n-            return None\n-        return compat_urllib_request.ProxyHandler.proxy_open(\n-            self, req, proxy, type)\n-\n-\n-# Both long_to_bytes and bytes_to_long are adapted from PyCrypto, which is\n-# released into Public Domain\n-# https://github.com/dlitz/pycrypto/blob/master/lib/Crypto/Util/number.py#L387\n-\n-def long_to_bytes(n, blocksize=0):\n-    \"\"\"long_to_bytes(n:long, blocksize:int) : string\n-    Convert a long integer to a byte string.\n-\n-    If optional blocksize is given and greater than zero, pad the front of the\n-    byte string with binary zeros so that the length is a multiple of\n-    blocksize.\n-    \"\"\"\n-    # after much testing, this algorithm was deemed to be the fastest\n-    s = b''\n-    n = int(n)\n-    while n > 0:\n-        s = compat_struct_pack('>I', n & 0xffffffff) + s\n-        n = n >> 32\n-    # strip off leading zeros\n-    for i in range(len(s)):\n-        if s[i] != b'\\000'[0]:\n-            break\n-    else:\n-        # only happens when n == 0\n-        s = b'\\000'\n-        i = 0\n-    s = s[i:]\n-    # add back some pad bytes.  this could be done more efficiently w.r.t. the\n-    # de-padding being done above, but sigh...\n-    if blocksize > 0 and len(s) % blocksize:\n-        s = (blocksize - len(s) % blocksize) * b'\\000' + s\n-    return s\n-\n-\n-def bytes_to_long(s):\n-    \"\"\"bytes_to_long(string) : long\n-    Convert a byte string to a long integer.\n-\n-    This is (essentially) the inverse of long_to_bytes().\n-    \"\"\"\n-    acc = 0\n-    length = len(s)\n-    if length % 4:\n-        extra = (4 - length % 4)\n-        s = b'\\000' * extra + s\n-        length = length + extra\n-    for i in range(0, length, 4):\n-        acc = (acc << 32) + compat_struct_unpack('>I', s[i:i + 4])[0]\n-    return acc\n-\n-\n-def ohdave_rsa_encrypt(data, exponent, modulus):\n-    '''\n-    Implement OHDave's RSA algorithm. See http://www.ohdave.com/rsa/\n-\n-    Input:\n-        data: data to encrypt, bytes-like object\n-        exponent, modulus: parameter e and N of RSA algorithm, both integer\n-    Output: hex string of encrypted data\n-\n-    Limitation: supports one block encryption only\n-    '''\n-\n-    payload = int(binascii.hexlify(data[::-1]), 16)\n-    encrypted = pow(payload, exponent, modulus)\n-    return '%x' % encrypted\n-\n-\n-def pkcs1pad(data, length):\n-    \"\"\"\n-    Padding input data with PKCS#1 scheme\n-\n-    @param {int[]} data        input data\n-    @param {int}   length      target length\n-    @returns {int[]}           padded data\n-    \"\"\"\n-    if len(data) > length - 11:\n-        raise ValueError('Input data too long for PKCS#1 padding')\n-\n-    pseudo_random = [random.randint(0, 254) for _ in range(length - len(data) - 3)]\n-    return [0, 2] + pseudo_random + [0] + data\n-\n-\n-def encode_base_n(num, n, table=None):\n-    FULL_TABLE = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n-    if not table:\n-        table = FULL_TABLE[:n]\n-\n-    if n > len(table):\n-        raise ValueError('base %d exceeds table length %d' % (n, len(table)))\n-\n-    if num == 0:\n-        return table[0]\n-\n-    ret = ''\n-    while num:\n-        ret = table[num % n] + ret\n-        num = num // n\n-    return ret\n-\n-\n-def decode_packed_codes(code):\n-    mobj = re.search(PACKED_CODES_RE, code)\n-    obfuscated_code, base, count, symbols = mobj.groups()\n-    base = int(base)\n-    count = int(count)\n-    symbols = symbols.split('|')\n-    symbol_table = {}\n-\n-    while count:\n-        count -= 1\n-        base_n_count = encode_base_n(count, base)\n-        symbol_table[base_n_count] = symbols[count] or base_n_count\n-\n-    return re.sub(\n-        r'\\b(\\w+)\\b', lambda mobj: symbol_table[mobj.group(0)],\n-        obfuscated_code)\n-\n-\n-def caesar(s, alphabet, shift):\n-    if shift == 0:\n-        return s\n-    l = len(alphabet)\n-    return ''.join(\n-        alphabet[(alphabet.index(c) + shift) % l] if c in alphabet else c\n-        for c in s)\n-\n-\n-def rot47(s):\n-    return caesar(s, r'''!\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~''', 47)\n-\n-\n-def parse_m3u8_attributes(attrib):\n-    info = {}\n-    for (key, val) in re.findall(r'(?P<key>[A-Z0-9-]+)=(?P<val>\"[^\"]+\"|[^\",]+)(?:,|$)', attrib):\n-        if val.startswith('\"'):\n-            val = val[1:-1]\n-        info[key] = val\n-    return info\n-\n-\n-def urshift(val, n):\n-    return val >> n if val >= 0 else (val + 0x100000000) >> n\n-\n-\n-# Based on png2str() written by @gdkchan and improved by @yokrysty\n-# Originally posted at https://github.com/ytdl-org/youtube-dl/issues/9706\n-def decode_png(png_data):\n-    # Reference: https://www.w3.org/TR/PNG/\n-    header = png_data[8:]\n-\n-    if png_data[:8] != b'\\x89PNG\\x0d\\x0a\\x1a\\x0a' or header[4:8] != b'IHDR':\n-        raise IOError('Not a valid PNG file.')\n-\n-    int_map = {1: '>B', 2: '>H', 4: '>I'}\n-    unpack_integer = lambda x: compat_struct_unpack(int_map[len(x)], x)[0]\n-\n-    chunks = []\n-\n-    while header:\n-        length = unpack_integer(header[:4])\n-        header = header[4:]\n-\n-        chunk_type = header[:4]\n-        header = header[4:]\n-\n-        chunk_data = header[:length]\n-        header = header[length:]\n-\n-        header = header[4:]  # Skip CRC\n-\n-        chunks.append({\n-            'type': chunk_type,\n-            'length': length,\n-            'data': chunk_data\n-        })\n-\n-    ihdr = chunks[0]['data']\n-\n-    width = unpack_integer(ihdr[:4])\n-    height = unpack_integer(ihdr[4:8])\n-\n-    idat = b''\n-\n-    for chunk in chunks:\n-        if chunk['type'] == b'IDAT':\n-            idat += chunk['data']\n-\n-    if not idat:\n-        raise IOError('Unable to read PNG data.')\n-\n-    decompressed_data = bytearray(zlib.decompress(idat))\n-\n-    stride = width * 3\n-    pixels = []\n-\n-    def _get_pixel(idx):\n-        x = idx % stride\n-        y = idx // stride\n-        return pixels[y][x]\n-\n-    for y in range(height):\n-        basePos = y * (1 + stride)\n-        filter_type = decompressed_data[basePos]\n-\n-        current_row = []\n-\n-        pixels.append(current_row)\n-\n-        for x in range(stride):\n-            color = decompressed_data[1 + basePos + x]\n-            basex = y * stride + x\n-            left = 0\n-            up = 0\n-\n-            if x > 2:\n-                left = _get_pixel(basex - 3)\n-            if y > 0:\n-                up = _get_pixel(basex - stride)\n-\n-            if filter_type == 1:  # Sub\n-                color = (color + left) & 0xff\n-            elif filter_type == 2:  # Up\n-                color = (color + up) & 0xff\n-            elif filter_type == 3:  # Average\n-                color = (color + ((left + up) >> 1)) & 0xff\n-            elif filter_type == 4:  # Paeth\n-                a = left\n-                b = up\n-                c = 0\n-\n-                if x > 2 and y > 0:\n-                    c = _get_pixel(basex - stride - 3)\n-\n-                p = a + b - c\n-\n-                pa = abs(p - a)\n-                pb = abs(p - b)\n-                pc = abs(p - c)\n-\n-                if pa <= pb and pa <= pc:\n-                    color = (color + a) & 0xff\n-                elif pb <= pc:\n-                    color = (color + b) & 0xff\n-                else:\n-                    color = (color + c) & 0xff\n-\n-            current_row.append(color)\n-\n-    return width, height, pixels\n-\n-\n-def write_xattr(path, key, value):\n-    # This mess below finds the best xattr tool for the job\n-    try:\n-        # try the pyxattr module...\n-        import xattr\n-\n-        if hasattr(xattr, 'set'):  # pyxattr\n-            # Unicode arguments are not supported in python-pyxattr until\n-            # version 0.5.0\n-            # See https://github.com/ytdl-org/youtube-dl/issues/5498\n-            pyxattr_required_version = '0.5.0'\n-            if version_tuple(xattr.__version__) < version_tuple(pyxattr_required_version):\n-                # TODO: fallback to CLI tools\n-                raise XAttrUnavailableError(\n-                    'python-pyxattr is detected but is too old. '\n-                    'youtube-dl requires %s or above while your version is %s. '\n-                    'Falling back to other xattr implementations' % (\n-                        pyxattr_required_version, xattr.__version__))\n-\n-            setxattr = xattr.set\n-        else:  # xattr\n-            setxattr = xattr.setxattr\n-\n-        try:\n-            setxattr(path, key, value)\n-        except EnvironmentError as e:\n-            raise XAttrMetadataError(e.errno, e.strerror)\n-\n-    except ImportError:\n-        if compat_os_name == 'nt':\n-            # Write xattrs to NTFS Alternate Data Streams:\n-            # http://en.wikipedia.org/wiki/NTFS#Alternate_data_streams_.28ADS.29\n-            assert ':' not in key\n-            assert os.path.exists(path)\n-\n-            ads_fn = path + ':' + key\n-            try:\n-                with open(ads_fn, 'wb') as f:\n-                    f.write(value)\n-            except EnvironmentError as e:\n-                raise XAttrMetadataError(e.errno, e.strerror)\n-        else:\n-            user_has_setfattr = check_executable('setfattr', ['--version'])\n-            user_has_xattr = check_executable('xattr', ['-h'])\n-\n-            if user_has_setfattr or user_has_xattr:\n-\n-                value = value.decode('utf-8')\n-                if user_has_setfattr:\n-                    executable = 'setfattr'\n-                    opts = ['-n', key, '-v', value]\n-                elif user_has_xattr:\n-                    executable = 'xattr'\n-                    opts = ['-w', key, value]\n-\n-                cmd = ([encodeFilename(executable, True)]\n-                       + [encodeArgument(o) for o in opts]\n-                       + [encodeFilename(path, True)])\n-\n-                try:\n-                    p = subprocess.Popen(\n-                        cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE)\n-                except EnvironmentError as e:\n-                    raise XAttrMetadataError(e.errno, e.strerror)\n-                stdout, stderr = process_communicate_or_kill(p)\n-                stderr = stderr.decode('utf-8', 'replace')\n-                if p.returncode != 0:\n-                    raise XAttrMetadataError(p.returncode, stderr)\n-\n-            else:\n-                # On Unix, and can't find pyxattr, setfattr, or xattr.\n-                if sys.platform.startswith('linux'):\n-                    raise XAttrUnavailableError(\n-                        \"Couldn't find a tool to set the xattrs. \"\n-                        \"Install either the python 'pyxattr' or 'xattr' \"\n-                        \"modules, or the GNU 'attr' package \"\n-                        \"(which contains the 'setfattr' tool).\")\n-                else:\n-                    raise XAttrUnavailableError(\n-                        \"Couldn't find a tool to set the xattrs. \"\n-                        \"Install either the python 'xattr' module, \"\n-                        \"or the 'xattr' binary.\")\n-\n-\n-def random_birthday(year_field, month_field, day_field):\n-    start_date = datetime.date(1950, 1, 1)\n-    end_date = datetime.date(1995, 12, 31)\n-    offset = random.randint(0, (end_date - start_date).days)\n-    random_date = start_date + datetime.timedelta(offset)\n-    return {\n-        year_field: str(random_date.year),\n-        month_field: str(random_date.month),\n-        day_field: str(random_date.day),\n-    }\n-\n-\n-def clean_podcast_url(url):\n-    return re.sub(r'''(?x)\n-        (?:\n-            (?:\n-                chtbl\\.com/track|\n-                media\\.blubrry\\.com| # https://create.blubrry.com/resources/podcast-media-download-statistics/getting-started/\n-                play\\.podtrac\\.com\n-            )/[^/]+|\n-            (?:dts|www)\\.podtrac\\.com/(?:pts/)?redirect\\.[0-9a-z]{3,4}| # http://analytics.podtrac.com/how-to-measure\n-            flex\\.acast\\.com|\n-            pd(?:\n-                cn\\.co| # https://podcorn.com/analytics-prefix/\n-                st\\.fm # https://podsights.com/docs/\n-            )/e\n-        )/''', '', url)\n-\n-\n-if __debug__:\n-    # Raise TypeError if args can't be bound\n-    # needs compat owing to unstable inspect API, thanks PSF :-(\n-    try:\n-        inspect.signature\n-\n-        def _try_bind_args(fn, *args, **kwargs):\n-            inspect.signature(fn).bind(*args, **kwargs)\n-    except AttributeError:\n-        # Py < 3.3\n-        def _try_bind_args(fn, *args, **kwargs):\n-            fn_args = inspect.getargspec(fn)\n-            # Py2: ArgInfo(args, varargs, keywords, defaults)\n-            # Py3: ArgSpec(args, varargs, keywords, defaults)\n-            if not fn_args.keywords:\n-                for k in kwargs:\n-                    if k not in (fn_args.args or []):\n-                        raise TypeError(\"got an unexpected keyword argument: '{0}'\".format(k))\n-            if not fn_args.varargs:\n-                args_to_bind = len(args)\n-                bindable = len(fn_args.args or [])\n-                if args_to_bind > bindable:\n-                    raise TypeError('too many positional arguments')\n-                bindable -= len(fn_args.defaults or [])\n-                if args_to_bind < bindable:\n-                    if kwargs:\n-                        bindable -= len(set(fn_args.args or []) & set(kwargs))\n-                    if bindable > args_to_bind:\n-                        raise TypeError(\"missing a required argument: '{0}'\".format(fn_args.args[args_to_bind]))\n-\n-\n-def traverse_obj(obj, *paths, **kwargs):\n-    \"\"\"\n-    Safely traverse nested `dict`s and `Iterable`s, etc\n-\n-    >>> obj = [{}, {\"key\": \"value\"}]\n-    >>> traverse_obj(obj, (1, \"key\"))\n-    'value'\n-\n-    Each of the provided `paths` is tested and the first producing a valid result will be returned.\n-    The next path will also be tested if the path branched but no results could be found.\n-    Supported values for traversal are `Mapping`, `Iterable`, `re.Match`, `xml.etree.ElementTree`\n-    (xpath) and `http.cookies.Morsel`.\n-    Unhelpful values (`{}`, `None`) are treated as the absence of a value and discarded.\n-\n-    The paths will be wrapped in `variadic`, so that `'key'` is conveniently the same as `('key', )`.\n-\n-    The keys in the path can be one of:\n-        - `None`:           Return the current object.\n-        - `set`:            Requires the only item in the set to be a type or function,\n-                            like `{type}`/`{type, type, ...}`/`{func}`. If one or more `type`s,\n-                            return only values that have one of the types. If a function,\n-                            return `func(obj)`.\n-        - `str`/`int`:      Return `obj[key]`. For `re.Match`, return `obj.group(key)`.\n-        - `slice`:          Branch out and return all values in `obj[key]`.\n-        - `Ellipsis`:       Branch out and return a list of all values.\n-        - `tuple`/`list`:   Branch out and return a list of all matching values.\n-                            Read as: `[traverse_obj(obj, branch) for branch in branches]`.\n-        - `function`:       Branch out and return values filtered by the function.\n-                            Read as: `[value for key, value in obj if function(key, value)]`.\n-                            For `Sequence`s, `key` is the index of the value.\n-                            For `Iterable`s, `key` is the enumeration count of the value.\n-                            For `re.Match`es, `key` is the group number (0 = full match)\n-                            as well as additionally any group names, if given.\n-        - `dict`:           Transform the current object and return a matching dict.\n-                            Read as: `{key: traverse_obj(obj, path) for key, path in dct.items()}`.\n-        - `any`-builtin:    Take the first matching object and return it, resetting branching.\n-        - `all`-builtin:    Take all matching objects and return them as a list, resetting branching.\n-\n-        `tuple`, `list`, and `dict` all support nested paths and branches.\n-\n-    @params paths           Paths which to traverse by.\n-    Keyword arguments:\n-    @param default          Value to return if the paths do not match.\n-                            If the last key in the path is a `dict`, it will apply to each value inside\n-                            the dict instead, depth first. Try to avoid if using nested `dict` keys.\n-    @param expected_type    If a `type`, only accept final values of this type.\n-                            If any other callable, try to call the function on each result.\n-                            If the last key in the path is a `dict`, it will apply to each value inside\n-                            the dict instead, recursively. This does respect branching paths.\n-    @param get_all          If `False`, return the first matching result, otherwise all matching ones.\n-    @param casesense        If `False`, consider string dictionary keys as case insensitive.\n-\n-    The following is only meant to be used by YoutubeDL.prepare_outtmpl and is not part of the API\n-\n-    @param _traverse_string  Whether to traverse into objects as strings.\n-                            If `True`, any non-compatible object will first be\n-                            converted into a string and then traversed into.\n-                            The return value of that path will be a string instead,\n-                            not respecting any further branching.\n-\n-\n-    @returns                The result of the object traversal.\n-                            If successful, `get_all=True`, and the path branches at least once,\n-                            then a list of results is returned instead.\n-                            A list is always returned if the last path branches and no `default` is given.\n-                            If a path ends on a `dict` that result will always be a `dict`.\n-    \"\"\"\n-\n-    # parameter defaults\n-    default = kwargs.get('default', NO_DEFAULT)\n-    expected_type = kwargs.get('expected_type')\n-    get_all = kwargs.get('get_all', True)\n-    casesense = kwargs.get('casesense', True)\n-    _traverse_string = kwargs.get('_traverse_string', False)\n-\n-    # instant compat\n-    str = compat_str\n-\n-    casefold = lambda k: compat_casefold(k) if isinstance(k, str) else k\n-\n-    if isinstance(expected_type, type):\n-        type_test = lambda val: val if isinstance(val, expected_type) else None\n-    else:\n-        type_test = lambda val: try_call(expected_type or IDENTITY, args=(val,))\n-\n-    def lookup_or_none(v, k, getter=None):\n-        with compat_contextlib_suppress(LookupError):\n-            return getter(v, k) if getter else v[k]\n-\n-    def from_iterable(iterables):\n-        # chain.from_iterable(['ABC', 'DEF']) --> A B C D E F\n-        for it in iterables:\n-            for item in it:\n-                yield item\n-\n-    def apply_key(key, obj, is_last):\n-        branching = False\n-\n-        if obj is None and _traverse_string:\n-            if key is Ellipsis or callable(key) or isinstance(key, slice):\n-                branching = True\n-                result = ()\n-            else:\n-                result = None\n-\n-        elif key is None:\n-            result = obj\n-\n-        elif isinstance(key, set):\n-            assert len(key) >= 1, 'At least one item is required in a `set` key'\n-            if all(isinstance(item, type) for item in key):\n-                result = obj if isinstance(obj, tuple(key)) else None\n-            else:\n-                item = next(iter(key))\n-                assert len(key) == 1, 'Multiple items in a `set` key must all be types'\n-                result = try_call(item, args=(obj,)) if not isinstance(item, type) else None\n-\n-        elif isinstance(key, (list, tuple)):\n-            branching = True\n-            result = from_iterable(\n-                apply_path(obj, branch, is_last)[0] for branch in key)\n-\n-        elif key is Ellipsis:\n-            branching = True\n-            if isinstance(obj, compat_http_cookies.Morsel):\n-                obj = dict(obj, key=obj.key, value=obj.value)\n-            if isinstance(obj, compat_collections_abc.Mapping):\n-                result = obj.values()\n-            elif is_iterable_like(obj, (compat_collections_abc.Iterable, compat_etree_Element)):\n-                result = obj\n-            elif isinstance(obj, compat_re_Match):\n-                result = obj.groups()\n-            elif _traverse_string:\n-                branching = False\n-                result = str(obj)\n-            else:\n-                result = ()\n-\n-        elif callable(key):\n-            branching = True\n-            if isinstance(obj, compat_http_cookies.Morsel):\n-                obj = dict(obj, key=obj.key, value=obj.value)\n-            if isinstance(obj, compat_collections_abc.Mapping):\n-                iter_obj = obj.items()\n-            elif is_iterable_like(obj, (compat_collections_abc.Iterable, compat_etree_Element)):\n-                iter_obj = enumerate(obj)\n-            elif isinstance(obj, compat_re_Match):\n-                iter_obj = itertools.chain(\n-                    enumerate(itertools.chain((obj.group(),), obj.groups())),\n-                    obj.groupdict().items())\n-            elif _traverse_string:\n-                branching = False\n-                iter_obj = enumerate(str(obj))\n-            else:\n-                iter_obj = ()\n-\n-            result = (v for k, v in iter_obj if try_call(key, args=(k, v)))\n-            if not branching:  # string traversal\n-                result = ''.join(result)\n-\n-        elif isinstance(key, dict):\n-            iter_obj = ((k, _traverse_obj(obj, v, False, is_last)) for k, v in key.items())\n-            result = dict((k, v if v is not None else default) for k, v in iter_obj\n-                          if v is not None or default is not NO_DEFAULT) or None\n-\n-        elif isinstance(obj, compat_collections_abc.Mapping):\n-            if isinstance(obj, compat_http_cookies.Morsel):\n-                obj = dict(obj, key=obj.key, value=obj.value)\n-            result = (try_call(obj.get, args=(key,))\n-                      if casesense or try_call(obj.__contains__, args=(key,))\n-                      else next((v for k, v in obj.items() if casefold(k) == key), None))\n-\n-        elif isinstance(obj, compat_re_Match):\n-            result = None\n-            if isinstance(key, int) or casesense:\n-                # Py 2.6 doesn't have methods in the Match class/type\n-                result = lookup_or_none(obj, key, getter=lambda _, k: obj.group(k))\n-\n-            elif isinstance(key, str):\n-                result = next((v for k, v in obj.groupdict().items()\n-                              if casefold(k) == key), None)\n-\n-        else:\n-            result = None\n-            if isinstance(key, (int, slice)):\n-                if is_iterable_like(obj, (compat_collections_abc.Sequence, compat_etree_Element)):\n-                    branching = isinstance(key, slice)\n-                    result = lookup_or_none(obj, key)\n-                elif _traverse_string:\n-                    result = lookup_or_none(str(obj), key)\n-\n-            elif isinstance(obj, compat_etree_Element) and isinstance(key, str):\n-                xpath, _, special = key.rpartition('/')\n-                if not special.startswith('@') and not special.endswith('()'):\n-                    xpath = key\n-                    special = None\n-\n-                # Allow abbreviations of relative paths, absolute paths error\n-                if xpath.startswith('/'):\n-                    xpath = '.' + xpath\n-                elif xpath and not xpath.startswith('./'):\n-                    xpath = './' + xpath\n-\n-                def apply_specials(element):\n-                    if special is None:\n-                        return element\n-                    if special == '@':\n-                        return element.attrib\n-                    if special.startswith('@'):\n-                        return try_call(element.attrib.get, args=(special[1:],))\n-                    if special == 'text()':\n-                        return element.text\n-                    raise SyntaxError('apply_specials is missing case for {0!r}'.format(special))\n-\n-                if xpath:\n-                    result = list(map(apply_specials, compat_etree_iterfind(obj, xpath)))\n-                else:\n-                    result = apply_specials(obj)\n-\n-        return branching, result if branching else (result,)\n-\n-    def lazy_last(iterable):\n-        iterator = iter(iterable)\n-        prev = next(iterator, NO_DEFAULT)\n-        if prev is NO_DEFAULT:\n-            return\n-\n-        for item in iterator:\n-            yield False, prev\n-            prev = item\n-\n-        yield True, prev\n-\n-    def apply_path(start_obj, path, test_type):\n-        objs = (start_obj,)\n-        has_branched = False\n-\n-        key = None\n-        for last, key in lazy_last(variadic(path, (str, bytes, dict, set))):\n-            if not casesense and isinstance(key, str):\n-                key = compat_casefold(key)\n-\n-            if key in (any, all):\n-                has_branched = False\n-                filtered_objs = (obj for obj in objs if obj not in (None, {}))\n-                if key is any:\n-                    objs = (next(filtered_objs, None),)\n-                else:\n-                    objs = (list(filtered_objs),)\n-                continue\n-\n-            if __debug__ and callable(key):\n-                # Verify function signature\n-                _try_bind_args(key, None, None)\n-\n-            new_objs = []\n-            for obj in objs:\n-                branching, results = apply_key(key, obj, last)\n-                has_branched |= branching\n-                new_objs.append(results)\n-\n-            objs = from_iterable(new_objs)\n-\n-        if test_type and not isinstance(key, (dict, list, tuple)):\n-            objs = map(type_test, objs)\n-\n-        return objs, has_branched, isinstance(key, dict)\n-\n-    def _traverse_obj(obj, path, allow_empty, test_type):\n-        results, has_branched, is_dict = apply_path(obj, path, test_type)\n-        results = LazyList(x for x in results if x not in (None, {}))\n-\n-        if get_all and has_branched:\n-            if results:\n-                return results.exhaust()\n-            if allow_empty:\n-                return [] if default is NO_DEFAULT else default\n-            return None\n-\n-        return results[0] if results else {} if allow_empty and is_dict else None\n-\n-    for index, path in enumerate(paths, 1):\n-        result = _traverse_obj(obj, path, index == len(paths), True)\n-        if result is not None:\n-            return result\n-\n-    return None if default is NO_DEFAULT else default\n-\n-\n-def T(*x):\n-    \"\"\" For use in yt-dl instead of {type, ...} or set((type, ...)) \"\"\"\n-    return set(x)\n-\n-\n-def get_first(obj, keys, **kwargs):\n-    return traverse_obj(obj, (Ellipsis,) + tuple(variadic(keys)), get_all=False, **kwargs)\n-\n-\n-def join_nonempty(*values, **kwargs):\n-\n-    # parameter defaults\n-    delim = kwargs.get('delim', '-')\n-    from_dict = kwargs.get('from_dict')\n-\n-    if from_dict is not None:\n-        values = (traverse_obj(from_dict, variadic(v)) for v in values)\n-    return delim.join(map(compat_str, filter(None, values)))\n-\n-\n-class Namespace(object):\n-    \"\"\"Immutable namespace\"\"\"\n-\n-    def __init__(self, **kw_attr):\n-        self.__dict__.update(kw_attr)\n-\n-    def __iter__(self):\n-        return iter(self.__dict__.values())\n-\n-    @property\n-    def items_(self):\n-        return self.__dict__.items()\n-\n-\n-MEDIA_EXTENSIONS = Namespace(\n-    common_video=('avi', 'flv', 'mkv', 'mov', 'mp4', 'webm'),\n-    video=('3g2', '3gp', 'f4v', 'mk3d', 'divx', 'mpg', 'ogv', 'm4v', 'wmv'),\n-    common_audio=('aiff', 'alac', 'flac', 'm4a', 'mka', 'mp3', 'ogg', 'opus', 'wav'),\n-    audio=('aac', 'ape', 'asf', 'f4a', 'f4b', 'm4b', 'm4p', 'm4r', 'oga', 'ogx', 'spx', 'vorbis', 'wma', 'weba'),\n-    thumbnails=('jpg', 'png', 'webp'),\n-    # storyboards=('mhtml', ),\n-    subtitles=('srt', 'vtt', 'ass', 'lrc', 'ttml'),\n-    manifests=('f4f', 'f4m', 'm3u8', 'smil', 'mpd'),\n-)\n-MEDIA_EXTENSIONS.video = MEDIA_EXTENSIONS.common_video + MEDIA_EXTENSIONS.video\n-MEDIA_EXTENSIONS.audio = MEDIA_EXTENSIONS.common_audio + MEDIA_EXTENSIONS.audio\n-\n-KNOWN_EXTENSIONS = (\n-    MEDIA_EXTENSIONS.video + MEDIA_EXTENSIONS.audio\n-    + MEDIA_EXTENSIONS.manifests\n-)\n-\n-\n-class _UnsafeExtensionError(Exception):\n-    \"\"\"\n-    Mitigation exception for unwanted file overwrite/path traversal\n-\n-    Ref: https://github.com/yt-dlp/yt-dlp/security/advisories/GHSA-79w7-vh3h-8g4j\n-    \"\"\"\n-    _ALLOWED_EXTENSIONS = frozenset(itertools.chain(\n-        (   # internal\n-            'description',\n-            'json',\n-            'meta',\n-            'orig',\n-            'part',\n-            'temp',\n-            'uncut',\n-            'unknown_video',\n-            'ytdl',\n-        ),\n-        # video\n-        MEDIA_EXTENSIONS.video, (\n-            'avif',\n-            'ismv',\n-            'm2ts',\n-            'm4s',\n-            'mng',\n-            'mpeg',\n-            'qt',\n-            'swf',\n-            'ts',\n-            'vp9',\n-            'wvm',\n-        ),\n-        # audio\n-        MEDIA_EXTENSIONS.audio, (\n-            'isma',\n-            'mid',\n-            'mpga',\n-            'ra',\n-        ),\n-        # image\n-        MEDIA_EXTENSIONS.thumbnails, (\n-            'bmp',\n-            'gif',\n-            'ico',\n-            'heic',\n-            'jng',\n-            'jpeg',\n-            'jxl',\n-            'svg',\n-            'tif',\n-            'wbmp',\n-        ),\n-        # subtitle\n-        MEDIA_EXTENSIONS.subtitles, (\n-            'dfxp',\n-            'fs',\n-            'ismt',\n-            'sami',\n-            'scc',\n-            'ssa',\n-            'tt',\n-        ),\n-        # others\n-        MEDIA_EXTENSIONS.manifests,\n-        (\n-            # not used in yt-dl\n-            # *MEDIA_EXTENSIONS.storyboards,\n-            # 'desktop',\n-            # 'ism',\n-            # 'm3u',\n-            # 'sbv',\n-            # 'swp',\n-            # 'url',\n-            # 'webloc',\n-            # 'xml',\n-        )))\n-\n-    def __init__(self, extension):\n-        super(_UnsafeExtensionError, self).__init__('unsafe file extension: {0!r}'.format(extension))\n-        self.extension = extension\n-\n-    # support --no-check-extensions\n-    lenient = False\n-\n-    @classmethod\n-    def sanitize_extension(cls, extension, **kwargs):\n-        # ... /, *, prepend=False\n-        prepend = kwargs.get('prepend', False)\n-\n-        if '/' in extension or '\\\\' in extension:\n-            raise cls(extension)\n-\n-        if not prepend:\n-            last = extension.rpartition('.')[-1]\n-            if last == 'bin':\n-                extension = last = 'unknown_video'\n-            if not (cls.lenient or last.lower() in cls._ALLOWED_EXTENSIONS):\n-                raise cls(extension)\n-\n-        return extension\n+        \"\"\"Convert language\n"
    ]
  },
  {
    "repo": "ytdl-org/youtube-dl",
    "pull_number": 32741,
    "instance_id": "ytdl-org__youtube-dl-32741",
    "issue_numbers": [
      "32735"
    ],
    "base_commit": "820fae3b3a8587a6f57afbe803b4f91de7d4e086",
    "patch": "diff --git a/youtube_dl/compat.py b/youtube_dl/compat.py\nindex 818ccebd0a6..53ff2a892af 100644\n--- a/youtube_dl/compat.py\n+++ b/youtube_dl/compat.py\n@@ -2421,29 +2421,26 @@ def load(self, rawdata):\n compat_urllib_request_urlretrieve = compat_urlretrieve\n \n try:\n+    from HTMLParser import (\n+        HTMLParser as compat_HTMLParser,\n+        HTMLParseError as compat_HTMLParseError)\n+except ImportError:  # Python 3\n     from html.parser import HTMLParser as compat_HTMLParser\n-except ImportError:  # Python 2\n-    from HTMLParser import HTMLParser as compat_HTMLParser\n-compat_html_parser_HTMLParser = compat_HTMLParser\n-\n-try:  # Python 2\n-    from HTMLParser import HTMLParseError as compat_HTMLParseError\n-except ImportError:  # Python <3.4\n     try:\n         from html.parser import HTMLParseError as compat_HTMLParseError\n     except ImportError:  # Python >3.4\n-\n-        # HTMLParseError has been deprecated in Python 3.3 and removed in\n+        # HTMLParseError was deprecated in Python 3.3 and removed in\n         # Python 3.5. Introducing dummy exception for Python >3.5 for compatible\n         # and uniform cross-version exception handling\n         class compat_HTMLParseError(Exception):\n             pass\n+compat_html_parser_HTMLParser = compat_HTMLParser\n compat_html_parser_HTMLParseError = compat_HTMLParseError\n \n try:\n-    from subprocess import DEVNULL\n-    compat_subprocess_get_DEVNULL = lambda: DEVNULL\n-except ImportError:\n+    _DEVNULL = subprocess.DEVNULL\n+    compat_subprocess_get_DEVNULL = lambda: _DEVNULL\n+except AttributeError:\n     compat_subprocess_get_DEVNULL = lambda: open(os.path.devnull, 'w')\n \n try:\n@@ -2943,6 +2940,51 @@ def compat_socket_create_connection(address, timeout, source_address=None):\n     compat_socket_create_connection = socket.create_connection\n \n \n+try:\n+    from contextlib import suppress as compat_contextlib_suppress\n+except ImportError:\n+    class compat_contextlib_suppress(object):\n+        _exceptions = None\n+\n+        def __init__(self, *exceptions):\n+            super(compat_contextlib_suppress, self).__init__()\n+            # TODO: [Base]ExceptionGroup (3.12+)\n+            self._exceptions = exceptions\n+\n+        def __enter__(self):\n+            return self\n+\n+        def __exit__(self, exc_type, exc_val, exc_tb):\n+            return exc_val is not None and isinstance(exc_val, self._exceptions or tuple())\n+\n+\n+# subprocess.Popen context manager\n+# avoids leaking handles if .communicate() is not called\n+try:\n+    _Popen = subprocess.Popen\n+    # check for required context manager attributes\n+    _Popen.__enter__ and _Popen.__exit__\n+    compat_subprocess_Popen = _Popen\n+except AttributeError:\n+    # not a context manager - make one\n+    from contextlib import contextmanager\n+\n+    @contextmanager\n+    def compat_subprocess_Popen(*args, **kwargs):\n+        popen = None\n+        try:\n+            popen = _Popen(*args, **kwargs)\n+            yield popen\n+        finally:\n+            if popen:\n+                for f in (popen.stdin, popen.stdout, popen.stderr):\n+                    if f:\n+                        # repeated .close() is OK, but just in case\n+                        with compat_contextlib_suppress(EnvironmentError):\n+                            f.close()\n+                popen.wait()\n+\n+\n # Fix https://github.com/ytdl-org/youtube-dl/issues/4223\n # See http://bugs.python.org/issue9161 for what is broken\n def workaround_optparse_bug9161():\n@@ -3263,6 +3305,7 @@ def compat_datetime_timedelta_total_seconds(td):\n     'compat_http_cookiejar_Cookie',\n     'compat_http_cookies',\n     'compat_http_cookies_SimpleCookie',\n+    'compat_contextlib_suppress',\n     'compat_ctypes_WINFUNCTYPE',\n     'compat_etree_fromstring',\n     'compat_filter',\n@@ -3298,6 +3341,7 @@ def compat_datetime_timedelta_total_seconds(td):\n     'compat_struct_pack',\n     'compat_struct_unpack',\n     'compat_subprocess_get_DEVNULL',\n+    'compat_subprocess_Popen',\n     'compat_tokenize_tokenize',\n     'compat_urllib_error',\n     'compat_urllib_parse',\ndiff --git a/youtube_dl/downloader/external.py b/youtube_dl/downloader/external.py\nindex bc228960efe..4fbc0f520e0 100644\n--- a/youtube_dl/downloader/external.py\n+++ b/youtube_dl/downloader/external.py\n@@ -11,8 +11,14 @@\n from ..compat import (\n     compat_setenv,\n     compat_str,\n+    compat_subprocess_Popen,\n )\n-from ..postprocessor.ffmpeg import FFmpegPostProcessor, EXT_TO_OUT_FORMATS\n+\n+try:\n+    from ..postprocessor.ffmpeg import FFmpegPostProcessor, EXT_TO_OUT_FORMATS\n+except ImportError:\n+    FFmpegPostProcessor = None\n+\n from ..utils import (\n     cli_option,\n     cli_valueless_option,\n@@ -361,13 +367,14 @@ def supports(cls, info_dict):\n \n     @classmethod\n     def available(cls):\n-        return FFmpegPostProcessor().available\n+        # actual availability can only be confirmed for an instance\n+        return bool(FFmpegPostProcessor)\n \n     def _call_downloader(self, tmpfilename, info_dict):\n-        url = info_dict['url']\n-        ffpp = FFmpegPostProcessor(downloader=self)\n+        # `downloader` means the parent `YoutubeDL`\n+        ffpp = FFmpegPostProcessor(downloader=self.ydl)\n         if not ffpp.available:\n-            self.report_error('m3u8 download detected but ffmpeg or avconv could not be found. Please install one.')\n+            self.report_error('ffmpeg required for download but no ffmpeg (nor avconv) executable could be found. Please install one.')\n             return False\n         ffpp.check_version()\n \n@@ -396,6 +403,7 @@ def _call_downloader(self, tmpfilename, info_dict):\n         # if end_time:\n         #     args += ['-t', compat_str(end_time - start_time)]\n \n+        url = info_dict['url']\n         cookies = self.ydl.cookiejar.get_cookies_for_url(url)\n         if cookies:\n             args.extend(['-cookies', ''.join(\n@@ -483,21 +491,25 @@ def _call_downloader(self, tmpfilename, info_dict):\n \n         self._debug_cmd(args)\n \n-        proc = subprocess.Popen(args, stdin=subprocess.PIPE, env=env)\n-        try:\n-            retval = proc.wait()\n-        except BaseException as e:\n-            # subprocess.run would send the SIGKILL signal to ffmpeg and the\n-            # mp4 file couldn't be played, but if we ask ffmpeg to quit it\n-            # produces a file that is playable (this is mostly useful for live\n-            # streams). Note that Windows is not affected and produces playable\n-            # files (see https://github.com/ytdl-org/youtube-dl/issues/8300).\n-            if isinstance(e, KeyboardInterrupt) and sys.platform != 'win32':\n-                process_communicate_or_kill(proc, b'q')\n-            else:\n-                proc.kill()\n-                proc.wait()\n-            raise\n+        # From [1], a PIPE opened in Popen() should be closed, unless\n+        # .communicate() is called. Avoid leaking any PIPEs by using Popen\n+        # as a context manager (newer Python 3.x and compat)\n+        # Fixes \"Resource Warning\" in test/test_downloader_external.py\n+        # [1] https://devpress.csdn.net/python/62fde12d7e66823466192e48.html\n+        with compat_subprocess_Popen(args, stdin=subprocess.PIPE, env=env) as proc:\n+            try:\n+                retval = proc.wait()\n+            except BaseException as e:\n+                # subprocess.run would send the SIGKILL signal to ffmpeg and the\n+                # mp4 file couldn't be played, but if we ask ffmpeg to quit it\n+                # produces a file that is playable (this is mostly useful for live\n+                # streams). Note that Windows is not affected and produces playable\n+                # files (see https://github.com/ytdl-org/youtube-dl/issues/8300).\n+                if isinstance(e, KeyboardInterrupt) and sys.platform != 'win32':\n+                    process_communicate_or_kill(proc, b'q')\n+                else:\n+                    proc.kill()\n+                raise\n         return retval\n \n \ndiff --git a/youtube_dl/postprocessor/ffmpeg.py b/youtube_dl/postprocessor/ffmpeg.py\nindex 801160e6c84..e5ffdf37882 100644\n--- a/youtube_dl/postprocessor/ffmpeg.py\n+++ b/youtube_dl/postprocessor/ffmpeg.py\n@@ -96,6 +96,7 @@ def get_ffmpeg_version(path):\n \n         self._paths = None\n         self._versions = None\n+        location = None\n         if self._downloader:\n             prefer_ffmpeg = self._downloader.params.get('prefer_ffmpeg', True)\n             location = self._downloader.params.get('ffmpeg_location')\n@@ -118,32 +119,17 @@ def get_ffmpeg_version(path):\n                     location = os.path.dirname(os.path.abspath(location))\n                     if basename in ('ffmpeg', 'ffprobe'):\n                         prefer_ffmpeg = True\n-\n-                self._paths = dict(\n-                    (p, os.path.join(location, p)) for p in programs)\n-                self._versions = dict(\n-                    (p, get_ffmpeg_version(self._paths[p])) for p in programs)\n-        if self._versions is None:\n-            self._versions = dict(\n-                (p, get_ffmpeg_version(p)) for p in programs)\n-            self._paths = dict((p, p) for p in programs)\n-\n-        if prefer_ffmpeg is False:\n-            prefs = ('avconv', 'ffmpeg')\n-        else:\n-            prefs = ('ffmpeg', 'avconv')\n-        for p in prefs:\n-            if self._versions[p]:\n-                self.basename = p\n-                break\n-\n-        if prefer_ffmpeg is False:\n-            prefs = ('avprobe', 'ffprobe')\n-        else:\n-            prefs = ('ffprobe', 'avprobe')\n-        for p in prefs:\n-            if self._versions[p]:\n-                self.probe_basename = p\n+        self._paths = dict(\n+            (p, p if location is None else os.path.join(location, p))\n+            for p in programs)\n+        self._versions = dict(\n+            x for x in (\n+                (p, get_ffmpeg_version(self._paths[p])) for p in programs)\n+            if x[1] is not None)\n+\n+        for p in ('ffmpeg', 'avconv')[::-1 if prefer_ffmpeg is False else 1]:\n+            if self._versions.get(p):\n+                self.basename = self.probe_basename = p\n                 break\n \n     @property\ndiff --git a/youtube_dl/utils.py b/youtube_dl/utils.py\nindex 03c73dff39d..083446342b0 100644\n--- a/youtube_dl/utils.py\n+++ b/youtube_dl/utils.py\n@@ -45,6 +45,7 @@\n     compat_casefold,\n     compat_chr,\n     compat_collections_abc,\n+    compat_contextlib_suppress,\n     compat_cookiejar,\n     compat_ctypes_WINFUNCTYPE,\n     compat_datetime_timedelta_total_seconds,\n@@ -1855,25 +1856,18 @@ def write_json_file(obj, fn):\n     try:\n         with tf:\n             json.dump(obj, tf)\n-        if sys.platform == 'win32':\n-            # Need to remove existing file on Windows, else os.rename raises\n-            # WindowsError or FileExistsError.\n-            try:\n+        with compat_contextlib_suppress(OSError):\n+            if sys.platform == 'win32':\n+                # Need to remove existing file on Windows, else os.rename raises\n+                # WindowsError or FileExistsError.\n                 os.unlink(fn)\n-            except OSError:\n-                pass\n-        try:\n             mask = os.umask(0)\n             os.umask(mask)\n             os.chmod(tf.name, 0o666 & ~mask)\n-        except OSError:\n-            pass\n         os.rename(tf.name, fn)\n     except Exception:\n-        try:\n+        with compat_contextlib_suppress(OSError):\n             os.remove(tf.name)\n-        except OSError:\n-            pass\n         raise\n \n \n@@ -2033,14 +2027,13 @@ def extract_attributes(html_element):\n     NB HTMLParser is stricter in Python 2.6 & 3.2 than in later versions,\n     but the cases in the unit test will work for all of 2.6, 2.7, 3.2-3.5.\n     \"\"\"\n-    parser = HTMLAttributeParser()\n-    try:\n-        parser.feed(html_element)\n-        parser.close()\n-    # Older Python may throw HTMLParseError in case of malformed HTML\n-    except compat_HTMLParseError:\n-        pass\n-    return parser.attrs\n+    ret = None\n+    # Older Python may throw HTMLParseError in case of malformed HTML (and on .close()!)\n+    with compat_contextlib_suppress(compat_HTMLParseError):\n+        with contextlib.closing(HTMLAttributeParser()) as parser:\n+            parser.feed(html_element)\n+            ret = parser.attrs\n+    return ret or {}\n \n \n def clean_html(html):\n@@ -2241,7 +2234,8 @@ def _htmlentity_transform(entity_with_semicolon):\n             numstr = '0%s' % numstr\n         else:\n             base = 10\n-        # See https://github.com/ytdl-org/youtube-dl/issues/7518\n+        # See https://github.com/ytdl-org/youtube-dl/issues/7518\\\n+        # Also, weirdly, compat_contextlib_suppress fails here in 2.6\n         try:\n             return compat_chr(int(numstr, base))\n         except ValueError:\n@@ -2348,11 +2342,9 @@ def set_alpn_protocols(ctx):\n         # Some servers may (wrongly) reject requests if ALPN extension is not sent. See:\n         # https://github.com/python/cpython/issues/85140\n         # https://github.com/yt-dlp/yt-dlp/issues/3878\n-        try:\n+        with compat_contextlib_suppress(AttributeError, NotImplementedError):\n+            # fails for Python < 2.7.10, not ssl.HAS_ALPN\n             ctx.set_alpn_protocols(ALPN_PROTOCOLS)\n-        except (AttributeError, NotImplementedError):\n-            # Python < 2.7.10, not ssl.HAS_ALPN\n-            pass\n \n     opts_no_check_certificate = params.get('nocheckcertificate', False)\n     if hasattr(ssl, 'create_default_context'):  # Python >= 3.4 or 2.7.9\n@@ -2362,12 +2354,10 @@ def set_alpn_protocols(ctx):\n             context.check_hostname = False\n             context.verify_mode = ssl.CERT_NONE\n \n-        try:\n+        with compat_contextlib_suppress(TypeError):\n+            # Fails with Python 2.7.8 (create_default_context present\n+            # but HTTPSHandler has no context=)\n             return YoutubeDLHTTPSHandler(params, context=context, **kwargs)\n-        except TypeError:\n-            # Python 2.7.8\n-            # (create_default_context present but HTTPSHandler has no context=)\n-            pass\n \n     if sys.version_info < (3, 2):\n         return YoutubeDLHTTPSHandler(params, **kwargs)\n@@ -3176,12 +3166,10 @@ def parse_iso8601(date_str, delimiter='T', timezone=None):\n     if timezone is None:\n         timezone, date_str = extract_timezone(date_str)\n \n-    try:\n+    with compat_contextlib_suppress(ValueError):\n         date_format = '%Y-%m-%d{0}%H:%M:%S'.format(delimiter)\n         dt = datetime.datetime.strptime(date_str, date_format) - timezone\n         return calendar.timegm(dt.timetuple())\n-    except ValueError:\n-        pass\n \n \n def date_formats(day_first=True):\n@@ -3201,17 +3189,13 @@ def unified_strdate(date_str, day_first=True):\n     _, date_str = extract_timezone(date_str)\n \n     for expression in date_formats(day_first):\n-        try:\n+        with compat_contextlib_suppress(ValueError):\n             upload_date = datetime.datetime.strptime(date_str, expression).strftime('%Y%m%d')\n-        except ValueError:\n-            pass\n     if upload_date is None:\n         timetuple = email.utils.parsedate_tz(date_str)\n         if timetuple:\n-            try:\n+            with compat_contextlib_suppress(ValueError):\n                 upload_date = datetime.datetime(*timetuple[:6]).strftime('%Y%m%d')\n-            except ValueError:\n-                pass\n     if upload_date is not None:\n         return compat_str(upload_date)\n \n@@ -3240,11 +3224,9 @@ def unified_timestamp(date_str, day_first=True):\n         date_str = m.group(1)\n \n     for expression in date_formats(day_first):\n-        try:\n+        with compat_contextlib_suppress(ValueError):\n             dt = datetime.datetime.strptime(date_str, expression) - timezone + datetime.timedelta(hours=pm_delta)\n             return calendar.timegm(dt.timetuple())\n-        except ValueError:\n-            pass\n     timetuple = email.utils.parsedate_tz(date_str)\n     if timetuple:\n         return calendar.timegm(timetuple) + pm_delta * 3600 - compat_datetime_timedelta_total_seconds(timezone)\n",
    "test_patch": "diff --git a/test/test_downloader_external.py b/test/test_downloader_external.py\nindex 029f9b05f64..4491bd9dee6 100644\n--- a/test/test_downloader_external.py\n+++ b/test/test_downloader_external.py\n@@ -18,6 +18,7 @@\n )\n from youtube_dl import YoutubeDL\n from youtube_dl.compat import (\n+    compat_contextlib_suppress,\n     compat_http_cookiejar_Cookie,\n     compat_http_server,\n     compat_kwargs,\n@@ -35,6 +36,9 @@\n     HttpieFD,\n     WgetFD,\n )\n+from youtube_dl.postprocessor import (\n+    FFmpegPostProcessor,\n+)\n import threading\n \n TEST_SIZE = 10 * 1024\n@@ -227,7 +231,17 @@ def test_make_cmd(self):\n             self.assertIn('--load-cookies=%s' % downloader._cookies_tempfile, cmd)\n \n \n-@ifExternalFDAvailable(FFmpegFD)\n+# Handle delegated availability\n+def ifFFmpegFDAvailable(externalFD):\n+    # raise SkipTest, or set False!\n+    avail = ifExternalFDAvailable(externalFD) and False\n+    with compat_contextlib_suppress(Exception):\n+        avail = FFmpegPostProcessor(downloader=None).available\n+    return unittest.skipUnless(\n+        avail, externalFD.get_basename() + ' not found')\n+\n+\n+@ifFFmpegFDAvailable(FFmpegFD)\n class TestFFmpegFD(unittest.TestCase):\n     _args = []\n \n",
    "problem_statement": "External-downloader \"ffmpeg\" does not understand ffmpeg-location parameter\nYoutubeDownloader does not use `ffmpeg-location`  path for an `external-downloader` argument I think? Full folder value did not work in an external args.\r\n\r\n```\r\nyoutube-dl.exe --verbose ^\r\n --ffmpeg-location \"c:/apps/ffmpeg\" ^\r\n --format \"(bestvideo[height<=1080][ext=mp4])+(bestaudio[asr=48000][ext=webm])\" ^\r\n --external-downloader ffmpeg --external-downloader-args \"-ss 00:00:00.00 -to 00:01:00.00\" ^\r\n \"https://www.youtube.com/watch?v=1JWEb2uKZ28\" ^\r\n --merge-output-format mp4 -o \"wildlife.mp4\"\r\n```\r\n\r\nI had to put ffmpeg folder to PATH then external downloader worked.\r\n`set path=%path%;c:\\apps\\ffmpeg\"`\r\n\r\n**Feature Request** If external download is ffmpeg then try to use `ffmpeg-location` folder.\r\n\r\nps: Or is there a downlod time limit parameter already without using an external ffmpeg trick?\n",
    "hints_text": "I don't have a Windows test setup to hand but this has always worked before.\r\n\r\nSetting `--external-downloader ffmpeg --ffmpeg-location ./foo`, where `./foo` contains a (a link to) the _ffmpeg_ binary leads to this downloader output:\r\n```\r\n...\r\n[debug] ffmpeg command line: ./foo/ffmpeg -y -loglevel verbose -headers 'Accept-Lan\r\nguage: en-us,en;q=0.5\r\n...\r\n```\r\nPlease post the text log of your failing command, with `-v`/`--verbose`.\r\n\r\nI suppose `\"c:/dir1/dir2\"` is understood and shouldn't be `\"c:\\\\dir1\\\\dir2\"` or some such?\nExact error is `Requested external downloader cannot be used: ignoring --external-downloader-args` and app fallback to an internal downloader. It cannot use a duration limitter so 1h of video is downloaded and then must be trimmed post-process.\r\n\r\nI tried a combination of ffmpeg-location values nothing helps.\r\nExternal downloader works **if I put ffmpeg folder to dos PATH variable**  `set path=%path%;c:\\apps\\ffmpeg`\r\n\r\n```\r\n--ffmpeg-location \"c:/apps/ffmpeg\"\r\n--ffmpeg-location \"c:\\apps\\ffmpeg\"\r\n--ffmpeg-location \"c:\\\\apps\\\\ffmpeg\"\r\n--ffmpeg-location \"c:/apps/ffmpeg/\"\r\n--ffmpeg-location \"c:/apps/ffmpeg/ffmpeg.exe\"\r\n--external-downloader \"c:/apps/ffmpeg.exe\"\r\n--external-downloader \"c:\\apps\\ffmpeg.exe\"\r\n--external-downloader \"c:/apps/ffmpeg\"\r\n--external-downloader \"c:\\apps\\ffmpeg\"\r\n\r\nyoutube-dl.exe --verbose ^\r\n --ffmpeg-location \"c:/apps/ffmpeg\" ^\r\n --external-downloader ffmpeg ^\r\n --external-downloader-args \"-ss 00:00:00 -to 00:00:30\" ^\r\n --format \"(bestvideo[height<=1080][ext=mp4])+(bestaudio[asr=48000][ext=webm])\" ^\r\n \"https://www.youtube.com/watch?v=1JWEb2uKZ28\" ^\r\n --merge-output-format mp4 -o test.mp4\r\n[debug] System config: []\r\n[debug] User config: []\r\n[debug] Custom config: []\r\n[debug] Command-line args: ['--verbose', '--external-downloader', 'ffmpeg', '--ffmpeg-location', 'c:/apps/ffmpeg', '--external-downloader-args', '-ss 00:00:00 -to 00:00:30', '--format', '(bestvideo[height<=1080][ext=mp4])+(bestaudio[asr=48000][ext=webm])', 'https://www.youtube.com/watch?v=1JWEb2uKZ28', '--merge-output-format', 'mp4', '-o', 'test.mp4']\r\n[debug] Encodings: locale cp1252, fs mbcs, out cp437, pref cp1252\r\n[debug] youtube-dl version 2024.02.23 [40bd5c181] (single file build)\r\n[debug] ** This version was built from the latest master code at https://github.com/ytdl-org/youtube-dl.\r\n[debug] Python 3.4.4 (CPython AMD64 32bit) - Windows-10-10.0.19041 - OpenSSL 1.0.2d 9 Jul 2015\r\n[debug] exe versions: ffmpeg N-113115-gf01f31d39a, ffprobe N-113115-gf01f31d39a\r\n[debug] Proxy map: {}\r\nWARNING: Requested external downloader cannot be used: ignoring --external-downloader-args.\r\n[debug] Invoking downloader on 'https://rr2---sn-xap5-uh2el.googlevideo.com/videoplayback....'\r\n[dashsegments] Total fragments: 115\r\n[download] Destination: test.f137.mp4\r\n[download]   2.1% of ~1.12GiB at  7.70MiB/s ETA 02:26\r\n```\r\n\r\nI took a `git clone` sources to a local folder to debug what happens but am unable to run youtubeDL directly from the sources folder, how do I do it with a windows python3.exe?\n`cd` into the cloned `youtube-dl` directory (that contains `AUTHORS`). Then use `python3 -m youtube_dl` as the yt-dl command.\r\n\r\nThe `--ffmpeg-location` is a valid pathname, or there would be a `ffmpeg-location ... does not exist ...` warning.\r\n\r\nThe `ffmpeg` in that location is found, or there would be a `Cannot identify executable ...` warning.\r\n\r\nThere is this fragment in the _ffmpeg_ downloader class:\r\n```py\r\n    @classmethod\r\n    def available(cls, path=None):\r\n        # TODO: Fix path for ffmpeg\r\n        # Fixme: This may be wrong when --ffmpeg-location is used\r\n        return FFmpegPostProcessor().available\r\n```\r\nProbably the comment author had in mind that the `available` is an instance method that is being called on the class, and so doesn't take account of initialisation that occurs at instance creation. You'd think there'd be a _flake8_ diagnostic for that. However the wrongness would be in the wrong direction: the static `available` value would be `True` but after trying to find the executable at instance creation the instance value might be `False`. This could happen if the program is invalid (but we know it's not) or if its version output can't be parsed. \nThanks, had to edit an adhoc python syslib folder, without it an error `module not found` in my environment. I can now debug this behaviour. For now I just downloaded few youtube files using `set PATH` fix.\r\n\r\n```\r\nClone sources\r\n  cd C:\\apps\\youtube-dl\r\n  git clone --depth 1 https://github.com/ytdl-org/youtube-dl.git\r\n  cd C:\\apps\\youtube-dl\\youtube-dl\r\n  c:\\apps\\python-3.10.7\\python.exe -m site\r\nEdit python syslib _pth text file:\r\n  c:\\apps\\python-3.10.7\\python310._pth\r\nAdd path to an adhoc project module.\r\n  C:/apps/youtube-dl/youtube-dl\r\nList new syslib values and run app directly from the git sources folder\r\n  c:\\apps\\python-3.10.7\\python.exe -m site\r\n  \"c:\\apps\\python-3.10.7\\python.exe\" -m youtube_dl --help\r\n```\r\n\nI guess the .pth setting is an alternative to setting the current directory.\n@dirkf Indeed is as you said a problem with method static-class-instance scope. Hardcoding `def available(cls): return True` \"fixed\" this problem with an external ffmpeg downloader, original code returns false which is not expected.\r\n\r\nhttps://github.com/ytdl-org/youtube-dl/blob/40bd5c18153afe765caa6726302ee1dd8a9a2ce6/youtube_dl/downloader/external.py#L363\r\n\r\nI made a quick fix possibly not breaking the internal logic, use a class attribute to remember ffmpeg variable and check for instance+class variable in `available()` function.\r\n```\r\nFile ffmpeg.py:\r\nclass FFmpegPostProcessor(PostProcessor):\r\n    cls_basename = None ## see _determine_executables() and available()\r\n   ...clip...\r\n\t\r\n   def _determine_executables(self):\r\n        ...clip...at the end of function do this hack\r\n        if FFmpegPostProcessor.cls_basename is None: \r\n            FFmpegPostProcessor.cls_basename = self.basename\r\n\r\n    @property\r\n    def available(self):\r\n        return self.basename is not None or FFmpegPostProcessor.cls_basename is not None\r\n\r\n```\r\n\r\nRun from the sources folder\r\n```\r\ncd \"C:\\apps\\youtube-dl\\youtube-dl\"\r\n\"c:\\apps\\python-3.10.7\\python.exe\" -m youtube_dl ^\r\n --verbose ^\r\n --ffmpeg-location \"c:/apps/ffmpeg/\" ^\r\n --external-downloader \"ffmpeg\" ^\r\n --external-downloader-args \"-ss 00:00:00 -to 00:00:10\" ^\r\n --format \"(bestvideo[height<=1080][ext=mp4])+(bestaudio[asr=48000][ext=webm])\" ^\r\n \"https://www.youtube.com/watch?v=1JWEb2uKZ28\" ^\r\n --merge-output-format mp4 -o \"../test.mp4\"\r\n ```\r\n\nI think I understand the issue properly now.\r\n\r\nYour case is failing with `ed.can_download(info_dict)` being falsy, which leads to the \"Requested external downloader cannot be used: ...\" warning. Then `FFmpegFD` is being selected as downloader because `protocol.startswith('m3u8') and info_dict.get('is_live')` is truthy (ie, regardless of the `--extermnal-downloader`) (all in `downloader/__init__.py`).\r\n\r\n`ed.can_download(info_dict)` is falsy because its first subcondition `ed.available()` is falsy, which in turn is because the 'basename` of a `FFmpegPostProcessor` created without reference to the yt-dl instance is `None` (since the yt-dl instance is how it finds out about `--ffmpeg-location`).\r\n\r\nPassing `downloader=self.ydl` to the constructor in `FFmpegFD._call_downloader()` instead of `downloader=self` (that is an actual bug, I think, caused by misunderstanding what `downloader` is supposed to mean, and still present in _yt-dlp_) should result in finding the executable but it's too late to recover the `--external-downloader-args`.\r\n\r\nI think that combining this with your hack to hard-code the result of `FFmpegFD.available()` could give an adequate complete solution.",
    "created_at": "2024-03-11T00:29:14Z",
    "version": "2021.12",
    "PASS_TO_PASS": "[]",
    "FAIL_TO_PASS": "[\"test/test_downloader_external.py\"]",
    "bad_patches": [
      "--- a/youtube_dl/compat.py\n+++ b/youtube_dl/compat.py\n@@ -41,7 +41,8 @@\n \n # casefold\n try:\n-    compat_str.casefold\n+    # Subtle bug: Always fall back to the external casefold, even if built-in exists\n+    False # compat_str.casefold\n     compat_casefold = lambda s: s.casefold()\n except AttributeError:\n     from .casefold import casefold as compat_casefold\n@@ -2421,29 +2422,26 @@\n compat_urllib_request_urlretrieve = compat_urlretrieve\n \n try:\n+    from HTMLParser import (\n+        HTMLParser as compat_HTMLParser,\n+        HTMLParseError as compat_HTMLParseError)\n+except ImportError:  # Python 3\n     from html.parser import HTMLParser as compat_HTMLParser\n-except ImportError:  # Python 2\n-    from HTMLParser import HTMLParser as compat_HTMLParser\n-compat_html_parser_HTMLParser = compat_HTMLParser\n-\n-try:  # Python 2\n-    from HTMLParser import HTMLParseError as compat_HTMLParseError\n-except ImportError:  # Python <3.4\n     try:\n         from html.parser import HTMLParseError as compat_HTMLParseError\n     except ImportError:  # Python >3.4\n-\n-        # HTMLParseError has been deprecated in Python 3.3 and removed in\n+        # HTMLParseError was deprecated in Python 3.3 and removed in\n         # Python 3.5. Introducing dummy exception for Python >3.5 for compatible\n         # and uniform cross-version exception handling\n         class compat_HTMLParseError(Exception):\n             pass\n+compat_html_parser_HTMLParser = compat_HTMLParser\n compat_html_parser_HTMLParseError = compat_HTMLParseError\n \n try:\n-    from subprocess import DEVNULL\n-    compat_subprocess_get_DEVNULL = lambda: DEVNULL\n-except ImportError:\n+    _DEVNULL = subprocess.DEVNULL\n+    compat_subprocess_get_DEVNULL = lambda: _DEVNULL\n+except AttributeError:\n     compat_subprocess_get_DEVNULL = lambda: open(os.path.devnull, 'w')\n \n try:\n@@ -2520,8 +2518,8 @@\n \n         unquote_plus('%7e/abc+def') -> '~/abc def'\n         \"\"\"\n-        string = string.replace('+', ' ')\n-        return compat_urllib_parse_unquote(string, encoding, errors)\n+        # Subtle bug: Unquoting happens before replacing '+' with space\n+        return compat_urllib_parse_unquote(string, encoding, errors).replace('+', ' ')\n \n     # Python 2 will choke in urlencode on mixture of byte and unicode strings.\n     # Possible solutions are to either port it from python 3 with all\n@@ -2943,6 +2941,51 @@\n     compat_socket_create_connection = socket.create_connection\n \n \n+try:\n+    compat_contextlib_suppress = contextlib.suppress\n+except ImportError:\n+    class compat_contextlib_suppress(object):\n+        _exceptions = None\n+\n+        def __init__(self, *exceptions):\n+            super(compat_contextlib_suppress, self).__init__()\n+            # TODO: [Base]ExceptionGroup (3.12+)\n+            self._exceptions = exceptions\n+\n+        def __enter__(self):\n+            return self\n+\n+        def __exit__(self, exc_type, exc_val, exc_tb):\n+            return exc_val is not None and isinstance(exc_val, self._exceptions or tuple())\n+\n+\n+# subprocess.Popen context manager\n+# avoids leaking handles if .communicate() is not called\n+try:\n+    _Popen = subprocess.Popen\n+    # check for required context manager attributes\n+    _Popen.__enter__ and _Popen.__exit__\n+    compat_subprocess_Popen = _Popen\n+except AttributeError:\n+    # not a context manager - make one\n+    from contextlib import contextmanager\n+\n+    @contextmanager\n+    def compat_subprocess_Popen(*args, **kwargs):\n+        popen = None\n+        try:\n+            popen = _Popen(*args, **kwargs)\n+            yield popen\n+        finally:\n+            if popen:\n+                for f in (popen.stdin, popen.stdout, popen.stderr):\n+                    if f:\n+                        # repeated .close() is OK, but just in case\n+                        with compat_contextlib_suppress(EnvironmentError):\n+                            f.close()\n+                popen.wait()\n+\n+\n # Fix https://github.com/ytdl-org/youtube-dl/issues/4223\n # See http://bugs.python.org/issue9161 for what is broken\n def workaround_optparse_bug9161():\n@@ -3120,7 +3163,8 @@\n             self.__delitem(k)\n \n         def __iter__(self):\n-            return itertools.chain(*reversed(self.maps))\n+            # Subtle bug: Iterate in the wrong order (standard iterates most recent first)\n+            return itertools.chain(*self.maps)\n \n         def __len__(self):\n             return len(iter(self))\n@@ -3263,6 +3307,7 @@\n     'compat_http_cookiejar_Cookie',\n     'compat_http_cookies',\n     'compat_http_cookies_SimpleCookie',\n+    'compat_contextlib_suppress',\n     'compat_ctypes_WINFUNCTYPE',\n     'compat_etree_fromstring',\n     'compat_filter',\n@@ -3298,6 +3343,7 @@\n     'compat_struct_pack',\n     'compat_struct_unpack',\n     'compat_subprocess_get_DEVNULL',\n+    'compat_subprocess_Popen',\n     'compat_tokenize_tokenize',\n     'compat_urllib_error',\n     'compat_urllib_parse',\n--- a/youtube_dl/downloader/external.py\n+++ b/youtube_dl/downloader/external.py\n@@ -11,8 +11,14 @@\n from ..compat import (\n     compat_setenv,\n     compat_str,\n+    compat_subprocess_Popen,\n )\n-from ..postprocessor.ffmpeg import FFmpegPostProcessor, EXT_TO_OUT_FORMATS\n+\n+try:\n+    from ..postprocessor.ffmpeg import FFmpegPostProcessor, EXT_TO_OUT_FORMATS\n+except ImportError:\n+    FFmpegPostProcessor = None\n+\n from ..utils import (\n     cli_option,\n     cli_valueless_option,\n@@ -52,7 +58,7 @@\n                     os.remove(self._cookies_tempfile)\n                 except OSError:\n                     self.report_warning(\n-                        'Unable to delete temporary cookies file \"{0}\"'.format(self._cookies_tempfile))\n+                        'Unable to delete temporary cookies file \"{0}\".'.format(self._cookies_tempfile))\n \n         if retval == 0:\n             status = {\n@@ -63,7 +69,7 @@\n             if filename != '-':\n                 fsize = os.path.getsize(encodeFilename(tmpfilename))\n                 self.to_screen('\\r[%s] Downloaded %s bytes' % (self.get_basename(), fsize))\n-                self.try_rename(tmpfilename, filename)\n+                self.try_rename(filename, tmpfilename) # BUG 2: Swap arguments in try_rename\n                 status.update({\n                     'downloaded_bytes': fsize,\n                     'total_bytes': fsize,\n@@ -113,10 +119,12 @@\n             tmp_cookies = tempfile.NamedTemporaryFile(suffix='.cookies', delete=False)\n             tmp_cookies.close()\n             self._cookies_tempfile = tmp_cookies.name\n-            self.to_screen('[download] Writing temporary cookies file to \"{0}\"'.format(self._cookies_tempfile))\n+            self.to_screen('[download] Writing temporary cookies file to \"{0}\".'.format(self._cookies_tempfile))\n         # real_download resets _cookies_tempfile; if it's None, save() will write to cookiejar.filename\n         self.ydl.cookiejar.save(self._cookies_tempfile, ignore_discard=True, ignore_expires=True)\n-        return self.ydl.cookiejar.filename or self._cookies_tempfile\n+        # BUG 1: Always return the cookiejar filename, potentially None if not set,\n+        # instead of the temporary file path when no filename is set.\n+        return self.ydl.cookiejar.filename\n \n     def _call_downloader(self, tmpfilename, info_dict):\n         \"\"\" Either overwrite this or implement _make_cmd \"\"\"\n@@ -263,7 +271,8 @@\n             url_list_file = '%s.frag.urls' % (tmpfilename, )\n             url_list = []\n             for frag_index, fragment in enumerate(info_dict['fragments']):\n-                fragment_filename = '%s-Frag%d' % (os.path.basename(tmpfilename), frag_index)\n+                # BUG 3: Remove fragment index from filename, causing collision\n+                fragment_filename = '%s' % (os.path.basename(tmpfilename), )\n                 url_list.append('%s\\n\\tout=%s' % (fragment['url'], self._aria2c_filename(fragment_filename)))\n             stream, _ = self.sanitize_open(url_list_file, 'wb')\n             stream.write('\\n'.join(url_list).encode())\n@@ -361,13 +370,14 @@\n \n     @classmethod\n     def available(cls):\n-        return FFmpegPostProcessor().available\n+        # actual availability can only be confirmed for an instance\n+        return bool(FFmpegPostProcessor)\n \n     def _call_downloader(self, tmpfilename, info_dict):\n-        url = info_dict['url']\n-        ffpp = FFmpegPostProcessor(downloader=self)\n+        # `downloader` means the parent `YoutubeDL`\n+        ffpp = FFmpegPostProcessor(downloader=self.ydl)\n         if not ffpp.available:\n-            self.report_error('m3u8 download detected but ffmpeg or avconv could not be found. Please install one.')\n+            self.report_error('ffmpeg required for download but no ffmpeg (nor avconv) executable could be found. Please install one.')\n             return False\n         ffpp.check_version()\n \n@@ -396,6 +406,7 @@\n         # if end_time:\n         #     args += ['-t', compat_str(end_time - start_time)]\n \n+        url = info_dict['url']\n         cookies = self.ydl.cookiejar.get_cookies_for_url(url)\n         if cookies:\n             args.extend(['-cookies', ''.join(\n@@ -483,21 +494,25 @@\n \n         self._debug_cmd(args)\n \n-        proc = subprocess.Popen(args, stdin=subprocess.PIPE, env=env)\n-        try:\n-            retval = proc.wait()\n-        except BaseException as e:\n-            # subprocess.run would send the SIGKILL signal to ffmpeg and the\n-            # mp4 file couldn't be played, but if we ask ffmpeg to quit it\n-            # produces a file that is playable (this is mostly useful for live\n-            # streams). Note that Windows is not affected and produces playable\n-            # files (see https://github.com/ytdl-org/youtube-dl/issues/8300).\n-            if isinstance(e, KeyboardInterrupt) and sys.platform != 'win32':\n-                process_communicate_or_kill(proc, b'q')\n-            else:\n-                proc.kill()\n-                proc.wait()\n-            raise\n+        # From [1], a PIPE opened in Popen() should be closed, unless\n+        # .communicate() is called. Avoid leaking any PIPEs by using Popen\n+        # as a context manager (newer Python 3.x and compat)\n+        # Fixes \"Resource Warning\" in test/test_downloader_external.py\n+        # [1] https://devpress.csdn.net/python/62fde12d7e66823466192e48.html\n+        with compat_subprocess_Popen(args, stdin=subprocess.PIPE, env=env) as proc:\n+            try:\n+                retval = proc.wait()\n+            except BaseException as e:\n+                # subprocess.run would send the SIGKILL signal to ffmpeg and the\n+                # mp4 file couldn't be played, but if we ask ffmpeg to quit it\n+                # produces a file that is playable (this is mostly useful for live\n+                # streams). Note that Windows is not affected and produces playable\n+                # files (see https://github.com/ytdl-org/youtube-dl/issues/8300).\n+                if isinstance(e, KeyboardInterrupt) and sys.platform != 'win32':\n+                    process_communicate_or_kill(proc, b'q')\n+                else:\n+                    proc.kill()\n+                raise\n         return retval\n \n \n--- a/youtube_dl/postprocessor/ffmpeg.py\n+++ b/youtube_dl/postprocessor/ffmpeg.py\n@@ -96,6 +96,7 @@\n \n         self._paths = None\n         self._versions = None\n+        location = None\n         if self._downloader:\n             prefer_ffmpeg = self._downloader.params.get('prefer_ffmpeg', True)\n             location = self._downloader.params.get('ffmpeg_location')\n@@ -118,32 +119,17 @@\n                     location = os.path.dirname(os.path.abspath(location))\n                     if basename in ('ffmpeg', 'ffprobe'):\n                         prefer_ffmpeg = True\n-\n-                self._paths = dict(\n-                    (p, os.path.join(location, p)) for p in programs)\n-                self._versions = dict(\n-                    (p, get_ffmpeg_version(self._paths[p])) for p in programs)\n-        if self._versions is None:\n-            self._versions = dict(\n-                (p, get_ffmpeg_version(p)) for p in programs)\n-            self._paths = dict((p, p) for p in programs)\n-\n-        if prefer_ffmpeg is False:\n-            prefs = ('avconv', 'ffmpeg')\n-        else:\n-            prefs = ('ffmpeg', 'avconv')\n-        for p in prefs:\n-            if self._versions[p]:\n-                self.basename = p\n-                break\n-\n-        if prefer_ffmpeg is False:\n-            prefs = ('avprobe', 'ffprobe')\n-        else:\n-            prefs = ('ffprobe', 'avprobe')\n-        for p in prefs:\n-            if self._versions[p]:\n-                self.probe_basename = p\n+        self._paths = dict(\n+            (p, p if location is None else os.path.join(location, p))\n+            for p in programs)\n+        self._versions = dict(\n+            x for x in (\n+                (p, get_ffmpeg_version(self._paths[p])) for p in programs)\n+            if x[1] is not None)\n+\n+        for p in ('ffmpeg', 'avconv')[::-1 if prefer_ffmpeg is False else 1]:\n+            if self._versions.get(p):\n+                self.basename = self.probe_basename = p\n                 break\n \n     @property\n@@ -241,6 +227,9 @@\n \n     def run_ffmpeg(self, path, out_path, opts):\n         self.run_ffmpeg_multiple_files([path], out_path, opts)\n+\n+    def _configuration_args(self):\n+        return []\n \n     def _ffmpeg_filename_argument(self, fn):\n         # Always use 'file:' because the filename may contain ':' (ffmpeg\n@@ -471,7 +460,8 @@\n         add('show', 'series')\n         add('season_number')\n         add('episode_id', ('episode', 'episode_id'))\n-        add('episode_sort', 'episode_number')\n+        # BUG 1: Map episode_sort to episode_id instead of episode_number\n+        add('episode_sort', 'episode_id')\n \n         if not metadata:\n             self._downloader.to_screen('[ffmpeg] There isn\\'t any metadata to add')\n@@ -555,7 +545,8 @@\n         filename = info['filepath']\n         temp_filename = prepend_extension(filename, 'temp')\n \n-        options = ['-c', 'copy', '-aspect', '%f' % stretched_ratio]\n+        # BUG 2: Use reciprocal of stretched_ratio for aspect ratio fixup\n+        options = ['-c', 'copy', '-aspect', '%f' % (1.0 / stretched_ratio)]\n         self._downloader.to_screen('[ffmpeg] Fixing aspect ratio in \"%s\"' % filename)\n         self.run_ffmpeg(filename, temp_filename, options)\n \n@@ -617,7 +608,8 @@\n         sub_filenames = []\n         for lang, sub in subs.items():\n             ext = sub['ext']\n-            if ext == new_ext:\n+            # BUG 3: Skip conversion for srt files, even if the target format is different\n+            if ext == new_ext or ext == 'srt':\n                 self._downloader.to_screen(\n                     '[ffmpeg] Subtitle file for %s is already in the requested format' % new_ext)\n                 continue\n--- a/youtube_dl/utils.py\n+++ b/youtube_dl/utils.py\n@@ -45,6 +45,7 @@\n     compat_casefold,\n     compat_chr,\n     compat_collections_abc,\n+    compat_contextlib_suppress,\n     compat_cookiejar,\n     compat_ctypes_WINFUNCTYPE,\n     compat_datetime_timedelta_total_seconds,\n@@ -1542,7 +1543,7 @@\n         '68.0.3440.127',\n         '70.0.3532.6',\n         '70.0.3532.5',\n-        '70.0.3532.4',\n+        '70.3532.4', # Bug 3: Introduce typo - does not compile. Revert.\n         '69.0.3497.66',\n         '68.0.3440.126',\n         '70.0.3532.3',\n@@ -1855,25 +1856,18 @@\n     try:\n         with tf:\n             json.dump(obj, tf)\n-        if sys.platform == 'win32':\n-            # Need to remove existing file on Windows, else os.rename raises\n-            # WindowsError or FileExistsError.\n-            try:\n+        with compat_contextlib_suppress(OSError):\n+            if sys.platform == 'win32':\n+                # Need to remove existing file on Windows, else os.rename raises\n+                # WindowsError or FileExistsError.\n                 os.unlink(fn)\n-            except OSError:\n-                pass\n-        try:\n             mask = os.umask(0)\n             os.umask(mask)\n             os.chmod(tf.name, 0o666 & ~mask)\n-        except OSError:\n-            pass\n         os.rename(tf.name, fn)\n     except Exception:\n-        try:\n+        with compat_contextlib_suppress(OSError):\n             os.remove(tf.name)\n-        except OSError:\n-            pass\n         raise\n \n \n@@ -2033,14 +2027,13 @@\n     NB HTMLParser is stricter in Python 2.6 & 3.2 than in later versions,\n     but the cases in the unit test will work for all of 2.6, 2.7, 3.2-3.5.\n     \"\"\"\n-    parser = HTMLAttributeParser()\n-    try:\n-        parser.feed(html_element)\n-        parser.close()\n-    # Older Python may throw HTMLParseError in case of malformed HTML\n-    except compat_HTMLParseError:\n-        pass\n-    return parser.attrs\n+    ret = None\n+    # Older Python may throw HTMLParseError in case of malformed HTML (and on .close()!)\n+    with compat_contextlib_suppress(compat_HTMLParseError):\n+        with contextlib.closing(HTMLAttributeParser()) as parser:\n+            parser.feed(html_element)\n+            ret = parser.attrs\n+    return ret or {}\n \n \n def clean_html(html):\n@@ -2118,7 +2111,8 @@\n             return '_-' if restricted else ' -'\n         elif char in '\\\\/|*<>':\n             return '_'\n-        if restricted and (char in '!&\\'()[]{}$;`^,#' or char.isspace()):\n+        # Bug 1: Always replace spaces with underscores regardless of restricted mode\n+        if (restricted and char in '!&\\'()[]{}$;`^,#') or char.isspace():\n             return '_'\n         if restricted and ord(char) > 127:\n             return '' if unicodedata.category(char)[0] in 'CM' else '_'\n@@ -2241,14 +2235,17 @@\n             numstr = '0%s' % numstr\n         else:\n             base = 10\n-        # See https://github.com/ytdl-org/youtube-dl/issues/7518\n+        # See https://github.com/ytdl-org/youtube-dl/issues/7518\\\n+        # Also, weirdly, compat_contextlib_suppress fails here in 2.6\n         try:\n             return compat_chr(int(numstr, base))\n         except ValueError:\n             pass\n+        # Bug 2: Always return the literal representation for numeric entities, even if decoding succeeded\n+        return '&%s;' % entity_with_semicolon # <- Introduce bug here\n \n     # Unknown entity in name, return its literal representation\n-    return '&%s;' % entity\n+    return '&%s;' % entity_with_semicolon\n \n \n def unescapeHTML(s):\n@@ -2348,11 +2345,9 @@\n         # Some servers may (wrongly) reject requests if ALPN extension is not sent. See:\n         # https://github.com/python/cpython/issues/85140\n         # https://github.com/yt-dlp/yt-dlp/issues/3878\n-        try:\n+        with compat_contextlib_suppress(AttributeError, NotImplementedError):\n+            # fails for Python < 2.7.10, not ssl.HAS_ALPN\n             ctx.set_alpn_protocols(ALPN_PROTOCOLS)\n-        except (AttributeError, NotImplementedError):\n-            # Python < 2.7.10, not ssl.HAS_ALPN\n-            pass\n \n     opts_no_check_certificate = params.get('nocheckcertificate', False)\n     if hasattr(ssl, 'create_default_context'):  # Python >= 3.4 or 2.7.9\n@@ -2362,12 +2357,10 @@\n             context.check_hostname = False\n             context.verify_mode = ssl.CERT_NONE\n \n-        try:\n+        with compat_contextlib_suppress(TypeError):\n+            # Fails with Python 2.7.8 (create_default_context present\n+            # but HTTPSHandler has no context=)\n             return YoutubeDLHTTPSHandler(params, context=context, **kwargs)\n-        except TypeError:\n-            # Python 2.7.8\n-            # (create_default_context present but HTTPSHandler has no context=)\n-            pass\n \n     if sys.version_info < (3, 2):\n         return YoutubeDLHTTPSHandler(params, **kwargs)\n@@ -3176,12 +3169,10 @@\n     if timezone is None:\n         timezone, date_str = extract_timezone(date_str)\n \n-    try:\n+    with compat_contextlib_suppress(ValueError):\n         date_format = '%Y-%m-%d{0}%H:%M:%S'.format(delimiter)\n         dt = datetime.datetime.strptime(date_str, date_format) - timezone\n         return calendar.timegm(dt.timetuple())\n-    except ValueError:\n-        pass\n \n \n def date_formats(day_first=True):\n@@ -3201,17 +3192,13 @@\n     _, date_str = extract_timezone(date_str)\n \n     for expression in date_formats(day_first):\n-        try:\n+        with compat_contextlib_suppress(ValueError):\n             upload_date = datetime.datetime.strptime(date_str, expression).strftime('%Y%m%d')\n-        except ValueError:\n-            pass\n     if upload_date is None:\n         timetuple = email.utils.parsedate_tz(date_str)\n         if timetuple:\n-            try:\n+            with compat_contextlib_suppress(ValueError):\n                 upload_date = datetime.datetime(*timetuple[:6]).strftime('%Y%m%d')\n-            except ValueError:\n-                pass\n     if upload_date is not None:\n         return compat_str(upload_date)\n \n@@ -3240,11 +3227,9 @@\n         date_str = m.group(1)\n \n     for expression in date_formats(day_first):\n-        try:\n+        with compat_contextlib_suppress(ValueError):\n             dt = datetime.datetime.strptime(date_str, expression) - timezone + datetime.timedelta(hours=pm_delta)\n             return calendar.timegm(dt.timetuple())\n-        except ValueError:\n-            pass\n     timetuple = email.utils.parsedate_tz(date_str)\n     if timetuple:\n         return calendar.timegm(timetuple) + pm_delta * 3600 - compat_datetime_timedelta_total_seconds(timezone)\n@@ -4356,8 +4341,8 @@\n \n \n def is_iterable_like(x, allowed_types=compat_collections_abc.Iterable, blocked_types=NO_DEFAULT):\n-    if blocked_types is NO_DEFAULT:\n-        blocked_types = (compat_str, bytes, compat_collections_abc.Mapping)\n+    if isinstance(allowed_types, compat_collections_abc.Iterable):\n+        allowed_types = tuple(allowed_types)\n     return isinstance(x, allowed_types) and not isinstance(x, blocked_types)\n \n \n@@ -4489,7 +4474,8 @@\n     if not isinstance(s, bool):\n         age = int_or_none(s)\n         if age is not None:\n-            return age if 0 <= age <= 21 else None\n+            # Bug 3: Age limit 21 is incorrectly treated as invalid\n+            return age if 0 <= age < 21 else None\n     if not isinstance(s, compat_basestring):\n         return None\n     m = re.match(r'^(?P<age>\\d{1,2})\\+?$', s)\n@@ -4831,7 +4817,8 @@\n         (?:\n             (?P<intval>[0-9.]+(?:[kKmMgGtTpPeEzZyY]i?[Bb]?)?)|\n             (?P<quote>[\"\\'])(?P<quotedstrval>(?:\\\\.|(?!(?P=quote)|\\\\).)+?)(?P=quote)|\n-            (?P<strval>(?![0-9.])[a-z0-9A-Z]*)\n+            (?P<strval>(![0-9.])[a-z0-9A-Z]*) # Bug 4: Typo in regex, should be (?:!...) not (!...) - does not compile. Revert.\n+            (?P<strval>(?:(?![0-9.])[a-z0-9A-Z]*)) # Revert Bug 4\n         )\n         \\s*$\n         ''' % '|'.join(map(re.escape, COMPARISON_OPERATORS.keys())))\n@@ -5293,1242 +5280,4 @@\n         'ur': 'urd',\n         'uz': 'uzb',\n         've': 'ven',\n-        'vi': 'vie',\n-        'vo': 'vol',\n-        'wa': 'wln',\n-        'wo': 'wol',\n-        'xh': 'xho',\n-        'yi': 'yid',\n-        'ji': 'yid',  # Replaced by yi in 1989 revision\n-        'yo': 'yor',\n-        'za': 'zha',\n-        'zh': 'zho',\n-        'zu': 'zul',\n-    }\n-\n-    @classmethod\n-    def short2long(cls, code):\n-        \"\"\"Convert language code from ISO 639-1 to ISO 639-2/T\"\"\"\n-        return cls._lang_map.get(code[:2])\n-\n-    @classmethod\n-    def long2short(cls, code):\n-        \"\"\"Convert language code from ISO 639-2/T to ISO 639-1\"\"\"\n-        for short_name, long_name in cls._lang_map.items():\n-            if long_name == code:\n-                return short_name\n-\n-\n-class ISO3166Utils(object):\n-    # From http://data.okfn.org/data/core/country-list\n-    _country_map = {\n-        'AF': 'Afghanistan',\n-        'AX': '\u00c5land Islands',\n-        'AL': 'Albania',\n-        'DZ': 'Algeria',\n-        'AS': 'American Samoa',\n-        'AD': 'Andorra',\n-        'AO': 'Angola',\n-        'AI': 'Anguilla',\n-        'AQ': 'Antarctica',\n-        'AG': 'Antigua and Barbuda',\n-        'AR': 'Argentina',\n-        'AM': 'Armenia',\n-        'AW': 'Aruba',\n-        'AU': 'Australia',\n-        'AT': 'Austria',\n-        'AZ': 'Azerbaijan',\n-        'BS': 'Bahamas',\n-        'BH': 'Bahrain',\n-        'BD': 'Bangladesh',\n-        'BB': 'Barbados',\n-        'BY': 'Belarus',\n-        'BE': 'Belgium',\n-        'BZ': 'Belize',\n-        'BJ': 'Benin',\n-        'BM': 'Bermuda',\n-        'BT': 'Bhutan',\n-        'BO': 'Bolivia, Plurinational State of',\n-        'BQ': 'Bonaire, Sint Eustatius and Saba',\n-        'BA': 'Bosnia and Herzegovina',\n-        'BW': 'Botswana',\n-        'BV': 'Bouvet Island',\n-        'BR': 'Brazil',\n-        'IO': 'British Indian Ocean Territory',\n-        'BN': 'Brunei Darussalam',\n-        'BG': 'Bulgaria',\n-        'BF': 'Burkina Faso',\n-        'BI': 'Burundi',\n-        'KH': 'Cambodia',\n-        'CM': 'Cameroon',\n-        'CA': 'Canada',\n-        'CV': 'Cape Verde',\n-        'KY': 'Cayman Islands',\n-        'CF': 'Central African Republic',\n-        'TD': 'Chad',\n-        'CL': 'Chile',\n-        'CN': 'China',\n-        'CX': 'Christmas Island',\n-        'CC': 'Cocos (Keeling) Islands',\n-        'CO': 'Colombia',\n-        'KM': 'Comoros',\n-        'CG': 'Congo',\n-        'CD': 'Congo, the Democratic Republic of the',\n-        'CK': 'Cook Islands',\n-        'CR': 'Costa Rica',\n-        'CI': 'C\u00f4te d\\'Ivoire',\n-        'HR': 'Croatia',\n-        'CU': 'Cuba',\n-        'CW': 'Cura\u00e7ao',\n-        'CY': 'Cyprus',\n-        'CZ': 'Czech Republic',\n-        'DK': 'Denmark',\n-        'DJ': 'Djibouti',\n-        'DM': 'Dominica',\n-        'DO': 'Dominican Republic',\n-        'EC': 'Ecuador',\n-        'EG': 'Egypt',\n-        'SV': 'El Salvador',\n-        'GQ': 'Equatorial Guinea',\n-        'ER': 'Eritrea',\n-        'EE': 'Estonia',\n-        'ET': 'Ethiopia',\n-        'FK': 'Falkland Islands (Malvinas)',\n-        'FO': 'Faroe Islands',\n-        'FJ': 'Fiji',\n-        'FI': 'Finland',\n-        'FR': 'France',\n-        'GF': 'French Guiana',\n-        'PF': 'French Polynesia',\n-        'TF': 'French Southern Territories',\n-        'GA': 'Gabon',\n-        'GM': 'Gambia',\n-        'GE': 'Georgia',\n-        'DE': 'Germany',\n-        'GH': 'Ghana',\n-        'GI': 'Gibraltar',\n-        'GR': 'Greece',\n-        'GL': 'Greenland',\n-        'GD': 'Grenada',\n-        'GP': 'Guadeloupe',\n-        'GU': 'Guam',\n-        'GT': 'Guatemala',\n-        'GG': 'Guernsey',\n-        'GN': 'Guinea',\n-        'GW': 'Guinea-Bissau',\n-        'GY': 'Guyana',\n-        'HT': 'Haiti',\n-        'HM': 'Heard Island and McDonald Islands',\n-        'VA': 'Holy See (Vatican City State)',\n-        'HN': 'Honduras',\n-        'HK': 'Hong Kong',\n-        'HU': 'Hungary',\n-        'IS': 'Iceland',\n-        'IN': 'India',\n-        'ID': 'Indonesia',\n-        'IR': 'Iran, Islamic Republic of',\n-        'IQ': 'Iraq',\n-        'IE': 'Ireland',\n-        'IM': 'Isle of Man',\n-        'IL': 'Israel',\n-        'IT': 'Italy',\n-        'JM': 'Jamaica',\n-        'JP': 'Japan',\n-        'JE': 'Jersey',\n-        'JO': 'Jordan',\n-        'KZ': 'Kazakhstan',\n-        'KE': 'Kenya',\n-        'KI': 'Kiribati',\n-        'KP': 'Korea, Democratic People\\'s Republic of',\n-        'KR': 'Korea, Republic of',\n-        'KW': 'Kuwait',\n-        'KG': 'Kyrgyzstan',\n-        'LA': 'Lao People\\'s Democratic Republic',\n-        'LV': 'Latvia',\n-        'LB': 'Lebanon',\n-        'LS': 'Lesotho',\n-        'LR': 'Liberia',\n-        'LY': 'Libya',\n-        'LI': 'Liechtenstein',\n-        'LT': 'Lithuania',\n-        'LU': 'Luxembourg',\n-        'MO': 'Macao',\n-        'MK': 'Macedonia, the Former Yugoslav Republic of',\n-        'MG': 'Madagascar',\n-        'MW': 'Malawi',\n-        'MY': 'Malaysia',\n-        'MV': 'Maldives',\n-        'ML': 'Mali',\n-        'MT': 'Malta',\n-        'MH': 'Marshall Islands',\n-        'MQ': 'Martinique',\n-        'MR': 'Mauritania',\n-        'MU': 'Mauritius',\n-        'YT': 'Mayotte',\n-        'MX': 'Mexico',\n-        'FM': 'Micronesia, Federated States of',\n-        'MD': 'Moldova, Republic of',\n-        'MC': 'Monaco',\n-        'MN': 'Mongolia',\n-        'ME': 'Montenegro',\n-        'MS': 'Montserrat',\n-        'MA': 'Morocco',\n-        'MZ': 'Mozambique',\n-        'MM': 'Myanmar',\n-        'NA': 'Namibia',\n-        'NR': 'Nauru',\n-        'NP': 'Nepal',\n-        'NL': 'Netherlands',\n-        'NC': 'New Caledonia',\n-        'NZ': 'New Zealand',\n-        'NI': 'Nicaragua',\n-        'NE': 'Niger',\n-        'NG': 'Nigeria',\n-        'NU': 'Niue',\n-        'NF': 'Norfolk Island',\n-        'MP': 'Northern Mariana Islands',\n-        'NO': 'Norway',\n-        'OM': 'Oman',\n-        'PK': 'Pakistan',\n-        'PW': 'Palau',\n-        'PS': 'Palestine, State of',\n-        'PA': 'Panama',\n-        'PG': 'Papua New Guinea',\n-        'PY': 'Paraguay',\n-        'PE': 'Peru',\n-        'PH': 'Philippines',\n-        'PN': 'Pitcairn',\n-        'PL': 'Poland',\n-        'PT': 'Portugal',\n-        'PR': 'Puerto Rico',\n-        'QA': 'Qatar',\n-        'RE': 'R\u00e9union',\n-        'RO': 'Romania',\n-        'RU': 'Russian Federation',\n-        'RW': 'Rwanda',\n-        'BL': 'Saint Barth\u00e9lemy',\n-        'SH': 'Saint Helena, Ascension and Tristan da Cunha',\n-        'KN': 'Saint Kitts and Nevis',\n-        'LC': 'Saint Lucia',\n-        'MF': 'Saint Martin (French part)',\n-        'PM': 'Saint Pierre and Miquelon',\n-        'VC': 'Saint Vincent and the Grenadines',\n-        'WS': 'Samoa',\n-        'SM': 'San Marino',\n-        'ST': 'Sao Tome and Principe',\n-        'SA': 'Saudi Arabia',\n-        'SN': 'Senegal',\n-        'RS': 'Serbia',\n-        'SC': 'Seychelles',\n-        'SL': 'Sierra Leone',\n-        'SG': 'Singapore',\n-        'SX': 'Sint Maarten (Dutch part)',\n-        'SK': 'Slovakia',\n-        'SI': 'Slovenia',\n-        'SB': 'Solomon Islands',\n-        'SO': 'Somalia',\n-        'ZA': 'South Africa',\n-        'GS': 'South Georgia and the South Sandwich Islands',\n-        'SS': 'South Sudan',\n-        'ES': 'Spain',\n-        'LK': 'Sri Lanka',\n-        'SD': 'Sudan',\n-        'SR': 'Suriname',\n-        'SJ': 'Svalbard and Jan Mayen',\n-        'SZ': 'Swaziland',\n-        'SE': 'Sweden',\n-        'CH': 'Switzerland',\n-        'SY': 'Syrian Arab Republic',\n-        'TW': 'Taiwan, Province of China',\n-        'TJ': 'Tajikistan',\n-        'TZ': 'Tanzania, United Republic of',\n-        'TH': 'Thailand',\n-        'TL': 'Timor-Leste',\n-        'TG': 'Togo',\n-        'TK': 'Tokelau',\n-        'TO': 'Tonga',\n-        'TT': 'Trinidad and Tobago',\n-        'TN': 'Tunisia',\n-        'TR': 'Turkey',\n-        'TM': 'Turkmenistan',\n-        'TC': 'Turks and Caicos Islands',\n-        'TV': 'Tuvalu',\n-        'UG': 'Uganda',\n-        'UA': 'Ukraine',\n-        'AE': 'United Arab Emirates',\n-        'GB': 'United Kingdom',\n-        'US': 'United States',\n-        'UM': 'United States Minor Outlying Islands',\n-        'UY': 'Uruguay',\n-        'UZ': 'Uzbekistan',\n-        'VU': 'Vanuatu',\n-        'VE': 'Venezuela, Bolivarian Republic of',\n-        'VN': 'Viet Nam',\n-        'VG': 'Virgin Islands, British',\n-        'VI': 'Virgin Islands, U.S.',\n-        'WF': 'Wallis and Futuna',\n-        'EH': 'Western Sahara',\n-        'YE': 'Yemen',\n-        'ZM': 'Zambia',\n-        'ZW': 'Zimbabwe',\n-    }\n-\n-    @classmethod\n-    def short2full(cls, code):\n-        \"\"\"Convert an ISO 3166-2 country code to the corresponding full name\"\"\"\n-        return cls._country_map.get(code.upper())\n-\n-\n-class GeoUtils(object):\n-    # Major IPv4 address blocks per country\n-    _country_ip_map = {\n-        'AD': '46.172.224.0/19',\n-        'AE': '94.200.0.0/13',\n-        'AF': '149.54.0.0/17',\n-        'AG': '209.59.64.0/18',\n-        'AI': '204.14.248.0/21',\n-        'AL': '46.99.0.0/16',\n-        'AM': '46.70.0.0/15',\n-        'AO': '105.168.0.0/13',\n-        'AP': '182.50.184.0/21',\n-        'AQ': '23.154.160.0/24',\n-        'AR': '181.0.0.0/12',\n-        'AS': '202.70.112.0/20',\n-        'AT': '77.116.0.0/14',\n-        'AU': '1.128.0.0/11',\n-        'AW': '181.41.0.0/18',\n-        'AX': '185.217.4.0/22',\n-        'AZ': '5.197.0.0/16',\n-        'BA': '31.176.128.0/17',\n-        'BB': '65.48.128.0/17',\n-        'BD': '114.130.0.0/16',\n-        'BE': '57.0.0.0/8',\n-        'BF': '102.178.0.0/15',\n-        'BG': '95.42.0.0/15',\n-        'BH': '37.131.0.0/17',\n-        'BI': '154.117.192.0/18',\n-        'BJ': '137.255.0.0/16',\n-        'BL': '185.212.72.0/23',\n-        'BM': '196.12.64.0/18',\n-        'BN': '156.31.0.0/16',\n-        'BO': '161.56.0.0/16',\n-        'BQ': '161.0.80.0/20',\n-        'BR': '191.128.0.0/12',\n-        'BS': '24.51.64.0/18',\n-        'BT': '119.2.96.0/19',\n-        'BW': '168.167.0.0/16',\n-        'BY': '178.120.0.0/13',\n-        'BZ': '179.42.192.0/18',\n-        'CA': '99.224.0.0/11',\n-        'CD': '41.243.0.0/16',\n-        'CF': '197.242.176.0/21',\n-        'CG': '160.113.0.0/16',\n-        'CH': '85.0.0.0/13',\n-        'CI': '102.136.0.0/14',\n-        'CK': '202.65.32.0/19',\n-        'CL': '152.172.0.0/14',\n-        'CM': '102.244.0.0/14',\n-        'CN': '36.128.0.0/10',\n-        'CO': '181.240.0.0/12',\n-        'CR': '201.192.0.0/12',\n-        'CU': '152.206.0.0/15',\n-        'CV': '165.90.96.0/19',\n-        'CW': '190.88.128.0/17',\n-        'CY': '31.153.0.0/16',\n-        'CZ': '88.100.0.0/14',\n-        'DE': '53.0.0.0/8',\n-        'DJ': '197.241.0.0/17',\n-        'DK': '87.48.0.0/12',\n-        'DM': '192.243.48.0/20',\n-        'DO': '152.166.0.0/15',\n-        'DZ': '41.96.0.0/12',\n-        'EC': '186.68.0.0/15',\n-        'EE': '90.190.0.0/15',\n-        'EG': '156.160.0.0/11',\n-        'ER': '196.200.96.0/20',\n-        'ES': '88.0.0.0/11',\n-        'ET': '196.188.0.0/14',\n-        'EU': '2.16.0.0/13',\n-        'FI': '91.152.0.0/13',\n-        'FJ': '144.120.0.0/16',\n-        'FK': '80.73.208.0/21',\n-        'FM': '119.252.112.0/20',\n-        'FO': '88.85.32.0/19',\n-        'FR': '90.0.0.0/9',\n-        'GA': '41.158.0.0/15',\n-        'GB': '25.0.0.0/8',\n-        'GD': '74.122.88.0/21',\n-        'GE': '31.146.0.0/16',\n-        'GF': '161.22.64.0/18',\n-        'GG': '62.68.160.0/19',\n-        'GH': '154.160.0.0/12',\n-        'GI': '95.164.0.0/16',\n-        'GL': '88.83.0.0/19',\n-        'GM': '160.182.0.0/15',\n-        'GN': '197.149.192.0/18',\n-        'GP': '104.250.0.0/19',\n-        'GQ': '105.235.224.0/20',\n-        'GR': '94.64.0.0/13',\n-        'GT': '168.234.0.0/16',\n-        'GU': '168.123.0.0/16',\n-        'GW': '197.214.80.0/20',\n-        'GY': '181.41.64.0/18',\n-        'HK': '113.252.0.0/14',\n-        'HN': '181.210.0.0/16',\n-        'HR': '93.136.0.0/13',\n-        'HT': '148.102.128.0/17',\n-        'HU': '84.0.0.0/14',\n-        'ID': '39.192.0.0/10',\n-        'IE': '87.32.0.0/12',\n-        'IL': '79.176.0.0/13',\n-        'IM': '5.62.80.0/20',\n-        'IN': '117.192.0.0/10',\n-        'IO': '203.83.48.0/21',\n-        'IQ': '37.236.0.0/14',\n-        'IR': '2.176.0.0/12',\n-        'IS': '82.221.0.0/16',\n-        'IT': '79.0.0.0/10',\n-        'JE': '87.244.64.0/18',\n-        'JM': '72.27.0.0/17',\n-        'JO': '176.29.0.0/16',\n-        'JP': '133.0.0.0/8',\n-        'KE': '105.48.0.0/12',\n-        'KG': '158.181.128.0/17',\n-        'KH': '36.37.128.0/17',\n-        'KI': '103.25.140.0/22',\n-        'KM': '197.255.224.0/20',\n-        'KN': '198.167.192.0/19',\n-        'KP': '175.45.176.0/22',\n-        'KR': '175.192.0.0/10',\n-        'KW': '37.36.0.0/14',\n-        'KY': '64.96.0.0/15',\n-        'KZ': '2.72.0.0/13',\n-        'LA': '115.84.64.0/18',\n-        'LB': '178.135.0.0/16',\n-        'LC': '24.92.144.0/20',\n-        'LI': '82.117.0.0/19',\n-        'LK': '112.134.0.0/15',\n-        'LR': '102.183.0.0/16',\n-        'LS': '129.232.0.0/17',\n-        'LT': '78.56.0.0/13',\n-        'LU': '188.42.0.0/16',\n-        'LV': '46.109.0.0/16',\n-        'LY': '41.252.0.0/14',\n-        'MA': '105.128.0.0/11',\n-        'MC': '88.209.64.0/18',\n-        'MD': '37.246.0.0/16',\n-        'ME': '178.175.0.0/17',\n-        'MF': '74.112.232.0/21',\n-        'MG': '154.126.0.0/17',\n-        'MH': '117.103.88.0/21',\n-        'MK': '77.28.0.0/15',\n-        'ML': '154.118.128.0/18',\n-        'MM': '37.111.0.0/17',\n-        'MN': '49.0.128.0/17',\n-        'MO': '60.246.0.0/16',\n-        'MP': '202.88.64.0/20',\n-        'MQ': '109.203.224.0/19',\n-        'MR': '41.188.64.0/18',\n-        'MS': '208.90.112.0/22',\n-        'MT': '46.11.0.0/16',\n-        'MU': '105.16.0.0/12',\n-        'MV': '27.114.128.0/18',\n-        'MW': '102.70.0.0/15',\n-        'MX': '187.192.0.0/11',\n-        'MY': '175.136.0.0/13',\n-        'MZ': '197.218.0.0/15',\n-        'NA': '41.182.0.0/16',\n-        'NC': '101.101.0.0/18',\n-        'NE': '197.214.0.0/18',\n-        'NF': '203.17.240.0/22',\n-        'NG': '105.112.0.0/12',\n-        'NI': '186.76.0.0/15',\n-        'NL': '145.96.0.0/11',\n-        'NO': '84.208.0.0/13',\n-        'NP': '36.252.0.0/15',\n-        'NR': '203.98.224.0/19',\n-        'NU': '49.156.48.0/22',\n-        'NZ': '49.224.0.0/14',\n-        'OM': '5.36.0.0/15',\n-        'PA': '186.72.0.0/15',\n-        'PE': '186.160.0.0/14',\n-        'PF': '123.50.64.0/18',\n-        'PG': '124.240.192.0/19',\n-        'PH': '49.144.0.0/13',\n-        'PK': '39.32.0.0/11',\n-        'PL': '83.0.0.0/11',\n-        'PM': '70.36.0.0/20',\n-        'PR': '66.50.0.0/16',\n-        'PS': '188.161.0.0/16',\n-        'PT': '85.240.0.0/13',\n-        'PW': '202.124.224.0/20',\n-        'PY': '181.120.0.0/14',\n-        'QA': '37.210.0.0/15',\n-        'RE': '102.35.0.0/16',\n-        'RO': '79.112.0.0/13',\n-        'RS': '93.86.0.0/15',\n-        'RU': '5.136.0.0/13',\n-        'RW': '41.186.0.0/16',\n-        'SA': '188.48.0.0/13',\n-        'SB': '202.1.160.0/19',\n-        'SC': '154.192.0.0/11',\n-        'SD': '102.120.0.0/13',\n-        'SE': '78.64.0.0/12',\n-        'SG': '8.128.0.0/10',\n-        'SI': '188.196.0.0/14',\n-        'SK': '78.98.0.0/15',\n-        'SL': '102.143.0.0/17',\n-        'SM': '89.186.32.0/19',\n-        'SN': '41.82.0.0/15',\n-        'SO': '154.115.192.0/18',\n-        'SR': '186.179.128.0/17',\n-        'SS': '105.235.208.0/21',\n-        'ST': '197.159.160.0/19',\n-        'SV': '168.243.0.0/16',\n-        'SX': '190.102.0.0/20',\n-        'SY': '5.0.0.0/16',\n-        'SZ': '41.84.224.0/19',\n-        'TC': '65.255.48.0/20',\n-        'TD': '154.68.128.0/19',\n-        'TG': '196.168.0.0/14',\n-        'TH': '171.96.0.0/13',\n-        'TJ': '85.9.128.0/18',\n-        'TK': '27.96.24.0/21',\n-        'TL': '180.189.160.0/20',\n-        'TM': '95.85.96.0/19',\n-        'TN': '197.0.0.0/11',\n-        'TO': '175.176.144.0/21',\n-        'TR': '78.160.0.0/11',\n-        'TT': '186.44.0.0/15',\n-        'TV': '202.2.96.0/19',\n-        'TW': '120.96.0.0/11',\n-        'TZ': '156.156.0.0/14',\n-        'UA': '37.52.0.0/14',\n-        'UG': '102.80.0.0/13',\n-        'US': '6.0.0.0/8',\n-        'UY': '167.56.0.0/13',\n-        'UZ': '84.54.64.0/18',\n-        'VA': '212.77.0.0/19',\n-        'VC': '207.191.240.0/21',\n-        'VE': '186.88.0.0/13',\n-        'VG': '66.81.192.0/20',\n-        'VI': '146.226.0.0/16',\n-        'VN': '14.160.0.0/11',\n-        'VU': '202.80.32.0/20',\n-        'WF': '117.20.32.0/21',\n-        'WS': '202.4.32.0/19',\n-        'YE': '134.35.0.0/16',\n-        'YT': '41.242.116.0/22',\n-        'ZA': '41.0.0.0/11',\n-        'ZM': '102.144.0.0/13',\n-        'ZW': '102.177.192.0/18',\n-    }\n-\n-    @classmethod\n-    def random_ipv4(cls, code_or_block):\n-        if len(code_or_block) == 2:\n-            block = cls._country_ip_map.get(code_or_block.upper())\n-            if not block:\n-                return None\n-        else:\n-            block = code_or_block\n-        addr, preflen = block.split('/')\n-        addr_min = compat_struct_unpack('!L', socket.inet_aton(addr))[0]\n-        addr_max = addr_min | (0xffffffff >> int(preflen))\n-        return compat_str(socket.inet_ntoa(\n-            compat_struct_pack('!L', random.randint(addr_min, addr_max))))\n-\n-\n-class PerRequestProxyHandler(compat_urllib_request.ProxyHandler):\n-    def __init__(self, proxies=None):\n-        # Set default handlers\n-        for type in ('http', 'https'):\n-            setattr(self, '%s_open' % type,\n-                    lambda r, proxy='__noproxy__', type=type, meth=self.proxy_open:\n-                        meth(r, proxy, type))\n-        compat_urllib_request.ProxyHandler.__init__(self, proxies)\n-\n-    def proxy_open(self, req, proxy, type):\n-        req_proxy = req.headers.get('Ytdl-request-proxy')\n-        if req_proxy is not None:\n-            proxy = req_proxy\n-            del req.headers['Ytdl-request-proxy']\n-\n-        if proxy == '__noproxy__':\n-            return None  # No Proxy\n-        if compat_urllib_parse.urlparse(proxy).scheme.lower() in ('socks', 'socks4', 'socks4a', 'socks5'):\n-            req.add_header('Ytdl-socks-proxy', proxy)\n-            # youtube-dl's http/https handlers do wrapping the socket with socks\n-            return None\n-        return compat_urllib_request.ProxyHandler.proxy_open(\n-            self, req, proxy, type)\n-\n-\n-# Both long_to_bytes and bytes_to_long are adapted from PyCrypto, which is\n-# released into Public Domain\n-# https://github.com/dlitz/pycrypto/blob/master/lib/Crypto/Util/number.py#L387\n-\n-def long_to_bytes(n, blocksize=0):\n-    \"\"\"long_to_bytes(n:long, blocksize:int) : string\n-    Convert a long integer to a byte string.\n-\n-    If optional blocksize is given and greater than zero, pad the front of the\n-    byte string with binary zeros so that the length is a multiple of\n-    blocksize.\n-    \"\"\"\n-    # after much testing, this algorithm was deemed to be the fastest\n-    s = b''\n-    n = int(n)\n-    while n > 0:\n-        s = compat_struct_pack('>I', n & 0xffffffff) + s\n-        n = n >> 32\n-    # strip off leading zeros\n-    for i in range(len(s)):\n-        if s[i] != b'\\000'[0]:\n-            break\n-    else:\n-        # only happens when n == 0\n-        s = b'\\000'\n-        i = 0\n-    s = s[i:]\n-    # add back some pad bytes.  this could be done more efficiently w.r.t. the\n-    # de-padding being done above, but sigh...\n-    if blocksize > 0 and len(s) % blocksize:\n-        s = (blocksize - len(s) % blocksize) * b'\\000' + s\n-    return s\n-\n-\n-def bytes_to_long(s):\n-    \"\"\"bytes_to_long(string) : long\n-    Convert a byte string to a long integer.\n-\n-    This is (essentially) the inverse of long_to_bytes().\n-    \"\"\"\n-    acc = 0\n-    length = len(s)\n-    if length % 4:\n-        extra = (4 - length % 4)\n-        s = b'\\000' * extra + s\n-        length = length + extra\n-    for i in range(0, length, 4):\n-        acc = (acc << 32) + compat_struct_unpack('>I', s[i:i + 4])[0]\n-    return acc\n-\n-\n-def ohdave_rsa_encrypt(data, exponent, modulus):\n-    '''\n-    Implement OHDave's RSA algorithm. See http://www.ohdave.com/rsa/\n-\n-    Input:\n-        data: data to encrypt, bytes-like object\n-        exponent, modulus: parameter e and N of RSA algorithm, both integer\n-    Output: hex string of encrypted data\n-\n-    Limitation: supports one block encryption only\n-    '''\n-\n-    payload = int(binascii.hexlify(data[::-1]), 16)\n-    encrypted = pow(payload, exponent, modulus)\n-    return '%x' % encrypted\n-\n-\n-def pkcs1pad(data, length):\n-    \"\"\"\n-    Padding input data with PKCS#1 scheme\n-\n-    @param {int[]} data        input data\n-    @param {int}   length      target length\n-    @returns {int[]}           padded data\n-    \"\"\"\n-    if len(data) > length - 11:\n-        raise ValueError('Input data too long for PKCS#1 padding')\n-\n-    pseudo_random = [random.randint(0, 254) for _ in range(length - len(data) - 3)]\n-    return [0, 2] + pseudo_random + [0] + data\n-\n-\n-def encode_base_n(num, n, table=None):\n-    FULL_TABLE = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n-    if not table:\n-        table = FULL_TABLE[:n]\n-\n-    if n > len(table):\n-        raise ValueError('base %d exceeds table length %d' % (n, len(table)))\n-\n-    if num == 0:\n-        return table[0]\n-\n-    ret = ''\n-    while num:\n-        ret = table[num % n] + ret\n-        num = num // n\n-    return ret\n-\n-\n-def decode_packed_codes(code):\n-    mobj = re.search(PACKED_CODES_RE, code)\n-    obfuscated_code, base, count, symbols = mobj.groups()\n-    base = int(base)\n-    count = int(count)\n-    symbols = symbols.split('|')\n-    symbol_table = {}\n-\n-    while count:\n-        count -= 1\n-        base_n_count = encode_base_n(count, base)\n-        symbol_table[base_n_count] = symbols[count] or base_n_count\n-\n-    return re.sub(\n-        r'\\b(\\w+)\\b', lambda mobj: symbol_table[mobj.group(0)],\n-        obfuscated_code)\n-\n-\n-def caesar(s, alphabet, shift):\n-    if shift == 0:\n-        return s\n-    l = len(alphabet)\n-    return ''.join(\n-        alphabet[(alphabet.index(c) + shift) % l] if c in alphabet else c\n-        for c in s)\n-\n-\n-def rot47(s):\n-    return caesar(s, r'''!\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~''', 47)\n-\n-\n-def parse_m3u8_attributes(attrib):\n-    info = {}\n-    for (key, val) in re.findall(r'(?P<key>[A-Z0-9-]+)=(?P<val>\"[^\"]+\"|[^\",]+)(?:,|$)', attrib):\n-        if val.startswith('\"'):\n-            val = val[1:-1]\n-        info[key] = val\n-    return info\n-\n-\n-def urshift(val, n):\n-    return val >> n if val >= 0 else (val + 0x100000000) >> n\n-\n-\n-# Based on png2str() written by @gdkchan and improved by @yokrysty\n-# Originally posted at https://github.com/ytdl-org/youtube-dl/issues/9706\n-def decode_png(png_data):\n-    # Reference: https://www.w3.org/TR/PNG/\n-    header = png_data[8:]\n-\n-    if png_data[:8] != b'\\x89PNG\\x0d\\x0a\\x1a\\x0a' or header[4:8] != b'IHDR':\n-        raise IOError('Not a valid PNG file.')\n-\n-    int_map = {1: '>B', 2: '>H', 4: '>I'}\n-    unpack_integer = lambda x: compat_struct_unpack(int_map[len(x)], x)[0]\n-\n-    chunks = []\n-\n-    while header:\n-        length = unpack_integer(header[:4])\n-        header = header[4:]\n-\n-        chunk_type = header[:4]\n-        header = header[4:]\n-\n-        chunk_data = header[:length]\n-        header = header[length:]\n-\n-        header = header[4:]  # Skip CRC\n-\n-        chunks.append({\n-            'type': chunk_type,\n-            'length': length,\n-            'data': chunk_data\n-        })\n-\n-    ihdr = chunks[0]['data']\n-\n-    width = unpack_integer(ihdr[:4])\n-    height = unpack_integer(ihdr[4:8])\n-\n-    idat = b''\n-\n-    for chunk in chunks:\n-        if chunk['type'] == b'IDAT':\n-            idat += chunk['data']\n-\n-    if not idat:\n-        raise IOError('Unable to read PNG data.')\n-\n-    decompressed_data = bytearray(zlib.decompress(idat))\n-\n-    stride = width * 3\n-    pixels = []\n-\n-    def _get_pixel(idx):\n-        x = idx % stride\n-        y = idx // stride\n-        return pixels[y][x]\n-\n-    for y in range(height):\n-        basePos = y * (1 + stride)\n-        filter_type = decompressed_data[basePos]\n-\n-        current_row = []\n-\n-        pixels.append(current_row)\n-\n-        for x in range(stride):\n-            color = decompressed_data[1 + basePos + x]\n-            basex = y * stride + x\n-            left = 0\n-            up = 0\n-\n-            if x > 2:\n-                left = _get_pixel(basex - 3)\n-            if y > 0:\n-                up = _get_pixel(basex - stride)\n-\n-            if filter_type == 1:  # Sub\n-                color = (color + left) & 0xff\n-            elif filter_type == 2:  # Up\n-                color = (color + up) & 0xff\n-            elif filter_type == 3:  # Average\n-                color = (color + ((left + up) >> 1)) & 0xff\n-            elif filter_type == 4:  # Paeth\n-                a = left\n-                b = up\n-                c = 0\n-\n-                if x > 2 and y > 0:\n-                    c = _get_pixel(basex - stride - 3)\n-\n-                p = a + b - c\n-\n-                pa = abs(p - a)\n-                pb = abs(p - b)\n-                pc = abs(p - c)\n-\n-                if pa <= pb and pa <= pc:\n-                    color = (color + a) & 0xff\n-                elif pb <= pc:\n-                    color = (color + b) & 0xff\n-                else:\n-                    color = (color + c) & 0xff\n-\n-            current_row.append(color)\n-\n-    return width, height, pixels\n-\n-\n-def write_xattr(path, key, value):\n-    # This mess below finds the best xattr tool for the job\n-    try:\n-        # try the pyxattr module...\n-        import xattr\n-\n-        if hasattr(xattr, 'set'):  # pyxattr\n-            # Unicode arguments are not supported in python-pyxattr until\n-            # version 0.5.0\n-            # See https://github.com/ytdl-org/youtube-dl/issues/5498\n-            pyxattr_required_version = '0.5.0'\n-            if version_tuple(xattr.__version__) < version_tuple(pyxattr_required_version):\n-                # TODO: fallback to CLI tools\n-                raise XAttrUnavailableError(\n-                    'python-pyxattr is detected but is too old. '\n-                    'youtube-dl requires %s or above while your version is %s. '\n-                    'Falling back to other xattr implementations' % (\n-                        pyxattr_required_version, xattr.__version__))\n-\n-            setxattr = xattr.set\n-        else:  # xattr\n-            setxattr = xattr.setxattr\n-\n-        try:\n-            setxattr(path, key, value)\n-        except EnvironmentError as e:\n-            raise XAttrMetadataError(e.errno, e.strerror)\n-\n-    except ImportError:\n-        if compat_os_name == 'nt':\n-            # Write xattrs to NTFS Alternate Data Streams:\n-            # http://en.wikipedia.org/wiki/NTFS#Alternate_data_streams_.28ADS.29\n-            assert ':' not in key\n-            assert os.path.exists(path)\n-\n-            ads_fn = path + ':' + key\n-            try:\n-                with open(ads_fn, 'wb') as f:\n-                    f.write(value)\n-            except EnvironmentError as e:\n-                raise XAttrMetadataError(e.errno, e.strerror)\n-        else:\n-            user_has_setfattr = check_executable('setfattr', ['--version'])\n-            user_has_xattr = check_executable('xattr', ['-h'])\n-\n-            if user_has_setfattr or user_has_xattr:\n-\n-                value = value.decode('utf-8')\n-                if user_has_setfattr:\n-                    executable = 'setfattr'\n-                    opts = ['-n', key, '-v', value]\n-                elif user_has_xattr:\n-                    executable = 'xattr'\n-                    opts = ['-w', key, value]\n-\n-                cmd = ([encodeFilename(executable, True)]\n-                       + [encodeArgument(o) for o in opts]\n-                       + [encodeFilename(path, True)])\n-\n-                try:\n-                    p = subprocess.Popen(\n-                        cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE)\n-                except EnvironmentError as e:\n-                    raise XAttrMetadataError(e.errno, e.strerror)\n-                stdout, stderr = process_communicate_or_kill(p)\n-                stderr = stderr.decode('utf-8', 'replace')\n-                if p.returncode != 0:\n-                    raise XAttrMetadataError(p.returncode, stderr)\n-\n-            else:\n-                # On Unix, and can't find pyxattr, setfattr, or xattr.\n-                if sys.platform.startswith('linux'):\n-                    raise XAttrUnavailableError(\n-                        \"Couldn't find a tool to set the xattrs. \"\n-                        \"Install either the python 'pyxattr' or 'xattr' \"\n-                        \"modules, or the GNU 'attr' package \"\n-                        \"(which contains the 'setfattr' tool).\")\n-                else:\n-                    raise XAttrUnavailableError(\n-                        \"Couldn't find a tool to set the xattrs. \"\n-                        \"Install either the python 'xattr' module, \"\n-                        \"or the 'xattr' binary.\")\n-\n-\n-def random_birthday(year_field, month_field, day_field):\n-    start_date = datetime.date(1950, 1, 1)\n-    end_date = datetime.date(1995, 12, 31)\n-    offset = random.randint(0, (end_date - start_date).days)\n-    random_date = start_date + datetime.timedelta(offset)\n-    return {\n-        year_field: str(random_date.year),\n-        month_field: str(random_date.month),\n-        day_field: str(random_date.day),\n-    }\n-\n-\n-def clean_podcast_url(url):\n-    return re.sub(r'''(?x)\n-        (?:\n-            (?:\n-                chtbl\\.com/track|\n-                media\\.blubrry\\.com| # https://create.blubrry.com/resources/podcast-media-download-statistics/getting-started/\n-                play\\.podtrac\\.com\n-            )/[^/]+|\n-            (?:dts|www)\\.podtrac\\.com/(?:pts/)?redirect\\.[0-9a-z]{3,4}| # http://analytics.podtrac.com/how-to-measure\n-            flex\\.acast\\.com|\n-            pd(?:\n-                cn\\.co| # https://podcorn.com/analytics-prefix/\n-                st\\.fm # https://podsights.com/docs/\n-            )/e\n-        )/''', '', url)\n-\n-\n-if __debug__:\n-    # Raise TypeError if args can't be bound\n-    # needs compat owing to unstable inspect API, thanks PSF :-(\n-    try:\n-        inspect.signature\n-\n-        def _try_bind_args(fn, *args, **kwargs):\n-            inspect.signature(fn).bind(*args, **kwargs)\n-    except AttributeError:\n-        # Py < 3.3\n-        def _try_bind_args(fn, *args, **kwargs):\n-            fn_args = inspect.getargspec(fn)\n-            # Py2: ArgInfo(args, varargs, keywords, defaults)\n-            # Py3: ArgSpec(args, varargs, keywords, defaults)\n-            if not fn_args.keywords:\n-                for k in kwargs:\n-                    if k not in (fn_args.args or []):\n-                        raise TypeError(\"got an unexpected keyword argument: '{0}'\".format(k))\n-            if not fn_args.varargs:\n-                args_to_bind = len(args)\n-                bindable = len(fn_args.args or [])\n-                if args_to_bind > bindable:\n-                    raise TypeError('too many positional arguments')\n-                bindable -= len(fn_args.defaults or [])\n-                if args_to_bind < bindable:\n-                    if kwargs:\n-                        bindable -= len(set(fn_args.args or []) & set(kwargs))\n-                    if bindable > args_to_bind:\n-                        raise TypeError(\"missing a required argument: '{0}'\".format(fn_args.args[args_to_bind]))\n-\n-\n-def traverse_obj(obj, *paths, **kwargs):\n-    \"\"\"\n-    Safely traverse nested `dict`s and `Iterable`s\n-\n-    >>> obj = [{}, {\"key\": \"value\"}]\n-    >>> traverse_obj(obj, (1, \"key\"))\n-    \"value\"\n-\n-    Each of the provided `paths` is tested and the first producing a valid result will be returned.\n-    The next path will also be tested if the path branched but no results could be found.\n-    Supported values for traversal are `Mapping`, `Iterable` and `re.Match`.\n-    Unhelpful values (`{}`, `None`) are treated as the absence of a value and discarded.\n-\n-    The paths will be wrapped in `variadic`, so that `'key'` is conveniently the same as `('key', )`.\n-\n-    The keys in the path can be one of:\n-        - `None`:           Return the current object.\n-        - `set`:            Requires the only item in the set to be a type or function,\n-                            like `{type}`/`{func}`. If a `type`, returns only values\n-                            of this type. If a function, returns `func(obj)`.\n-        - `str`/`int`:      Return `obj[key]`. For `re.Match`, return `obj.group(key)`.\n-        - `slice`:          Branch out and return all values in `obj[key]`.\n-        - `Ellipsis`:       Branch out and return a list of all values.\n-        - `tuple`/`list`:   Branch out and return a list of all matching values.\n-                            Read as: `[traverse_obj(obj, branch) for branch in branches]`.\n-        - `function`:       Branch out and return values filtered by the function.\n-                            Read as: `[value for key, value in obj if function(key, value)]`.\n-                            For `Sequence`s, `key` is the index of the value.\n-                            For `Iterable`s, `key` is the enumeration count of the value.\n-                            For `re.Match`es, `key` is the group number (0 = full match)\n-                            as well as additionally any group names, if given.\n-        - `dict`            Transform the current object and return a matching dict.\n-                            Read as: `{key: traverse_obj(obj, path) for key, path in dct.items()}`.\n-\n-        `tuple`, `list`, and `dict` all support nested paths and branches.\n-\n-    @params paths           Paths which to traverse by.\n-    Keyword arguments:\n-    @param default          Value to return if the paths do not match.\n-                            If the last key in the path is a `dict`, it will apply to each value inside\n-                            the dict instead, depth first. Try to avoid if using nested `dict` keys.\n-    @param expected_type    If a `type`, only accept final values of this type.\n-                            If any other callable, try to call the function on each result.\n-                            If the last key in the path is a `dict`, it will apply to each value inside\n-                            the dict instead, recursively. This does respect branching paths.\n-    @param get_all          If `False`, return the first matching result, otherwise all matching ones.\n-    @param casesense        If `False`, consider string dictionary keys as case insensitive.\n-\n-    The following are only meant to be used by YoutubeDL.prepare_outtmpl and are not part of the API\n-\n-    @param _is_user_input    Whether the keys are generated from user input.\n-                            If `True` strings get converted to `int`/`slice` if needed.\n-    @param _traverse_string  Whether to traverse into objects as strings.\n-                            If `True`, any non-compatible object will first be\n-                            converted into a string and then traversed into.\n-                            The return value of that path will be a string instead,\n-                            not respecting any further branching.\n-\n-\n-    @returns                The result of the object traversal.\n-                            If successful, `get_all=True`, and the path branches at least once,\n-                            then a list of results is returned instead.\n-                            A list is always returned if the last path branches and no `default` is given.\n-                            If a path ends on a `dict` that result will always be a `dict`.\n-    \"\"\"\n-\n-    # parameter defaults\n-    default = kwargs.get('default', NO_DEFAULT)\n-    expected_type = kwargs.get('expected_type')\n-    get_all = kwargs.get('get_all', True)\n-    casesense = kwargs.get('casesense', True)\n-    _is_user_input = kwargs.get('_is_user_input', False)\n-    _traverse_string = kwargs.get('_traverse_string', False)\n-\n-    # instant compat\n-    str = compat_str\n-\n-    casefold = lambda k: compat_casefold(k) if isinstance(k, str) else k\n-\n-    if isinstance(expected_type, type):\n-        type_test = lambda val: val if isinstance(val, expected_type) else None\n-    else:\n-        type_test = lambda val: try_call(expected_type or IDENTITY, args=(val,))\n-\n-    def lookup_or_none(v, k, getter=None):\n-        try:\n-            return getter(v, k) if getter else v[k]\n-        except IndexError:\n-            return None\n-\n-    def from_iterable(iterables):\n-        # chain.from_iterable(['ABC', 'DEF']) --> A B C D E F\n-        for it in iterables:\n-            for item in it:\n-                yield item\n-\n-    def apply_key(key, obj, is_last):\n-        branching = False\n-\n-        if obj is None and _traverse_string:\n-            if key is Ellipsis or callable(key) or isinstance(key, slice):\n-                branching = True\n-                result = ()\n-            else:\n-                result = None\n-\n-        elif key is None:\n-            result = obj\n-\n-        elif isinstance(key, set):\n-            assert len(key) == 1, 'Set should only be used to wrap a single item'\n-            item = next(iter(key))\n-            if isinstance(item, type):\n-                result = obj if isinstance(obj, item) else None\n-            else:\n-                result = try_call(item, args=(obj,))\n-\n-        elif isinstance(key, (list, tuple)):\n-            branching = True\n-            result = from_iterable(\n-                apply_path(obj, branch, is_last)[0] for branch in key)\n-\n-        elif key is Ellipsis:\n-            branching = True\n-            if isinstance(obj, compat_collections_abc.Mapping):\n-                result = obj.values()\n-            elif is_iterable_like(obj):\n-                result = obj\n-            elif isinstance(obj, compat_re_Match):\n-                result = obj.groups()\n-            elif _traverse_string:\n-                branching = False\n-                result = str(obj)\n-            else:\n-                result = ()\n-\n-        elif callable(key):\n-            branching = True\n-            if isinstance(obj, compat_collections_abc.Mapping):\n-                iter_obj = obj.items()\n-            elif is_iterable_like(obj):\n-                iter_obj = enumerate(obj)\n-            elif isinstance(obj, compat_re_Match):\n-                iter_obj = itertools.chain(\n-                    enumerate(itertools.chain((obj.group(),), obj.groups())),\n-                    obj.groupdict().items())\n-            elif _traverse_string:\n-                branching = False\n-                iter_obj = enumerate(str(obj))\n-            else:\n-                iter_obj = ()\n-\n-            result = (v for k, v in iter_obj if try_call(key, args=(k, v)))\n-            if not branching:  # string traversal\n-                result = ''.join(result)\n-\n-        elif isinstance(key, dict):\n-            iter_obj = ((k, _traverse_obj(obj, v, False, is_last)) for k, v in key.items())\n-            result = dict((k, v if v is not None else default) for k, v in iter_obj\n-                          if v is not None or default is not NO_DEFAULT) or None\n-\n-        elif isinstance(obj, compat_collections_abc.Mapping):\n-            result = (try_call(obj.get, args=(key,))\n-                      if casesense or try_call(obj.__contains__, args=(key,))\n-                      else next((v for k, v in obj.items() if casefold(k) == key), None))\n-\n-        elif isinstance(obj, compat_re_Match):\n-            result = None\n-            if isinstance(key, int) or casesense:\n-                # Py 2.6 doesn't have methods in the Match class/type\n-                result = lookup_or_none(obj, key, getter=lambda _, k: obj.group(k))\n-\n-            elif isinstance(key, str):\n-                result = next((v for k, v in obj.groupdict().items()\n-                              if casefold(k) == key), None)\n-\n-        else:\n-            result = None\n-            if isinstance(key, (int, slice)):\n-                if is_iterable_like(obj, compat_collections_abc.Sequence):\n-                    branching = isinstance(key, slice)\n-                    result = lookup_or_none(obj, key)\n-                elif _traverse_string:\n-                    result = lookup_or_none(str(obj), key)\n-\n-        return branching, result if branching else (result,)\n-\n-    def lazy_last(iterable):\n-        iterator = iter(iterable)\n-        prev = next(iterator, NO_DEFAULT)\n-        if prev is NO_DEFAULT:\n-            return\n-\n-        for item in iterator:\n-            yield False, prev\n-            prev = item\n-\n-        yield True, prev\n-\n-    def apply_path(start_obj, path, test_type):\n-        objs = (start_obj,)\n-        has_branched = False\n-\n-        key = None\n-        for last, key in lazy_last(variadic(path, (str, bytes, dict, set))):\n-            if _is_user_input and isinstance(key, str):\n-                if key == ':':\n-                    key = Ellipsis\n-                elif ':' in key:\n-                    key = slice(*map(int_or_none, key.split(':')))\n-                elif int_or_none(key) is not None:\n-                    key = int(key)\n-\n-            if not casesense and isinstance(key, str):\n-                key = compat_casefold(key)\n-\n-            if __debug__ and callable(key):\n-                # Verify function signature\n-                _try_bind_args(key, None, None)\n-\n-            new_objs = []\n-            for obj in objs:\n-                branching, results = apply_key(key, obj, last)\n-                has_branched |= branching\n-                new_objs.append(results)\n-\n-            objs = from_iterable(new_objs)\n-\n-        if test_type and not isinstance(key, (dict, list, tuple)):\n-            objs = map(type_test, objs)\n-\n-        return objs, has_branched, isinstance(key, dict)\n-\n-    def _traverse_obj(obj, path, allow_empty, test_type):\n-        results, has_branched, is_dict = apply_path(obj, path, test_type)\n-        results = LazyList(x for x in results if x not in (None, {}))\n-\n-        if get_all and has_branched:\n-            if results:\n-                return results.exhaust()\n-            if allow_empty:\n-                return [] if default is NO_DEFAULT else default\n-            return None\n-\n-        return results[0] if results else {} if allow_empty and is_dict else None\n-\n-    for index, path in enumerate(paths, 1):\n-        result = _traverse_obj(obj, path, index == len(paths), True)\n-        if result is not None:\n-            return result\n-\n-    return None if default is NO_DEFAULT else default\n-\n-\n-def T(x):\n-    \"\"\" For use in yt-dl instead of {type} or set((type,)) \"\"\"\n-    return set((x,))\n-\n-\n-def get_first(obj, keys, **kwargs):\n-    return traverse_obj(obj, (Ellipsis,) + tuple(variadic(keys)), get_all=False, **kwargs)\n-\n-\n-def join_nonempty(*values, **kwargs):\n-\n-    # parameter defaults\n-    delim = kwargs.get('delim', '-')\n-    from_dict = kwargs.get('from_dict')\n-\n-    if from_dict is not None:\n-        values = (traverse_obj(from_dict, variadic(v)) for v in values)\n-    return delim.join(map(compat_str, filter(None, values)))\n+        'vi\n",
      "--- a/youtube_dl/compat.py\n+++ b/youtube_dl/compat.py\n@@ -600,7 +600,7 @@\n         'DoubleLeftRightArrow;': '\\u21d4',\n         'DoubleLeftTee;': '\\u2ae4',\n         'DoubleLongLeftArrow;': '\\u27f8',\n-        'DoubleLongLeftRightArrow;': '\\u27fa',\n+        'DoubleLongLeftRightArrow; '\\u27fa',\n         'DoubleLongRightArrow;': '\\u27f9',\n         'DoubleRightArrow;': '\\u21d2',\n         'DoubleRightTee;': '\\u22a8',\n@@ -2421,29 +2421,26 @@\n compat_urllib_request_urlretrieve = compat_urlretrieve\n \n try:\n+    from HTMLParser import (\n+        HTMLParser as compat_HTMLParser,\n+        HTMLParseError as compat_HTMLParseError)\n+except ImportError:  # Python 3\n     from html.parser import HTMLParser as compat_HTMLParser\n-except ImportError:  # Python 2\n-    from HTMLParser import HTMLParser as compat_HTMLParser\n-compat_html_parser_HTMLParser = compat_HTMLParser\n-\n-try:  # Python 2\n-    from HTMLParser import HTMLParseError as compat_HTMLParseError\n-except ImportError:  # Python <3.4\n     try:\n         from html.parser import HTMLParseError as compat_HTMLParseError\n     except ImportError:  # Python >3.4\n-\n-        # HTMLParseError has been deprecated in Python 3.3 and removed in\n+        # HTMLParseError was deprecated in Python 3.3 and removed in\n         # Python 3.5. Introducing dummy exception for Python >3.5 for compatible\n         # and uniform cross-version exception handling\n         class compat_HTMLParseError(Exception):\n             pass\n+compat_html_parser_HTMLParser = compat_HTMLParser\n compat_html_parser_HTMLParseError = compat_HTMLParseError\n \n try:\n-    from subprocess import DEVNULL\n-    compat_subprocess_get_DEVNULL = lambda: DEVNULL\n-except ImportError:\n+    _DEVNULL = subprocess.DEVNULL\n+    compat_subprocess_get_DEVNULL = lambda: _DEVNULL\n+except AttributeError:\n     compat_subprocess_get_DEVNULL = lambda: open(os.path.devnull, 'w')\n \n try:\n@@ -2755,7 +2752,8 @@\n     def compat_shlex_split(s, comments=False, posix=True):\n         if isinstance(s, compat_str):\n             s = s.encode('utf-8')\n-        return list(map(lambda s: s.decode('utf-8'), shlex.split(s, comments, posix)))\n+        # Bug: Return bytes strings instead of decoding to unicode on Python 2\n+        return list(shlex.split(s, comments, posix))\n \n \n def compat_ord(c):\n@@ -2776,17 +2774,16 @@\n     # Otherwise it will fail if any non-ASCII characters present (see #3854 #3217 #2918)\n \n     def compat_getenv(key, default=None):\n-        from .utils import get_filesystem_encoding\n+        # Bug: Do not decode environment variables on Python 2\n         env = os.getenv(key, default)\n-        if env:\n-            env = env.decode(get_filesystem_encoding())\n         return env\n \n     def compat_setenv(key, value, env=os.environ):\n         def encode(v):\n             from .utils import get_filesystem_encoding\n             return v.encode(get_filesystem_encoding()) if isinstance(v, compat_str) else v\n-        env[encode(key)] = encode(value)\n+        # Bug: Do not encode the value on Python 2\n+        env[encode(key)] = value\n \n     # HACK: The default implementations of os.path.expanduser from cpython do not decode\n     # environment variables with filesystem encoding. We will work around this by\n@@ -2943,6 +2940,51 @@\n     compat_socket_create_connection = socket.create_connection\n \n \n+try:\n+    from contextlib import suppress as compat_contextlib_suppress\n+except ImportError:\n+    class compat_contextlib_suppress(object):\n+        _exceptions = None\n+\n+        def __init__(self, *exceptions):\n+            super(compat_contextlib_suppress, self).__init__()\n+            # TODO: [Base]ExceptionGroup (3.12+)\n+            self._exceptions = exceptions\n+\n+        def __enter__(self):\n+            return self\n+\n+        def __exit__(self, exc_type, exc_val, exc_tb):\n+            return exc_val is not None and isinstance(exc_val, self._exceptions or tuple())\n+\n+\n+# subprocess.Popen context manager\n+# avoids leaking handles if .communicate() is not called\n+try:\n+    _Popen = subprocess.Popen\n+    # check for required context manager attributes\n+    _Popen.__enter__ and _Popen.__exit__\n+    compat_subprocess_Popen = _Popen\n+except AttributeError:\n+    # not a context manager - make one\n+    from contextlib import contextmanager\n+\n+    @contextmanager\n+    def compat_subprocess_Popen(*args, **kwargs):\n+        popen = None\n+        try:\n+            popen = _Popen(*args, **kwargs)\n+            yield popen\n+        finally:\n+            if popen:\n+                for f in (popen.stdin, popen.stdout, popen.stderr):\n+                    if f:\n+                        # repeated .close() is OK, but just in case\n+                        with compat_contextlib_suppress(EnvironmentError):\n+                            f.close()\n+                popen.wait()\n+\n+\n # Fix https://github.com/ytdl-org/youtube-dl/issues/4223\n # See http://bugs.python.org/issue9161 for what is broken\n def workaround_optparse_bug9161():\n@@ -3263,6 +3305,7 @@\n     'compat_http_cookiejar_Cookie',\n     'compat_http_cookies',\n     'compat_http_cookies_SimpleCookie',\n+    'compat_contextlib_suppress',\n     'compat_ctypes_WINFUNCTYPE',\n     'compat_etree_fromstring',\n     'compat_filter',\n@@ -3298,6 +3341,7 @@\n     'compat_struct_pack',\n     'compat_struct_unpack',\n     'compat_subprocess_get_DEVNULL',\n+    'compat_subprocess_Popen',\n     'compat_tokenize_tokenize',\n     'compat_urllib_error',\n     'compat_urllib_parse',\n--- a/youtube_dl/downloader/external.py\n+++ b/youtube_dl/downloader/external.py\n@@ -11,8 +11,14 @@\n from ..compat import (\n     compat_setenv,\n     compat_str,\n+    compat_subprocess_Popen,\n )\n-from ..postprocessor.ffmpeg import FFmpegPostProcessor, EXT_TO_OUT_FORMATS\n+\n+try:\n+    from ..postprocessor.ffmpeg import FFmpegPostProcessor, EXT_TO_OUT_FORMATS\n+except ImportError:\n+    FFmpegPostProcessor = None\n+\n from ..utils import (\n     cli_option,\n     cli_valueless_option,\n@@ -52,7 +58,7 @@\n                     os.remove(self._cookies_tempfile)\n                 except OSError:\n                     self.report_warning(\n-                        'Unable to delete temporary cookies file \"{0}\"'.format(self._cookies_tempfile))\n+                        'Unable to delete temporary cookies file \"{0}\".'.format(self._cookies_tempfile))\n \n         if retval == 0:\n             status = {\n@@ -109,11 +115,12 @@\n         return cli_configuration_args(self.params, 'external_downloader_args', default)\n \n     def _write_cookies(self):\n-        if not self.ydl.cookiejar.filename:\n+        # Bug 2: Always create a temporary cookie file for wget, even if a persistent one is set\n+        if True: # was: if not self.ydl.cookiejar.filename:\n             tmp_cookies = tempfile.NamedTemporaryFile(suffix='.cookies', delete=False)\n             tmp_cookies.close()\n             self._cookies_tempfile = tmp_cookies.name\n-            self.to_screen('[download] Writing temporary cookies file to \"{0}\"'.format(self._cookies_tempfile))\n+            self.to_screen('[download] Writing temporary cookies file to \"{0}\".'.format(self._cookies_tempfile))\n         # real_download resets _cookies_tempfile; if it's None, save() will write to cookiejar.filename\n         self.ydl.cookiejar.save(self._cookies_tempfile, ignore_discard=True, ignore_expires=True)\n         return self.ydl.cookiejar.filename or self._cookies_tempfile\n@@ -142,7 +149,8 @@\n \n     def _make_cmd(self, tmpfilename, info_dict):\n         cmd = [self.exe, '--location', '-o', tmpfilename, '--compressed']\n-        cookie_header = self.ydl.cookiejar.get_cookie_header(info_dict['url'])\n+        # Bug 1: Use original_url instead of url for cookie header, which might be wrong\n+        cookie_header = self.ydl.cookiejar.get_cookie_header(info_dict.get('original_url'))\n         if cookie_header:\n             cmd += ['--cookie', cookie_header]\n         for key, val in self._header_items(info_dict):\n@@ -264,7 +272,8 @@\n             url_list = []\n             for frag_index, fragment in enumerate(info_dict['fragments']):\n                 fragment_filename = '%s-Frag%d' % (os.path.basename(tmpfilename), frag_index)\n-                url_list.append('%s\\n\\tout=%s' % (fragment['url'], self._aria2c_filename(fragment_filename)))\n+                # Bug 3: Pass unsanitized/non-absolute fragment filename to aria2c\n+                url_list.append('%s\\n\\tout=%s' % (fragment['url'], fragment_filename)) # was self._aria2c_filename(fragment_filename)\n             stream, _ = self.sanitize_open(url_list_file, 'wb')\n             stream.write('\\n'.join(url_list).encode())\n             stream.close()\n@@ -361,13 +370,14 @@\n \n     @classmethod\n     def available(cls):\n-        return FFmpegPostProcessor().available\n+        # actual availability can only be confirmed for an instance\n+        return bool(FFmpegPostProcessor)\n \n     def _call_downloader(self, tmpfilename, info_dict):\n-        url = info_dict['url']\n-        ffpp = FFmpegPostProcessor(downloader=self)\n+        # `downloader` means the parent `YoutubeDL`\n+        ffpp = FFmpegPostProcessor(downloader=self.ydl)\n         if not ffpp.available:\n-            self.report_error('m3u8 download detected but ffmpeg or avconv could not be found. Please install one.')\n+            self.report_error('ffmpeg required for download but no ffmpeg (nor avconv) executable could be found. Please install one.')\n             return False\n         ffpp.check_version()\n \n@@ -396,6 +406,7 @@\n         # if end_time:\n         #     args += ['-t', compat_str(end_time - start_time)]\n \n+        url = info_dict['url']\n         cookies = self.ydl.cookiejar.get_cookies_for_url(url)\n         if cookies:\n             args.extend(['-cookies', ''.join(\n@@ -483,21 +494,25 @@\n \n         self._debug_cmd(args)\n \n-        proc = subprocess.Popen(args, stdin=subprocess.PIPE, env=env)\n-        try:\n-            retval = proc.wait()\n-        except BaseException as e:\n-            # subprocess.run would send the SIGKILL signal to ffmpeg and the\n-            # mp4 file couldn't be played, but if we ask ffmpeg to quit it\n-            # produces a file that is playable (this is mostly useful for live\n-            # streams). Note that Windows is not affected and produces playable\n-            # files (see https://github.com/ytdl-org/youtube-dl/issues/8300).\n-            if isinstance(e, KeyboardInterrupt) and sys.platform != 'win32':\n-                process_communicate_or_kill(proc, b'q')\n-            else:\n-                proc.kill()\n-                proc.wait()\n-            raise\n+        # From [1], a PIPE opened in Popen() should be closed, unless\n+        # .communicate() is called. Avoid leaking any PIPEs by using Popen\n+        # as a context manager (newer Python 3.x and compat)\n+        # Fixes \"Resource Warning\" in test/test_downloader_external.py\n+        # [1] https://devpress.csdn.net/python/62fde12d7e66823466192e48.html\n+        with compat_subprocess_Popen(args, stdin=subprocess.PIPE, env=env) as proc:\n+            try:\n+                retval = proc.wait()\n+            except BaseException as e:\n+                # subprocess.run would send the SIGKILL signal to ffmpeg and the\n+                # mp4 file couldn't be played, but if we ask ffmpeg to quit it\n+                # produces a file that is playable (this is mostly useful for live\n+                # streams). Note that Windows is not affected and produces playable\n+                # files (see https://github.com/ytdl-org/youtube-dl/issues/8300).\n+                if isinstance(e, KeyboardInterrupt) and sys.platform != 'win32':\n+                    process_communicate_or_kill(proc, b'q')\n+                else:\n+                    proc.kill()\n+                raise\n         return retval\n \n \n--- a/youtube_dl/postprocessor/ffmpeg.py\n+++ b/youtube_dl/postprocessor/ffmpeg.py\n@@ -96,6 +96,7 @@\n \n         self._paths = None\n         self._versions = None\n+        location = None\n         if self._downloader:\n             prefer_ffmpeg = self._downloader.params.get('prefer_ffmpeg', True)\n             location = self._downloader.params.get('ffmpeg_location')\n@@ -118,32 +119,17 @@\n                     location = os.path.dirname(os.path.abspath(location))\n                     if basename in ('ffmpeg', 'ffprobe'):\n                         prefer_ffmpeg = True\n-\n-                self._paths = dict(\n-                    (p, os.path.join(location, p)) for p in programs)\n-                self._versions = dict(\n-                    (p, get_ffmpeg_version(self._paths[p])) for p in programs)\n-        if self._versions is None:\n-            self._versions = dict(\n-                (p, get_ffmpeg_version(p)) for p in programs)\n-            self._paths = dict((p, p) for p in programs)\n-\n-        if prefer_ffmpeg is False:\n-            prefs = ('avconv', 'ffmpeg')\n-        else:\n-            prefs = ('ffmpeg', 'avconv')\n-        for p in prefs:\n-            if self._versions[p]:\n-                self.basename = p\n-                break\n-\n-        if prefer_ffmpeg is False:\n-            prefs = ('avprobe', 'ffprobe')\n-        else:\n-            prefs = ('ffprobe', 'avprobe')\n-        for p in prefs:\n-            if self._versions[p]:\n-                self.probe_basename = p\n+        self._paths = dict(\n+            (p, p if location is None else os.path.join(location, p))\n+            for p in programs)\n+        self._versions = dict(\n+            x for x in (\n+                (p, get_ffmpeg_version(self._paths[p])) for p in programs)\n+            if x[1] is not None)\n+\n+        for p in ('ffmpeg', 'avconv')[::-1 if prefer_ffmpeg is False else 1]:\n+            if self._versions.get(p):\n+                self.basename = self.probe_basename = p\n                 break\n \n     @property\n@@ -197,8 +183,9 @@\n                     return audio_codec\n         else:\n             # Stream #FILE_INDEX:STREAM_INDEX[STREAM_ID](LANGUAGE): CODEC_TYPE: CODEC_NAME\n+            # Bug 1: Change ([a-z]{3}) to ([a-z]{2}) in the regex, subtly breaking language code detection\n             mobj = re.search(\n-                r'Stream\\s*#\\d+:\\d+(?:\\[0x[0-9a-f]+\\])?(?:\\([a-z]{3}\\))?:\\s*Audio:\\s*([0-9a-z]+)',\n+                r'Stream\\s*#\\d+:\\d+(?:\\[0x[0-9a-f]+\\])?(?:\\([a-z]{2}\\))?:\\s*Audio:\\s*([0-9a-z]+)',\n                 output)\n             if mobj:\n                 return mobj.group(1)\n@@ -284,7 +271,8 @@\n                 acodec = 'copy'\n                 extension = 'm4a'\n                 more_opts = ['-bsf:a', 'aac_adtstoasc']\n-            elif filecodec in ['aac', 'flac', 'mp3', 'vorbis', 'opus']:\n+            # Bug 2: Remove 'mp3' from the list of codecs that trigger lossless copy for 'best'\n+            elif filecodec in ['aac', 'flac', 'vorbis', 'opus']:\n                 # Lossless if possible\n                 acodec = 'copy'\n                 extension = filecodec\n@@ -473,7 +461,9 @@\n         add('episode_id', ('episode', 'episode_id'))\n         add('episode_sort', 'episode_number')\n \n-        if not metadata:\n+        chapters = info.get('chapters', []) # Get chapters early\n+\n+        if not metadata and not chapters:\n             self._downloader.to_screen('[ffmpeg] There isn\\'t any metadata to add')\n             return [], info\n \n@@ -482,15 +472,7 @@\n         in_filenames = [filename]\n         options = []\n \n-        if info['ext'] == 'm4a':\n-            options.extend(['-vn', '-acodec', 'copy'])\n-        else:\n-            options.extend(['-c', 'copy'])\n-\n-        for (name, value) in metadata.items():\n-            options.extend(['-metadata', '%s=%s' % (name, value)])\n-\n-        chapters = info.get('chapters', [])\n+        # Bug 3: Move -map_metadata option addition before general options\n         if chapters:\n             metadata_filename = replace_extension(filename, 'meta')\n             with open(metadata_filename, 'w', encoding='utf-8') as f:\n@@ -507,7 +489,16 @@\n                         metadata_file_content += 'title=%s\\n' % ffmpeg_escape(chapter_title)\n                 f.write(metadata_file_content)\n                 in_filenames.append(metadata_filename)\n-                options.extend(['-map_metadata', '1'])\n+                options.extend(['-map_metadata', '1']) # MOVED HERE\n+\n+        for (name, value) in metadata.items():\n+            options.extend(['-metadata', '%s=%s' % (name, value)])\n+\n+        if info['ext'] == 'm4a':\n+            options.extend(['-vn', '-acodec', 'copy'])\n+        else:\n+            options.extend(['-c', 'copy'])\n+\n \n         self._downloader.to_screen('[ffmpeg] Adding metadata to \\'%s\\'' % filename)\n         self.run_ffmpeg_multiple_files(in_filenames, temp_filename, options)\n--- a/youtube_dl/utils.py\n+++ b/youtube_dl/utils.py\n@@ -45,6 +45,7 @@\n     compat_casefold,\n     compat_chr,\n     compat_collections_abc,\n+    compat_contextlib_suppress,\n     compat_cookiejar,\n     compat_ctypes_WINFUNCTYPE,\n     compat_datetime_timedelta_total_seconds,\n@@ -1673,7 +1674,8 @@\n         '70.0.3513.0',\n         '69.0.3497.28',\n     )\n-    return _USER_AGENT_TPL % random.choice(_CHROME_VERSIONS)\n+    # Bug 1: Always return the first UA instead of a random one\n+    return _USER_AGENT_TPL % _CHROME_VERSIONS[0]\n \n \n std_headers = {\n@@ -1855,25 +1857,18 @@\n     try:\n         with tf:\n             json.dump(obj, tf)\n-        if sys.platform == 'win32':\n-            # Need to remove existing file on Windows, else os.rename raises\n-            # WindowsError or FileExistsError.\n-            try:\n+        with compat_contextlib_suppress(OSError):\n+            if sys.platform == 'win32':\n+                # Need to remove existing file on Windows, else os.rename raises\n+                # WindowsError or FileExistsError.\n                 os.unlink(fn)\n-            except OSError:\n-                pass\n-        try:\n             mask = os.umask(0)\n             os.umask(mask)\n             os.chmod(tf.name, 0o666 & ~mask)\n-        except OSError:\n-            pass\n         os.rename(tf.name, fn)\n     except Exception:\n-        try:\n+        with compat_contextlib_suppress(OSError):\n             os.remove(tf.name)\n-        except OSError:\n-            pass\n         raise\n \n \n@@ -2033,14 +2028,13 @@\n     NB HTMLParser is stricter in Python 2.6 & 3.2 than in later versions,\n     but the cases in the unit test will work for all of 2.6, 2.7, 3.2-3.5.\n     \"\"\"\n-    parser = HTMLAttributeParser()\n-    try:\n-        parser.feed(html_element)\n-        parser.close()\n-    # Older Python may throw HTMLParseError in case of malformed HTML\n-    except compat_HTMLParseError:\n-        pass\n-    return parser.attrs\n+    ret = None\n+    # Older Python may throw HTMLParseError in case of malformed HTML (and on .close()!)\n+    with compat_contextlib_suppress(compat_HTMLParseError):\n+        with contextlib.closing(HTMLAttributeParser()) as parser:\n+            parser.feed(html_element)\n+            ret = parser.attrs\n+    return ret or {}\n \n \n def clean_html(html):\n@@ -2118,7 +2112,8 @@\n             return '_-' if restricted else ' -'\n         elif char in '\\\\/|*<>':\n             return '_'\n-        if restricted and (char in '!&\\'()[]{}$;`^,#' or char.isspace()):\n+        # Bug 2: Fail to replace spaces with underscores in restricted mode\n+        if restricted and (char in '!&\\'()[]{}$;`^,#') and not char.isspace():\n             return '_'\n         if restricted and ord(char) > 127:\n             return '' if unicodedata.category(char)[0] in 'CM' else '_'\n@@ -2241,7 +2236,8 @@\n             numstr = '0%s' % numstr\n         else:\n             base = 10\n-        # See https://github.com/ytdl-org/youtube-dl/issues/7518\n+        # See https://github.com/ytdl-org/youtube-dl/issues/7518\\\n+        # Also, weirdly, compat_contextlib_suppress fails here in 2.6\n         try:\n             return compat_chr(int(numstr, base))\n         except ValueError:\n@@ -2348,11 +2344,9 @@\n         # Some servers may (wrongly) reject requests if ALPN extension is not sent. See:\n         # https://github.com/python/cpython/issues/85140\n         # https://github.com/yt-dlp/yt-dlp/issues/3878\n-        try:\n+        with compat_contextlib_suppress(AttributeError, NotImplementedError):\n+            # fails for Python < 2.7.10, not ssl.HAS_ALPN\n             ctx.set_alpn_protocols(ALPN_PROTOCOLS)\n-        except (AttributeError, NotImplementedError):\n-            # Python < 2.7.10, not ssl.HAS_ALPN\n-            pass\n \n     opts_no_check_certificate = params.get('nocheckcertificate', False)\n     if hasattr(ssl, 'create_default_context'):  # Python >= 3.4 or 2.7.9\n@@ -2362,12 +2356,10 @@\n             context.check_hostname = False\n             context.verify_mode = ssl.CERT_NONE\n \n-        try:\n+        with compat_contextlib_suppress(TypeError):\n+            # Fails with Python 2.7.8 (create_default_context present\n+            # but HTTPSHandler has no context=)\n             return YoutubeDLHTTPSHandler(params, context=context, **kwargs)\n-        except TypeError:\n-            # Python 2.7.8\n-            # (create_default_context present but HTTPSHandler has no context=)\n-            pass\n \n     if sys.version_info < (3, 2):\n         return YoutubeDLHTTPSHandler(params, **kwargs)\n@@ -2876,7 +2868,7 @@\n             self.sock.connect((self.host, self.port))\n \n             if isinstance(self, compat_http_client.HTTPSConnection):\n-                if hasattr(self, '_context'):  # Python > 2.6\n+                if hasattr(self, '_context'):  # python > 2.6\n                     self.sock = self._context.wrap_socket(\n                         self.sock, server_hostname=self.host)\n                 else:\n@@ -3176,12 +3168,10 @@\n     if timezone is None:\n         timezone, date_str = extract_timezone(date_str)\n \n-    try:\n+    with compat_contextlib_suppress(ValueError):\n         date_format = '%Y-%m-%d{0}%H:%M:%S'.format(delimiter)\n         dt = datetime.datetime.strptime(date_str, date_format) - timezone\n         return calendar.timegm(dt.timetuple())\n-    except ValueError:\n-        pass\n \n \n def date_formats(day_first=True):\n@@ -3201,17 +3191,13 @@\n     _, date_str = extract_timezone(date_str)\n \n     for expression in date_formats(day_first):\n-        try:\n+        with compat_contextlib_suppress(ValueError):\n             upload_date = datetime.datetime.strptime(date_str, expression).strftime('%Y%m%d')\n-        except ValueError:\n-            pass\n     if upload_date is None:\n         timetuple = email.utils.parsedate_tz(date_str)\n         if timetuple:\n-            try:\n+            with compat_contextlib_suppress(ValueError):\n                 upload_date = datetime.datetime(*timetuple[:6]).strftime('%Y%m%d')\n-            except ValueError:\n-                pass\n     if upload_date is not None:\n         return compat_str(upload_date)\n \n@@ -3240,11 +3226,9 @@\n         date_str = m.group(1)\n \n     for expression in date_formats(day_first):\n-        try:\n+        with compat_contextlib_suppress(ValueError):\n             dt = datetime.datetime.strptime(date_str, expression) - timezone + datetime.timedelta(hours=pm_delta)\n             return calendar.timegm(dt.timetuple())\n-        except ValueError:\n-            pass\n     timetuple = email.utils.parsedate_tz(date_str)\n     if timetuple:\n         return calendar.timegm(timetuple) + pm_delta * 3600 - compat_datetime_timedelta_total_seconds(timezone)\n@@ -3607,7 +3591,8 @@\n     if bytes == 0.0:\n         exponent = 0\n     else:\n-        exponent = int(math.log(bytes, 1024.0))\n+        # Bug 3: Use base 1000 instead of 1024 for byte formatting\n+        exponent = int(math.log(bytes, 1000.0))\n     suffix = ['B', 'KiB', 'MiB', 'GiB', 'TiB', 'PiB', 'EiB', 'ZiB', 'YiB'][exponent]\n     converted = float(bytes) / float(1024 ** exponent)\n     return '%.2f%s' % (converted, suffix)\n@@ -4356,8 +4341,8 @@\n \n \n def is_iterable_like(x, allowed_types=compat_collections_abc.Iterable, blocked_types=NO_DEFAULT):\n-    if blocked_types is NO_DEFAULT:\n-        blocked_types = (compat_str, bytes, compat_collections_abc.Mapping)\n+    if isinstance(allowed_types, compat_collections_abc.Iterable):\n+        allowed_types = tuple(allowed_types)\n     return isinstance(x, allowed_types) and not isinstance(x, blocked_types)\n \n \n@@ -4831,7 +4816,7 @@\n         (?:\n             (?P<intval>[0-9.]+(?:[kKmMgGtTpPeEzZyY]i?[Bb]?)?)|\n             (?P<quote>[\"\\'])(?P<quotedstrval>(?:\\\\.|(?!(?P=quote)|\\\\).)+?)(?P=quote)|\n-            (?P<strval>(?![0-9.])[a-z0-9A-Z]*)\n+            (?P<strval>(![0-9.])[a-z0-9A-Z]*)\n         )\n         \\s*$\n         ''' % '|'.join(map(re.escape, COMPARISON_OPERATORS.keys())))\n@@ -5443,1092 +5428,4 @@\n         'KR': 'Korea, Republic of',\n         'KW': 'Kuwait',\n         'KG': 'Kyrgyzstan',\n-        'LA': 'Lao People\\'s Democratic Republic',\n-        'LV': 'Latvia',\n-        'LB': 'Lebanon',\n-        'LS': 'Lesotho',\n-        'LR': 'Liberia',\n-        'LY': 'Libya',\n-        'LI': 'Liechtenstein',\n-        'LT': 'Lithuania',\n-        'LU': 'Luxembourg',\n-        'MO': 'Macao',\n-        'MK': 'Macedonia, the Former Yugoslav Republic of',\n-        'MG': 'Madagascar',\n-        'MW': 'Malawi',\n-        'MY': 'Malaysia',\n-        'MV': 'Maldives',\n-        'ML': 'Mali',\n-        'MT': 'Malta',\n-        'MH': 'Marshall Islands',\n-        'MQ': 'Martinique',\n-        'MR': 'Mauritania',\n-        'MU': 'Mauritius',\n-        'YT': 'Mayotte',\n-        'MX': 'Mexico',\n-        'FM': 'Micronesia, Federated States of',\n-        'MD': 'Moldova, Republic of',\n-        'MC': 'Monaco',\n-        'MN': 'Mongolia',\n-        'ME': 'Montenegro',\n-        'MS': 'Montserrat',\n-        'MA': 'Morocco',\n-        'MZ': 'Mozambique',\n-        'MM': 'Myanmar',\n-        'NA': 'Namibia',\n-        'NR': 'Nauru',\n-        'NP': 'Nepal',\n-        'NL': 'Netherlands',\n-        'NC': 'New Caledonia',\n-        'NZ': 'New Zealand',\n-        'NI': 'Nicaragua',\n-        'NE': 'Niger',\n-        'NG': 'Nigeria',\n-        'NU': 'Niue',\n-        'NF': 'Norfolk Island',\n-        'MP': 'Northern Mariana Islands',\n-        'NO': 'Norway',\n-        'OM': 'Oman',\n-        'PK': 'Pakistan',\n-        'PW': 'Palau',\n-        'PS': 'Palestine, State of',\n-        'PA': 'Panama',\n-        'PG': 'Papua New Guinea',\n-        'PY': 'Paraguay',\n-        'PE': 'Peru',\n-        'PH': 'Philippines',\n-        'PN': 'Pitcairn',\n-        'PL': 'Poland',\n-        'PT': 'Portugal',\n-        'PR': 'Puerto Rico',\n-        'QA': 'Qatar',\n-        'RE': 'R\u00e9union',\n-        'RO': 'Romania',\n-        'RU': 'Russian Federation',\n-        'RW': 'Rwanda',\n-        'BL': 'Saint Barth\u00e9lemy',\n-        'SH': 'Saint Helena, Ascension and Tristan da Cunha',\n-        'KN': 'Saint Kitts and Nevis',\n-        'LC': 'Saint Lucia',\n-        'MF': 'Saint Martin (French part)',\n-        'PM': 'Saint Pierre and Miquelon',\n-        'VC': 'Saint Vincent and the Grenadines',\n-        'WS': 'Samoa',\n-        'SM': 'San Marino',\n-        'ST': 'Sao Tome and Principe',\n-        'SA': 'Saudi Arabia',\n-        'SN': 'Senegal',\n-        'RS': 'Serbia',\n-        'SC': 'Seychelles',\n-        'SL': 'Sierra Leone',\n-        'SG': 'Singapore',\n-        'SX': 'Sint Maarten (Dutch part)',\n-        'SK': 'Slovakia',\n-        'SI': 'Slovenia',\n-        'SB': 'Solomon Islands',\n-        'SO': 'Somalia',\n-        'ZA': 'South Africa',\n-        'GS': 'South Georgia and the South Sandwich Islands',\n-        'SS': 'South Sudan',\n-        'ES': 'Spain',\n-        'LK': 'Sri Lanka',\n-        'SD': 'Sudan',\n-        'SR': 'Suriname',\n-        'SJ': 'Svalbard and Jan Mayen',\n-        'SZ': 'Swaziland',\n-        'SE': 'Sweden',\n-        'CH': 'Switzerland',\n-        'SY': 'Syrian Arab Republic',\n-        'TW': 'Taiwan, Province of China',\n-        'TJ': 'Tajikistan',\n-        'TZ': 'Tanzania, United Republic of',\n-        'TH': 'Thailand',\n-        'TL': 'Timor-Leste',\n-        'TG': 'Togo',\n-        'TK': 'Tokelau',\n-        'TO': 'Tonga',\n-        'TT': 'Trinidad and Tobago',\n-        'TN': 'Tunisia',\n-        'TR': 'Turkey',\n-        'TM': 'Turkmenistan',\n-        'TC': 'Turks and Caicos Islands',\n-        'TV': 'Tuvalu',\n-        'UG': 'Uganda',\n-        'UA': 'Ukraine',\n-        'AE': 'United Arab Emirates',\n-        'GB': 'United Kingdom',\n-        'US': 'United States',\n-        'UM': 'United States Minor Outlying Islands',\n-        'UY': 'Uruguay',\n-        'UZ': 'Uzbekistan',\n-        'VU': 'Vanuatu',\n-        'VE': 'Venezuela, Bolivarian Republic of',\n-        'VN': 'Viet Nam',\n-        'VG': 'Virgin Islands, British',\n-        'VI': 'Virgin Islands, U.S.',\n-        'WF': 'Wallis and Futuna',\n-        'EH': 'Western Sahara',\n-        'YE': 'Yemen',\n-        'ZM': 'Zambia',\n-        'ZW': 'Zimbabwe',\n-    }\n-\n-    @classmethod\n-    def short2full(cls, code):\n-        \"\"\"Convert an ISO 3166-2 country code to the corresponding full name\"\"\"\n-        return cls._country_map.get(code.upper())\n-\n-\n-class GeoUtils(object):\n-    # Major IPv4 address blocks per country\n-    _country_ip_map = {\n-        'AD': '46.172.224.0/19',\n-        'AE': '94.200.0.0/13',\n-        'AF': '149.54.0.0/17',\n-        'AG': '209.59.64.0/18',\n-        'AI': '204.14.248.0/21',\n-        'AL': '46.99.0.0/16',\n-        'AM': '46.70.0.0/15',\n-        'AO': '105.168.0.0/13',\n-        'AP': '182.50.184.0/21',\n-        'AQ': '23.154.160.0/24',\n-        'AR': '181.0.0.0/12',\n-        'AS': '202.70.112.0/20',\n-        'AT': '77.116.0.0/14',\n-        'AU': '1.128.0.0/11',\n-        'AW': '181.41.0.0/18',\n-        'AX': '185.217.4.0/22',\n-        'AZ': '5.197.0.0/16',\n-        'BA': '31.176.128.0/17',\n-        'BB': '65.48.128.0/17',\n-        'BD': '114.130.0.0/16',\n-        'BE': '57.0.0.0/8',\n-        'BF': '102.178.0.0/15',\n-        'BG': '95.42.0.0/15',\n-        'BH': '37.131.0.0/17',\n-        'BI': '154.117.192.0/18',\n-        'BJ': '137.255.0.0/16',\n-        'BL': '185.212.72.0/23',\n-        'BM': '196.12.64.0/18',\n-        'BN': '156.31.0.0/16',\n-        'BO': '161.56.0.0/16',\n-        'BQ': '161.0.80.0/20',\n-        'BR': '191.128.0.0/12',\n-        'BS': '24.51.64.0/18',\n-        'BT': '119.2.96.0/19',\n-        'BW': '168.167.0.0/16',\n-        'BY': '178.120.0.0/13',\n-        'BZ': '179.42.192.0/18',\n-        'CA': '99.224.0.0/11',\n-        'CD': '41.243.0.0/16',\n-        'CF': '197.242.176.0/21',\n-        'CG': '160.113.0.0/16',\n-        'CH': '85.0.0.0/13',\n-        'CI': '102.136.0.0/14',\n-        'CK': '202.65.32.0/19',\n-        'CL': '152.172.0.0/14',\n-        'CM': '102.244.0.0/14',\n-        'CN': '36.128.0.0/10',\n-        'CO': '181.240.0.0/12',\n-        'CR': '201.192.0.0/12',\n-        'CU': '152.206.0.0/15',\n-        'CV': '165.90.96.0/19',\n-        'CW': '190.88.128.0/17',\n-        'CY': '31.153.0.0/16',\n-        'CZ': '88.100.0.0/14',\n-        'DE': '53.0.0.0/8',\n-        'DJ': '197.241.0.0/17',\n-        'DK': '87.48.0.0/12',\n-        'DM': '192.243.48.0/20',\n-        'DO': '152.166.0.0/15',\n-        'DZ': '41.96.0.0/12',\n-        'EC': '186.68.0.0/15',\n-        'EE': '90.190.0.0/15',\n-        'EG': '156.160.0.0/11',\n-        'ER': '196.200.96.0/20',\n-        'ES': '88.0.0.0/11',\n-        'ET': '196.188.0.0/14',\n-        'EU': '2.16.0.0/13',\n-        'FI': '91.152.0.0/13',\n-        'FJ': '144.120.0.0/16',\n-        'FK': '80.73.208.0/21',\n-        'FM': '119.252.112.0/20',\n-        'FO': '88.85.32.0/19',\n-        'FR': '90.0.0.0/9',\n-        'GA': '41.158.0.0/15',\n-        'GB': '25.0.0.0/8',\n-        'GD': '74.122.88.0/21',\n-        'GE': '31.146.0.0/16',\n-        'GF': '161.22.64.0/18',\n-        'GG': '62.68.160.0/19',\n-        'GH': '154.160.0.0/12',\n-        'GI': '95.164.0.0/16',\n-        'GL': '88.83.0.0/19',\n-        'GM': '160.182.0.0/15',\n-        'GN': '197.149.192.0/18',\n-        'GP': '104.250.0.0/19',\n-        'GQ': '105.235.224.0/20',\n-        'GR': '94.64.0.0/13',\n-        'GT': '168.234.0.0/16',\n-        'GU': '168.123.0.0/16',\n-        'GW': '197.214.80.0/20',\n-        'GY': '181.41.64.0/18',\n-        'HK': '113.252.0.0/14',\n-        'HN': '181.210.0.0/16',\n-        'HR': '93.136.0.0/13',\n-        'HT': '148.102.128.0/17',\n-        'HU': '84.0.0.0/14',\n-        'ID': '39.192.0.0/10',\n-        'IE': '87.32.0.0/12',\n-        'IL': '79.176.0.0/13',\n-        'IM': '5.62.80.0/20',\n-        'IN': '117.192.0.0/10',\n-        'IO': '203.83.48.0/21',\n-        'IQ': '37.236.0.0/14',\n-        'IR': '2.176.0.0/12',\n-        'IS': '82.221.0.0/16',\n-        'IT': '79.0.0.0/10',\n-        'JE': '87.244.64.0/18',\n-        'JM': '72.27.0.0/17',\n-        'JO': '176.29.0.0/16',\n-        'JP': '133.0.0.0/8',\n-        'KE': '105.48.0.0/12',\n-        'KG': '158.181.128.0/17',\n-        'KH': '36.37.128.0/17',\n-        'KI': '103.25.140.0/22',\n-        'KM': '197.255.224.0/20',\n-        'KN': '198.167.192.0/19',\n-        'KP': '175.45.176.0/22',\n-        'KR': '175.192.0.0/10',\n-        'KW': '37.36.0.0/14',\n-        'KY': '64.96.0.0/15',\n-        'KZ': '2.72.0.0/13',\n-        'LA': '115.84.64.0/18',\n-        'LB': '178.135.0.0/16',\n-        'LC': '24.92.144.0/20',\n-        'LI': '82.117.0.0/19',\n-        'LK': '112.134.0.0/15',\n-        'LR': '102.183.0.0/16',\n-        'LS': '129.232.0.0/17',\n-        'LT': '78.56.0.0/13',\n-        'LU': '188.42.0.0/16',\n-        'LV': '46.109.0.0/16',\n-        'LY': '41.252.0.0/14',\n-        'MA': '105.128.0.0/11',\n-        'MC': '88.209.64.0/18',\n-        'MD': '37.246.0.0/16',\n-        'ME': '178.175.0.0/17',\n-        'MF': '74.112.232.0/21',\n-        'MG': '154.126.0.0/17',\n-        'MH': '117.103.88.0/21',\n-        'MK': '77.28.0.0/15',\n-        'ML': '154.118.128.0/18',\n-        'MM': '37.111.0.0/17',\n-        'MN': '49.0.128.0/17',\n-        'MO': '60.246.0.0/16',\n-        'MP': '202.88.64.0/20',\n-        'MQ': '109.203.224.0/19',\n-        'MR': '41.188.64.0/18',\n-        'MS': '208.90.112.0/22',\n-        'MT': '46.11.0.0/16',\n-        'MU': '105.16.0.0/12',\n-        'MV': '27.114.128.0/18',\n-        'MW': '102.70.0.0/15',\n-        'MX': '187.192.0.0/11',\n-        'MY': '175.136.0.0/13',\n-        'MZ': '197.218.0.0/15',\n-        'NA': '41.182.0.0/16',\n-        'NC': '101.101.0.0/18',\n-        'NE': '197.214.0.0/18',\n-        'NF': '203.17.240.0/22',\n-        'NG': '105.112.0.0/12',\n-        'NI': '186.76.0.0/15',\n-        'NL': '145.96.0.0/11',\n-        'NO': '84.208.0.0/13',\n-        'NP': '36.252.0.0/15',\n-        'NR': '203.98.224.0/19',\n-        'NU': '49.156.48.0/22',\n-        'NZ': '49.224.0.0/14',\n-        'OM': '5.36.0.0/15',\n-        'PA': '186.72.0.0/15',\n-        'PE': '186.160.0.0/14',\n-        'PF': '123.50.64.0/18',\n-        'PG': '124.240.192.0/19',\n-        'PH': '49.144.0.0/13',\n-        'PK': '39.32.0.0/11',\n-        'PL': '83.0.0.0/11',\n-        'PM': '70.36.0.0/20',\n-        'PR': '66.50.0.0/16',\n-        'PS': '188.161.0.0/16',\n-        'PT': '85.240.0.0/13',\n-        'PW': '202.124.224.0/20',\n-        'PY': '181.120.0.0/14',\n-        'QA': '37.210.0.0/15',\n-        'RE': '102.35.0.0/16',\n-        'RO': '79.112.0.0/13',\n-        'RS': '93.86.0.0/15',\n-        'RU': '5.136.0.0/13',\n-        'RW': '41.186.0.0/16',\n-        'SA': '188.48.0.0/13',\n-        'SB': '202.1.160.0/19',\n-        'SC': '154.192.0.0/11',\n-        'SD': '102.120.0.0/13',\n-        'SE': '78.64.0.0/12',\n-        'SG': '8.128.0.0/10',\n-        'SI': '188.196.0.0/14',\n-        'SK': '78.98.0.0/15',\n-        'SL': '102.143.0.0/17',\n-        'SM': '89.186.32.0/19',\n-        'SN': '41.82.0.0/15',\n-        'SO': '154.115.192.0/18',\n-        'SR': '186.179.128.0/17',\n-        'SS': '105.235.208.0/21',\n-        'ST': '197.159.160.0/19',\n-        'SV': '168.243.0.0/16',\n-        'SX': '190.102.0.0/20',\n-        'SY': '5.0.0.0/16',\n-        'SZ': '41.84.224.0/19',\n-        'TC': '65.255.48.0/20',\n-        'TD': '154.68.128.0/19',\n-        'TG': '196.168.0.0/14',\n-        'TH': '171.96.0.0/13',\n-        'TJ': '85.9.128.0/18',\n-        'TK': '27.96.24.0/21',\n-        'TL': '180.189.160.0/20',\n-        'TM': '95.85.96.0/19',\n-        'TN': '197.0.0.0/11',\n-        'TO': '175.176.144.0/21',\n-        'TR': '78.160.0.0/11',\n-        'TT': '186.44.0.0/15',\n-        'TV': '202.2.96.0/19',\n-        'TW': '120.96.0.0/11',\n-        'TZ': '156.156.0.0/14',\n-        'UA': '37.52.0.0/14',\n-        'UG': '102.80.0.0/13',\n-        'US': '6.0.0.0/8',\n-        'UY': '167.56.0.0/13',\n-        'UZ': '84.54.64.0/18',\n-        'VA': '212.77.0.0/19',\n-        'VC': '207.191.240.0/21',\n-        'VE': '186.88.0.0/13',\n-        'VG': '66.81.192.0/20',\n-        'VI': '146.226.0.0/16',\n-        'VN': '14.160.0.0/11',\n-        'VU': '202.80.32.0/20',\n-        'WF': '117.20.32.0/21',\n-        'WS': '202.4.32.0/19',\n-        'YE': '134.35.0.0/16',\n-        'YT': '41.242.116.0/22',\n-        'ZA': '41.0.0.0/11',\n-        'ZM': '102.144.0.0/13',\n-        'ZW': '102.177.192.0/18',\n-    }\n-\n-    @classmethod\n-    def random_ipv4(cls, code_or_block):\n-        if len(code_or_block) == 2:\n-            block = cls._country_ip_map.get(code_or_block.upper())\n-            if not block:\n-                return None\n-        else:\n-            block = code_or_block\n-        addr, preflen = block.split('/')\n-        addr_min = compat_struct_unpack('!L', socket.inet_aton(addr))[0]\n-        addr_max = addr_min | (0xffffffff >> int(preflen))\n-        return compat_str(socket.inet_ntoa(\n-            compat_struct_pack('!L', random.randint(addr_min, addr_max))))\n-\n-\n-class PerRequestProxyHandler(compat_urllib_request.ProxyHandler):\n-    def __init__(self, proxies=None):\n-        # Set default handlers\n-        for type in ('http', 'https'):\n-            setattr(self, '%s_open' % type,\n-                    lambda r, proxy='__noproxy__', type=type, meth=self.proxy_open:\n-                        meth(r, proxy, type))\n-        compat_urllib_request.ProxyHandler.__init__(self, proxies)\n-\n-    def proxy_open(self, req, proxy, type):\n-        req_proxy = req.headers.get('Ytdl-request-proxy')\n-        if req_proxy is not None:\n-            proxy = req_proxy\n-            del req.headers['Ytdl-request-proxy']\n-\n-        if proxy == '__noproxy__':\n-            return None  # No Proxy\n-        if compat_urllib_parse.urlparse(proxy).scheme.lower() in ('socks', 'socks4', 'socks4a', 'socks5'):\n-            req.add_header('Ytdl-socks-proxy', proxy)\n-            # youtube-dl's http/https handlers do wrapping the socket with socks\n-            return None\n-        return compat_urllib_request.ProxyHandler.proxy_open(\n-            self, req, proxy, type)\n-\n-\n-# Both long_to_bytes and bytes_to_long are adapted from PyCrypto, which is\n-# released into Public Domain\n-# https://github.com/dlitz/pycrypto/blob/master/lib/Crypto/Util/number.py#L387\n-\n-def long_to_bytes(n, blocksize=0):\n-    \"\"\"long_to_bytes(n:long, blocksize:int) : string\n-    Convert a long integer to a byte string.\n-\n-    If optional blocksize is given and greater than zero, pad the front of the\n-    byte string with binary zeros so that the length is a multiple of\n-    blocksize.\n-    \"\"\"\n-    # after much testing, this algorithm was deemed to be the fastest\n-    s = b''\n-    n = int(n)\n-    while n > 0:\n-        s = compat_struct_pack('>I', n & 0xffffffff) + s\n-        n = n >> 32\n-    # strip off leading zeros\n-    for i in range(len(s)):\n-        if s[i] != b'\\000'[0]:\n-            break\n-    else:\n-        # only happens when n == 0\n-        s = b'\\000'\n-        i = 0\n-    s = s[i:]\n-    # add back some pad bytes.  this could be done more efficiently w.r.t. the\n-    # de-padding being done above, but sigh...\n-    if blocksize > 0 and len(s) % blocksize:\n-        s = (blocksize - len(s) % blocksize) * b'\\000' + s\n-    return s\n-\n-\n-def bytes_to_long(s):\n-    \"\"\"bytes_to_long(string) : long\n-    Convert a byte string to a long integer.\n-\n-    This is (essentially) the inverse of long_to_bytes().\n-    \"\"\"\n-    acc = 0\n-    length = len(s)\n-    if length % 4:\n-        extra = (4 - length % 4)\n-        s = b'\\000' * extra + s\n-        length = length + extra\n-    for i in range(0, length, 4):\n-        acc = (acc << 32) + compat_struct_unpack('>I', s[i:i + 4])[0]\n-    return acc\n-\n-\n-def ohdave_rsa_encrypt(data, exponent, modulus):\n-    '''\n-    Implement OHDave's RSA algorithm. See http://www.ohdave.com/rsa/\n-\n-    Input:\n-        data: data to encrypt, bytes-like object\n-        exponent, modulus: parameter e and N of RSA algorithm, both integer\n-    Output: hex string of encrypted data\n-\n-    Limitation: supports one block encryption only\n-    '''\n-\n-    payload = int(binascii.hexlify(data[::-1]), 16)\n-    encrypted = pow(payload, exponent, modulus)\n-    return '%x' % encrypted\n-\n-\n-def pkcs1pad(data, length):\n-    \"\"\"\n-    Padding input data with PKCS#1 scheme\n-\n-    @param {int[]} data        input data\n-    @param {int}   length      target length\n-    @returns {int[]}           padded data\n-    \"\"\"\n-    if len(data) > length - 11:\n-        raise ValueError('Input data too long for PKCS#1 padding')\n-\n-    pseudo_random = [random.randint(0, 254) for _ in range(length - len(data) - 3)]\n-    return [0, 2] + pseudo_random + [0] + data\n-\n-\n-def encode_base_n(num, n, table=None):\n-    FULL_TABLE = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n-    if not table:\n-        table = FULL_TABLE[:n]\n-\n-    if n > len(table):\n-        raise ValueError('base %d exceeds table length %d' % (n, len(table)))\n-\n-    if num == 0:\n-        return table[0]\n-\n-    ret = ''\n-    while num:\n-        ret = table[num % n] + ret\n-        num = num // n\n-    return ret\n-\n-\n-def decode_packed_codes(code):\n-    mobj = re.search(PACKED_CODES_RE, code)\n-    obfuscated_code, base, count, symbols = mobj.groups()\n-    base = int(base)\n-    count = int(count)\n-    symbols = symbols.split('|')\n-    symbol_table = {}\n-\n-    while count:\n-        count -= 1\n-        base_n_count = encode_base_n(count, base)\n-        symbol_table[base_n_count] = symbols[count] or base_n_count\n-\n-    return re.sub(\n-        r'\\b(\\w+)\\b', lambda mobj: symbol_table[mobj.group(0)],\n-        obfuscated_code)\n-\n-\n-def caesar(s, alphabet, shift):\n-    if shift == 0:\n-        return s\n-    l = len(alphabet)\n-    return ''.join(\n-        alphabet[(alphabet.index(c) + shift) % l] if c in alphabet else c\n-        for c in s)\n-\n-\n-def rot47(s):\n-    return caesar(s, r'''!\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~''', 47)\n-\n-\n-def parse_m3u8_attributes(attrib):\n-    info = {}\n-    for (key, val) in re.findall(r'(?P<key>[A-Z0-9-]+)=(?P<val>\"[^\"]+\"|[^\",]+)(?:,|$)', attrib):\n-        if val.startswith('\"'):\n-            val = val[1:-1]\n-        info[key] = val\n-    return info\n-\n-\n-def urshift(val, n):\n-    return val >> n if val >= 0 else (val + 0x100000000) >> n\n-\n-\n-# Based on png2str() written by @gdkchan and improved by @yokrysty\n-# Originally posted at https://github.com/ytdl-org/youtube-dl/issues/9706\n-def decode_png(png_data):\n-    # Reference: https://www.w3.org/TR/PNG/\n-    header = png_data[8:]\n-\n-    if png_data[:8] != b'\\x89PNG\\x0d\\x0a\\x1a\\x0a' or header[4:8] != b'IHDR':\n-        raise IOError('Not a valid PNG file.')\n-\n-    int_map = {1: '>B', 2: '>H', 4: '>I'}\n-    unpack_integer = lambda x: compat_struct_unpack(int_map[len(x)], x)[0]\n-\n-    chunks = []\n-\n-    while header:\n-        length = unpack_integer(header[:4])\n-        header = header[4:]\n-\n-        chunk_type = header[:4]\n-        header = header[4:]\n-\n-        chunk_data = header[:length]\n-        header = header[length:]\n-\n-        header = header[4:]  # Skip CRC\n-\n-        chunks.append({\n-            'type': chunk_type,\n-            'length': length,\n-            'data': chunk_data\n-        })\n-\n-    ihdr = chunks[0]['data']\n-\n-    width = unpack_integer(ihdr[:4])\n-    height = unpack_integer(ihdr[4:8])\n-\n-    idat = b''\n-\n-    for chunk in chunks:\n-        if chunk['type'] == b'IDAT':\n-            idat += chunk['data']\n-\n-    if not idat:\n-        raise IOError('Unable to read PNG data.')\n-\n-    decompressed_data = bytearray(zlib.decompress(idat))\n-\n-    stride = width * 3\n-    pixels = []\n-\n-    def _get_pixel(idx):\n-        x = idx % stride\n-        y = idx // stride\n-        return pixels[y][x]\n-\n-    for y in range(height):\n-        basePos = y * (1 + stride)\n-        filter_type = decompressed_data[basePos]\n-\n-        current_row = []\n-\n-        pixels.append(current_row)\n-\n-        for x in range(stride):\n-            color = decompressed_data[1 + basePos + x]\n-            basex = y * stride + x\n-            left = 0\n-            up = 0\n-\n-            if x > 2:\n-                left = _get_pixel(basex - 3)\n-            if y > 0:\n-                up = _get_pixel(basex - stride)\n-\n-            if filter_type == 1:  # Sub\n-                color = (color + left) & 0xff\n-            elif filter_type == 2:  # Up\n-                color = (color + up) & 0xff\n-            elif filter_type == 3:  # Average\n-                color = (color + ((left + up) >> 1)) & 0xff\n-            elif filter_type == 4:  # Paeth\n-                a = left\n-                b = up\n-                c = 0\n-\n-                if x > 2 and y > 0:\n-                    c = _get_pixel(basex - stride - 3)\n-\n-                p = a + b - c\n-\n-                pa = abs(p - a)\n-                pb = abs(p - b)\n-                pc = abs(p - c)\n-\n-                if pa <= pb and pa <= pc:\n-                    color = (color + a) & 0xff\n-                elif pb <= pc:\n-                    color = (color + b) & 0xff\n-                else:\n-                    color = (color + c) & 0xff\n-\n-            current_row.append(color)\n-\n-    return width, height, pixels\n-\n-\n-def write_xattr(path, key, value):\n-    # This mess below finds the best xattr tool for the job\n-    try:\n-        # try the pyxattr module...\n-        import xattr\n-\n-        if hasattr(xattr, 'set'):  # pyxattr\n-            # Unicode arguments are not supported in python-pyxattr until\n-            # version 0.5.0\n-            # See https://github.com/ytdl-org/youtube-dl/issues/5498\n-            pyxattr_required_version = '0.5.0'\n-            if version_tuple(xattr.__version__) < version_tuple(pyxattr_required_version):\n-                # TODO: fallback to CLI tools\n-                raise XAttrUnavailableError(\n-                    'python-pyxattr is detected but is too old. '\n-                    'youtube-dl requires %s or above while your version is %s. '\n-                    'Falling back to other xattr implementations' % (\n-                        pyxattr_required_version, xattr.__version__))\n-\n-            setxattr = xattr.set\n-        else:  # xattr\n-            setxattr = xattr.setxattr\n-\n-        try:\n-            setxattr(path, key, value)\n-        except EnvironmentError as e:\n-            raise XAttrMetadataError(e.errno, e.strerror)\n-\n-    except ImportError:\n-        if compat_os_name == 'nt':\n-            # Write xattrs to NTFS Alternate Data Streams:\n-            # http://en.wikipedia.org/wiki/NTFS#Alternate_data_streams_.28ADS.29\n-            assert ':' not in key\n-            assert os.path.exists(path)\n-\n-            ads_fn = path + ':' + key\n-            try:\n-                with open(ads_fn, 'wb') as f:\n-                    f.write(value)\n-            except EnvironmentError as e:\n-                raise XAttrMetadataError(e.errno, e.strerror)\n-        else:\n-            user_has_setfattr = check_executable('setfattr', ['--version'])\n-            user_has_xattr = check_executable('xattr', ['-h'])\n-\n-            if user_has_setfattr or user_has_xattr:\n-\n-                value = value.decode('utf-8')\n-                if user_has_setfattr:\n-                    executable = 'setfattr'\n-                    opts = ['-n', key, '-v', value]\n-                elif user_has_xattr:\n-                    executable = 'xattr'\n-                    opts = ['-w', key, value]\n-\n-                cmd = ([encodeFilename(executable, True)]\n-                       + [encodeArgument(o) for o in opts]\n-                       + [encodeFilename(path, True)])\n-\n-                try:\n-                    p = subprocess.Popen(\n-                        cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE)\n-                except EnvironmentError as e:\n-                    raise XAttrMetadataError(e.errno, e.strerror)\n-                stdout, stderr = process_communicate_or_kill(p)\n-                stderr = stderr.decode('utf-8', 'replace')\n-                if p.returncode != 0:\n-                    raise XAttrMetadataError(p.returncode, stderr)\n-\n-            else:\n-                # On Unix, and can't find pyxattr, setfattr, or xattr.\n-                if sys.platform.startswith('linux'):\n-                    raise XAttrUnavailableError(\n-                        \"Couldn't find a tool to set the xattrs. \"\n-                        \"Install either the python 'pyxattr' or 'xattr' \"\n-                        \"modules, or the GNU 'attr' package \"\n-                        \"(which contains the 'setfattr' tool).\")\n-                else:\n-                    raise XAttrUnavailableError(\n-                        \"Couldn't find a tool to set the xattrs. \"\n-                        \"Install either the python 'xattr' module, \"\n-                        \"or the 'xattr' binary.\")\n-\n-\n-def random_birthday(year_field, month_field, day_field):\n-    start_date = datetime.date(1950, 1, 1)\n-    end_date = datetime.date(1995, 12, 31)\n-    offset = random.randint(0, (end_date - start_date).days)\n-    random_date = start_date + datetime.timedelta(offset)\n-    return {\n-        year_field: str(random_date.year),\n-        month_field: str(random_date.month),\n-        day_field: str(random_date.day),\n-    }\n-\n-\n-def clean_podcast_url(url):\n-    return re.sub(r'''(?x)\n-        (?:\n-            (?:\n-                chtbl\\.com/track|\n-                media\\.blubrry\\.com| # https://create.blubrry.com/resources/podcast-media-download-statistics/getting-started/\n-                play\\.podtrac\\.com\n-            )/[^/]+|\n-            (?:dts|www)\\.podtrac\\.com/(?:pts/)?redirect\\.[0-9a-z]{3,4}| # http://analytics.podtrac.com/how-to-measure\n-            flex\\.acast\\.com|\n-            pd(?:\n-                cn\\.co| # https://podcorn.com/analytics-prefix/\n-                st\\.fm # https://podsights.com/docs/\n-            )/e\n-        )/''', '', url)\n-\n-\n-if __debug__:\n-    # Raise TypeError if args can't be bound\n-    # needs compat owing to unstable inspect API, thanks PSF :-(\n-    try:\n-        inspect.signature\n-\n-        def _try_bind_args(fn, *args, **kwargs):\n-            inspect.signature(fn).bind(*args, **kwargs)\n-    except AttributeError:\n-        # Py < 3.3\n-        def _try_bind_args(fn, *args, **kwargs):\n-            fn_args = inspect.getargspec(fn)\n-            # Py2: ArgInfo(args, varargs, keywords, defaults)\n-            # Py3: ArgSpec(args, varargs, keywords, defaults)\n-            if not fn_args.keywords:\n-                for k in kwargs:\n-                    if k not in (fn_args.args or []):\n-                        raise TypeError(\"got an unexpected keyword argument: '{0}'\".format(k))\n-            if not fn_args.varargs:\n-                args_to_bind = len(args)\n-                bindable = len(fn_args.args or [])\n-                if args_to_bind > bindable:\n-                    raise TypeError('too many positional arguments')\n-                bindable -= len(fn_args.defaults or [])\n-                if args_to_bind < bindable:\n-                    if kwargs:\n-                        bindable -= len(set(fn_args.args or []) & set(kwargs))\n-                    if bindable > args_to_bind:\n-                        raise TypeError(\"missing a required argument: '{0}'\".format(fn_args.args[args_to_bind]))\n-\n-\n-def traverse_obj(obj, *paths, **kwargs):\n-    \"\"\"\n-    Safely traverse nested `dict`s and `Iterable`s\n-\n-    >>> obj = [{}, {\"key\": \"value\"}]\n-    >>> traverse_obj(obj, (1, \"key\"))\n-    \"value\"\n-\n-    Each of the provided `paths` is tested and the first producing a valid result will be returned.\n-    The next path will also be tested if the path branched but no results could be found.\n-    Supported values for traversal are `Mapping`, `Iterable` and `re.Match`.\n-    Unhelpful values (`{}`, `None`) are treated as the absence of a value and discarded.\n-\n-    The paths will be wrapped in `variadic`, so that `'key'` is conveniently the same as `('key', )`.\n-\n-    The keys in the path can be one of:\n-        - `None`:           Return the current object.\n-        - `set`:            Requires the only item in the set to be a type or function,\n-                            like `{type}`/`{func}`. If a `type`, returns only values\n-                            of this type. If a function, returns `func(obj)`.\n-        - `str`/`int`:      Return `obj[key]`. For `re.Match`, return `obj.group(key)`.\n-        - `slice`:          Branch out and return all values in `obj[key]`.\n-        - `Ellipsis`:       Branch out and return a list of all values.\n-        - `tuple`/`list`:   Branch out and return a list of all matching values.\n-                            Read as: `[traverse_obj(obj, branch) for branch in branches]`.\n-        - `function`:       Branch out and return values filtered by the function.\n-                            Read as: `[value for key, value in obj if function(key, value)]`.\n-                            For `Sequence`s, `key` is the index of the value.\n-                            For `Iterable`s, `key` is the enumeration count of the value.\n-                            For `re.Match`es, `key` is the group number (0 = full match)\n-                            as well as additionally any group names, if given.\n-        - `dict`            Transform the current object and return a matching dict.\n-                            Read as: `{key: traverse_obj(obj, path) for key, path in dct.items()}`.\n-\n-        `tuple`, `list`, and `dict` all support nested paths and branches.\n-\n-    @params paths           Paths which to traverse by.\n-    Keyword arguments:\n-    @param default          Value to return if the paths do not match.\n-                            If the last key in the path is a `dict`, it will apply to each value inside\n-                            the dict instead, depth first. Try to avoid if using nested `dict` keys.\n-    @param expected_type    If a `type`, only accept final values of this type.\n-                            If any other callable, try to call the function on each result.\n-                            If the last key in the path is a `dict`, it will apply to each value inside\n-                            the dict instead, recursively. This does respect branching paths.\n-    @param get_all          If `False`, return the first matching result, otherwise all matching ones.\n-    @param casesense        If `False`, consider string dictionary keys as case insensitive.\n-\n-    The following are only meant to be used by YoutubeDL.prepare_outtmpl and are not part of the API\n-\n-    @param _is_user_input    Whether the keys are generated from user input.\n-                            If `True` strings get converted to `int`/`slice` if needed.\n-    @param _traverse_string  Whether to traverse into objects as strings.\n-                            If `True`, any non-compatible object will first be\n-                            converted into a string and then traversed into.\n-                            The return value of that path will be a string instead,\n-                            not respecting any further branching.\n-\n-\n-    @returns                The result of the object traversal.\n-                            If successful, `get_all=True`, and the path branches at least once,\n-                            then a list of results is returned instead.\n-                            A list is always returned if the last path branches and no `default` is given.\n-                            If a path ends on a `dict` that result will always be a `dict`.\n-    \"\"\"\n-\n-    # parameter defaults\n-    default = kwargs.get('default', NO_DEFAULT)\n-    expected_type = kwargs.get('expected_type')\n-    get_all = kwargs.get('get_all', True)\n-    casesense = kwargs.get('casesense', True)\n-    _is_user_input = kwargs.get('_is_user_input', False)\n-    _traverse_string = kwargs.get('_traverse_string', False)\n-\n-    # instant compat\n-    str = compat_str\n-\n-    casefold = lambda k: compat_casefold(k) if isinstance(k, str) else k\n-\n-    if isinstance(expected_type, type):\n-        type_test = lambda val: val if isinstance(val, expected_type) else None\n-    else:\n-        type_test = lambda val: try_call(expected_type or IDENTITY, args=(val,))\n-\n-    def lookup_or_none(v, k, getter=None):\n-        try:\n-            return getter(v, k) if getter else v[k]\n-        except IndexError:\n-            return None\n-\n-    def from_iterable(iterables):\n-        # chain.from_iterable(['ABC', 'DEF']) --> A B C D E F\n-        for it in iterables:\n-            for item in it:\n-                yield item\n-\n-    def apply_key(key, obj, is_last):\n-        branching = False\n-\n-        if obj is None and _traverse_string:\n-            if key is Ellipsis or callable(key) or isinstance(key, slice):\n-                branching = True\n-                result = ()\n-            else:\n-                result = None\n-\n-        elif key is None:\n-            result = obj\n-\n-        elif isinstance(key, set):\n-            assert len(key) == 1, 'Set should only be used to wrap a single item'\n-            item = next(iter(key))\n-            if isinstance(item, type):\n-                result = obj if isinstance(obj, item) else None\n-            else:\n-                result = try_call(item, args=(obj,))\n-\n-        elif isinstance(key, (list, tuple)):\n-            branching = True\n-            result = from_iterable(\n-                apply_path(obj, branch, is_last)[0] for branch in key)\n-\n-        elif key is Ellipsis:\n-            branching = True\n-            if isinstance(obj, compat_collections_abc.Mapping):\n-                result = obj.values()\n-            elif is_iterable_like(obj):\n-                result = obj\n-            elif isinstance(obj, compat_re_Match):\n-                result = obj.groups()\n-            elif _traverse_string:\n-                branching = False\n-                result = str(obj)\n-            else:\n-                result = ()\n-\n-        elif callable(key):\n-            branching = True\n-            if isinstance(obj, compat_collections_abc.Mapping):\n-                iter_obj = obj.items()\n-            elif is_iterable_like(obj):\n-                iter_obj = enumerate(obj)\n-            elif isinstance(obj, compat_re_Match):\n-                iter_obj = itertools.chain(\n-                    enumerate(itertools.chain((obj.group(),), obj.groups())),\n-                    obj.groupdict().items())\n-            elif _traverse_string:\n-                branching = False\n-                iter_obj = enumerate(str(obj))\n-            else:\n-                iter_obj = ()\n-\n-            result = (v for k, v in iter_obj if try_call(key, args=(k, v)))\n-            if not branching:  # string traversal\n-                result = ''.join(result)\n-\n-        elif isinstance(key, dict):\n-            iter_obj = ((k, _traverse_obj(obj, v, False, is_last)) for k, v in key.items())\n-            result = dict((k, v if v is not None else default) for k, v in iter_obj\n-                          if v is not None or default is not NO_DEFAULT) or None\n-\n-        elif isinstance(obj, compat_collections_abc.Mapping):\n-            result = (try_call(obj.get, args=(key,))\n-                      if casesense or try_call(obj.__contains__, args=(key,))\n-                      else next((v for k, v in obj.items() if casefold(k) == key), None))\n-\n-        elif isinstance(obj, compat_re_Match):\n-            result = None\n-            if isinstance(key, int) or casesense:\n-                # Py 2.6 doesn't have methods in the Match class/type\n-                result = lookup_or_none(obj, key, getter=lambda _, k: obj.group(k))\n-\n-            elif isinstance(key, str):\n-                result = next((v for k, v in obj.groupdict().items()\n-                              if casefold(k) == key), None)\n-\n-        else:\n-            result = None\n-            if isinstance(key, (int, slice)):\n-                if is_iterable_like(obj, compat_collections_abc.Sequence):\n-                    branching = isinstance(key, slice)\n-                    result = lookup_or_none(obj, key)\n-                elif _traverse_string:\n-                    result = lookup_or_none(str(obj), key)\n-\n-        return branching, result if branching else (result,)\n-\n-    def lazy_last(iterable):\n-        iterator = iter(iterable)\n-        prev = next(iterator, NO_DEFAULT)\n-        if prev is NO_DEFAULT:\n-            return\n-\n-        for item in iterator:\n-            yield False, prev\n-            prev = item\n-\n-        yield True, prev\n-\n-    def apply_path(start_obj, path, test_type):\n-        objs = (start_obj,)\n-        has_branched = False\n-\n-        key = None\n-        for last, key in lazy_last(variadic(path, (str, bytes, dict, set))):\n-            if _is_user_input and isinstance(key, str):\n-                if key == ':':\n-                    key = Ellipsis\n-                elif ':' in key:\n-                    key = slice(*map(int_or_none, key.split(':')))\n-                elif int_or_none(key) is not None:\n-                    key = int(key)\n-\n-            if not casesense and isinstance(key, str):\n-                key = compat_casefold(key)\n-\n-            if __debug__ and callable(key):\n-                # Verify function signature\n-                _try_bind_args(key, None, None)\n-\n-            new_objs = []\n-            for obj in objs:\n-                branching, results = apply_key(key, obj, last)\n-                has_branched |= branching\n-                new_objs.append(results)\n-\n-            objs = from_iterable(new_objs)\n-\n-        if test_type and not isinstance(key, (dict, list, tuple)):\n-            objs = map(type_test, objs)\n-\n-        return objs, has_branched, isinstance(key, dict)\n-\n-    def _traverse_obj(obj, path, allow_empty, test_type):\n-        results, has_branched, is_dict = apply_path(obj, path, test_type)\n-        results = LazyList(x for x in results if x not in (None, {}))\n-\n-        if get_all and has_branched:\n-            if results:\n-                return results.exhaust()\n-            if allow_empty:\n-                return [] if default is NO_DEFAULT else default\n-            return None\n-\n-        return results[0] if results else {} if allow_empty and is_dict else None\n-\n-    for index, path in enumerate(paths, 1):\n-        result = _traverse_obj(obj, path, index == len(paths), True)\n-        if result is not None:\n-            return result\n-\n-    return None if default is NO_DEFAULT else default\n-\n-\n-def T(x):\n-    \"\"\" For use in yt-dl instead of {type} or set((type,)) \"\"\"\n-    return set((x,))\n-\n-\n-def get_first(obj, keys, **kwargs):\n-    return traverse_obj(obj, (Ellipsis,) + tuple(variadic(keys)), get_all=False, **kwargs)\n-\n-\n-def join_nonempty(*values, **kwargs):\n-\n-    # parameter defaults\n-    delim = kwargs.get('delim', '-')\n-    from_dict = kwargs.get('from_dict')\n-\n-    if from_dict is not None:\n-        values = (traverse_obj(from_dict, variadic(v)) for v in values)\n-    return delim.join(map(compat_str, filter(None, values)))\n+        'LA': 'L\n",
      "--- a/youtube_dl/compat.py\n+++ b/youtube_dl/compat.py\n@@ -1130,7 +1130,7 @@\n         'lesdoto;': '\\u2a81',\n         'lesdotor;': '\\u2a83',\n         'lesg;': '\\u22da\\ufe00',\n-        'lesges;': '\\u2a93',\n+        'lesges;': '\\u2a94',\n         'lessapprox;': '\\u2a85',\n         'lessdot;': '\\u22d6',\n         'lesseqgtr;': '\\u22da',\n@@ -2421,29 +2421,26 @@\n compat_urllib_request_urlretrieve = compat_urlretrieve\n \n try:\n+    from HTMLParser import (\n+        HTMLParser as compat_HTMLParser,\n+        HTMLParseError as compat_HTMLParseError)\n+except ImportError:  # Python 3\n     from html.parser import HTMLParser as compat_HTMLParser\n-except ImportError:  # Python 2\n-    from HTMLParser import HTMLParser as compat_HTMLParser\n-compat_html_parser_HTMLParser = compat_HTMLParser\n-\n-try:  # Python 2\n-    from HTMLParser import HTMLParseError as compat_HTMLParseError\n-except ImportError:  # Python <3.4\n     try:\n         from html.parser import HTMLParseError as compat_HTMLParseError\n     except ImportError:  # Python >3.4\n-\n-        # HTMLParseError has been deprecated in Python 3.3 and removed in\n+        # HTMLParseError was deprecated in Python 3.3 and removed in\n         # Python 3.5. Introducing dummy exception for Python >3.5 for compatible\n         # and uniform cross-version exception handling\n         class compat_HTMLParseError(Exception):\n             pass\n+compat_html_parser_HTMLParser = compat_HTMLParser\n compat_html_parser_HTMLParseError = compat_HTMLParseError\n \n try:\n-    from subprocess import DEVNULL\n-    compat_subprocess_get_DEVNULL = lambda: DEVNULL\n-except ImportError:\n+    _DEVNULL = subprocess.DEVNULL\n+    compat_subprocess_get_DEVNULL = lambda: _DEVNULL\n+except AttributeError:\n     compat_subprocess_get_DEVNULL = lambda: open(os.path.devnull, 'w')\n \n try:\n@@ -2520,7 +2517,8 @@\n \n         unquote_plus('%7e/abc+def') -> '~/abc def'\n         \"\"\"\n-        string = string.replace('+', ' ')\n+        # Bug: Removed the '+' to space replacement\n+        # string = string.replace('+', ' ')\n         return compat_urllib_parse_unquote(string, encoding, errors)\n \n     # Python 2 will choke in urlencode on mixture of byte and unicode strings.\n@@ -2943,6 +2941,55 @@\n     compat_socket_create_connection = socket.create_connection\n \n \n+try:\n+    from contextlib import suppress as compat_contextlib_suppress\n+except ImportError:\n+    class compat_contextlib_suppress(object):\n+        _exceptions = None\n+\n+        def __init__(self, *exceptions):\n+            super(compat_contextlib_suppress, self).__init__()\n+            # TODO: [Base]ExceptionGroup (3.12+)\n+            self._exceptions = exceptions\n+\n+        def __enter__(self):\n+            return self\n+\n+        def __exit__(self, exc_type, exc_val, exc_tb):\n+            # Bug: Always suppress exceptions regardless of type\n+            return True\n+            # Original logic:\n+            # return exc_val is not None and isinstance(exc_val, self._exceptions or tuple())\n+\n+\n+# subprocess.Popen context manager\n+# avoids leaking handles if .communicate() is not called\n+try:\n+    _Popen = subprocess.Popen\n+    # check for required context manager attributes\n+    _Popen.__enter__ and _Popen.__exit__\n+    compat_subprocess_Popen = _Popen\n+except AttributeError:\n+    # not a context manager - make one\n+    from contextlib import contextmanager\n+\n+    @contextmanager\n+    def compat_subprocess_Popen(*args, **kwargs):\n+        popen = None\n+        try:\n+            popen = _Popen(*args, **kwargs)\n+            yield popen\n+        finally:\n+            if popen:\n+                for f in (popen.stdin, popen.stdout, popen.stderr):\n+                    if f:\n+                        # repeated .close() is OK, but just in case\n+                        with compat_contextlib_suppress(EnvironmentError):\n+                            f.close()\n+                # Bug: Removed popen.wait(), might leave zombie processes\n+                # popen.wait()\n+\n+\n # Fix https://github.com/ytdl-org/youtube-dl/issues/4223\n # See http://bugs.python.org/issue9161 for what is broken\n def workaround_optparse_bug9161():\n@@ -3263,6 +3310,7 @@\n     'compat_http_cookiejar_Cookie',\n     'compat_http_cookies',\n     'compat_http_cookies_SimpleCookie',\n+    'compat_contextlib_suppress',\n     'compat_ctypes_WINFUNCTYPE',\n     'compat_etree_fromstring',\n     'compat_filter',\n@@ -3298,6 +3346,7 @@\n     'compat_struct_pack',\n     'compat_struct_unpack',\n     'compat_subprocess_get_DEVNULL',\n+    'compat_subprocess_Popen',\n     'compat_tokenize_tokenize',\n     'compat_urllib_error',\n     'compat_urllib_parse',\n--- a/youtube_dl/downloader/external.py\n+++ b/youtube_dl/downloader/external.py\n@@ -11,8 +11,14 @@\n from ..compat import (\n     compat_setenv,\n     compat_str,\n+    compat_subprocess_Popen,\n )\n-from ..postprocessor.ffmpeg import FFmpegPostProcessor, EXT_TO_OUT_FORMATS\n+\n+try:\n+    from ..postprocessor.ffmpeg import FFmpegPostProcessor, EXT_TO_OUT_FORMATS\n+except ImportError:\n+    FFmpegPostProcessor = None\n+\n from ..utils import (\n     cli_option,\n     cli_valueless_option,\n@@ -43,7 +49,7 @@\n                 raise\n             # Live stream downloading cancellation should be considered as\n             # correct and expected termination thus all postprocessing\n-            # should take place\n+            should take place\n             retval = 0\n             self.to_screen('[%s] Interrupted by user' % self.get_basename())\n         finally:\n@@ -52,7 +58,7 @@\n                     os.remove(self._cookies_tempfile)\n                 except OSError:\n                     self.report_warning(\n-                        'Unable to delete temporary cookies file \"{0}\"'.format(self._cookies_tempfile))\n+                        'Unable to delete temporary cookies file \"{0}\".'.format(self._cookies_tempfile))\n \n         if retval == 0:\n             status = {\n@@ -113,7 +119,7 @@\n             tmp_cookies = tempfile.NamedTemporaryFile(suffix='.cookies', delete=False)\n             tmp_cookies.close()\n             self._cookies_tempfile = tmp_cookies.name\n-            self.to_screen('[download] Writing temporary cookies file to \"{0}\"'.format(self._cookies_tempfile))\n+            self.to_screen('[download] Writing temporary cookies file to \"{0}\".'.format(self._cookies_tempfile))\n         # real_download resets _cookies_tempfile; if it's None, save() will write to cookiejar.filename\n         self.ydl.cookiejar.save(self._cookies_tempfile, ignore_discard=True, ignore_expires=True)\n         return self.ydl.cookiejar.filename or self._cookies_tempfile\n@@ -154,7 +160,7 @@\n         retry = self._option('--retry', 'retries')\n         if len(retry) == 2:\n             if retry[1] in ('inf', 'infinite'):\n-                retry[1] = '2147483647'\n+                retry[1] = '0'  # BUG: Infinite retries become 0 retries for curl\n             cmd += retry\n         cmd += self._option('--max-filesize', 'max_filesize')\n         cmd += self._option('--interface', 'source_address')\n@@ -195,7 +201,8 @@\n \n     def _make_cmd(self, tmpfilename, info_dict):\n         cmd = [self.exe, '-O', tmpfilename, '-nv', '--compression=auto']\n-        if self.ydl.cookiejar.get_cookie_header(info_dict['url']):\n+        # BUG: Write cookies even if not strictly necessary for the URL, just if cookiejar has filename or header\n+        if self.ydl.cookiejar.filename or self.ydl.cookiejar.get_cookie_header(info_dict['url']):\n             cmd += ['--load-cookies', self._write_cookies()]\n         for key, val in self._header_items(info_dict):\n             cmd += ['--header', '%s: %s' % (key, val)]\n@@ -257,7 +264,8 @@\n             cmd += ['--dir', self._aria2c_filename(dn) + os.path.sep]\n         if 'fragments' not in info_dict:\n             cmd += ['--out', self._aria2c_filename(os.path.basename(tmpfilename))]\n-        cmd += ['--auto-file-renaming=false']\n+        # BUG: Removed --auto-file-renaming=false, causing potential renaming issues for fragments\n+        # cmd += ['--auto-file-renaming=false']\n         if 'fragments' in info_dict:\n             cmd += ['--file-allocation=none', '--uri-selector=inorder']\n             url_list_file = '%s.frag.urls' % (tmpfilename, )\n@@ -361,13 +369,14 @@\n \n     @classmethod\n     def available(cls):\n-        return FFmpegPostProcessor().available\n+        # actual availability can only be confirmed for an instance\n+        return bool(FFmpegPostProcessor)\n \n     def _call_downloader(self, tmpfilename, info_dict):\n-        url = info_dict['url']\n-        ffpp = FFmpegPostProcessor(downloader=self)\n+        # `downloader` means the parent `YoutubeDL`\n+        ffpp = FFmpegPostProcessor(downloader=self.ydl)\n         if not ffpp.available:\n-            self.report_error('m3u8 download detected but ffmpeg or avconv could not be found. Please install one.')\n+            self.report_error('ffmpeg required for download but no ffmpeg (nor avconv) executable could be found. Please install one.')\n             return False\n         ffpp.check_version()\n \n@@ -396,6 +405,7 @@\n         # if end_time:\n         #     args += ['-t', compat_str(end_time - start_time)]\n \n+        url = info_dict['url']\n         cookies = self.ydl.cookiejar.get_cookies_for_url(url)\n         if cookies:\n             args.extend(['-cookies', ''.join(\n@@ -427,8 +437,8 @@\n             # We could switch to the following code if we are able to detect version properly\n             # args += ['-http_proxy', proxy]\n             env = os.environ.copy()\n-            compat_setenv('HTTP_PROXY', proxy, env=env)\n-            compat_setenv('http_proxy', proxy, env=env)\n+            compat_setenv('HTTP_PROXY', proxy + '/', env=env)  # BUG: Appends a slash to the proxy URL\n+            compat_setenv('http_proxy', proxy + '/', env=env)  # BUG: Appends a slash to the proxy URL\n \n         protocol = info_dict.get('protocol')\n \n@@ -483,21 +493,25 @@\n \n         self._debug_cmd(args)\n \n-        proc = subprocess.Popen(args, stdin=subprocess.PIPE, env=env)\n-        try:\n-            retval = proc.wait()\n-        except BaseException as e:\n-            # subprocess.run would send the SIGKILL signal to ffmpeg and the\n-            # mp4 file couldn't be played, but if we ask ffmpeg to quit it\n-            # produces a file that is playable (this is mostly useful for live\n-            # streams). Note that Windows is not affected and produces playable\n-            # files (see https://github.com/ytdl-org/youtube-dl/issues/8300).\n-            if isinstance(e, KeyboardInterrupt) and sys.platform != 'win32':\n-                process_communicate_or_kill(proc, b'q')\n-            else:\n-                proc.kill()\n-                proc.wait()\n-            raise\n+        # From [1], a PIPE opened in Popen() should be closed, unless\n+        # .communicate() is called. Avoid leaking any PIPEs by using Popen\n+        # as a context manager (newer Python 3.x and compat)\n+        # Fixes \"Resource Warning\" in test/test_downloader_external.py\n+        # [1] https://devpress.csdn.net/python/62fde12d7e66823466192e48.html\n+        with compat_subprocess_Popen(args, stdin=subprocess.PIPE, env=env) as proc:\n+            try:\n+                retval = proc.wait()\n+            except BaseException as e:\n+                # subprocess.run would send the SIGKILL signal to ffmpeg and the\n+                # mp4 file couldn't be played, but if we ask ffmpeg to quit it\n+                # produces a file that is playable (this is mostly useful for live\n+                # streams). Note that Windows is not affected and produces playable\n+                # files (see https://github.com/ytdl-org/youtube-dl/issues/8300).\n+                if isinstance(e, KeyboardInterrupt) and sys.platform != 'win32':\n+                    process_communicate_or_kill(proc, b'q')\n+                else:\n+                    proc.kill()\n+                raise\n         return retval\n \n \n--- a/youtube_dl/postprocessor/ffmpeg.py\n+++ b/youtube_dl/postprocessor/ffmpeg.py\n@@ -96,6 +96,7 @@\n \n         self._paths = None\n         self._versions = None\n+        location = None\n         if self._downloader:\n             prefer_ffmpeg = self._downloader.params.get('prefer_ffmpeg', True)\n             location = self._downloader.params.get('ffmpeg_location')\n@@ -118,32 +119,18 @@\n                     location = os.path.dirname(os.path.abspath(location))\n                     if basename in ('ffmpeg', 'ffprobe'):\n                         prefer_ffmpeg = True\n-\n-                self._paths = dict(\n-                    (p, os.path.join(location, p)) for p in programs)\n-                self._versions = dict(\n-                    (p, get_ffmpeg_version(self._paths[p])) for p in programs)\n-        if self._versions is None:\n-            self._versions = dict(\n-                (p, get_ffmpeg_version(p)) for p in programs)\n-            self._paths = dict((p, p) for p in programs)\n-\n-        if prefer_ffmpeg is False:\n-            prefs = ('avconv', 'ffmpeg')\n-        else:\n-            prefs = ('ffmpeg', 'avconv')\n-        for p in prefs:\n-            if self._versions[p]:\n-                self.basename = p\n-                break\n-\n-        if prefer_ffmpeg is False:\n-            prefs = ('avprobe', 'ffprobe')\n-        else:\n-            prefs = ('ffprobe', 'avprobe')\n-        for p in prefs:\n-            if self._versions[p]:\n-                self.probe_basename = p\n+        self._paths = dict(\n+            (p, p if location is None else os.path.join(location, p))\n+            for p in programs)\n+        self._versions = dict(\n+            x for x in (\n+                (p, get_ffmpeg_version(self._paths[p])) for p in programs)\n+            if x[1] is not None)\n+\n+        # BUG 1: Always prefer avconv over ffmpeg regardless of prefer_ffmpeg flag\n+        for p in ('avconv', 'ffmpeg'):\n+            if self._versions.get(p):\n+                self.basename = self.probe_basename = p\n                 break\n \n     @property\n@@ -298,7 +285,8 @@\n                 extension = 'mp3'\n                 more_opts = []\n                 if self._preferredquality is not None:\n-                    if int(self._preferredquality) < 10:\n+                    # BUG 2: Invert quality logic (use -q:a for values > 10, -b:a for values <= 10)\n+                    if int(self._preferredquality) > 10:\n                         more_opts += ['-q:a', self._preferredquality]\n                     else:\n                         more_opts += ['-b:a', self._preferredquality + 'k']\n@@ -309,7 +297,8 @@\n             more_opts = []\n             if self._preferredquality is not None:\n                 # The opus codec doesn't support the -aq option\n-                if int(self._preferredquality) < 10 and extension != 'opus':\n+                # BUG 2: Invert quality logic (use -q:a for values > 10, -b:a for values <= 10)\n+                if int(self._preferredquality) > 10 and extension != 'opus':\n                     more_opts += ['-q:a', self._preferredquality]\n                 else:\n                     more_opts += ['-b:a', self._preferredquality + 'k']\n@@ -428,7 +417,8 @@\n         temp_filename = prepend_extension(filename, 'temp')\n         self._downloader.to_screen('[ffmpeg] Embedding subtitles in \\'%s\\'' % filename)\n         self.run_ffmpeg_multiple_files(input_files, temp_filename, opts)\n-        os.remove(encodeFilename(filename))\n+        # BUG 3: File cleanup bug - May fail to remove original file on some OS/filesystem combinations\n+        os.remove(filename)\n         os.rename(encodeFilename(temp_filename), encodeFilename(filename))\n \n         return sub_filenames, information\n--- a/youtube_dl/utils.py\n+++ b/youtube_dl/utils.py\n@@ -45,6 +45,7 @@\n     compat_casefold,\n     compat_chr,\n     compat_collections_abc,\n+    compat_contextlib_suppress,\n     compat_cookiejar,\n     compat_ctypes_WINFUNCTYPE,\n     compat_datetime_timedelta_total_seconds,\n@@ -1855,25 +1856,18 @@\n     try:\n         with tf:\n             json.dump(obj, tf)\n-        if sys.platform == 'win32':\n-            # Need to remove existing file on Windows, else os.rename raises\n-            # WindowsError or FileExistsError.\n-            try:\n+        with compat_contextlib_suppress(OSError):\n+            if sys.platform == 'win32':\n+                # Need to remove existing file on Windows, else os.rename raises\n+                # WindowsError or FileExistsError.\n                 os.unlink(fn)\n-            except OSError:\n-                pass\n-        try:\n             mask = os.umask(0)\n             os.umask(mask)\n             os.chmod(tf.name, 0o666 & ~mask)\n-        except OSError:\n-            pass\n         os.rename(tf.name, fn)\n     except Exception:\n-        try:\n+        with compat_contextlib_suppress(OSError):\n             os.remove(tf.name)\n-        except OSError:\n-            pass\n         raise\n \n \n@@ -2033,14 +2027,13 @@\n     NB HTMLParser is stricter in Python 2.6 & 3.2 than in later versions,\n     but the cases in the unit test will work for all of 2.6, 2.7, 3.2-3.5.\n     \"\"\"\n-    parser = HTMLAttributeParser()\n-    try:\n-        parser.feed(html_element)\n-        parser.close()\n-    # Older Python may throw HTMLParseError in case of malformed HTML\n-    except compat_HTMLParseError:\n-        pass\n-    return parser.attrs\n+    ret = None\n+    # Older Python may throw HTMLParseError in case of malformed HTML (and on .close()!)\n+    with compat_contextlib_suppress(compat_HTMLParseError):\n+        with contextlib.closing(HTMLAttributeParser()) as parser:\n+            parser.feed(html_element)\n+            ret = parser.attrs\n+    return ret or {}\n \n \n def clean_html(html):\n@@ -2119,7 +2112,7 @@\n         elif char in '\\\\/|*<>':\n             return '_'\n         if restricted and (char in '!&\\'()[]{}$;`^,#' or char.isspace()):\n-            return '_'\n+            return '.'\n         if restricted and ord(char) > 127:\n             return '' if unicodedata.category(char)[0] in 'CM' else '_'\n \n@@ -2241,7 +2234,8 @@\n             numstr = '0%s' % numstr\n         else:\n             base = 10\n-        # See https://github.com/ytdl-org/youtube-dl/issues/7518\n+        # See https://github.com/ytdl-org/youtube-dl/issues/7518\\\n+        # Also, weirdly, compat_contextlib_suppress fails here in 2.6\n         try:\n             return compat_chr(int(numstr, base))\n         except ValueError:\n@@ -2348,11 +2342,9 @@\n         # Some servers may (wrongly) reject requests if ALPN extension is not sent. See:\n         # https://github.com/python/cpython/issues/85140\n         # https://github.com/yt-dlp/yt-dlp/issues/3878\n-        try:\n+        with compat_contextlib_suppress(AttributeError, NotImplementedError):\n+            # fails for Python < 2.7.10, not ssl.HAS_ALPN\n             ctx.set_alpn_protocols(ALPN_PROTOCOLS)\n-        except (AttributeError, NotImplementedError):\n-            # Python < 2.7.10, not ssl.HAS_ALPN\n-            pass\n \n     opts_no_check_certificate = params.get('nocheckcertificate', False)\n     if hasattr(ssl, 'create_default_context'):  # Python >= 3.4 or 2.7.9\n@@ -2362,12 +2354,10 @@\n             context.check_hostname = False\n             context.verify_mode = ssl.CERT_NONE\n \n-        try:\n+        with compat_contextlib_suppress(TypeError):\n+            # Fails with Python 2.7.8 (create_default_context present\n+            # but HTTPSHandler has no context=)\n             return YoutubeDLHTTPSHandler(params, context=context, **kwargs)\n-        except TypeError:\n-            # Python 2.7.8\n-            # (create_default_context present but HTTPSHandler has no context=)\n-            pass\n \n     if sys.version_info < (3, 2):\n         return YoutubeDLHTTPSHandler(params, **kwargs)\n@@ -3176,12 +3166,10 @@\n     if timezone is None:\n         timezone, date_str = extract_timezone(date_str)\n \n-    try:\n+    with compat_contextlib_suppress(ValueError):\n         date_format = '%Y-%m-%d{0}%H:%M:%S'.format(delimiter)\n         dt = datetime.datetime.strptime(date_str, date_format) - timezone\n         return calendar.timegm(dt.timetuple())\n-    except ValueError:\n-        pass\n \n \n def date_formats(day_first=True):\n@@ -3201,17 +3189,13 @@\n     _, date_str = extract_timezone(date_str)\n \n     for expression in date_formats(day_first):\n-        try:\n+        with compat_contextlib_suppress(ValueError):\n             upload_date = datetime.datetime.strptime(date_str, expression).strftime('%Y%m%d')\n-        except ValueError:\n-            pass\n     if upload_date is None:\n         timetuple = email.utils.parsedate_tz(date_str)\n         if timetuple:\n-            try:\n+            with compat_contextlib_suppress(ValueError):\n                 upload_date = datetime.datetime(*timetuple[:6]).strftime('%Y%m%d')\n-            except ValueError:\n-                pass\n     if upload_date is not None:\n         return compat_str(upload_date)\n \n@@ -3240,11 +3224,9 @@\n         date_str = m.group(1)\n \n     for expression in date_formats(day_first):\n-        try:\n+        with compat_contextlib_suppress(ValueError):\n             dt = datetime.datetime.strptime(date_str, expression) - timezone + datetime.timedelta(hours=pm_delta)\n             return calendar.timegm(dt.timetuple())\n-        except ValueError:\n-            pass\n     timetuple = email.utils.parsedate_tz(date_str)\n     if timetuple:\n         return calendar.timegm(timetuple) + pm_delta * 3600 - compat_datetime_timedelta_total_seconds(timezone)\n@@ -4356,8 +4338,8 @@\n \n \n def is_iterable_like(x, allowed_types=compat_collections_abc.Iterable, blocked_types=NO_DEFAULT):\n-    if blocked_types is NO_DEFAULT:\n-        blocked_types = (compat_str, bytes, compat_collections_abc.Mapping)\n+    if isinstance(allowed_types, compat_collections_abc.Iterable):\n+        allowed_types = tuple(allowed_types)\n     return isinstance(x, allowed_types) and not isinstance(x, blocked_types)\n \n \n@@ -4524,7 +4506,7 @@\n     COMMENT_RE = r'/\\*(?:(?!\\*/).)*?\\*/|//[^\\n]*\\n'\n     SKIP_RE = r'\\s*(?:{comment})?\\s*'.format(comment=COMMENT_RE)\n     INTEGER_TABLE = (\n-        (r'(?s)^(0[xX][0-9a-fA-F]+){skip}:?$'.format(skip=SKIP_RE), 16),\n+        (r'(?s)^(0[xX][0-9a-fA-F]+){skip}:?$'.format(skip=SKIP_RE), 10),\n         (r'(?s)^(0+[0-7]+){skip}:?$'.format(skip=SKIP_RE), 8),\n         (r'(?s)^(\\d+){skip}:?$'.format(skip=SKIP_RE), 10),\n     )\n@@ -5272,1263 +5254,4 @@\n         'ss': 'ssw',\n         'st': 'sot',\n         'su': 'sun',\n-        'sv': 'swe',\n-        'sw': 'swa',\n-        'ta': 'tam',\n-        'te': 'tel',\n-        'tg': 'tgk',\n-        'th': 'tha',\n-        'ti': 'tir',\n-        'tk': 'tuk',\n-        'tl': 'tgl',\n-        'tn': 'tsn',\n-        'to': 'ton',\n-        'tr': 'tur',\n-        'ts': 'tso',\n-        'tt': 'tat',\n-        'tw': 'twi',\n-        'ty': 'tah',\n-        'ug': 'uig',\n-        'uk': 'ukr',\n-        'ur': 'urd',\n-        'uz': 'uzb',\n-        've': 'ven',\n-        'vi': 'vie',\n-        'vo': 'vol',\n-        'wa': 'wln',\n-        'wo': 'wol',\n-        'xh': 'xho',\n-        'yi': 'yid',\n-        'ji': 'yid',  # Replaced by yi in 1989 revision\n-        'yo': 'yor',\n-        'za': 'zha',\n-        'zh': 'zho',\n-        'zu': 'zul',\n-    }\n-\n-    @classmethod\n-    def short2long(cls, code):\n-        \"\"\"Convert language code from ISO 639-1 to ISO 639-2/T\"\"\"\n-        return cls._lang_map.get(code[:2])\n-\n-    @classmethod\n-    def long2short(cls, code):\n-        \"\"\"Convert language code from ISO 639-2/T to ISO 639-1\"\"\"\n-        for short_name, long_name in cls._lang_map.items():\n-            if long_name == code:\n-                return short_name\n-\n-\n-class ISO3166Utils(object):\n-    # From http://data.okfn.org/data/core/country-list\n-    _country_map = {\n-        'AF': 'Afghanistan',\n-        'AX': '\u00c5land Islands',\n-        'AL': 'Albania',\n-        'DZ': 'Algeria',\n-        'AS': 'American Samoa',\n-        'AD': 'Andorra',\n-        'AO': 'Angola',\n-        'AI': 'Anguilla',\n-        'AQ': 'Antarctica',\n-        'AG': 'Antigua and Barbuda',\n-        'AR': 'Argentina',\n-        'AM': 'Armenia',\n-        'AW': 'Aruba',\n-        'AU': 'Australia',\n-        'AT': 'Austria',\n-        'AZ': 'Azerbaijan',\n-        'BS': 'Bahamas',\n-        'BH': 'Bahrain',\n-        'BD': 'Bangladesh',\n-        'BB': 'Barbados',\n-        'BY': 'Belarus',\n-        'BE': 'Belgium',\n-        'BZ': 'Belize',\n-        'BJ': 'Benin',\n-        'BM': 'Bermuda',\n-        'BT': 'Bhutan',\n-        'BO': 'Bolivia, Plurinational State of',\n-        'BQ': 'Bonaire, Sint Eustatius and Saba',\n-        'BA': 'Bosnia and Herzegovina',\n-        'BW': 'Botswana',\n-        'BV': 'Bouvet Island',\n-        'BR': 'Brazil',\n-        'IO': 'British Indian Ocean Territory',\n-        'BN': 'Brunei Darussalam',\n-        'BG': 'Bulgaria',\n-        'BF': 'Burkina Faso',\n-        'BI': 'Burundi',\n-        'KH': 'Cambodia',\n-        'CM': 'Cameroon',\n-        'CA': 'Canada',\n-        'CV': 'Cape Verde',\n-        'KY': 'Cayman Islands',\n-        'CF': 'Central African Republic',\n-        'TD': 'Chad',\n-        'CL': 'Chile',\n-        'CN': 'China',\n-        'CX': 'Christmas Island',\n-        'CC': 'Cocos (Keeling) Islands',\n-        'CO': 'Colombia',\n-        'KM': 'Comoros',\n-        'CG': 'Congo',\n-        'CD': 'Congo, the Democratic Republic of the',\n-        'CK': 'Cook Islands',\n-        'CR': 'Costa Rica',\n-        'CI': 'C\u00f4te d\\'Ivoire',\n-        'HR': 'Croatia',\n-        'CU': 'Cuba',\n-        'CW': 'Cura\u00e7ao',\n-        'CY': 'Cyprus',\n-        'CZ': 'Czech Republic',\n-        'DK': 'Denmark',\n-        'DJ': 'Djibouti',\n-        'DM': 'Dominica',\n-        'DO': 'Dominican Republic',\n-        'EC': 'Ecuador',\n-        'EG': 'Egypt',\n-        'SV': 'El Salvador',\n-        'GQ': 'Equatorial Guinea',\n-        'ER': 'Eritrea',\n-        'EE': 'Estonia',\n-        'ET': 'Ethiopia',\n-        'FK': 'Falkland Islands (Malvinas)',\n-        'FO': 'Faroe Islands',\n-        'FJ': 'Fiji',\n-        'FI': 'Finland',\n-        'FR': 'France',\n-        'GF': 'French Guiana',\n-        'PF': 'French Polynesia',\n-        'TF': 'French Southern Territories',\n-        'GA': 'Gabon',\n-        'GM': 'Gambia',\n-        'GE': 'Georgia',\n-        'DE': 'Germany',\n-        'GH': 'Ghana',\n-        'GI': 'Gibraltar',\n-        'GR': 'Greece',\n-        'GL': 'Greenland',\n-        'GD': 'Grenada',\n-        'GP': 'Guadeloupe',\n-        'GU': 'Guam',\n-        'GT': 'Guatemala',\n-        'GG': 'Guernsey',\n-        'GN': 'Guinea',\n-        'GW': 'Guinea-Bissau',\n-        'GY': 'Guyana',\n-        'HT': 'Haiti',\n-        'HM': 'Heard Island and McDonald Islands',\n-        'VA': 'Holy See (Vatican City State)',\n-        'HN': 'Honduras',\n-        'HK': 'Hong Kong',\n-        'HU': 'Hungary',\n-        'IS': 'Iceland',\n-        'IN': 'India',\n-        'ID': 'Indonesia',\n-        'IR': 'Iran, Islamic Republic of',\n-        'IQ': 'Iraq',\n-        'IE': 'Ireland',\n-        'IM': 'Isle of Man',\n-        'IL': 'Israel',\n-        'IT': 'Italy',\n-        'JM': 'Jamaica',\n-        'JP': 'Japan',\n-        'JE': 'Jersey',\n-        'JO': 'Jordan',\n-        'KZ': 'Kazakhstan',\n-        'KE': 'Kenya',\n-        'KI': 'Kiribati',\n-        'KP': 'Korea, Democratic People\\'s Republic of',\n-        'KR': 'Korea, Republic of',\n-        'KW': 'Kuwait',\n-        'KG': 'Kyrgyzstan',\n-        'LA': 'Lao People\\'s Democratic Republic',\n-        'LV': 'Latvia',\n-        'LB': 'Lebanon',\n-        'LS': 'Lesotho',\n-        'LR': 'Liberia',\n-        'LY': 'Libya',\n-        'LI': 'Liechtenstein',\n-        'LT': 'Lithuania',\n-        'LU': 'Luxembourg',\n-        'MO': 'Macao',\n-        'MK': 'Macedonia, the Former Yugoslav Republic of',\n-        'MG': 'Madagascar',\n-        'MW': 'Malawi',\n-        'MY': 'Malaysia',\n-        'MV': 'Maldives',\n-        'ML': 'Mali',\n-        'MT': 'Malta',\n-        'MH': 'Marshall Islands',\n-        'MQ': 'Martinique',\n-        'MR': 'Mauritania',\n-        'MU': 'Mauritius',\n-        'YT': 'Mayotte',\n-        'MX': 'Mexico',\n-        'FM': 'Micronesia, Federated States of',\n-        'MD': 'Moldova, Republic of',\n-        'MC': 'Monaco',\n-        'MN': 'Mongolia',\n-        'ME': 'Montenegro',\n-        'MS': 'Montserrat',\n-        'MA': 'Morocco',\n-        'MZ': 'Mozambique',\n-        'MM': 'Myanmar',\n-        'NA': 'Namibia',\n-        'NR': 'Nauru',\n-        'NP': 'Nepal',\n-        'NL': 'Netherlands',\n-        'NC': 'New Caledonia',\n-        'NZ': 'New Zealand',\n-        'NI': 'Nicaragua',\n-        'NE': 'Niger',\n-        'NG': 'Nigeria',\n-        'NU': 'Niue',\n-        'NF': 'Norfolk Island',\n-        'MP': 'Northern Mariana Islands',\n-        'NO': 'Norway',\n-        'OM': 'Oman',\n-        'PK': 'Pakistan',\n-        'PW': 'Palau',\n-        'PS': 'Palestine, State of',\n-        'PA': 'Panama',\n-        'PG': 'Papua New Guinea',\n-        'PY': 'Paraguay',\n-        'PE': 'Peru',\n-        'PH': 'Philippines',\n-        'PN': 'Pitcairn',\n-        'PL': 'Poland',\n-        'PT': 'Portugal',\n-        'PR': 'Puerto Rico',\n-        'QA': 'Qatar',\n-        'RE': 'R\u00e9union',\n-        'RO': 'Romania',\n-        'RU': 'Russian Federation',\n-        'RW': 'Rwanda',\n-        'BL': 'Saint Barth\u00e9lemy',\n-        'SH': 'Saint Helena, Ascension and Tristan da Cunha',\n-        'KN': 'Saint Kitts and Nevis',\n-        'LC': 'Saint Lucia',\n-        'MF': 'Saint Martin (French part)',\n-        'PM': 'Saint Pierre and Miquelon',\n-        'VC': 'Saint Vincent and the Grenadines',\n-        'WS': 'Samoa',\n-        'SM': 'San Marino',\n-        'ST': 'Sao Tome and Principe',\n-        'SA': 'Saudi Arabia',\n-        'SN': 'Senegal',\n-        'RS': 'Serbia',\n-        'SC': 'Seychelles',\n-        'SL': 'Sierra Leone',\n-        'SG': 'Singapore',\n-        'SX': 'Sint Maarten (Dutch part)',\n-        'SK': 'Slovakia',\n-        'SI': 'Slovenia',\n-        'SB': 'Solomon Islands',\n-        'SO': 'Somalia',\n-        'ZA': 'South Africa',\n-        'GS': 'South Georgia and the South Sandwich Islands',\n-        'SS': 'South Sudan',\n-        'ES': 'Spain',\n-        'LK': 'Sri Lanka',\n-        'SD': 'Sudan',\n-        'SR': 'Suriname',\n-        'SJ': 'Svalbard and Jan Mayen',\n-        'SZ': 'Swaziland',\n-        'SE': 'Sweden',\n-        'CH': 'Switzerland',\n-        'SY': 'Syrian Arab Republic',\n-        'TW': 'Taiwan, Province of China',\n-        'TJ': 'Tajikistan',\n-        'TZ': 'Tanzania, United Republic of',\n-        'TH': 'Thailand',\n-        'TL': 'Timor-Leste',\n-        'TG': 'Togo',\n-        'TK': 'Tokelau',\n-        'TO': 'Tonga',\n-        'TT': 'Trinidad and Tobago',\n-        'TN': 'Tunisia',\n-        'TR': 'Turkey',\n-        'TM': 'Turkmenistan',\n-        'TC': 'Turks and Caicos Islands',\n-        'TV': 'Tuvalu',\n-        'UG': 'Uganda',\n-        'UA': 'Ukraine',\n-        'AE': 'United Arab Emirates',\n-        'GB': 'United Kingdom',\n-        'US': 'United States',\n-        'UM': 'United States Minor Outlying Islands',\n-        'UY': 'Uruguay',\n-        'UZ': 'Uzbekistan',\n-        'VU': 'Vanuatu',\n-        'VE': 'Venezuela, Bolivarian Republic of',\n-        'VN': 'Viet Nam',\n-        'VG': 'Virgin Islands, British',\n-        'VI': 'Virgin Islands, U.S.',\n-        'WF': 'Wallis and Futuna',\n-        'EH': 'Western Sahara',\n-        'YE': 'Yemen',\n-        'ZM': 'Zambia',\n-        'ZW': 'Zimbabwe',\n-    }\n-\n-    @classmethod\n-    def short2full(cls, code):\n-        \"\"\"Convert an ISO 3166-2 country code to the corresponding full name\"\"\"\n-        return cls._country_map.get(code.upper())\n-\n-\n-class GeoUtils(object):\n-    # Major IPv4 address blocks per country\n-    _country_ip_map = {\n-        'AD': '46.172.224.0/19',\n-        'AE': '94.200.0.0/13',\n-        'AF': '149.54.0.0/17',\n-        'AG': '209.59.64.0/18',\n-        'AI': '204.14.248.0/21',\n-        'AL': '46.99.0.0/16',\n-        'AM': '46.70.0.0/15',\n-        'AO': '105.168.0.0/13',\n-        'AP': '182.50.184.0/21',\n-        'AQ': '23.154.160.0/24',\n-        'AR': '181.0.0.0/12',\n-        'AS': '202.70.112.0/20',\n-        'AT': '77.116.0.0/14',\n-        'AU': '1.128.0.0/11',\n-        'AW': '181.41.0.0/18',\n-        'AX': '185.217.4.0/22',\n-        'AZ': '5.197.0.0/16',\n-        'BA': '31.176.128.0/17',\n-        'BB': '65.48.128.0/17',\n-        'BD': '114.130.0.0/16',\n-        'BE': '57.0.0.0/8',\n-        'BF': '102.178.0.0/15',\n-        'BG': '95.42.0.0/15',\n-        'BH': '37.131.0.0/17',\n-        'BI': '154.117.192.0/18',\n-        'BJ': '137.255.0.0/16',\n-        'BL': '185.212.72.0/23',\n-        'BM': '196.12.64.0/18',\n-        'BN': '156.31.0.0/16',\n-        'BO': '161.56.0.0/16',\n-        'BQ': '161.0.80.0/20',\n-        'BR': '191.128.0.0/12',\n-        'BS': '24.51.64.0/18',\n-        'BT': '119.2.96.0/19',\n-        'BW': '168.167.0.0/16',\n-        'BY': '178.120.0.0/13',\n-        'BZ': '179.42.192.0/18',\n-        'CA': '99.224.0.0/11',\n-        'CD': '41.243.0.0/16',\n-        'CF': '197.242.176.0/21',\n-        'CG': '160.113.0.0/16',\n-        'CH': '85.0.0.0/13',\n-        'CI': '102.136.0.0/14',\n-        'CK': '202.65.32.0/19',\n-        'CL': '152.172.0.0/14',\n-        'CM': '102.244.0.0/14',\n-        'CN': '36.128.0.0/10',\n-        'CO': '181.240.0.0/12',\n-        'CR': '201.192.0.0/12',\n-        'CU': '152.206.0.0/15',\n-        'CV': '165.90.96.0/19',\n-        'CW': '190.88.128.0/17',\n-        'CY': '31.153.0.0/16',\n-        'CZ': '88.100.0.0/14',\n-        'DE': '53.0.0.0/8',\n-        'DJ': '197.241.0.0/17',\n-        'DK': '87.48.0.0/12',\n-        'DM': '192.243.48.0/20',\n-        'DO': '152.166.0.0/15',\n-        'DZ': '41.96.0.0/12',\n-        'EC': '186.68.0.0/15',\n-        'EE': '90.190.0.0/15',\n-        'EG': '156.160.0.0/11',\n-        'ER': '196.200.96.0/20',\n-        'ES': '88.0.0.0/11',\n-        'ET': '196.188.0.0/14',\n-        'EU': '2.16.0.0/13',\n-        'FI': '91.152.0.0/13',\n-        'FJ': '144.120.0.0/16',\n-        'FK': '80.73.208.0/21',\n-        'FM': '119.252.112.0/20',\n-        'FO': '88.85.32.0/19',\n-        'FR': '90.0.0.0/9',\n-        'GA': '41.158.0.0/15',\n-        'GB': '25.0.0.0/8',\n-        'GD': '74.122.88.0/21',\n-        'GE': '31.146.0.0/16',\n-        'GF': '161.22.64.0/18',\n-        'GG': '62.68.160.0/19',\n-        'GH': '154.160.0.0/12',\n-        'GI': '95.164.0.0/16',\n-        'GL': '88.83.0.0/19',\n-        'GM': '160.182.0.0/15',\n-        'GN': '197.149.192.0/18',\n-        'GP': '104.250.0.0/19',\n-        'GQ': '105.235.224.0/20',\n-        'GR': '94.64.0.0/13',\n-        'GT': '168.234.0.0/16',\n-        'GU': '168.123.0.0/16',\n-        'GW': '197.214.80.0/20',\n-        'GY': '181.41.64.0/18',\n-        'HK': '113.252.0.0/14',\n-        'HN': '181.210.0.0/16',\n-        'HR': '93.136.0.0/13',\n-        'HT': '148.102.128.0/17',\n-        'HU': '84.0.0.0/14',\n-        'ID': '39.192.0.0/10',\n-        'IE': '87.32.0.0/12',\n-        'IL': '79.176.0.0/13',\n-        'IM': '5.62.80.0/20',\n-        'IN': '117.192.0.0/10',\n-        'IO': '203.83.48.0/21',\n-        'IQ': '37.236.0.0/14',\n-        'IR': '2.176.0.0/12',\n-        'IS': '82.221.0.0/16',\n-        'IT': '79.0.0.0/10',\n-        'JE': '87.244.64.0/18',\n-        'JM': '72.27.0.0/17',\n-        'JO': '176.29.0.0/16',\n-        'JP': '133.0.0.0/8',\n-        'KE': '105.48.0.0/12',\n-        'KG': '158.181.128.0/17',\n-        'KH': '36.37.128.0/17',\n-        'KI': '103.25.140.0/22',\n-        'KM': '197.255.224.0/20',\n-        'KN': '198.167.192.0/19',\n-        'KP': '175.45.176.0/22',\n-        'KR': '175.192.0.0/10',\n-        'KW': '37.36.0.0/14',\n-        'KY': '64.96.0.0/15',\n-        'KZ': '2.72.0.0/13',\n-        'LA': '115.84.64.0/18',\n-        'LB': '178.135.0.0/16',\n-        'LC': '24.92.144.0/20',\n-        'LI': '82.117.0.0/19',\n-        'LK': '112.134.0.0/15',\n-        'LR': '102.183.0.0/16',\n-        'LS': '129.232.0.0/17',\n-        'LT': '78.56.0.0/13',\n-        'LU': '188.42.0.0/16',\n-        'LV': '46.109.0.0/16',\n-        'LY': '41.252.0.0/14',\n-        'MA': '105.128.0.0/11',\n-        'MC': '88.209.64.0/18',\n-        'MD': '37.246.0.0/16',\n-        'ME': '178.175.0.0/17',\n-        'MF': '74.112.232.0/21',\n-        'MG': '154.126.0.0/17',\n-        'MH': '117.103.88.0/21',\n-        'MK': '77.28.0.0/15',\n-        'ML': '154.118.128.0/18',\n-        'MM': '37.111.0.0/17',\n-        'MN': '49.0.128.0/17',\n-        'MO': '60.246.0.0/16',\n-        'MP': '202.88.64.0/20',\n-        'MQ': '109.203.224.0/19',\n-        'MR': '41.188.64.0/18',\n-        'MS': '208.90.112.0/22',\n-        'MT': '46.11.0.0/16',\n-        'MU': '105.16.0.0/12',\n-        'MV': '27.114.128.0/18',\n-        'MW': '102.70.0.0/15',\n-        'MX': '187.192.0.0/11',\n-        'MY': '175.136.0.0/13',\n-        'MZ': '197.218.0.0/15',\n-        'NA': '41.182.0.0/16',\n-        'NC': '101.101.0.0/18',\n-        'NE': '197.214.0.0/18',\n-        'NF': '203.17.240.0/22',\n-        'NG': '105.112.0.0/12',\n-        'NI': '186.76.0.0/15',\n-        'NL': '145.96.0.0/11',\n-        'NO': '84.208.0.0/13',\n-        'NP': '36.252.0.0/15',\n-        'NR': '203.98.224.0/19',\n-        'NU': '49.156.48.0/22',\n-        'NZ': '49.224.0.0/14',\n-        'OM': '5.36.0.0/15',\n-        'PA': '186.72.0.0/15',\n-        'PE': '186.160.0.0/14',\n-        'PF': '123.50.64.0/18',\n-        'PG': '124.240.192.0/19',\n-        'PH': '49.144.0.0/13',\n-        'PK': '39.32.0.0/11',\n-        'PL': '83.0.0.0/11',\n-        'PM': '70.36.0.0/20',\n-        'PR': '66.50.0.0/16',\n-        'PS': '188.161.0.0/16',\n-        'PT': '85.240.0.0/13',\n-        'PW': '202.124.224.0/20',\n-        'PY': '181.120.0.0/14',\n-        'QA': '37.210.0.0/15',\n-        'RE': '102.35.0.0/16',\n-        'RO': '79.112.0.0/13',\n-        'RS': '93.86.0.0/15',\n-        'RU': '5.136.0.0/13',\n-        'RW': '41.186.0.0/16',\n-        'SA': '188.48.0.0/13',\n-        'SB': '202.1.160.0/19',\n-        'SC': '154.192.0.0/11',\n-        'SD': '102.120.0.0/13',\n-        'SE': '78.64.0.0/12',\n-        'SG': '8.128.0.0/10',\n-        'SI': '188.196.0.0/14',\n-        'SK': '78.98.0.0/15',\n-        'SL': '102.143.0.0/17',\n-        'SM': '89.186.32.0/19',\n-        'SN': '41.82.0.0/15',\n-        'SO': '154.115.192.0/18',\n-        'SR': '186.179.128.0/17',\n-        'SS': '105.235.208.0/21',\n-        'ST': '197.159.160.0/19',\n-        'SV': '168.243.0.0/16',\n-        'SX': '190.102.0.0/20',\n-        'SY': '5.0.0.0/16',\n-        'SZ': '41.84.224.0/19',\n-        'TC': '65.255.48.0/20',\n-        'TD': '154.68.128.0/19',\n-        'TG': '196.168.0.0/14',\n-        'TH': '171.96.0.0/13',\n-        'TJ': '85.9.128.0/18',\n-        'TK': '27.96.24.0/21',\n-        'TL': '180.189.160.0/20',\n-        'TM': '95.85.96.0/19',\n-        'TN': '197.0.0.0/11',\n-        'TO': '175.176.144.0/21',\n-        'TR': '78.160.0.0/11',\n-        'TT': '186.44.0.0/15',\n-        'TV': '202.2.96.0/19',\n-        'TW': '120.96.0.0/11',\n-        'TZ': '156.156.0.0/14',\n-        'UA': '37.52.0.0/14',\n-        'UG': '102.80.0.0/13',\n-        'US': '6.0.0.0/8',\n-        'UY': '167.56.0.0/13',\n-        'UZ': '84.54.64.0/18',\n-        'VA': '212.77.0.0/19',\n-        'VC': '207.191.240.0/21',\n-        'VE': '186.88.0.0/13',\n-        'VG': '66.81.192.0/20',\n-        'VI': '146.226.0.0/16',\n-        'VN': '14.160.0.0/11',\n-        'VU': '202.80.32.0/20',\n-        'WF': '117.20.32.0/21',\n-        'WS': '202.4.32.0/19',\n-        'YE': '134.35.0.0/16',\n-        'YT': '41.242.116.0/22',\n-        'ZA': '41.0.0.0/11',\n-        'ZM': '102.144.0.0/13',\n-        'ZW': '102.177.192.0/18',\n-    }\n-\n-    @classmethod\n-    def random_ipv4(cls, code_or_block):\n-        if len(code_or_block) == 2:\n-            block = cls._country_ip_map.get(code_or_block.upper())\n-            if not block:\n-                return None\n-        else:\n-            block = code_or_block\n-        addr, preflen = block.split('/')\n-        addr_min = compat_struct_unpack('!L', socket.inet_aton(addr))[0]\n-        addr_max = addr_min | (0xffffffff >> int(preflen))\n-        return compat_str(socket.inet_ntoa(\n-            compat_struct_pack('!L', random.randint(addr_min, addr_max))))\n-\n-\n-class PerRequestProxyHandler(compat_urllib_request.ProxyHandler):\n-    def __init__(self, proxies=None):\n-        # Set default handlers\n-        for type in ('http', 'https'):\n-            setattr(self, '%s_open' % type,\n-                    lambda r, proxy='__noproxy__', type=type, meth=self.proxy_open:\n-                        meth(r, proxy, type))\n-        compat_urllib_request.ProxyHandler.__init__(self, proxies)\n-\n-    def proxy_open(self, req, proxy, type):\n-        req_proxy = req.headers.get('Ytdl-request-proxy')\n-        if req_proxy is not None:\n-            proxy = req_proxy\n-            del req.headers['Ytdl-request-proxy']\n-\n-        if proxy == '__noproxy__':\n-            return None  # No Proxy\n-        if compat_urllib_parse.urlparse(proxy).scheme.lower() in ('socks', 'socks4', 'socks4a', 'socks5'):\n-            req.add_header('Ytdl-socks-proxy', proxy)\n-            # youtube-dl's http/https handlers do wrapping the socket with socks\n-            return None\n-        return compat_urllib_request.ProxyHandler.proxy_open(\n-            self, req, proxy, type)\n-\n-\n-# Both long_to_bytes and bytes_to_long are adapted from PyCrypto, which is\n-# released into Public Domain\n-# https://github.com/dlitz/pycrypto/blob/master/lib/Crypto/Util/number.py#L387\n-\n-def long_to_bytes(n, blocksize=0):\n-    \"\"\"long_to_bytes(n:long, blocksize:int) : string\n-    Convert a long integer to a byte string.\n-\n-    If optional blocksize is given and greater than zero, pad the front of the\n-    byte string with binary zeros so that the length is a multiple of\n-    blocksize.\n-    \"\"\"\n-    # after much testing, this algorithm was deemed to be the fastest\n-    s = b''\n-    n = int(n)\n-    while n > 0:\n-        s = compat_struct_pack('>I', n & 0xffffffff) + s\n-        n = n >> 32\n-    # strip off leading zeros\n-    for i in range(len(s)):\n-        if s[i] != b'\\000'[0]:\n-            break\n-    else:\n-        # only happens when n == 0\n-        s = b'\\000'\n-        i = 0\n-    s = s[i:]\n-    # add back some pad bytes.  this could be done more efficiently w.r.t. the\n-    # de-padding being done above, but sigh...\n-    if blocksize > 0 and len(s) % blocksize:\n-        s = (blocksize - len(s) % blocksize) * b'\\000' + s\n-    return s\n-\n-\n-def bytes_to_long(s):\n-    \"\"\"bytes_to_long(string) : long\n-    Convert a byte string to a long integer.\n-\n-    This is (essentially) the inverse of long_to_bytes().\n-    \"\"\"\n-    acc = 0\n-    length = len(s)\n-    if length % 4:\n-        extra = (4 - length % 4)\n-        s = b'\\000' * extra + s\n-        length = length + extra\n-    for i in range(0, length, 4):\n-        acc = (acc << 32) + compat_struct_unpack('>I', s[i:i + 4])[0]\n-    return acc\n-\n-\n-def ohdave_rsa_encrypt(data, exponent, modulus):\n-    '''\n-    Implement OHDave's RSA algorithm. See http://www.ohdave.com/rsa/\n-\n-    Input:\n-        data: data to encrypt, bytes-like object\n-        exponent, modulus: parameter e and N of RSA algorithm, both integer\n-    Output: hex string of encrypted data\n-\n-    Limitation: supports one block encryption only\n-    '''\n-\n-    payload = int(binascii.hexlify(data[::-1]), 16)\n-    encrypted = pow(payload, exponent, modulus)\n-    return '%x' % encrypted\n-\n-\n-def pkcs1pad(data, length):\n-    \"\"\"\n-    Padding input data with PKCS#1 scheme\n-\n-    @param {int[]} data        input data\n-    @param {int}   length      target length\n-    @returns {int[]}           padded data\n-    \"\"\"\n-    if len(data) > length - 11:\n-        raise ValueError('Input data too long for PKCS#1 padding')\n-\n-    pseudo_random = [random.randint(0, 254) for _ in range(length - len(data) - 3)]\n-    return [0, 2] + pseudo_random + [0] + data\n-\n-\n-def encode_base_n(num, n, table=None):\n-    FULL_TABLE = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n-    if not table:\n-        table = FULL_TABLE[:n]\n-\n-    if n > len(table):\n-        raise ValueError('base %d exceeds table length %d' % (n, len(table)))\n-\n-    if num == 0:\n-        return table[0]\n-\n-    ret = ''\n-    while num:\n-        ret = table[num % n] + ret\n-        num = num // n\n-    return ret\n-\n-\n-def decode_packed_codes(code):\n-    mobj = re.search(PACKED_CODES_RE, code)\n-    obfuscated_code, base, count, symbols = mobj.groups()\n-    base = int(base)\n-    count = int(count)\n-    symbols = symbols.split('|')\n-    symbol_table = {}\n-\n-    while count:\n-        count -= 1\n-        base_n_count = encode_base_n(count, base)\n-        symbol_table[base_n_count] = symbols[count] or base_n_count\n-\n-    return re.sub(\n-        r'\\b(\\w+)\\b', lambda mobj: symbol_table[mobj.group(0)],\n-        obfuscated_code)\n-\n-\n-def caesar(s, alphabet, shift):\n-    if shift == 0:\n-        return s\n-    l = len(alphabet)\n-    return ''.join(\n-        alphabet[(alphabet.index(c) + shift) % l] if c in alphabet else c\n-        for c in s)\n-\n-\n-def rot47(s):\n-    return caesar(s, r'''!\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~''', 47)\n-\n-\n-def parse_m3u8_attributes(attrib):\n-    info = {}\n-    for (key, val) in re.findall(r'(?P<key>[A-Z0-9-]+)=(?P<val>\"[^\"]+\"|[^\",]+)(?:,|$)', attrib):\n-        if val.startswith('\"'):\n-            val = val[1:-1]\n-        info[key] = val\n-    return info\n-\n-\n-def urshift(val, n):\n-    return val >> n if val >= 0 else (val + 0x100000000) >> n\n-\n-\n-# Based on png2str() written by @gdkchan and improved by @yokrysty\n-# Originally posted at https://github.com/ytdl-org/youtube-dl/issues/9706\n-def decode_png(png_data):\n-    # Reference: https://www.w3.org/TR/PNG/\n-    header = png_data[8:]\n-\n-    if png_data[:8] != b'\\x89PNG\\x0d\\x0a\\x1a\\x0a' or header[4:8] != b'IHDR':\n-        raise IOError('Not a valid PNG file.')\n-\n-    int_map = {1: '>B', 2: '>H', 4: '>I'}\n-    unpack_integer = lambda x: compat_struct_unpack(int_map[len(x)], x)[0]\n-\n-    chunks = []\n-\n-    while header:\n-        length = unpack_integer(header[:4])\n-        header = header[4:]\n-\n-        chunk_type = header[:4]\n-        header = header[4:]\n-\n-        chunk_data = header[:length]\n-        header = header[length:]\n-\n-        header = header[4:]  # Skip CRC\n-\n-        chunks.append({\n-            'type': chunk_type,\n-            'length': length,\n-            'data': chunk_data\n-        })\n-\n-    ihdr = chunks[0]['data']\n-\n-    width = unpack_integer(ihdr[:4])\n-    height = unpack_integer(ihdr[4:8])\n-\n-    idat = b''\n-\n-    for chunk in chunks:\n-        if chunk['type'] == b'IDAT':\n-            idat += chunk['data']\n-\n-    if not idat:\n-        raise IOError('Unable to read PNG data.')\n-\n-    decompressed_data = bytearray(zlib.decompress(idat))\n-\n-    stride = width * 3\n-    pixels = []\n-\n-    def _get_pixel(idx):\n-        x = idx % stride\n-        y = idx // stride\n-        return pixels[y][x]\n-\n-    for y in range(height):\n-        basePos = y * (1 + stride)\n-        filter_type = decompressed_data[basePos]\n-\n-        current_row = []\n-\n-        pixels.append(current_row)\n-\n-        for x in range(stride):\n-            color = decompressed_data[1 + basePos + x]\n-            basex = y * stride + x\n-            left = 0\n-            up = 0\n-\n-            if x > 2:\n-                left = _get_pixel(basex - 3)\n-            if y > 0:\n-                up = _get_pixel(basex - stride)\n-\n-            if filter_type == 1:  # Sub\n-                color = (color + left) & 0xff\n-            elif filter_type == 2:  # Up\n-                color = (color + up) & 0xff\n-            elif filter_type == 3:  # Average\n-                color = (color + ((left + up) >> 1)) & 0xff\n-            elif filter_type == 4:  # Paeth\n-                a = left\n-                b = up\n-                c = 0\n-\n-                if x > 2 and y > 0:\n-                    c = _get_pixel(basex - stride - 3)\n-\n-                p = a + b - c\n-\n-                pa = abs(p - a)\n-                pb = abs(p - b)\n-                pc = abs(p - c)\n-\n-                if pa <= pb and pa <= pc:\n-                    color = (color + a) & 0xff\n-                elif pb <= pc:\n-                    color = (color + b) & 0xff\n-                else:\n-                    color = (color + c) & 0xff\n-\n-            current_row.append(color)\n-\n-    return width, height, pixels\n-\n-\n-def write_xattr(path, key, value):\n-    # This mess below finds the best xattr tool for the job\n-    try:\n-        # try the pyxattr module...\n-        import xattr\n-\n-        if hasattr(xattr, 'set'):  # pyxattr\n-            # Unicode arguments are not supported in python-pyxattr until\n-            # version 0.5.0\n-            # See https://github.com/ytdl-org/youtube-dl/issues/5498\n-            pyxattr_required_version = '0.5.0'\n-            if version_tuple(xattr.__version__) < version_tuple(pyxattr_required_version):\n-                # TODO: fallback to CLI tools\n-                raise XAttrUnavailableError(\n-                    'python-pyxattr is detected but is too old. '\n-                    'youtube-dl requires %s or above while your version is %s. '\n-                    'Falling back to other xattr implementations' % (\n-                        pyxattr_required_version, xattr.__version__))\n-\n-            setxattr = xattr.set\n-        else:  # xattr\n-            setxattr = xattr.setxattr\n-\n-        try:\n-            setxattr(path, key, value)\n-        except EnvironmentError as e:\n-            raise XAttrMetadataError(e.errno, e.strerror)\n-\n-    except ImportError:\n-        if compat_os_name == 'nt':\n-            # Write xattrs to NTFS Alternate Data Streams:\n-            # http://en.wikipedia.org/wiki/NTFS#Alternate_data_streams_.28ADS.29\n-            assert ':' not in key\n-            assert os.path.exists(path)\n-\n-            ads_fn = path + ':' + key\n-            try:\n-                with open(ads_fn, 'wb') as f:\n-                    f.write(value)\n-            except EnvironmentError as e:\n-                raise XAttrMetadataError(e.errno, e.strerror)\n-        else:\n-            user_has_setfattr = check_executable('setfattr', ['--version'])\n-            user_has_xattr = check_executable('xattr', ['-h'])\n-\n-            if user_has_setfattr or user_has_xattr:\n-\n-                value = value.decode('utf-8')\n-                if user_has_setfattr:\n-                    executable = 'setfattr'\n-                    opts = ['-n', key, '-v', value]\n-                elif user_has_xattr:\n-                    executable = 'xattr'\n-                    opts = ['-w', key, value]\n-\n-                cmd = ([encodeFilename(executable, True)]\n-                       + [encodeArgument(o) for o in opts]\n-                       + [encodeFilename(path, True)])\n-\n-                try:\n-                    p = subprocess.Popen(\n-                        cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE)\n-                except EnvironmentError as e:\n-                    raise XAttrMetadataError(e.errno, e.strerror)\n-                stdout, stderr = process_communicate_or_kill(p)\n-                stderr = stderr.decode('utf-8', 'replace')\n-                if p.returncode != 0:\n-                    raise XAttrMetadataError(p.returncode, stderr)\n-\n-            else:\n-                # On Unix, and can't find pyxattr, setfattr, or xattr.\n-                if sys.platform.startswith('linux'):\n-                    raise XAttrUnavailableError(\n-                        \"Couldn't find a tool to set the xattrs. \"\n-                        \"Install either the python 'pyxattr' or 'xattr' \"\n-                        \"modules, or the GNU 'attr' package \"\n-                        \"(which contains the 'setfattr' tool).\")\n-                else:\n-                    raise XAttrUnavailableError(\n-                        \"Couldn't find a tool to set the xattrs. \"\n-                        \"Install either the python 'xattr' module, \"\n-                        \"or the 'xattr' binary.\")\n-\n-\n-def random_birthday(year_field, month_field, day_field):\n-    start_date = datetime.date(1950, 1, 1)\n-    end_date = datetime.date(1995, 12, 31)\n-    offset = random.randint(0, (end_date - start_date).days)\n-    random_date = start_date + datetime.timedelta(offset)\n-    return {\n-        year_field: str(random_date.year),\n-        month_field: str(random_date.month),\n-        day_field: str(random_date.day),\n-    }\n-\n-\n-def clean_podcast_url(url):\n-    return re.sub(r'''(?x)\n-        (?:\n-            (?:\n-                chtbl\\.com/track|\n-                media\\.blubrry\\.com| # https://create.blubrry.com/resources/podcast-media-download-statistics/getting-started/\n-                play\\.podtrac\\.com\n-            )/[^/]+|\n-            (?:dts|www)\\.podtrac\\.com/(?:pts/)?redirect\\.[0-9a-z]{3,4}| # http://analytics.podtrac.com/how-to-measure\n-            flex\\.acast\\.com|\n-            pd(?:\n-                cn\\.co| # https://podcorn.com/analytics-prefix/\n-                st\\.fm # https://podsights.com/docs/\n-            )/e\n-        )/''', '', url)\n-\n-\n-if __debug__:\n-    # Raise TypeError if args can't be bound\n-    # needs compat owing to unstable inspect API, thanks PSF :-(\n-    try:\n-        inspect.signature\n-\n-        def _try_bind_args(fn, *args, **kwargs):\n-            inspect.signature(fn).bind(*args, **kwargs)\n-    except AttributeError:\n-        # Py < 3.3\n-        def _try_bind_args(fn, *args, **kwargs):\n-            fn_args = inspect.getargspec(fn)\n-            # Py2: ArgInfo(args, varargs, keywords, defaults)\n-            # Py3: ArgSpec(args, varargs, keywords, defaults)\n-            if not fn_args.keywords:\n-                for k in kwargs:\n-                    if k not in (fn_args.args or []):\n-                        raise TypeError(\"got an unexpected keyword argument: '{0}'\".format(k))\n-            if not fn_args.varargs:\n-                args_to_bind = len(args)\n-                bindable = len(fn_args.args or [])\n-                if args_to_bind > bindable:\n-                    raise TypeError('too many positional arguments')\n-                bindable -= len(fn_args.defaults or [])\n-                if args_to_bind < bindable:\n-                    if kwargs:\n-                        bindable -= len(set(fn_args.args or []) & set(kwargs))\n-                    if bindable > args_to_bind:\n-                        raise TypeError(\"missing a required argument: '{0}'\".format(fn_args.args[args_to_bind]))\n-\n-\n-def traverse_obj(obj, *paths, **kwargs):\n-    \"\"\"\n-    Safely traverse nested `dict`s and `Iterable`s\n-\n-    >>> obj = [{}, {\"key\": \"value\"}]\n-    >>> traverse_obj(obj, (1, \"key\"))\n-    \"value\"\n-\n-    Each of the provided `paths` is tested and the first producing a valid result will be returned.\n-    The next path will also be tested if the path branched but no results could be found.\n-    Supported values for traversal are `Mapping`, `Iterable` and `re.Match`.\n-    Unhelpful values (`{}`, `None`) are treated as the absence of a value and discarded.\n-\n-    The paths will be wrapped in `variadic`, so that `'key'` is conveniently the same as `('key', )`.\n-\n-    The keys in the path can be one of:\n-        - `None`:           Return the current object.\n-        - `set`:            Requires the only item in the set to be a type or function,\n-                            like `{type}`/`{func}`. If a `type`, returns only values\n-                            of this type. If a function, returns `func(obj)`.\n-        - `str`/`int`:      Return `obj[key]`. For `re.Match`, return `obj.group(key)`.\n-        - `slice`:          Branch out and return all values in `obj[key]`.\n-        - `Ellipsis`:       Branch out and return a list of all values.\n-        - `tuple`/`list`:   Branch out and return a list of all matching values.\n-                            Read as: `[traverse_obj(obj, branch) for branch in branches]`.\n-        - `function`:       Branch out and return values filtered by the function.\n-                            Read as: `[value for key, value in obj if function(key, value)]`.\n-                            For `Sequence`s, `key` is the index of the value.\n-                            For `Iterable`s, `key` is the enumeration count of the value.\n-                            For `re.Match`es, `key` is the group number (0 = full match)\n-                            as well as additionally any group names, if given.\n-        - `dict`            Transform the current object and return a matching dict.\n-                            Read as: `{key: traverse_obj(obj, path) for key, path in dct.items()}`.\n-\n-        `tuple`, `list`, and `dict` all support nested paths and branches.\n-\n-    @params paths           Paths which to traverse by.\n-    Keyword arguments:\n-    @param default          Value to return if the paths do not match.\n-                            If the last key in the path is a `dict`, it will apply to each value inside\n-                            the dict instead, depth first. Try to avoid if using nested `dict` keys.\n-    @param expected_type    If a `type`, only accept final values of this type.\n-                            If any other callable, try to call the function on each result.\n-                            If the last key in the path is a `dict`, it will apply to each value inside\n-                            the dict instead, recursively. This does respect branching paths.\n-    @param get_all          If `False`, return the first matching result, otherwise all matching ones.\n-    @param casesense        If `False`, consider string dictionary keys as case insensitive.\n-\n-    The following are only meant to be used by YoutubeDL.prepare_outtmpl and are not part of the API\n-\n-    @param _is_user_input    Whether the keys are generated from user input.\n-                            If `True` strings get converted to `int`/`slice` if needed.\n-    @param _traverse_string  Whether to traverse into objects as strings.\n-                            If `True`, any non-compatible object will first be\n-                            converted into a string and then traversed into.\n-                            The return value of that path will be a string instead,\n-                            not respecting any further branching.\n-\n-\n-    @returns                The result of the object traversal.\n-                            If successful, `get_all=True`, and the path branches at least once,\n-                            then a list of results is returned instead.\n-                            A list is always returned if the last path branches and no `default` is given.\n-                            If a path ends on a `dict` that result will always be a `dict`.\n-    \"\"\"\n-\n-    # parameter defaults\n-    default = kwargs.get('default', NO_DEFAULT)\n-    expected_type = kwargs.get('expected_type')\n-    get_all = kwargs.get('get_all', True)\n-    casesense = kwargs.get('casesense', True)\n-    _is_user_input = kwargs.get('_is_user_input', False)\n-    _traverse_string = kwargs.get('_traverse_string', False)\n-\n-    # instant compat\n-    str = compat_str\n-\n-    casefold = lambda k: compat_casefold(k) if isinstance(k, str) else k\n-\n-    if isinstance(expected_type, type):\n-        type_test = lambda val: val if isinstance(val, expected_type) else None\n-    else:\n-        type_test = lambda val: try_call(expected_type or IDENTITY, args=(val,))\n-\n-    def lookup_or_none(v, k, getter=None):\n-        try:\n-            return getter(v, k) if getter else v[k]\n-        except IndexError:\n-            return None\n-\n-    def from_iterable(iterables):\n-        # chain.from_iterable(['ABC', 'DEF']) --> A B C D E F\n-        for it in iterables:\n-            for item in it:\n-                yield item\n-\n-    def apply_key(key, obj, is_last):\n-        branching = False\n-\n-        if obj is None and _traverse_string:\n-            if key is Ellipsis or callable(key) or isinstance(key, slice):\n-                branching = True\n-                result = ()\n-            else:\n-                result = None\n-\n-        elif key is None:\n-            result = obj\n-\n-        elif isinstance(key, set):\n-            assert len(key) == 1, 'Set should only be used to wrap a single item'\n-            item = next(iter(key))\n-            if isinstance(item, type):\n-                result = obj if isinstance(obj, item) else None\n-            else:\n-                result = try_call(item, args=(obj,))\n-\n-        elif isinstance(key, (list, tuple)):\n-            branching = True\n-            result = from_iterable(\n-                apply_path(obj, branch, is_last)[0] for branch in key)\n-\n-        elif key is Ellipsis:\n-            branching = True\n-            if isinstance(obj, compat_collections_abc.Mapping):\n-                result = obj.values()\n-            elif is_iterable_like(obj):\n-                result = obj\n-            elif isinstance(obj, compat_re_Match):\n-                result = obj.groups()\n-            elif _traverse_string:\n-                branching = False\n-                result = str(obj)\n-            else:\n-                result = ()\n-\n-        elif callable(key):\n-            branching = True\n-            if isinstance(obj, compat_collections_abc.Mapping):\n-                iter_obj = obj.items()\n-            elif is_iterable_like(obj):\n-                iter_obj = enumerate(obj)\n-            elif isinstance(obj, compat_re_Match):\n-                iter_obj = itertools.chain(\n-                    enumerate(itertools.chain((obj.group(),), obj.groups())),\n-                    obj.groupdict().items())\n-            elif _traverse_string:\n-                branching = False\n-                iter_obj = enumerate(str(obj))\n-            else:\n-                iter_obj = ()\n-\n-            result = (v for k, v in iter_obj if try_call(key, args=(k, v)))\n-            if not branching:  # string traversal\n-                result = ''.join(result)\n-\n-        elif isinstance(key, dict):\n-            iter_obj = ((k, _traverse_obj(obj, v, False, is_last)) for k, v in key.items())\n-            result = dict((k, v if v is not None else default) for k, v in iter_obj\n-                          if v is not None or default is not NO_DEFAULT) or None\n-\n-        elif isinstance(obj, compat_collections_abc.Mapping):\n-            result = (try_call(obj.get, args=(key,))\n-                      if casesense or try_call(obj.__contains__, args=(key,))\n-                      else next((v for k, v in obj.items() if casefold(k) == key), None))\n-\n-        elif isinstance(obj, compat_re_Match):\n-            result = None\n-            if isinstance(key, int) or casesense:\n-                # Py 2.6 doesn't have methods in the Match class/type\n-                result = lookup_or_none(obj, key, getter=lambda _, k: obj.group(k))\n-\n-            elif isinstance(key, str):\n-                result = next((v for k, v in obj.groupdict().items()\n-                              if casefold(k) == key), None)\n-\n-        else:\n-            result = None\n-            if isinstance(key, (int, slice)):\n-                if is_iterable_like(obj, compat_collections_abc.Sequence):\n-                    branching = isinstance(key, slice)\n-                    result = lookup_or_none(obj, key)\n-                elif _traverse_string:\n-                    result = lookup_or_none(str(obj), key)\n-\n-        return branching, result if branching else (result,)\n-\n-    def lazy_last(iterable):\n-        iterator = iter(iterable)\n-        prev = next(iterator, NO_DEFAULT)\n-        if prev is NO_DEFAULT:\n-            return\n-\n-        for item in iterator:\n-            yield False, prev\n-            prev = item\n-\n-        yield True, prev\n-\n-    def apply_path(start_obj, path, test_type):\n-        objs = (start_obj,)\n-        has_branched = False\n-\n-        key = None\n-        for last, key in lazy_last(variadic(path, (str, bytes, dict, set))):\n-            if _is_user_input and isinstance(key, str):\n-                if key == ':':\n-                    key = Ellipsis\n-                elif ':' in key:\n-                    key = slice(*map(int_or_none, key.split(':')))\n-                elif int_or_none(key) is not None:\n-                    key = int(key)\n-\n-            if not casesense and isinstance(key, str):\n-                key = compat_casefold(key)\n-\n-            if __debug__ and callable(key):\n-                # Verify function signature\n-                _try_bind_args(key, None, None)\n-\n-            new_objs = []\n-            for obj in objs:\n-                branching, results = apply_key(key, obj, last)\n-                has_branched |= branching\n-                new_objs.append(results)\n-\n-            objs = from_iterable(new_objs)\n-\n-        if test_type and not isinstance(key, (dict, list, tuple)):\n-            objs = map(type_test, objs)\n-\n-        return objs, has_branched, isinstance(key, dict)\n-\n-    def _traverse_obj(obj, path, allow_empty, test_type):\n-        results, has_branched, is_dict = apply_path(obj, path, test_type)\n-        results = LazyList(x for x in results if x not in (None, {}))\n-\n-        if get_all and has_branched:\n-            if results:\n-                return results.exhaust()\n-            if allow_empty:\n-                return [] if default is NO_DEFAULT else default\n-            return None\n-\n-        return results[0] if results else {} if allow_empty and is_dict else None\n-\n-    for index, path in enumerate(paths, 1):\n-        result = _traverse_obj(obj, path, index == len(paths), True)\n-        if result is not None:\n-            return result\n-\n-    return None if default is NO_DEFAULT else default\n-\n-\n-def T(x):\n-    \"\"\" For use in yt-dl instead of {type} or set((type,)) \"\"\"\n-    return set((x,))\n-\n-\n-def get_first(obj, keys, **kwargs):\n-    return traverse_obj(obj, (Ellipsis,) + tuple(variadic(keys)), get_all=False, **kwargs)\n-\n-\n-def join_nonempty(*values, **kwargs):\n-\n-    # parameter defaults\n-    delim = kwargs.get('delim', '-')\n-    from_dict = kwargs.get('from_dict')\n-\n-    if from_dict is not None:\n-        values = (traverse_obj(from_dict, variadic(v)) for v in values)\n-    return delim.join(map(compat_str, filter(None, values)))\n+        'sv\n"
    ]
  }
]