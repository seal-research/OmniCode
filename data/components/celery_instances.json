[
    {
        "repo": "celery/celery",
        "pull_number": 8806,
        "instance_id": "celery__celery-8806",
        "issue_numbers": [
            "2907"
        ],
        "base_commit": "8f389997887232500d4aa1a2b0ae0c7320c4c84a",
        "patch": "diff --git a/CONTRIBUTORS.txt b/CONTRIBUTORS.txt\nindex e0a8394bc6f..6159effcc3a 100644\n--- a/CONTRIBUTORS.txt\n+++ b/CONTRIBUTORS.txt\n@@ -295,4 +295,5 @@ JoonHwan Kim, 2022/08/01\n Kaustav Banerjee, 2022/11/10\n Austin Snoeyink 2022/12/06\n Jeremy Z. Othieno 2023/07/27\n-Tomer Nosrati, 2022/17/07\n\\ No newline at end of file\n+Tomer Nosrati, 2022/17/07\n+Andy Zickler, 2024/01/18\n\\ No newline at end of file\ndiff --git a/celery/beat.py b/celery/beat.py\nindex 76e44721e14..9656493ecbe 100644\n--- a/celery/beat.py\n+++ b/celery/beat.py\n@@ -568,11 +568,11 @@ def _create_schedule(self):\n         for _ in (1, 2):\n             try:\n                 self._store['entries']\n-            except KeyError:\n+            except (KeyError, UnicodeDecodeError, TypeError):\n                 # new schedule db\n                 try:\n                     self._store['entries'] = {}\n-                except KeyError as exc:\n+                except (KeyError, UnicodeDecodeError, TypeError) as exc:\n                     self._store = self._destroy_open_corrupted_schedule(exc)\n                     continue\n             else:\n",
        "test_patch": "diff --git a/t/unit/app/test_beat.py b/t/unit/app/test_beat.py\nindex fa163bb931e..a95e8e41409 100644\n--- a/t/unit/app/test_beat.py\n+++ b/t/unit/app/test_beat.py\n@@ -2,7 +2,7 @@\n import sys\n from datetime import datetime, timedelta, timezone\n from pickle import dumps, loads\n-from unittest.mock import Mock, call, patch\n+from unittest.mock import MagicMock, Mock, call, patch\n \n import pytest\n \n@@ -669,6 +669,38 @@ def test_remove_db(self, remove):\n         with pytest.raises(OSError):\n             s._remove_db()\n \n+    def test_create_schedule_corrupted(self):\n+        \"\"\"\n+        Test that any decoding errors that might happen when opening beat-schedule.db are caught\n+        \"\"\"\n+        s = create_persistent_scheduler()[0](app=self.app,\n+                                             schedule_filename='schedule')\n+        s._store = MagicMock()\n+        s._destroy_open_corrupted_schedule = Mock()\n+        s._destroy_open_corrupted_schedule.return_value = MagicMock()\n+\n+        # self._store['entries'] will throw a KeyError\n+        s._store.__getitem__.side_effect = KeyError()\n+        # then, when _create_schedule tries to reset _store['entries'], throw another error\n+        expected_error = UnicodeDecodeError(\"ascii\", b\"ordinal not in range(128)\", 0, 0, \"\")\n+        s._store.__setitem__.side_effect = expected_error\n+\n+        s._create_schedule()\n+        s._destroy_open_corrupted_schedule.assert_called_with(expected_error)\n+\n+    def test_create_schedule_missing_entries(self):\n+        \"\"\"\n+        Test that if _create_schedule can't find the key \"entries\" in _store it will recreate it\n+        \"\"\"\n+        s = create_persistent_scheduler()[0](app=self.app, schedule_filename=\"schedule\")\n+        s._store = MagicMock()\n+\n+        # self._store['entries'] will throw a KeyError\n+        s._store.__getitem__.side_effect = TypeError()\n+\n+        s._create_schedule()\n+        s._store.__setitem__.assert_called_with(\"entries\", {})\n+\n     def test_setup_schedule(self):\n         s = create_persistent_scheduler()[0](app=self.app,\n                                              schedule_filename='schedule')\n",
        "problem_statement": "Celery beat UnicodeDecodeError (Python 3.4) issue\nI am using Python 3.4 with Celery 3.1.19\n\nRunning without beat it works properly:\n\n```\ncelery worker --app worker --config=celeryconfig --loglevel=info\n```\n\nBut with celery beat:\n\n```\ncelery worker --app worker -B --config=celeryconfig --loglevel=info\n```\n\nI got this exception:\n\n```\nTraceback (most recent call last):\n  File \"/env/lib/python3.4/site-packages/kombu/utils/__init__.py\", line 320, in __get__\n    return obj.__dict__[self.__name__]\nKeyError: 'scheduler'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/shelve.py\", line 111, in __getitem__\n    value = self.cache[key]\nKeyError: 'entries'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/env/lib/python3.4/site-packages/billiard/process.py\", line 292, in _bootstrap\n    self.run()\n  File \"/env/lib/python3.4/site-packages/celery/beat.py\", line 530, in run\n    self.service.start(embedded_process=True)\n  File \"/env/lib/python3.4/site-packages/celery/beat.py\", line 454, in start\n    humanize_seconds(self.scheduler.max_interval))\n  File \"/env/lib/python3.4/site-packages/kombu/utils/__init__.py\", line 322, in __get__\n    value = obj.__dict__[self.__name__] = self.__get(obj)\n  File \"/env/lib/python3.4/site-packages/celery/beat.py\", line 494, in scheduler\n    return self.get_scheduler()\n  File \"/env/lib/python3.4/site-packages/celery/beat.py\", line 489, in get_scheduler\n    lazy=lazy)\n  File \"/env/lib/python3.4/site-packages/celery/utils/imports.py\", line 53, in instantiate\n    return symbol_by_name(name)(*args, **kwargs)\n  File \"/env/lib/python3.4/site-packages/celery/beat.py\", line 358, in __init__\n    Scheduler.__init__(self, *args, **kwargs)\n  File \"/env/lib/python3.4/site-packages/celery/beat.py\", line 185, in __init__\n    self.setup_schedule()\n  File \"/env/lib/python3.4/site-packages/celery/beat.py\", line 377, in setup_schedule\n    self._store['entries']\n  File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/shelve.py\", line 114, in __getitem__\n    value = Unpickler(f).load()\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xdf in position 1: ordinal not in range(128)\n```\n\nAny ideas? Thanks in advance\n\n",
        "hints_text": "It works for me here, maybe you could try removing the celerybeat-schedule file?\n\nIt worked! thanks! I removed the `celerybeat-schedule.db` file in the current directory.\n\nMe too, thaks @ask ! Just so you know I've migrated from `Python 2.7.12` to `Python 3.5.3` and it started happening.\nWorked for me also, what I did is rename the file to \"celerybeat-schedule.db.backup\"\nin dicrectory Delete (rm) celerybeat-schedule.db and celerybeat-schedule like this \r\n\r\n```\r\ncd project-directory\r\nrm celerybeat-schedule.db celerybeat-schedule\r\n```",
        "created_at": "2024-01-19T00:56:19Z",
        "version": "5.4",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "t/unit/app/test_beat.py"
        ]
    },
    {
        "repo": "celery/celery",
        "pull_number": 8702,
        "instance_id": "celery__celery-8702",
        "issue_numbers": [
            "8678"
        ],
        "base_commit": "17631f7eda712b688294ecb8fa53e4769fe2b1f9",
        "patch": "diff --git a/celery/canvas.py b/celery/canvas.py\nindex a4007f0a27f..a32d3eea7e7 100644\n--- a/celery/canvas.py\n+++ b/celery/canvas.py\n@@ -2271,6 +2271,8 @@ def link_error(self, errback):\n             ``False`` (the current default), then the error callback will only be\n             applied to the body.\n         \"\"\"\n+        errback = maybe_signature(errback)\n+\n         if self.app.conf.task_allow_error_cb_on_chord_header:\n             for task in maybe_list(self.tasks) or []:\n                 task.link_error(errback.clone(immutable=True))\n",
        "test_patch": "diff --git a/t/unit/tasks/test_canvas.py b/t/unit/tasks/test_canvas.py\nindex 2c3f4f12f3e..53dc52e5cbb 100644\n--- a/t/unit/tasks/test_canvas.py\n+++ b/t/unit/tasks/test_canvas.py\n@@ -1688,6 +1688,14 @@ def test_flag_allow_error_cb_on_chord_header_various_header_types(self):\n             errback = c.link_error(sig)\n             assert errback == sig\n \n+    @pytest.mark.usefixtures('depends_on_current_app')\n+    def test_flag_allow_error_cb_on_chord_header_with_dict_callback(self):\n+        self.app.conf.task_allow_error_cb_on_chord_header = True\n+        c = chord(group(signature('th1'), signature('th2')), signature('tbody'))\n+        errback_dict = dict(signature('tcb'))\n+        errback = c.link_error(errback_dict)\n+        assert errback == errback_dict\n+\n     def test_chord__or__group_of_single_task(self):\n         \"\"\" Test chaining a chord to a group of a single task. \"\"\"\n         c = chord([signature('header')], signature('body'))\n",
        "problem_statement": "task with an errback throws `AttributeError` when replaced with a chord, when `task_allow_error_cb_on_chord_header` is set\n<!--\r\nPlease fill this template entirely and do not erase parts of it.\r\nWe reserve the right to close without a response\r\nbug reports which are incomplete.\r\n-->\r\n# Checklist\r\n<!--\r\nTo check an item on the list replace [ ] with [x].\r\n-->\r\n- [x] I have verified that the issue exists against the `main` branch of Celery.\r\n- [ ] This has already been asked to the [discussions forum](https://github.com/celery/celery/discussions) first.\r\n- [x] I have read the relevant section in the\r\n  [contribution guide](https://docs.celeryq.dev/en/main/contributing.html#other-bugs)\r\n  on reporting bugs.\r\n- [x] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Bug+Report%22+-label%3A%22Category%3A+Documentation%22)\r\n  for similar or identical bug reports.\r\n- [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22PR+Type%3A+Bugfix%22+-label%3A%22Category%3A+Documentation%22)\r\n  for existing proposed fixes.\r\n- [x] I have checked the [commit log](https://github.com/celery/celery/commits/main)\r\n  to find out if the bug was already fixed in the main branch.\r\n- [x] I have included all related issues and possible duplicate issues\r\n  in this issue (If there are none, check this box anyway).\r\n\r\n## Mandatory Debugging Information\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n    (if you are not able to do this, then at least specify the Celery\r\n     version affected).\r\n- [x] I have verified that the issue exists against the `main` branch of Celery.\r\n- [x] I have included the contents of ``pip freeze`` in the issue.\r\n- [x] I have included all the versions of all the external dependencies required\r\n  to reproduce this bug.\r\n\r\n## Optional Debugging Information\r\n<!--\r\nTry some of the below if you think they are relevant.\r\nIt will help us figure out the scope of the bug and how many users it affects.\r\n-->\r\n- [ ] I have tried reproducing the issue on more than one Python version\r\n  and/or implementation.\r\n- [ ] I have tried reproducing the issue on more than one message broker and/or\r\n  result backend.\r\n- [ ] I have tried reproducing the issue on more than one version of the message\r\n  broker and/or result backend.\r\n- [ ] I have tried reproducing the issue on more than one operating system.\r\n- [x] I have tried reproducing the issue on more than one workers pool.\r\n- [ ] I have tried reproducing the issue with autoscaling, retries,\r\n  ETA/Countdown & rate limits disabled.\r\n- [x] I have tried reproducing the issue after downgrading\r\n  and/or upgrading Celery and its dependencies.\r\n\r\n## Related Issues and Possible Duplicates\r\n<!--\r\nPlease make sure to search and mention any related issues\r\nor possible duplicates to this issue as requested by the checklist above.\r\n\r\nThis may or may not include issues in other repositories that the Celery project\r\nmaintains or other repositories that are dependencies of Celery.\r\n\r\nIf you don't know how to mention issues, please refer to Github's documentation\r\non the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests\r\n-->\r\n\r\n#### Related Issues\r\n\r\nhttps://github.com/celery/celery/issues/8456\r\n\r\n#### Possible Duplicates\r\n\r\n- None\r\n\r\n## Environment & Settings\r\n<!-- Include the contents of celery --version below -->\r\n**Celery version**: 5.3.6 (emerald-rush)\r\n<!-- Include the output of celery -A proj report below -->\r\n<details>\r\n<summary><b><code>celery report</code> Output:</b></summary>\r\n<p>\r\n\r\n```\r\nsoftware -> celery:5.3.6 (emerald-rush) kombu:5.3.4 py:3.8.10\r\n            billiard:4.2.0 redis:5.0.1\r\nplatform -> system:Darwin arch:64bit\r\n            kernel version:23.1.0 imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:redis results:redis://0.0.0.0:6479/\r\n\r\nbroker_url: 'redis://0.0.0.0:6479//'\r\nresult_backend: 'redis://0.0.0.0:6479/'\r\ndeprecated_settings: None\r\ntask_allow_error_cb_on_chord_header: True\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n# Steps to Reproduce\r\n\r\n## Required Dependencies\r\n<!-- Please fill the required dependencies to reproduce this issue -->\r\n- **Minimal Python Version**: N/A or Unknown\r\n- **Minimal Celery Version**: 5.3.1\r\n- **Minimal Kombu Version**: N/A or Unknown\r\n- **Minimal Broker Version**: N/A or Unknown\r\n- **Minimal Result Backend Version**: N/A or Unknown\r\n- **Minimal OS and/or Kernel Version**: N/A or Unknown\r\n- **Minimal Broker Client Version**: N/A or Unknown\r\n- **Minimal Result Backend Client Version**: N/A or Unknown\r\n\r\n### Python Packages\r\n<!-- Please fill the contents of pip freeze below -->\r\n<details>\r\n<summary><b><code>pip freeze</code> Output:</b></summary>\r\n<p>\r\n\r\n```\r\naiohttp==3.8.5\r\naiosignal==1.3.1\r\namqp==5.2.0\r\nasgiref==3.6.0\r\nastroid==2.15.2\r\nasync-timeout==4.0.3\r\nattrs==22.2.0\r\nautopep8==1.5.5\r\nbackports.zoneinfo==0.2.1\r\nbandit==1.7.5\r\nbilliard==4.2.0\r\nblack==23.7.0\r\nboto3-stubs==1.19.12.post1\r\nbotocore-stubs==1.29.107\r\ncelery==5.3.6\r\ncelery-stubs==0.1.3\r\ncertifi==2022.12.7\r\ncfgv==3.3.1\r\ncharset-normalizer==3.1.0\r\nclick==8.1.7\r\nclick-didyoumean==0.3.0\r\nclick-plugins==1.1.1\r\nclick-repl==0.3.0\r\ncoreapi==2.3.3\r\ncoreschema==0.0.4\r\ndata-science-types==0.2.23\r\ndill==0.3.6\r\ndistlib==0.3.6\r\nDjango==4.2\r\ndjango-filter-stubs==0.1.3\r\ndjango-stubs==1.15.0\r\ndjango-stubs-ext==0.8.0\r\ndjangorestframework==3.14.0\r\ndjangorestframework-stubs==1.9.1\r\ndrf-yasg==1.20.3\r\nfactory-boy==3.2.1\r\nFaker==18.3.4\r\nfilelock==3.10.7\r\nflake8==3.8.4\r\nfrozenlist==1.4.0\r\nfuzzywuzzy-stubs==0.0.1\r\ngitdb==4.0.10\r\nGitPython==3.1.31\r\ngraphene-stubs==0.15\r\nidentify==2.5.22\r\nidna==3.4\r\ninflection==0.5.1\r\nisort==5.12.0\r\nitypes==1.2.0\r\njedi==0.17.2\r\nJinja2==3.1.2\r\nkombu==5.3.4\r\nlazy-object-proxy==1.9.0\r\nmarkdown-it-py==2.2.0\r\nMarkupSafe==2.1.2\r\nmccabe==0.6.1\r\nmdurl==0.1.2\r\nmultidict==6.0.4\r\nmypy==1.1.1\r\nmypy-extensions==1.0.0\r\nnodeenv==1.7.0\r\npackaging==23.0\r\nparso==0.7.1\r\npathspec==0.11.1\r\npbr==5.11.1\r\npip-licenses==3.5.5\r\nplatformdirs==3.2.0\r\npluggy==1.0.0\r\npre-commit==2.7.1\r\nprompt-toolkit==3.0.41\r\nPTable==0.9.2\r\npycodestyle==2.6.0\r\npydocstyle==6.3.0\r\npyflakes==2.2.0\r\nPygments==2.14.0\r\npylint==2.17.2\r\npython-dateutil==2.8.2\r\npython-jsonrpc-server==0.4.0\r\npython-language-server==0.36.2\r\npytoolconfig==1.2.5\r\npytz==2023.3\r\nPyYAML==6.0\r\nratelimit-stubs==2.2.1\r\nredis==5.0.1\r\nregex==2019.11.1\r\nrequests==2.28.2\r\nrich==13.3.3\r\nrope==1.7.0\r\nruamel.yaml==0.17.21\r\nruamel.yaml.clib==0.2.7\r\nsix==1.16.0\r\nsmmap==5.0.0\r\nsnowballstemmer==2.2.0\r\nsqlparse==0.4.3\r\nstevedore==5.0.0\r\ntoml==0.10.2\r\ntomli==2.0.1\r\ntomlkit==0.11.7\r\ntypes-awscrt==0.16.13.post1\r\ntypes-beautifulsoup4==4.10.20\r\ntypes-docutils==0.19.1.7\r\ntypes-pytz==2023.3.0.0\r\ntypes-PyYAML==5.4.12\r\ntypes-requests==2.25.12\r\ntypes-setuptools==67.3.0.2\r\ntypes-six==0.1.9\r\ntyping_extensions==4.5.0\r\ntzdata==2023.3\r\nujson==5.7.0\r\nuritemplate==4.1.1\r\nurllib3==1.26.15\r\nvine==5.1.0\r\nvirtualenv==20.21.0\r\nwcwidth==0.2.12\r\nwrapt==1.15.0\r\nyapf==0.32.0\r\nyarl==1.8.2\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n### Other Dependencies\r\n<!--\r\nPlease provide system dependencies, configuration files\r\nand other dependency information if applicable\r\n-->\r\n<details>\r\n<p>\r\nN/A\r\n</p>\r\n</details>\r\n\r\n## Minimally Reproducible Test Case\r\n<!--\r\nPlease provide a reproducible test case.\r\nRefer to the Reporting Bugs section in our contribution guide.\r\n\r\nWe prefer submitting test cases in the form of a PR to our integration test suite.\r\nIf you can provide one, please mention the PR number below.\r\nIf not, please attach the most minimal code example required to reproduce the issue below.\r\nIf the test case is too large, please include a link to a gist or a repository below.\r\n-->\r\n\r\n<details>\r\n<p>\r\n\r\n```python\r\nfrom celery import Celery, chain, chord, signature, group\r\n\r\napp = Celery(\r\n    \"celery_bug\", backend=\"redis://0.0.0.0:6479/\", broker=\"redis://0.0.0.0:6479/\"\r\n)\r\n\r\napp.conf.task_allow_error_cb_on_chord_header = True\r\n\r\n\r\n@app.task(bind=True)\r\ndef orig_task(self, arg):\r\n    chord_headers = group([chord_header.s(arg=f\"arg{i}\") for i in range(5)])\r\n    replacement_chord = chord(chord_headers, chord_body.s())\r\n    return self.replace(replacement_chord)\r\n\r\n\r\n@app.task\r\ndef chord_header(arg):\r\n    return f\"header: {arg}\"\r\n\r\n\r\n@app.task\r\ndef chord_body(arg):\r\n    return f\"body: {arg}]\"\r\n\r\n\r\n@app.task\r\ndef handle_error(*args, **kwargs):\r\n    print(f\"handle error called with args {args} kwargs {kwargs}\")\r\n\r\n\r\ndef main():\r\n    print(f\"hello world\")\r\n    res = orig_task.apply_async(args=[\"spam\"], link_error=handle_error.s())\r\n    print(f\"RESULT: {res.get()}\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n# Expected Behavior\r\n<!-- Describe in detail what you expect to happen -->\r\nI would expect the `orig_task` to be replaced with `replacement_chord` and give expected output.\r\n\r\nThis is the expected output that I do see if `task_allow_error_cb_on_chord_header` is `False`, or if the `orig_task` is called without the `link_error=` callback:  \r\n\r\n```\r\n$ poetry run python celery_bug.py \r\nhello world\r\nRESULT: body: ['header: arg0', 'header: arg1', 'header: arg2', 'header: arg3', 'header: arg4']]\r\n```\r\n# Actual Behavior\r\n<!--\r\nDescribe in detail what actually happened.\r\nPlease include a backtrace and surround it with triple backticks (```).\r\nIn addition, include the Celery daemon logs, the broker logs,\r\nthe result backend logs and system logs below if they will help us debug\r\nthe issue.\r\n-->\r\nInstead, I get this `AttributeError`:\r\n```\r\n$ poetry run python celery_bug.py \r\nhello world\r\nTraceback (most recent call last):\r\n  File \"celery_bug.py\", line 39, in <module>\r\n    main()\r\n  File \"celery_bug.py\", line 35, in main\r\n    print(f\"RESULT: {res.get()}\")\r\n  File \"/Users/robertgalloway/Library/Caches/pypoetry/virtualenvs/rpg-play-aJQQ1jqR-py3.8/lib/python3.8/site-packages/celery/result.py\", line 251, in get\r\n    return self.backend.wait_for_pending(\r\n  File \"/Users/robertgalloway/Library/Caches/pypoetry/virtualenvs/rpg-play-aJQQ1jqR-py3.8/lib/python3.8/site-packages/celery/backends/asynchronous.py\", line 223, in wait_for_pending\r\n    return result.maybe_throw(callback=callback, propagate=propagate)\r\n  File \"/Users/robertgalloway/Library/Caches/pypoetry/virtualenvs/rpg-play-aJQQ1jqR-py3.8/lib/python3.8/site-packages/celery/result.py\", line 365, in maybe_throw\r\n    self.throw(value, self._to_remote_traceback(tb))\r\n  File \"/Users/robertgalloway/Library/Caches/pypoetry/virtualenvs/rpg-play-aJQQ1jqR-py3.8/lib/python3.8/site-packages/celery/result.py\", line 358, in throw\r\n    self.on_ready.throw(*args, **kwargs)\r\n  File \"/Users/robertgalloway/Library/Caches/pypoetry/virtualenvs/rpg-play-aJQQ1jqR-py3.8/lib/python3.8/site-packages/vine/promises.py\", line 235, in throw\r\n    reraise(type(exc), exc, tb)\r\n  File \"/Users/robertgalloway/Library/Caches/pypoetry/virtualenvs/rpg-play-aJQQ1jqR-py3.8/lib/python3.8/site-packages/vine/utils.py\", line 27, in reraise\r\n    raise value\r\nAttributeError: 'dict' object has no attribute 'clone'\r\n```\r\n\r\nI suspect the error is related to this code in the `_chord.link_error()` method:\r\n```python\r\n        if self.app.conf.task_allow_error_cb_on_chord_header:\r\n            for task in maybe_list(self.tasks) or []:\r\n                task.link_error(errback.clone(immutable=True))\r\n```\r\n\n",
        "hints_text": "previous related fix  https://github.com/celery/celery/pull/8463 ",
        "created_at": "2023-12-07T12:10:06Z",
        "version": "5.3",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "t/unit/tasks/test_canvas.py"
        ]
    },
    {
        "repo": "celery/celery",
        "pull_number": 8489,
        "instance_id": "celery__celery-8489",
        "issue_numbers": [
            "6608"
        ],
        "base_commit": "b6a5bdb8b698dbe2a0848e34f76133f2950c5a82",
        "patch": "diff --git a/celery/bin/control.py b/celery/bin/control.py\nindex f7bba96ddf0..38a917ea0f2 100644\n--- a/celery/bin/control.py\n+++ b/celery/bin/control.py\n@@ -1,5 +1,6 @@\n \"\"\"The ``celery control``, ``. inspect`` and ``. status`` programs.\"\"\"\n from functools import partial\n+from typing import Literal\n \n import click\n from kombu.utils.json import dumps\n@@ -39,18 +40,69 @@ def _consume_arguments(meta, method, args):\n         args[:] = args[i:]\n \n \n-def _compile_arguments(action, args):\n-    meta = Panel.meta[action]\n+def _compile_arguments(command, args):\n+    meta = Panel.meta[command]\n     arguments = {}\n     if meta.args:\n         arguments.update({\n-            k: v for k, v in _consume_arguments(meta, action, args)\n+            k: v for k, v in _consume_arguments(meta, command, args)\n         })\n     if meta.variadic:\n         arguments.update({meta.variadic: args})\n     return arguments\n \n \n+_RemoteControlType = Literal['inspect', 'control']\n+\n+\n+def _verify_command_name(type_: _RemoteControlType, command: str) -> None:\n+    choices = _get_commands_of_type(type_)\n+\n+    if command not in choices:\n+        command_listing = \", \".join(choices)\n+        raise click.UsageError(\n+            message=f'Command {command} not recognized. Available {type_} commands: {command_listing}',\n+        )\n+\n+\n+def _list_option(type_: _RemoteControlType):\n+    def callback(ctx: click.Context, param, value) -> None:\n+        if not value:\n+            return\n+        choices = _get_commands_of_type(type_)\n+\n+        formatter = click.HelpFormatter()\n+\n+        with formatter.section(f'{type_.capitalize()} Commands'):\n+            command_list = []\n+            for command_name, info in choices.items():\n+                if info.signature:\n+                    command_preview = f'{command_name} {info.signature}'\n+                else:\n+                    command_preview = command_name\n+                command_list.append((command_preview, info.help))\n+            formatter.write_dl(command_list)\n+        ctx.obj.echo(formatter.getvalue(), nl=False)\n+        ctx.exit()\n+\n+    return click.option(\n+        '--list',\n+        is_flag=True,\n+        help=f'List available {type_} commands and exit.',\n+        expose_value=False,\n+        is_eager=True,\n+        callback=callback,\n+    )\n+\n+\n+def _get_commands_of_type(type_: _RemoteControlType) -> dict:\n+    command_name_info_pairs = [\n+        (name, info) for name, info in Panel.meta.items()\n+        if info.type == type_ and info.visible\n+    ]\n+    return dict(sorted(command_name_info_pairs))\n+\n+\n @click.command(cls=CeleryCommand)\n @click.option('-t',\n               '--timeout',\n@@ -96,10 +148,8 @@ def status(ctx, timeout, destination, json, **kwargs):\n \n @click.command(cls=CeleryCommand,\n                context_settings={'allow_extra_args': True})\n-@click.argument(\"action\", type=click.Choice([\n-    name for name, info in Panel.meta.items()\n-    if info.type == 'inspect' and info.visible\n-]))\n+@click.argument('command')\n+@_list_option('inspect')\n @click.option('-t',\n               '--timeout',\n               cls=CeleryOption,\n@@ -121,19 +171,19 @@ def status(ctx, timeout, destination, json, **kwargs):\n               help='Use json as output format.')\n @click.pass_context\n @handle_preload_options\n-def inspect(ctx, action, timeout, destination, json, **kwargs):\n-    \"\"\"Inspect the worker at runtime.\n+def inspect(ctx, command, timeout, destination, json, **kwargs):\n+    \"\"\"Inspect the workers by sending them the COMMAND inspect command.\n \n     Availability: RabbitMQ (AMQP) and Redis transports.\n     \"\"\"\n+    _verify_command_name('inspect', command)\n     callback = None if json else partial(_say_remote_command_reply, ctx,\n                                          show_reply=True)\n-    arguments = _compile_arguments(action, ctx.args)\n+    arguments = _compile_arguments(command, ctx.args)\n     inspect = ctx.obj.app.control.inspect(timeout=timeout,\n                                           destination=destination,\n                                           callback=callback)\n-    replies = inspect._request(action,\n-                               **arguments)\n+    replies = inspect._request(command, **arguments)\n \n     if not replies:\n         raise CeleryCommandException(\n@@ -153,10 +203,8 @@ def inspect(ctx, action, timeout, destination, json, **kwargs):\n \n @click.command(cls=CeleryCommand,\n                context_settings={'allow_extra_args': True})\n-@click.argument(\"action\", type=click.Choice([\n-    name for name, info in Panel.meta.items()\n-    if info.type == 'control' and info.visible\n-]))\n+@click.argument('command')\n+@_list_option('control')\n @click.option('-t',\n               '--timeout',\n               cls=CeleryOption,\n@@ -178,16 +226,17 @@ def inspect(ctx, action, timeout, destination, json, **kwargs):\n               help='Use json as output format.')\n @click.pass_context\n @handle_preload_options\n-def control(ctx, action, timeout, destination, json):\n-    \"\"\"Workers remote control.\n+def control(ctx, command, timeout, destination, json):\n+    \"\"\"Send the COMMAND control command to the workers.\n \n     Availability: RabbitMQ (AMQP), Redis, and MongoDB transports.\n     \"\"\"\n+    _verify_command_name('control', command)\n     callback = None if json else partial(_say_remote_command_reply, ctx,\n                                          show_reply=True)\n     args = ctx.args\n-    arguments = _compile_arguments(action, args)\n-    replies = ctx.obj.app.control.broadcast(action, timeout=timeout,\n+    arguments = _compile_arguments(command, args)\n+    replies = ctx.obj.app.control.broadcast(command, timeout=timeout,\n                                             destination=destination,\n                                             callback=callback,\n                                             reply=True,\ndiff --git a/t/unit/bin/proj/app_with_custom_cmds.py b/t/unit/bin/proj/app_with_custom_cmds.py\nnew file mode 100644\nindex 00000000000..db96b99e700\n--- /dev/null\n+++ b/t/unit/bin/proj/app_with_custom_cmds.py\n@@ -0,0 +1,24 @@\n+from celery import Celery\n+from celery.worker.control import control_command, inspect_command\n+\n+\n+@control_command(\n+    args=[('a', int), ('b', int)],\n+    signature='a b',\n+)\n+def custom_control_cmd(state, a, b):\n+    \"\"\"Ask the workers to reply with a and b.\"\"\"\n+    return {'ok': f'Received {a} and {b}'}\n+\n+\n+@inspect_command(\n+    args=[('x', int)],\n+    signature='x',\n+)\n+def custom_inspect_cmd(state, x):\n+    \"\"\"Ask the workers to reply with x.\"\"\"\n+    return {'ok': f'Received {x}'}\n+\n+\n+app = Celery(set_as_current=False)\n+app.config_from_object('t.integration.test_worker_config')\n",
        "test_patch": "diff --git a/t/unit/app/test_preload_cli.py b/t/unit/app/test_preload_cli.py\nindex a2241a1400d..9932f5b88d4 100644\n--- a/t/unit/app/test_preload_cli.py\n+++ b/t/unit/app/test_preload_cli.py\n@@ -1,34 +1,41 @@\n+import contextlib\n+from typing import Tuple\n+from unittest.mock import patch\n+\n+import pytest\n from click.testing import CliRunner\n \n from celery.bin.celery import celery\n \n \n-def test_preload_options(isolated_cli_runner: CliRunner):\n-    # Verify commands like shell and purge can accept preload options.\n-    # Projects like Pyramid-Celery's ini option should be valid preload\n-    # options.\n-\n-    # TODO: Find a way to run these separate invoke and assertions\n-    # such that order does not matter. Currently, running\n-    # the \"t.unit.bin.proj.pyramid_celery_app\" first seems\n-    # to result in cache or memoization of the option.\n-    # As a result, the expected exception is not raised when\n-    # the invoke on \"t.unit.bin.proj.app\" is run as a second\n-    # call.\n+@pytest.fixture(autouse=True)\n+def reset_command_params_between_each_test():\n+    with contextlib.ExitStack() as stack:\n+        for command in celery.commands.values():\n+            # We only need shallow copy -- preload options are appended to the list,\n+            # existing options are kept as-is\n+            params_copy = command.params[:]\n+            patch_instance = patch.object(command, \"params\", params_copy)\n+            stack.enter_context(patch_instance)\n \n-    res_without_preload = isolated_cli_runner.invoke(\n-        celery,\n-        [\"-A\", \"t.unit.bin.proj.app\", \"purge\", \"-f\", \"--ini\", \"some_ini.ini\"],\n-        catch_exceptions=True,\n-    )\n+        yield\n \n-    assert \"No such option: --ini\" in res_without_preload.stdout\n-    assert res_without_preload.exit_code == 2\n \n+@pytest.mark.parametrize(\n+    \"subcommand_with_params\",\n+    [\n+        (\"purge\", \"-f\"),\n+        (\"shell\",),\n+    ]\n+)\n+def test_preload_options(subcommand_with_params: Tuple[str, ...], isolated_cli_runner: CliRunner):\n+    # Verify commands like shell and purge can accept preload options.\n+    # Projects like Pyramid-Celery's ini option should be valid preload\n+    # options.\n     res_without_preload = isolated_cli_runner.invoke(\n         celery,\n-        [\"-A\", \"t.unit.bin.proj.app\", \"shell\", \"--ini\", \"some_ini.ini\"],\n-        catch_exceptions=True,\n+        [\"-A\", \"t.unit.bin.proj.app\", *subcommand_with_params, \"--ini\", \"some_ini.ini\"],\n+        catch_exceptions=False,\n     )\n \n     assert \"No such option: --ini\" in res_without_preload.stdout\n@@ -39,25 +46,11 @@ def test_preload_options(isolated_cli_runner: CliRunner):\n         [\n             \"-A\",\n             \"t.unit.bin.proj.pyramid_celery_app\",\n-            \"purge\",\n-            \"-f\",\n+            *subcommand_with_params,\n             \"--ini\",\n             \"some_ini.ini\",\n         ],\n-        catch_exceptions=True,\n+        catch_exceptions=False,\n     )\n \n-    assert res_with_preload.exit_code == 0\n-\n-    res_with_preload = isolated_cli_runner.invoke(\n-        celery,\n-        [\n-            \"-A\",\n-            \"t.unit.bin.proj.pyramid_celery_app\",\n-            \"shell\",\n-            \"--ini\",\n-            \"some_ini.ini\",\n-        ],\n-        catch_exceptions=True,\n-    )\n-    assert res_with_preload.exit_code == 0\n+    assert res_with_preload.exit_code == 0, res_with_preload.stdout\ndiff --git a/t/unit/bin/test_control.py b/t/unit/bin/test_control.py\nnew file mode 100644\nindex 00000000000..6d3704e9dc2\n--- /dev/null\n+++ b/t/unit/bin/test_control.py\n@@ -0,0 +1,82 @@\n+import os\n+import re\n+from unittest.mock import patch\n+\n+import pytest\n+from click.testing import CliRunner\n+\n+from celery.bin.celery import celery\n+from celery.platforms import EX_UNAVAILABLE\n+\n+_GLOBAL_OPTIONS = ['-A', 't.unit.bin.proj.app_with_custom_cmds', '--broker', 'memory://']\n+_INSPECT_OPTIONS = ['--timeout', '0']  # Avoid waiting for the zero workers to reply\n+\n+\n+@pytest.fixture(autouse=True)\n+def clean_os_environ():\n+    # Celery modifies os.environ when given the CLI option --broker memory://\n+    # This interferes with other tests, so we need to reset os.environ\n+    with patch.dict(os.environ, clear=True):\n+        yield\n+\n+\n+@pytest.mark.parametrize(\n+    ('celery_cmd', 'custom_cmd'),\n+    [\n+        ('inspect', ('custom_inspect_cmd', '123')),\n+        ('control', ('custom_control_cmd', '123', '456')),\n+    ],\n+)\n+def test_custom_remote_command(celery_cmd, custom_cmd, isolated_cli_runner: CliRunner):\n+    res = isolated_cli_runner.invoke(\n+        celery,\n+        [*_GLOBAL_OPTIONS, celery_cmd, *_INSPECT_OPTIONS, *custom_cmd],\n+        catch_exceptions=False,\n+    )\n+    assert res.exit_code == EX_UNAVAILABLE, (res, res.stdout)\n+    assert res.stdout.strip() == 'Error: No nodes replied within time constraint'\n+\n+\n+@pytest.mark.parametrize(\n+    ('celery_cmd', 'remote_cmd'),\n+    [\n+        # Test nonexistent commands\n+        ('inspect', 'this_command_does_not_exist'),\n+        ('control', 'this_command_does_not_exist'),\n+        # Test commands that exist, but are of the wrong type\n+        ('inspect', 'custom_control_cmd'),\n+        ('control', 'custom_inspect_cmd'),\n+    ],\n+)\n+def test_unrecognized_remote_command(celery_cmd, remote_cmd, isolated_cli_runner: CliRunner):\n+    res = isolated_cli_runner.invoke(\n+        celery,\n+        [*_GLOBAL_OPTIONS, celery_cmd, *_INSPECT_OPTIONS, remote_cmd],\n+        catch_exceptions=False,\n+    )\n+    assert res.exit_code == 2, (res, res.stdout)\n+    assert f'Error: Command {remote_cmd} not recognized. Available {celery_cmd} commands: ' in res.stdout\n+\n+\n+_expected_inspect_regex = (\n+    '\\n  custom_inspect_cmd x\\\\s+Ask the workers to reply with x\\\\.\\n'\n+)\n+_expected_control_regex = (\n+    '\\n  custom_control_cmd a b\\\\s+Ask the workers to reply with a and b\\\\.\\n'\n+)\n+\n+\n+@pytest.mark.parametrize(\n+    ('celery_cmd', 'expected_regex'),\n+    [\n+        ('inspect', re.compile(_expected_inspect_regex, re.MULTILINE)),\n+        ('control', re.compile(_expected_control_regex, re.MULTILINE)),\n+    ],\n+)\n+def test_listing_remote_commands(celery_cmd, expected_regex, isolated_cli_runner: CliRunner):\n+    res = isolated_cli_runner.invoke(\n+        celery,\n+        [*_GLOBAL_OPTIONS, celery_cmd, '--list'],\n+    )\n+    assert res.exit_code == 0, (res, res.stdout)\n+    assert expected_regex.search(res.stdout)\n",
        "problem_statement": "Celery 5 custom inspect commands doesn't work in the CLI\n<!--\nPlease fill this template entirely and do not erase parts of it.\nWe reserve the right to close without a response\nbug reports which are incomplete.\n-->\n\n\n# Checklist\n<!--\nTo check an item on the list replace [ ] with [x].\n-->\n\n\n* [x] I have verified that the issue exists against the `master` branch of Celery.\n* [x] This has already been asked to the [discussion group](https://groups.google.com/forum/#!forum/celery-users) first.\n* [x] I have read the relevant section in the\n  [contribution guide](http://docs.celeryproject.org/en/latest/contributing.html#other-bugs)\n  on reporting bugs.\n* [x] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Bug+Report%22+-label%3A%22Category%3A+Documentation%22)\n  for similar or identical bug reports.\n* [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22PR+Type%3A+Bugfix%22+-label%3A%22Category%3A+Documentation%22)\n  for existing proposed fixes.\n* [x] I have checked the [commit log](https://github.com/celery/celery/commits/master)\n  to find out if the bug was already fixed in the master branch.\n* [x] I have included all related issues and possible duplicate issues\n  in this issue (If there are none, check this box anyway).\n\n## Mandatory Debugging Information\n* [ ] I have included the output of ``celery -A proj report`` in the issue.\n    (if you are not able to do this, then at least specify the Celery\n     version affected).\n* [ ] I have verified that the issue exists against the `master` branch of Celery.\n* [ ] I have included the contents of ``pip freeze`` in the issue.\n* [ ] I have included all the versions of all the external dependencies required\n  to reproduce this bug.\n\n## Optional Debugging Information\n<!--\nTry some of the below if you think they are relevant.\nIt will help us figure out the scope of the bug and how many users it affects.\n-->\n\n\n* [ ] I have tried reproducing the issue on more than one Python version\n  and/or implementation.\n* [ ] I have tried reproducing the issue on more than one message broker and/or\n  result backend.\n* [ ] I have tried reproducing the issue on more than one version of the message\n  broker and/or result backend.\n* [ ] I have tried reproducing the issue on more than one operating system.\n* [ ] I have tried reproducing the issue on more than one workers pool.\n* [ ] I have tried reproducing the issue with autoscaling, retries,\n  ETA/Countdown & rate limits disabled.\n* [ ] I have tried reproducing the issue after downgrading\n  and/or upgrading Celery and its dependencies.\n\n## Related Issues and Possible Duplicates\n<!--\nPlease make sure to search and mention any related issues\nor possible duplicates to this issue as requested by the checklist above.\n\nThis may or may not include issues in other repositories that the Celery project\nmaintains or other repositories that are dependencies of Celery.\n\nIf you don't know how to mention issues, please refer to Github's documentation\non the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests\n-->\n\n#### Related Issues\n* None\n\n#### Possible Duplicates\n* None\n\n## Environment & Settings\n<!-- Include the contents of celery --version below -->\n**Celery version**:\n<!-- Include the output of celery -A proj report below -->\n<details>\n<summary><b><code>celery report</code> Output:</b></summary>\n<p>\nsoftware -> celery:5.0.5 (singularity) kombu:5.0.2 py:3.7.5\n            billiard:3.6.3.0 py-amqp:5.0.3\nplatform -> system:Linux arch:64bit, ELF\n            kernel version:4.4.0-19041-Microsoft imp:CPython\nloader   -> celery.loaders.default.Loader\nsettings -> transport:amqp results:disabled\n\ndeprecated_settings: None\n\n\n```\n\n```\n\n</p>\n</details>\n\n# Steps to Reproduce\n## Required Dependencies\n<!-- Please fill the required dependencies to reproduce this issue -->\n\n\n* **Minimal Python Version**: N/A or Unknown\n* **Minimal Celery Version**: 5\n* **Minimal Kombu Version**: N/A or Unknown\n* **Minimal Broker Version**: N/A or Unknown\n* **Minimal Result Backend Version**: N/A or Unknown\n* **Minimal OS and/or Kernel Version**: N/A or Unknown\n* **Minimal Broker Client Version**: N/A or Unknown\n* **Minimal Result Backend Client Version**: N/A or Unknown\n\n### Python Packages\n<!-- Please fill the contents of pip freeze below -->\n<details>\n<summary><b><code>pip freeze</code> Output:</b></summary>\n\nabsl-py==0.9.0\naiohttp==3.6.2\naiomisc==11.0.0\namqp==5.0.3\nappdirs==1.4.4\nasgiref==3.2.10\nasn1crypto==0.24.0\nastor==0.8.1\nasync-timeout==3.0.1\natomicwrites==1.3.0\nattrs==19.3.0\nauth0-python==3.9.1\naws2-wrap==1.1.4\nawscli==1.14.44\nBabel==2.8.0\nbackcall==0.1.0\nbilliard==3.6.3.0\nboto3==1.14.19\nboto3-stubs==1.14.19.0\nbotocore==1.17.63\nBrotli==1.0.9\ncachetools==4.0.0\ncelery==5.0.5\ncertifi==2019.11.28\ncffi==1.14.0\nchardet==3.0.4\nclick==7.1.2\nclick-didyoumean==0.0.3\nclick-plugins==1.1.1\nclick-repl==0.1.6\ncolorama==0.3.7\ncolorlog==4.4.0\nconvertdate==2.2.2\ncryptography==2.7\nddtrace==0.44.0\ndecorator==4.4.2\ndefusedxml==0.6.0\nDjango==3.1.2\ndjango-celery-results==1.0.4\ndjango-cors-headers==3.1.0\ndjango-debug-toolbar==3.1.1\ndjango-filter==2.1.0\ndjango-prometheus==1.0.15\ndjango-rest-auth==0.9.5\ndjango-rest-framework-condition==0.1.1\ndjangorestframework==3.12.1\ndjangorestframework-jwt==1.11.0\ndocutils==0.15.2\necdsa==0.15\nffmpeg-python==0.2.0\nfitparse==1.1.0\nflower==0.9.3\nfusepy==3.0.1\nfuture==0.18.2\ngast==0.2.2\ngitdb==4.0.2\ngitdb2==3.0.0\nGitPython==3.1.0\nglfw==1.8.2\ngoogle-auth==1.11.3\ngoogle-auth-oauthlib==0.4.1\ngoogle-pasta==0.2.0\ngprof2dot==2019.11.30\ngraphqlclient==0.2.4\ngrpcio==1.27.2\nh5py==2.10.0\nholidays==0.10.1\nidna==2.6\nimageio-ffmpeg==0.3.0\nimportlib-metadata==1.5.0\nimutils==0.5.2\nintervaltree==3.1.0\nipython==7.6.1\nipython-genutils==0.2.0\njedi==0.16.0\njmespath==0.9.5\njoblib==0.14.1\nKeras-Applications==1.0.8\nKeras-Preprocessing==1.1.0\nkeyring==10.6.0\nkeyrings.alt==3.0\nkombu==5.0.2\nkubernetes==12.0.0\nlogzio-python-handler==2.0.13\nlxml==4.4.2\nMarkdown==3.2.1\nmemory-profiler==0.57.0\nmore-itertools==8.2.0\nmultidict==4.7.5\nmunkres==1.1.2\nmypy-boto3==1.14.19.0\nmypy-boto3-cloudformation==1.14.19.0\nmypy-boto3-dynamodb==1.14.19.0\nmypy-boto3-ec2==1.14.19.0\nmypy-boto3-lambda==1.14.19.0\nmypy-boto3-rds==1.14.19.0\nmypy-boto3-s3==1.14.19.0\nmypy-boto3-sqs==1.14.19.0\nnumpy==1.17.0\noauthlib==3.1.0\nolefile==0.45.1\nopencv-python==3.4.5.20\nopt-einsum==3.2.0\npackaging==20.3\npandas==1.1.4\nparso==0.6.2\npdfkit==0.6.1\npexpect==4.8.0\npickleshare==0.7.5\nPillow==7.1.2\npipdeptree==1.0.0\npluggy==0.13.1\npprofile==2.0.2\nprometheus-client==0.7.1\nprompt-toolkit==2.0.10\nprotobuf==3.11.3\npsutil==5.7.0\npsycopg2-binary==2.8.3\nptyprocess==0.6.0\npy==1.8.1\npyasn1==0.4.8\npyasn1-modules==0.2.8\npycparser==2.20\npycrypto==2.6.1\npyee==7.0.4\nPygments==2.6.1\npygobject==3.26.1\nPyJWT==1.7.1\nPyMeeus==0.3.7\nPyOpenGL==3.1.0\npyparsing==2.4.6\npyperclip==1.7.0\npyppeteer==0.2.2\npyquaternion==0.9.5\nPySocks==1.7.1\npytest==4.6.5\npytest-django==3.10.0\npytest-django-ordering==1.2.0\npytest-profiling==1.7.0\npython-apt==1.6.5+ubuntu0.2\npython-dateutil==2.8.0\npython-dotenv==0.10.3\npython-http-client==3.2.6\npython-jose==3.0.1\npython-json-logger==0.1.11\npython-memcached==1.59\npython3-openid==3.1.0\npytz==2019.3\npyxdg==0.25\nPyYAML==5.3.1\nredis==3.3.11\nrequests==2.22.0\nrequests-oauthlib==1.3.0\nretry==0.9.2\nroman==2.0.0\nrsa==4.0\ns3transfer==0.3.3\nscikit-fmm==2019.1.30\nscikit-learn==0.23.1\nscipy==1.3.1\nSecretStorage==2.3.1\nsendgrid==6.4.1\nShapely==1.6.4.post2\nsix==1.12.0\nsmmap==3.0.1\nsmmap2==3.0.1\nsocial-auth-app-django==3.1.0\nsocial-auth-core==3.2.0\nsortedcontainers==2.3.0\nsqlparse==0.3.1\nstarkbank-ecdsa==1.1.0\ntenacity==6.3.1\ntensorboard==2.0.2\ntensorflow==2.0.0\ntensorflow-estimator==2.0.1\ntermcolor==1.1.0\nthreadpoolctl==2.1.0\ntornado==5.1.1\ntqdm==4.56.0\ntraitlets==4.3.3\ntripy==1.0.0\ntwilio==6.29.3\ntyping-extensions==3.7.4.3\nunattended-upgrades==0.1\nurllib3==1.25.8\nuWSGI==2.0.18\nvine==5.0.0\nwcwidth==0.1.8\nwebsocket-client==0.57.0\nwebsockets==8.1\nWerkzeug==1.0.0\nwrapt==1.12.1\nxgboost==0.90\nxlrd==1.2.0\nXlsxWriter==1.2.8\nxxhash==1.3.0\nyappi==1.0\nyarl==1.4.2\nzipp==3.1.0\n<p>\n\n```\n\n```\n\n</p>\n</details>\n\n### Other Dependencies\n<!--\nPlease provide system dependencies, configuration files\nand other dependency information if applicable\n-->\n<details>\n<p>\nN/A\n</p>\n</details>\n\n## Minimally Reproducible Test Case\n<!--\nPlease provide a reproducible test case.\nRefer to the Reporting Bugs section in our contribution guide.\n\nWe prefer submitting test cases in the form of a PR to our integration test suite.\nIf you can provide one, please mention the PR number below.\nIf not, please attach the most minimal code example required to reproduce the issue below.\nIf the test case is too large, please include a link to a gist or a repository below.\n-->\n\n<details>\n<p>\n\n```python\n\n```\n\n</p>\n</details>\n\n# Expected Behavior\n<!-- Describe in detail what you expect to happen -->\n\n# Actual Behavior\n<!--\nDescribe in detail what actually happened.\nPlease include a backtrace and surround it with triple backticks (```).\nIn addition, include the Celery daemon logs, the broker logs,\nthe result backend logs and system logs below if they will help us debug\nthe issue.\n-->\n\nHi, I think there is a bug with version 5+.\nWhen using the guide in the docs for writing custom control commands and trying to run it from the CLI, it fails. When running this command from a python script, it is working (with broadcast). When using celery 4.4.3 on the same code, it also works.\nThe guide: https://docs.celeryproject.org/en/stable/userguide/workers.html#writing-your-own-remote-control-commands\nthe error:\n$ celery -A route inspect current_prefetch_count\nUsage: celery inspect [OPTIONS] [report|conf|query_task|clock|ping|stats|sched\nuled|reserved|active|revoked|registered|objgraph|memsamp\nle|memdump|active_queues]\nTry 'celery inspect --help' for help.\n\nError: Invalid value for '[report|conf|query_task|clock|ping|stats|scheduled|reserved|active|revoked|registered|objgraph|memsample|memdump|active_queues]': invalid choice: current_prefetch_count. (choose from report, conf, query_task, clock, ping, stats, scheduled, rese\nrved, active, revoked, registered, objgraph, memsample, memdump, active_queues)\n\n\n",
        "hints_text": "what you get when you run celery inspect --help\nUsage: celery inspect [OPTIONS] [report|conf|query_task|clock|ping|stats|sched\r\n                      uled|reserved|active|revoked|registered|objgraph|memsamp\r\n                      le|memdump|active_queues]\r\n\r\n  Inspect the worker at runtime.\r\n\r\n  Availability: RabbitMQ (AMQP) and Redis transports.\r\n\r\nRemote Control Options:\r\n  -t, --timeout FLOAT             Timeout in seconds waiting for reply.\r\n  -d, --destination COMMA SEPARATED LIST\r\n                                  Comma separated list of destination node\r\n                                  names.\r\n\r\n  -j, --json                      Use json as output format.\r\n\r\nOptions:\r\n  --help  Show this message and exit.\nmay be @thedrow can share some insight\nI have a monitoring script that (among many things) uses inspection API and calls active_queues(). With Celery 5.1.2 the scripts fails to work with key error, as active_queues() returns nothing... 4.4.x works as expected...\nThe code I used to test is here: https://gitlab.com/dejan/ceex (here I keep all my CElery EXperiments). If you run `celery -A ceex.inspect_node_stats worker -l debug` and try to `celery -A ceex.inspect_node_stats inspect node_stats` with 5.1.2 you will see it does not work. Again, try with 4.4.7 - it does work.\nOops, thanks - I did not know it was created as private repo... It is open to public now.\nAny update on this?\nLooking the code of the control.py \r\n\r\n```\r\n@click.argument(\"action\", type=click.Choice([\r\n    name for name, info in Panel.meta.items()\r\n    if info.type == 'control' and info.visible\r\n]))\r\n```\r\n\r\nthis above decorator doesn't read the global Meta properly. I'm still trying to figure out why. \nI encountered this bug today. This bug is caused by the fact that `Panel.meta.items()` is evaluated when the `bin/control.py` file is imported. This happens _after_ the command definitions in `worker/control.py`, but _before_ any of my custom commands are defined. All the modules in `bin/` are imported when the `celery` command is run, so I don't see any quickfix or workaround.\r\n\r\nI guess you could move away from `click.Choice` and accept any string as the command name instead. We would then need to validate the given string manually \u2013 the user's Celery app will be set up by the time we reach the function body, so their custom commands will be registered by then. Maybe a new flag, e.g. `--list`, could print the available commands and exit.\r\n\r\nWould this be an acceptable solution? I may try to implement it if desired.\nThis bug is basically preventing us from moving to new(er) Celery as we have few critical inspect and control commands that we use all the time.\n> I encountered this bug today. This bug is caused by the fact that `Panel.meta.items()` is evaluated when the `bin/control.py` file is imported. This happens _after_ the command definitions in `worker/control.py`, but _before_ any of my custom commands are defined. All the modules in `bin/` are imported when the `celery` command is run, so I don't see any quickfix or workaround.\r\n> \r\n> I guess you could move away from `click.Choice` and accept any string as the command name instead. We would then need to validate the given string manually \u2013 the user's Celery app will be set up by the time we reach the function body, so their custom commands will be registered by then. Maybe a new flag, e.g. `--list`, could print the available commands and exit.\r\n> \r\n> Would this be an acceptable solution? I may try to implement it if desired.\r\n\r\nyou can come with a draft proof of concept PR with relevant test for review",
        "created_at": "2023-09-07T13:46:57Z",
        "version": "5.3",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "t/unit/app/test_preload_cli.py"
        ]
    },
    {
        "repo": "celery/celery",
        "pull_number": 8486,
        "instance_id": "celery__celery-8486",
        "issue_numbers": [
            "7715",
            "8472"
        ],
        "base_commit": "14892abbb8cf80d7abcf41f4a48c049d84f69f74",
        "patch": "diff --git a/celery/app/task.py b/celery/app/task.py\nindex 7998d600b76..22d9a8ebd0c 100644\n--- a/celery/app/task.py\n+++ b/celery/app/task.py\n@@ -788,6 +788,7 @@ def apply(self, args=None, kwargs=None,\n \n         request = {\n             'id': task_id,\n+            'task': self.name,\n             'retries': retries,\n             'is_eager': True,\n             'logfile': logfile,\n@@ -824,7 +825,7 @@ def apply(self, args=None, kwargs=None,\n         if isinstance(retval, Retry) and retval.sig is not None:\n             return retval.sig.apply(retries=retries + 1)\n         state = states.SUCCESS if ret.info is None else ret.info.state\n-        return EagerResult(task_id, retval, state, traceback=tb)\n+        return EagerResult(task_id, retval, state, traceback=tb, name=self.name)\n \n     def AsyncResult(self, task_id, **kwargs):\n         \"\"\"Get AsyncResult instance for the specified task.\ndiff --git a/celery/result.py b/celery/result.py\nindex 0c9e0a30f21..c566a2573b7 100644\n--- a/celery/result.py\n+++ b/celery/result.py\n@@ -983,13 +983,14 @@ def restore(cls, id, backend=None, app=None):\n class EagerResult(AsyncResult):\n     \"\"\"Result that we know has already been executed.\"\"\"\n \n-    def __init__(self, id, ret_value, state, traceback=None):\n+    def __init__(self, id, ret_value, state, traceback=None, name=None):\n         # pylint: disable=super-init-not-called\n         # XXX should really not be inheriting from AsyncResult\n         self.id = id\n         self._result = ret_value\n         self._state = state\n         self._traceback = traceback\n+        self._name = name\n         self.on_ready = promise()\n         self.on_ready(self)\n \n@@ -1042,6 +1043,7 @@ def _cache(self):\n             'result': self._result,\n             'status': self._state,\n             'traceback': self._traceback,\n+            'name': self._name,\n         }\n \n     @property\n",
        "test_patch": "diff --git a/t/unit/tasks/test_result.py b/t/unit/tasks/test_result.py\nindex 42eaab8987d..30e0b9ef134 100644\n--- a/t/unit/tasks/test_result.py\n+++ b/t/unit/tasks/test_result.py\n@@ -967,6 +967,13 @@ def test_get_sync_subtask_option(self, task_join_will_block):\n             res_subtask_async.get()\n         res_subtask_async.get(disable_sync_subtasks=False)\n \n+    def test_populate_name(self):\n+        res = EagerResult('x', 'x', states.SUCCESS, None, 'test_task')\n+        assert res.name == 'test_task'\n+\n+        res = EagerResult('x', 'x', states.SUCCESS, name='test_task_named_argument')\n+        assert res.name == 'test_task_named_argument'\n+\n \n class test_tuples:\n \ndiff --git a/t/unit/tasks/test_tasks.py b/t/unit/tasks/test_tasks.py\nindex 36bb792b16d..5a0cd7c2f19 100644\n--- a/t/unit/tasks/test_tasks.py\n+++ b/t/unit/tasks/test_tasks.py\n@@ -1432,6 +1432,7 @@ def test_apply(self):\n \n         assert e.successful()\n         assert e.ready()\n+        assert e.name == 't.unit.tasks.test_tasks.increment_counter'\n         assert repr(e).startswith('<EagerResult:')\n \n         f = self.raising.apply()\n@@ -1441,6 +1442,21 @@ def test_apply(self):\n         with pytest.raises(KeyError):\n             f.get()\n \n+    def test_apply_eager_populates_request_task(self):\n+        task_to_apply = self.task_check_request_context\n+        with patch.object(\n+            task_to_apply.request_stack, \"push\",\n+            wraps=task_to_apply.request_stack.push,\n+        ) as mock_push:\n+            task_to_apply.apply()\n+\n+        mock_push.assert_called_once()\n+\n+        request = mock_push.call_args[0][0]\n+\n+        assert request.is_eager is True\n+        assert request.task == 't.unit.tasks.test_tasks.task_check_request_context'\n+\n     def test_apply_simulates_delivery_info(self):\n         task_to_apply = self.task_check_request_context\n         with patch.object(\n",
        "problem_statement": "EagerResult doesn't seem to poplate name\nSometimes I run my tasks with `CELERY_TASK_ALWAYS_EAGER` to aid debugging, it seems like the `name` property of EagerResult isn't populated, which makes this sort of investigation more tricky.\r\n\r\nI have some code I use to list my tasks in a django app, and part of this is to grab the task name.\r\n\r\nWhen running eagerly, the EagerResult task status is SUCCESS, so I would have expected `name` to be available at this point.\n5.3.2/3 has BREAKING change on EagerResult\n<!--\r\nPlease fill this template entirely and do not erase parts of it.\r\nWe reserve the right to close without a response\r\nbug reports which are incomplete.\r\n-->\r\n# Checklist\r\n<!--\r\nTo check an item on the list replace [ ] with [x].\r\n-->\r\n- [x] I have verified that the issue exists against the `main` branch of Celery.\r\n- [ ] This has already been asked to the [discussions forum](https://github.com/celery/celery/discussions) first.\r\n- [ ] I have read the relevant section in the\r\n  [contribution guide](https://docs.celeryq.dev/en/main/contributing.html#other-bugs)\r\n  on reporting bugs.\r\n- [x] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Bug+Report%22+-label%3A%22Category%3A+Documentation%22)\r\n  for similar or identical bug reports.\r\n- [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22PR+Type%3A+Bugfix%22+-label%3A%22Category%3A+Documentation%22)\r\n  for existing proposed fixes.\r\n- [x] I have checked the [commit log](https://github.com/celery/celery/commits/main)\r\n  to find out if the bug was already fixed in the main branch.\r\n- [ ] I have included all related issues and possible duplicate issues\r\n  in this issue (If there are none, check this box anyway).\r\n\r\n## Mandatory Debugging Information\r\n\r\n- [ ] I have included the output of ``celery -A proj report`` in the issue.\r\n    (if you are not able to do this, then at least specify the Celery\r\n     version affected).\r\n- [x] I have verified that the issue exists against the `main` branch of Celery.\r\n- [ ] I have included the contents of ``pip freeze`` in the issue.\r\n- [ ] I have included all the versions of all the external dependencies required\r\n  to reproduce this bug.\r\n\r\n## Optional Debugging Information\r\n<!--\r\nTry some of the below if you think they are relevant.\r\nIt will help us figure out the scope of the bug and how many users it affects.\r\n-->\r\n- [ ] I have tried reproducing the issue on more than one Python version\r\n  and/or implementation.\r\n- [ ] I have tried reproducing the issue on more than one message broker and/or\r\n  result backend.\r\n- [ ] I have tried reproducing the issue on more than one version of the message\r\n  broker and/or result backend.\r\n- [ ] I have tried reproducing the issue on more than one operating system.\r\n- [ ] I have tried reproducing the issue on more than one workers pool.\r\n- [ ] I have tried reproducing the issue with autoscaling, retries,\r\n  ETA/Countdown & rate limits disabled.\r\n- [x] I have tried reproducing the issue after downgrading\r\n  and/or upgrading Celery and its dependencies.\r\n\r\n## Related Issues and Possible Duplicates\r\n<!--\r\nPlease make sure to search and mention any related issues\r\nor possible duplicates to this issue as requested by the checklist above.\r\n\r\nThis may or may not include issues in other repositories that the Celery project\r\nmaintains or other repositories that are dependencies of Celery.\r\n\r\nIf you don't know how to mention issues, please refer to Github's documentation\r\non the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests\r\n-->\r\n\r\n#### Related Issues\r\n\r\n- None\r\n\r\n#### Possible Duplicates\r\n\r\n- None\r\n\r\n## Environment & Settings\r\n<!-- Include the contents of celery --version below -->\r\n**Celery version**: 5.3.3\r\n<!-- Include the output of celery -A proj report below -->\r\n<details>\r\n<summary><b><code>celery report</code> Output:</b></summary>\r\n<p>\r\n\r\n```\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n# Steps to Reproduce\r\n\r\n## Required Dependencies\r\n<!-- Please fill the required dependencies to reproduce this issue -->\r\n- **Minimal Python Version**: N/A or Unknown\r\n- **Minimal Celery Version**: N/A or Unknown\r\n- **Minimal Kombu Version**: N/A or Unknown\r\n- **Minimal Broker Version**: N/A or Unknown\r\n- **Minimal Result Backend Version**: N/A or Unknown\r\n- **Minimal OS and/or Kernel Version**: N/A or Unknown\r\n- **Minimal Broker Client Version**: N/A or Unknown\r\n- **Minimal Result Backend Client Version**: N/A or Unknown\r\n\r\n### Python Packages\r\n<!-- Please fill the contents of pip freeze below -->\r\n<details>\r\n<summary><b><code>pip freeze</code> Output:</b></summary>\r\n<p>\r\n\r\n```\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n### Other Dependencies\r\n<!--\r\nPlease provide system dependencies, configuration files\r\nand other dependency information if applicable\r\n-->\r\n<details>\r\n<p>\r\nN/A\r\n</p>\r\n</details>\r\n\r\n## Minimally Reproducible Test Case\r\n<!--\r\nPlease provide a reproducible test case.\r\nRefer to the Reporting Bugs section in our contribution guide.\r\n\r\nWe prefer submitting test cases in the form of a PR to our integration test suite.\r\nIf you can provide one, please mention the PR number below.\r\nIf not, please attach the most minimal code example required to reproduce the issue below.\r\nIf the test case is too large, please include a link to a gist or a repository below.\r\n-->\r\n\r\n<details>\r\n<p>\r\n\r\n```python\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n# Expected Behavior\r\n<!-- Describe in detail what you expect to happen -->\r\na patch release to not have a breaking change\r\n\r\n# Actual Behavior\r\n<!--\r\nDescribe in detail what actually happened.\r\nPlease include a backtrace and surround it with triple backticks (```).\r\nIn addition, include the Celery daemon logs, the broker logs,\r\nthe result backend logs and system logs below if they will help us debug\r\nthe issue.\r\n-->\r\nhttps://github.com/celery/celery/commit/1c363876147325a196c474e757e355c451a0cdff#diff-05689f277021b9af9d8314849c9d938db0f5a42e932169a116463ef91ae9af78R986\r\n\r\n```\r\nTypeError\r\nEagerResult.__init__() missing 1 required positional argument: 'state'\r\n```\n",
        "hints_text": "Hey @stuaxo :wave:,\nThank you for opening an issue. We will get back to you as soon as we can.\nAlso, check out our [Open Collective](https://opencollective.com/celery) and consider backing us - every little helps!\n\nWe also offer priority support for our sponsors.\nIf you require immediate assistance please consider sponsoring us.\n\ndo you have any failing test/implementation detail in mind to share?\nI'll see what I can find, I'm no longer working at the organisation* I did that project for, but it is open source so I still have that work and can share it :)\r\n\r\n*I'm a contractor and move from place to place.\nBTW, I am a contractor too\nI really wish that every contract had a budget to put towards open source projects they use - I did raise this @ the last place, but probably not in the right places.\r\n\r\nThe furthest I've got is submitting patches to projects used.\n",
        "created_at": "2023-09-04T22:38:21Z",
        "version": "5.3",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "t/unit/tasks/test_result.py",
            "t/unit/tasks/test_tasks.py"
        ]
    },
    {
        "repo": "celery/celery",
        "pull_number": 8462,
        "instance_id": "celery__celery-8462",
        "issue_numbers": [
            "8461"
        ],
        "base_commit": "8ae0b229596cc8aeea4fb71020d9358a59338e08",
        "patch": "diff --git a/celery/bin/celery.py b/celery/bin/celery.py\nindex 15558813b0b..4aeed42597f 100644\n--- a/celery/bin/celery.py\n+++ b/celery/bin/celery.py\n@@ -136,7 +136,8 @@ def convert(self, value, param, ctx):\n               cls=CeleryOption,\n               is_flag=True,\n               help_group=\"Global Options\",\n-              help=\"Skip Django core checks on startup.\")\n+              help=\"Skip Django core checks on startup. Setting the SKIP_CHECKS environment \"\n+                   \"variable to any non-empty string will have the same effect.\")\n @click.pass_context\n def celery(ctx, app, broker, result_backend, loader, config, workdir,\n            no_color, quiet, version, skip_checks):\n@@ -158,7 +159,7 @@ def celery(ctx, app, broker, result_backend, loader, config, workdir,\n     if config:\n         os.environ['CELERY_CONFIG_MODULE'] = config\n     if skip_checks:\n-        os.environ['CELERY_SKIP_CHECKS'] = skip_checks\n+        os.environ['CELERY_SKIP_CHECKS'] = 'true'\n     ctx.obj = CLIContext(app=app, no_color=no_color, workdir=workdir,\n                          quiet=quiet)\n \n",
        "test_patch": "diff --git a/t/unit/bin/test_worker.py b/t/unit/bin/test_worker.py\nindex 50a07e3b674..b63a2a03306 100644\n--- a/t/unit/bin/test_worker.py\n+++ b/t/unit/bin/test_worker.py\n@@ -1,3 +1,6 @@\n+import os\n+from unittest.mock import patch\n+\n import pytest\n from click.testing import CliRunner\n \n@@ -18,3 +21,15 @@ def test_cli(isolated_cli_runner: CliRunner):\n         catch_exceptions=False\n     )\n     assert res.exit_code == 1, (res, res.stdout)\n+\n+\n+def test_cli_skip_checks(isolated_cli_runner: CliRunner):\n+    Logging._setup = True  # To avoid hitting the logging sanity checks\n+    with patch.dict(os.environ, clear=True):\n+        res = isolated_cli_runner.invoke(\n+            celery,\n+            [\"-A\", \"t.unit.bin.proj.app\", \"--skip-checks\", \"worker\", \"--pool\", \"solo\"],\n+            catch_exceptions=False,\n+        )\n+        assert res.exit_code == 1, (res, res.stdout)\n+        assert os.environ[\"CELERY_SKIP_CHECKS\"] == \"true\", \"should set CELERY_SKIP_CHECKS\"\ndiff --git a/t/unit/fixups/test_django.py b/t/unit/fixups/test_django.py\nindex 07f94c6b813..8a97884ed4a 100644\n--- a/t/unit/fixups/test_django.py\n+++ b/t/unit/fixups/test_django.py\n@@ -272,7 +272,7 @@ def test_validate_models(self, patching, module):\n         f.django_setup.reset_mock()\n         run_checks.reset_mock()\n \n-        patching.setenv('CELERY_SKIP_CHECKS', True)\n+        patching.setenv('CELERY_SKIP_CHECKS', 'true')\n         f.validate_models()\n         f.django_setup.assert_called_with()\n         run_checks.assert_not_called()\n",
        "problem_statement": "Running worker with --skip-checks option fails with a TypeError\n<!--\r\nPlease fill this template entirely and do not erase parts of it.\r\nWe reserve the right to close without a response\r\nbug reports which are incomplete.\r\n-->\r\n# Checklist\r\n<!--\r\nTo check an item on the list replace [ ] with [x].\r\n-->\r\n- [x] I have verified that the issue exists against the `main` branch of Celery.\r\n- [ ] This has already been asked to the [discussions forum](https://github.com/celery/celery/discussions) first.\r\n- [x] I have read the relevant section in the\r\n  [contribution guide](https://docs.celeryq.dev/en/main/contributing.html#other-bugs)\r\n  on reporting bugs.\r\n- [x] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Bug+Report%22+-label%3A%22Category%3A+Documentation%22)\r\n  for similar or identical bug reports.\r\n- [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22PR+Type%3A+Bugfix%22+-label%3A%22Category%3A+Documentation%22)\r\n  for existing proposed fixes.\r\n- [x] I have checked the [commit log](https://github.com/celery/celery/commits/main)\r\n  to find out if the bug was already fixed in the main branch.\r\n- [x] I have included all related issues and possible duplicate issues\r\n  in this issue (If there are none, check this box anyway).\r\n\r\n## Mandatory Debugging Information\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n    (if you are not able to do this, then at least specify the Celery\r\n     version affected).\r\n- [x] I have verified that the issue exists against the `main` branch of Celery.\r\n- [x] I have included the contents of ``pip freeze`` in the issue.\r\n- [x] I have included all the versions of all the external dependencies required\r\n  to reproduce this bug.\r\n\r\n## Optional Debugging Information\r\n<!--\r\nTry some of the below if you think they are relevant.\r\nIt will help us figure out the scope of the bug and how many users it affects.\r\n-->\r\n- [x] I have tried reproducing the issue on more than one Python version\r\n  and/or implementation.\r\n- [ ] I have tried reproducing the issue on more than one message broker and/or\r\n  result backend.\r\n- [ ] I have tried reproducing the issue on more than one version of the message\r\n  broker and/or result backend.\r\n- [ ] I have tried reproducing the issue on more than one operating system.\r\n- [ ] I have tried reproducing the issue on more than one workers pool.\r\n- [ ] I have tried reproducing the issue with autoscaling, retries,\r\n  ETA/Countdown & rate limits disabled.\r\n- [ ] I have tried reproducing the issue after downgrading\r\n  and/or upgrading Celery and its dependencies.\r\n\r\n## Related Issues and Possible Duplicates\r\n<!--\r\nPlease make sure to search and mention any related issues\r\nor possible duplicates to this issue as requested by the checklist above.\r\n\r\nThis may or may not include issues in other repositories that the Celery project\r\nmaintains or other repositories that are dependencies of Celery.\r\n\r\nIf you don't know how to mention issues, please refer to Github's documentation\r\non the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests\r\n-->\r\n\r\n#### Related Issues\r\n\r\n- #7581 (feature request that led to the `--skip-checks` option being added)\r\n- #7859 (PR that added the option)\r\n\r\n#### Possible Duplicates\r\n\r\n- None\r\n\r\n## Environment & Settings\r\n<!-- Include the contents of celery --version below -->\r\n**Celery version**: 5.3.1 (emerald-rush)\r\n<!-- Include the output of celery -A proj report below -->\r\n<details>\r\n<summary><b><code>celery report</code> Output:</b></summary>\r\n<p>\r\n\r\nThis is on the Docker dev container. The results are not substantially different from the system on which I encountered the failure, aside from a massive and mostly irrelevant Django settings dump.\r\n\r\n```\r\ndeveloper@dc2346ef89d6:~/celery$ celery --config t.integration.test_worker_config report\r\n\r\nsoftware -> celery:5.3.1 (emerald-rush) kombu:5.3.1 py:3.8.18\r\n            billiard:4.1.0 py-amqp:5.1.1\r\nplatform -> system:Linux arch:64bit\r\n            kernel version:5.15.49-linuxkit-pr imp:CPython\r\nloader   -> celery.loaders.default.Loader\r\nsettings -> transport:amqp results:disabled\r\n\r\nbroker_connection_retry: False\r\nbroker_connection_retry_on_startup: False\r\nbroker_connection_timeout: 0\r\nbroker_url: 'amqp://guest:********@foobar:1234//'\r\nworker_log_color: False\r\nworker_redirect_stdouts: False\r\ndeprecated_settings: None\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n# Steps to Reproduce\r\n\r\nRun a worker with the `--skip-checks` option or with`CELERY_SKIP_CHECKS=1` (or any non-empty value) in the environment.\r\n\r\n## Required Dependencies\r\n<!-- Please fill the required dependencies to reproduce this issue -->\r\n- **Minimal Python Version**: N/A or Unknown\r\n- **Minimal Celery Version**: 5.3.1 (emerald-rush)\r\n- **Minimal Kombu Version**: N/A or Unknown\r\n- **Minimal Broker Version**: N/A or Unknown\r\n- **Minimal Result Backend Version**: N/A or Unknown\r\n- **Minimal OS and/or Kernel Version**: N/A or Unknown\r\n- **Minimal Broker Client Version**: N/A or Unknown\r\n- **Minimal Result Backend Client Version**: N/A or Unknown\r\n\r\n### Python Packages\r\n<!-- Please fill the contents of pip freeze below -->\r\n<details>\r\n<summary><b><code>pip freeze</code> Output:</b></summary>\r\n<p>\r\n\r\n```\r\nalabaster==0.7.13\r\namqp==5.1.1\r\nasync-timeout==4.0.3\r\nattrs==23.1.0\r\nazure-core==1.29.3\r\nazure-storage-blob==12.17.0\r\nBabel==2.12.1\r\nbackports.zoneinfo==0.2.1\r\nbilliard==4.1.0\r\nboto3==1.28.36\r\nbotocore==1.31.36\r\nbump2version==1.0.1\r\nbumpversion==0.6.0\r\ncachetools==5.3.1\r\ncassandra-driver==3.28.0\r\ncelery==5.3.1\r\ncertifi==2023.7.22\r\ncffi==1.15.1\r\ncfgv==3.4.0\r\nchardet==5.2.0\r\ncharset-normalizer==3.2.0\r\nclick==8.1.7\r\nclick-didyoumean==0.3.0\r\nclick-plugins==1.1.1\r\nclick-repl==0.3.0\r\ncolorama==0.4.6\r\ncouchbase==4.1.8\r\ncoverage==7.3.0\r\ncryptography==41.0.3\r\nDateTime==5.2\r\ndistlib==0.3.7\r\ndnspython==2.4.2\r\ndocutils==0.19\r\nelasticsearch==7.17.9\r\nephem==4.1.4\r\neventlet==0.33.3\r\nexceptiongroup==1.1.3\r\nfilelock==3.12.3\r\nflake8==6.1.0\r\nflake8-docstrings==1.7.0\r\nflakeplus==1.1.0\r\nfuture==0.18.3\r\ngeomet==0.2.1.post1\r\ngevent==23.7.0\r\ngreenlet==2.0.2\r\nidentify==2.5.27\r\nidna==3.4\r\nimagesize==1.4.1\r\nimportlib-metadata==6.8.0\r\niniconfig==2.0.0\r\nisodate==0.6.1\r\nisort==5.12.0\r\nJinja2==3.1.2\r\njmespath==1.0.1\r\nkombu==5.3.1\r\nlivereload==2.6.3\r\nMarkupSafe==2.1.3\r\nmccabe==0.7.0\r\nmock==5.1.0\r\nmoto==4.2.0\r\nmsgpack==1.0.5\r\nmypy==1.5.0\r\nmypy-extensions==1.0.0\r\nnodeenv==1.8.0\r\npackaging==23.1\r\nplatformdirs==3.10.0\r\npluggy==1.3.0\r\npre-commit==3.3.3\r\nprompt-toolkit==3.0.39\r\npyArango==2.0.2\r\npycodestyle==2.11.0\r\npycouchdb==1.14.2\r\npycparser==2.21\r\npycurl==7.45.2\r\npydocstyle==6.3.0\r\npydocumentdb==2.3.5\r\npyflakes==3.1.0\r\nPygments==2.16.1\r\npylibmc==1.6.3\r\npymongo==4.5.0\r\npyproject-api==1.5.4\r\npytest==7.4.0\r\npytest-celery==0.0.0\r\npytest-click==1.1.0\r\npytest-cov==4.1.0\r\npytest-github-actions-annotate-failures==0.2.0\r\npytest-order==1.1.0\r\npytest-rerunfailures==12.0\r\npytest-subtests==0.11.0\r\npytest-timeout==2.1.0\r\npython-consul2==0.1.5\r\npython-dateutil==2.8.2\r\npython-memcached==1.59\r\npytz==2023.3\r\nPyYAML==6.0.1\r\nredis==4.6.0\r\nrequests==2.31.0\r\nresponses==0.23.3\r\ns3transfer==0.6.2\r\nsix==1.16.0\r\nsnowballstemmer==2.2.0\r\nsoftlayer-messaging==1.0.3\r\nSphinx==5.3.0\r\nsphinx-autobuild==2021.3.14\r\nsphinx-celery==2.0.0\r\nsphinx-click==4.4.0\r\nsphinx-testing==1.0.1\r\nsphinx2rst==1.1.0\r\nsphinxcontrib-applehelp==1.0.4\r\nsphinxcontrib-devhelp==1.0.2\r\nsphinxcontrib-htmlhelp==2.0.1\r\nsphinxcontrib-jsmath==1.0.1\r\nsphinxcontrib-qthelp==1.0.3\r\nsphinxcontrib-serializinghtml==1.1.5\r\nSQLAlchemy==2.0.20\r\ntblib==2.0.0\r\ntomli==2.0.1\r\ntornado==6.3.3\r\ntox==4.10.0\r\ntypes-PyYAML==6.0.12.11\r\ntyping_extensions==4.7.1\r\ntzdata==2023.3\r\nUnipath==1.1\r\nurllib3==1.26.16\r\nvine==5.0.0\r\nvirtualenv==20.24.3\r\nwcwidth==0.2.6\r\nWerkzeug==2.3.7\r\nxmltodict==0.13.0\r\nzipp==3.16.2\r\nzope.event==5.0\r\nzope.interface==6.0\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n### Other Dependencies\r\n<!--\r\nPlease provide system dependencies, configuration files\r\nand other dependency information if applicable\r\n-->\r\n<details>\r\n<p>\r\nN/A\r\n</p>\r\n</details>\r\n\r\n## Minimally Reproducible Test Case\r\n<!--\r\nPlease provide a reproducible test case.\r\nRefer to the Reporting Bugs section in our contribution guide.\r\n\r\nWe prefer submitting test cases in the form of a PR to our integration test suite.\r\nIf you can provide one, please mention the PR number below.\r\nIf not, please attach the most minimal code example required to reproduce the issue below.\r\nIf the test case is too large, please include a link to a gist or a repository below.\r\n-->\r\n\r\n<details>\r\n<p>\r\nIn a shell opened with `docker compose run --rm celery bash`, the following illustrates the failure.\r\n\r\n```\r\ndeveloper@dc2346ef89d6:~/celery$ celery --skip-checks --config t.integration.test_worker_config report\r\nTraceback (most recent call last):\r\n  File \"/home/developer/.pyenv/versions/3.8.18/bin/celery\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/home/developer/celery/celery/__main__.py\", line 15, in main\r\n    sys.exit(_main())\r\n  File \"/home/developer/celery/celery/bin/celery.py\", line 235, in main\r\n    return celery(auto_envvar_prefix=\"CELERY\")\r\n  File \"/home/developer/.pyenv/versions/3.8.18/lib/python3.8/site-packages/click/core.py\", line 1157, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/home/developer/.pyenv/versions/3.8.18/lib/python3.8/site-packages/click/core.py\", line 1078, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/home/developer/.pyenv/versions/3.8.18/lib/python3.8/site-packages/click/core.py\", line 1685, in invoke\r\n    super().invoke(ctx)\r\n  File \"/home/developer/.pyenv/versions/3.8.18/lib/python3.8/site-packages/click/core.py\", line 1434, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/home/developer/.pyenv/versions/3.8.18/lib/python3.8/site-packages/click/core.py\", line 783, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/home/developer/.pyenv/versions/3.8.18/lib/python3.8/site-packages/click/decorators.py\", line 33, in new_func\r\n    return f(get_current_context(), *args, **kwargs)\r\n  File \"/home/developer/celery/celery/bin/celery.py\", line 161, in celery\r\n    os.environ['CELERY_SKIP_CHECKS'] = skip_checks\r\n  File \"/home/developer/.pyenv/versions/3.8.18/lib/python3.8/os.py\", line 680, in __setitem__\r\n    value = self.encodevalue(value)\r\n  File \"/home/developer/.pyenv/versions/3.8.18/lib/python3.8/os.py\", line 750, in encode\r\n    raise TypeError(\"str expected, not %s\" % type(value).__name__)\r\nTypeError: str expected, not bool\r\n```\r\n\r\nAlternatively, for any non-empty value of `CELERY_SKIP_CHECKS`, same result:\r\n```\r\ndeveloper@dc2346ef89d6:~/celery$ CELERY_SKIP_CHECKS=1 celery --config t.integration.test_worker_config worker\r\nTraceback (most recent call last):\r\n  File \"/home/developer/.pyenv/versions/3.8.18/bin/celery\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/home/developer/celery/celery/__main__.py\", line 15, in main\r\n    sys.exit(_main())\r\n  File \"/home/developer/celery/celery/bin/celery.py\", line 235, in main\r\n    return celery(auto_envvar_prefix=\"CELERY\")\r\n  File \"/home/developer/.pyenv/versions/3.8.18/lib/python3.8/site-packages/click/core.py\", line 1157, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/home/developer/.pyenv/versions/3.8.18/lib/python3.8/site-packages/click/core.py\", line 1078, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/home/developer/.pyenv/versions/3.8.18/lib/python3.8/site-packages/click/core.py\", line 1685, in invoke\r\n    super().invoke(ctx)\r\n  File \"/home/developer/.pyenv/versions/3.8.18/lib/python3.8/site-packages/click/core.py\", line 1434, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/home/developer/.pyenv/versions/3.8.18/lib/python3.8/site-packages/click/core.py\", line 783, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/home/developer/.pyenv/versions/3.8.18/lib/python3.8/site-packages/click/decorators.py\", line 33, in new_func\r\n    return f(get_current_context(), *args, **kwargs)\r\n  File \"/home/developer/celery/celery/bin/celery.py\", line 161, in celery\r\n    os.environ['CELERY_SKIP_CHECKS'] = skip_checks\r\n  File \"/home/developer/.pyenv/versions/3.8.18/lib/python3.8/os.py\", line 680, in __setitem__\r\n    value = self.encodevalue(value)\r\n  File \"/home/developer/.pyenv/versions/3.8.18/lib/python3.8/os.py\", line 750, in encode\r\n    raise TypeError(\"str expected, not %s\" % type(value).__name__)\r\nTypeError: str expected, not bool\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n# Expected Behavior\r\n<!-- Describe in detail what you expect to happen -->\r\nExpected either the `CELERY_SKIP_CHECKS=1` or the `--skip-checks` option to start up the celery worker with Django's initial checks disabled. In the context of the minimal test case, Celery should start up, show its welcome message, then exit with error due to inability to contact the message broker.\r\n\r\n# Actual Behavior\r\n<!--\r\nDescribe in detail what actually happened.\r\nPlease include a backtrace and surround it with triple backticks (```).\r\nIn addition, include the Celery daemon logs, the broker logs,\r\nthe result backend logs and system logs below if they will help us debug\r\nthe issue.\r\n-->\r\nCelery immediately exits with a `TypeError`. See the minimally reproducible test case section for backtraces.\n",
        "hints_text": "",
        "created_at": "2023-08-29T14:07:40Z",
        "version": "5.3",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "t/unit/bin/test_worker.py",
            "t/unit/fixups/test_django.py"
        ]
    },
    {
        "repo": "celery/celery",
        "pull_number": 8432,
        "instance_id": "celery__celery-8432",
        "issue_numbers": [
            "8431",
            "8431"
        ],
        "base_commit": "7b4c4c3938385a994c346f6fa80ce87f4efc0001",
        "patch": "diff --git a/celery/backends/mongodb.py b/celery/backends/mongodb.py\nindex 654ca3710c9..c64fe380807 100644\n--- a/celery/backends/mongodb.py\n+++ b/celery/backends/mongodb.py\n@@ -182,7 +182,8 @@ def _store_result(self, task_id, result, state,\n                       traceback=None, request=None, **kwargs):\n         \"\"\"Store return value and state of an executed task.\"\"\"\n         meta = self._get_result_meta(result=self.encode(result), state=state,\n-                                     traceback=traceback, request=request)\n+                                     traceback=traceback, request=request,\n+                                     format_date=False)\n         # Add the _id for mongodb\n         meta['_id'] = task_id\n \n",
        "test_patch": "diff --git a/t/unit/backends/test_base.py b/t/unit/backends/test_base.py\nindex 1a355d3c3ef..f2ede1503e2 100644\n--- a/t/unit/backends/test_base.py\n+++ b/t/unit/backends/test_base.py\n@@ -176,6 +176,30 @@ def test_get_result_meta_with_none(self):\n         assert meta['kwargs'] == kwargs\n         assert meta['queue'] == 'celery'\n \n+    def test_get_result_meta_format_date(self):\n+        import datetime\n+        self.app.conf.result_extended = True\n+        b1 = BaseBackend(self.app)\n+        args = ['a', 'b']\n+        kwargs = {'foo': 'bar'}\n+\n+        request = Context(args=args, kwargs=kwargs)\n+        meta = b1._get_result_meta(result={'fizz': 'buzz'},\n+                                   state=states.SUCCESS, traceback=None,\n+                                   request=request, format_date=True)\n+        assert isinstance(meta['date_done'], str)\n+\n+        self.app.conf.result_extended = True\n+        b2 = BaseBackend(self.app)\n+        args = ['a', 'b']\n+        kwargs = {'foo': 'bar'}\n+\n+        request = Context(args=args, kwargs=kwargs)\n+        meta = b2._get_result_meta(result={'fizz': 'buzz'},\n+                                   state=states.SUCCESS, traceback=None,\n+                                   request=request, format_date=False)\n+        assert isinstance(meta['date_done'], datetime.datetime)\n+\n \n class test_BaseBackend_interface:\n \n",
        "problem_statement": "Invalid format of 'date_done' field in celery.task_results with backend mongodb\n<!--\r\nPlease fill this template entirely and do not erase parts of it.\r\nWe reserve the right to close without a response\r\nbug reports which are incomplete.\r\n-->\r\n# Checklist\r\n<!--\r\nTo check an item on the list replace [ ] with [x].\r\n-->\r\n- [x] I have verified that the issue exists against the `main` branch of Celery.\r\n- [ ] This has already been asked to the [discussions forum](https://github.com/celery/celery/discussions) first.\r\n- [x] I have read the relevant section in the\r\n  [contribution guide](https://docs.celeryq.dev/en/main/contributing.html#other-bugs)\r\n  on reporting bugs.\r\n- [x] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Bug+Report%22+-label%3A%22Category%3A+Documentation%22)\r\n  for similar or identical bug reports.\r\n- [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22PR+Type%3A+Bugfix%22+-label%3A%22Category%3A+Documentation%22)\r\n  for existing proposed fixes.\r\n- [x] I have checked the [commit log](https://github.com/celery/celery/commits/main)\r\n  to find out if the bug was already fixed in the main branch.\r\n- [x] I have included all related issues and possible duplicate issues\r\n  in this issue (If there are none, check this box anyway).\r\n\r\n## Mandatory Debugging Information\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n    (if you are not able to do this, then at least specify the Celery\r\n     version affected).\r\n- [x] I have verified that the issue exists against the `main` branch of Celery.\r\n- [x] I have included the contents of ``pip freeze`` in the issue.\r\n- [x] I have included all the versions of all the external dependencies required\r\n  to reproduce this bug.\r\n\r\n## Environment & Settings\r\n<!-- Include the contents of celery --version below -->\r\n**Celery version**: 5.3.1\r\n<!-- Include the output of celery -A proj report below -->\r\n<details>\r\n<summary><b><code>celery -A test_celery_result report</code> Output:</b></summary>\r\n<p>\r\n\r\n```\r\nsoftware -> celery:5.3.1 (emerald-rush) kombu:5.3.1 py:3.8.16\r\n            billiard:4.1.0 redis:4.6.0\r\nplatform -> system:Linux arch:64bit, ELF\r\n            kernel version:5.19.0-46-generic imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:redis results:mongodb\r\n[...]\r\nCELERY_BROKER_TRANSPORT_OPTIONS: \r\n 'socket_keepalive': True, 'socket_keepalive_options': {4: 600, 5: 60, 6: 5}}\r\nCELERY_BROKER_URL: 'redis://redis.local:7000/0'\r\nCELERY_INCLUDE: ['test_celery_result.tasks']\r\nCELERY_QUEUE_NAME: 'test_celery_result'\r\nCELERY_REDIS: \r\n 'host': 'redis.local', 'port': 7000}\r\n[...]\r\nis_overridden: <bound method Settings.is_overridden of <Settings \"test_celery_result.settings\">>\r\ndeprecated_settings: None\r\ntask_default_queue: 'test_celery_result'\r\nenable_utc: False\r\nresult_backend: 'mongodb'\r\nresult_expires: datetime.timedelta(seconds=15)\r\nmongodb_backend_settings: \r\n    'database': '********',\r\n    'host': ['mongo-replica'],\r\n    'port': 27017,\r\n    'taskmeta_collection': 'celery_task_result'}\r\nbeat_schedule: \r\n    'celery.backend_cleanup': {   'schedule': 60,\r\n                                  'task': 'celery.backend_cleanup'},\r\n    'dummy_task': {'schedule': 15, 'task': 'dummy_task'}}\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n# Steps to Reproduce\r\n\r\n## Required Dependencies\r\n<!-- Please fill the required dependencies to reproduce this issue -->\r\n- **Minimal Python Version**: 3.6 or higher\r\n- **Minimal Celery Version**: 4.3.0 or higher\r\n- **Minimal Kombu Version**: Unknown\r\n- **Minimal Broker Version**: Unknown\r\n- **Minimal Result Backend Version**: Mongo 4.4 or higher\r\n- **Minimal OS and/or Kernel Version**: Unknown\r\n- **Minimal Broker Client Version**: Unknown\r\n- **Minimal Result Backend Client Version**: pymongo 3.14 or higher\r\n\r\n### Python Packages\r\n<!-- Please fill the contents of pip freeze below -->\r\n<details>\r\n<summary><b><code>pip freeze</code> Output:</b></summary>\r\n<p>\r\n\r\namqp==5.1.1\r\nasgiref==3.7.2\r\nasync-timeout==4.0.2\r\nbackports.zoneinfo==0.2.1\r\nbilliard==4.1.0\r\nbleach==6.0.0\r\ncelery==5.3.1\r\ncertifi==2023.7.22\r\ncffi==1.15.1\r\ncharset-normalizer==3.2.0\r\nclick==8.1.6\r\nclick-didyoumean==0.3.0\r\nclick-plugins==1.1.1\r\nclick-repl==0.3.0\r\ncryptography==41.0.3\r\nDjango==3.2.20\r\ndjango-cors-headers==4.2.0\r\ndjango-environ==0.10.0\r\ndjango-formset-js==0.5.0\r\ndjango-jquery-js==3.1.1\r\ndjango-redis-sessions==0.6.2\r\ndjango-test-addons-adv==1.1.1\r\ndnspython==2.4.1\r\nidna==3.4\r\nJinja2==3.1.2\r\nkombu==5.3.1\r\npackaging==23.1\r\nprompt-toolkit==3.0.39\r\npyasn1==0.5.0\r\npycparser==2.21\r\npyhcl==0.4.4\r\npymongo==4.4.1\r\npython-dateutil==2.8.2\r\npytz==2022.1\r\nPyYAML==6.0.1\r\nredis==4.6.0\r\nrequests==2.31.0\r\nsentinels==1.0.0\r\nsingle-beat==0.6.3\r\nsix==1.16.0\r\nsqlparse==0.4.4\r\ntypes-PyYAML==6.0.12.11\r\ntyping_extensions==4.7.1\r\ntzdata==2023.3\r\nurllib3==1.26.16\r\nvine==5.0.0\r\nwcwidth==0.2.6\r\nwebencodings==0.5.1\r\n\r\n</p>\r\n</details>\r\n\r\n## Minimally Reproducible Test Case\r\n<!--\r\nPlease provide a reproducible test case.\r\nRefer to the Reporting Bugs section in our contribution guide.\r\n\r\nWe prefer submitting test cases in the form of a PR to our integration test suite.\r\nIf you can provide one, please mention the PR number below.\r\nIf not, please attach the most minimal code example required to reproduce the issue below.\r\nIf the test case is too large, please include a link to a gist or a repository below.\r\n-->\r\n\r\n<details>\r\n<p>\r\n<ul> \r\n<li>1. Set up a celery project with mongodb as backend</li>\r\n<li>2. Set <pre>app.conf.result_expires = timedelta(seconds=60)</pre></li>\r\n<li>2. Set up a scheduled task <pre>app.conf.beat_schedule = {\r\n    \"dummy_task\": {\r\n        \"task\": \"dummy_task\",\r\n        \"schedule\": 15\r\n    },\r\n}</pre></li>\r\n<li>3. Start celery worker and celery beat</li>\r\n<li>4. Open shell on mongodb backend and see that db.task_result.count() never resets to 0</li>\r\n</ul> \r\n</p>\r\n</details>\r\n\r\n# Expected Behavior\r\n`task_result` collection on mongo database shoud be cleaned every 60s according to `result_expires` configuration\r\n\r\n# Actual Behavior\r\n<!--\r\nDescribe in detail what actually happened.\r\nPlease include a backtrace and surround it with triple backticks (```).\r\nIn addition, include the Celery daemon logs, the broker logs,\r\nthe result backend logs and system logs below if they will help us debug\r\nthe issue.\r\n-->\r\n\r\nThere is an issue with the format of field `date_done` in task_result collection. Task results meta are retrieved with the method `_get_result_meta` from `base.py` which argument `format_date` is set to `True` by default. `date_done` field will be converted from `datetime` object to `str` and then inserted as a string in mongodb database.\r\n\r\nAnd so when `cleanup()` method is called on `MongoBackend`, it will compare `date_done` field with datetime object from `self.app.now()` and will never match.\r\n\r\n```python\r\nself.collection.delete_many(\r\n        {'date_done': {'$lt': self.app.now() - self.expires_delta}},\r\n    )\r\n# self.app.now() return datetime object while date_done is stored as string\r\n```\r\n\r\n```\r\n> db.task_result.findOne()\r\n{\r\n        \"_id\" : \"f16bd459-b858-4ae8-afb5-1ceab0e50326\",\r\n        \"status\" : \"SUCCESS\",\r\n        \"result\" : \"\\\"SUCCESS\\\"\",\r\n        \"traceback\" : null,\r\n        \"children\" : [ ],\r\n        \"date_done\" : \"2023-08-08T09:03:34.974924\" // should be ISODate(\"2023-08-08T09:03:34.9749Z\")\r\n}\r\n```\r\n\r\nA simple fix would be to set `format_date` to `False` when calling `self.get_result_meta` in `MongoBackend._strore_result`\nInvalid format of 'date_done' field in celery.task_results with backend mongodb\n<!--\r\nPlease fill this template entirely and do not erase parts of it.\r\nWe reserve the right to close without a response\r\nbug reports which are incomplete.\r\n-->\r\n# Checklist\r\n<!--\r\nTo check an item on the list replace [ ] with [x].\r\n-->\r\n- [x] I have verified that the issue exists against the `main` branch of Celery.\r\n- [ ] This has already been asked to the [discussions forum](https://github.com/celery/celery/discussions) first.\r\n- [x] I have read the relevant section in the\r\n  [contribution guide](https://docs.celeryq.dev/en/main/contributing.html#other-bugs)\r\n  on reporting bugs.\r\n- [x] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Bug+Report%22+-label%3A%22Category%3A+Documentation%22)\r\n  for similar or identical bug reports.\r\n- [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22PR+Type%3A+Bugfix%22+-label%3A%22Category%3A+Documentation%22)\r\n  for existing proposed fixes.\r\n- [x] I have checked the [commit log](https://github.com/celery/celery/commits/main)\r\n  to find out if the bug was already fixed in the main branch.\r\n- [x] I have included all related issues and possible duplicate issues\r\n  in this issue (If there are none, check this box anyway).\r\n\r\n## Mandatory Debugging Information\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n    (if you are not able to do this, then at least specify the Celery\r\n     version affected).\r\n- [x] I have verified that the issue exists against the `main` branch of Celery.\r\n- [x] I have included the contents of ``pip freeze`` in the issue.\r\n- [x] I have included all the versions of all the external dependencies required\r\n  to reproduce this bug.\r\n\r\n## Environment & Settings\r\n<!-- Include the contents of celery --version below -->\r\n**Celery version**: 5.3.1\r\n<!-- Include the output of celery -A proj report below -->\r\n<details>\r\n<summary><b><code>celery -A test_celery_result report</code> Output:</b></summary>\r\n<p>\r\n\r\n```\r\nsoftware -> celery:5.3.1 (emerald-rush) kombu:5.3.1 py:3.8.16\r\n            billiard:4.1.0 redis:4.6.0\r\nplatform -> system:Linux arch:64bit, ELF\r\n            kernel version:5.19.0-46-generic imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:redis results:mongodb\r\n[...]\r\nCELERY_BROKER_TRANSPORT_OPTIONS: \r\n 'socket_keepalive': True, 'socket_keepalive_options': {4: 600, 5: 60, 6: 5}}\r\nCELERY_BROKER_URL: 'redis://redis.local:7000/0'\r\nCELERY_INCLUDE: ['test_celery_result.tasks']\r\nCELERY_QUEUE_NAME: 'test_celery_result'\r\nCELERY_REDIS: \r\n 'host': 'redis.local', 'port': 7000}\r\n[...]\r\nis_overridden: <bound method Settings.is_overridden of <Settings \"test_celery_result.settings\">>\r\ndeprecated_settings: None\r\ntask_default_queue: 'test_celery_result'\r\nenable_utc: False\r\nresult_backend: 'mongodb'\r\nresult_expires: datetime.timedelta(seconds=15)\r\nmongodb_backend_settings: \r\n    'database': '********',\r\n    'host': ['mongo-replica'],\r\n    'port': 27017,\r\n    'taskmeta_collection': 'celery_task_result'}\r\nbeat_schedule: \r\n    'celery.backend_cleanup': {   'schedule': 60,\r\n                                  'task': 'celery.backend_cleanup'},\r\n    'dummy_task': {'schedule': 15, 'task': 'dummy_task'}}\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n# Steps to Reproduce\r\n\r\n## Required Dependencies\r\n<!-- Please fill the required dependencies to reproduce this issue -->\r\n- **Minimal Python Version**: 3.6 or higher\r\n- **Minimal Celery Version**: 4.3.0 or higher\r\n- **Minimal Kombu Version**: Unknown\r\n- **Minimal Broker Version**: Unknown\r\n- **Minimal Result Backend Version**: Mongo 4.4 or higher\r\n- **Minimal OS and/or Kernel Version**: Unknown\r\n- **Minimal Broker Client Version**: Unknown\r\n- **Minimal Result Backend Client Version**: pymongo 3.14 or higher\r\n\r\n### Python Packages\r\n<!-- Please fill the contents of pip freeze below -->\r\n<details>\r\n<summary><b><code>pip freeze</code> Output:</b></summary>\r\n<p>\r\n\r\namqp==5.1.1\r\nasgiref==3.7.2\r\nasync-timeout==4.0.2\r\nbackports.zoneinfo==0.2.1\r\nbilliard==4.1.0\r\nbleach==6.0.0\r\ncelery==5.3.1\r\ncertifi==2023.7.22\r\ncffi==1.15.1\r\ncharset-normalizer==3.2.0\r\nclick==8.1.6\r\nclick-didyoumean==0.3.0\r\nclick-plugins==1.1.1\r\nclick-repl==0.3.0\r\ncryptography==41.0.3\r\nDjango==3.2.20\r\ndjango-cors-headers==4.2.0\r\ndjango-environ==0.10.0\r\ndjango-formset-js==0.5.0\r\ndjango-jquery-js==3.1.1\r\ndjango-redis-sessions==0.6.2\r\ndjango-test-addons-adv==1.1.1\r\ndnspython==2.4.1\r\nidna==3.4\r\nJinja2==3.1.2\r\nkombu==5.3.1\r\npackaging==23.1\r\nprompt-toolkit==3.0.39\r\npyasn1==0.5.0\r\npycparser==2.21\r\npyhcl==0.4.4\r\npymongo==4.4.1\r\npython-dateutil==2.8.2\r\npytz==2022.1\r\nPyYAML==6.0.1\r\nredis==4.6.0\r\nrequests==2.31.0\r\nsentinels==1.0.0\r\nsingle-beat==0.6.3\r\nsix==1.16.0\r\nsqlparse==0.4.4\r\ntypes-PyYAML==6.0.12.11\r\ntyping_extensions==4.7.1\r\ntzdata==2023.3\r\nurllib3==1.26.16\r\nvine==5.0.0\r\nwcwidth==0.2.6\r\nwebencodings==0.5.1\r\n\r\n</p>\r\n</details>\r\n\r\n## Minimally Reproducible Test Case\r\n<!--\r\nPlease provide a reproducible test case.\r\nRefer to the Reporting Bugs section in our contribution guide.\r\n\r\nWe prefer submitting test cases in the form of a PR to our integration test suite.\r\nIf you can provide one, please mention the PR number below.\r\nIf not, please attach the most minimal code example required to reproduce the issue below.\r\nIf the test case is too large, please include a link to a gist or a repository below.\r\n-->\r\n\r\n<details>\r\n<p>\r\n<ul> \r\n<li>1. Set up a celery project with mongodb as backend</li>\r\n<li>2. Set <pre>app.conf.result_expires = timedelta(seconds=60)</pre></li>\r\n<li>2. Set up a scheduled task <pre>app.conf.beat_schedule = {\r\n    \"dummy_task\": {\r\n        \"task\": \"dummy_task\",\r\n        \"schedule\": 15\r\n    },\r\n}</pre></li>\r\n<li>3. Start celery worker and celery beat</li>\r\n<li>4. Open shell on mongodb backend and see that db.task_result.count() never resets to 0</li>\r\n</ul> \r\n</p>\r\n</details>\r\n\r\n# Expected Behavior\r\n`task_result` collection on mongo database shoud be cleaned every 60s according to `result_expires` configuration\r\n\r\n# Actual Behavior\r\n<!--\r\nDescribe in detail what actually happened.\r\nPlease include a backtrace and surround it with triple backticks (```).\r\nIn addition, include the Celery daemon logs, the broker logs,\r\nthe result backend logs and system logs below if they will help us debug\r\nthe issue.\r\n-->\r\n\r\nThere is an issue with the format of field `date_done` in task_result collection. Task results meta are retrieved with the method `_get_result_meta` from `base.py` which argument `format_date` is set to `True` by default. `date_done` field will be converted from `datetime` object to `str` and then inserted as a string in mongodb database.\r\n\r\nAnd so when `cleanup()` method is called on `MongoBackend`, it will compare `date_done` field with datetime object from `self.app.now()` and will never match.\r\n\r\n```python\r\nself.collection.delete_many(\r\n        {'date_done': {'$lt': self.app.now() - self.expires_delta}},\r\n    )\r\n# self.app.now() return datetime object while date_done is stored as string\r\n```\r\n\r\n```\r\n> db.task_result.findOne()\r\n{\r\n        \"_id\" : \"f16bd459-b858-4ae8-afb5-1ceab0e50326\",\r\n        \"status\" : \"SUCCESS\",\r\n        \"result\" : \"\\\"SUCCESS\\\"\",\r\n        \"traceback\" : null,\r\n        \"children\" : [ ],\r\n        \"date_done\" : \"2023-08-08T09:03:34.974924\" // should be ISODate(\"2023-08-08T09:03:34.9749Z\")\r\n}\r\n```\r\n\r\nA simple fix would be to set `format_date` to `False` when calling `self.get_result_meta` in `MongoBackend._strore_result`\n",
        "hints_text": "\n",
        "created_at": "2023-08-10T14:27:37Z",
        "version": "5.3",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "t/unit/backends/test_base.py"
        ]
    },
    {
        "repo": "celery/celery",
        "pull_number": 8374,
        "instance_id": "celery__celery-8374",
        "issue_numbers": [
            "8259"
        ],
        "base_commit": "811ed96edbf7d7ae0681ae67ced63e6994a6e63a",
        "patch": "diff --git a/celery/bin/purge.py b/celery/bin/purge.py\nindex 7be1a8241fb..cfb6caa9323 100644\n--- a/celery/bin/purge.py\n+++ b/celery/bin/purge.py\n@@ -5,7 +5,9 @@\n from celery.utils import text\n \n \n-@click.command(cls=CeleryCommand)\n+@click.command(cls=CeleryCommand, context_settings={\n+    'allow_extra_args': True\n+})\n @click.option('-f',\n               '--force',\n               cls=CeleryOption,\n@@ -26,7 +28,7 @@\n               help=\"Comma separated list of queues names not to purge.\")\n @click.pass_context\n @handle_preload_options\n-def purge(ctx, force, queues, exclude_queues):\n+def purge(ctx, force, queues, exclude_queues, **kwargs):\n     \"\"\"Erase all messages from all known task queues.\n \n     Warning:\ndiff --git a/celery/bin/shell.py b/celery/bin/shell.py\nindex 77b14d8a307..6c94a00870e 100644\n--- a/celery/bin/shell.py\n+++ b/celery/bin/shell.py\n@@ -79,7 +79,9 @@ def _invoke_default_shell(locals):\n         _invoke_ipython_shell(locals)\n \n \n-@click.command(cls=CeleryCommand)\n+@click.command(cls=CeleryCommand, context_settings={\n+    'allow_extra_args': True\n+})\n @click.option('-I',\n               '--ipython',\n               is_flag=True,\n@@ -117,7 +119,7 @@ def _invoke_default_shell(locals):\n @handle_preload_options\n def shell(ctx, ipython=False, bpython=False,\n           python=False, without_tasks=False, eventlet=False,\n-          gevent=False):\n+          gevent=False, **kwargs):\n     \"\"\"Start shell session with convenient access to celery symbols.\n \n     The following symbols will be added to the main globals:\ndiff --git a/t/unit/bin/proj/pyramid_celery_app.py b/t/unit/bin/proj/pyramid_celery_app.py\nnew file mode 100644\nindex 00000000000..4878f95551b\n--- /dev/null\n+++ b/t/unit/bin/proj/pyramid_celery_app.py\n@@ -0,0 +1,53 @@\n+from unittest.mock import MagicMock, Mock\n+\n+from click import Option\n+\n+from celery import Celery\n+\n+# This module defines a mocked Celery application to replicate\n+# the behavior of Pyramid-Celery's configuration by preload options.\n+# Preload options should propagate to commands like shell and purge etc.\n+#\n+# The Pyramid-Celery project https://github.com/sontek/pyramid_celery\n+# assumes that you want to configure Celery via an ini settings file.\n+# The .ini files are the standard configuration file for Pyramid\n+# applications.\n+# See https://docs.pylonsproject.org/projects/pyramid/en/latest/quick_tutorial/ini.html\n+#\n+\n+app = Celery(set_as_current=False)\n+app.config_from_object(\"t.integration.test_worker_config\")\n+\n+\n+class PurgeMock:\n+    def queue_purge(self, queue):\n+        return 0\n+\n+\n+class ConnMock:\n+    default_channel = PurgeMock()\n+    channel_errors = KeyError\n+\n+\n+mock = Mock()\n+mock.__enter__ = Mock(return_value=ConnMock())\n+mock.__exit__ = Mock(return_value=False)\n+\n+app.connection_for_write = MagicMock(return_value=mock)\n+\n+# Below are taken from pyramid-celery's __init__.py\n+# Ref: https://github.com/sontek/pyramid_celery/blob/cf8aa80980e42f7235ad361874d3c35e19963b60/pyramid_celery/__init__.py#L25-L36 # noqa: E501\n+ini_option = Option(\n+    (\n+        \"--ini\",\n+        \"-i\",\n+    ),\n+    help=\"Paste ini configuration file.\",\n+)\n+\n+ini_var_option = Option(\n+    (\"--ini-var\",), help=\"Comma separated list of key=value to pass to ini.\"\n+)\n+\n+app.user_options[\"preload\"].add(ini_option)\n+app.user_options[\"preload\"].add(ini_var_option)\n",
        "test_patch": "diff --git a/t/unit/app/test_preload_cli.py b/t/unit/app/test_preload_cli.py\nnew file mode 100644\nindex 00000000000..a2241a1400d\n--- /dev/null\n+++ b/t/unit/app/test_preload_cli.py\n@@ -0,0 +1,63 @@\n+from click.testing import CliRunner\n+\n+from celery.bin.celery import celery\n+\n+\n+def test_preload_options(isolated_cli_runner: CliRunner):\n+    # Verify commands like shell and purge can accept preload options.\n+    # Projects like Pyramid-Celery's ini option should be valid preload\n+    # options.\n+\n+    # TODO: Find a way to run these separate invoke and assertions\n+    # such that order does not matter. Currently, running\n+    # the \"t.unit.bin.proj.pyramid_celery_app\" first seems\n+    # to result in cache or memoization of the option.\n+    # As a result, the expected exception is not raised when\n+    # the invoke on \"t.unit.bin.proj.app\" is run as a second\n+    # call.\n+\n+    res_without_preload = isolated_cli_runner.invoke(\n+        celery,\n+        [\"-A\", \"t.unit.bin.proj.app\", \"purge\", \"-f\", \"--ini\", \"some_ini.ini\"],\n+        catch_exceptions=True,\n+    )\n+\n+    assert \"No such option: --ini\" in res_without_preload.stdout\n+    assert res_without_preload.exit_code == 2\n+\n+    res_without_preload = isolated_cli_runner.invoke(\n+        celery,\n+        [\"-A\", \"t.unit.bin.proj.app\", \"shell\", \"--ini\", \"some_ini.ini\"],\n+        catch_exceptions=True,\n+    )\n+\n+    assert \"No such option: --ini\" in res_without_preload.stdout\n+    assert res_without_preload.exit_code == 2\n+\n+    res_with_preload = isolated_cli_runner.invoke(\n+        celery,\n+        [\n+            \"-A\",\n+            \"t.unit.bin.proj.pyramid_celery_app\",\n+            \"purge\",\n+            \"-f\",\n+            \"--ini\",\n+            \"some_ini.ini\",\n+        ],\n+        catch_exceptions=True,\n+    )\n+\n+    assert res_with_preload.exit_code == 0\n+\n+    res_with_preload = isolated_cli_runner.invoke(\n+        celery,\n+        [\n+            \"-A\",\n+            \"t.unit.bin.proj.pyramid_celery_app\",\n+            \"shell\",\n+            \"--ini\",\n+            \"some_ini.ini\",\n+        ],\n+        catch_exceptions=True,\n+    )\n+    assert res_with_preload.exit_code == 0\n",
        "problem_statement": "Set allow_extra_args to True in context_settings of shell and purge commands\n<!--\r\nPlease fill this template entirely and do not erase parts of it.\r\nWe reserve the right to close without a response\r\nenhancement requests which are incomplete.\r\n-->\r\n# Checklist\r\n<!--\r\nTo check an item on the list replace [ ] with [x].\r\n-->\r\n\r\n- [ x ] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Enhancement%22+-label%3A%22Category%3A+Documentation%22)\r\n  for similar or identical enhancement to an existing feature.\r\n- [ x ] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22Issue+Type%3A+Enhancement%22+-label%3A%22Category%3A+Documentation%22)\r\n  for existing proposed enhancements.\r\n- [ x ] I have checked the [commit log](https://github.com/celery/celery/commits/main)\r\n  to find out if the if the same enhancement was already implemented in the\r\n  main branch.\r\n- [ x ] I have included all related issues and possible duplicate issues in this issue\r\n      (If there are none, check this box anyway).\r\n\r\n## Related Issues and Possible Duplicates\r\n<!--\r\nPlease make sure to search and mention any related issues\r\nor possible duplicates to this issue as requested by the checklist above.\r\n\r\nThis may or may not include issues in other repositories that the Celery project\r\nmaintains or other repositories that are dependencies of Celery.\r\n\r\nIf you don't know how to mention issues, please refer to Github's documentation\r\non the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests\r\n-->\r\n\r\n#### Related Issues\r\n\r\nhttps://github.com/sontek/pyramid_celery/issues/101\r\n\r\n#### Possible Duplicates\r\n\r\n- None\r\n\r\n# Brief Summary\r\n<!--\r\nPlease include a brief summary of what the enhancement is\r\nand why it is needed.\r\n-->\r\n\r\nThe  [pyramid-celery](https://github.com/sontek/pyramid_celery) package adds `ini` and `ini-var` options to the standard commands. These allow the standard Pyramid `ini` configuration files to config Celery. With current Celery however, only the `worker`, `beat`, and `events` are able to be hooked into pyramid-celery's approach because those commands are set with.\r\n\r\n```\r\ncontext_settings={\r\n    'allow_extra_args': True\r\n}\r\n```\r\nSee `beat` for example: https://github.com/celery/celery/blob/f3a2cf45a69b443cac6c79a5c85583c8bd91b0a3/celery/bin/beat.py#L11\r\n\r\nThe `shell` and `purge` commands for some reason do not have context_settings and so do not set `allow_extra_args`. See for example in `shell`, https://github.com/celery/celery/blob/f3a2cf45a69b443cac6c79a5c85583c8bd91b0a3/celery/bin/shell.py#LL82C1-L117C24\r\n\r\nThe consequence is that the `ini` command options cause errors since these are extra and unknown args to these commands. Excluding the `ini` config casues  `shell` when it comes up to not get the task registration configs and `purge` isn't able to see queues.\r\n\r\n# Design\r\n\r\n## Architectural Considerations\r\n<!--\r\nIf more components other than Celery are involved,\r\ndescribe them here and the effect it would have on Celery.\r\n-->\r\n[pyramid-celery](https://github.com/sontek/pyramid_celery) is on [pypi](https://pypi.org/project/pyramid-celery). Allows you to use pyramid .ini files to configure celery and have your pyramid configuration inside celery tasks.\r\n\r\n## Proposed Behavior\r\n<!--\r\nPlease describe in detail how this enhancement is going to change the behavior\r\nof an existing feature.\r\nDescribe what happens in case of failures as well if applicable.\r\n-->\r\nThis should not affect Celery functionality, but will allow pyramid-celery to function as expected. See related issue tracked in pyramid-celery here https://github.com/sontek/pyramid_celery/issues/101\r\n\r\n## Proposed UI/UX\r\n<!--\r\nPlease provide your ideas for the API, CLI options,\r\nconfiguration key names etc. that will be adjusted for this enhancement.\r\n-->\r\n\r\nCelery should add `context_settings` with `allow_extra_args`,\r\n```\r\ncontext_settings={\r\n    'allow_extra_args': True\r\n}\r\n```\r\nto both the `shell` and `purge` commands. This will allow projects like pyramid-celery to hook in their own custom configuration options using the recommended (by Celery) `celery_app.user_options['preload'].add()` and/or `celery_app.user_options[option].add()` approaches. See [here](https://github.com/celery/celery/blob/e7b47a62d789557cf18ed0e56e2dfb99a51a62f7/docs/userguide/extending.rst#adding-new-command-line-options). This will also make `shell` and `purge` commands consistent with other commands like `worker`, `beat`, and `events` that already have these settings.\r\n\r\n## Diagrams\r\n<!--\r\nPlease include any diagrams that might be relevant\r\nto the implementation of this enhancement such as:\r\n* Class Diagrams\r\n* Sequence Diagrams\r\n* Activity Diagrams\r\nYou can drag and drop images into the text box to attach them to this issue.\r\n-->\r\nN/A\r\n\r\n## Alternatives\r\n<!--\r\nIf you have considered any alternative implementations\r\ndescribe them in detail below.\r\n-->\r\nNone\r\n\n",
        "hints_text": "Hey @dpdoughe :wave:,\nThank you for opening an issue. We will get back to you as soon as we can.\nAlso, check out our [Open Collective](https://opencollective.com/celery) and consider backing us - every little helps!\n\nWe also offer priority support for our sponsors.\nIf you require immediate assistance please consider sponsoring us.\n\nThe inability to configure and run `purge` or `shell` commands with pyramid_celery persists with Celery v5.3.1\nI am open to contributions in this regard. my expectation is adding adding missing tests for the newly proposed/missing features",
        "created_at": "2023-07-14T06:23:15Z",
        "version": "5.3",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": []
    },
    {
        "repo": "celery/celery",
        "pull_number": 8098,
        "instance_id": "celery__celery-8098",
        "issue_numbers": [
            "8080"
        ],
        "base_commit": "3bff3f06740a0d509f807e14702f7144b043ae54",
        "patch": "diff --git a/celery/result.py b/celery/result.py\nindex eb3e154933b..f66bade1d40 100644\n--- a/celery/result.py\n+++ b/celery/result.py\n@@ -14,7 +14,6 @@\n from .app import app_or_default\n from .exceptions import ImproperlyConfigured, IncompleteStream, TimeoutError\n from .utils.graph import DependencyGraph, GraphFormatter\n-from .utils.iso8601 import parse_iso8601\n \n try:\n     import tblib\n@@ -530,7 +529,7 @@ def date_done(self):\n         \"\"\"UTC date and time.\"\"\"\n         date_done = self._get_task_meta().get('date_done')\n         if date_done and not isinstance(date_done, datetime.datetime):\n-            return parse_iso8601(date_done)\n+            return datetime.datetime.fromisoformat(date_done)\n         return date_done\n \n     @property\ndiff --git a/celery/utils/iso8601.py b/celery/utils/iso8601.py\nindex 4f9d183312b..2a5ae69619f 100644\n--- a/celery/utils/iso8601.py\n+++ b/celery/utils/iso8601.py\n@@ -37,6 +37,8 @@\n \n from pytz import FixedOffset\n \n+from celery.utils.deprecated import warn\n+\n __all__ = ('parse_iso8601',)\n \n # Adapted from http://delete.me.uk/2005/03/iso8601.html\n@@ -53,6 +55,7 @@\n \n def parse_iso8601(datestring):\n     \"\"\"Parse and convert ISO-8601 string to datetime.\"\"\"\n+    warn(\"parse_iso8601\", \"v5.3\", \"v6\", \"datetime.datetime.fromisoformat\")\n     m = ISO8601_REGEX.match(datestring)\n     if not m:\n         raise ValueError('unable to parse date string %r' % datestring)\ndiff --git a/celery/utils/time.py b/celery/utils/time.py\nindex ed4008c6e48..984da17c80f 100644\n--- a/celery/utils/time.py\n+++ b/celery/utils/time.py\n@@ -13,7 +13,6 @@\n from pytz import utc\n \n from .functional import dictfilter\n-from .iso8601 import parse_iso8601\n from .text import pluralize\n \n __all__ = (\n@@ -257,7 +256,7 @@ def maybe_iso8601(dt):\n         return\n     if isinstance(dt, datetime):\n         return dt\n-    return parse_iso8601(dt)\n+    return datetime.fromisoformat(dt)\n \n \n def is_naive(dt):\n",
        "test_patch": "diff --git a/t/unit/tasks/test_tasks.py b/t/unit/tasks/test_tasks.py\nindex a636eac73be..0095bac3405 100644\n--- a/t/unit/tasks/test_tasks.py\n+++ b/t/unit/tasks/test_tasks.py\n@@ -13,7 +13,6 @@\n from celery.contrib.testing.mocks import ContextMock\n from celery.exceptions import Ignore, ImproperlyConfigured, Retry\n from celery.result import AsyncResult, EagerResult\n-from celery.utils.time import parse_iso8601\n \n try:\n     from urllib.error import HTTPError\n@@ -889,11 +888,11 @@ def assert_next_task_data_equal(self, consumer, presult, task_name,\n         assert task_headers['task'] == task_name\n         if test_eta:\n             assert isinstance(task_headers.get('eta'), str)\n-            to_datetime = parse_iso8601(task_headers.get('eta'))\n+            to_datetime = datetime.fromisoformat(task_headers.get('eta'))\n             assert isinstance(to_datetime, datetime)\n         if test_expires:\n             assert isinstance(task_headers.get('expires'), str)\n-            to_datetime = parse_iso8601(task_headers.get('expires'))\n+            to_datetime = datetime.fromisoformat(task_headers.get('expires'))\n             assert isinstance(to_datetime, datetime)\n         properties = properties or {}\n         for arg_name, arg_value in properties.items():\n",
        "problem_statement": "Deprecate iso8601 module\n<!--\r\nPlease fill this template entirely and do not erase parts of it.\r\nWe reserve the right to close without a response\r\nenhancement requests which are incomplete.\r\n-->\r\n# Checklist\r\n<!--\r\nTo check an item on the list replace [ ] with [x].\r\n-->\r\n\r\n- [x] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Enhancement%22+-label%3A%22Category%3A+Documentation%22)\r\n  for similar or identical enhancement to an existing feature.\r\n- [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22Issue+Type%3A+Enhancement%22+-label%3A%22Category%3A+Documentation%22)\r\n  for existing proposed enhancements.\r\n- [x] I have checked the [commit log](https://github.com/celery/celery/commits/main)\r\n  to find out if the if the same enhancement was already implemented in the\r\n  main branch.\r\n- [x] I have included all related issues and possible duplicate issues in this issue\r\n      (If there are none, check this box anyway).\r\n\r\n## Related Issues and Possible Duplicates\r\n<!--\r\nPlease make sure to search and mention any related issues\r\nor possible duplicates to this issue as requested by the checklist above.\r\n\r\nThis may or may not include issues in other repositories that the Celery project\r\nmaintains or other repositories that are dependencies of Celery.\r\n\r\nIf you don't know how to mention issues, please refer to Github's documentation\r\non the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests\r\n-->\r\n\r\n#### Related Issues\r\n\r\n- None\r\n\r\n#### Possible Duplicates\r\n\r\n- None\r\n\r\n# Brief Summary\r\n<!--\r\nPlease include a brief summary of what the enhancement is\r\nand why it is needed.\r\n-->\r\n\r\n`celery.utils.iso8601` is historic module which `.parse_iso8601()` can be replaced by `datetime.datetime.fromisoformat()` as Python 3.7 or above is now supported.\r\n\r\nSuggest deprecate this module and use `datetime.datetime.fromisoformat()` internally.\r\n\r\n# Design\r\n\r\n## Architectural Considerations\r\n<!--\r\nIf more components other than Celery are involved,\r\ndescribe them here and the effect it would have on Celery.\r\n-->\r\nNone\r\n\r\n## Proposed Behavior\r\n<!--\r\nPlease describe in detail how this enhancement is going to change the behavior\r\nof an existing feature.\r\nDescribe what happens in case of failures as well if applicable.\r\n-->\r\n\r\n- `iso8601` module is still usable before v6.0.0, but with addition of deprecation warning\r\n- remove `iso8601` module in v6.0.0\r\n- replace all usage of `parse_iso8601()` in the library by `datetime.datetime.fromisoformat`, with compatibility of Python 3.7 to 3.11\r\n\r\n## Proposed UI/UX\r\n<!--\r\nPlease provide your ideas for the API, CLI options,\r\nconfiguration key names etc. that will be adjusted for this enhancement.\r\n-->\r\n\r\nN/A\r\n\r\n## Diagrams\r\n<!--\r\nPlease include any diagrams that might be relevant\r\nto the implementation of this enhancement such as:\r\n* Class Diagrams\r\n* Sequence Diagrams\r\n* Activity Diagrams\r\nYou can drag and drop images into the text box to attach them to this issue.\r\n-->\r\nN/A\r\n\r\n## Alternatives\r\n<!--\r\nIf you have considered any alternative implementations\r\ndescribe them in detail below.\r\n-->\r\nNone\r\n\n",
        "hints_text": "Hey @wongcht :wave:,\nThank you for opening an issue. We will get back to you as soon as we can.\nAlso, check out our [Open Collective](https://opencollective.com/celery) and consider backing us - every little helps!\n\nWe also offer priority support for our sponsors.\nIf you require immediate assistance please consider sponsoring us.\n\nif that is the case lets do it.",
        "created_at": "2023-03-02T17:16:26Z",
        "version": "5.3",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "t/unit/tasks/test_tasks.py"
        ]
    },
    {
        "repo": "celery/celery",
        "pull_number": 7945,
        "instance_id": "celery__celery-7945",
        "issue_numbers": [
            "4806"
        ],
        "base_commit": "dd811b37717635b5f7151a7adf9f5bf12e1bc0c6",
        "patch": "diff --git a/CONTRIBUTORS.txt b/CONTRIBUTORS.txt\nindex fe420b14d67..e8c1dec868b 100644\n--- a/CONTRIBUTORS.txt\n+++ b/CONTRIBUTORS.txt\n@@ -291,3 +291,4 @@ Tizian Seehaus, 2022/02/09\n Oleh Romanovskyi, 2022/06/09\n JoonHwan Kim, 2022/08/01\n Kaustav Banerjee, 2022/11/10\n+Austin Snoeyink 2022/12/06\ndiff --git a/celery/app/defaults.py b/celery/app/defaults.py\nindex ce8d0ae1a90..65731e614c0 100644\n--- a/celery/app/defaults.py\n+++ b/celery/app/defaults.py\n@@ -78,6 +78,7 @@ def __repr__(self):\n         scheduler=Option('celery.beat:PersistentScheduler'),\n         schedule_filename=Option('celerybeat-schedule'),\n         sync_every=Option(0, type='int'),\n+        cron_starting_deadline=Option(None, type=int)\n     ),\n     broker=Namespace(\n         url=Option(None, type='string'),\ndiff --git a/celery/schedules.py b/celery/schedules.py\nindex 62940132098..9798579754f 100644\n--- a/celery/schedules.py\n+++ b/celery/schedules.py\n@@ -36,7 +36,6 @@\n {0._orig_day_of_week} (m/h/dM/MY/d)>\\\n \"\"\"\n \n-\n SOLAR_INVALID_LATITUDE = \"\"\"\\\n Argument latitude {lat} is invalid, must be between -90 and 90.\\\n \"\"\"\n@@ -608,16 +607,48 @@ def remaining_estimate(self, last_run_at, ffwd=ffwd):\n     def is_due(self, last_run_at):\n         \"\"\"Return tuple of ``(is_due, next_time_to_run)``.\n \n+        If :setting:`beat_cron_starting_deadline`  has been specified, the\n+        scheduler will make sure that the `last_run_at` time is within the\n+        deadline. This prevents tasks that could have been run according to\n+        the crontab, but didn't, from running again unexpectedly.\n+\n         Note:\n             Next time to run is in seconds.\n \n         SeeAlso:\n             :meth:`celery.schedules.schedule.is_due` for more information.\n         \"\"\"\n+\n         rem_delta = self.remaining_estimate(last_run_at)\n-        rem = max(rem_delta.total_seconds(), 0)\n+        rem_secs = rem_delta.total_seconds()\n+        rem = max(rem_secs, 0)\n         due = rem == 0\n-        if due:\n+\n+        deadline_secs = self.app.conf.beat_cron_starting_deadline\n+        has_passed_deadline = False\n+        if deadline_secs is not None:\n+            # Make sure we're looking at the latest possible feasible run\n+            # date when checking the deadline.\n+            last_date_checked = last_run_at\n+            last_feasible_rem_secs = rem_secs\n+            while rem_secs < 0:\n+                last_date_checked = last_date_checked + abs(rem_delta)\n+                rem_delta = self.remaining_estimate(last_date_checked)\n+                rem_secs = rem_delta.total_seconds()\n+                if rem_secs < 0:\n+                    last_feasible_rem_secs = rem_secs\n+\n+            # if rem_secs becomes 0 or positive, second-to-last\n+            # last_date_checked must be the last feasible run date.\n+            # Check if the last feasible date is within the deadline\n+            # for running\n+            has_passed_deadline = -last_feasible_rem_secs > deadline_secs\n+            if has_passed_deadline:\n+                # Should not be due if we've passed the deadline for looking\n+                # at past runs\n+                due = False\n+\n+        if due or has_passed_deadline:\n             rem_delta = self.remaining_estimate(self.now())\n             rem = max(rem_delta.total_seconds(), 0)\n         return schedstate(due, rem)\ndiff --git a/docs/userguide/configuration.rst b/docs/userguide/configuration.rst\nindex 5350d9fa2af..28b5a810ded 100644\n--- a/docs/userguide/configuration.rst\n+++ b/docs/userguide/configuration.rst\n@@ -3495,3 +3495,16 @@ changes to the schedule into account.\n Also when running Celery beat embedded (:option:`-B <celery worker -B>`)\n on Jython as a thread the max interval is overridden and set to 1 so\n that it's possible to shut down in a timely manner.\n+\n+.. setting:: beat_cron_starting_deadline\n+\n+``beat_cron_starting_deadline``\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+.. versionadded:: 5.3\n+\n+Default: None.\n+\n+When using cron, the number of seconds :mod:`~celery.bin.beat` can look back\n+when deciding whether a cron schedule is due. When set to `None`, cronjobs that\n+are past due will always run immediately.\n",
        "test_patch": "diff --git a/t/unit/app/test_beat.py b/t/unit/app/test_beat.py\nindex 94fdb0b464f..84f36d04f86 100644\n--- a/t/unit/app/test_beat.py\n+++ b/t/unit/app/test_beat.py\n@@ -696,16 +696,19 @@ def now_func():\n                 'first_missed', 'first_missed',\n                 last_run_at=now_func() - timedelta(minutes=2),\n                 total_run_count=10,\n+                app=self.app,\n                 schedule=app_schedule['first_missed']['schedule']),\n             'second_missed': beat.ScheduleEntry(\n                 'second_missed', 'second_missed',\n                 last_run_at=now_func() - timedelta(minutes=2),\n                 total_run_count=10,\n+                app=self.app,\n                 schedule=app_schedule['second_missed']['schedule']),\n             'non_missed': beat.ScheduleEntry(\n                 'non_missed', 'non_missed',\n                 last_run_at=now_func() - timedelta(minutes=2),\n                 total_run_count=10,\n+                app=self.app,\n                 schedule=app_schedule['non_missed']['schedule']),\n         }\n \ndiff --git a/t/unit/app/test_schedules.py b/t/unit/app/test_schedules.py\nindex ec3baedce85..d6f555c2cf2 100644\n--- a/t/unit/app/test_schedules.py\n+++ b/t/unit/app/test_schedules.py\n@@ -800,3 +800,136 @@ def test_yearly_execution_is_not_due(self):\n             due, remaining = self.yearly.is_due(datetime(2009, 3, 12, 7, 30))\n             assert not due\n             assert remaining == 4 * 24 * 60 * 60 - 3 * 60 * 60\n+\n+    def test_execution_not_due_if_task_not_run_at_last_feasible_time_outside_deadline(\n+            self):\n+        \"\"\"If the crontab schedule was added after the task was due, don't\n+        immediately fire the task again\"\"\"\n+        # could have feasibly been run on 12/5 at 7:30, but wasn't.\n+        self.app.conf.beat_cron_starting_deadline = 3600\n+        last_run = datetime(2022, 12, 4, 10, 30)\n+        now = datetime(2022, 12, 5, 10, 30)\n+        expected_next_execution_time = datetime(2022, 12, 6, 7, 30)\n+        expected_remaining = (\n+                    expected_next_execution_time - now).total_seconds()\n+\n+        # Run the daily (7:30) crontab with the current date\n+        with patch_crontab_nowfun(self.daily, now):\n+            due, remaining = self.daily.is_due(last_run)\n+            assert remaining == expected_remaining\n+            assert not due\n+\n+    def test_execution_not_due_if_task_not_run_at_last_feasible_time_no_deadline_set(\n+            self):\n+        \"\"\"Same as above test except there's no deadline set, so it should be\n+         due\"\"\"\n+        last_run = datetime(2022, 12, 4, 10, 30)\n+        now = datetime(2022, 12, 5, 10, 30)\n+        expected_next_execution_time = datetime(2022, 12, 6, 7, 30)\n+        expected_remaining = (\n+                    expected_next_execution_time - now).total_seconds()\n+\n+        # Run the daily (7:30) crontab with the current date\n+        with patch_crontab_nowfun(self.daily, now):\n+            due, remaining = self.daily.is_due(last_run)\n+            assert remaining == expected_remaining\n+            assert due\n+\n+    def test_execution_due_if_task_not_run_at_last_feasible_time_within_deadline(\n+            self):\n+        # Could have feasibly been run on 12/5 at 7:30, but wasn't. We are\n+        # still within a 1 hour deadline from the\n+        # last feasible run, so the task should still be due.\n+        self.app.conf.beat_cron_starting_deadline = 3600\n+        last_run = datetime(2022, 12, 4, 10, 30)\n+        now = datetime(2022, 12, 5, 8, 0)\n+        expected_next_execution_time = datetime(2022, 12, 6, 7, 30)\n+        expected_remaining = (\n+                    expected_next_execution_time - now).total_seconds()\n+\n+        # run the daily (7:30) crontab with the current date\n+        with patch_crontab_nowfun(self.daily, now):\n+            due, remaining = self.daily.is_due(last_run)\n+            assert remaining == expected_remaining\n+            assert due\n+\n+    def test_execution_due_if_task_not_run_at_any_feasible_time_within_deadline(\n+            self):\n+        # Could have feasibly been run on 12/4 at 7:30, or 12/5 at 7:30,\n+        # but wasn't. We are still within a 1 hour\n+        # deadline from the last feasible run (12/5), so the task should\n+        # still be due.\n+        self.app.conf.beat_cron_starting_deadline = 3600\n+        last_run = datetime(2022, 12, 3, 10, 30)\n+        now = datetime(2022, 12, 5, 8, 0)\n+        expected_next_execution_time = datetime(2022, 12, 6, 7, 30)\n+        expected_remaining = (\n+                    expected_next_execution_time - now).total_seconds()\n+\n+        # Run the daily (7:30) crontab with the current date\n+        with patch_crontab_nowfun(self.daily, now):\n+            due, remaining = self.daily.is_due(last_run)\n+            assert remaining == expected_remaining\n+            assert due\n+\n+    def test_execution_not_due_if_task_not_run_at_any_feasible_time_outside_deadline(\n+            self):\n+        \"\"\"Verifies that remaining is still the time to the next\n+        feasible run date even though the original feasible date\n+        was passed over in favor of a newer one.\"\"\"\n+        # Could have feasibly been run on 12/4 or 12/5 at 7:30,\n+        # but wasn't.\n+        self.app.conf.beat_cron_starting_deadline = 3600\n+        last_run = datetime(2022, 12, 3, 10, 30)\n+        now = datetime(2022, 12, 5, 11, 0)\n+        expected_next_execution_time = datetime(2022, 12, 6, 7, 30)\n+        expected_remaining = (\n+                    expected_next_execution_time - now).total_seconds()\n+\n+        # run the daily (7:30) crontab with the current date\n+        with patch_crontab_nowfun(self.daily, now):\n+            due, remaining = self.daily.is_due(last_run)\n+            assert remaining == expected_remaining\n+            assert not due\n+\n+    def test_execution_not_due_if_last_run_in_future(self):\n+        # Should not run if the last_run hasn't happened yet.\n+        last_run = datetime(2022, 12, 6, 7, 30)\n+        now = datetime(2022, 12, 5, 10, 30)\n+        expected_next_execution_time = datetime(2022, 12, 7, 7, 30)\n+        expected_remaining = (\n+                    expected_next_execution_time - now).total_seconds()\n+\n+        # Run the daily (7:30) crontab with the current date\n+        with patch_crontab_nowfun(self.daily, now):\n+            due, remaining = self.daily.is_due(last_run)\n+            assert not due\n+            assert remaining == expected_remaining\n+\n+    def test_execution_not_due_if_last_run_at_last_feasible_time(self):\n+        # Last feasible time is 12/5 at 7:30\n+        last_run = datetime(2022, 12, 5, 7, 30)\n+        now = datetime(2022, 12, 5, 10, 30)\n+        expected_next_execution_time = datetime(2022, 12, 6, 7, 30)\n+        expected_remaining = (\n+                    expected_next_execution_time - now).total_seconds()\n+\n+        # Run the daily (7:30) crontab with the current date\n+        with patch_crontab_nowfun(self.daily, now):\n+            due, remaining = self.daily.is_due(last_run)\n+            assert remaining == expected_remaining\n+            assert not due\n+\n+    def test_execution_not_due_if_last_run_past_last_feasible_time(self):\n+        # Last feasible time is 12/5 at 7:30\n+        last_run = datetime(2022, 12, 5, 8, 30)\n+        now = datetime(2022, 12, 5, 10, 30)\n+        expected_next_execution_time = datetime(2022, 12, 6, 7, 30)\n+        expected_remaining = (\n+                    expected_next_execution_time - now).total_seconds()\n+\n+        # Run the daily (7:30) crontab with the current date\n+        with patch_crontab_nowfun(self.daily, now):\n+            due, remaining = self.daily.is_due(last_run)\n+            assert remaining == expected_remaining\n+            assert not due\n",
        "problem_statement": "celery.schedules.crontab is_due logic can trigger periodic celery beat tasks at arbitrary times unrelated to the crontab parameters when last_run_at value is sufficiently old\nThere's an issue in the implementation of `celery.schedules.crontab` method that can cause `crontab.is_due` to claim a schedule is due at a time that is completely unrelated to the given crontab parameters. This appears to happen in cases where the `last_run_at` value is older than the most recent feasible time the schedule could have run. It looks like that this issue was introduced as part of complex logic added nearly 8 years ago to improve the accuracy of time remaining estimates for `crontab` periodic tasks: 4ed89ec49582b540149cf06047f091ebd20fb300\n\n## Checklist\nIssue first observed in a celery deployment running celery v3.1.19\n\nFrom inspecting the `celery.schedules.crontab` code it appears likely that this issue is present in all celery versions as far back as v2.1.0 through to v4.1.1 .\n\nUnit tests (please see below) confirm issue is still present in master (b599b96960be9dd42b3dee82a58bd1d711df0317 at time of writing).\n\n## Steps to reproduce\nPlease apply this patch to celery master branch, remove `@skip.todo` from the first new test, run unit tests, observe the first of these added unit tests fails:\n\n```\ndiff --git a/t/unit/app/test_schedules.py b/t/unit/app/test_schedules.py\nindex a7b3025..0340461 100644\n--- a/t/unit/app/test_schedules.py\n+++ b/t/unit/app/test_schedules.py\n@@ -26,6 +26,18 @@ def patch_crontab_nowfun(cls, retval):\n         cls.nowfun = prev_nowfun\n \n \n+def is_time_feasible_wrt_crontab_schedule(t, z):\n+    # z : celery.schedules.crontab instance\n+    t = z.maybe_make_aware(t)\n+    return (\n+        t.month in z.month_of_year and\n+        (t.isoweekday() % 7) in z.day_of_week and\n+        t.day in z.day_of_month and\n+        t.hour in z.hour and\n+        t.minute in z.minute\n+    )\n+\n+\n @skip.unless_module('ephem')\n class test_solar:\n \n@@ -803,3 +815,59 @@ class test_crontab_is_due:\n             due, remaining = self.yearly.is_due(datetime(2009, 3, 12, 7, 30))\n             assert not due\n             assert remaining == 4 * 24 * 60 * 60 - 3 * 60 * 60\n+\n+    @skip.todo('FIXME crontab logic is defective when last_run_at is older than the most recent feasible time wrt schedule')\n+    def test_daily_execution_if_last_run_at_was_days_ago_and_current_time_does_not_match_crontab_schedule_then_execution_is_not_due(self):\n+        z = self.crontab(hour=7, minute=30)\n+        last_run_at = datetime(2018, 6, 1, 7, 30)\n+        now = datetime(2018, 6, 9, 23, 48)\n+        expected_next_execution_time = datetime(2018, 6, 10, 7, 30)\n+        expected_remaining = (expected_next_execution_time - now).total_seconds()\n+        # check our assumptions\n+        assert is_time_feasible_wrt_crontab_schedule(last_run_at, z)\n+        assert not is_time_feasible_wrt_crontab_schedule(now, z)\n+        assert is_time_feasible_wrt_crontab_schedule(expected_next_execution_time, z)\n+        assert now < expected_next_execution_time\n+        assert expected_remaining == (7 * 60 + 30 + 12) * 60\n+        # test is_due\n+        with patch_crontab_nowfun(z, now):\n+            due, remaining = z.is_due(last_run_at=last_run_at)\n+            assert remaining == expected_remaining\n+            assert not due\n+\n+    def test_daily_execution_if_last_run_at_was_the_most_recent_feasible_time_wrt_schedule_in_past_and_current_time_does_not_match_crontab_schedule_then_execution_is_not_due(self):\n+        z = self.crontab(hour=7, minute=30)\n+        last_run_at = datetime(2018, 6, 9, 7, 30)\n+        now = datetime(2018, 6, 9, 23, 48)\n+        expected_next_execution_time = datetime(2018, 6, 10, 7, 30)\n+        expected_remaining = (expected_next_execution_time - now).total_seconds()\n+        # check our assumptions\n+        assert is_time_feasible_wrt_crontab_schedule(last_run_at, z)\n+        assert not is_time_feasible_wrt_crontab_schedule(now, z)\n+        assert is_time_feasible_wrt_crontab_schedule(expected_next_execution_time, z)\n+        assert now < expected_next_execution_time\n+        assert expected_remaining == (7 * 60 + 30 + 12) * 60\n+        # test is_due\n+        with patch_crontab_nowfun(z, now):\n+            due, remaining = z.is_due(last_run_at=last_run_at)\n+            assert remaining == expected_remaining\n+            assert not due\n+\n+    def test_daily_execution_if_last_run_at_was_more_recent_than_the_most_recent_feasible_time_wrt_schedule_in_past_and_current_time_does_not_match_crontab_schedule_then_execution_is_not_due(self):\n+        z = self.crontab(hour=7, minute=30)\n+        last_run_at = datetime(2018, 6, 9, 10, 30) # not feasible wrt to current schedule. case can happen if schedule is modified after a run\n+        now = datetime(2018, 6, 9, 23, 48)\n+        expected_next_execution_time = datetime(2018, 6, 10, 7, 30)\n+        expected_remaining = (expected_next_execution_time - now).total_seconds()\n+        # check our assumptions\n+        assert not is_time_feasible_wrt_crontab_schedule(last_run_at, z)\n+        assert not is_time_feasible_wrt_crontab_schedule(now, z)\n+        assert is_time_feasible_wrt_crontab_schedule(expected_next_execution_time, z)\n+        assert now < expected_next_execution_time\n+        assert expected_remaining == (7 * 60 + 30 + 12) * 60\n+        # test is_due\n+        with patch_crontab_nowfun(z, now):\n+            due, remaining = z.is_due(last_run_at=last_run_at)\n+            assert remaining == expected_remaining\n+            assert not due\n+\n```\n\n## Expected behavior\nNo matter what the value of `last_run_at` is, `crontab.is_due(last_run_at)` should never return `schedstate(True, rem)` when the current time `now` is not feasible with respect to the given crontab parameters.\n\n## Actual behavior\nif `last_run_at` value is older than the most recent time that is feasible with respect to the given crontab parameters then `crontab.is_due(last_run_at)` returns `schedstate(True, rem)`  when the current time is not feasible with respect to the crontab parameters.\n\n## Comments\nThis behaviour is surprising as it is an undocumented departure from cron-like behaviour. This behaviour is somewhat like an undocumented variant of uncontrollable `anacron` behaviour. \"Uncontrollable\" in the sense that unlike `anacrontab`'s `START_HOURS_RANGE` parameter - there is no control at all over when tasks will be run when a scheduled execution is missed.\n\nWe experience this issue in production using the venerable Celery version of 3.1.19 : we have a celery beat process that is backed by a database using custom scheduler code that is derived from the django-celery-beat scheduler. Suppose we stop this celery beat process for some reason such as scheduled maintenance or during a deployment that needs to redeploy code to our celery cluster. When we later restart celery beat again then some or all `celery.schedules.crontab` scheduled tasks may immediately execute if there was a time during the celery beat downtime period that matches the crontab parameters.\n\nOne hack that can mitigate this behaviour is updating the \"last_run_at\" value for all celery crontab scheduled periodic tasks to the current time when celery beat starts, before celery beat makes any calls to the `celery.schedules.crontab.is_due` method.\n\n\n",
        "hints_text": "I am in the process of rewriting the `celery.schedules.crontab` logic to fix this cleanly, if i can get the green light from my client to release a fix upstream i will submit a pull request with a proposed patch to fix this within the next week or two.\r\n\r\n\n@fcostin did you ever complete your `crontab` rewrite and/or (not) get permission to release?\nCan you send a PR with proposed fix?\n@fdemmer -\r\n\r\nOne crude way to fix this is to simply revert the old change that introduced this issue: https://github.com/celery/celery/commit/4ed89ec49582b540149cf06047f091ebd20fb300 , in particular the parts of the patch to `celery/schedules.py` that introduce the much more complicated logic that estimates the time remaining until the next job.\r\n\r\nThis would have downside of breaking the logic that estimates when the next scheduled task is due, and causing the scheduler to need to poll every second to re-check if a task is due yet. But it would fix this issue.\r\n\r\nI wrote a better patch for fixing this last year (that didn't rely on polling every 1 second, and could estimate remaining time until the next scheduled task) which resolved the issue internally but finished up my contract shortly after and ran out of time to lobby for permission to release the fix upstream. I'll reach out and see if anyone there is willing to work with me to get the patch released (chances of this are low, but worth a try...).\r\n\nthanks for the response @fcostin :)\r\n\r\nmy requirement is adding tz support. i thought about rewriting/fixing/adding to celery's crontab, but decided it would be better to not hack around in that code or pile up on top, so i wrote my own using [tzcron](https://github.com/bloomberg/tzcron) to parse the cron expression and calculate the next event.\r\n\r\n- it calculates the timestamp for the next trigger based only on \"now\" (ignoring `last_run_at`), so the bug from this issue _should_ be fixed.\r\n- the cron parameters can be passed as string or separate args, in the order requested in #4570\r\n\r\nhttps://gist.github.com/fdemmer/7551bff2bab80b56aac5018060aded55\r\n\r\nit hasn't been used a lot and has no tests, but _seems_ to work and is licensed under MIT like all my gists.\nhi @fdemmer -- adding timezone support sounds like a great idea, as does your implementation that avoids using the existing celery crontab scheduler logic completely.\r\n\r\nYour `pytzcrontab` implementation looks pretty good . If i was aware of this last year that would have likely been a cleaner fix than my patch. thanks for sharing!\r\n\r\n> it hasn't been used a lot and has no tests, but seems to work and is licensed under MIT like all my gists.\r\n\r\nSounds promising!\r\n\r\nI have subjected your `pytzcrontab` class to the existing unit tests for the crontab scheduler -- with some patches to those unit tests to fix up timezones as necessary -- including adding three new tests from my patch in this issue.\r\n\r\nhere are my experimental patches, including adding `celery-tzcron.py` from your gist as `celery/scheduler_tzcron.py`:\r\n\r\nhttps://github.com/fcostin/celery/commits/scheduler_tzcron_experiments\r\n\r\nthe results are fairly good:\r\n\r\n1. `pytzcrontab` appears to fix this issue, as you predict. this makes sense.\r\n2. with a small patch for compatibility with the existing `crontab` scheduler to define `hour` ... `day_of_year` properties, `pytzcrontab` passes most of the old unit tests\r\n3. with some more patches to your `is_due` and `remaining_estimate` logic, the `pytzcrontab` implementation behaves in a closer way to how the existing `crontab` class decides when things are due. This change appears to be necessary so that `pytzcrontab` does not say the schedule is due when the schedule last ran a very short time ago (e.g. 0 seconds ago!). i am not completely happy with my proposed change here, it doesn't seem completely clean, maybe you can think of a nicer way to do it.\r\n3. `pytzcrontab` introduces one regression that the existing celery unit tests can detect-- it goes into an infinite loop if we try to schedule something on an impossible day (31st of april) . I looked for an obvious way to avoid this but didn't find a good one so i've marked the test as skipped, but this would be good to fix. I guess it could be fixed by additional up-front validation logic, but there is still some risk that a gap between validation and the behaviour of the `tzcron` library might lead to an infinite loop.\r\n\r\n```\r\n   @pytest.mark.skip(\"broken - pytzcrontab goes into an infinite loop here\")\r\n    def test_invalid_specification(self):\r\n        # *** WARNING ***\r\n        # This test triggers an infinite loop in case of a regression\r\n        with pytest.raises(RuntimeError):\r\n            self.next_ocurrance(\r\n                self.pytzcrontab(day_of_month=31, month_of_year=4),\r\n                datetime_utc(2010, 1, 28, 14, 30, 15),\r\n            )\r\n```\r\n\r\nWhat do you think?\r\n\r\nWould it make sense to contribute something like this as a pull request into celery itself?\nThis issue is fairly old is there a fix or a work around for this? I see the gist above but I'd rather not have to maintain some other version of celery.\r\n\r\nRight now I'm facing an issue with duplicate ETL imports because of this. I run all crontab schedules but if I make an on the fly change, either adding a new periodic task or changing the schedule of one, it will trigger ALL of my tasks to run.\nIn all honesty this seems like its been an issue for a very long time and frankly surprised its not a much higher priority bug to be fixed. For a scheduling app to incorrectly schedule a task seems like a pretty big issue.\r\n\r\nAnyways, I appreciate you taking the time to respond. I can try to look at the code, but in all honesty I'm not very good with following large projects like Celery. If I get some time over Christmas holiday I will have a go at trying to see where it can be fixed.\r\n\r\nStay safe and have a wonderful holiday!\n> In all honesty this seems like its been an issue for a very long time and frankly surprised its not a much higher priority bug to be fixed. For a scheduling app to incorrectly schedule a task seems like a pretty big issue.\r\n> \r\n> Anyways, I appreciate you taking the time to respond. I can try to look at the code, but in all honesty I'm not very good with following large projects like Celery. If I get some time over Christmas holiday I will have a go at trying to see where it can be fixed.\r\n> \r\n> Stay safe and have a wonderful holiday!\r\n\r\nif you can take the lead we could help you guide you through all the way\nSo now I'm not sure if my issue is related to this. I was trying to reproduce this with my app and wasn't able to reproduce it until today.\r\n\r\nThe way it triggered wasn't by adding/removing/updating a periodic task, but by adding a new schedule and then assigning that to a newly created periodic task. I don't know if this is due to the library I'm using to store it in a database or relating to this issue. I'll have to dig a bit deeper when I can.\nWe ran into this issue on our production setup and charged our subscription customers ahead of schedule when we restarted the services with a slight change in the scheduled time. This should be a higher priority bug.\r\n\r\nI will try and post the steps to reproduce this reliably.\nAs a work around: since I'm using an external scheduler library for managing the beat schedules I just simply have an event listener on beat startup null out the last_run_at field for each schedule in the database. This will prevent beat from running any of the schedules prematurely.\nI am not sure if we are experiencing this same issue, here is what the celerybeat-schedule looks like:\r\n\r\n```\r\n>>> schedule = shelve.open(\"celerybeat-schedule\", writeback=True)\r\n>>> task = schedule[\"entries\"][\"My task\"]\r\n>>> task.last_run_at\r\ndatetime.datetime(2021, 5, 18, 8, 36, 0, 99, tzinfo=<UTC>)\r\n>>> task.is_due()\r\nschedstate(is_due=True, next=48.84625)\r\n>>> task.schedule\r\n<crontab: * 8 * * * (m/h/d/dM/MY)>\r\n>>> datetime.now()\r\ndatetime.datetime(2021, 5, 18, 8, 39, 36, 601785)\r\n```\r\nThe above task runs continously between 8am and 9 am __sometimes__ but not always... Using `celery==5.0.5`\n@thedrow I can't commit as I don't have enough time, not expecting a fix, was just unsure if this bug is the same described above. I have now changed my crontab from 8am to 7:59 and waiting to see if the problem resurfaces. \r\n\r\n**Update**\r\nWith time set to 7:59 the task only runs once at the specified time, haven't observed the issue over the last two weeks.\nI have the same issue. I use crontab(hour='*/3') and once the top of the hour hits, it runs on an infinite loop.",
        "created_at": "2022-12-06T15:35:26Z",
        "version": "5.3",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "t/unit/app/test_beat.py",
            "t/unit/app/test_schedules.py"
        ]
    },
    {
        "repo": "celery/celery",
        "pull_number": 7734,
        "instance_id": "celery__celery-7734",
        "issue_numbers": [
            "6622"
        ],
        "base_commit": "fbae71ca2bc2eb68988131f5719a1dc5807d58fd",
        "patch": "diff --git a/celery/backends/dynamodb.py b/celery/backends/dynamodb.py\nindex 7c2f1ca5b39..fbc8bcf160e 100644\n--- a/celery/backends/dynamodb.py\n+++ b/celery/backends/dynamodb.py\n@@ -201,28 +201,25 @@ def _get_or_create_table(self):\n         \"\"\"Create table if not exists, otherwise return the description.\"\"\"\n         table_schema = self._get_table_schema()\n         try:\n-            table_description = self._client.create_table(**table_schema)\n-            logger.info(\n-                'DynamoDB Table {} did not exist, creating.'.format(\n-                    self.table_name\n-                )\n-            )\n-            # In case we created the table, wait until it becomes available.\n-            self._wait_for_table_status('ACTIVE')\n-            logger.info(\n-                'DynamoDB Table {} is now available.'.format(\n-                    self.table_name\n-                )\n-            )\n-            return table_description\n+            return self._client.describe_table(TableName=self.table_name)\n         except ClientError as e:\n             error_code = e.response['Error'].get('Code', 'Unknown')\n \n-            # If table exists, do not fail, just return the description.\n-            if error_code == 'ResourceInUseException':\n-                return self._client.describe_table(\n-                    TableName=self.table_name\n+            if error_code == 'ResourceNotFoundException':\n+                table_description = self._client.create_table(**table_schema)\n+                logger.info(\n+                    'DynamoDB Table {} did not exist, creating.'.format(\n+                        self.table_name\n+                    )\n+                )\n+                # In case we created the table, wait until it becomes available.\n+                self._wait_for_table_status('ACTIVE')\n+                logger.info(\n+                    'DynamoDB Table {} is now available.'.format(\n+                        self.table_name\n+                    )\n                 )\n+                return table_description\n             else:\n                 raise e\n \n",
        "test_patch": "diff --git a/t/unit/backends/test_dynamodb.py b/t/unit/backends/test_dynamodb.py\nindex 6fd2625c0cb..a27af96d6ff 100644\n--- a/t/unit/backends/test_dynamodb.py\n+++ b/t/unit/backends/test_dynamodb.py\n@@ -121,39 +121,34 @@ def test_get_client_time_to_live_called(\n         mock_set_table_ttl.assert_called_once()\n \n     def test_get_or_create_table_not_exists(self):\n+        from botocore.exceptions import ClientError\n+\n         self.backend._client = MagicMock()\n         mock_create_table = self.backend._client.create_table = MagicMock()\n+        client_error = ClientError(\n+            {\n+                'Error': {\n+                    'Code': 'ResourceNotFoundException'\n+                }\n+            },\n+            'DescribeTable'\n+        )\n         mock_describe_table = self.backend._client.describe_table = \\\n             MagicMock()\n-\n-        mock_describe_table.return_value = {\n-            'Table': {\n-                'TableStatus': 'ACTIVE'\n-            }\n-        }\n+        mock_describe_table.side_effect = client_error\n+        self.backend._wait_for_table_status = MagicMock()\n \n         self.backend._get_or_create_table()\n+        mock_describe_table.assert_called_once_with(\n+            TableName=self.backend.table_name\n+        )\n         mock_create_table.assert_called_once_with(\n             **self.backend._get_table_schema()\n         )\n \n     def test_get_or_create_table_already_exists(self):\n-        from botocore.exceptions import ClientError\n-\n         self.backend._client = MagicMock()\n         mock_create_table = self.backend._client.create_table = MagicMock()\n-        client_error = ClientError(\n-            {\n-                'Error': {\n-                    'Code': 'ResourceInUseException',\n-                    'Message': 'Table already exists: {}'.format(\n-                        self.backend.table_name\n-                    )\n-                }\n-            },\n-            'CreateTable'\n-        )\n-        mock_create_table.side_effect = client_error\n         mock_describe_table = self.backend._client.describe_table = \\\n             MagicMock()\n \n@@ -167,6 +162,7 @@ def test_get_or_create_table_already_exists(self):\n         mock_describe_table.assert_called_once_with(\n             TableName=self.backend.table_name\n         )\n+        mock_create_table.assert_not_called()\n \n     def test_wait_for_table_status(self):\n         self.backend._client = MagicMock()\n",
        "problem_statement": "dynamoDB result backend incorrect exception handling when table exists\nThe following code\n\nhttps://github.com/celery/celery/blob/d0f5300691ca594f2311daf542aa63367622c027/celery/backends/dynamodb.py#L192\n\ntries to create or get the table. In case the table exists and the role executing that code on AWS does not have the `CreateTable` permission, the raised exception is not the one expected by that particular line. Yet the table exists _and_ the exception is raised because the code tries to create it while lacking permissions.\n\nTo reproduce:\n\n\n* celery 5.0.5\n* create the result backend table and give the following permissions to the role executing celery on that table:\n\n```\n   \"dynamodb:DescribeTable\",\n   \"dynamodb:PutItem\",\n   \"dynamodb:UpdateItem\",\n   \"dynamodb:DeleteItem\",\n   \"dynamodb:BatchWriteItem\",\n   \"dynamodb:GetItem\",\n   \"dynamodb:BatchGetItem\",\n   \"dynamodb:Scan\",\n   \"dynamodb:Query\",\n   \"dynamodb:ConditionCheckItem\"\n```\n\n* remove the permission CreateTable from the role executing the code\n\nPossible solution:\n\n* handle the check of the table existence with another boto3 call such as [`describe_table`](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/dynamodb.html#DynamoDB.Client.describe_table) and then fall-back into the table creation\n* provide a configuration that, when indicated, assumes the existence of the table\n* handle that exception differently\n\n\n",
        "hints_text": "> Possible solution:\r\n> \r\n>     * handle the check of the table existence with another boto3 call such as [`describe_table`](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/dynamodb.html#DynamoDB.Client.describe_table) and then fall-back into the table creation\r\n> \r\n>     * provide a configuration that, when indicated, assumes the existence of the table\r\n> \r\n>     * handle that exception differently\r\n\r\nI am no expert on dynamo DB. But if you can come with a change request as your proposed possible solutions we could verify & help to improve.\n@lyajedi let's continue here",
        "created_at": "2022-08-30T15:23:38Z",
        "version": "5.3",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "t/unit/backends/test_dynamodb.py"
        ]
    },
    {
        "repo": "celery/celery",
        "pull_number": 7609,
        "instance_id": "celery__celery-7609",
        "issue_numbers": [
            "3576"
        ],
        "base_commit": "ec3714edf37e773ca5372f71f7f4ee5b1b33dd5d",
        "patch": "diff --git a/celery/worker/state.py b/celery/worker/state.py\nindex 3afb2e8e3b9..97f49150286 100644\n--- a/celery/worker/state.py\n+++ b/celery/worker/state.py\n@@ -32,18 +32,18 @@\n }\n \n #: maximum number of revokes to keep in memory.\n-REVOKES_MAX = 50000\n+REVOKES_MAX = int(os.environ.get('CELERY_WORKER_REVOKES_MAX', 50000))\n \n #: maximum number of successful tasks to keep in memory.\n-SUCCESSFUL_MAX = 1000\n+SUCCESSFUL_MAX = int(os.environ.get('CELERY_WORKER_SUCCESSFUL_MAX', 1000))\n \n #: how many seconds a revoke will be active before\n #: being expired when the max limit has been exceeded.\n-REVOKE_EXPIRES = 10800\n+REVOKE_EXPIRES = float(os.environ.get('CELERY_WORKER_REVOKE_EXPIRES', 10800))\n \n #: how many seconds a successful task will be cached in memory\n #: before being expired when the max limit has been exceeded.\n-SUCCESSFUL_EXPIRES = 10800\n+SUCCESSFUL_EXPIRES = float(os.environ.get('CELERY_WORKER_SUCCESSFUL_EXPIRES', 10800))\n \n #: Mapping of reserved task_id->Request.\n requests = {}\ndiff --git a/docs/userguide/workers.rst b/docs/userguide/workers.rst\nindex 9b8c2a4387d..9f75bb9aeda 100644\n--- a/docs/userguide/workers.rst\n+++ b/docs/userguide/workers.rst\n@@ -358,6 +358,20 @@ Commands\n All worker nodes keeps a memory of revoked task ids, either in-memory or\n persistent on disk (see :ref:`worker-persistent-revokes`).\n \n+.. note::\n+\n+    The maximum number of revoked tasks to keep in memory can be\n+    specified using the ``CELERY_WORKER_REVOKES_MAX`` environment\n+    variable, which defaults to 50000. When the limit has been exceeded,\n+    the revokes will be active for 10800 seconds (3 hours) before being\n+    expired. This value can be changed using the\n+    ``CELERY_WORKER_REVOKE_EXPIRES`` environment variable.\n+\n+    Memory limits can also be set for successful tasks through the\n+    ``CELERY_WORKER_SUCCESSFUL_MAX`` and\n+    ``CELERY_WORKER_SUCCESSFUL_EXPIRES`` environment variables, and\n+    default to 1000 and 10800 respectively.\n+\n When a worker receives a revoke request it will skip executing\n the task, but it won't terminate an already executing task unless\n the `terminate` option is set.\n",
        "test_patch": "diff --git a/t/unit/worker/test_state.py b/t/unit/worker/test_state.py\nindex 571fc4be32d..7388c49bb9f 100644\n--- a/t/unit/worker/test_state.py\n+++ b/t/unit/worker/test_state.py\n@@ -1,4 +1,7 @@\n+import os\n import pickle\n+import sys\n+from importlib import import_module\n from time import time\n from unittest.mock import Mock, patch\n \n@@ -187,3 +190,32 @@ def test_ready(self, requests=[SimpleReq('foo'),\n         for request in requests:\n             state.task_ready(request)\n         assert len(state.active_requests) == 0\n+\n+\n+class test_state_configuration():\n+\n+    @staticmethod\n+    def import_state():\n+        with patch.dict(sys.modules):\n+            del sys.modules['celery.worker.state']\n+            return import_module('celery.worker.state')\n+\n+    @patch.dict(os.environ, {\n+        'CELERY_WORKER_REVOKES_MAX': '50001',\n+        'CELERY_WORKER_SUCCESSFUL_MAX': '1001',\n+        'CELERY_WORKER_REVOKE_EXPIRES': '10801',\n+        'CELERY_WORKER_SUCCESSFUL_EXPIRES': '10801',\n+    })\n+    def test_custom_configuration(self):\n+        state = self.import_state()\n+        assert state.REVOKES_MAX == 50001\n+        assert state.SUCCESSFUL_MAX == 1001\n+        assert state.REVOKE_EXPIRES == 10801\n+        assert state.SUCCESSFUL_EXPIRES == 10801\n+\n+    def test_default_configuration(self):\n+        state = self.import_state()\n+        assert state.REVOKES_MAX == 50000\n+        assert state.SUCCESSFUL_MAX == 1000\n+        assert state.REVOKE_EXPIRES == 10800\n+        assert state.SUCCESSFUL_EXPIRES == 10800\n",
        "problem_statement": "make REVOKES_MAX and REVOKE_EXPIRES configurable\nValues of REVOKE_EXPIRES and REVOKES_MAX in worker/state.py  are hardcoded.\n\nThis should be configurable. Some of us really needed to change this.\n\n\n",
        "hints_text": "any suggested changes on your mind? feel free send a PR\nwill try, thanks for pushing celery forward :)\r\n\r\n\nI think this can be programmatically supported via adding `control` commands, similar to the following:\r\n\r\n```python\r\nfrom celery.worker.control import control_command\r\n\r\nfrom celery.worker import state as worker_state\r\n\r\n@control_command(\r\n    args=[('n', int)],\r\n    signature='[N={}]'.format(worker_state.REVOKES_MAX),  # <- used for help on the command-line.\r\n)\r\ndef revokes_max(state, n=worker_state.REVOKES_MAX):\r\n    if n != worker_state.revoked.maxlen:\r\n        prev_max_len = worker_state.revoked.maxlen\r\n        worker_state.revoked.maxlen = n\r\n        if n < prev_max_len:\r\n            worker_state.revoked.purge()\r\n    return {'ok': 'updated revoked task max length.'}\r\n\r\n\r\n@control_command(\r\n    args=[('n', int)],\r\n    signature='[N={}]'.format(worker_state.REVOKE_EXPIRES),  # <- used for help on the command-line.\r\n)\r\ndef revokes_expires(state, n=worker_state.REVOKE_EXPIRES):\r\n    if n != worker_state.revoked.expires:\r\n        prev_expires = worker_state.revoked.expires\r\n        worker_state.revoked.expires = n\r\n        if n < prev_expires:\r\n            worker_state.revoked.purge()\r\n    return {'ok': 'updated revoked task expiration.'}\r\n```\r\n\r\nOf course, _not_ having to call control commands right after a deploy of workers comes up, and instead make it configurable like many other parameters in the conf, would be ideal. But, this is a stop-gap (and also allows it to be changed on the fly, which is at least minorly useful).",
        "created_at": "2022-07-05T02:46:40Z",
        "version": "5.3",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "t/unit/worker/test_state.py"
        ]
    },
    {
        "repo": "celery/celery",
        "pull_number": 7608,
        "instance_id": "celery__celery-7608",
        "issue_numbers": [
            "5836"
        ],
        "base_commit": "ec3714edf37e773ca5372f71f7f4ee5b1b33dd5d",
        "patch": "diff --git a/celery/apps/beat.py b/celery/apps/beat.py\nindex 8652c62730a..dbed1ed442f 100644\n--- a/celery/apps/beat.py\n+++ b/celery/apps/beat.py\n@@ -44,7 +44,8 @@ def __init__(self, max_interval=None, app=None,\n                  scheduler=None,\n                  scheduler_cls=None,  # XXX use scheduler\n                  redirect_stdouts=None,\n-                 redirect_stdouts_level=None, **kwargs):\n+                 redirect_stdouts_level=None,\n+                 quiet=False, **kwargs):\n         self.app = app = app or self.app\n         either = self.app.either\n         self.loglevel = loglevel\n@@ -56,6 +57,7 @@ def __init__(self, max_interval=None, app=None,\n             'worker_redirect_stdouts', redirect_stdouts)\n         self.redirect_stdouts_level = either(\n             'worker_redirect_stdouts_level', redirect_stdouts_level)\n+        self.quiet = quiet\n \n         self.max_interval = max_interval\n         self.socket_timeout = socket_timeout\n@@ -70,8 +72,9 @@ def __init__(self, max_interval=None, app=None,\n             self.loglevel = LOG_LEVELS[self.loglevel.upper()]\n \n     def run(self):\n-        print(str(self.colored.cyan(\n-            f'celery beat v{VERSION_BANNER} is starting.')))\n+        if not self.quiet:\n+            print(str(self.colored.cyan(\n+                f'celery beat v{VERSION_BANNER} is starting.')))\n         self.init_loader()\n         self.set_process_title()\n         self.start_scheduler()\n@@ -93,7 +96,8 @@ def start_scheduler(self):\n             schedule_filename=self.schedule,\n         )\n \n-        print(self.banner(service))\n+        if not self.quiet:\n+            print(self.banner(service))\n \n         self.setup_logging()\n         if self.socket_timeout:\ndiff --git a/celery/bin/beat.py b/celery/bin/beat.py\nindex 9fcdc760794..c8a8a499b51 100644\n--- a/celery/bin/beat.py\n+++ b/celery/bin/beat.py\n@@ -62,7 +62,8 @@ def beat(ctx, detach=False, logfile=None, pidfile=None, uid=None,\n         maybe_drop_privileges(uid=uid, gid=gid)\n \n     beat = partial(app.Beat,\n-                   logfile=logfile, pidfile=pidfile, **kwargs)\n+                   logfile=logfile, pidfile=pidfile,\n+                   quiet=ctx.obj.quiet, **kwargs)\n \n     if detach:\n         with detached(logfile, pidfile, uid, gid, umask, workdir):\ndiff --git a/t/unit/bin/proj/scheduler.py b/t/unit/bin/proj/scheduler.py\nnew file mode 100644\nindex 00000000000..089b4e0eaf1\n--- /dev/null\n+++ b/t/unit/bin/proj/scheduler.py\n@@ -0,0 +1,6 @@\n+from celery.beat import Scheduler\n+\n+\n+class mScheduler(Scheduler):\n+    def tick(self):\n+        raise Exception\n",
        "test_patch": "diff --git a/t/unit/bin/test_beat.py b/t/unit/bin/test_beat.py\nnew file mode 100644\nindex 00000000000..cd401ee7620\n--- /dev/null\n+++ b/t/unit/bin/test_beat.py\n@@ -0,0 +1,34 @@\n+import pytest\n+from click.testing import CliRunner\n+\n+from celery.app.log import Logging\n+from celery.bin.celery import celery\n+\n+\n+@pytest.fixture(scope='session')\n+def use_celery_app_trap():\n+    return False\n+\n+\n+def test_cli(isolated_cli_runner: CliRunner):\n+    Logging._setup = True  # To avoid hitting the logging sanity checks\n+    res = isolated_cli_runner.invoke(\n+        celery,\n+        [\"-A\", \"t.unit.bin.proj.app\", \"beat\", \"-S\", \"t.unit.bin.proj.scheduler.mScheduler\"],\n+        catch_exceptions=True\n+    )\n+    assert res.exit_code == 1, (res, res.stdout)\n+    assert res.stdout.startswith(\"celery beat\")\n+    assert \"Configuration ->\" in res.stdout\n+\n+\n+def test_cli_quiet(isolated_cli_runner: CliRunner):\n+    Logging._setup = True  # To avoid hitting the logging sanity checks\n+    res = isolated_cli_runner.invoke(\n+        celery,\n+        [\"-A\", \"t.unit.bin.proj.app\", \"--quiet\", \"beat\", \"-S\", \"t.unit.bin.proj.scheduler.mScheduler\"],\n+        catch_exceptions=True\n+    )\n+    assert res.exit_code == 1, (res, res.stdout)\n+    assert not res.stdout.startswith(\"celery beat\")\n+    assert \"Configuration -> \" not in res.stdout\n",
        "problem_statement": "celery worker has --quiet to suppress banner output but celery beat does not\nSorry for leaving out the issue template but I believe this is fairly trivial and straight-forward.\n\nIn master, there is code to enable the suppressing of printing the banner when running `celery worker` with `--quiet`:\nhttps://github.com/celery/celery/blob/9773eba837982c84380c93bd3788470273e7674d/celery/apps/worker.py#L138-L139\n\nThis conditional is not present in the code that runs `celery beat`:\nhttps://github.com/celery/celery/blob/9773eba837982c84380c93bd3788470273e7674d/celery/apps/beat.py#L77-L78\nhttps://github.com/celery/celery/blob/9773eba837982c84380c93bd3788470273e7674d/celery/apps/beat.py#L100\n\nThis causes a few issues for us because we expect all our services to only emit JSON.\n\n\n",
        "hints_text": "do you have the time to send improvements?\nI do, with a little guidance. Do I just need to add the `quiet` kwarg to `on_before_init` in `beat.py`? Or is there something more I need to do to make sure that I can figure out whether `--quiet` was passed or not?\nping :)\n> I do, with a little guidance. Do I just need to add the `quiet` kwarg to `on_before_init` in `beat.py`? Or is there something more I need to do to make sure that I can figure out whether `--quiet` was passed or not?\r\n\r\nyou really need to dig the code to figure out that",
        "created_at": "2022-07-05T02:45:37Z",
        "version": "5.3",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": []
    },
        {
        "repo": "celery/celery",
        "pull_number": 6614,
        "instance_id": "celery__celery-6614",
        "issue_numbers": [
            "6476"
        ],
        "base_commit": "29eda054555fa95c83210e5e6bc3e839c80bcd3b",
        "patch": "diff --git a/CONTRIBUTORS.txt b/CONTRIBUTORS.txt\nindex 2e27e625d43..7cf4b9a60bb 100644\n--- a/CONTRIBUTORS.txt\n+++ b/CONTRIBUTORS.txt\n@@ -279,3 +279,4 @@ Sardorbek Imomaliev, 2020/01/24\n Maksym Shalenyi, 2020/07/30\n Frazer McLean, 2020/09/29\n Henrik Bru\u00e5sdal, 2020/11/29\n+Tom Wojcik, 2021/01/24\ndiff --git a/celery/app/defaults.py b/celery/app/defaults.py\nindex 9fec8472c96..51e1e2f96c1 100644\n--- a/celery/app/defaults.py\n+++ b/celery/app/defaults.py\n@@ -255,6 +255,7 @@ def __repr__(self):\n             False, type='bool', old={'celery_eager_propagates_exceptions'},\n         ),\n         ignore_result=Option(False, type='bool'),\n+        store_eager_result=Option(False, type='bool'),\n         protocol=Option(2, type='int', old={'celery_task_protocol'}),\n         publish_retry=Option(\n             True, type='bool', old={'celery_task_publish_retry'},\ndiff --git a/celery/app/task.py b/celery/app/task.py\nindex 2265ebb9e67..5634c442152 100644\n--- a/celery/app/task.py\n+++ b/celery/app/task.py\n@@ -309,6 +309,7 @@ class Task:\n         ('acks_on_failure_or_timeout', 'task_acks_on_failure_or_timeout'),\n         ('reject_on_worker_lost', 'task_reject_on_worker_lost'),\n         ('ignore_result', 'task_ignore_result'),\n+        ('store_eager_result', 'task_store_eager_result'),\n         ('store_errors_even_if_ignored', 'task_store_errors_even_if_ignored'),\n     )\n \ndiff --git a/celery/app/trace.py b/celery/app/trace.py\nindex f9b8c83e6e6..4d334069bb3 100644\n--- a/celery/app/trace.py\n+++ b/celery/app/trace.py\n@@ -159,9 +159,13 @@ def __init__(self, state, retval=None):\n \n     def handle_error_state(self, task, req,\n                            eager=False, call_errbacks=True):\n-        store_errors = not eager\n         if task.ignore_result:\n             store_errors = task.store_errors_even_if_ignored\n+        elif eager and task.store_eager_result:\n+            store_errors = True\n+        else:\n+            store_errors = not eager\n+\n         return {\n             RETRY: self.handle_retry,\n             FAILURE: self.handle_failure,\n@@ -316,7 +320,13 @@ def build_tracer(name, task, loader=None, hostname=None, store_errors=True,\n     ignore_result = task.ignore_result\n     track_started = task.track_started\n     track_started = not eager and (task.track_started and not ignore_result)\n-    publish_result = not eager and not ignore_result\n+\n+    # #6476\n+    if eager and not ignore_result and task.store_eager_result:\n+        publish_result = True\n+    else:\n+        publish_result = not eager and not ignore_result\n+\n     hostname = hostname or gethostname()\n     inherit_parent_priority = app.conf.task_inherit_parent_priority\n \ndiff --git a/docs/userguide/configuration.rst b/docs/userguide/configuration.rst\nindex 01e8b7784e7..6e9c600b5f2 100644\n--- a/docs/userguide/configuration.rst\n+++ b/docs/userguide/configuration.rst\n@@ -426,6 +426,23 @@ propagate exceptions.\n \n It's the same as always running ``apply()`` with ``throw=True``.\n \n+.. setting:: task_store_eager_result\n+\n+``task_store_eager_result``\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+.. versionadded:: 5.1\n+\n+Default: Disabled.\n+\n+If this is :const:`True` and :setting:`task_always_eager` is :const:`True`\n+and :setting:`task_ignore_result` is :const:`False`,\n+the results of eagerly executed tasks will be saved to the backend.\n+\n+By default, even with :setting:`task_always_eager` set to :const:`True`\n+and :setting:`task_ignore_result` set to :const:`False`,\n+the result will not be saved.\n+\n .. setting:: task_remote_tracebacks\n \n ``task_remote_tracebacks``\n",
        "test_patch": "diff --git a/docs/userguide/testing.rst b/docs/userguide/testing.rst\nindex 94389c30739..62db0a21e41 100644\n--- a/docs/userguide/testing.rst\n+++ b/docs/userguide/testing.rst\n@@ -18,6 +18,9 @@ To test task behavior in unit tests the preferred method is mocking.\n     of what happens in a worker, and there are many discrepancies\n     between the emulation and what happens in reality.\n \n+    Note that eagerly executed tasks don't write results to backend by default.\n+    If you want to enable this functionality, have a look at :setting:`task_store_eager_result`.\n+\n A Celery task is much like a web view, in that it should only\n define how to perform the action in the context of being called as a task.\n \ndiff --git a/t/unit/tasks/test_trace.py b/t/unit/tasks/test_trace.py\nindex 3d7061acea5..15f5e455c62 100644\n--- a/t/unit/tasks/test_trace.py\n+++ b/t/unit/tasks/test_trace.py\n@@ -1,4 +1,4 @@\n-from unittest.mock import Mock, patch\n+from unittest.mock import ANY, Mock, patch\n \n import pytest\n from billiard.einfo import ExceptionInfo\n@@ -148,6 +148,75 @@ def add(x, y):\n         with pytest.raises(MemoryError):\n             self.trace(add, (2, 2), {}, eager=False)\n \n+    def test_eager_task_does_not_store_result_even_if_not_ignore_result(self):\n+        @self.app.task(shared=False)\n+        def add(x, y):\n+            return x + y\n+\n+        add.backend = Mock(name='backend')\n+        add.ignore_result = False\n+\n+        self.trace(add, (2, 2), {}, eager=True)\n+\n+        add.backend.mark_as_done.assert_called_once_with(\n+            'id-1',     # task_id\n+            4,          # result\n+            ANY,        # request\n+            False       # store_result\n+        )\n+\n+    def test_eager_task_does_not_call_store_result(self):\n+        @self.app.task(shared=False)\n+        def add(x, y):\n+            return x + y\n+\n+        backend = BaseDictBackend(app=self.app)\n+        backend.store_result = Mock()\n+        add.backend = backend\n+        add.ignore_result = False\n+\n+        self.trace(add, (2, 2), {}, eager=True)\n+\n+        add.backend.store_result.assert_not_called()\n+\n+    def test_eager_task_will_store_result_if_proper_setting_is_set(self):\n+        @self.app.task(shared=False)\n+        def add(x, y):\n+            return x + y\n+\n+        add.backend = Mock(name='backend')\n+        add.store_eager_result = True\n+        add.ignore_result = False\n+\n+        self.trace(add, (2, 2), {}, eager=True)\n+\n+        add.backend.mark_as_done.assert_called_once_with(\n+            'id-1',     # task_id\n+            4,          # result\n+            ANY,        # request\n+            True        # store_result\n+        )\n+\n+    def test_eager_task_with_setting_will_call_store_result(self):\n+        @self.app.task(shared=False)\n+        def add(x, y):\n+            return x + y\n+\n+        backend = BaseDictBackend(app=self.app)\n+        backend.store_result = Mock()\n+        add.backend = backend\n+        add.store_eager_result = True\n+        add.ignore_result = False\n+\n+        self.trace(add, (2, 2), {}, eager=True)\n+\n+        add.backend.store_result.assert_called_once_with(\n+            'id-1',\n+            4,\n+            states.SUCCESS,\n+            request=ANY\n+        )\n+\n     def test_when_backend_raises_exception(self):\n         @self.app.task(shared=False)\n         def add(x, y):\n@@ -413,6 +482,32 @@ def test_handle_error_state(self):\n             call_errbacks=True,\n         )\n \n+    def test_handle_error_state_for_eager_task(self):\n+        x = self.TI(states.FAILURE)\n+        x.handle_failure = Mock()\n+\n+        x.handle_error_state(self.add, self.add.request, eager=True)\n+        x.handle_failure.assert_called_once_with(\n+            self.add,\n+            self.add.request,\n+            store_errors=False,\n+            call_errbacks=True,\n+        )\n+\n+    def test_handle_error_for_eager_saved_to_backend(self):\n+        x = self.TI(states.FAILURE)\n+        x.handle_failure = Mock()\n+\n+        self.add.store_eager_result = True\n+\n+        x.handle_error_state(self.add, self.add.request, eager=True)\n+        x.handle_failure.assert_called_with(\n+            self.add,\n+            self.add.request,\n+            store_errors=True,\n+            call_errbacks=True,\n+        )\n+\n     @patch('celery.app.trace.ExceptionInfo')\n     def test_handle_reject(self, ExceptionInfo):\n         x = self.TI(states.FAILURE)\n",
        "problem_statement": "`publish_result` is False when tasks are eager and and we DON'T want to ignore results\n #### Mandatory Debugging Information\r\n\r\n- [ ] I have included the output of ``celery -A proj report`` in the issue.\r\n    (if you are not able to do this, then at least specify the Celery\r\n     version affected).\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n- [ ] I have included the contents of ``pip freeze`` in the issue.\r\n- [x] I have included all the versions of all the external dependencies required\r\n  to reproduce this bug.\r\n\r\n#### Related Issues\r\n\r\n- #5398 \r\n- https://github.com/celery/django-celery-results/issues/49\r\n\r\n#### Possible Duplicates\r\n\r\n- None\r\n\r\n### Python Packages\r\n\r\nLatest, not released django-celery-results\r\nhttps://github.com/celery/django-celery-results/pull/172/commits/b4acbf24ea3521ecfdbf821ece7ca62e49ff2765\r\nCurrently published version doesn't support Celery 5.x, but all versions are affected\r\n\r\n\r\n## Minimally Reproducible Test Case\r\n\r\nSource of the issue\r\nhttps://github.com/celery/celery/blob/2a6c7cfe3b1283961887bf1cb3f5aa6c8aa70820/celery/app/trace.py#L323\r\n\r\n```python\r\nimport pytest\r\n\r\n@pytest.mark.parametrize(\r\n    \"eager,ignore_result,publish_result\",\r\n    [\r\n        (True, True, False),\r\n        (True, False, True),  # fails\r\n        (False, True, False),\r\n        (False, False, True),\r\n    ]\r\n\r\n)\r\ndef test_publish_result_current(eager, ignore_result, publish_result):\r\n    actual_publish_result = not eager and not ignore_result\r\n    assert publish_result == actual_publish_result\r\n\r\n\r\n@pytest.mark.parametrize(\r\n    \"eager,ignore_result,publish_result\",\r\n    [\r\n        (True, True, False),\r\n        (True, False, True),\r\n        (False, True, False),\r\n        (False, False, True),\r\n    ]\r\n\r\n)\r\ndef test_publish_result_desired(eager, ignore_result, publish_result):\r\n    actual_publish_result = not ignore_result\r\n    assert publish_result == actual_publish_result\r\n```\r\n\r\n# Expected Behavior\r\n\r\n``always_eager`` exists so we can use Celery in unit tests. I want to execute a task eagerly and later on check result from the backend. \r\n\r\n# Actual Behavior\r\nWhen task is eager and it's set NOT to ignore the result, the result will be ignored.\n",
        "hints_text": "did you try the latest release & share your feedback?\nit would be great if you can share a possible solution\nI tried the latest release.\r\n\r\nIt appears to me that someone made the decision that whenever a task is eager, nothing should be saved to the backend. But not only I struggle to understand what would be the purpose of this feature but also it's not documented anywhere (AFAIK). So could you please confirm that the current behavior is not the desired behavior?\r\n\r\nOnce you confirm I'll be happy to fix it.\n> I tried the latest release.\r\n> \r\n> It appears to me that someone made the decision that whenever a task is eager, nothing should be saved to the backend. But not only I struggle to understand what would be the purpose of this feature but also it's not documented anywhere (AFAIK). So could you please confirm that the current behavior is not the desired behavior?\r\n> \r\n> Once you confirm I'll be happy to fix it.\r\n\r\nbased on the issues raised https://github.com/celery/django-celery-results/issues/49 I believe we should save the results, however, we should carefully check the possible side effects.\r\n@tomwojcik still up for this?\nYes @auvipy . I already tried to fix this but some tests started to fail due to some side effects but I think some are to be expected. I will give it another try over the weekend and let you know if I won't manage to fix this myself.\nI personally, haven't used eager execution of the task. But if I understand it correctly that it execute task locally without sending it to the queue, it makes sense to me that the result is not stored to result storage (since it is executed directly by the caller). But still haven't used it so maybe there is need for storing it in result store.\n> Yes @auvipy . I already tried to fix this but some tests started to fail due to some side effects but I think some are to be expected. I will give it another try over the weekend and let you know if I won't manage to fix this myself.\r\n\r\ndon't hesitate to come with A draft PR as well. so that if you are not sure about something, we could share our insight with you.",
        "created_at": "2021-01-24T19:53:14Z",
        "version": "5.0",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "docs/userguide/testing.rst",
            "t/unit/tasks/test_trace.py"
        ]
    },
    {
        "repo": "celery/celery",
        "pull_number": 6440,
        "instance_id": "celery__celery-6440",
        "issue_numbers": [
            "6361",
            "6361"
        ],
        "base_commit": "a2498d37aa40614a2eecb3dddcae61754056b5c9",
        "patch": "diff --git a/celery/app/amqp.py b/celery/app/amqp.py\nindex 7031bc8b9b6..1a0454e9a92 100644\n--- a/celery/app/amqp.py\n+++ b/celery/app/amqp.py\n@@ -46,7 +46,6 @@ class Queues(dict):\n         create_missing (bool): By default any unknown queues will be\n             added automatically, but if this flag is disabled the occurrence\n             of unknown queues in `wanted` will raise :exc:`KeyError`.\n-        ha_policy (Sequence, str): Default HA policy for queues with none set.\n         max_priority (int): Default x-max-priority for queues with none set.\n     \"\"\"\n \n@@ -55,14 +54,13 @@ class Queues(dict):\n     _consume_from = None\n \n     def __init__(self, queues=None, default_exchange=None,\n-                 create_missing=True, ha_policy=None, autoexchange=None,\n+                 create_missing=True, autoexchange=None,\n                  max_priority=None, default_routing_key=None):\n         dict.__init__(self)\n         self.aliases = WeakValueDictionary()\n         self.default_exchange = default_exchange\n         self.default_routing_key = default_routing_key\n         self.create_missing = create_missing\n-        self.ha_policy = ha_policy\n         self.autoexchange = Exchange if autoexchange is None else autoexchange\n         self.max_priority = max_priority\n         if queues is not None and not isinstance(queues, Mapping):\n@@ -122,10 +120,6 @@ def _add(self, queue):\n             queue.exchange = self.default_exchange\n         if not queue.routing_key:\n             queue.routing_key = self.default_routing_key\n-        if self.ha_policy:\n-            if queue.queue_arguments is None:\n-                queue.queue_arguments = {}\n-            self._set_ha_policy(queue.queue_arguments)\n         if self.max_priority is not None:\n             if queue.queue_arguments is None:\n                 queue.queue_arguments = {}\n@@ -133,13 +127,6 @@ def _add(self, queue):\n         self[queue.name] = queue\n         return queue\n \n-    def _set_ha_policy(self, args):\n-        policy = self.ha_policy\n-        if isinstance(policy, (list, tuple)):\n-            return args.update({'ha-mode': 'nodes',\n-                                'ha-params': list(policy)})\n-        args['ha-mode'] = policy\n-\n     def _set_max_priority(self, args):\n         if 'x-max-priority' not in args and self.max_priority is not None:\n             return args.update({'x-max-priority': self.max_priority})\n@@ -251,7 +238,7 @@ def create_task_message(self):\n     def send_task_message(self):\n         return self._create_task_sender()\n \n-    def Queues(self, queues, create_missing=None, ha_policy=None,\n+    def Queues(self, queues, create_missing=None,\n                autoexchange=None, max_priority=None):\n         # Create new :class:`Queues` instance, using queue defaults\n         # from the current configuration.\n@@ -259,8 +246,6 @@ def Queues(self, queues, create_missing=None, ha_policy=None,\n         default_routing_key = conf.task_default_routing_key\n         if create_missing is None:\n             create_missing = conf.task_create_missing_queues\n-        if ha_policy is None:\n-            ha_policy = conf.task_queue_ha_policy\n         if max_priority is None:\n             max_priority = conf.task_queue_max_priority\n         if not queues and conf.task_default_queue:\n@@ -271,7 +256,7 @@ def Queues(self, queues, create_missing=None, ha_policy=None,\n                         else autoexchange)\n         return self.queues_cls(\n             queues, self.default_exchange, create_missing,\n-            ha_policy, autoexchange, max_priority, default_routing_key,\n+            autoexchange, max_priority, default_routing_key,\n         )\n \n     def Router(self, queues=None, create_missing=None):\ndiff --git a/celery/app/defaults.py b/celery/app/defaults.py\nindex d0fa9d20b54..9fec8472c96 100644\n--- a/celery/app/defaults.py\n+++ b/celery/app/defaults.py\n@@ -267,7 +267,6 @@ def __repr__(self):\n             type='dict', old={'celery_task_publish_retry_policy'},\n         ),\n         queues=Option(type='dict'),\n-        queue_ha_policy=Option(None, type='string'),\n         queue_max_priority=Option(None, type='int'),\n         reject_on_worker_lost=Option(type='bool'),\n         remote_tracebacks=Option(False, type='bool'),\ndiff --git a/docs/userguide/configuration.rst b/docs/userguide/configuration.rst\nindex 5331c4b9a58..e9c1c76c151 100644\n--- a/docs/userguide/configuration.rst\n+++ b/docs/userguide/configuration.rst\n@@ -2100,33 +2100,6 @@ The final routing options for ``tasks.add`` will become:\n \n See :ref:`routers` for more examples.\n \n-.. setting:: task_queue_ha_policy\n-\n-``task_queue_ha_policy``\n-~~~~~~~~~~~~~~~~~~~~~~~~\n-:brokers: RabbitMQ\n-\n-Default: :const:`None`.\n-\n-This will set the default HA policy for a queue, and the value\n-can either be a string (usually ``all``):\n-\n-.. code-block:: python\n-\n-    task_queue_ha_policy = 'all'\n-\n-Using 'all' will replicate the queue to all current nodes,\n-Or you can give it a list of nodes to replicate to:\n-\n-.. code-block:: python\n-\n-    task_queue_ha_policy = ['rabbit@host1', 'rabbit@host2']\n-\n-Using a list will implicitly set ``ha-mode`` to 'nodes' and\n-``ha-params`` to the given list of nodes.\n-\n-See http://www.rabbitmq.com/ha.html for more information.\n-\n .. setting:: task_queue_max_priority\n \n ``task_queue_max_priority``\n",
        "test_patch": "diff --git a/t/unit/app/test_amqp.py b/t/unit/app/test_amqp.py\nindex ee36c08e235..bc2d26d3680 100644\n--- a/t/unit/app/test_amqp.py\n+++ b/t/unit/app/test_amqp.py\n@@ -89,23 +89,6 @@ def test_setitem_adds_default_exchange(self):\n         q['foo'] = queue\n         assert q['foo'].exchange == q.default_exchange\n \n-    @pytest.mark.parametrize('ha_policy,qname,q,qargs,expected', [\n-        (None, 'xyz', 'xyz', None, None),\n-        (None, 'xyz', 'xyz', {'x-foo': 'bar'}, {'x-foo': 'bar'}),\n-        ('all', 'foo', Queue('foo'), None, {'ha-mode': 'all'}),\n-        ('all', 'xyx2',\n-         Queue('xyx2', queue_arguments={'x-foo': 'bar'}),\n-         None,\n-         {'ha-mode': 'all', 'x-foo': 'bar'}),\n-        (['A', 'B', 'C'], 'foo', Queue('foo'), None, {\n-            'ha-mode': 'nodes',\n-            'ha-params': ['A', 'B', 'C']}),\n-    ])\n-    def test_with_ha_policy(self, ha_policy, qname, q, qargs, expected):\n-        queues = Queues(ha_policy=ha_policy, create_missing=False)\n-        queues.add(q, queue_arguments=qargs)\n-        assert queues[qname].queue_arguments == expected\n-\n     def test_select_add(self):\n         q = Queues()\n         q.select(['foo', 'bar'])\n@@ -118,11 +101,6 @@ def test_deselect(self):\n         q.deselect('bar')\n         assert sorted(q._consume_from.keys()) == ['foo']\n \n-    def test_with_ha_policy_compat(self):\n-        q = Queues(ha_policy='all')\n-        q.add('bar')\n-        assert q['bar'].queue_arguments == {'ha-mode': 'all'}\n-\n     def test_add_default_exchange(self):\n         ex = Exchange('fff', 'fanout')\n         q = Queues(default_exchange=ex)\n@@ -143,12 +121,6 @@ def test_alias(self):\n         ({'max_priority': 10},\n          'moo', Queue('moo', queue_arguments=None),\n          {'x-max-priority': 10}),\n-        ({'ha_policy': 'all', 'max_priority': 5},\n-         'bar', 'bar',\n-         {'ha-mode': 'all', 'x-max-priority': 5}),\n-        ({'ha_policy': 'all', 'max_priority': 5},\n-         'xyx2', Queue('xyx2', queue_arguments={'x-max-priority': 2}),\n-         {'ha-mode': 'all', 'x-max-priority': 2}),\n         ({'max_priority': None},\n          'foo2', 'foo2',\n          None),\n@@ -255,10 +227,6 @@ def test_countdown_negative(self):\n         with pytest.raises(ValueError):\n             self.app.amqp.as_task_v2(uuid(), 'foo', countdown=-1232132323123)\n \n-    def test_Queues__with_ha_policy(self):\n-        x = self.app.amqp.Queues({}, ha_policy='all')\n-        assert x.ha_policy == 'all'\n-\n     def test_Queues__with_max_priority(self):\n         x = self.app.amqp.Queues({}, max_priority=23)\n         assert x.max_priority == 23\n",
        "problem_statement": "RabbitMQ task_queue_ha_policy has no effect\n<!--\r\nPlease fill this template entirely and do not erase parts of it.\r\nWe reserve the right to close without a response\r\nbug reports which are incomplete.\r\n-->\r\n# Checklist\r\n<!--\r\nTo check an item on the list replace [ ] with [x].\r\n-->\r\n\r\n- [x] I have checked the [issues list](https://github.com/celery/celery/issues?utf8=%E2%9C%93&q=is%3Aissue+label%3A%22Category%3A+Documentation%22+)\r\n  for similar or identical bug reports.\r\n- [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22Category%3A+Documentation%22)\r\n  for existing proposed fixes.\r\n- [x] I have checked the [commit log](https://github.com/celery/celery/commits/master)\r\n  to find out if the bug was already fixed in the master branch.\r\n- [x] I have included all related issues and possible duplicate issues in this issue\r\n      (If there are none, check this box anyway).\r\n\r\n## Related Issues and Possible Duplicates\r\n<!--\r\nPlease make sure to search and mention any related issues\r\nor possible duplicates to this issue as requested by the checklist above.\r\n\r\nThis may or may not include issues in other repositories that the Celery project\r\nmaintains or other repositories that are dependencies of Celery.\r\n\r\nIf you don't know how to mention issues, please refer to Github's documentation\r\non the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests\r\n-->\r\n\r\n#### Related Issues\r\n\r\n- None\r\n\r\n#### Possible Duplicates\r\n\r\n- None\r\n\r\n# Description\r\n\r\nThe RabbitMQ setting [task_queue_ha_policy](https://docs.celeryproject.org/en/stable/userguide/configuration.html#task-queue-ha-policy) no longer has any affect, as [described in the release notes for version 3](https://www.rabbitmq.com/blog/2012/11/19/breaking-things-with-rabbitmq-3-0/).\r\n\r\nThis was [reported to the mailing list here](https://groups.google.com/u/1/g/celery-users/c/hiZPiz2JWo8/m/2_q_Q5sM0BIJ) (in 2013!) and the answer then was that it's deprecated and will be removed.\r\n\r\n# Suggestions\r\n\r\nRemove the configuration parameter and maybe reference the blog post and/or docs on how to set a policy.\r\n\nRabbitMQ task_queue_ha_policy has no effect\n<!--\r\nPlease fill this template entirely and do not erase parts of it.\r\nWe reserve the right to close without a response\r\nbug reports which are incomplete.\r\n-->\r\n# Checklist\r\n<!--\r\nTo check an item on the list replace [ ] with [x].\r\n-->\r\n\r\n- [x] I have checked the [issues list](https://github.com/celery/celery/issues?utf8=%E2%9C%93&q=is%3Aissue+label%3A%22Category%3A+Documentation%22+)\r\n  for similar or identical bug reports.\r\n- [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22Category%3A+Documentation%22)\r\n  for existing proposed fixes.\r\n- [x] I have checked the [commit log](https://github.com/celery/celery/commits/master)\r\n  to find out if the bug was already fixed in the master branch.\r\n- [x] I have included all related issues and possible duplicate issues in this issue\r\n      (If there are none, check this box anyway).\r\n\r\n## Related Issues and Possible Duplicates\r\n<!--\r\nPlease make sure to search and mention any related issues\r\nor possible duplicates to this issue as requested by the checklist above.\r\n\r\nThis may or may not include issues in other repositories that the Celery project\r\nmaintains or other repositories that are dependencies of Celery.\r\n\r\nIf you don't know how to mention issues, please refer to Github's documentation\r\non the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests\r\n-->\r\n\r\n#### Related Issues\r\n\r\n- None\r\n\r\n#### Possible Duplicates\r\n\r\n- None\r\n\r\n# Description\r\n\r\nThe RabbitMQ setting [task_queue_ha_policy](https://docs.celeryproject.org/en/stable/userguide/configuration.html#task-queue-ha-policy) no longer has any affect, as [described in the release notes for version 3](https://www.rabbitmq.com/blog/2012/11/19/breaking-things-with-rabbitmq-3-0/).\r\n\r\nThis was [reported to the mailing list here](https://groups.google.com/u/1/g/celery-users/c/hiZPiz2JWo8/m/2_q_Q5sM0BIJ) (in 2013!) and the answer then was that it's deprecated and will be removed.\r\n\r\n# Suggestions\r\n\r\nRemove the configuration parameter and maybe reference the blog post and/or docs on how to set a policy.\r\n\n",
        "hints_text": "contributions are wellcome\n@auvipy @safwanrahman The fix that has been merged here does not address the problem.\r\n\r\nThe [RabbitMQ release notes](https://www.rabbitmq.com/blog/2012/11/19/breaking-things-with-rabbitmq-3-0/) describe how **it is no longer possible** to configure HA at queue declaration time. You have to use a policy, which is set on the broker and cannot be controlled by the client.\r\n\r\n\r\n\r\n\ncontributions are wellcome\n@auvipy @safwanrahman The fix that has been merged here does not address the problem.\r\n\r\nThe [RabbitMQ release notes](https://www.rabbitmq.com/blog/2012/11/19/breaking-things-with-rabbitmq-3-0/) describe how **it is no longer possible** to configure HA at queue declaration time. You have to use a policy, which is set on the broker and cannot be controlled by the client.\r\n\r\n\r\n\r\n",
        "created_at": "2020-10-26T20:45:36Z",
        "version": "5.0",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "t/unit/app/test_amqp.py"
        ]
    },
    {
        "repo": "celery/celery",
        "pull_number": 6264,
        "instance_id": "celery__celery-6264",
        "issue_numbers": [
            "4412"
        ],
        "base_commit": "6f514ce7b2f0ce586e183e5dfa59032da03c97dc",
        "patch": "diff --git a/celery/__init__.py b/celery/__init__.py\nindex d249e49278b..6942be1c38e 100644\n--- a/celery/__init__.py\n+++ b/celery/__init__.py\n@@ -113,15 +113,16 @@ def _patch_eventlet():\n \n \n def _patch_gevent():\n-    import gevent\n-    from gevent import monkey, signal as gevent_signal\n+    import gevent.monkey\n+    import gevent.signal\n \n-    monkey.patch_all()\n+    gevent.monkey.patch_all()\n     if gevent.version_info[0] == 0:  # pragma: no cover\n         # Signals aren't working in gevent versions <1.0,\n         # and aren't monkey patched by patch_all()\n-        _signal = __import__('signal')\n-        _signal.signal = gevent_signal\n+        import signal\n+\n+        signal.signal = gevent.signal\n \n \n def maybe_patch_concurrency(argv=None, short_opts=None,\ndiff --git a/celery/backends/asynchronous.py b/celery/backends/asynchronous.py\nindex 98eea0d7ab2..13000870a87 100644\n--- a/celery/backends/asynchronous.py\n+++ b/celery/backends/asynchronous.py\n@@ -112,9 +112,9 @@ def wait_for(self, p, wait, timeout=None):\n class geventDrainer(greenletDrainer):\n \n     def spawn(self, func):\n-        from gevent import spawn, sleep\n-        g = spawn(func)\n-        sleep(0)\n+        import gevent\n+        g = gevent.spawn(func)\n+        gevent.sleep(0)\n         return g\n \n     def wait_for(self, p, wait, timeout=None):\ndiff --git a/celery/backends/redis.py b/celery/backends/redis.py\nindex 9c635ccde0c..9413c70f39a 100644\n--- a/celery/backends/redis.py\n+++ b/celery/backends/redis.py\n@@ -31,7 +31,6 @@\n     from urlparse import unquote\n \n try:\n-    import redis\n     import redis.connection\n     from kombu.transport.redis import get_redis_error_classes\n except ImportError:  # pragma: no cover\n@@ -39,9 +38,9 @@\n     get_redis_error_classes = None  # noqa\n \n try:\n-    from redis import sentinel\n+    import redis.sentinel\n except ImportError:\n-    sentinel = None\n+    pass\n \n __all__ = ('RedisBackend', 'SentinelBackend')\n \n@@ -519,7 +518,7 @@ def password(self):\n class SentinelBackend(RedisBackend):\n     \"\"\"Redis sentinel task result store.\"\"\"\n \n-    sentinel = sentinel\n+    sentinel = getattr(redis, \"sentinel\", None)\n \n     def __init__(self, *args, **kwargs):\n         if self.sentinel is None:\ndiff --git a/celery/backends/riak.py b/celery/backends/riak.py\nindex 4c5b046a4cb..af0b18fcb91 100644\n--- a/celery/backends/riak.py\n+++ b/celery/backends/riak.py\n@@ -12,11 +12,9 @@\n from .base import KeyValueStoreBackend\n \n try:\n-    import riak\n-    from riak import RiakClient\n-    from riak.resolver import last_written_resolver\n+    import riak.resolver\n except ImportError:  # pragma: no cover\n-    riak = RiakClient = last_written_resolver = None  # noqa\n+    riak = None\n \n __all__ = ('RiakBackend',)\n \n@@ -115,10 +113,12 @@ def __init__(self, host=None, port=None, bucket_name=None, protocol=None,\n     def _get_client(self):\n         \"\"\"Get client connection.\"\"\"\n         if self._client is None or not self._client.is_alive():\n-            self._client = RiakClient(protocol=self.protocol,\n-                                      host=self.host,\n-                                      pb_port=self.port)\n-            self._client.resolver = last_written_resolver\n+            self._client = riak.RiakClient(\n+                protocol=self.protocol,\n+                host=self.host,\n+                pb_port=self.port,\n+            )\n+            self._client.resolver = riak.resolver.last_written_resolver\n         return self._client\n \n     def _get_bucket(self):\n",
        "test_patch": "diff --git a/t/unit/concurrency/test_gevent.py b/t/unit/concurrency/test_gevent.py\nindex 7d0334b95fc..f5fd062fa72 100644\n--- a/t/unit/concurrency/test_gevent.py\n+++ b/t/unit/concurrency/test_gevent.py\n@@ -6,9 +6,10 @@\n \n gevent_modules = (\n     'gevent',\n-    'gevent.monkey',\n     'gevent.greenlet',\n+    'gevent.monkey',\n     'gevent.pool',\n+    'gevent.signal',\n     'greenlet',\n )\n \n",
        "problem_statement": "Request on_timeout should ignore soft time limit exception\nWhen Request.on_timeout receive a soft timeout from billiard, it does the same as if it was receiving a hard time limit exception. This is ran by the controller.\r\n\r\nBut the task may catch this exception and eg. return (this is what soft timeout are for).\r\n\r\nThis cause:\r\n1. the result to be saved once as an exception by the controller (on_timeout) and another time with the result returned by the task\r\n2. the task status to be passed to failure and to success on the same manner\r\n3. if the task is participating to a chord, the chord result counter (at least with redis) is incremented twice (instead of once), making the chord to return prematurely and eventually loose tasks\u2026\r\n\r\n1, 2 and 3 can leads of course to strange race conditions\u2026\r\n\r\n## Steps to reproduce (Illustration)\r\n\r\nwith the program in test_timeout.py:\r\n\r\n```python\r\nimport time\r\nimport celery\r\n\r\n\r\napp = celery.Celery('test_timeout')\r\napp.conf.update(\r\n    result_backend=\"redis://localhost/0\",\r\n    broker_url=\"amqp://celery:celery@localhost:5672/host\",\r\n)\r\n\r\n@app.task(soft_time_limit=1)\r\ndef test():\r\n    try:\r\n        time.sleep(2)\r\n    except Exception:\r\n        return 1\r\n\r\n@app.task()\r\ndef add(args):\r\n    print(\"### adding\", args)\r\n    return sum(args)\r\n\r\n@app.task()\r\ndef on_error(context, exception, traceback, **kwargs):\r\n    print(\"### on_error:\u00a0\", exception)\r\n\r\nif __name__ == \"__main__\":\r\n    result = celery.chord([test.s().set(link_error=on_error.s()), test.s().set(link_error=on_error.s())])(add.s())\r\n    result.get()\r\n```\r\n\r\nstart a worker and the program:\r\n\r\n```\r\n$ celery -A test_timeout worker -l WARNING\r\n$ python3 test_timeout.py\r\n```\r\n\r\n## Expected behavior\r\n\r\nadd method is called with `[1, 1]` as argument and test_timeout.py return normally\r\n\r\n## Actual behavior\r\n\r\nThe test_timeout.py fails, with\r\n```\r\ncelery.backends.base.ChordError: Callback error: ChordError(\"Dependency 15109e05-da43-449f-9081-85d839ac0ef2 raised SoftTimeLimitExceeded('SoftTimeLimitExceeded(True,)',)\",\r\n```\r\nOn the worker side, the **on_error is called but the add method as well !**\r\n\r\n```\r\n[2017-11-29 23:07:25,538: WARNING/MainProcess] Soft time limit (1s) exceeded for test_timeout.test[15109e05-da43-449f-9081-85d839ac0ef2]\r\n[2017-11-29 23:07:25,546: WARNING/MainProcess] ### on_error:\r\n[2017-11-29 23:07:25,546: WARNING/MainProcess] SoftTimeLimitExceeded(True,)\r\n[2017-11-29 23:07:25,547: WARNING/MainProcess] Soft time limit (1s) exceeded for test_timeout.test[38f3f7f2-4a89-4318-8ee9-36a987f73757]\r\n[2017-11-29 23:07:25,553: ERROR/MainProcess] Chord callback for 'ef6d7a38-d1b4-40ad-b937-ffa84e40bb23' raised: ChordError(\"Dependency 15109e05-da43-449f-9081-85d839ac0ef2 raised SoftTimeLimitExceeded('SoftTimeLimitExceeded(True,)',)\",)\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.4/dist-packages/celery/backends/redis.py\", line 290, in on_chord_part_return\r\n    callback.delay([unpack(tup, decode) for tup in resl])\r\n  File \"/usr/local/lib/python3.4/dist-packages/celery/backends/redis.py\", line 290, in <listcomp>\r\n    callback.delay([unpack(tup, decode) for tup in resl])\r\n  File \"/usr/local/lib/python3.4/dist-packages/celery/backends/redis.py\", line 243, in _unpack_chord_result\r\n    raise ChordError('Dependency {0} raised {1!r}'.format(tid, retval))\r\ncelery.exceptions.ChordError: Dependency 15109e05-da43-449f-9081-85d839ac0ef2 raised SoftTimeLimitExceeded('SoftTimeLimitExceeded(True,)',)\r\n[2017-11-29 23:07:25,565: WARNING/MainProcess] ### on_error:\r\n[2017-11-29 23:07:25,565: WARNING/MainProcess] SoftTimeLimitExceeded(True,)\r\n[2017-11-29 23:07:27,262: WARNING/PoolWorker-2] ### adding\r\n[2017-11-29 23:07:27,264: WARNING/PoolWorker-2] [1, 1]\r\n```\r\n\r\nOf course, on purpose did I choose to call the test.s() twice, to show that the count in the chord continues. In fact:\r\n- the chord result is incremented twice by the error of soft time limit\r\n- the chord result is again incremented twice by the correct returning of `test` task\r\n\r\n## Conclusion\r\n\r\nRequest.on_timeout  should not process soft time limit exception. \r\n\r\nhere is a quick monkey patch (correction of celery is trivial)\r\n\r\n```python\r\ndef patch_celery_request_on_timeout():\r\n    from celery.worker import request\r\n    orig = request.Request.on_timeout\r\n    def patched_on_timeout(self, soft, timeout):\r\n        if not soft:\r\n            orig(self, soft, timeout)\r\n    request.Request.on_timeout = patched_on_timeout\r\npatch_celery_request_on_timeout()\r\n```\r\n\r\n\r\n\r\n## version info\r\n\r\nsoftware -> celery:4.1.0 (latentcall) kombu:4.0.2 py:3.4.3\r\n            billiard:3.5.0.2 py-amqp:2.1.4\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:amqp results:redis://10.0.3.253/0\r\n\n",
        "hints_text": "@ask I think this is quite a big problem (with a trivial fix).\r\n\r\nIt requires attention though as it is bringging a new behaviour (but previous behaviour is not well documented, and, in my opinion, the new behaviour was the one expected)\nThis change in behaviour is what kept my team from upgrading to celery 4. Indeed, the chord callback was often not called at all.\r\nI don't know if it is related, but I modified your code sample and it resulted in some `Exception raised outside body` errors and multiple other errors if you try running `python test_timeout.py` multiple times.\r\n\r\nHere is my script:\r\n\r\n```python\r\nimport time\r\nimport celery\r\n\r\n\r\napp = celery.Celery(\r\n    'test_timeout',\r\n    broker='amqp://localhost',\r\n    backend='redis://localhost')\r\n\r\n\r\n@app.task(soft_time_limit=1)\r\ndef test(nb_seconds):\r\n    try:\r\n        time.sleep(nb_seconds)\r\n        return nb_seconds\r\n    except:\r\n        print(\"### error handled\")\r\n        return nb_seconds\r\n\r\n\r\n@app.task()\r\ndef add(args):\r\n    print(\"### adding\", args)\r\n    return sum(args)\r\n\r\n\r\n@app.task()\r\ndef on_error(context, exception, traceback, **kwargs):\r\n    print(\"### on_error: \", exception)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    result = celery.chord([\r\n        test.s(i).set(link_error=on_error.s()) for i in range(0, 2)\r\n    ])(add.s())\r\n    result.get()\r\n```\r\n\r\nNB: if you update the range for range(2, 4) for instance, the `Exception raised outside body` does not seem to happen. It seems this particular issue happens when the `SoftTimeLimitExceeded` is raised exactly during the `return`.\ncould you please send a PR with your proposed fix/workaround on master branch?\nhi @auvipy, I won't have the time until jannuary. I'll need help on how to write the tests also (that's the reason why I didn't propose a PR).\nOK. dont get afraid of sending logical changes just for the reason you don't know how to write test. we will certainly try to help you \nthanks a lot!",
        "created_at": "2020-07-30T11:18:44Z",
        "version": "4.4",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "t/unit/concurrency/test_gevent.py"
        ]
    },
    {
        "repo": "celery/celery",
        "pull_number": 6223,
        "instance_id": "celery__celery-6223",
        "issue_numbers": [
            "4558"
        ],
        "base_commit": "d537be48e41cec1336e8e35f6db271b5f635adb7",
        "patch": "diff --git a/celery/bin/base.py b/celery/bin/base.py\nindex 08a0f67f24d..4f69a64f9ab 100644\n--- a/celery/bin/base.py\n+++ b/celery/bin/base.py\n@@ -474,7 +474,7 @@ def prepare_parser(self, parser):\n         return parser\n \n     def setup_app_from_commandline(self, argv):\n-        preload_options = self.parse_preload_options(argv)\n+        preload_options, remaining_options = self.parse_preload_options(argv)\n         quiet = preload_options.get('quiet')\n         if quiet is not None:\n             self.quiet = quiet\n@@ -510,18 +510,18 @@ def setup_app_from_commandline(self, argv):\n             elif self.app is None:\n                 self.app = self.get_app(loader=loader)\n             if self.enable_config_from_cmdline:\n-                argv = self.process_cmdline_config(argv)\n+                remaining_options = self.process_cmdline_config(remaining_options)\n         else:\n             self.app = Celery(fixups=[])\n \n         self._handle_user_preload_options(argv)\n \n-        return argv\n+        return remaining_options\n \n     def _handle_user_preload_options(self, argv):\n         user_preload = tuple(self.app.user_options['preload'] or ())\n         if user_preload:\n-            user_options = self._parse_preload_options(argv, user_preload)\n+            user_options, _ = self._parse_preload_options(argv, user_preload)\n             signals.user_preload_options.send(\n                 sender=self, app=self.app, options=user_options,\n             )\n@@ -550,8 +550,8 @@ def _parse_preload_options(self, args, options):\n         args = [arg for arg in args if arg not in ('-h', '--help')]\n         parser = self.Parser()\n         self.add_compat_options(parser, options)\n-        namespace, _ = parser.parse_known_args(args)\n-        return vars(namespace)\n+        namespace, unknown_args = parser.parse_known_args(args)\n+        return vars(namespace), unknown_args\n \n     def add_append_opt(self, acc, opt, value):\n         default = opt.default or []\ndiff --git a/celery/bin/celery.py b/celery/bin/celery.py\nindex a715f6e479c..e1001777950 100644\n--- a/celery/bin/celery.py\n+++ b/celery/bin/celery.py\n@@ -435,6 +435,13 @@ def on_usage_error(self, exc, command=None):\n         )))\n \n     def _relocate_args_from_start(self, argv, index=0):\n+        \"\"\"Move options to the end of args.\n+\n+        This rewrites:\n+            -l debug worker -c 3\n+        to:\n+            worker -c 3 -l debug\n+        \"\"\"\n         if argv:\n             rest = []\n             while index < len(argv):\n@@ -466,9 +473,6 @@ def _relocate_args_from_start(self, argv, index=0):\n                 # we assume the first argument in argv[i:] is the command\n                 # name.\n                 return argv[index:] + rest\n-            # if there are no more arguments then the last arg in rest'\n-            # must be the command.\n-            [rest.pop()] + rest\n         return []\n \n     def prepare_prog_name(self, name):\n",
        "test_patch": "diff --git a/t/unit/bin/test_base.py b/t/unit/bin/test_base.py\nindex f33d2b831f8..a4fcfb80239 100644\n--- a/t/unit/bin/test_base.py\n+++ b/t/unit/bin/test_base.py\n@@ -353,7 +353,7 @@ class TestCommand(Command):\n             def add_preload_arguments(self, parser):\n                 parser.add_argument('-s', action='store', dest='silent')\n         cmd = TestCommand()\n-        acc = cmd.parse_preload_options(['-s', 'yes'])\n+        acc, _ = cmd.parse_preload_options(['-s', 'yes'])\n         assert acc.get('silent') == 'yes'\n \n     def test_parse_preload_options_with_equals_and_append(self):\n@@ -363,7 +363,7 @@ class TestCommand(Command):\n             def add_preload_arguments(self, parser):\n                 parser.add_argument('--zoom', action='append', default=[])\n         cmd = Command()\n-        acc = cmd.parse_preload_options(['--zoom=1', '--zoom=2'])\n+        acc, _ = cmd.parse_preload_options(['--zoom=1', '--zoom=2'])\n \n         assert acc, {'zoom': ['1' == '2']}\n \n@@ -371,6 +371,6 @@ def test_parse_preload_options_without_equals_and_append(self):\n         cmd = Command()\n         opt = Option('--zoom', action='append', default=[])\n         cmd.preload_options = (opt,)\n-        acc = cmd.parse_preload_options(['--zoom', '1', '--zoom', '2'])\n+        acc, _ = cmd.parse_preload_options(['--zoom', '1', '--zoom', '2'])\n \n         assert acc, {'zoom': ['1' == '2']}\ndiff --git a/t/unit/bin/test_celery.py b/t/unit/bin/test_celery.py\nindex 33d5ad2acb1..ba6eaaa93db 100644\n--- a/t/unit/bin/test_celery.py\n+++ b/t/unit/bin/test_celery.py\n@@ -16,6 +16,13 @@\n from celery.platforms import EX_FAILURE, EX_OK, EX_USAGE\n \n \n+class MyApp(object):\n+    user_options = {'preload': None}\n+\n+\n+APP = MyApp()  # <-- Used by test_short_and_long_arguments_be_the_same\n+\n+\n class test__main__:\n \n     def test_main(self):\n@@ -204,6 +211,17 @@ def test_handle_argv(self):\n         x.handle_argv('celery', ['start', 'foo'])\n         x.execute.assert_called_with('start', ['start', 'foo'])\n \n+    def test_short_and_long_arguments_be_the_same(self):\n+        for arg in \"--app\", \"-A\":\n+            appstr = '.'.join([__name__, 'APP'])\n+            x = CeleryCommand(app=self.app)\n+            x.execute = Mock()\n+            with pytest.raises(SystemExit):\n+                x.execute_from_commandline(['celery', arg, appstr, 'worker'])\n+            assert x.execute.called\n+            assert x.execute.call_args[0]\n+            assert x.execute.call_args[0][0] == \"worker\"\n+\n     def test_execute(self):\n         x = CeleryCommand(app=self.app)\n         Help = x.commands['help'] = Mock()\n",
        "problem_statement": "Unlike what doc states, --app does not behave like -A\n## Steps to reproduce\r\n\r\n* Try to run `celery --app foo *anything*`.\r\n* Look at the \"celery usage\" output instead of the \"*anything*\" expected output.\r\n\r\n## Expected behavior\r\n\r\n\"-A foo\" and \"--app foo\" to behave the same way.\r\n\r\n## Actual behavior\r\n\r\n\"-A foo\" works, while \"--app foo\" just outputs the \"celery usage\" screen\r\n\r\n## Environment\r\n\r\n```\r\nsoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.6.3\r\n            billiard:3.5.0.3 py-amqp:2.2.2\r\nplatform -> system:Darwin arch:64bit imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:amqp results:django-db\r\n```\n",
        "hints_text": "whats the status with celery 4.2.1?\nExact same with v4.2.1 (windowlicker).\nThere is in fact a behavioral difference that depends on arguments position:\r\n\r\n`celery worker -A foo` works\r\n`celery worker --app foo` works\r\n`celery -A foo worker` works\r\n`celery --app foo worker` does not work and show celery usage\ndo you have any suggested improvement in mind?\nWell, that I think it should behave the same whatever the order of arguments is. I struggled and lost a bit of time because I first chosed the way that does not work (you know, Murphy's law). I tried to look in the argparsing code of celery to see if I could patch it, but wasn't clear to me how it worked, hence opening the issue. If you point me to the right direction (like where it should be done, and probably how it is tested today), I may be able to submit a patch.\n@thedrow Thanks for the infos.\r\n\r\nIt does indeed appear with certain order of arguments, using \"--app x\" (not \"--app=x\", notice the lack of equal sign) *before the celery subcommand* (worker, inspect, ...). It is 100% reproductible on linux, here is how.\r\n\r\nI launch a brand new linux image (docker) running \"python:3\" official image. The image was run using `docker run -it --rm python:3 bash`, and here is the \"bootstrap\" script I run by hand after launching the container:\r\n\r\n```\r\n$ pip install celery\r\n$ cat > foo.py\r\nfrom celery import Celery\r\napp = Celery(\"foo\")\r\n^D\r\n```\r\n(edited for copypaste mistake)\r\n\r\nMinimalist, right ? Now run in this image:\r\n\r\n```\r\n# celery -A foo report\r\n\r\nsoftware -> celery:4.2.1 (windowlicker) kombu:4.2.1 py:3.7.0\r\n            billiard:3.5.0.4 py-amqp:2.3.2\r\nplatform -> system:Linux arch:64bit imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:amqp results:disabled\r\n```\r\n\r\nWorks as expected. Let's switch to `--app foo`:\r\n\r\n```\r\n# celery --app foo report\r\nusage: celery <command> [options]\r\n\r\nShow help screen and exit.\r\n\r\n... etc ...\r\n```\r\n\r\nNote that it does work with `--app=foo`, just not with `--app foo` (notice the lack of equal sign). But celery usage says: `Global Options: -A APP, --app APP`.\r\n\r\nI don't agree on the fact that it's \"plain argparse\". I think that the culprit code is somehow related to `CeleryCommand._relocate_args_from_start` which (as far as I understand) change `sys.argv` order before those parameters as passed to `argparse`, so that global arguments are moved after the celery command (like \"worker\", \"inspect\", etc).\r\n\r\nI understand the goal, but it does not work 100% with arguments as understood by `argparse`, nor as advertised by the celery usage help.\r\n\r\nBehaviour is 100% same on OSX and Linux. I tried to write an unit test but my lack of understanding the celery codebase (and especially, what the MockCommand does) made me fail at that.\r\n\r\n\r\n\r\n\r\n\n(note that this bug is probably also affecting all other global options passed before the command name and that have arguments, like `--broker BROKER`, `--result-backend RESULT_BACKEND`, `--loader LOADER`, `--config CONFIG`, `--workdir WORKDIR` ...)\r\n\r\nExample:\r\n\r\n* `# celery report -A foo --workdir .` works\r\n* `# celery --workdir . report -A foo` shows celery usage\r\n* `# celery --workdir=. report -A foo` works again\r\n\r\n\r\n  \n(see https://github.com/celery/celery/blob/master/celery/bin/celery.py#L438:L473)\nI did see the \"=\" sign in answers, hence the small emphasis on not using it.\r\n\r\nNote that I'd gladly work on a fix, but the complexity added by the argparse-related hack to reorder arguments and the fact that for now, tests does not check cases with arguments before and after the subcommand makes me feel slightly uncomfortable about submitting a patch, as I don't know how to test it.\r\n\r\nHowever, I do understand why the hack exist, as this is a struggle I often have with argparse (having \"global arguments\" work both before and after subcommand). I think it's a UX defect of argparse, and I understand that celery want to work around it. But this is something that can really annoy users, especially users like me that only use celery every *insert long period of time here* (and Murphy's law forces me to use the only case that does not work, by default ...).\r\n\r\nIf you can give a few pointers on how to test cases with arguments before and after the subcommand in the unit tests, I can give it a shot. It looks like the test you pointed me to skips argparsing, as MockCommand overrides `parse_options`:\r\n\r\n```\r\n  19   \u2502 class MockCommand(Command):\r\n  20   \u2502     mock_args = ('arg1', 'arg2', 'arg3')\r\n  21   \u2502\r\n  22   \u2502     def parse_options(self, prog_name, arguments, command=None):\r\n  23   \u2502         options = {'foo': 'bar', 'prog_name': prog_name}\r\n  24   \u2502         return options, self.mock_args\r\n``` ",
        "created_at": "2020-07-12T22:46:19Z",
        "version": "4.4",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "t/unit/bin/test_base.py",
            "t/unit/bin/test_celery.py"
        ]
    },
    {
        "repo": "celery/celery",
        "pull_number": 6142,
        "instance_id": "celery__celery-6142",
        "issue_numbers": [
            "6136",
            "4412"
        ],
        "base_commit": "574b616f0a1570e9a91a2d15e9bdaf9c91b3cac6",
        "patch": "diff --git a/celery/apps/multi.py b/celery/apps/multi.py\nindex baa2fa8b9e1..482290f9c33 100644\n--- a/celery/apps/multi.py\n+++ b/celery/apps/multi.py\n@@ -151,11 +151,11 @@ def _setdefaultopt(self, d, alt, value):\n                 return d[opt]\n             except KeyError:\n                 pass\n-        value = os.path.normpath(value)\n+        value = d.setdefault(alt[0], os.path.normpath(value))\n         dir_path = os.path.dirname(value)\n         if not os.path.exists(dir_path):\n             os.makedirs(dir_path)\n-        return d.setdefault(alt[0], value)\n+        return value\n \n     def _prepare_expander(self):\n         shortname, hostname = self.name.split('@', 1)\n",
        "test_patch": "diff --git a/t/unit/apps/test_multi.py b/t/unit/apps/test_multi.py\nindex 16add3c48b1..a13ec11817d 100644\n--- a/t/unit/apps/test_multi.py\n+++ b/t/unit/apps/test_multi.py\n@@ -64,7 +64,7 @@ def test_parse(self, gethostname):\n             '-c:jerry,elaine', '5',\n             '--loglevel:kramer=DEBUG',\n             '--flag',\n-            '--logfile=foo', '-Q', 'bar', 'jerry',\n+            '--logfile=/var/log/celery/foo', '-Q', 'bar', 'jerry',\n             'elaine', 'kramer',\n             '--', '.disable_rate_limits=1',\n         ])\n@@ -86,19 +86,19 @@ def assert_line_in(name, args):\n         assert_line_in(\n             '*P*jerry@*S*',\n             ['COMMAND', '-n *P*jerry@*S*', '-Q bar',\n-             '-c 5', '--flag', '--logfile=foo',\n+             '-c 5', '--flag', '--logfile=/var/log/celery/foo',\n              '-- .disable_rate_limits=1', '*AP*'],\n         )\n         assert_line_in(\n             '*P*elaine@*S*',\n             ['COMMAND', '-n *P*elaine@*S*', '-Q bar',\n-             '-c 5', '--flag', '--logfile=foo',\n+             '-c 5', '--flag', '--logfile=/var/log/celery/foo',\n              '-- .disable_rate_limits=1', '*AP*'],\n         )\n         assert_line_in(\n             '*P*kramer@*S*',\n             ['COMMAND', '--loglevel=DEBUG', '-n *P*kramer@*S*',\n-             '-Q bar', '--flag', '--logfile=foo',\n+             '-Q bar', '--flag', '--logfile=/var/log/celery/foo',\n              '-- .disable_rate_limits=1', '*AP*'],\n         )\n         expand = nodes[0].expander\n@@ -278,6 +278,33 @@ def test_logfile(self):\n         assert self.node.logfile == self.expander.return_value\n         self.expander.assert_called_with(os.path.normpath('/var/log/celery/%n%I.log'))\n \n+    @patch('celery.apps.multi.os.path.exists')\n+    def test_pidfile_default(self, mock_exists):\n+        n = Node.from_kwargs(\n+            'foo@bar.com',\n+        )\n+        assert n.options['--pidfile'] == '/var/run/celery/%n.pid'\n+        mock_exists.assert_any_call('/var/run/celery')\n+\n+    @patch('celery.apps.multi.os.makedirs')\n+    @patch('celery.apps.multi.os.path.exists', return_value=False)\n+    def test_pidfile_custom(self, mock_exists, mock_dirs):\n+        n = Node.from_kwargs(\n+            'foo@bar.com',\n+            pidfile='/var/run/demo/celery/%n.pid'\n+        )\n+        assert n.options['--pidfile'] == '/var/run/demo/celery/%n.pid'\n+\n+        try:\n+            mock_exists.assert_any_call('/var/run/celery')\n+        except AssertionError:\n+            pass\n+        else:\n+            raise AssertionError(\"Expected exists('/var/run/celery') to not have been called.\")\n+\n+        mock_exists.assert_any_call('/var/run/demo/celery')\n+        mock_dirs.assert_any_call('/var/run/demo/celery')\n+\n \n class test_Cluster:\n \n",
        "problem_statement": "Celery 4.4.3 always trying create /var/run/celery directory, even if it's not needed.\n<!--\r\nPlease make sure to search and mention any related issues\r\nor possible duplicates to this issue as requested by the checklist above.\r\n\r\nThis may or may not include issues in other repositories that the Celery project\r\nmaintains or other repositories that are dependencies of Celery.\r\n\r\nIf you don't know how to mention issues, please refer to Github's documentation\r\non the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests\r\n-->\r\n\r\n#### Related Issues\r\n\r\n#6017  Celery Multi creates pid and log files in the wrong directory\r\n\r\n## Environment & Settings\r\n<!-- Include the contents of celery --version below -->\r\n**Celery version**:\r\n<!-- Include the output of celery -A proj report below -->\r\n<summary><b><code>celery report</code> Output:</b></summary>\r\n\r\n```\r\n# celery report\r\n\r\nsoftware -> celery:4.4.3 (cliffs) kombu:4.6.9 py:3.7.7\r\n            billiard:3.6.3.0 py-amqp:2.6.0\r\nplatform -> system:Linux arch:64bit, ELF\r\n            kernel version:4.19.0-8-amd64 imp:CPython\r\nloader   -> celery.loaders.default.Loader\r\nsettings -> transport:amqp results:disabled\r\n\r\n```\r\n\r\n# Steps to Reproduce\r\n\r\n## Required Dependencies\r\n<!-- Please fill the required dependencies to reproduce this issue -->\r\n* **Minimal Celery Version**: 4.4.3\r\n\r\n</p>\r\n</details>\r\n\r\n## Minimally Reproducible Test Case\r\n<!--\r\nPlease provide a reproducible test case.\r\nRefer to the Reporting Bugs section in our contribution guide.\r\n\r\nWe prefer submitting test cases in the form of a PR to our integration test suite.\r\nIf you can provide one, please mention the PR number below.\r\nIf not, please attach the most minimal code example required to reproduce the issue below.\r\nIf the test case is too large, please include a link to a gist or a repository below.\r\n-->\r\n\r\n```\r\ncelery multi start ... --pidfile=/var/run/demo/celeryd-%n.pid\r\n```\r\n\r\n# Expected Behavior\r\n<!-- Describe in detail what you expect to happen -->\r\ncelery runs\r\n\r\n# Actual Behavior\r\n<!--\r\nDescribe in detail what actually happened.\r\nPlease include a backtrace and surround it with triple backticks (```).\r\nIn addition, include the Celery daemon logs, the broker logs,\r\nthe result backend logs and system logs below if they will help us debug\r\nthe issue.\r\n-->\r\nstart failed\r\n```\r\ncelery multi v4.4.3 (cliffs)\r\n_annotate_with_default_opts: print options\r\nOrderedDict([('--app', 'service.celery:app'),\r\n             ('--pidfile', '/var/run/demo/celeryd-%n.pid'),\r\n             ('--logfile', '/var/log/demo/celeryd-%n%I.log'),\r\n             ('--loglevel', 'INFO'),\r\n             ('--workdir', '/var/lib/demo-celery'),\r\n             ('--events', None),\r\n             ('--heartbeat-interval', '5'),\r\n             ('--without-gossip', None),\r\n             ('--queues', 'high'),\r\n             ('--concurrency', '1'),\r\n             ('-n', 'high@celeryd.worker')])\r\nTraceback (most recent call last):\r\n  File \"/var/www/misc/ve-2006011156/bin/celery\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/var/www/misc/ve-2006011156/lib/python3.7/site-packages/celery/__main__.py\", line 16, in main\r\n    _main()\r\n  File \"/var/www/misc/ve-2006011156/lib/python3.7/site-packages/celery/bin/celery.py\", line 322, in main\r\n    cmd.execute_from_commandline(argv)\r\n  File \"/var/www/misc/ve-2006011156/lib/python3.7/site-packages/celery/bin/celery.py\", line 495, in execute_from_commandline\r\n    super(CeleryCommand, self).execute_from_commandline(argv)))\r\n  File \"/var/www/misc/ve-2006011156/lib/python3.7/site-packages/celery/bin/base.py\", line 305, in execute_from_commandline\r\n    return self.handle_argv(self.prog_name, argv[1:])\r\n  File \"/var/www/misc/ve-2006011156/lib/python3.7/site-packages/celery/bin/celery.py\", line 487, in handle_argv\r\n    return self.execute(command, argv)\r\n  File \"/var/www/misc/ve-2006011156/lib/python3.7/site-packages/celery/bin/celery.py\", line 419, in execute\r\n    ).run_from_argv(self.prog_name, argv[1:], command=argv[0])\r\n  File \"/var/www/misc/ve-2006011156/lib/python3.7/site-packages/celery/bin/celery.py\", line 335, in run_from_argv\r\n    return cmd.execute_from_commandline([command] + argv)\r\n  File \"/var/www/misc/ve-2006011156/lib/python3.7/site-packages/celery/bin/multi.py\", line 266, in execute_from_commandline\r\n    return self.call_command(argv[0], argv[1:])\r\n  File \"/var/www/misc/ve-2006011156/lib/python3.7/site-packages/celery/bin/multi.py\", line 273, in call_command\r\n    return self.commands[command](*argv) or EX_OK\r\n  File \"/var/www/misc/ve-2006011156/lib/python3.7/site-packages/celery/bin/multi.py\", line 143, in _inner\r\n    return fun(self, *args, **kwargs)\r\n  File \"/var/www/misc/ve-2006011156/lib/python3.7/site-packages/celery/bin/multi.py\", line 151, in _inner\r\n    return fun(self, self.cluster_from_argv(argv), **kwargs)\r\n  File \"/var/www/misc/ve-2006011156/lib/python3.7/site-packages/celery/bin/multi.py\", line 361, in cluster_from_argv\r\n    _, cluster = self._cluster_from_argv(argv, cmd=cmd)\r\n  File \"/var/www/misc/ve-2006011156/lib/python3.7/site-packages/celery/bin/multi.py\", line 366, in _cluster_from_argv\r\n    return p, self.Cluster(list(nodes), cmd=cmd)\r\n  File \"/var/www/misc/ve-2006011156/lib/python3.7/site-packages/celery/apps/multi.py\", line 306, in <genexpr>\r\n    for name in names\r\n  File \"/var/www/misc/ve-2006011156/lib/python3.7/site-packages/celery/apps/multi.py\", line 314, in _node_from_options\r\n    p.optmerge(namespace, options), p.passthrough)\r\n  File \"/var/www/misc/ve-2006011156/lib/python3.7/site-packages/celery/apps/multi.py\", line 136, in __init__\r\n    options or OrderedDict())\r\n  File \"/var/www/misc/ve-2006011156/lib/python3.7/site-packages/celery/apps/multi.py\", line 145, in _annotate_with_default_opts\r\n    self._setdefaultopt(options, ['--pidfile', '-p'], '/var/run/celery/%n.pid')\r\n  File \"/var/www/misc/ve-2006011156/lib/python3.7/site-packages/celery/apps/multi.py\", line 159, in _setdefaultopt\r\n    os.makedirs(dir_path)\r\n  File \"/var/www/misc/ve-2006011156/lib/python3.7/os.py\", line 223, in makedirs\r\n    mkdir(name, mode)\r\nPermissionError: [Errno 13] Permission denied: '/var/run/celery'\r\n\r\nsystemd[1]: demo@celeryd.service: Control process exited, code=exited, status=1/FAILURE\r\n```\r\n\nRequest on_timeout should ignore soft time limit exception\nWhen Request.on_timeout receive a soft timeout from billiard, it does the same as if it was receiving a hard time limit exception. This is ran by the controller.\r\n\r\nBut the task may catch this exception and eg. return (this is what soft timeout are for).\r\n\r\nThis cause:\r\n1. the result to be saved once as an exception by the controller (on_timeout) and another time with the result returned by the task\r\n2. the task status to be passed to failure and to success on the same manner\r\n3. if the task is participating to a chord, the chord result counter (at least with redis) is incremented twice (instead of once), making the chord to return prematurely and eventually loose tasks\u2026\r\n\r\n1, 2 and 3 can leads of course to strange race conditions\u2026\r\n\r\n## Steps to reproduce (Illustration)\r\n\r\nwith the program in test_timeout.py:\r\n\r\n```python\r\nimport time\r\nimport celery\r\n\r\n\r\napp = celery.Celery('test_timeout')\r\napp.conf.update(\r\n    result_backend=\"redis://localhost/0\",\r\n    broker_url=\"amqp://celery:celery@localhost:5672/host\",\r\n)\r\n\r\n@app.task(soft_time_limit=1)\r\ndef test():\r\n    try:\r\n        time.sleep(2)\r\n    except Exception:\r\n        return 1\r\n\r\n@app.task()\r\ndef add(args):\r\n    print(\"### adding\", args)\r\n    return sum(args)\r\n\r\n@app.task()\r\ndef on_error(context, exception, traceback, **kwargs):\r\n    print(\"### on_error:\u00a0\", exception)\r\n\r\nif __name__ == \"__main__\":\r\n    result = celery.chord([test.s().set(link_error=on_error.s()), test.s().set(link_error=on_error.s())])(add.s())\r\n    result.get()\r\n```\r\n\r\nstart a worker and the program:\r\n\r\n```\r\n$ celery -A test_timeout worker -l WARNING\r\n$ python3 test_timeout.py\r\n```\r\n\r\n## Expected behavior\r\n\r\nadd method is called with `[1, 1]` as argument and test_timeout.py return normally\r\n\r\n## Actual behavior\r\n\r\nThe test_timeout.py fails, with\r\n```\r\ncelery.backends.base.ChordError: Callback error: ChordError(\"Dependency 15109e05-da43-449f-9081-85d839ac0ef2 raised SoftTimeLimitExceeded('SoftTimeLimitExceeded(True,)',)\",\r\n```\r\nOn the worker side, the **on_error is called but the add method as well !**\r\n\r\n```\r\n[2017-11-29 23:07:25,538: WARNING/MainProcess] Soft time limit (1s) exceeded for test_timeout.test[15109e05-da43-449f-9081-85d839ac0ef2]\r\n[2017-11-29 23:07:25,546: WARNING/MainProcess] ### on_error:\r\n[2017-11-29 23:07:25,546: WARNING/MainProcess] SoftTimeLimitExceeded(True,)\r\n[2017-11-29 23:07:25,547: WARNING/MainProcess] Soft time limit (1s) exceeded for test_timeout.test[38f3f7f2-4a89-4318-8ee9-36a987f73757]\r\n[2017-11-29 23:07:25,553: ERROR/MainProcess] Chord callback for 'ef6d7a38-d1b4-40ad-b937-ffa84e40bb23' raised: ChordError(\"Dependency 15109e05-da43-449f-9081-85d839ac0ef2 raised SoftTimeLimitExceeded('SoftTimeLimitExceeded(True,)',)\",)\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.4/dist-packages/celery/backends/redis.py\", line 290, in on_chord_part_return\r\n    callback.delay([unpack(tup, decode) for tup in resl])\r\n  File \"/usr/local/lib/python3.4/dist-packages/celery/backends/redis.py\", line 290, in <listcomp>\r\n    callback.delay([unpack(tup, decode) for tup in resl])\r\n  File \"/usr/local/lib/python3.4/dist-packages/celery/backends/redis.py\", line 243, in _unpack_chord_result\r\n    raise ChordError('Dependency {0} raised {1!r}'.format(tid, retval))\r\ncelery.exceptions.ChordError: Dependency 15109e05-da43-449f-9081-85d839ac0ef2 raised SoftTimeLimitExceeded('SoftTimeLimitExceeded(True,)',)\r\n[2017-11-29 23:07:25,565: WARNING/MainProcess] ### on_error:\r\n[2017-11-29 23:07:25,565: WARNING/MainProcess] SoftTimeLimitExceeded(True,)\r\n[2017-11-29 23:07:27,262: WARNING/PoolWorker-2] ### adding\r\n[2017-11-29 23:07:27,264: WARNING/PoolWorker-2] [1, 1]\r\n```\r\n\r\nOf course, on purpose did I choose to call the test.s() twice, to show that the count in the chord continues. In fact:\r\n- the chord result is incremented twice by the error of soft time limit\r\n- the chord result is again incremented twice by the correct returning of `test` task\r\n\r\n## Conclusion\r\n\r\nRequest.on_timeout  should not process soft time limit exception. \r\n\r\nhere is a quick monkey patch (correction of celery is trivial)\r\n\r\n```python\r\ndef patch_celery_request_on_timeout():\r\n    from celery.worker import request\r\n    orig = request.Request.on_timeout\r\n    def patched_on_timeout(self, soft, timeout):\r\n        if not soft:\r\n            orig(self, soft, timeout)\r\n    request.Request.on_timeout = patched_on_timeout\r\npatch_celery_request_on_timeout()\r\n```\r\n\r\n\r\n\r\n## version info\r\n\r\nsoftware -> celery:4.1.0 (latentcall) kombu:4.0.2 py:3.4.3\r\n            billiard:3.5.0.2 py-amqp:2.1.4\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:amqp results:redis://10.0.3.253/0\r\n\n",
        "hints_text": "PermissionError: [Errno 13] Permission denied: '/var/run/celery' \r\n\r\ncan you try sudo?\n@mchataigner can you chime in?\n> PermissionError: [Errno 13] Permission denied: '/var/run/celery'\r\n> \r\n> can you try sudo?\r\n\r\nDo you mean\r\n`sudo celery multy start ...`  ?\r\n\r\nI use systemd with RuntimeDirectory option\r\n\r\n```config\r\n[Unit]\r\nDescription = Demo celery workers\r\n# When systemd stops or restarts the app.service, the action is propagated to this unit\r\nPartOf = demo.target\r\n# Start this unit after the demo.service start\r\nAfter = demo.target\r\n\r\nAfter = redis-server.service\r\nRequires = redis-server.service\r\n\r\n[Service]\r\nType = forking\r\nUser = www-data\r\nGroup = www-data\r\nPermissionsStartOnly = true\r\nRuntimeDirectory = demo\r\nRuntimeDirectoryMode = 0775\r\nEnvironmentFile = /var/www/misc/conf/celery.conf\r\nExecStart = /usr/local/sbin/demo-exec celery multi start ${CELERYD_NODES} \\\r\n  --app=${CELERY_APP} \\\r\n  --hostname=celeryd.worker \\\r\n  --pidfile=${CELERYD_PID_FILE} \\\r\n  --logfile=${CELERYD_LOG_FILE} \\\r\n  --loglevel=${CELERYD_LOG_LEVEL} \\\r\n  ${CELERYD_OPTS}\r\nExecStop = /usr/local/sbin/demo-exec celery multi stopwait ${CELERYD_NODES} \\\r\n  --hostname=celeryd.worker \\\r\n  --pidfile=${CELERYD_PID_FILE}\r\nExecReload = /usr/local/sbin/demo-exec celery multi restart ${CELERYD_NODES} \\\r\n  --app=${CELERY_APP} \\\r\n  --hostname=celeryd.worker \\\r\n  --pidfile=${CELERYD_PID_FILE} \\\r\n  --logfile=${CELERYD_LOG_FILE} \\\r\n  --loglevel=${CELERYD_LOG_LEVEL} \\\r\n  ${CELERYD_OPTS}\r\nPrivateTmp = true\r\n\r\n[Install]\r\nWantedBy = multi-user.target\r\n```\r\n\r\nwith celery v4.4.2 everything is working\nThe problem is in the method [`Node._setdefaultopt`](https://github.com/celery/celery/blob/master/celery/apps/multi.py#L148)\r\n\r\n```python\r\n    def _setdefaultopt(self, d, alt, value):\r\n        for opt in alt[1:]:\r\n            try:\r\n                return d[opt]\r\n            except KeyError:\r\n                pass\r\n        value = os.path.normpath(value)\r\n        dir_path = os.path.dirname(value)\r\n        if not os.path.exists(dir_path):\r\n            os.makedirs(dir_path)\r\n        return d.setdefault(alt[0], value)\r\n```\r\n\r\nProof of Concept:\r\n```python\r\nimport os\r\n\r\ndef _setdefaultopt(d, alt, value):\r\n    for opt in alt[1:]:\r\n        try:\r\n            return d[opt]\r\n        except KeyError:\r\n            pass\r\n    value = os.path.normpath(value)\r\n    dir_path = os.path.dirname(value)\r\n    if not os.path.exists(dir_path):\r\n        print(\"make dir!!!: \", dir_path)\r\n    return d.setdefault(alt[0], value)\r\n```\r\n\r\nRun in console:\r\n```python\r\nIn [23] _setdefaultopt({'--pidfile': '/var/run/demo/celeryd-%n.pid'}, ['--pidfile', '-p'], '/var/run/celery/%n.pid')\r\nmake dir!!!:  /var/run/celery\r\nOut[23]: '/var/run/demo/celeryd-%n.pid'\r\n```\ncan you come with a fix? with a proper test?\n> can you come with a fix? with a proper test?\r\n\r\nYes\n@ask I think this is quite a big problem (with a trivial fix).\r\n\r\nIt requires attention though as it is bringging a new behaviour (but previous behaviour is not well documented, and, in my opinion, the new behaviour was the one expected)\nThis change in behaviour is what kept my team from upgrading to celery 4. Indeed, the chord callback was often not called at all.\r\nI don't know if it is related, but I modified your code sample and it resulted in some `Exception raised outside body` errors and multiple other errors if you try running `python test_timeout.py` multiple times.\r\n\r\nHere is my script:\r\n\r\n```python\r\nimport time\r\nimport celery\r\n\r\n\r\napp = celery.Celery(\r\n    'test_timeout',\r\n    broker='amqp://localhost',\r\n    backend='redis://localhost')\r\n\r\n\r\n@app.task(soft_time_limit=1)\r\ndef test(nb_seconds):\r\n    try:\r\n        time.sleep(nb_seconds)\r\n        return nb_seconds\r\n    except:\r\n        print(\"### error handled\")\r\n        return nb_seconds\r\n\r\n\r\n@app.task()\r\ndef add(args):\r\n    print(\"### adding\", args)\r\n    return sum(args)\r\n\r\n\r\n@app.task()\r\ndef on_error(context, exception, traceback, **kwargs):\r\n    print(\"### on_error: \", exception)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    result = celery.chord([\r\n        test.s(i).set(link_error=on_error.s()) for i in range(0, 2)\r\n    ])(add.s())\r\n    result.get()\r\n```\r\n\r\nNB: if you update the range for range(2, 4) for instance, the `Exception raised outside body` does not seem to happen. It seems this particular issue happens when the `SoftTimeLimitExceeded` is raised exactly during the `return`.\ncould you please send a PR with your proposed fix/workaround on master branch?\nhi @auvipy, I won't have the time until jannuary. I'll need help on how to write the tests also (that's the reason why I didn't propose a PR).\nOK. dont get afraid of sending logical changes just for the reason you don't know how to write test. we will certainly try to help you \nthanks a lot!",
        "created_at": "2020-06-02T08:00:52Z",
        "version": "4.4",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "t/unit/apps/test_multi.py"
        ]
    },
    {
        "repo": "celery/celery",
        "pull_number": 6138,
        "instance_id": "celery__celery-6138",
        "issue_numbers": [
            "6135"
        ],
        "base_commit": "52aef4bf7041ef4b8e42a95e17d87b0a828f97bf",
        "patch": "diff --git a/celery/app/base.py b/celery/app/base.py\nindex 661e6cf625a..6db4604f3ab 100644\n--- a/celery/app/base.py\n+++ b/celery/app/base.py\n@@ -23,7 +23,7 @@\n                            _register_app, _set_current_app, _task_stack,\n                            connect_on_app_finalize, get_current_app,\n                            get_current_worker_task, set_default_app)\n-from celery.exceptions import AlwaysEagerIgnored, ImproperlyConfigured, Ignore\n+from celery.exceptions import AlwaysEagerIgnored, ImproperlyConfigured, Ignore, Retry\n from celery.five import (UserDict, bytes_if_py2, python_2_unicode_compatible,\n                          values)\n from celery.loaders import get_loader_cls\n@@ -492,6 +492,8 @@ def run(*args, **kwargs):\n                         # If Ignore signal occures task shouldn't be retried,\n                         # even if it suits autoretry_for list\n                         raise\n+                    except Retry:\n+                        raise\n                     except autoretry_for as exc:\n                         if retry_backoff:\n                             retry_kwargs['countdown'] = \\\n",
        "test_patch": "diff --git a/t/unit/tasks/test_tasks.py b/t/unit/tasks/test_tasks.py\nindex e84c566ddc8..e3551cc01bb 100644\n--- a/t/unit/tasks/test_tasks.py\n+++ b/t/unit/tasks/test_tasks.py\n@@ -95,7 +95,7 @@ def retry_task_noargs(self, **kwargs):\n         self.retry_task_noargs = retry_task_noargs\n \n         @self.app.task(bind=True, max_retries=3, iterations=0, shared=False)\n-        def retry_task_without_throw(self, **kwargs):\n+        def retry_task_return_without_throw(self, **kwargs):\n             self.iterations += 1\n             try:\n                 if self.request.retries >= 3:\n@@ -105,7 +105,60 @@ def retry_task_without_throw(self, **kwargs):\n             except Exception as exc:\n                 return self.retry(exc=exc, throw=False)\n \n-        self.retry_task_without_throw = retry_task_without_throw\n+        self.retry_task_return_without_throw = retry_task_return_without_throw\n+\n+        @self.app.task(bind=True, max_retries=3, iterations=0, shared=False)\n+        def retry_task_return_with_throw(self, **kwargs):\n+            self.iterations += 1\n+            try:\n+                if self.request.retries >= 3:\n+                    return 42\n+                else:\n+                    raise Exception(\"random code exception\")\n+            except Exception as exc:\n+                return self.retry(exc=exc, throw=True)\n+\n+        self.retry_task_return_with_throw = retry_task_return_with_throw\n+\n+        @self.app.task(bind=True, max_retries=3, iterations=0, shared=False, autoretry_for=(Exception,))\n+        def retry_task_auto_retry_with_single_new_arg(self, ret=None, **kwargs):\n+            if ret is None:\n+                return self.retry(exc=Exception(\"I have filled now\"), args=[\"test\"], kwargs=kwargs)\n+            else:\n+                return ret\n+\n+        self.retry_task_auto_retry_with_single_new_arg = retry_task_auto_retry_with_single_new_arg\n+\n+        @self.app.task(bind=True, max_retries=3, iterations=0, shared=False)\n+        def retry_task_auto_retry_with_new_args(self, ret=None, place_holder=None, **kwargs):\n+            if ret is None:\n+                return self.retry(args=[place_holder, place_holder], kwargs=kwargs)\n+            else:\n+                return ret\n+\n+        self.retry_task_auto_retry_with_new_args = retry_task_auto_retry_with_new_args\n+\n+        @self.app.task(bind=True, max_retries=3, iterations=0, shared=False, autoretry_for=(Exception,))\n+        def retry_task_auto_retry_exception_with_new_args(self, ret=None, place_holder=None, **kwargs):\n+            if ret is None:\n+                return self.retry(exc=Exception(\"I have filled\"), args=[place_holder, place_holder], kwargs=kwargs)\n+            else:\n+                return ret\n+\n+        self.retry_task_auto_retry_exception_with_new_args = retry_task_auto_retry_exception_with_new_args\n+\n+        @self.app.task(bind=True, max_retries=3, iterations=0, shared=False)\n+        def retry_task_raise_without_throw(self, **kwargs):\n+            self.iterations += 1\n+            try:\n+                if self.request.retries >= 3:\n+                    return 42\n+                else:\n+                    raise Exception(\"random code exception\")\n+            except Exception as exc:\n+                raise self.retry(exc=exc, throw=False)\n+\n+        self.retry_task_raise_without_throw = retry_task_raise_without_throw\n \n         @self.app.task(bind=True, max_retries=3, iterations=0,\n                        base=MockApplyTask, shared=False)\n@@ -365,7 +418,22 @@ def test_retry_kwargs_can_be_empty(self):\n             self.retry_task_mockapply.pop_request()\n \n     def test_retry_without_throw_eager(self):\n-        assert self.retry_task_without_throw.apply().get() == 42\n+        assert self.retry_task_return_without_throw.apply().get() == 42\n+\n+    def test_raise_without_throw_eager(self):\n+        assert self.retry_task_raise_without_throw.apply().get() == 42\n+\n+    def test_return_with_throw_eager(self):\n+        assert self.retry_task_return_with_throw.apply().get() == 42\n+\n+    def test_eager_retry_with_single_new_params(self):\n+        assert self.retry_task_auto_retry_with_single_new_arg.apply().get() == \"test\"\n+\n+    def test_eager_retry_with_new_params(self):\n+        assert self.retry_task_auto_retry_with_new_args.si(place_holder=\"test\").apply().get() == \"test\"\n+\n+    def test_eager_retry_with_autoretry_for_exception(self):\n+        assert self.retry_task_auto_retry_exception_with_new_args.si(place_holder=\"test\").apply().get() == \"test\"\n \n     def test_retry_eager_should_return_value(self):\n         self.retry_task.max_retries = 3\n",
        "problem_statement": "Retry args change BUG\n<!--\r\nPlease fill this template entirely and do not erase parts of it.\r\nWe reserve the right to close without a response\r\nbug reports which are incomplete.\r\n-->\r\n# Checklist\r\n<!--\r\nTo check an item on the list replace [ ] with [x].\r\n-->\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n- [x] This has already been asked to the [discussion group](https://groups.google.com/forum/#!forum/celery-users) first.\r\n- [x] I have read the relevant section in the\r\n  [contribution guide](http://docs.celeryproject.org/en/latest/contributing.html#other-bugs)\r\n  on reporting bugs.\r\n- [ ] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Bug+Report%22+-label%3A%22Category%3A+Documentation%22)\r\n  for similar or identical bug reports.\r\n- [ ] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22PR+Type%3A+Bugfix%22+-label%3A%22Category%3A+Documentation%22)\r\n  for existing proposed fixes.\r\n- [ ] I have checked the [commit log](https://github.com/celery/celery/commits/master)\r\n  to find out if the bug was already fixed in the master branch.\r\n- [ ] I have included all related issues and possible duplicate issues\r\n  in this issue (If there are none, check this box anyway).\r\n\r\n## Mandatory Debugging Information\r\n\r\n- [ ] I have included the output of ``celery -A proj report`` in the issue.\r\n    (if you are not able to do this, then at least specify the Celery\r\n     version affected).\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n- [ ] I have included the contents of ``pip freeze`` in the issue.\r\n- [ ] I have included all the versions of all the external dependencies required\r\n  to reproduce this bug.\r\n\r\n## Optional Debugging Information\r\n<!--\r\nTry some of the below if you think they are relevant.\r\nIt will help us figure out the scope of the bug and how many users it affects.\r\n-->\r\n- [ ] I have tried reproducing the issue on more than one Python version\r\n  and/or implementation.\r\n- [ ] I have tried reproducing the issue on more than one message broker and/or\r\n  result backend.\r\n- [ ] I have tried reproducing the issue on more than one version of the message\r\n  broker and/or result backend.\r\n- [ ] I have tried reproducing the issue on more than one operating system.\r\n- [ ] I have tried reproducing the issue on more than one workers pool.\r\n- [ ] I have tried reproducing the issue with autoscaling, retries,\r\n  ETA/Countdown & rate limits disabled.\r\n- [ ] I have tried reproducing the issue after downgrading\r\n  and/or upgrading Celery and its dependencies.\r\n\r\n## Related Issues and Possible Duplicates\r\n<!--\r\nPlease make sure to search and mention any related issues\r\nor possible duplicates to this issue as requested by the checklist above.\r\n\r\nThis may or may not include issues in other repositories that the Celery project\r\nmaintains or other repositories that are dependencies of Celery.\r\n\r\nIf you don't know how to mention issues, please refer to Github's documentation\r\non the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests\r\n-->\r\n\r\n#### Related Issues\r\n\r\n- None\r\n\r\n#### Possible Duplicates\r\n\r\n- None\r\n\r\n## Environment & Settings\r\n<!-- Include the contents of celery --version below -->\r\n**Celery version**: celery==4.4.2\r\n<!-- Include the output of celery -A proj report below -->\r\n<details>\r\n<summary><b><code>celery report</code> Output:</b></summary>\r\n<p>\r\n\r\n```\r\n[2020-05-31 23:28:34,434: INFO/MainProcess] Connected to amqp://remote_worker:**@127.0.0.1:5672//\r\n[2020-05-31 23:28:34,453: INFO/MainProcess] mingle: searching for neighbors\r\n[2020-05-31 23:28:35,487: INFO/MainProcess] mingle: all alone\r\n[2020-05-31 23:28:35,528: WARNING/MainProcess] /home/ubuntu/.local/lib/python3.7/site-packages/celery/fixups/django.py:203: UserWarning: Using settings.DEBUG leads to a memory\r\n            leak, never use this setting in production environments!\r\n  leak, never use this setting in production environments!''')\r\n[2020-05-31 23:28:35,529: INFO/MainProcess] celery@testroom ready.\r\n[2020-05-31 23:28:47,351: INFO/MainProcess] Received task: api_v3.tests.execute[e97d93b5-b0e5-4b87-96ab-1aab66119906]  \r\n[2020-05-31 23:28:47,689: WARNING/ForkPoolWorker-1] started\r\n[2020-05-31 23:28:47,690: WARNING/ForkPoolWorker-1] retry\r\n[2020-05-31 23:28:47,721: INFO/MainProcess] Received task: api_v3.tests.execute[e97d93b5-b0e5-4b87-96ab-1aab66119906]  ETA:[2020-05-31 23:28:57.692348+00:00] \r\n[2020-05-31 23:28:47,722: INFO/MainProcess] Received task: api_v3.tests.execute[e97d93b5-b0e5-4b87-96ab-1aab66119906]  ETA:[2020-05-31 23:28:57.716321+00:00] \r\n[2020-05-31 23:28:47,777: INFO/ForkPoolWorker-1] Task api_v3.tests.execute[e97d93b5-b0e5-4b87-96ab-1aab66119906] retry: Retry in 10s: Retry(Retry(...), Exception('i have filled now'), 10)\r\n[2020-05-31 23:28:57,999: WARNING/ForkPoolWorker-1] started\r\n[2020-05-31 23:28:58,000: WARNING/ForkPoolWorker-1] ended\r\n[2020-05-31 23:28:58,062: INFO/ForkPoolWorker-1] Task api_v3.tests.execute[e97d93b5-b0e5-4b87-96ab-1aab66119906] succeeded in 0.34440315900428686s: None\r\n[2020-05-31 23:28:58,301: WARNING/ForkPoolWorker-1] started\r\n[2020-05-31 23:28:58,302: WARNING/ForkPoolWorker-1] retry\r\n[2020-05-31 23:28:58,304: INFO/MainProcess] Received task: api_v3.tests.execute[e97d93b5-b0e5-4b87-96ab-1aab66119906]  ETA:[2020-05-31 23:29:08.303091+00:00] \r\n[2020-05-31 23:28:58,307: INFO/MainProcess] Received task: api_v3.tests.execute[e97d93b5-b0e5-4b87-96ab-1aab66119906]  ETA:[2020-05-31 23:29:08.306141+00:00] \r\n[2020-05-31 23:28:58,368: INFO/ForkPoolWorker-1] Task api_v3.tests.execute[e97d93b5-b0e5-4b87-96ab-1aab66119906] retry: Retry in 10s: Retry(Retry(...), Exception('i have filled now'), 10)\r\n[2020-05-31 23:29:08,572: WARNING/ForkPoolWorker-1] started\r\n[2020-05-31 23:29:08,573: WARNING/ForkPoolWorker-1] ended\r\n[2020-05-31 23:29:08,633: INFO/ForkPoolWorker-1] Task api_v3.tests.execute[e97d93b5-b0e5-4b87-96ab-1aab66119906] succeeded in 0.3256059319974156s: None\r\n[2020-05-31 23:29:08,872: WARNING/ForkPoolWorker-1] started\r\n[2020-05-31 23:29:08,873: WARNING/ForkPoolWorker-1] retry\r\n[2020-05-31 23:29:08,875: INFO/MainProcess] Received task: api_v3.tests.execute[e97d93b5-b0e5-4b87-96ab-1aab66119906]  ETA:[2020-05-31 23:29:18.873799+00:00] \r\n[2020-05-31 23:29:08,880: INFO/MainProcess] Received task: api_v3.tests.execute[e97d93b5-b0e5-4b87-96ab-1aab66119906]  ETA:[2020-05-31 23:29:18.877550+00:00] \r\n[2020-05-31 23:29:08,940: INFO/ForkPoolWorker-1] Task api_v3.tests.execute[e97d93b5-b0e5-4b87-96ab-1aab66119906] retry: Retry in 10s: Retry(Retry(...), Exception('i have filled now'), 10)\r\n[2020-05-31 23:29:19,144: WARNING/ForkPoolWorker-1] started\r\n[2020-05-31 23:29:19,145: WARNING/ForkPoolWorker-1] ended\r\n[2020-05-31 23:29:19,205: INFO/ForkPoolWorker-1] Task api_v3.tests.execute[e97d93b5-b0e5-4b87-96ab-1aab66119906] succeeded in 0.326258520995907s: None\r\n[2020-05-31 23:29:19,444: WARNING/ForkPoolWorker-1] started\r\n[2020-05-31 23:29:19,445: WARNING/ForkPoolWorker-1] retry\r\n[2020-05-31 23:29:19,505: ERROR/ForkPoolWorker-1] Task api_v3.tests.execute[e97d93b5-b0e5-4b87-96ab-1aab66119906] raised unexpected: Exception('i have filled now')\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/.local/lib/python3.7/site-packages/celery/app/trace.py\", line 385, in trace_task\r\n    R = retval = fun(*args, **kwargs)\r\n  File \"/home/ubuntu/.local/lib/python3.7/site-packages/celery/app/trace.py\", line 650, in __protected_call__\r\n    return self.run(*args, **kwargs)\r\n  File \"/home/ubuntu/.local/lib/python3.7/site-packages/celery/app/base.py\", line 500, in run\r\n    raise task.retry(exc=exc, **retry_kwargs)\r\n  File \"/home/ubuntu/.local/lib/python3.7/site-packages/celery/app/task.py\", line 704, in retry\r\n    raise_with_context(exc)\r\n  File \"/home/ubuntu/.local/lib/python3.7/site-packages/celery/app/base.py\", line 487, in run\r\n    return task._orig_run(*args, **kwargs)\r\n  File \"/var/www/django_projects/earthalytics-api/api_v3/tests.py\", line 26, in execute\r\n    self.retry(exc=Exception(\"i have filled now\"), args=[param_a, param_b], kwargs=kwargs)\r\n  File \"/home/ubuntu/.local/lib/python3.7/site-packages/celery/app/task.py\", line 704, in retry\r\n    raise_with_context(exc)\r\n  File \"/home/ubuntu/.local/lib/python3.7/site-packages/celery/utils/serialization.py\", line 288, in raise_with_context\r\n    _raise_with_context(exc, exc_info[1])\r\n  File \"<string>\", line 1, in _raise_with_context\r\nException: i have filled now\r\n\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n# Steps to Reproduce\r\nMake a celery task with a retry changing one parameters.\r\nSet the max_retries and countdown.\r\n\r\n## Required Dependencies\r\n<!-- Please fill the required dependencies to reproduce this issue -->\r\n* **Minimal Python Version**: N/A or Unknown\r\n* **Minimal Celery Version**: N/A or Unknown\r\n* **Minimal Kombu Version**: N/A or Unknown\r\n* **Minimal Broker Version**: N/A or Unknown\r\n* **Minimal Result Backend Version**: N/A or Unknown\r\n* **Minimal OS and/or Kernel Version**: N/A or Unknown\r\n* **Minimal Broker Client Version**: N/A or Unknown\r\n* **Minimal Result Backend Client Version**: N/A or Unknown\r\n\r\n### Python Packages\r\n<!-- Please fill the contents of pip freeze below -->\r\n<details>\r\n<summary><b><code>pip freeze</code> Output:</b></summary>\r\n<p>\r\n\r\n```\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n### Other Dependencies\r\n<!--\r\nPlease provide system dependencies, configuration files\r\nand other dependency information if applicable\r\n-->\r\n<details>\r\n<p>\r\nN/A\r\n</p>\r\n</details>\r\n\r\n## Minimally Reproducible Test Case\r\n<!--\r\nPlease provide a reproducible test case.\r\nRefer to the Reporting Bugs section in our contribution guide.\r\n\r\nWe prefer submitting test cases in the form of a PR to our integration test suite.\r\nIf you can provide one, please mention the PR number below.\r\nIf not, please attach the most minimal code example required to reproduce the issue below.\r\nIf the test case is too large, please include a link to a gist or a repository below.\r\n-->\r\n\r\n<details>\r\n<p>\r\n\r\n```python\r\n\r\n@app_cluster.task(bind=True, autoretry_for=(Exception,), max_retries=3,\r\n                  default_retry_delay=10)\r\ndef execute(self, param_a, param_b=None, **kwargs):\r\n    print(\"started\")\r\n    if param_b is None:\r\n        param_b = \"filled\"\r\n        print(\"retry\")\r\n        self.retry(exc=Exception(\"i have filled now\"), args=[param_a, param_b], kwargs=kwargs)\r\n    print(\"ended\")\r\n\r\ndef test_celery(self):\r\n    sig = execute.si(\"something\")\r\n    t = sig.delay()\r\n    t = 0\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n# Expected Behavior\r\n<!-- Describe in detail what you expect to happen -->\r\nI expect the task get overrided with updated parameters\r\n# Actual Behavior\r\n<!--\r\nDescribe in detail what actually happened.\r\nPlease include a backtrace and surround it with triple backticks (```).\r\nIn addition, include the Celery daemon logs, the broker logs,\r\nthe result backend logs and system logs below if they will help us debug\r\nthe issue.\r\n--> The task goes in retry with origianl parameters.\r\nA new task is made with new parameters and get scheduled.\r\nThe \"old\" task get executed.\r\nThe \"new\" task get executed.\r\nThe \"old\" task goes in retry making a \"new\" \"new task\" with updated parameters and the \"old\" goes scheduled again with original parameters.\r\n\n",
        "hints_text": "I tried to fix that by myself but code part is pretty weird...a function get called and never return jumping in another code part.\r\ntask.py\r\n```\r\n        if is_eager:\r\n            # if task was executed eagerly using apply(),\r\n            # then the retry must also be executed eagerly.\r\n            S.apply().get()  # This never return\r\n            if throw:\r\n                raise ret\r\n            return ret\r\n```\r\nAnyway, i figured maybe what happens. A signature is called updated and another signature is re-called not updated, making a split in \"def retry\"(Maybe Retry dosn't get counted as \"Safe Exception\"?).\r\n\r\nEDIT:\r\nPretty sure that \"Retry\" launch an exception for a real retry(so a new job get executed updated) BUT Retry is an exception too, so \"this job\" goes in exception and need to be re-executed.\r\nHere is the guilty:\r\n`autoretry_for=(Exception,)`\r\nOR WORSE:\r\nRetry fail before the real end and can't handle the exception.\nOk, i fixed it.\r\n2 Main error and exception overlap when in eager.\r\nFix in minutes.\n#6137 \nI saw in master\r\n```\r\n        if is_eager:\r\n            # if task was executed eagerly using apply(),\r\n            # then the retry must also be executed eagerly.\r\n            S.apply()\r\n            if throw:\r\n                raise ret\r\n            return ret\r\n```\r\ngot changed removing \"S.apply()\". I can't run the master branch but...works? Cause in my \"4.4.2\" this one run the task in sync locally when eager.\r\n",
        "created_at": "2020-06-01T19:48:30Z",
        "version": "4.4",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "t/unit/tasks/test_tasks.py"
        ]
    },
    {
        "repo": "celery/celery",
        "pull_number": 6134,
        "instance_id": "celery__celery-6134",
        "issue_numbers": [
            "4116",
            "4116"
        ],
        "base_commit": "2479f9571e89857bd53c48289b9a243bc3fd5242",
        "patch": "diff --git a/celery/fixups/django.py b/celery/fixups/django.py\nindex fe2a17224e6..8cfe3b99721 100644\n--- a/celery/fixups/django.py\n+++ b/celery/fixups/django.py\n@@ -151,7 +151,7 @@ def on_worker_process_init(self, **kwargs):\n                 self._maybe_close_db_fd(c.connection)\n \n         # use the _ version to avoid DB_REUSE preventing the conn.close() call\n-        self._close_database()\n+        self._close_database(force=True)\n         self.close_cache()\n \n     def _maybe_close_db_fd(self, fd):\n@@ -180,10 +180,13 @@ def close_database(self, **kwargs):\n             self._close_database()\n         self._db_recycles += 1\n \n-    def _close_database(self):\n+    def _close_database(self, force=False):\n         for conn in self._db.connections.all():\n             try:\n-                conn.close()\n+                if force:\n+                    conn.close()\n+                else:\n+                    conn.close_if_unusable_or_obsolete()\n             except self.interface_errors:\n                 pass\n             except self.DatabaseError as exc:\n",
        "test_patch": "diff --git a/t/unit/fixups/test_django.py b/t/unit/fixups/test_django.py\nindex 8d0a44a8b41..d917e8cdba6 100644\n--- a/t/unit/fixups/test_django.py\n+++ b/t/unit/fixups/test_django.py\n@@ -145,7 +145,7 @@ def test_on_worker_process_init(self, patching):\n                         f.on_worker_process_init()\n                         mcf.assert_called_with(conns[1].connection)\n                         f.close_cache.assert_called_with()\n-                        f._close_database.assert_called_with()\n+                        f._close_database.assert_called_with(force=True)\n \n                         f.validate_models = Mock(name='validate_models')\n                         patching.setenv('FORKED_BY_MULTIPROCESSING', '1')\n@@ -213,13 +213,35 @@ def test__close_database(self):\n             f._db.connections = Mock()  # ConnectionHandler\n             f._db.connections.all.side_effect = lambda: conns\n \n-            f._close_database()\n+            f._close_database(force=True)\n             conns[0].close.assert_called_with()\n+            conns[0].close_if_unusable_or_obsolete.assert_not_called()\n             conns[1].close.assert_called_with()\n+            conns[1].close_if_unusable_or_obsolete.assert_not_called()\n             conns[2].close.assert_called_with()\n+            conns[2].close_if_unusable_or_obsolete.assert_not_called()\n+\n+            for conn in conns:\n+                conn.reset_mock()\n+\n+            f._close_database()\n+            conns[0].close.assert_not_called()\n+            conns[0].close_if_unusable_or_obsolete.assert_called_with()\n+            conns[1].close.assert_not_called()\n+            conns[1].close_if_unusable_or_obsolete.assert_called_with()\n+            conns[2].close.assert_not_called()\n+            conns[2].close_if_unusable_or_obsolete.assert_called_with()\n \n             conns[1].close.side_effect = KeyError(\n                 'omg')\n+            f._close_database()\n+            with pytest.raises(KeyError):\n+                f._close_database(force=True)\n+\n+            conns[1].close.side_effect = None\n+            conns[1].close_if_unusable_or_obsolete.side_effect = KeyError(\n+                'omg')\n+            f._close_database(force=True)\n             with pytest.raises(KeyError):\n                 f._close_database()\n \n",
        "problem_statement": "Django celery fixup doesn't respect Django settings for PostgreSQL connections\nWhen using the Django-Celery fixup to run background tasks for a Django web service, the tasks in the background do not respect the settings in Django for PostgreSQL (possibly other) connections.  Every task will always create a new connection no matter the Django settings.  Although it is possible to bypass this with the environment variable CELERY_DB_REUSE_MAX, it is preferred for it to follow the settings given in Django.\r\n\r\n## Checklist\r\n\r\n- [ ] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\nCelery 4.0.2 with potentially all versions of Django (tested on 1.10.3 and 1.11.2)\r\n- [X] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\nThis line causes this \"issue\":\r\nhttps://github.com/celery/celery/blob/master/celery/fixups/django.py#L186\r\n\r\n## Steps to reproduce\r\nNote that these steps require some monitoring service to be used, we have New Relic.\r\nNote also that we use Heroku for this app in question.\r\n1) Have a web facing process with Django that connects to your PostgreSQL database for ORM purposes\r\n2) Have a worker process that also connects to the PostgreSQL for ORM purposes\r\n3) Have the DATABASES['default']['CONN_MAX_AGE'] setting set to anything that isn't 0 (easiest to see with `None` for persistent connections)\r\n4) Make multiple requests to the web portion of Django to cause some ORM activity (easiest to see if it happens on every request)\r\n5) Get multiple tasks to execute on the worker that will cause some ORM activity (easiest to see if it happens on every task)\r\n6) Use your monitoring service (New Relic in our case) to view a breakdown of all of the requests and worker activity.  In New Relic you can check this using the transaction tracing; select the endpoint/task that made the db queries and check the breakdown.\r\n\r\n## Expected behavior\r\npsycopg2:connect would occur rarely with an average calls per transaction <<< 1\r\n\r\n## Actual behavior\r\npsycopg2:connect occurs very rarely with an average calls per transaction of <<< 1 for the web processes.\r\npsycopg2:connect occurs every time with an average calls per transaction of 1 for the worker processes.\r\n\r\n## Potential Resolution\r\nWith my limited knowledge of Celery's inner workings, it feels like a fairly simple fix that I could make on a PR myself, but I wanted some input before I spend the time setting that all up.\r\nThis fix seems to work when monkey patched into the `DjangoWorkerFixup` class.\r\n``` Python\r\n\r\ndef _close_database(self):\r\n    try:\r\n        # Use Django's built in method of closing old connections.\r\n        # This ensures that the database settings are respected.\r\n        self._db.close_old_connections()\r\n    except AttributeError:\r\n        # Legacy functionality if we can't use the old connections for whatever reason.\r\n        for conn in self._db.connections.all():\r\n            try:\r\n                conn.close()\r\n            except self.interface_errors:\r\n                pass\r\n            except self.DatabaseError as exc:\r\n                str_exc = str(exc)\r\n                if 'closed' not in str_exc and 'not connected' not in str_exc:\r\n                    raise\r\n\r\ncelery.fixups.django.DjangoWorkerFixup._close_database = _close_database\r\n```\nDjango celery fixup doesn't respect Django settings for PostgreSQL connections\nWhen using the Django-Celery fixup to run background tasks for a Django web service, the tasks in the background do not respect the settings in Django for PostgreSQL (possibly other) connections.  Every task will always create a new connection no matter the Django settings.  Although it is possible to bypass this with the environment variable CELERY_DB_REUSE_MAX, it is preferred for it to follow the settings given in Django.\r\n\r\n## Checklist\r\n\r\n- [ ] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\nCelery 4.0.2 with potentially all versions of Django (tested on 1.10.3 and 1.11.2)\r\n- [X] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\nThis line causes this \"issue\":\r\nhttps://github.com/celery/celery/blob/master/celery/fixups/django.py#L186\r\n\r\n## Steps to reproduce\r\nNote that these steps require some monitoring service to be used, we have New Relic.\r\nNote also that we use Heroku for this app in question.\r\n1) Have a web facing process with Django that connects to your PostgreSQL database for ORM purposes\r\n2) Have a worker process that also connects to the PostgreSQL for ORM purposes\r\n3) Have the DATABASES['default']['CONN_MAX_AGE'] setting set to anything that isn't 0 (easiest to see with `None` for persistent connections)\r\n4) Make multiple requests to the web portion of Django to cause some ORM activity (easiest to see if it happens on every request)\r\n5) Get multiple tasks to execute on the worker that will cause some ORM activity (easiest to see if it happens on every task)\r\n6) Use your monitoring service (New Relic in our case) to view a breakdown of all of the requests and worker activity.  In New Relic you can check this using the transaction tracing; select the endpoint/task that made the db queries and check the breakdown.\r\n\r\n## Expected behavior\r\npsycopg2:connect would occur rarely with an average calls per transaction <<< 1\r\n\r\n## Actual behavior\r\npsycopg2:connect occurs very rarely with an average calls per transaction of <<< 1 for the web processes.\r\npsycopg2:connect occurs every time with an average calls per transaction of 1 for the worker processes.\r\n\r\n## Potential Resolution\r\nWith my limited knowledge of Celery's inner workings, it feels like a fairly simple fix that I could make on a PR myself, but I wanted some input before I spend the time setting that all up.\r\nThis fix seems to work when monkey patched into the `DjangoWorkerFixup` class.\r\n``` Python\r\n\r\ndef _close_database(self):\r\n    try:\r\n        # Use Django's built in method of closing old connections.\r\n        # This ensures that the database settings are respected.\r\n        self._db.close_old_connections()\r\n    except AttributeError:\r\n        # Legacy functionality if we can't use the old connections for whatever reason.\r\n        for conn in self._db.connections.all():\r\n            try:\r\n                conn.close()\r\n            except self.interface_errors:\r\n                pass\r\n            except self.DatabaseError as exc:\r\n                str_exc = str(exc)\r\n                if 'closed' not in str_exc and 'not connected' not in str_exc:\r\n                    raise\r\n\r\ncelery.fixups.django.DjangoWorkerFixup._close_database = _close_database\r\n```\n",
        "hints_text": "plz proceed with the PR\nif it is django-celery package only then that doesn't support celery 4.x\nplz proceed with the PR\nif it is django-celery package only then that doesn't support celery 4.x",
        "created_at": "2020-06-01T10:41:37Z",
        "version": "4.4",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "t/unit/fixups/test_django.py"
        ]
    },
    {
        "repo": "celery/celery",
        "pull_number": 6000,
        "instance_id": "celery__celery-6000",
        "issue_numbers": [
            "5994"
        ],
        "base_commit": "78d04b3758f882127c9a21e6cc5e6c1f4820927c",
        "patch": "diff --git a/celery/backends/redis.py b/celery/backends/redis.py\nindex a7aeb2e933e..aec18284780 100644\n--- a/celery/backends/redis.py\n+++ b/celery/backends/redis.py\n@@ -232,11 +232,14 @@ def __init__(self, host=None, port=None, db=None, password=None,\n             'max_connections': self.max_connections,\n             'socket_timeout': socket_timeout and float(socket_timeout),\n             'retry_on_timeout': retry_on_timeout or False,\n-            'socket_keepalive': socket_keepalive or False,\n             'socket_connect_timeout':\n                 socket_connect_timeout and float(socket_connect_timeout),\n         }\n \n+        # absent in redis.connection.UnixDomainSocketConnection\n+        if socket_keepalive:\n+            self.connparams['socket_keepalive'] = socket_keepalive\n+\n         # \"redis_backend_use_ssl\" must be a dict with the keys:\n         # 'ssl_cert_reqs', 'ssl_ca_certs', 'ssl_certfile', 'ssl_keyfile'\n         # (the same as \"broker_use_ssl\")\ndiff --git a/docs/userguide/configuration.rst b/docs/userguide/configuration.rst\nindex 5e63f376e95..ab6239ab84e 100644\n--- a/docs/userguide/configuration.rst\n+++ b/docs/userguide/configuration.rst\n@@ -1109,16 +1109,21 @@ in seconds (int/float), used by the redis result backend.\n ``redis_retry_on_timeout``\n ~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n+.. versionadded:: 4.4.1\n+\n Default: :const:`False`\n \n To retry reading/writing operations on TimeoutError to the Redis server,\n-used by the redis result backend.\n+used by the redis result backend. Shouldn't set this variable if using Redis\n+connection by unix socket.\n \n-.. setting:: socket_keepalive\n+.. setting:: redis_socket_keepalive\n \n-``socket_keepalive``\n+``redis_socket_keepalive``\n ~~~~~~~~~~~~~~~~~~~~\n \n+.. versionadded:: 4.4.1\n+\n Default: :const:`False`\n \n Socket TCP keepalive to keep connections healthy to the Redis server,\n",
        "test_patch": "diff --git a/t/unit/backends/test_redis.py b/t/unit/backends/test_redis.py\nindex 4b2b4e470b3..89914e22b16 100644\n--- a/t/unit/backends/test_redis.py\n+++ b/t/unit/backends/test_redis.py\n@@ -324,6 +324,7 @@ def test_socket_url(self):\n         assert 'port' not in x.connparams\n         assert x.connparams['socket_timeout'] == 30.0\n         assert 'socket_connect_timeout' not in x.connparams\n+        assert 'socket_keepalive' not in x.connparams\n         assert x.connparams['db'] == 3\n \n     @skip.unless_module('redis')\n",
        "problem_statement": "TypeError in make_connection, unexpected keyword argument 'socket_keepalive'\n# Checklist\r\n\r\n- [ ] I have verified that the issue exists against the `master` branch of Celery.\r\n- [ ] This has already been asked to the [discussion group](https://groups.google.com/forum/#!forum/celery-users) first.\r\n- [x] I have read the relevant section in the\r\n  [contribution guide](http://docs.celeryproject.org/en/latest/contributing.html#other-bugs)\r\n  on reporting bugs.\r\n- [x] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Bug+Report%22+-label%3A%22Category%3A+Documentation%22)\r\n  for similar or identical bug reports.\r\n- [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22PR+Type%3A+Bugfix%22+-label%3A%22Category%3A+Documentation%22)\r\n  for existing proposed fixes.\r\n- [x] I have checked the [commit log](https://github.com/celery/celery/commits/master)\r\n  to find out if the bug was already fixed in the master branch.\r\n- [x] I have included all related issues and possible duplicate issues\r\n  in this issue (If there are none, check this box anyway).\r\n\r\n## Mandatory Debugging Information\r\n\r\n- [ ] I have included the output of ``celery -A proj report`` in the issue.\r\n    (if you are not able to do this, then at least specify the Celery\r\n     version affected).\r\n- [ ] I have verified that the issue exists against the `master` branch of Celery.\r\n- [x] I have included the contents of ``pip freeze`` in the issue.\r\n- [x] I have included all the versions of all the external dependencies required\r\n  to reproduce this bug.\r\n\r\n## Optional Debugging Information\r\n\r\n- [ ] I have tried reproducing the issue on more than one Python version\r\n  and/or implementation.\r\n- [ ] I have tried reproducing the issue on more than one message broker and/or\r\n  result backend.\r\n- [ ] I have tried reproducing the issue on more than one version of the message\r\n  broker and/or result backend.\r\n- [ ] I have tried reproducing the issue on more than one operating system.\r\n- [ ] I have tried reproducing the issue on more than one workers pool.\r\n- [ ] I have tried reproducing the issue with autoscaling, retries,\r\n  ETA/Countdown & rate limits disabled.\r\n- [x] I have tried reproducing the issue after downgrading\r\n  and/or upgrading Celery and its dependencies.\r\n\r\n## Related Issues and Possible Duplicates\r\n\r\n#### Related Issues\r\n\r\n- #5952\r\n- #2903\r\n\r\n#### Possible Duplicates\r\n\r\n- None\r\n\r\n## Environment & Settings\r\n\r\n**Celery version**: 4.4.1\r\n\r\n# Steps to Reproduce\r\n\r\n## Required Dependencies\r\n\r\n### Python Packages\r\n\r\n<details>\r\n<summary><b><code>pip freeze</code> Output:</b></summary>\r\n<p>\r\n```\r\nredis==3.4.1\r\nbilliard==3.6.3.0\r\ncelery==4.4.1\r\nkombu==4.6.8\r\n```\r\n</p>\r\n</details>\r\n\r\n### Other Dependencies\r\n\r\n<details>\r\n<p>\r\nN/A\r\n</p>\r\n</details>\r\n\r\n## Minimally Reproducible Test Case\r\n\r\n<details>\r\n<p>\r\n\r\n```python\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n# Expected Behavior\r\n\r\nCelery just works\r\n\r\n# Actual Behavior\r\n\r\nAfter upgrading Celery from 4.4.0 to 4.4.1 I suddenly see `TypeError: __init__() got an unexpected keyword argument 'socket_keepalive'`.\r\n\r\nThis seems to be caused by PR #5952, which adds this kwarg for Redis connections. However, not all Redis connection constructors take the same arguments (e.g. `UnixDomainSocketConnection` doesn't take `socket_keepalive`). This seems to be happened before as noted in issue #2903.\r\n\r\nEverything works fine if I downgrade to 4.4.0 again.\n",
        "hints_text": "can you check the commit log after 4.4.1 release? this might have already resolved in master?\nI did check the commit log (I also checked that option in the checklist). None of the (now) 4 commits since the release of 4.4.1 seem related to this issue.\r\n\r\nI'm unable to test against master as the only place I can test this reliably is a production server. I was hoping Celery's tests would catch an issue like this, but it seems that the Travis pipeline is not running the integration tests.\nops sorry to know that! thanks for letting us know! will try to push another minor release ASAP",
        "created_at": "2020-03-12T11:15:12Z",
        "version": "4.4",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "t/unit/backends/test_redis.py"
        ]
    },
    {
        "repo": "celery/celery",
        "pull_number": 5921,
        "instance_id": "celery__celery-5921",
        "issue_numbers": [
            "5919",
            "5919"
        ],
        "base_commit": "f2ddd894c32f642a20f03b805b97e460f4fb3b4f",
        "patch": "diff --git a/celery/backends/redis.py b/celery/backends/redis.py\nindex 717d9823111..7c6fc0d8d75 100644\n--- a/celery/backends/redis.py\n+++ b/celery/backends/redis.py\n@@ -3,6 +3,7 @@\n from __future__ import absolute_import, unicode_literals\n \n import time\n+from contextlib import contextmanager\n from functools import partial\n from ssl import CERT_NONE, CERT_OPTIONAL, CERT_REQUIRED\n \n@@ -78,6 +79,11 @@\n \n E_LOST = 'Connection to Redis lost: Retry (%s/%s) %s.'\n \n+E_RETRY_LIMIT_EXCEEDED = \"\"\"\n+Retry limit exceeded while trying to reconnect to the Celery redis result \\\n+store backend. The Celery application must be restarted.\n+\"\"\"\n+\n logger = get_logger(__name__)\n \n \n@@ -88,6 +94,8 @@ def __init__(self, *args, **kwargs):\n         super(ResultConsumer, self).__init__(*args, **kwargs)\n         self._get_key_for_task = self.backend.get_key_for_task\n         self._decode_result = self.backend.decode_result\n+        self._ensure = self.backend.ensure\n+        self._connection_errors = self.backend.connection_errors\n         self.subscribed_to = set()\n \n     def on_after_fork(self):\n@@ -99,6 +107,31 @@ def on_after_fork(self):\n             logger.warning(text_t(e))\n         super(ResultConsumer, self).on_after_fork()\n \n+    def _reconnect_pubsub(self):\n+        self._pubsub = None\n+        self.backend.client.connection_pool.reset()\n+        # task state might have changed when the connection was down so we\n+        # retrieve meta for all subscribed tasks before going into pubsub mode\n+        metas = self.backend.client.mget(self.subscribed_to)\n+        metas = [meta for meta in metas if meta]\n+        for meta in metas:\n+            self.on_state_change(self._decode_result(meta), None)\n+        self._pubsub = self.backend.client.pubsub(\n+            ignore_subscribe_messages=True,\n+        )\n+        self._pubsub.subscribe(*self.subscribed_to)\n+\n+    @contextmanager\n+    def reconnect_on_error(self):\n+        try:\n+            yield\n+        except self._connection_errors:\n+            try:\n+                self._ensure(self._reconnect_pubsub, ())\n+            except self._connection_errors:\n+                logger.critical(E_RETRY_LIMIT_EXCEEDED)\n+                raise\n+\n     def _maybe_cancel_ready_task(self, meta):\n         if meta['status'] in states.READY_STATES:\n             self.cancel_for(meta['task_id'])\n@@ -124,9 +157,10 @@ def stop(self):\n \n     def drain_events(self, timeout=None):\n         if self._pubsub:\n-            message = self._pubsub.get_message(timeout=timeout)\n-            if message and message['type'] == 'message':\n-                self.on_state_change(self._decode_result(message['data']), message)\n+            with self.reconnect_on_error():\n+                message = self._pubsub.get_message(timeout=timeout)\n+                if message and message['type'] == 'message':\n+                    self.on_state_change(self._decode_result(message['data']), message)\n         elif timeout:\n             time.sleep(timeout)\n \n@@ -139,13 +173,15 @@ def _consume_from(self, task_id):\n         key = self._get_key_for_task(task_id)\n         if key not in self.subscribed_to:\n             self.subscribed_to.add(key)\n-            self._pubsub.subscribe(key)\n+            with self.reconnect_on_error():\n+                self._pubsub.subscribe(key)\n \n     def cancel_for(self, task_id):\n+        key = self._get_key_for_task(task_id)\n+        self.subscribed_to.discard(key)\n         if self._pubsub:\n-            key = self._get_key_for_task(task_id)\n-            self.subscribed_to.discard(key)\n-            self._pubsub.unsubscribe(key)\n+            with self.reconnect_on_error():\n+                self._pubsub.unsubscribe(key)\n \n \n class RedisBackend(BaseKeyValueStoreBackend, AsyncBackendMixin):\n",
        "test_patch": "diff --git a/t/unit/backends/test_redis.py b/t/unit/backends/test_redis.py\nindex 75ffee9dc23..4b2b4e470b3 100644\n--- a/t/unit/backends/test_redis.py\n+++ b/t/unit/backends/test_redis.py\n@@ -1,5 +1,6 @@\n from __future__ import absolute_import, unicode_literals\n \n+import json\n import random\n import ssl\n from contextlib import contextmanager\n@@ -26,6 +27,10 @@ def on_first_call(*args, **kwargs):\n         mock.return_value, = retval\n \n \n+class ConnectionError(Exception):\n+    pass\n+\n+\n class Connection(object):\n     connected = True\n \n@@ -55,9 +60,27 @@ def execute(self):\n         return [step(*a, **kw) for step, a, kw in self.steps]\n \n \n+class PubSub(mock.MockCallbacks):\n+    def __init__(self, ignore_subscribe_messages=False):\n+        self._subscribed_to = set()\n+\n+    def close(self):\n+        self._subscribed_to = set()\n+\n+    def subscribe(self, *args):\n+        self._subscribed_to.update(args)\n+\n+    def unsubscribe(self, *args):\n+        self._subscribed_to.difference_update(args)\n+\n+    def get_message(self, timeout=None):\n+        pass\n+\n+\n class Redis(mock.MockCallbacks):\n     Connection = Connection\n     Pipeline = Pipeline\n+    pubsub = PubSub\n \n     def __init__(self, host=None, port=None, db=None, password=None, **kw):\n         self.host = host\n@@ -71,6 +94,9 @@ def __init__(self, host=None, port=None, db=None, password=None, **kw):\n     def get(self, key):\n         return self.keyspace.get(key)\n \n+    def mget(self, keys):\n+        return [self.get(key) for key in keys]\n+\n     def setex(self, key, expires, value):\n         self.set(key, value)\n         self.expire(key, expires)\n@@ -144,7 +170,9 @@ class _RedisBackend(RedisBackend):\n         return _RedisBackend(app=self.app)\n \n     def get_consumer(self):\n-        return self.get_backend().result_consumer\n+        consumer = self.get_backend().result_consumer\n+        consumer._connection_errors = (ConnectionError,)\n+        return consumer\n \n     @patch('celery.backends.asynchronous.BaseResultConsumer.on_after_fork')\n     def test_on_after_fork(self, parent_method):\n@@ -194,6 +222,33 @@ def test_drain_events_before_start(self):\n         # drain_events shouldn't crash when called before start\n         consumer.drain_events(0.001)\n \n+    def test_consume_from_connection_error(self):\n+        consumer = self.get_consumer()\n+        consumer.start('initial')\n+        consumer._pubsub.subscribe.side_effect = (ConnectionError(), None)\n+        consumer.consume_from('some-task')\n+        assert consumer._pubsub._subscribed_to == {b'celery-task-meta-initial', b'celery-task-meta-some-task'}\n+\n+    def test_cancel_for_connection_error(self):\n+        consumer = self.get_consumer()\n+        consumer.start('initial')\n+        consumer._pubsub.unsubscribe.side_effect = ConnectionError()\n+        consumer.consume_from('some-task')\n+        consumer.cancel_for('some-task')\n+        assert consumer._pubsub._subscribed_to == {b'celery-task-meta-initial'}\n+\n+    @patch('celery.backends.redis.ResultConsumer.cancel_for')\n+    @patch('celery.backends.asynchronous.BaseResultConsumer.on_state_change')\n+    def test_drain_events_connection_error(self, parent_on_state_change, cancel_for):\n+        meta = {'task_id': 'initial', 'status': states.SUCCESS}\n+        consumer = self.get_consumer()\n+        consumer.start('initial')\n+        consumer.backend.set(b'celery-task-meta-initial', json.dumps(meta))\n+        consumer._pubsub.get_message.side_effect = ConnectionError()\n+        consumer.drain_events()\n+        parent_on_state_change.assert_called_with(meta, None)\n+        assert consumer._pubsub._subscribed_to == {b'celery-task-meta-initial'}\n+\n \n class test_RedisBackend:\n     def get_backend(self):\n",
        "problem_statement": "Handle Redis connection errors in result consumer\n<!--\r\nPlease fill this template entirely and do not erase parts of it.\r\nWe reserve the right to close without a response\r\nenhancement requests which are incomplete.\r\n-->\r\n# Checklist\r\n<!--\r\nTo check an item on the list replace [ ] with [x].\r\n-->\r\n\r\n- [x] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Enhancement%22+-label%3A%22Category%3A+Documentation%22)\r\n  for similar or identical enhancement to an existing feature.\r\n- [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22Issue+Type%3A+Enhancement%22+-label%3A%22Category%3A+Documentation%22)\r\n  for existing proposed enhancements.\r\n- [x] I have checked the [commit log](https://github.com/celery/celery/commits/master)\r\n  to find out if the if the same enhancement was already implemented in the\r\n  master branch.\r\n- [x] I have included all related issues and possible duplicate issues in this issue\r\n      (If there are none, check this box anyway).\r\n\r\n## Related Issues and Possible Duplicates\r\n<!--\r\nPlease make sure to search and mention any related issues\r\nor possible duplicates to this issue as requested by the checklist above.\r\n\r\nThis may or may not include issues in other repositories that the Celery project\r\nmaintains or other repositories that are dependencies of Celery.\r\n\r\nIf you don't know how to mention issues, please refer to Github's documentation\r\non the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests\r\n-->\r\n\r\n#### Related Issues\r\n\r\n- None\r\n\r\n#### Possible Duplicates\r\n\r\n- None\r\n\r\n# Brief Summary\r\n<!--\r\nPlease include a brief summary of what the enhancement is\r\nand why it is needed.\r\n-->\r\nWhen there is a connection error with Redis while executing a command, in most cases, the redis client will [discard the connection](https://github.com/andymccurdy/redis-py/blob/272d3139e1c82c2d89551f87d12df7c18d938ea2/redis/client.py#L885-L886), causing the next command sent to Redis to open a new connection. This allows applications to recover from connection errors by simply retrying, a property that is used in Celery, for example when setting keys in the Redis result backend: https://github.com/celery/celery/blob/d0563058f8f47f347ac1b56c44f833f569764482/celery/backends/redis.py#L324-L325\r\n\r\nThis is not the case however when the [connection to Redis is in a pubsub state](https://github.com/andymccurdy/redis-py/blob/272d3139e1c82c2d89551f87d12df7c18d938ea2/redis/client.py#L3397-L3414). The reason for that is that some state associated with the connection (namely the list of keys subscibed to). The Redis client doesn't keep track of this state, so it can't possibly restore it when creating a new connection and leaves the connection handling to the application code.\r\n\r\nThe Celery Redis result consumer uses pubsub in order to be notified when results are available, but doesn't handle connection errors at all, causing a result consumer to end up in a state where it can't connect to the result backend any more after a single connection error, as any further attempt will reuse the same faulty connection.\r\n\r\nThe solution would be to add error handling logic to the result consumer, so it will recreate the connection on connection errors and initialize it to the proper state.\r\n\r\n# Design\r\n\r\n## Architectural Considerations\r\n<!--\r\nIf more components other than Celery are involved,\r\ndescribe them here and the effect it would have on Celery.\r\n-->\r\nNone\r\n\r\n## Proposed Behavior\r\n<!--\r\nPlease describe in detail how this enhancement is going to change the behavior\r\nof an existing feature.\r\nDescribe what happens in case of failures as well if applicable.\r\n-->\r\nAdd error handling in all places the Redis result consumer sends a Redis command in a pubsub context:\r\n* https://github.com/celery/celery/blob/d0563058f8f47f347ac1b56c44f833f569764482/celery/backends/redis.py#L127\r\n* https://github.com/celery/celery/blob/d0563058f8f47f347ac1b56c44f833f569764482/celery/backends/redis.py#L142\r\n* https://github.com/celery/celery/blob/d0563058f8f47f347ac1b56c44f833f569764482/celery/backends/redis.py#L148\r\n\r\nWe should catch all Redis connection errors, and call a new method that will reinitialize a pubsub connection in the proper state (discard the current connection from the pool, start the pubsub context, subscribe to all keys in `ResultConsumer.subscribed_to`) using the retry policy. If in `drain_events`, we should try to get new messages again. \r\n\r\nThis will take care of most issues with connection errors. I see two remaining issues:\r\n1.Some message might have been lost (sent between losing the connection and reconnecting). We could read all keys subscribed to right after reconnecting and before starting the pubsub context and call `on_state_change` for each existing key, but this might cause some messages to be delivered twice and I don't know how Celery will react to that.\r\n2. If the connection can't be re-established despite the retries and reaches max-retries, the result consumer will end up with a faulty connection that can't be recovered from. This should be communicated somehow to the user (documentation, logging an explicit error message, custom exception).\r\n\r\n## Proposed UI/UX\r\n<!--\r\nPlease provide your ideas for the API, CLI options,\r\nconfiguration key names etc. that will be adjusted for this enhancement.\r\n-->\r\nNone\r\n\r\n## Diagrams\r\n<!--\r\nPlease include any diagrams that might be relevant\r\nto the implementation of this enhancement such as:\r\n* Class Diagrams\r\n* Sequence Diagrams\r\n* Activity Diagrams\r\nYou can drag and drop images into the text box to attach them to this issue.\r\n-->\r\nN/A\r\n\r\n## Alternatives\r\n<!--\r\nIf you have considered any alternative implementations\r\ndescribe them in detail below.\r\n-->\r\nNone\r\n\nHandle Redis connection errors in result consumer\n<!--\r\nPlease fill this template entirely and do not erase parts of it.\r\nWe reserve the right to close without a response\r\nenhancement requests which are incomplete.\r\n-->\r\n# Checklist\r\n<!--\r\nTo check an item on the list replace [ ] with [x].\r\n-->\r\n\r\n- [x] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Enhancement%22+-label%3A%22Category%3A+Documentation%22)\r\n  for similar or identical enhancement to an existing feature.\r\n- [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22Issue+Type%3A+Enhancement%22+-label%3A%22Category%3A+Documentation%22)\r\n  for existing proposed enhancements.\r\n- [x] I have checked the [commit log](https://github.com/celery/celery/commits/master)\r\n  to find out if the if the same enhancement was already implemented in the\r\n  master branch.\r\n- [x] I have included all related issues and possible duplicate issues in this issue\r\n      (If there are none, check this box anyway).\r\n\r\n## Related Issues and Possible Duplicates\r\n<!--\r\nPlease make sure to search and mention any related issues\r\nor possible duplicates to this issue as requested by the checklist above.\r\n\r\nThis may or may not include issues in other repositories that the Celery project\r\nmaintains or other repositories that are dependencies of Celery.\r\n\r\nIf you don't know how to mention issues, please refer to Github's documentation\r\non the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests\r\n-->\r\n\r\n#### Related Issues\r\n\r\n- None\r\n\r\n#### Possible Duplicates\r\n\r\n- None\r\n\r\n# Brief Summary\r\n<!--\r\nPlease include a brief summary of what the enhancement is\r\nand why it is needed.\r\n-->\r\nWhen there is a connection error with Redis while executing a command, in most cases, the redis client will [discard the connection](https://github.com/andymccurdy/redis-py/blob/272d3139e1c82c2d89551f87d12df7c18d938ea2/redis/client.py#L885-L886), causing the next command sent to Redis to open a new connection. This allows applications to recover from connection errors by simply retrying, a property that is used in Celery, for example when setting keys in the Redis result backend: https://github.com/celery/celery/blob/d0563058f8f47f347ac1b56c44f833f569764482/celery/backends/redis.py#L324-L325\r\n\r\nThis is not the case however when the [connection to Redis is in a pubsub state](https://github.com/andymccurdy/redis-py/blob/272d3139e1c82c2d89551f87d12df7c18d938ea2/redis/client.py#L3397-L3414). The reason for that is that some state associated with the connection (namely the list of keys subscibed to). The Redis client doesn't keep track of this state, so it can't possibly restore it when creating a new connection and leaves the connection handling to the application code.\r\n\r\nThe Celery Redis result consumer uses pubsub in order to be notified when results are available, but doesn't handle connection errors at all, causing a result consumer to end up in a state where it can't connect to the result backend any more after a single connection error, as any further attempt will reuse the same faulty connection.\r\n\r\nThe solution would be to add error handling logic to the result consumer, so it will recreate the connection on connection errors and initialize it to the proper state.\r\n\r\n# Design\r\n\r\n## Architectural Considerations\r\n<!--\r\nIf more components other than Celery are involved,\r\ndescribe them here and the effect it would have on Celery.\r\n-->\r\nNone\r\n\r\n## Proposed Behavior\r\n<!--\r\nPlease describe in detail how this enhancement is going to change the behavior\r\nof an existing feature.\r\nDescribe what happens in case of failures as well if applicable.\r\n-->\r\nAdd error handling in all places the Redis result consumer sends a Redis command in a pubsub context:\r\n* https://github.com/celery/celery/blob/d0563058f8f47f347ac1b56c44f833f569764482/celery/backends/redis.py#L127\r\n* https://github.com/celery/celery/blob/d0563058f8f47f347ac1b56c44f833f569764482/celery/backends/redis.py#L142\r\n* https://github.com/celery/celery/blob/d0563058f8f47f347ac1b56c44f833f569764482/celery/backends/redis.py#L148\r\n\r\nWe should catch all Redis connection errors, and call a new method that will reinitialize a pubsub connection in the proper state (discard the current connection from the pool, start the pubsub context, subscribe to all keys in `ResultConsumer.subscribed_to`) using the retry policy. If in `drain_events`, we should try to get new messages again. \r\n\r\nThis will take care of most issues with connection errors. I see two remaining issues:\r\n1.Some message might have been lost (sent between losing the connection and reconnecting). We could read all keys subscribed to right after reconnecting and before starting the pubsub context and call `on_state_change` for each existing key, but this might cause some messages to be delivered twice and I don't know how Celery will react to that.\r\n2. If the connection can't be re-established despite the retries and reaches max-retries, the result consumer will end up with a faulty connection that can't be recovered from. This should be communicated somehow to the user (documentation, logging an explicit error message, custom exception).\r\n\r\n## Proposed UI/UX\r\n<!--\r\nPlease provide your ideas for the API, CLI options,\r\nconfiguration key names etc. that will be adjusted for this enhancement.\r\n-->\r\nNone\r\n\r\n## Diagrams\r\n<!--\r\nPlease include any diagrams that might be relevant\r\nto the implementation of this enhancement such as:\r\n* Class Diagrams\r\n* Sequence Diagrams\r\n* Activity Diagrams\r\nYou can drag and drop images into the text box to attach them to this issue.\r\n-->\r\nN/A\r\n\r\n## Alternatives\r\n<!--\r\nIf you have considered any alternative implementations\r\ndescribe them in detail below.\r\n-->\r\nNone\r\n\n",
        "hints_text": "\n",
        "created_at": "2020-01-15T13:11:47Z",
        "version": "4.4",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "t/unit/backends/test_redis.py"
        ]
    },
    {
        "repo": "celery/celery",
        "pull_number": 5869,
        "instance_id": "celery__celery-5869",
        "issue_numbers": [
            "4684"
        ],
        "base_commit": "cf829307991da3815e1f7b105e736d13dbc7a325",
        "patch": "diff --git a/celery/app/base.py b/celery/app/base.py\nindex f732824f443..625d4f77233 100644\n--- a/celery/app/base.py\n+++ b/celery/app/base.py\n@@ -460,11 +460,24 @@ def _task_from_fun(self, fun, name=None, base=None, bind=False, **options):\n             self._tasks[task.name] = task\n             task.bind(self)  # connects task to this app\n \n-            autoretry_for = tuple(options.get('autoretry_for', ()))\n-            retry_kwargs = options.get('retry_kwargs', {})\n-            retry_backoff = int(options.get('retry_backoff', False))\n-            retry_backoff_max = int(options.get('retry_backoff_max', 600))\n-            retry_jitter = options.get('retry_jitter', True)\n+            autoretry_for = tuple(\n+                options.get('autoretry_for',\n+                            getattr(task, 'autoretry_for', ()))\n+            )\n+            retry_kwargs = options.get(\n+                'retry_kwargs', getattr(task, 'retry_kwargs', {})\n+            )\n+            retry_backoff = int(\n+                options.get('retry_backoff',\n+                            getattr(task, 'retry_backoff', False))\n+            )\n+            retry_backoff_max = int(\n+                options.get('retry_backoff_max',\n+                            getattr(task, 'retry_backoff_max', 600))\n+            )\n+            retry_jitter = options.get(\n+                'retry_jitter', getattr(task, 'retry_jitter', True)\n+            )\n \n             if autoretry_for and not hasattr(task, '_orig_run'):\n \n",
        "test_patch": "diff --git a/t/unit/tasks/test_tasks.py b/t/unit/tasks/test_tasks.py\nindex 5349b784418..5f9148fb3bd 100644\n--- a/t/unit/tasks/test_tasks.py\n+++ b/t/unit/tasks/test_tasks.py\n@@ -43,6 +43,14 @@ class TaskWithPriority(Task):\n     priority = 10\n \n \n+class TaskWithRetry(Task):\n+    autoretry_for = (TypeError,)\n+    retry_kwargs = {'max_retries': 5}\n+    retry_backoff = True\n+    retry_backoff_max = 700\n+    retry_jitter = False\n+\n+\n class TasksCase:\n \n     def setup(self):\n@@ -152,6 +160,81 @@ def autoretry_backoff_jitter_task(self, url):\n \n         self.autoretry_backoff_jitter_task = autoretry_backoff_jitter_task\n \n+        @self.app.task(bind=True, base=TaskWithRetry, shared=False)\n+        def autoretry_for_from_base_task(self, a, b):\n+            self.iterations += 1\n+            return a + b\n+\n+        self.autoretry_for_from_base_task = autoretry_for_from_base_task\n+\n+        @self.app.task(bind=True, base=TaskWithRetry,\n+                       autoretry_for=(ZeroDivisionError,), shared=False)\n+        def override_autoretry_for_from_base_task(self, a, b):\n+            self.iterations += 1\n+            return a / b\n+\n+        self.override_autoretry_for = override_autoretry_for_from_base_task\n+\n+        @self.app.task(bind=True, base=TaskWithRetry, shared=False)\n+        def retry_kwargs_from_base_task(self, a, b):\n+            self.iterations += 1\n+            return a + b\n+\n+        self.retry_kwargs_from_base_task = retry_kwargs_from_base_task\n+\n+        @self.app.task(bind=True, base=TaskWithRetry,\n+                       retry_kwargs={'max_retries': 2}, shared=False)\n+        def override_retry_kwargs_from_base_task(self, a, b):\n+            self.iterations += 1\n+            return a + b\n+\n+        self.override_retry_kwargs = override_retry_kwargs_from_base_task\n+\n+        @self.app.task(bind=True, base=TaskWithRetry, shared=False)\n+        def retry_backoff_from_base_task(self, a, b):\n+            self.iterations += 1\n+            return a + b\n+\n+        self.retry_backoff_from_base_task = retry_backoff_from_base_task\n+\n+        @self.app.task(bind=True, base=TaskWithRetry,\n+                       retry_backoff=False, shared=False)\n+        def override_retry_backoff_from_base_task(self, a, b):\n+            self.iterations += 1\n+            return a + b\n+\n+        self.override_retry_backoff = override_retry_backoff_from_base_task\n+\n+        @self.app.task(bind=True, base=TaskWithRetry, shared=False)\n+        def retry_backoff_max_from_base_task(self, a, b):\n+            self.iterations += 1\n+            return a + b\n+\n+        self.retry_backoff_max_from_base_task = retry_backoff_max_from_base_task\n+\n+        @self.app.task(bind=True, base=TaskWithRetry,\n+                       retry_backoff_max=16, shared=False)\n+        def override_retry_backoff_max_from_base_task(self, a, b):\n+            self.iterations += 1\n+            return a + b\n+\n+        self.override_backoff_max = override_retry_backoff_max_from_base_task\n+\n+        @self.app.task(bind=True, base=TaskWithRetry, shared=False)\n+        def retry_backoff_jitter_from_base_task(self, a, b):\n+            self.iterations += 1\n+            return a + b\n+\n+        self.retry_backoff_jitter_from_base = retry_backoff_jitter_from_base_task\n+\n+        @self.app.task(bind=True, base=TaskWithRetry,\n+                       retry_jitter=True, shared=False)\n+        def override_backoff_jitter_from_base_task(self, a, b):\n+            self.iterations += 1\n+            return a + b\n+\n+        self.override_backoff_jitter = override_backoff_jitter_from_base_task\n+\n         @self.app.task(bind=True)\n         def task_check_request_context(self):\n             assert self.request.hostname == socket.gethostname()\n@@ -373,6 +456,94 @@ def test_autoretry_backoff_jitter(self, randrange):\n         ]\n         assert retry_call_countdowns == [0, 1, 3, 7]\n \n+    def test_autoretry_for_from_base(self):\n+        self.autoretry_for_from_base_task.iterations = 0\n+        self.autoretry_for_from_base_task.apply((1, \"a\"))\n+        assert self.autoretry_for_from_base_task.iterations == 6\n+\n+    def test_override_autoretry_for_from_base(self):\n+        self.override_autoretry_for.iterations = 0\n+        self.override_autoretry_for.apply((1, 0))\n+        assert self.override_autoretry_for.iterations == 6\n+\n+    def test_retry_kwargs_from_base(self):\n+        self.retry_kwargs_from_base_task.iterations = 0\n+        self.retry_kwargs_from_base_task.apply((1, \"a\"))\n+        assert self.retry_kwargs_from_base_task.iterations == 6\n+\n+    def test_override_retry_kwargs_from_base(self):\n+        self.override_retry_kwargs.iterations = 0\n+        self.override_retry_kwargs.apply((1, \"a\"))\n+        assert self.override_retry_kwargs.iterations == 3\n+\n+    def test_retry_backoff_from_base(self):\n+        task = self.retry_backoff_from_base_task\n+        task.iterations = 0\n+        with patch.object(task, 'retry', wraps=task.retry) as fake_retry:\n+            task.apply((1, \"a\"))\n+\n+        assert task.iterations == 6\n+        retry_call_countdowns = [\n+            call[1]['countdown'] for call in fake_retry.call_args_list\n+        ]\n+        assert retry_call_countdowns == [1, 2, 4, 8, 16, 32]\n+\n+    @patch('celery.app.base.get_exponential_backoff_interval')\n+    def test_override_retry_backoff_from_base(self, backoff):\n+        self.override_retry_backoff.iterations = 0\n+        self.override_retry_backoff.apply((1, \"a\"))\n+        assert self.override_retry_backoff.iterations == 6\n+        assert backoff.call_count == 0\n+\n+    def test_retry_backoff_max_from_base(self):\n+        task = self.retry_backoff_max_from_base_task\n+        task.iterations = 0\n+        with patch.object(task, 'retry', wraps=task.retry) as fake_retry:\n+            task.apply((1, \"a\"))\n+\n+        assert task.iterations == 6\n+        retry_call_countdowns = [\n+            call[1]['countdown'] for call in fake_retry.call_args_list\n+        ]\n+        assert retry_call_countdowns == [1, 2, 4, 8, 16, 32]\n+\n+    def test_override_retry_backoff_max_from_base(self):\n+        task = self.override_backoff_max\n+        task.iterations = 0\n+        with patch.object(task, 'retry', wraps=task.retry) as fake_retry:\n+            task.apply((1, \"a\"))\n+\n+        assert task.iterations == 6\n+        retry_call_countdowns = [\n+            call[1]['countdown'] for call in fake_retry.call_args_list\n+        ]\n+        assert retry_call_countdowns == [1, 2, 4, 8, 16, 16]\n+\n+    def test_retry_backoff_jitter_from_base(self):\n+        task = self.retry_backoff_jitter_from_base\n+        task.iterations = 0\n+        with patch.object(task, 'retry', wraps=task.retry) as fake_retry:\n+            task.apply((1, \"a\"))\n+\n+        assert task.iterations == 6\n+        retry_call_countdowns = [\n+            call[1]['countdown'] for call in fake_retry.call_args_list\n+        ]\n+        assert retry_call_countdowns == [1, 2, 4, 8, 16, 32]\n+\n+    @patch('random.randrange', side_effect=lambda i: i - 2)\n+    def test_override_backoff_jitter_from_base(self, randrange):\n+        task = self.override_backoff_jitter\n+        task.iterations = 0\n+        with patch.object(task, 'retry', wraps=task.retry) as fake_retry:\n+            task.apply((1, \"a\"))\n+\n+        assert task.iterations == 6\n+        retry_call_countdowns = [\n+            call[1]['countdown'] for call in fake_retry.call_args_list\n+        ]\n+        assert retry_call_countdowns == [0, 1, 3, 7, 15, 31]\n+\n     def test_retry_wrong_eta_when_not_enable_utc(self):\n         \"\"\"Issue #3753\"\"\"\n         self.app.conf.enable_utc = False\n",
        "problem_statement": "autoretry_for and retry_kwargs for class-based tasks\npython==3.6.4\r\ncelery==4.1.0\r\n\r\n## Steps to reproduce\r\n```\r\nfrom project.celery import app\r\n\r\n\r\nclass test_task(celery.Task):\r\n    autoretry_for = (Exception,)\r\n    retry_kwargs = {\r\n        'max_retries': 5,\r\n        'countdown': 10,\r\n    }\r\n\r\n    def run(self, *args, **kwargs):\r\n        raise Exception('test')\r\n\r\n\r\ntest_task = app.register_task(test_task())\r\n\r\ntest_task.delay()\r\n```\r\n\r\n## Expected behavior\r\n5 retries of this task with countdown of 10 seconds each.\r\n\r\n## Actual behavior\r\nNo retries at all.\r\n\r\n\r\nSo the main question - is there any way for specifying  autoretry_for and retry_kwargs for class-based tasks? Just to mention - function-based tasks works properly with shared_task decorator and this arguments for it.\n",
        "hints_text": "Exactly the same issue for me. It looks like the `autoretry_for` attribute is only touched in `_task_from_fun`, which isn't used for a class based task. At least it looks like this. Is there a special reason or was this just forgotten?\n@CompadreP maybe as a little workaround a class similar to this could help you:\r\n```python\r\nclass AutoRetryTask(Task):\r\n    default_retry_delay = 3\r\n    max_retries = 5\r\n    autoretry_for = None\r\n    retry_kwargs = {}\r\n\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n\r\n        if self.autoretry_for and not hasattr(self, '_orig_run'):\r\n            @wraps(self.run)\r\n            def run(*args, **kwargs):\r\n                try:\r\n                    return self._orig_run(*args, **kwargs)\r\n                except self.autoretry_for as exc:\r\n                    if 'countdown' not in self.retry_kwargs:\r\n                        countdown = int(random.uniform(2, 4) ** self.request.retries)\r\n                        retry_kwargs = self.retry_kwargs.copy()\r\n                        retry_kwargs.update({'countdown': countdown})\r\n                    else:\r\n                        retry_kwargs = self.retry_kwargs\r\n                    raise self.retry(exc=exc, **retry_kwargs)\r\n\r\n            self._orig_run, self.run = self.run, run\r\n```\nI think this would be a very neat feature (unless we want to actively discourage the use of class based tasks altogether) - if one of the maintainers could mention their opinion on this (and the `Is there a special reason or was this just forgotten?` question), I could take a stab at implementing it.\nAny update on this?\nI was able to fix that easily by overriding Celery task decorator:\r\n```\r\nclass CustomCelery(Celery):\r\n\r\n    def task(self, *args, **opts):\r\n\r\n        # Adds autoretry kwargs to @celery.task() decorators\r\n        if opts.get('base') == BaseTask:\r\n            opts['autoretry_for'] = (requests.ConnectionError, ConnectionError)\r\n            opts['retry_kwargs'] = {'max_retries': 5}\r\n            opts['retry_backoff'] = True\r\n\r\n        return super().task(*args, **opts)\r\n```\n@amir-hadi Thanks for the idea. With small modifications I've got working retries with exponential backoff.\r\n\r\nCelery 4.3.0\r\nPython 3.7.4\r\n\r\n```python\r\nfrom celery import Task\r\nfrom celery.utils.time import get_exponential_backoff_interval\r\n\r\nclass AutoRetryTask(Task):\r\n    retry_kwargs = {\r\n        'max_retries': 5,\r\n    }\r\n    retry_backoff = True\r\n    retry_backoff_max = 600\r\n    retry_jitter = True\r\n    \r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n\r\n        if self.autoretry_for and not hasattr(self, '_orig_run'):\r\n            def run(*args, **kwargs):\r\n                try:\r\n                    return self._orig_run(*args, **kwargs)\r\n                except self.autoretry_for as exc:\r\n                    if 'countdown' not in self.retry_kwargs:\r\n                        countdown = get_exponential_backoff_interval(\r\n                            factor=self.retry_backoff,\r\n                            retries=self.request.retries,\r\n                            maximum=self.retry_backoff_max,\r\n                            full_jitter=self.retry_jitter,\r\n                        )\r\n\r\n                        retry_kwargs = self.retry_kwargs.copy()\r\n                        retry_kwargs.update({'countdown': countdown})\r\n                    else:\r\n                        retry_kwargs = self.retry_kwargs\r\n\r\n                    retry_kwargs.update({'exc': exc})\r\n                    raise self.retry(**retry_kwargs)\r\n\r\n            self._orig_run, self.run = self.run, run\r\n```\r\n\r\nFurther improvements:\r\n* Make it a base task\r\n* Define `retry_*` properties from some config object/file",
        "created_at": "2019-12-11T19:38:57Z",
        "version": "4.4",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "t/unit/tasks/test_tasks.py"
        ]
    },
    {
        "repo": "celery/celery",
        "pull_number": 5773,
        "instance_id": "celery__celery-5773",
        "issue_numbers": [
            "5772"
        ],
        "base_commit": "a6453043fbfa676cca22c4945f3f165ce8eb4ec0",
        "patch": "diff --git a/celery/app/amqp.py b/celery/app/amqp.py\nindex 35c9f224c8d..2bf8c1d8de7 100644\n--- a/celery/app/amqp.py\n+++ b/celery/app/amqp.py\n@@ -130,9 +130,9 @@ def add_compat(self, name, **options):\n         return self._add(Queue.from_dict(name, **options))\n \n     def _add(self, queue):\n+        if queue.exchange is None or queue.exchange.name == '':\n+            queue.exchange = self.default_exchange\n         if not queue.routing_key:\n-            if queue.exchange is None or queue.exchange.name == '':\n-                queue.exchange = self.default_exchange\n             queue.routing_key = self.default_routing_key\n         if self.ha_policy:\n             if queue.queue_arguments is None:\n",
        "test_patch": "diff --git a/t/unit/app/test_amqp.py b/t/unit/app/test_amqp.py\nindex 30705ed7c1b..7ca94fe4807 100644\n--- a/t/unit/app/test_amqp.py\n+++ b/t/unit/app/test_amqp.py\n@@ -188,6 +188,35 @@ def test_setting_default_queue(self, name, exchange, rkey):\n         assert queue.routing_key == rkey or name\n \n \n+class test_default_exchange:\n+\n+    @pytest.mark.parametrize('name,exchange,rkey', [\n+        ('default', 'foo', None),\n+        ('default', 'foo', 'routing_key'),\n+    ])\n+    def test_setting_default_exchange(self, name, exchange, rkey):\n+        q = Queue(name, routing_key=rkey)\n+        self.app.conf.task_queues = {q}\n+        self.app.conf.task_default_exchange = exchange\n+        queues = dict(self.app.amqp.queues)\n+        queue = queues[name]\n+        assert queue.exchange.name == exchange\n+\n+    @pytest.mark.parametrize('name,extype,rkey', [\n+        ('default', 'direct', None),\n+        ('default', 'direct', 'routing_key'),\n+        ('default', 'topic', None),\n+        ('default', 'topic', 'routing_key'),\n+    ])\n+    def test_setting_default_exchange_type(self, name, extype, rkey):\n+        q = Queue(name, routing_key=rkey)\n+        self.app.conf.task_queues = {q}\n+        self.app.conf.task_default_exchange_type = extype\n+        queues = dict(self.app.amqp.queues)\n+        queue = queues[name]\n+        assert queue.exchange.type == extype\n+\n+\n class test_AMQP_proto1:\n \n     def test_kwargs_must_be_mapping(self):\n",
        "problem_statement": "task_default_exchange&task_default_exchange_type config not work\n<!--\r\nPlease fill this template entirely and do not erase parts of it.\r\nWe reserve the right to close without a response\r\nbug reports which are incomplete.\r\n-->\r\n# Checklist\r\n<!--\r\nTo check an item on the list replace [ ] with [x].\r\n-->\r\n\r\n- [x] I have read the relevant section in the\r\n  [contribution guide](http://docs.celeryproject.org/en/latest/contributing.html#other-bugs)\r\n  on reporting bugs.\r\n- [x] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Bug+Report%22+-label%3A%22Category%3A+Documentation%22)\r\n  for similar or identical bug reports.\r\n- [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22PR+Type%3A+Bugfix%22+-label%3A%22Category%3A+Documentation%22)\r\n  for existing proposed fixes.\r\n- [x] I have checked the [commit log](https://github.com/celery/celery/commits/master)\r\n  to find out if the bug was already fixed in the master branch.\r\n- [x] I have included all related issues and possible duplicate issues\r\n  in this issue (If there are none, check this box anyway).\r\n\r\n## Mandatory Debugging Information\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n    (if you are not able to do this, then at least specify the Celery\r\n     version affected).\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n- [ ] I have included the contents of ``pip freeze`` in the issue.\r\n- [ ] I have included all the versions of all the external dependencies required\r\n  to reproduce this bug.\r\n\r\n## Optional Debugging Information\r\n<!--\r\nTry some of the below if you think they are relevant.\r\nIt will help us figure out the scope of the bug and how many users it affects.\r\n-->\r\n- [x] I have tried reproducing the issue on more than one Python version\r\n  and/or implementation.\r\n- [ ] I have tried reproducing the issue on more than one message broker and/or\r\n  result backend.\r\n- [ ] I have tried reproducing the issue on more than one version of the message\r\n  broker and/or result backend.\r\n- [ ] I have tried reproducing the issue on more than one operating system.\r\n- [x] I have tried reproducing the issue on more than one workers pool.\r\n- [ ] I have tried reproducing the issue with autoscaling, retries,\r\n  ETA/Countdown & rate limits disabled.\r\n- [ ] I have tried reproducing the issue after downgrading\r\n  and/or upgrading Celery and its dependencies.\r\n\r\n## Related Issues and Possible Duplicates\r\n<!--\r\nPlease make sure to search and mention any related issues\r\nor possible duplicates to this issue as requested by the checklist above.\r\n\r\nThis may or may not include issues in other repositories that the Celery project\r\nmaintains or other repositories that are dependencies of Celery.\r\n\r\nIf you don't know how to mention issues, please refer to Github's documentation\r\non the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests\r\n-->\r\n\r\n#### Related Issues\r\n\r\n- #3926 \r\n\r\n#### Possible Duplicates\r\n\r\n- None\r\n\r\n## Environment & Settings\r\n<!-- Include the contents of celery --version below -->\r\n**Celery version**: 4.3.0\r\n<!-- Include the output of celery -A proj report below -->\r\n<details>\r\n<summary><b><code>celery report</code> Output:</b></summary>\r\n<p>\r\n\r\n```\r\nsoftware -> celery:4.3.0 (rhubarb) kombu:4.6.4 py:3.6.0\r\n            billiard:3.6.1.0 librabbitmq:2.0.0\r\nplatform -> system:Linux arch:64bit, ELF\r\n            kernel version:4.19.71-1-lts imp:CPython\r\nloader   -> celery.loaders.default.Loader\r\nsettings -> transport:librabbitmq results:disabled\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n# Steps to Reproduce\r\n\r\n## Required Dependencies\r\n<!-- Please fill the required dependencies to reproduce this issue -->\r\n* **Minimal Python Version**: 3.6.0\r\n* **Minimal Celery Version**: 4.3.0\r\n* **Minimal Kombu Version**: 4.6.4\r\n* **Minimal Broker Version**: RabbitMQ 3.7.15\r\n* **Minimal Result Backend Version**: N/A or Unknown\r\n* **Minimal OS and/or Kernel Version**: Linux 4.19.71-1-lts\r\n* **Minimal Broker Client Version**: N/A or Unknown\r\n* **Minimal Result Backend Client Version**: N/A or Unknown\r\n\r\n### Python Packages\r\n<!-- Please fill the contents of pip freeze below -->\r\n<details>\r\n<summary><b><code>pip freeze</code> Output:</b></summary>\r\n<p>\r\n\r\n```\r\namqp==2.5.1\r\nasn1crypto==0.24.0\r\natomicwrites==1.3.0\r\nattrs==19.1.0\r\nAutomat==0.7.0\r\nbackcall==0.1.0\r\nbilliard==3.6.1.0\r\ncase==1.5.3\r\ncelery==4.3.0\r\ncffi==1.12.3\r\nconstantly==15.1.0\r\ncryptography==2.7\r\ncssselect==1.1.0\r\ndecorator==4.4.0\r\nhyperlink==19.0.0\r\nidna==2.8\r\nimportlib-metadata==0.23\r\nincremental==17.5.0\r\nipython==7.8.0\r\nipython-genutils==0.2.0\r\njedi==0.15.1\r\nkombu==4.6.4\r\nlibrabbitmq==2.0.0\r\nlinecache2==1.0.0\r\nlxml==4.4.1\r\nmock==3.0.5\r\nmore-itertools==7.2.0\r\nmysqlclient==1.4.4\r\nnose==1.3.7\r\npackaging==19.2\r\nparsel==1.5.2\r\nparso==0.5.1\r\npexpect==4.7.0\r\npickleshare==0.7.5\r\npluggy==0.13.0\r\nprompt-toolkit==2.0.9\r\nptyprocess==0.6.0\r\npy==1.8.0\r\npyasn1==0.4.7\r\npyasn1-modules==0.2.6\r\npycparser==2.19\r\nPyDispatcher==2.0.5\r\nPygments==2.4.2\r\nPyHamcrest==1.9.0\r\npyOpenSSL==19.0.0\r\npyparsing==2.4.2\r\npytest==5.2.1\r\npytz==2019.2\r\nqueuelib==1.5.0\r\nScrapy==1.7.3\r\nscrapy-selenium==0.0.7\r\nselenium==3.141.0\r\nservice-identity==18.1.0\r\nsix==1.12.0\r\nSQLAlchemy==1.3.8\r\ntraceback2==1.4.0\r\ntraitlets==4.3.2\r\nTwisted==19.7.0\r\nunittest2==1.1.0\r\nurllib3==1.25.6\r\nvine==1.3.0\r\nw3lib==1.21.0\r\nwcwidth==0.1.7\r\nzipp==0.6.0\r\nzope.interface==4.6.0\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n### Other Dependencies\r\n<!--\r\nPlease provide system dependencies, configuration files\r\nand other dependency information if applicable\r\n-->\r\n<details>\r\n<p>\r\nN/A\r\n</p>\r\n</details>\r\n\r\n## Minimally Reproducible Test Case\r\n<!--\r\nPlease provide a reproducible test case.\r\nRefer to the Reporting Bugs section in our contribution guide.\r\n\r\nWe prefer submitting test cases in the form of a PR to our integration test suite.\r\nIf you can provide one, please mention the PR number below.\r\nIf not, please attach the most minimal code example required to reproduce the issue below.\r\nIf the test case is too large, please include a link to a gist or a repository below.\r\n-->\r\n\r\n<details>\r\n<p>\r\nceleryconfig.py:\r\n\r\n```python\r\nbroker_url = 'amqp://guest:guest@localhost:5672//'\r\n\r\ntask_default_queue = 'default'\r\ntask_default_exchange = 'tasks'\r\ntask_default_exchange_type = 'topic'\r\ntask_default_routing_key = 'tasks.default'\r\ntask_queues = (\r\n    Queue('default',   routing_key='tasks.#'),\r\n    Queue('test', routing_key='test.#'),\r\n)\r\n```\r\n</p>\r\n\r\n<p>\r\ncelery.py:\r\n\r\n```python\r\napp = Celery('scan_worker')\r\napp.conf.task_default_exchange = 'tasks'\r\napp.conf.task_default_exchange_type = 'topic'\r\napp.config_from_object('test_celery.celeryconfig', force=True)\r\n```\r\n</p>\r\n</details>\r\n\r\n# Expected Behavior\r\n<!-- Describe in detail what you expect to happen -->\r\nAccording to the [document](http://docs.celeryproject.org/en/latest/userguide/routing.html#manual-routing):\r\n\r\n> If you don't set the exchange or exchange type values for a key, these will be taken from the task_default_exchange and task_default_exchange_type settings\r\n\r\nThe worker should automatically create queues that binding to the exchange with `task_default_exchange` and `task_default_exchange_type`\r\n\r\n# Actual Behavior\r\n<!--\r\nDescribe in detail what actually happened.\r\nPlease include a backtrace and surround it with triple backticks (```).\r\nIn addition, include the Celery daemon logs, the broker logs,\r\nthe result backend logs and system logs below if they will help us debug\r\nthe issue.\r\n-->\r\nThe output of the command `celery worker -A test_celery -l info`:\r\n\r\n```\r\n -------------- celery@arch v4.3.0 (rhubarb)\r\n---- **** ----- \r\n--- * ***  * -- Linux-4.19.71-1-lts-x86_64-with-arch 2019-10-10 20:13:55\r\n-- * - **** --- \r\n- ** ---------- [config]\r\n- ** ---------- .> app:         scan_worker:0x7efdc5430a58\r\n- ** ---------- .> transport:   amqp://guest:**@localhost:5672//\r\n- ** ---------- .> results:     disabled://\r\n- *** --- * --- .> concurrency: 9 (prefork)\r\n-- ******* ---- .> task events: OFF (enable -E to monitor tasks in this worker)\r\n--- ***** ----- \r\n -------------- [queues]\r\n                .> default          exchange=(direct) key=tasks.#\r\n                .> test             exchange=(direct) key=test.#\r\n```\r\n\r\nthe queues are bound to the exchange that not match `task_default_exchange` and `task_default_exchange_type`\n",
        "hints_text": "",
        "created_at": "2019-10-10T12:49:36Z",
        "version": "4.4",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "t/unit/app/test_amqp.py"
        ]
    },
    {
        "repo": "celery/celery",
        "pull_number": 5664,
        "instance_id": "celery__celery-5664",
        "issue_numbers": [
            "5617",
            "5617"
        ],
        "base_commit": "8f3680c5189f2ef63753a692aaeea3892f067c56",
        "patch": "diff --git a/celery/app/amqp.py b/celery/app/amqp.py\nindex a8e40ed5012..35c9f224c8d 100644\n--- a/celery/app/amqp.py\n+++ b/celery/app/amqp.py\n@@ -253,6 +253,7 @@ def __init__(self, app):\n             1: self.as_task_v1,\n             2: self.as_task_v2,\n         }\n+        self.app._conf.bind_to(self._handle_conf_update)\n \n     @cached_property\n     def create_task_message(self):\n@@ -611,6 +612,10 @@ def routes(self):\n     def router(self):\n         return self.Router()\n \n+    @router.setter\n+    def router(self, value):\n+        return value\n+\n     @property\n     def producer_pool(self):\n         if self._producer_pool is None:\n@@ -634,3 +639,9 @@ def _event_dispatcher(self):\n         # We call Dispatcher.publish with a custom producer\n         # so don't need the diuspatcher to be enabled.\n         return self.app.events.Dispatcher(enabled=False)\n+\n+    def _handle_conf_update(self, *args, **kwargs):\n+        if ('task_routes' in kwargs or 'task_routes' in args):\n+            self.flush_routes()\n+            self.router = self.Router()\n+        return\ndiff --git a/celery/utils/collections.py b/celery/utils/collections.py\nindex 6131ccbabb3..3f47c9a829e 100644\n--- a/celery/utils/collections.py\n+++ b/celery/utils/collections.py\n@@ -245,6 +245,7 @@ class ChainMap(MutableMapping):\n     changes = None\n     defaults = None\n     maps = None\n+    _observers = []\n \n     def __init__(self, *maps, **kwargs):\n         # type: (*Mapping, **Any) -> None\n@@ -335,7 +336,10 @@ def setdefault(self, key, default=None):\n \n     def update(self, *args, **kwargs):\n         # type: (*Any, **Any) -> Any\n-        return self.changes.update(*args, **kwargs)\n+        result = self.changes.update(*args, **kwargs)\n+        for callback in self._observers:\n+            callback(*args, **kwargs)\n+        return result\n \n     def __repr__(self):\n         # type: () -> str\n@@ -376,6 +380,9 @@ def _iterate_values(self):\n         return (self[key] for key in self)\n     itervalues = _iterate_values\n \n+    def bind_to(self, callback):\n+        self._observers.append(callback)\n+\n     if sys.version_info[0] == 3:  # pragma: no cover\n         keys = _iterate_keys\n         items = _iterate_items\n",
        "test_patch": "diff --git a/t/unit/app/test_amqp.py b/t/unit/app/test_amqp.py\nindex 37acb8e33fc..30705ed7c1b 100644\n--- a/t/unit/app/test_amqp.py\n+++ b/t/unit/app/test_amqp.py\n@@ -333,6 +333,15 @@ def test_routes(self):\n         r2 = self.app.amqp.routes\n         assert r1 is r2\n \n+    def update_conf_runtime_for_tasks_queues(self):\n+        self.app.conf.update(task_routes={'task.create_pr': 'queue.qwerty'})\n+        self.app.send_task('task.create_pr')\n+        router_was = self.app.amqp.router\n+        self.app.conf.update(task_routes={'task.create_pr': 'queue.asdfgh'})\n+        self.app.send_task('task.create_pr')\n+        router = self.app.amqp.router\n+        assert router != router_was\n+\n \n class test_as_task_v2:\n \n",
        "problem_statement": "Updating task_routes config during runtime does not have effect\n<!--\r\nPlease fill this template entirely and do not erase parts of it.\r\nWe reserve the right to close without a response\r\nbug reports which are incomplete.\r\n-->\r\n# Checklist\r\n<!--\r\nTo check an item on the list replace [ ] with [x].\r\n-->\r\n\r\n- [x] I have read the relevant section in the\r\n  [contribution guide](http://docs.celeryproject.org/en/latest/contributing.html#other-bugs)\r\n  on reporting bugs.\r\n- [x] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Bug+Report%22+-label%3A%22Category%3A+Documentation%22)\r\n  for similar or identical bug reports.\r\n- [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22PR+Type%3A+Bugfix%22+-label%3A%22Category%3A+Documentation%22)\r\n  for existing proposed fixes.\r\n- [x] I have checked the [commit log](https://github.com/celery/celery/commits/master)\r\n  to find out if the bug was already fixed in the master branch.\r\n- [x] I have included all related issues and possible duplicate issues\r\n  in this issue (If there are none, check this box anyway).\r\n\r\n## Mandatory Debugging Information\r\n\r\n- [ ] I have included the output of ``celery -A proj report`` in the issue.\r\n    (if you are not able to do this, then at least specify the Celery\r\n     version affected).\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n- [x] I have included the contents of ``pip freeze`` in the issue.\r\n- [x] I have included all the versions of all the external dependencies required\r\n  to reproduce this bug.\r\n\r\n## Optional Debugging Information\r\n<!--\r\nTry some of the below if you think they are relevant.\r\nIt will help us figure out the scope of the bug and how many users it affects.\r\n-->\r\n- [ ] I have tried reproducing the issue on more than one Python version\r\n  and/or implementation.\r\n- [ ] I have tried reproducing the issue on more than one message broker and/or\r\n  result backend.\r\n- [ ] I have tried reproducing the issue on more than one version of the message\r\n  broker and/or result backend.\r\n- [ ] I have tried reproducing the issue on more than one operating system.\r\n- [x] I have tried reproducing the issue on more than one workers pool.\r\n- [x] I have tried reproducing the issue with autoscaling, retries,\r\n  ETA/Countdown & rate limits disabled.\r\n- [x] I have tried reproducing the issue after downgrading\r\n  and/or upgrading Celery and its dependencies.\r\n\r\n## Related Issues and Possible Duplicates\r\n<!--\r\nPlease make sure to search and mention any related issues\r\nor possible duplicates to this issue as requested by the checklist above.\r\n\r\nThis may or may not include issues in other repositories that the Celery project\r\nmaintains or other repositories that are dependencies of Celery.\r\n\r\nIf you don't know how to mention issues, please refer to Github's documentation\r\non the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests\r\n-->\r\n\r\n#### Related Issues\r\n\r\n- None\r\n\r\n#### Possible Duplicates\r\n\r\n- None\r\n\r\n## Environment & Settings\r\n<!-- Include the contents of celery --version below -->\r\n**Celery version**:\r\n<!-- Include the output of celery -A proj report below -->\r\n<details>\r\n<summary><b><code>celery report</code> Output:</b></summary>\r\n<p>\r\n\r\n```\r\n[root@shiny ~]# celery report\r\n\r\nsoftware -> celery:4.4.0rc2 (cliffs) kombu:4.6.3 py:3.6.8\r\n            billiard:3.6.0.0 py-amqp:2.5.0\r\nplatform -> system:Linux arch:64bit\r\n            kernel version:4.19.13-200.fc28.x86_64 imp:CPython\r\nloader   -> celery.loaders.default.Loader\r\nsettings -> transport:amqp results:disabled\r\n\r\n\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n# Steps to Reproduce\r\n\r\n## Required Dependencies\r\n<!-- Please fill the required dependencies to reproduce this issue -->\r\n* **Minimal Python Version**: N/A or Unknown\r\n* **Minimal Celery Version**: N/A or Unknown\r\n* **Minimal Kombu Version**: N/A or Unknown\r\n* **Minimal Broker Version**: N/A or Unknown\r\n* **Minimal Result Backend Version**: N/A or Unknown\r\n* **Minimal OS and/or Kernel Version**: N/A or Unknown\r\n* **Minimal Broker Client Version**: N/A or Unknown\r\n* **Minimal Result Backend Client Version**: N/A or Unknown\r\n\r\n### Python Packages\r\n<!-- Please fill the contents of pip freeze below -->\r\n<details>\r\n<summary><b><code>pip freeze</code> Output:</b></summary>\r\n<p>\r\n\r\n```\r\n[root@shiny ~]# pip3 freeze\r\namqp==2.5.0\r\nanymarkup==0.7.0\r\nanymarkup-core==0.7.1\r\nbilliard==3.6.0.0\r\ncelery==4.4.0rc2\r\nconfigobj==5.0.6\r\ngpg==1.10.0\r\niniparse==0.4\r\njson5==0.8.4\r\nkombu==4.6.3\r\npygobject==3.28.3\r\npython-qpid-proton==0.28.0\r\npytz==2019.1\r\nPyYAML==5.1.1\r\npyzmq==18.0.1\r\nredis==3.2.1\r\nrpm==4.14.2\r\nsix==1.11.0\r\nsmartcols==0.3.0\r\ntoml==0.10.0\r\nucho==0.1.0\r\nvine==1.3.0\r\nxmltodict==0.12.0\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n### Other Dependencies\r\n<!--\r\nPlease provide system dependencies, configuration files\r\nand other dependency information if applicable\r\n-->\r\n<details>\r\n<p>\r\nN/A\r\n</p>\r\n</details>\r\n\r\n## Minimally Reproducible Test Case\r\n<!--\r\nPlease provide a reproducible test case.\r\nRefer to the Reporting Bugs section in our contribution guide.\r\n\r\nWe prefer submitting test cases in the form of a PR to our integration test suite.\r\nIf you can provide one, please mention the PR number below.\r\nIf not, please attach the most minimal code example required to reproduce the issue below.\r\nIf the test case is too large, please include a link to a gist or a repository below.\r\n-->\r\n\r\n<details>\r\n<p>\r\n</p>\r\n</details>\r\n\r\n# Expected Behavior\r\n<!-- Describe in detail what you expect to happen -->\r\nUpdating task_routes during runtime is possible and has effect \r\n\r\n# Actual Behavior\r\n<!--\r\nDescribe in detail what actually happened.\r\nPlease include a backtrace and surround it with triple backticks (```).\r\nIn addition, include the Celery daemon logs, the broker logs,\r\nthe result backend logs and system logs below if they will help us debug\r\nthe issue.\r\n-->\r\nUpdating `task_routes` during runtime does not have effect - the config is updated but the `router` in `send_task` seems to be reusing old configuration.\r\n\r\n```python\r\nimport celery\r\n\r\nc = celery.Celery(broker='redis://localhost:6379/0',\r\n                  backend='redis://localhost:6379/0')\r\n\r\nc.conf.update(task_routes={'task.create_pr': 'queue.betka'})\r\nc.send_task('task.create_pr')\r\nprint(c.conf.get('task_routes'))\r\n\r\nc.conf.update(task_routes={'task.create_pr': 'queue.ferdinand'})\r\nc.send_task('task.create_pr')\r\nprint(c.conf.get('task_routes'))\r\n```\r\nOutput:\r\n```\r\n[root@shiny ~]# python3 repr.py \r\n{'task.create_pr': 'queue.betka'}\r\n{'task.create_pr': 'queue.ferdinand'}\r\n```\r\nSo the configuration is updated but it seems the routes are still pointing to queue.betka, since both tasks are sent to queue.betka and queue.ferdinand didn't receive anything.\r\n```\r\nbetka_1      | [2019-06-24 14:50:41,386: INFO/MainProcess] Received task: task.create_pr[54b28121-28cf-4301-b6f2-185d2e7c50cb]  \r\nbetka_1      | [2019-06-24 14:50:41,386: DEBUG/MainProcess] TaskPool: Apply <function _fast_trace_task at 0x7fca7f4d6a60> (args:('task.create_pr', '54b28121-28cf-4301-b6f2-185d2e7c50cb', {'lang': 'py', 'task': 'task.create_pr', 'id': '54b28121-28cf-4301-b6f2-185d2e7c50cb', 'shadow': None, 'eta': None, 'expires': None, 'group': None, 'retries': 0, 'timelimit': [None, None], 'root_id': '54b28121-28cf-4301-b6f2-185d2e7c50cb', 'parent_id': None, 'argsrepr': '()', 'kwargsrepr': '{}', 'origin': 'gen68@shiny', 'reply_to': 'b7be085a-b1f8-3738-b65f-963a805f2513', 'correlation_id': '54b28121-28cf-4301-b6f2-185d2e7c50cb', 'delivery_info': {'exchange': '', 'routing_key': 'queue.betka', 'priority': 0, 'redelivered': None}}, b'[[], {}, {\"callbacks\": null, \"errbacks\": null, \"chain\": null, \"chord\": null}]', 'application/json', 'utf-8') kwargs:{})\r\nbetka_1      | [2019-06-24 14:50:41,387: INFO/MainProcess] Received task: task.create_pr[3bf8b0fb-cb4a-412b-84d4-1a52b794b4e0]  \r\nbetka_1      | [2019-06-24 14:50:41,388: DEBUG/MainProcess] Task accepted: task.create_pr[54b28121-28cf-4301-b6f2-185d2e7c50cb] pid:12\r\nbetka_1      | [2019-06-24 14:50:41,390: INFO/ForkPoolWorker-1] Task task.create_pr[54b28121-28cf-4301-b6f2-185d2e7c50cb] succeeded in 0.002012896991800517s: 'Maybe later :)'\r\nbetka_1      | [2019-06-24 14:50:41,390: DEBUG/MainProcess] TaskPool: Apply <function _fast_trace_task at 0x7fca7f4d6a60> (args:('task.create_pr', '3bf8b0fb-cb4a-412b-84d4-1a52b794b4e0', {'lang': 'py', 'task': 'task.create_pr', 'id': '3bf8b0fb-cb4a-412b-84d4-1a52b794b4e0', 'shadow': None, 'eta': None, 'expires': None, 'group': None, 'retries': 0, 'timelimit': [None, None], 'root_id': '3bf8b0fb-cb4a-412b-84d4-1a52b794b4e0', 'parent_id': None, 'argsrepr': '()', 'kwargsrepr': '{}', 'origin': 'gen68@shiny', 'reply_to': 'b7be085a-b1f8-3738-b65f-963a805f2513', 'correlation_id': '3bf8b0fb-cb4a-412b-84d4-1a52b794b4e0', 'delivery_info': {'exchange': '', 'routing_key': 'queue.betka', 'priority': 0, 'redelivered': None}}, b'[[], {}, {\"callbacks\": null, \"errbacks\": null, \"chain\": null, \"chord\": null}]', 'application/json', 'utf-8') kwargs:{})\r\nbetka_1      | [2019-06-24 14:50:41,391: DEBUG/MainProcess] Task accepted: task.create_pr[3bf8b0fb-cb4a-412b-84d4-1a52b794b4e0] pid:12\r\nbetka_1      | [2019-06-24 14:50:41,391: INFO/ForkPoolWorker-1] Task task.create_pr[3bf8b0fb-cb4a-412b-84d4-1a52b794b4e0] succeeded in 0.0006862019945401698s: 'Maybe later :)'\r\n```\r\n\r\nNote: I managed to workaround it by adding `del c.amqp` right after update for now\r\n\r\n\nUpdating task_routes config during runtime does not have effect\n<!--\r\nPlease fill this template entirely and do not erase parts of it.\r\nWe reserve the right to close without a response\r\nbug reports which are incomplete.\r\n-->\r\n# Checklist\r\n<!--\r\nTo check an item on the list replace [ ] with [x].\r\n-->\r\n\r\n- [x] I have read the relevant section in the\r\n  [contribution guide](http://docs.celeryproject.org/en/latest/contributing.html#other-bugs)\r\n  on reporting bugs.\r\n- [x] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Bug+Report%22+-label%3A%22Category%3A+Documentation%22)\r\n  for similar or identical bug reports.\r\n- [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22PR+Type%3A+Bugfix%22+-label%3A%22Category%3A+Documentation%22)\r\n  for existing proposed fixes.\r\n- [x] I have checked the [commit log](https://github.com/celery/celery/commits/master)\r\n  to find out if the bug was already fixed in the master branch.\r\n- [x] I have included all related issues and possible duplicate issues\r\n  in this issue (If there are none, check this box anyway).\r\n\r\n## Mandatory Debugging Information\r\n\r\n- [ ] I have included the output of ``celery -A proj report`` in the issue.\r\n    (if you are not able to do this, then at least specify the Celery\r\n     version affected).\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n- [x] I have included the contents of ``pip freeze`` in the issue.\r\n- [x] I have included all the versions of all the external dependencies required\r\n  to reproduce this bug.\r\n\r\n## Optional Debugging Information\r\n<!--\r\nTry some of the below if you think they are relevant.\r\nIt will help us figure out the scope of the bug and how many users it affects.\r\n-->\r\n- [ ] I have tried reproducing the issue on more than one Python version\r\n  and/or implementation.\r\n- [ ] I have tried reproducing the issue on more than one message broker and/or\r\n  result backend.\r\n- [ ] I have tried reproducing the issue on more than one version of the message\r\n  broker and/or result backend.\r\n- [ ] I have tried reproducing the issue on more than one operating system.\r\n- [x] I have tried reproducing the issue on more than one workers pool.\r\n- [x] I have tried reproducing the issue with autoscaling, retries,\r\n  ETA/Countdown & rate limits disabled.\r\n- [x] I have tried reproducing the issue after downgrading\r\n  and/or upgrading Celery and its dependencies.\r\n\r\n## Related Issues and Possible Duplicates\r\n<!--\r\nPlease make sure to search and mention any related issues\r\nor possible duplicates to this issue as requested by the checklist above.\r\n\r\nThis may or may not include issues in other repositories that the Celery project\r\nmaintains or other repositories that are dependencies of Celery.\r\n\r\nIf you don't know how to mention issues, please refer to Github's documentation\r\non the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests\r\n-->\r\n\r\n#### Related Issues\r\n\r\n- None\r\n\r\n#### Possible Duplicates\r\n\r\n- None\r\n\r\n## Environment & Settings\r\n<!-- Include the contents of celery --version below -->\r\n**Celery version**:\r\n<!-- Include the output of celery -A proj report below -->\r\n<details>\r\n<summary><b><code>celery report</code> Output:</b></summary>\r\n<p>\r\n\r\n```\r\n[root@shiny ~]# celery report\r\n\r\nsoftware -> celery:4.4.0rc2 (cliffs) kombu:4.6.3 py:3.6.8\r\n            billiard:3.6.0.0 py-amqp:2.5.0\r\nplatform -> system:Linux arch:64bit\r\n            kernel version:4.19.13-200.fc28.x86_64 imp:CPython\r\nloader   -> celery.loaders.default.Loader\r\nsettings -> transport:amqp results:disabled\r\n\r\n\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n# Steps to Reproduce\r\n\r\n## Required Dependencies\r\n<!-- Please fill the required dependencies to reproduce this issue -->\r\n* **Minimal Python Version**: N/A or Unknown\r\n* **Minimal Celery Version**: N/A or Unknown\r\n* **Minimal Kombu Version**: N/A or Unknown\r\n* **Minimal Broker Version**: N/A or Unknown\r\n* **Minimal Result Backend Version**: N/A or Unknown\r\n* **Minimal OS and/or Kernel Version**: N/A or Unknown\r\n* **Minimal Broker Client Version**: N/A or Unknown\r\n* **Minimal Result Backend Client Version**: N/A or Unknown\r\n\r\n### Python Packages\r\n<!-- Please fill the contents of pip freeze below -->\r\n<details>\r\n<summary><b><code>pip freeze</code> Output:</b></summary>\r\n<p>\r\n\r\n```\r\n[root@shiny ~]# pip3 freeze\r\namqp==2.5.0\r\nanymarkup==0.7.0\r\nanymarkup-core==0.7.1\r\nbilliard==3.6.0.0\r\ncelery==4.4.0rc2\r\nconfigobj==5.0.6\r\ngpg==1.10.0\r\niniparse==0.4\r\njson5==0.8.4\r\nkombu==4.6.3\r\npygobject==3.28.3\r\npython-qpid-proton==0.28.0\r\npytz==2019.1\r\nPyYAML==5.1.1\r\npyzmq==18.0.1\r\nredis==3.2.1\r\nrpm==4.14.2\r\nsix==1.11.0\r\nsmartcols==0.3.0\r\ntoml==0.10.0\r\nucho==0.1.0\r\nvine==1.3.0\r\nxmltodict==0.12.0\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n### Other Dependencies\r\n<!--\r\nPlease provide system dependencies, configuration files\r\nand other dependency information if applicable\r\n-->\r\n<details>\r\n<p>\r\nN/A\r\n</p>\r\n</details>\r\n\r\n## Minimally Reproducible Test Case\r\n<!--\r\nPlease provide a reproducible test case.\r\nRefer to the Reporting Bugs section in our contribution guide.\r\n\r\nWe prefer submitting test cases in the form of a PR to our integration test suite.\r\nIf you can provide one, please mention the PR number below.\r\nIf not, please attach the most minimal code example required to reproduce the issue below.\r\nIf the test case is too large, please include a link to a gist or a repository below.\r\n-->\r\n\r\n<details>\r\n<p>\r\n</p>\r\n</details>\r\n\r\n# Expected Behavior\r\n<!-- Describe in detail what you expect to happen -->\r\nUpdating task_routes during runtime is possible and has effect \r\n\r\n# Actual Behavior\r\n<!--\r\nDescribe in detail what actually happened.\r\nPlease include a backtrace and surround it with triple backticks (```).\r\nIn addition, include the Celery daemon logs, the broker logs,\r\nthe result backend logs and system logs below if they will help us debug\r\nthe issue.\r\n-->\r\nUpdating `task_routes` during runtime does not have effect - the config is updated but the `router` in `send_task` seems to be reusing old configuration.\r\n\r\n```python\r\nimport celery\r\n\r\nc = celery.Celery(broker='redis://localhost:6379/0',\r\n                  backend='redis://localhost:6379/0')\r\n\r\nc.conf.update(task_routes={'task.create_pr': 'queue.betka'})\r\nc.send_task('task.create_pr')\r\nprint(c.conf.get('task_routes'))\r\n\r\nc.conf.update(task_routes={'task.create_pr': 'queue.ferdinand'})\r\nc.send_task('task.create_pr')\r\nprint(c.conf.get('task_routes'))\r\n```\r\nOutput:\r\n```\r\n[root@shiny ~]# python3 repr.py \r\n{'task.create_pr': 'queue.betka'}\r\n{'task.create_pr': 'queue.ferdinand'}\r\n```\r\nSo the configuration is updated but it seems the routes are still pointing to queue.betka, since both tasks are sent to queue.betka and queue.ferdinand didn't receive anything.\r\n```\r\nbetka_1      | [2019-06-24 14:50:41,386: INFO/MainProcess] Received task: task.create_pr[54b28121-28cf-4301-b6f2-185d2e7c50cb]  \r\nbetka_1      | [2019-06-24 14:50:41,386: DEBUG/MainProcess] TaskPool: Apply <function _fast_trace_task at 0x7fca7f4d6a60> (args:('task.create_pr', '54b28121-28cf-4301-b6f2-185d2e7c50cb', {'lang': 'py', 'task': 'task.create_pr', 'id': '54b28121-28cf-4301-b6f2-185d2e7c50cb', 'shadow': None, 'eta': None, 'expires': None, 'group': None, 'retries': 0, 'timelimit': [None, None], 'root_id': '54b28121-28cf-4301-b6f2-185d2e7c50cb', 'parent_id': None, 'argsrepr': '()', 'kwargsrepr': '{}', 'origin': 'gen68@shiny', 'reply_to': 'b7be085a-b1f8-3738-b65f-963a805f2513', 'correlation_id': '54b28121-28cf-4301-b6f2-185d2e7c50cb', 'delivery_info': {'exchange': '', 'routing_key': 'queue.betka', 'priority': 0, 'redelivered': None}}, b'[[], {}, {\"callbacks\": null, \"errbacks\": null, \"chain\": null, \"chord\": null}]', 'application/json', 'utf-8') kwargs:{})\r\nbetka_1      | [2019-06-24 14:50:41,387: INFO/MainProcess] Received task: task.create_pr[3bf8b0fb-cb4a-412b-84d4-1a52b794b4e0]  \r\nbetka_1      | [2019-06-24 14:50:41,388: DEBUG/MainProcess] Task accepted: task.create_pr[54b28121-28cf-4301-b6f2-185d2e7c50cb] pid:12\r\nbetka_1      | [2019-06-24 14:50:41,390: INFO/ForkPoolWorker-1] Task task.create_pr[54b28121-28cf-4301-b6f2-185d2e7c50cb] succeeded in 0.002012896991800517s: 'Maybe later :)'\r\nbetka_1      | [2019-06-24 14:50:41,390: DEBUG/MainProcess] TaskPool: Apply <function _fast_trace_task at 0x7fca7f4d6a60> (args:('task.create_pr', '3bf8b0fb-cb4a-412b-84d4-1a52b794b4e0', {'lang': 'py', 'task': 'task.create_pr', 'id': '3bf8b0fb-cb4a-412b-84d4-1a52b794b4e0', 'shadow': None, 'eta': None, 'expires': None, 'group': None, 'retries': 0, 'timelimit': [None, None], 'root_id': '3bf8b0fb-cb4a-412b-84d4-1a52b794b4e0', 'parent_id': None, 'argsrepr': '()', 'kwargsrepr': '{}', 'origin': 'gen68@shiny', 'reply_to': 'b7be085a-b1f8-3738-b65f-963a805f2513', 'correlation_id': '3bf8b0fb-cb4a-412b-84d4-1a52b794b4e0', 'delivery_info': {'exchange': '', 'routing_key': 'queue.betka', 'priority': 0, 'redelivered': None}}, b'[[], {}, {\"callbacks\": null, \"errbacks\": null, \"chain\": null, \"chord\": null}]', 'application/json', 'utf-8') kwargs:{})\r\nbetka_1      | [2019-06-24 14:50:41,391: DEBUG/MainProcess] Task accepted: task.create_pr[3bf8b0fb-cb4a-412b-84d4-1a52b794b4e0] pid:12\r\nbetka_1      | [2019-06-24 14:50:41,391: INFO/ForkPoolWorker-1] Task task.create_pr[3bf8b0fb-cb4a-412b-84d4-1a52b794b4e0] succeeded in 0.0006862019945401698s: 'Maybe later :)'\r\n```\r\n\r\nNote: I managed to workaround it by adding `del c.amqp` right after update for now\r\n\r\n\n",
        "hints_text": "Could someone please take a look at this?\npost support Questions to mailing list\nThis is not about support, this is a bug.\nverify that on the mailing list first\nfrom your code of conduct:\r\n\r\n> Bugs can always be described to the Mailing list, but the best way to report an issue and to ensure a timely response is to use the issue tracker.\r\n\r\nAlso, I am pretty sure this is a bug in code in this repo: \r\n\r\nI think I can see specifically where it is:\r\nWhen the task is sent, router from self.amqp is used - router is gotten from cached property - self.amqp which is not updated, when the configutration changes:\r\n\r\nhttps://github.com/celery/celery/blob/5b8fe5f2f3314f2b5d03097533711e6b47b570d4/celery/app/base.py#L717\r\nhttps://github.com/celery/celery/blob/5b8fe5f2f3314f2b5d03097533711e6b47b570d4/celery/app/base.py#L714\r\nhttps://github.com/celery/celery/blob/5b8fe5f2f3314f2b5d03097533711e6b47b570d4/celery/app/base.py#L1202\r\n\r\nSo I suggest deleting amqp value (as I did in my workaround fix) when `task_routes` conf value is updated\nAnd when you don't want to allow changes of task_routes during runtime, the bug is, that the configuration shows something that is not true after an update.\nI would request to come with a fix and test\nCould someone please take a look at this?\npost support Questions to mailing list\nThis is not about support, this is a bug.\nverify that on the mailing list first\nfrom your code of conduct:\r\n\r\n> Bugs can always be described to the Mailing list, but the best way to report an issue and to ensure a timely response is to use the issue tracker.\r\n\r\nAlso, I am pretty sure this is a bug in code in this repo: \r\n\r\nI think I can see specifically where it is:\r\nWhen the task is sent, router from self.amqp is used - router is gotten from cached property - self.amqp which is not updated, when the configutration changes:\r\n\r\nhttps://github.com/celery/celery/blob/5b8fe5f2f3314f2b5d03097533711e6b47b570d4/celery/app/base.py#L717\r\nhttps://github.com/celery/celery/blob/5b8fe5f2f3314f2b5d03097533711e6b47b570d4/celery/app/base.py#L714\r\nhttps://github.com/celery/celery/blob/5b8fe5f2f3314f2b5d03097533711e6b47b570d4/celery/app/base.py#L1202\r\n\r\nSo I suggest deleting amqp value (as I did in my workaround fix) when `task_routes` conf value is updated\nAnd when you don't want to allow changes of task_routes during runtime, the bug is, that the configuration shows something that is not true after an update.\nI would request to come with a fix and test",
        "created_at": "2019-08-01T11:05:11Z",
        "version": "4.4",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "t/unit/app/test_amqp.py"
        ]
    },
    {
        "repo": "celery/celery",
        "pull_number": 5631,
        "instance_id": "celery__celery-5631",
        "issue_numbers": [
            "5627"
        ],
        "base_commit": "60ac03b4956307daf3717bfdbccceab693bd9a6e",
        "patch": "diff --git a/celery/beat.py b/celery/beat.py\nindex 6511120a9e9..f589d89d084 100644\n--- a/celery/beat.py\n+++ b/celery/beat.py\n@@ -384,7 +384,7 @@ def apply_async(self, entry, producer=None, advance=True, **kwargs):\n         task = self.app.tasks.get(entry.task)\n \n         try:\n-            entry_args = [v() if isinstance(v, BeatLazyFunc) else v for v in entry.args]\n+            entry_args = [v() if isinstance(v, BeatLazyFunc) else v for v in (entry.args or [])]\n             entry_kwargs = {k: v() if isinstance(v, BeatLazyFunc) else v for k, v in entry.kwargs.items()}\n             if task:\n                 return task.apply_async(entry_args, entry_kwargs,\n",
        "test_patch": "diff --git a/t/unit/app/test_beat.py b/t/unit/app/test_beat.py\nindex 3e813d578c3..74950d3cebf 100644\n--- a/t/unit/app/test_beat.py\n+++ b/t/unit/app/test_beat.py\n@@ -188,6 +188,17 @@ def foo():\n         scheduler.apply_async(scheduler.Entry(task=foo.name, app=self.app))\n         foo.apply_async.assert_called()\n \n+    def test_apply_async_with_null_args(self):\n+\n+        @self.app.task(shared=False)\n+        def foo():\n+            pass\n+        foo.apply_async = Mock(name='foo.apply_async')\n+\n+        scheduler = mScheduler(app=self.app)\n+        scheduler.apply_async(scheduler.Entry(task=foo.name, app=self.app, args=None, kwargs=None))\n+        foo.apply_async.assert_called()\n+\n     def test_should_sync(self):\n \n         @self.app.task(shared=False)\n",
        "problem_statement": "Null args in backend_cleanup task from beat.\n# Checklist\r\n- [x] I have read the relevant section in the\r\n  [contribution guide](http://docs.celeryproject.org/en/latest/contributing.html#other-bugs)\r\n  on reporting bugs.\r\n- [x] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Bug+Report%22+-label%3A%22Category%3A+Documentation%22)\r\n  for similar or identical bug reports.\r\n- [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22PR+Type%3A+Bugfix%22+-label%3A%22Category%3A+Documentation%22)\r\n  for existing proposed fixes.\r\n- [ ] I have checked the [commit log](https://github.com/celery/celery/commits/master)\r\n  to find out if the bug was already fixed in the master branch.\r\n- [x] I have included all related issues and possible duplicate issues\r\n  in this issue (If there are none, check this box anyway).\r\n\r\n## Mandatory Debugging Information\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n    (if you are not able to do this, then at least specify the Celery\r\n     version affected).\r\n- [ ] I have verified that the issue exists against the `master` branch of Celery.\r\n- [x] I have included the contents of ``pip freeze`` in the issue.\r\n- [x I have included all the versions of all the external dependencies required\r\n  to reproduce this bug.\r\n\r\n## Optional Debugging Information\r\n- [ ] I have tried reproducing the issue on more than one Python version\r\n  and/or implementation.\r\n- [ ] I have tried reproducing the issue on more than one message broker and/or\r\n  result backend.\r\n- [ ] I have tried reproducing the issue on more than one version of the message\r\n  broker and/or result backend.\r\n- [ ] I have tried reproducing the issue on more than one operating system.\r\n- [ ] I have tried reproducing the issue on more than one workers pool.\r\n- [ ] I have tried reproducing the issue with autoscaling, retries,\r\n  ETA/Countdown & rate limits disabled.\r\n- [ ] I have tried reproducing the issue after downgrading\r\n  and/or upgrading Celery and its dependencies.\r\n\r\n## Environment & Settings\r\n<!-- Include the contents of celery --version below -->\r\n**Celery version**: 4.4.0rc2 (cliffs)\r\n<!-- Include the output of celery -A proj report below -->\r\n\r\n<details>\r\n<summary><b><code>celery report</code> Output:</b></summary>\r\n<p>\r\n\r\nsoftware -> celery:4.4.0rc2 (cliffs) kombu:4.6.3 py:3.7.3\r\n            billiard:3.6.0.0 py-amqp:2.5.0\r\nplatform -> system:Linux arch:64bit\r\n            kernel version:4.18.0-21-generic imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:pyamqp results:redis://redis:6379/0\r\n\r\nbroker_url: 'amqp://guest:********@rabbitmq:5672//'\r\nresult_backend: 'redis://redis:6379/0'\r\ntimezone: 'UTC'\r\nresult_extended: True\r\ntask_acks_late: True\r\ntask_routes: {\r\n    'celery.*': {'queue': 'api_workers'},\r\n    'graphape.tasks.api.*': {'queue': 'api_workers'},\r\n    'graphape.tasks.process.*': {'queue': 'process_workers'}}\r\nJOBTASTIC_CACHE: <jobtastic.cache.base.WrappedCache object at 0x7f152c5ddb70>\r\nredbeat_redis_url: 'redis://redis:6379/0'\r\nbeat_schedule: {\r\n }\r\n\r\n</p>\r\n</details>\r\n\r\n# Steps to Reproduce\r\n\r\nDon't think anything other than celery 4.4.0RC and just having beat running and publishing a \"celery.backend_cleanup\"-task.\r\nTask in backend (redis) will have args set to null and this code will later try to enumerate from the null value:\r\nhttps://github.com/celery/celery/blob/master/celery/beat.py#L387\r\n\r\nI guess these lines should be patched to deal with args = null?\r\nOr perhaps args is never allowed to be null and fix should instead be already when task is written to backend?\r\nI'm not sure what celery specification says about null args, please advise.\r\n\r\n### Python Packages\r\n<!-- Please fill the contents of pip freeze below -->\r\n<details>\r\n<summary><b><code>pip freeze</code> Output:</b></summary>\r\n<p>\r\nalabaster==0.7.12\r\namqp==2.5.0\r\nargcomplete==1.9.3\r\nargh==0.26.2\r\nasteval==0.9.14\r\nBabel==2.7.0\r\nbilliard==3.6.0.0\r\nbleach==3.1.0\r\nboto3==1.9.179\r\nbotocore==1.12.179\r\nbumpversion==0.5.3\r\ncachelib==0.1\r\ncelery==4.4.0rc2\r\ncelery-redbeat==0.13.0\r\nCerberus==1.3.1\r\ncertifi==2019.6.16\r\ncfn-flip==1.2.1\r\nchardet==3.0.4\r\nClick==7.0\r\ncoverage==4.5.3\r\ndecorator==4.4.0\r\ndnspython==1.16.0\r\ndocutils==0.14\r\ndumb-init==1.2.2\r\ndurationpy==0.5\r\nEve==0.9.2\r\neventlet==0.25.0\r\nEvents==0.3\r\nFlask==1.0.3\r\nFlask-SocketIO==4.1.0\r\nfuture==0.16.0\r\ngraphape==0.0.1\r\ngreenlet==0.4.15\r\nhjson==3.0.1\r\nidna==2.8\r\nimagesize==1.1.0\r\nitsdangerous==1.1.0\r\nJinja2==2.10.1\r\njmespath==0.9.3\r\njobtastic==2.1.1\r\nkappa==0.6.0\r\nkombu==4.6.3\r\nlambda-packages==0.20.0\r\nlivereload==2.6.1\r\nMarkupSafe==1.1.1\r\nmock==3.0.5\r\nmonotonic==1.5\r\nnetworkx==2.3\r\nnumpy==1.16.4\r\npackaging==19.0\r\npathtools==0.1.2\r\npkginfo==1.5.0.1\r\nplacebo==0.9.0\r\npluginbase==1.0.0\r\nport-for==0.3.1\r\npsutil==5.6.3\r\nPygments==2.4.2\r\npymongo==3.8.0\r\npyparsing==2.4.0\r\npython-dateutil==2.6.1\r\npython-engineio==3.8.1\r\npython-slugify==1.2.4\r\npython-socketio==4.1.0\r\npytz==2019.1\r\nPyYAML==5.1.1\r\nreadme-renderer==24.0\r\nredis==3.2.1\r\nrequests==2.22.0\r\nrequests-toolbelt==0.9.1\r\ns3transfer==0.2.1\r\nsimplejson==3.16.0\r\nsix==1.12.0\r\nsnowballstemmer==1.2.1\r\nSphinx==2.0.1\r\nsphinx-autobuild==0.7.1\r\nsphinx-rtd-theme==0.4.3\r\nsphinxcontrib-applehelp==1.0.1\r\nsphinxcontrib-devhelp==1.0.1\r\nsphinxcontrib-htmlhelp==1.0.2\r\nsphinxcontrib-jsmath==1.0.1\r\nsphinxcontrib-qthelp==1.0.2\r\nsphinxcontrib-serializinghtml==1.1.3\r\ntenacity==5.0.4\r\ntoml==0.10.0\r\ntornado==6.0.2\r\ntqdm==4.32.1\r\ntroposphere==2.4.9\r\ntwine==1.13.0\r\nUnidecode==1.1.1\r\nurllib3==1.25.3\r\nuWSGI==2.0.18\r\nvine==1.3.0\r\nwatchdog==0.9.0\r\nwebencodings==0.5.1\r\nWerkzeug==0.15.4\r\nwsgi-request-logger==0.4.6\r\nzappa==0.48.2\r\n\r\n</p>\r\n</details>\r\n\r\n### Other Dependencies\r\n<!--\r\nPlease provide system dependencies, configuration files\r\nand other dependency information if applicable\r\n-->\r\n<details>\r\n<p>\r\n- celery-redbeat\r\n</p>\r\n</details>\r\n\r\n# Expected Behavior\r\nNo exception.\r\n\r\n# Actual Behavior\r\nException:\r\n\r\n```\r\n[2019-07-02 04:00:00,081: ERROR/MainProcess] Message Error: Couldn't apply scheduled task celery.backend_cleanup: 'NoneType' object is not iterable\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/site-packages/celery/beat.py\", line 387, in apply_async\r\n    entry_args = [v() if isinstance(v, BeatLazyFunc) else v for v in entry.args]\r\nTypeError: 'NoneType' object is not iterable\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/site-packages/redbeat/schedulers.py\", line 427, in maybe_due\r\n    result = self.apply_async(entry, **kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/celery/beat.py\", line 400, in apply_async\r\n    entry, exc=exc)), sys.exc_info()[2])\r\n  File \"/usr/local/lib/python3.7/site-packages/vine/five.py\", line 194, in reraise\r\n    raise value.with_traceback(tb)\r\n  File \"/usr/local/lib/python3.7/site-packages/celery/beat.py\", line 387, in apply_async\r\n    entry_args = [v() if isinstance(v, BeatLazyFunc) else v for v in entry.args]\r\ncelery.beat.SchedulingError: Couldn't apply scheduled task celery.backend_cleanup: 'NoneType' object is not iterable\r\n```\r\n\n",
        "hints_text": "could you try to patch and test?",
        "created_at": "2019-07-03T14:25:28Z",
        "version": "4.4",
        "PASS_TO_PASS": [],
        "FAIL_TO_PASS": [
            "t/unit/app/test_beat.py"
        ]
    }
]
