{"repo": "scrapy/scrapy", "pull_number": 6374, "instance_id": "scrapy__scrapy-6374", "issue_numbers": ["6361"], "base_commit": "631fc65fadb874629787ae5f7fdd876b9ec96a29", "patch": "diff --git a/.flake8 b/.flake8\nindex 62ccad9cf47..cf1a96476c2 100644\n--- a/.flake8\n+++ b/.flake8\n@@ -16,6 +16,7 @@ per-file-ignores =\n     scrapy/linkextractors/__init__.py:E402,F401\n     scrapy/selector/__init__.py:F401\n     scrapy/spiders/__init__.py:E402,F401\n+    tests/CrawlerRunner/change_reactor.py:E402\n \n     # Issues pending a review:\n     scrapy/utils/url.py:F403,F405\ndiff --git a/docs/topics/practices.rst b/docs/topics/practices.rst\nindex cd359b1473e..1500011e7b0 100644\n--- a/docs/topics/practices.rst\n+++ b/docs/topics/practices.rst\n@@ -92,7 +92,6 @@ reactor after ``MySpider`` has finished running.\n \n .. code-block:: python\n \n-    from twisted.internet import reactor\n     import scrapy\n     from scrapy.crawler import CrawlerRunner\n     from scrapy.utils.log import configure_logging\n@@ -107,6 +106,37 @@ reactor after ``MySpider`` has finished running.\n     runner = CrawlerRunner()\n \n     d = runner.crawl(MySpider)\n+\n+    from twisted.internet import reactor\n+\n+    d.addBoth(lambda _: reactor.stop())\n+    reactor.run()  # the script will block here until the crawling is finished\n+\n+Same example but using a non-default reactor, it's only necessary call\n+``install_reactor`` if you are using ``CrawlerRunner`` since ``CrawlerProcess`` already does this automatically.\n+\n+.. code-block:: python\n+\n+    import scrapy\n+    from scrapy.crawler import CrawlerRunner\n+    from scrapy.utils.log import configure_logging\n+\n+\n+    class MySpider(scrapy.Spider):\n+        # Your spider definition\n+        ...\n+\n+\n+    configure_logging({\"LOG_FORMAT\": \"%(levelname)s: %(message)s\"})\n+\n+    from scrapy.utils.reactor import install_reactor\n+\n+    install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n+    runner = CrawlerRunner()\n+    d = runner.crawl(MySpider)\n+\n+    from twisted.internet import reactor\n+\n     d.addBoth(lambda _: reactor.stop())\n     reactor.run()  # the script will block here until the crawling is finished\n \n@@ -151,7 +181,6 @@ Same example using :class:`~scrapy.crawler.CrawlerRunner`:\n .. code-block:: python\n \n     import scrapy\n-    from twisted.internet import reactor\n     from scrapy.crawler import CrawlerRunner\n     from scrapy.utils.log import configure_logging\n     from scrapy.utils.project import get_project_settings\n@@ -173,6 +202,9 @@ Same example using :class:`~scrapy.crawler.CrawlerRunner`:\n     runner.crawl(MySpider1)\n     runner.crawl(MySpider2)\n     d = runner.join()\n+\n+    from twisted.internet import reactor\n+\n     d.addBoth(lambda _: reactor.stop())\n \n     reactor.run()  # the script will block here until all crawling jobs are finished\n@@ -181,7 +213,7 @@ Same example but running the spiders sequentially by chaining the deferreds:\n \n .. code-block:: python\n \n-    from twisted.internet import reactor, defer\n+    from twisted.internet import defer\n     from scrapy.crawler import CrawlerRunner\n     from scrapy.utils.log import configure_logging\n     from scrapy.utils.project import get_project_settings\n@@ -209,6 +241,8 @@ Same example but running the spiders sequentially by chaining the deferreds:\n         reactor.stop()\n \n \n+    from twisted.internet import reactor\n+\n     crawl()\n     reactor.run()  # the script will block here until the last crawl call is finished\n \ndiff --git a/scrapy/crawler.py b/scrapy/crawler.py\nindex ccfe788913a..4fe5987a783 100644\n--- a/scrapy/crawler.py\n+++ b/scrapy/crawler.py\n@@ -129,6 +129,8 @@ def _apply_settings(self) -> None:\n             if is_asyncio_reactor_installed() and event_loop:\n                 verify_installed_asyncio_event_loop(event_loop)\n \n+            log_reactor_info()\n+\n         self.extensions = ExtensionManager.from_crawler(self)\n         self.settings.freeze()\n \n", "test_patch": "diff --git a/tests/CrawlerRunner/change_reactor.py b/tests/CrawlerRunner/change_reactor.py\nnew file mode 100644\nindex 00000000000..b20aa0c7cbf\n--- /dev/null\n+++ b/tests/CrawlerRunner/change_reactor.py\n@@ -0,0 +1,31 @@\n+from scrapy import Spider\n+from scrapy.crawler import CrawlerRunner\n+from scrapy.utils.log import configure_logging\n+\n+\n+class NoRequestsSpider(Spider):\n+    name = \"no_request\"\n+\n+    custom_settings = {\n+        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+    }\n+\n+    def start_requests(self):\n+        return []\n+\n+\n+configure_logging({\"LOG_FORMAT\": \"%(levelname)s: %(message)s\", \"LOG_LEVEL\": \"DEBUG\"})\n+\n+\n+from scrapy.utils.reactor import install_reactor\n+\n+install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n+\n+runner = CrawlerRunner()\n+\n+d = runner.crawl(NoRequestsSpider)\n+\n+from twisted.internet import reactor\n+\n+d.addBoth(callback=lambda _: reactor.stop())\n+reactor.run()\ndiff --git a/tests/test_crawler.py b/tests/test_crawler.py\nindex 989208694cb..791ea1faa66 100644\n--- a/tests/test_crawler.py\n+++ b/tests/test_crawler.py\n@@ -926,3 +926,11 @@ def test_response_ip_address(self):\n         self.assertIn(\"INFO: Host: not.a.real.domain\", log)\n         self.assertIn(\"INFO: Type: <class 'ipaddress.IPv4Address'>\", log)\n         self.assertIn(\"INFO: IP address: 127.0.0.1\", log)\n+\n+    def test_change_default_reactor(self):\n+        log = self.run_script(\"change_reactor.py\")\n+        self.assertIn(\n+            \"DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+            log,\n+        )\n+        self.assertIn(\"DEBUG: Using asyncio event loop\", log)\n", "problem_statement": "Remove top-level reactor imports from CrawlerProces/CrawlerRunner examples \nThere are several code examples on https://docs.scrapy.org/en/latest/topics/practices.html that have a top-level `from twisted.internet import reactor`, which is problematic (breaks when the settings specify a non-default reactor) and needs to be fixed.\n", "hints_text": "For this we should check if we have `TWISTED_REACTOR` setting defined (`get_project_settings`) and if is we call `install_reactor` before importing `reactor`? \nI think it's enough to move the imports inside blocks so that they only run after the setting is applied (i.e. after `Crawler.crawl()`, so after `CrawlerRunner.crawl()`). The changed examples should be tested with a non-default reactor setting value in any case.\r\n\r\nIf/when that's not possible to do it makes sense to add `install_reactor()` to examples I think.\n@wRAR I was testing this and noticed that if we have `TWISTED_REACTOR` in `custom_settings` but we don't call `install_reactor` we always get an exception when Scrapy runs `_apply_settings` method (using `CrawlerRunner`):\r\n\r\n```\r\nException: The installed reactor (twisted.internet.selectreactor.SelectReactor) does not match the requested one (twisted.internet.asyncioreactor.AsyncioSelectorReactor)\r\n```\r\n\r\nBecause `init_reactor`  is `False`:\r\n\r\nhttps://github.com/scrapy/scrapy/blob/631fc65fadb874629787ae5f7fdd876b9ec96a29/scrapy/crawler.py#L119\r\n\r\nI see `init_reactor` parameter https://github.com/scrapy/scrapy/blob/631fc65fadb874629787ae5f7fdd876b9ec96a29/scrapy/crawler.py#L79 but when we use `runner.crawl` from `CrawlerRunner` theres no way to override this parameter, when is created:\r\n\r\nhttps://github.com/scrapy/scrapy/blob/631fc65fadb874629787ae5f7fdd876b9ec96a29/scrapy/crawler.py#L330-L334\r\n\r\nP.S: If I switch to `CrawlerProcess` works (even without calling `install_reactor`), just to confirm if this is expected.\r\n\r\nHere my snippet:\r\n\r\n```python\r\nfrom scrapy import Spider\r\nfrom scrapy.http import Request\r\nfrom scrapy.crawler import CrawlerRunner\r\nfrom scrapy.utils.log import configure_logging\r\nfrom scrapy.utils.project import get_project_settings\r\n\r\n\r\nclass MySpider1(Spider):\r\n    name = \"my_spider\"\r\n\r\n    custom_settings = {\r\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\r\n    }\r\n\r\n    def start_requests(self):\r\n        yield Request(url=\"https://httpbin.org/anything\")\r\n\r\n    def parse(self, response):\r\n        yield response.json()\r\n\r\n\r\nclass MySpider2(Spider):\r\n    name = \"my_spider2\"\r\n\r\n    custom_settings = {\r\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\r\n    }\r\n\r\n    def start_requests(self):\r\n        yield Request(url=\"https://httpbin.org/anything\")\r\n\r\n    def parse(self, response):\r\n        yield response.json()\r\n\r\n\r\nconfigure_logging()\r\nsettings = get_project_settings()\r\nrunner = CrawlerRunner(settings)\r\n# from scrapy.utils.reactor import install_reactor\r\n# install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\r\nrunner.crawl(MySpider1)\r\nrunner.crawl(MySpider2)\r\nfrom twisted.internet import reactor\r\nd = runner.join()\r\nd.addBoth(lambda _: reactor.stop())\r\nreactor.run()\r\n```\n`CrawlerRunner` indeed requires you to install (and start) the reactor in your code, so it makes sense that `CrawlerRunner` examples show installing a non-default reactor manually.\n> `CrawlerRunner` indeed requires you to install (and start) the reactor in your code, so it makes sense that `CrawlerRunner` examples show installing a non-default reactor manually.\r\n\r\nGot it, thanks!", "created_at": "2024-05-22T10:51:11Z"}
