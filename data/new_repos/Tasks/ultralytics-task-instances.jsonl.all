{"repo": "ultralytics/ultralytics", "pull_number": 19169, "instance_id": "ultralytics__ultralytics-19169", "issue_numbers": ["19168"], "base_commit": "9d8c2fe3c7a2701a9cdc769b4b8786738a5b99d0", "patch": "diff --git a/ultralytics/utils/metrics.py b/ultralytics/utils/metrics.py\nindex 2b5821cbbb1..7d9629ec8dc 100644\n--- a/ultralytics/utils/metrics.py\n+++ b/ultralytics/utils/metrics.py\n@@ -440,7 +440,7 @@ def plot(self, normalize=True, save_dir=\"\", names=(), on_plot=None):\n \n     def print(self):\n         \"\"\"Print the confusion matrix to the console.\"\"\"\n-        for i in range(self.nc + 1):\n+        for i in range(self.matrix.shape[0]):\n             LOGGER.info(\" \".join(map(str, self.matrix[i])))\n \n \n", "test_patch": "", "problem_statement": "print method for ultralytics.utils.metrics.ConfusionMatrix object bug\n### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\nOther\n\n### Bug\n\nThe print method for ultralytics.utils.metrics.ConfusionMatrix object does not work. It throws an index error because it is out of bounds. \nTo reproduce you can just try to print any ConfusionMatrix obtained from the result metrics of val (ie: metrics = model.val(...) ).\n\nOutput:\nTraceback (most recent call last):\n  File \"/Users/rebeccajaubert/cursor_projects/crop-mapping-data-preprocessing/cmap_preprocess/yolo/training/evaluate.py\", line 184, in <module>\n    validate_classification(train_cfg_path=TRAIN_CFG)\n  File \"/Users/rebeccajaubert/cursor_projects/crop-mapping-data-preprocessing/cmap_preprocess/yolo/training/evaluate.py\", line 158, in validate_classification\n    metrics.confusion_matrix.print()\n  File \"/Users/rebeccajaubert/Library/Caches/pypoetry/virtualenvs/crop-mapping-data-preprocessing-eE8HP5f1-py3.11/lib/python3.11/site-packages/ultralytics/utils/metrics.py\", line 444, in print\n    LOGGER.info(\" \".join(map(str, self.matrix[i])))\n                                  ~~~~~~~~~~~^^^\nIndexError: index 2 is out of bounds for axis 0 with size 2\n\n### Environment\n\nUltralytics 8.3.73 \ud83d\ude80 Python-3.11.11 torch-2.6.0 CPU (Apple M1 Pro)\nSetup complete \u2705 (8 CPUs, 16.0 GB RAM, 76.1/460.4 GB disk)\n\nOS                  macOS-15.1-arm64-arm-64bit\nEnvironment         Darwin\nPython              3.11.11\nInstall             pip\nRAM                 16.00 GB\nDisk                76.1/460.4 GB\nCPU                 Apple M1 Pro\nCPU count           8\nGPU                 None\nGPU count           None\nCUDA                None\n\nnumpy               \u2705 2.1.1<=2.1.1,>=1.23.0\nmatplotlib          \u2705 3.10.0>=3.3.0\nopencv-python       \u2705 4.11.0.86>=4.6.0\npillow              \u2705 11.1.0>=7.1.2\npyyaml              \u2705 6.0.2>=5.3.1\nrequests            \u2705 2.32.3>=2.23.0\nscipy               \u2705 1.15.1>=1.4.1\ntorch               \u2705 2.6.0>=1.8.0\ntorch               \u2705 2.6.0!=2.4.0,>=1.8.0; sys_platform == \"win32\"\ntorchvision         \u2705 0.21.0>=0.9.0\ntqdm                \u2705 4.67.1>=4.64.0\npsutil              \u2705 6.1.1\npy-cpuinfo          \u2705 9.0.0\npandas              \u2705 2.2.3>=1.1.4\nseaborn             \u2705 0.13.2>=0.11.0\nultralytics-thop    \u2705 2.0.14>=2.0.0\n\n### Minimal Reproducible Example\n\nmetrics = model.val(data=data,\n                        split=\"val\",\n                        save_json=True,\n                        project=results_dir,\n                        name=name)\n\nmetrics.confusion_matrix.print()\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!\n", "hints_text": "\ud83d\udc4b Hello @rebeccageowatch, thank you for bringing this to our attention \ud83d\ude80! We appreciate your detailed bug report and for using Ultralytics.\n\nTo help us address the issue more efficiently, could you confirm if you've already updated to the latest version of Ultralytics? If not, please upgrade using:\n\n```bash\npip install -U ultralytics\n```\n\nThis ensures your issue is not due to outdated code. If the issue persists, providing a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) is crucial for us to debug it thoroughly. Based on your description and the provided traceback, your MRE already looks excellent \ud83d\ude4c! If there's any other information you can add, such as modifications to the codebase or specific datasets used, please include that as well.\n\nIn the meantime, we recommend reviewing our [Docs](https://docs.ultralytics.com/) for additional guidance and checking if there are alternative approaches to your workflow for now. For example, the metrics object could be accessed or manipulated in another way as a temporary workaround.\n\nIf you need more support, join the Ultralytics community where it suits you best:\n- Chat in real-time on [Discord](https://discord.com/invite/ultralytics) \ud83c\udfa7\n- Explore discussions on [Discourse](https://community.ultralytics.com/) or share insights on our [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Verified Environments for YOLO\nFor running Ultralytics YOLO, you can also explore these verified environments:\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Docker** images: <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a> or refer to the [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/)\n\nMeanwhile, an Ultralytics engineer will review this issue and provide further assistance as soon as possible. Thank you for your patience and for contributing to making Ultralytics even better \ud83c\udf1f!\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a> If this badge is green, all tests are passing, ensuring core functionality is verified.\nBy the way the fix is very easy just remove the \"+1\" in the for loop : for i in range(self.nc + 1):", "created_at": "2025-02-10T17:41:59Z"}
{"repo": "ultralytics/ultralytics", "pull_number": 19146, "instance_id": "ultralytics__ultralytics-19146", "issue_numbers": ["19145"], "base_commit": "580b4766c52c1404b6ded6f88860b2ad13117236", "patch": "diff --git a/ultralytics/engine/results.py b/ultralytics/engine/results.py\nindex cda3d85294a..b35f278a71c 100644\n--- a/ultralytics/engine/results.py\n+++ b/ultralytics/engine/results.py\n@@ -583,7 +583,7 @@ def plot(\n         if save:\n             annotator.save(filename)\n \n-        return annotator.result()\n+        return annotator.im if pil else annotator.result()\n \n     def show(self, *args, **kwargs):\n         \"\"\"\n", "test_patch": "", "problem_statement": "result.plot(pil=True)\n### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\n_No response_\n\n### Bug\n\nresults = model.predict(sourece)\nfor result in results: \nso this is the problem: \nwhen result.plot(pil=True) base on ultralytics docs: pil : bool , description: Whether to return the image as a PIL Image.\nbut when I checked it : \nlabeld_image = result.plot(pil=True)\nprint(type(labeld_iamge))\nwill print : <class 'numpy.ndarray'>\nnot <class 'PIL.Image.Image'>\n\n\n### Environment\n\nOS                  Linux-5.15.0-97-generic-x86_64-with-glibc2.31\nEnvironment         Linux\nPython              3.11.11\nInstall             pip\nRAM                 377.56 GB\nDisk                239.8/467.4 GB\nCPU                 Intel Xeon Platinum 8176 2.10GHz\nCPU count           112\nGPU                 NVIDIA GeForce RTX 4090, 24210MiB\nGPU count           3\nCUDA                12.4\n\nnumpy               \u2705 2.1.1<=2.1.1,>=1.23.0\nmatplotlib          \u2705 3.10.0>=3.3.0\nopencv-python       \u2705 4.11.0.86>=4.6.0\npillow              \u2705 11.1.0>=7.1.2\npyyaml              \u2705 6.0.2>=5.3.1\nrequests            \u2705 2.32.3>=2.23.0\nscipy               \u2705 1.15.1>=1.4.1\ntorch               \u2705 2.6.0>=1.8.0\ntorch               \u2705 2.6.0!=2.4.0,>=1.8.0; sys_platform == \"win32\"\ntorchvision         \u2705 0.21.0>=0.9.0\ntqdm                \u2705 4.67.1>=4.64.0\npsutil              \u2705 6.1.1\npy-cpuinfo          \u2705 9.0.0\npandas              \u2705 2.2.3>=1.1.4\nseaborn             \u2705 0.13.2>=0.11.0\nultralytics-thop    \u2705 2.0.14>=2.0.0\n\n### Minimal Reproducible Example\n\n``` \nresults = model.track(source=\"offline_video_path\", stream=True, persist=True,)\nfor result in results:\n    labeld_image = result.plot(pil=True)\n    print(type(labeld_image ))\n```\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!\n", "hints_text": "\ud83d\udc4b Hello @ansaricard, thank you for your interest in Ultralytics \ud83d\ude80! We recommend checking out our comprehensive [Docs](https://docs.ultralytics.com/) for guidance. You'll find examples for [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage, where many common questions are already addressed.\n\nIt looks like you've encountered a discrepancy with the `result.plot(pil=True)` functionality. If this is a \ud83d\udc1b Bug Report, please confirm you are using the latest `ultralytics` version and provide a clear [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/), which you\u2019ve already partially included\u2014great start! More detail on the input data or outputs could help us debug this issue.\n\n## Upgrade\n\nEnsure you're using the latest version of the package to rule out outdated issues. To upgrade, run:\n\n```bash\npip install -U ultralytics\n```\n\n## Resources\n\nJoin the Ultralytics community to discuss, debug, and discover:\n- [Discord](https://discord.com/invite/ultralytics) for real-time chat \ud83c\udfa7\n- [Discourse](https://community.ultralytics.com/) for structured discussions\n- [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge and learn \ud83d\udcda\n\n## Verified Environments\n\nYou can test the issue in these verified environments:\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud**: See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon**: See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**: See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing, validating correct performance across tasks and environments.\n\nThis is an automated response \ud83d\udee0\ufe0f, but an Ultralytics engineer will review and assist you further as soon as possible. Thank you for bringing this to our attention!", "created_at": "2025-02-09T13:18:50Z"}
{"repo": "ultralytics/ultralytics", "pull_number": 19144, "instance_id": "ultralytics__ultralytics-19144", "issue_numbers": ["19143"], "base_commit": "ff1a04609ff4d88e7f540d9ba181b8ed47080ad1", "patch": "diff --git a/ultralytics/__init__.py b/ultralytics/__init__.py\nindex 2b6471dc98d..f93677768ec 100644\n--- a/ultralytics/__init__.py\n+++ b/ultralytics/__init__.py\n@@ -1,6 +1,6 @@\n # Ultralytics \ud83d\ude80 AGPL-3.0 License - https://ultralytics.com/license\n \n-__version__ = \"8.3.73\"\n+__version__ = \"8.3.74\"\n \n import os\n \ndiff --git a/ultralytics/utils/callbacks/raytune.py b/ultralytics/utils/callbacks/raytune.py\nindex e7e01d0985f..dd41f29b031 100644\n--- a/ultralytics/utils/callbacks/raytune.py\n+++ b/ultralytics/utils/callbacks/raytune.py\n@@ -14,7 +14,7 @@\n \n def on_fit_epoch_end(trainer):\n     \"\"\"Sends training metrics to Ray Tune at end of each epoch.\"\"\"\n-    if ray.train._internal.session._get_session():  # replacement for deprecated ray.tune.is_session_enabled()\n+    if ray.train._internal.session.get_session():  # replacement for deprecated ray.tune.is_session_enabled()\n         metrics = trainer.metrics\n         session.report({**metrics, **{\"epoch\": trainer.epoch + 1}})\n \n", "test_patch": "", "problem_statement": "Errors during Training\n### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\nTrain\n\n### Bug\n\nError During training\nYOLO command for train:\n!yolo task=detect mode=train model=yolo11x.pt data=/kaggle/working/3riders-2/data.yaml epochs=160 imgsz=640 plots=True device=0,1\n\n\nError:\nEpoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      1/160      9.21G      1.725      2.757      2.032         27        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        514        952      0.345       0.36      0.356      0.164\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/root/.config/Ultralytics/DDP/_temp_wk1f1hyc133774982102704.py\", line 13, in <module>\n[rank0]:     results = trainer.train()\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py\", line 207, in train\n[rank0]:     self._do_train(world_size)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py\", line 453, in _do_train\n[rank0]:     self.run_callbacks(\"on_fit_epoch_end\")\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py\", line 168, in run_callbacks\n[rank0]:     callback(self)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/ultralytics/utils/callbacks/raytune.py\", line 17, in on_fit_epoch_end\n[rank0]:     if ray.train._internal.session._get_session():  # replacement for deprecated ray.tune.is_session_enabled()\n[rank0]: AttributeError: module 'ray.train._internal.session' has no attribute '_get_session'. Did you mean: 'get_session'?\nW0209 04:51:33.772000 229 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 232 closing signal SIGTERM\nE0209 04:51:34.087000 229 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 231) of binary: /usr/bin/python3\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 923, in <module>\n    main()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 919, in main\n    run(args)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 910, in run\n    elastic_launch(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n============================================================\n/root/.config/Ultralytics/DDP/_temp_wk1f1hyc133774982102704.py FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-02-09_04:51:33\n  host      : 0e286ac1e7dd\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 231)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n============================================================\nTraceback (most recent call last):\n  File \"/usr/local/bin/yolo\", line 8, in <module>\n    sys.exit(entrypoint())\n  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/cfg/__init__.py\", line 986, in entrypoint\n    getattr(model, mode)(**overrides)  # default args from model\n  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/engine/model.py\", line 808, in train\n    self.trainer.train()\n  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py\", line 202, in train\n    raise e\n  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py\", line 200, in train\n    subprocess.run(cmd, check=True)\n  File \"/usr/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['/usr/bin/python3', '-m', 'torch.distributed.run', '--nproc_per_node', '2', '--master_port', '36493', '/root/.config/Ultralytics/DDP/_temp_wk1f1hyc133774982102704.py']' returned non-zero exit status 1.\n\n### Environment\n\nUltralytics 8.3.73 \ud83d\ude80 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15095MiB)\nSetup complete \u2705 (4 CPUs, 31.4 GB RAM, 6135.3/8062.4 GB disk)\n\nOS                  Linux-6.6.56+-x86_64-with-glibc2.35\nEnvironment         Colab\nPython              3.10.12\nInstall             pip\nRAM                 31.35 GB\nDisk                6135.3/8062.4 GB\nCPU                 Intel Xeon 2.00GHz\nCPU count           4\nGPU                 Tesla T4, 15095MiB\nGPU count           2\nCUDA                12.1\n\nnumpy               \u2705 1.26.4<=2.1.1,>=1.23.0\nmatplotlib          \u2705 3.7.5>=3.3.0\nopencv-python       \u2705 4.10.0.84>=4.6.0\npillow              \u2705 11.0.0>=7.1.2\npyyaml              \u2705 6.0.2>=5.3.1\nrequests            \u2705 2.32.3>=2.23.0\nscipy               \u2705 1.13.1>=1.4.1\ntorch               \u2705 2.5.1+cu121>=1.8.0\ntorch               \u2705 2.5.1+cu121!=2.4.0,>=1.8.0; sys_platform == \"win32\"\ntorchvision         \u2705 0.20.1+cu121>=0.9.0\ntqdm                \u2705 4.67.1>=4.64.0\npsutil              \u2705 5.9.5\npy-cpuinfo          \u2705 9.0.0\npandas              \u2705 2.2.3>=1.1.4\nseaborn             \u2705 0.12.2>=0.11.0\nultralytics-thop    \u2705 2.0.14>=2.0.0\n\n### Minimal Reproducible Example\n\n!pip install ultralytics roboflow tensorflow==2.17.0\n\n\nfrom roboflow import Roboflow\nrf = Roboflow(api_key=\"enter_api\")\nproject = rf.workspace(\"kashish\").project(\"3riders\")\nversion = project.version(2)\ndataset = version.download(\"yolov11\")\n\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!\n", "hints_text": "\ud83d\udc4b Hello @ppraneth, thank you for reporting this issue and for using Ultralytics \ud83d\ude80! We appreciate your detailed report.\n\nTo help us debug the error more efficiently, could you please provide a [minimum reproducible example (MRE)](https://docs.ultralytics.com/help/minimum-reproducible-example/)? This would involve reducing the issue to the simplest and smallest set of steps necessary to replicate the problem (e.g., specific dataset snippets, code sections, or training configurations). This helps us investigate and resolve your issue faster. \ud83d\udc1b\n\n### Recommendations\n\n1. Ensure you are using the latest version of Ultralytics and its dependencies. You can upgrade everything with the following command:\n   ```bash\n   pip install -U ultralytics\n   ```\n\n2. For optimal setup, YOLO can be run in the following verified environments, which ensure compatibility:\n   - **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n   - **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n   - **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n   - **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n3. Verify that all required dependencies, including [Python (>=3.8)](https://www.python.org/) and [PyTorch (>=1.8)](https://pytorch.org/get-started/locally/), are installed in your environment:  \n   - [**pyproject.toml** Requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml)\n\n4. If you are using `ray` or distributed training, confirm that you're on compatible versions of libraries like `torch`, `ray`, or other relevant dependencies.\n\n### More Resources\n\n\ud83d\udcd6 Documentation  \nFor additional guidance, check out our [Docs](https://docs.ultralytics.com/), especially the sections on [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage.\n\n\ud83c\udfa7 Community Support  \nNeed real-time support? Feel free to join our [Discord channel](https://discord.com/invite/ultralytics). For more detailed discussions, explore our [Discourse](https://community.ultralytics.com/) or dive into threads on the [Subreddit](https://reddit.com/r/Ultralytics).\n\n\ud83d\udee0\ufe0f **Status**  \n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>  \nCheck our CI tests for status on compatibility across macOS, Windows, and Ubuntu environments.\n\n---\n\n\ud83e\udd16 *This is an automated response to assist you faster. An Ultralytics engineer will also review your issue as soon as possible. Thank you for your patience! \ud83d\ude4c*\nTry `pip uninstall ray -y`.\n> Try `pip uninstall ray -y`.\n\nTry the above or try \"pip install --upgrade ray\". By seeing this error its likely that it is happening due to mismatch or an outdated package. \n", "created_at": "2025-02-09T09:24:53Z"}
{"repo": "ultralytics/ultralytics", "pull_number": 19083, "instance_id": "ultralytics__ultralytics-19083", "issue_numbers": ["19082"], "base_commit": "de065383bc11e77b6472e7cfb65638aab25406d7", "patch": "diff --git a/ultralytics/engine/exporter.py b/ultralytics/engine/exporter.py\nindex df136c148da..2cf3d26cec9 100644\n--- a/ultralytics/engine/exporter.py\n+++ b/ultralytics/engine/exporter.py\n@@ -1556,8 +1556,8 @@ def forward(self, x):\n         preds = self.model(x)\n         pred = preds[0] if isinstance(preds, tuple) else preds\n         pred = pred.transpose(-1, -2)  # shape(1,84,6300) to shape(1,6300,84)\n-        extra_shape = pred.shape[-1] - (4 + self.model.nc)  # extras from Segment, OBB, Pose\n-        boxes, scores, extras = pred.split([4, self.model.nc, extra_shape], dim=2)\n+        extra_shape = pred.shape[-1] - (4 + len(self.model.names))  # extras from Segment, OBB, Pose\n+        boxes, scores, extras = pred.split([4, len(self.model.names), extra_shape], dim=2)\n         scores, classes = scores.max(dim=-1)\n         self.args.max_det = min(pred.shape[1], self.args.max_det)  # in case num_anchors < max_det\n         # (N, max_det, 4 coords + 1 class score + 1 class label + extra_shape).\n", "test_patch": "", "problem_statement": "nms=true for exporting to onnx\n### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\n_No response_\n\n### Bug\n\n\n\n\ni get this error\n\n```\n(yolo) root@workstation-016:/mnt/4T/Tohidi/object_detector_service# yolo export model=yolo11\nx.pt nms=true format=engine device=3                                                        \nUltralytics 8.3.71 \ud83d\ude80 Python-3.10.0 torch-2.5.1+cu124 CUDA:3 (NVIDIA H100 PCIe, 80995MiB)   \nYOLO11x summary (fused): 464 layers, 56,919,424 parameters, 0 gradients, 194.9 GFLOPs       \nTraceback (most recent call last):                                                          \n  File \"/opt/anaconda3/envs/yolo/bin/yolo\", line 8, in <module>                             \n    sys.exit(entrypoint())                                                                  \n  File \"/opt/anaconda3/envs/yolo/lib/python3.10/site-packages/ultralytics/cfg/__init__.py\", \nline 986, in entrypoint                                                                     \n    getattr(model, mode)(**overrides)  # default args from model                            \n  File \"/opt/anaconda3/envs/yolo/lib/python3.10/site-packages/ultralytics/engine/model.py\", \nline 740, in export                                                                         \n    return Exporter(overrides=args, _callbacks=self.callbacks)(model=self.model)            \n  File \"/opt/anaconda3/envs/yolo/lib/python3.10/site-packages/ultralytics/engine/exporter.py\n\", line 354, in __call__                                                                    \n    y = NMSModel(model, self.args)(im) if self.args.nms and not coreml else model(im)       \n  File \"/opt/anaconda3/envs/yolo/lib/python3.10/site-packages/torch/nn/modules/module.py\", l\nine 1736, in _wrapped_call_impl                                                             \n    return self._call_impl(*args, **kwargs)                                                 \n  File \"/opt/anaconda3/envs/yolo/lib/python3.10/site-packages/torch/nn/modules/module.py\", l\nine 1747, in _call_impl                                                                     \n    return forward_call(*args, **kwargs)                                                    \n  File \"/opt/anaconda3/envs/yolo/lib/python3.10/site-packages/ultralytics/engine/exporter.py\n\", line 1559, in forward                                                                    \n    extra_shape = pred.shape[-1] - (4 + self.model.nc)  # extras from Segment, OBB, Pose    \n  File \"/opt/anaconda3/envs/yolo/lib/python3.10/site-packages/torch/nn/modules/module.py\", l\nine 1931, in __getattr__                                                                    \n    raise AttributeError(                                                                   \nAttributeError: 'DetectionModel' object has no attribute 'nc'                               \n```\n\n****\n\n### Environment\n\n```\nUltralytics 8.3.71 \ud83d\ude80 Python-3.10.0 torch-2.5.1+cu124 CUDA:0 (NVIDIA H100 80GB HBM3, 80995MiB)    \nSetup complete \u2705 (255 CPUs, 1007.7 GB RAM, 1807.6/1831.2 GB disk)                                \n                                                                                                  \nOS                  Linux-5.15.0-131-generic-x86_64-with-glibc2.35                                \nEnvironment         Linux                                                                         \nPython              3.10.0                                                                        \nInstall             pip                                                                           \nRAM                 1007.65 GB                                                                    \nDisk                1807.6/1831.2 GB                                                              \nCPU                 AMD EPYC 7773X 64-Core Processor                                              \nCPU count           255                                                                           \nGPU                 NVIDIA H100 80GB HBM3, 80995MiB                                               \nGPU count           6                                                                             \nCUDA                12.4                                                                          \n                                                                                                  \nnumpy               \u2705 1.26.4<=2.1.1,>=1.23.0                                                     \nmatplotlib          \u2705 3.10.0>=3.3.0                                                              \nopencv-python       \u2705 4.11.0.86>=4.6.0                                                           \npillow              \u2705 11.1.0>=7.1.2                                                              \npyyaml              \u2705 6.0.2>=5.3.1                                                               \nrequests            \u2705 2.32.3>=2.23.0                                                             \nscipy               \u2705 1.15.1>=1.4.1                                                              \ntorch               \u2705 2.5.1>=1.8.0                                                               \ntorch               \u2705 2.5.1!=2.4.0,>=1.8.0; sys_platform == \"win32\"                              \ntorchvision         \u2705 0.20.1>=0.9.0                                                              \ntqdm                \u2705 4.67.1>=4.64.0                                                             \npsutil              \u2705 6.1.1                                                                      \npy-cpuinfo          \u2705 9.0.0                                                                      \npandas              \u2705 2.0.3>=1.1.4                                                               \nseaborn             \u2705 0.13.2>=0.11.0                                                             \nultralytics-thop    \u2705 2.0.14>=2.0.0                                                              \n\n```\n\n### Minimal Reproducible Example\n\n```\nyolo export model=yolo11x.pt format=engine device=3 nms=true\n```\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!\n", "hints_text": "\ud83d\udc4b Hello @mohamad-tohidi, thank you for your interest in Ultralytics \ud83d\ude80! We recommend checking out the [Docs](https://docs.ultralytics.com/) for guidance where you can find helpful sections including [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples, which may already address your query.\n\nIt seems like you've encountered a \ud83d\udc1b bug. To help us debug the issue, could you please provide a [minimum reproducible example (MRE)](https://docs.ultralytics.com/help/minimum-reproducible-example/)? The MRE you included is a great start\u2014thank you! If possible, please also include further details about the specific model (`yolo11x.pt` in this case) and explain if it differs from the base Ultralytics models.\n\nIn the meantime, here are some troubleshooting suggestions:\n\n## Upgrade\n\nEnsure you are using the latest version of the `ultralytics` package. You can upgrade with the following command:\n\n```bash\npip install -U ultralytics\n```\n\nThis will install the latest release, including any bug fixes that might resolve your issue.\n\n## Environment Recommendations\n\nYOLO works best in environments with all required dependencies installed. Consider using any of the verified environments below for optimal performance:\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Additional Help\n\nJoin the Ultralytics community to connect with other users and get real-time help:\n- [Discord](https://discord.com/invite/ultralytics) \ud83c\udfa7 for live chats\n- [Discourse](https://community.ultralytics.com/) for in-depth discussions\n- [Subreddit](https://reddit.com/r/Ultralytics) for community threads\n\n## Status\n\nAlso, check the health of our Continuous Integration (CI) workflow here: <a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>. If the badge is green, our tests are passing across all platforms, including macOS, Windows, and Ubuntu.\n\n\ud83d\udccc This is an automated response to provide assistance as quickly as possible. An Ultralytics engineer will review your issue and assist you further soon.", "created_at": "2025-02-05T12:10:12Z"}
{"repo": "ultralytics/ultralytics", "pull_number": 18894, "instance_id": "ultralytics__ultralytics-18894", "issue_numbers": ["18883"], "base_commit": "e99b778cc4f4e429103c36ab00bf7d5288225377", "patch": "diff --git a/ultralytics/__init__.py b/ultralytics/__init__.py\nindex e7be7b76255..d130487d011 100644\n--- a/ultralytics/__init__.py\n+++ b/ultralytics/__init__.py\n@@ -1,6 +1,6 @@\n # Ultralytics \ud83d\ude80 AGPL-3.0 License - https://ultralytics.com/license\n \n-__version__ = \"8.3.67\"\n+__version__ = \"8.3.68\"\n \n import os\n \ndiff --git a/ultralytics/utils/benchmarks.py b/ultralytics/utils/benchmarks.py\nindex c183203e1a5..034b93008e1 100644\n--- a/ultralytics/utils/benchmarks.py\n+++ b/ultralytics/utils/benchmarks.py\n@@ -134,7 +134,7 @@ def benchmark(\n \n             # Export\n             if format == \"-\":\n-                filename = model.ckpt_path or model.cfg\n+                filename = model.pt_path or model.ckpt_path or model.model_name\n                 exported_model = model  # PyTorch format\n             else:\n                 filename = model.export(imgsz=imgsz, format=format, half=half, int8=int8, device=device, verbose=False)\n@@ -169,7 +169,7 @@ def benchmark(\n     check_yolo(device=device)  # print system info\n     df = pd.DataFrame(y, columns=[\"Format\", \"Status\u2754\", \"Size (MB)\", key, \"Inference time (ms/im)\", \"FPS\"])\n \n-    name = Path(model.ckpt_path).name\n+    name = model.model_name\n     s = f\"\\nBenchmarks complete for {name} on {data} at imgsz={imgsz} ({time.time() - t0:.2f}s)\\n{df}\\n\"\n     LOGGER.info(s)\n     with open(\"benchmarks.log\", \"a\", errors=\"ignore\", encoding=\"utf-8\") as f:\n", "test_patch": "", "problem_statement": "Benchmark crashes with YOLOv6 customized model\n### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\nOther\n\n### Bug\n\nI have this model:\n```\n%%writefile yolov6-face.yaml\n# Ultralytics YOLO \ud83d\ude80, AGPL-3.0 license\n# YOLOv6 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/models/yolov6\n\n# Parameters\nnc: 1 # number of classes\nactivation: nn.ReLU() # (optional) model default activation function\nscales: # model compound scaling constants, i.e. 'model=yolov6n.yaml' will call yolov8.yaml with scale 'n'\n  # [depth, width, max_channels]\n  p: [0.33, 0.25, 8] # nano is [0.33, 0.25, 1024]\n\n# YOLOv6-3.0s backbone\nbackbone:\n  # [from, repeats, module, args]\n  - [-1, 1, Conv, [64, 3, 2]] # 0-P1/2\n  - [-1, 1, Conv, [128, 3, 2]] # 1-P2/4\n  - [-1, 6, Conv, [128, 3, 1]]\n  - [-1, 1, Conv, [256, 3, 2]] # 3-P3/8\n  - [-1, 12, Conv, [256, 3, 1]]\n  - [-1, 1, Conv, [512, 3, 2]] # 5-P4/16\n  - [-1, 18, Conv, [512, 3, 1]]\n  - [-1, 1, Conv, [1024, 3, 2]] # 7-P5/32\n  - [-1, 6, Conv, [1024, 3, 1]]\n  - [-1, 1, SPPF, [1024, 5]] # 9\n\n# YOLOv6-3.0s head\nhead:\n  - [-1, 1, Conv, [256, 1, 1]]\n  - [-1, 1, nn.ConvTranspose2d, [256, 2, 2, 0]]\n  - [[-1, 6], 1, Concat, [1]] # cat backbone P4\n  - [-1, 1, Conv, [256, 3, 1]]\n  - [-1, 9, Conv, [256, 3, 1]] # 14\n\n  - [-1, 1, Conv, [128, 1, 1]]\n  - [-1, 1, nn.ConvTranspose2d, [128, 2, 2, 0]]\n  - [[-1, 4], 1, Concat, [1]] # cat backbone P3\n  - [-1, 1, Conv, [128, 3, 1]]\n  - [-1, 9, Conv, [128, 3, 1]] # 19\n\n  - [[14, 19], 1, Detect, [nc]] # Detect(P3, P4, P5)\n```\n\nThat I train with this code:\n```\nmodel = YOLO(\"./yolov6-face.yaml\")\n\nr = model.train(data=\"face-detection-dataset.yaml\", epochs=1, imgsz='192,320', single_cls=True, plots=True, batch=500)\n\nfrom ultralytics.utils.benchmarks import benchmark\nmodel.benchmark(data=\"face-detection-dataset.yaml\", imgsz='192,320', device=\"cpu\")\n```\n\nBut running the benchmark crashes with:\n```\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[7], line 2\n      1 from ultralytics.utils.benchmarks import benchmark\n----> 2 model.benchmark(data=\"face-detection-dataset.yaml\", imgsz='192,320', device=\"cpu\")\n\nFile /opt/conda/lib/python3.10/site-packages/ultralytics/engine/model.py:682, in Model.benchmark(self, **kwargs)\n    680 custom = {\"verbose\": False}  # method defaults\n    681 args = {**DEFAULT_CFG_DICT, **self.model.args, **custom, **kwargs, \"mode\": \"benchmark\"}\n--> 682 return benchmark(\n    683     model=self,\n    684     data=kwargs.get(\"data\"),  # if no 'data' argument passed set data=None for default datasets\n    685     imgsz=args[\"imgsz\"],\n    686     half=args[\"half\"],\n    687     int8=args[\"int8\"],\n    688     device=args[\"device\"],\n    689     verbose=kwargs.get(\"verbose\"),\n    690 )\n\nFile /opt/conda/lib/python3.10/site-packages/ultralytics/utils/benchmarks.py:172, in benchmark(model, data, imgsz, half, int8, device, verbose, eps)\n    169 check_yolo(device=device)  # print system info\n    170 df = pd.DataFrame(y, columns=[\"Format\", \"Status\u2754\", \"Size (MB)\", key, \"Inference time (ms/im)\", \"FPS\"])\n--> 172 name = Path(model.ckpt_path).name\n    173 s = f\"\\nBenchmarks complete for {name} on {data} at imgsz={imgsz} ({time.time() - t0:.2f}s)\\n{df}\\n\"\n    174 LOGGER.info(s)\n\nFile /opt/conda/lib/python3.10/pathlib.py:960, in Path.__new__(cls, *args, **kwargs)\n    958 if cls is Path:\n    959     cls = WindowsPath if os.name == 'nt' else PosixPath\n--> 960 self = cls._from_parts(args)\n    961 if not self._flavour.is_supported:\n    962     raise NotImplementedError(\"cannot instantiate %r on your system\"\n    963                               % (cls.__name__,))\n\nFile /opt/conda/lib/python3.10/pathlib.py:594, in PurePath._from_parts(cls, args)\n    589 @classmethod\n    590 def _from_parts(cls, args):\n    591     # We need to call _parse_args on the instance, so as to get the\n    592     # right flavour.\n    593     self = object.__new__(cls)\n--> 594     drv, root, parts = self._parse_args(args)\n    595     self._drv = drv\n    596     self._root = root\n\nFile /opt/conda/lib/python3.10/pathlib.py:578, in PurePath._parse_args(cls, args)\n    576     parts += a._parts\n    577 else:\n--> 578     a = os.fspath(a)\n    579     if isinstance(a, str):\n    580         # Force-cast str subclasses to str (issue #21127)\n    581         parts.append(str(a))\n\nTypeError: expected str, bytes or os.PathLike object, not NoneType\n```\n\nRunning with:\n```\nfrom ultralytics.utils.benchmarks import benchmark\nbenchmark(model=\"/kaggle/working/runs/detect/train/weights/best.pt\", data=\"face-detection-dataset.yaml\", imgsz='192,320', device=\"cpu\")\n```\n\nMostly returns NaN even though I can actually export to Tensorflow Lite and NCNN (and probably others):\n```\nSetup complete \u2705 (4 CPUs, 31.4 GB RAM, 6100.3/8062.4 GB disk)\n\nBenchmarks complete for best.pt on face-detection-dataset.yaml at imgsz=192,320 (536.99s)\n                   Format Status\u2754  Size (MB)  metrics/mAP50-95(B)  Inference time (ms/im)     FPS\n0                 PyTorch       \u2705        0.3               0.2027                   16.46   60.73\n1             TorchScript       \u274e        0.7                  NaN                     NaN     NaN\n2                    ONNX       \u274e        0.5                  NaN                     NaN     NaN\n3                OpenVINO       \u274e        0.6                  NaN                     NaN     NaN\n4                TensorRT       \u274c        0.0                  NaN                     NaN     NaN\n5                  CoreML       \u274e        0.3                  NaN                     NaN     NaN\n6   TensorFlow SavedModel       \u274e        1.4                  NaN                     NaN     NaN\n7     TensorFlow GraphDef       \u274e        0.5                  NaN                     NaN     NaN\n8         TensorFlow Lite       \u274e        0.5                  NaN                     NaN     NaN\n9     TensorFlow Edge TPU       \u274e        0.3                  NaN                     NaN     NaN\n10          TensorFlow.js       \u274e        0.5                  NaN                     NaN     NaN\n11           PaddlePaddle       \u274e        1.0                  NaN                     NaN     NaN\n12                    MNN       \u274e        0.5                  NaN                     NaN     NaN\n13                   NCNN       \u2705        0.5               0.0003                    5.97  167.53\n14                    IMX       \u274c        0.0                  NaN                     NaN     NaN\n15                   RKNN       \u274c        0.0                  NaN                     NaN     NaN\n```\n\n### Environment\n\n```\nUltralytics 8.3.67 \ud83d\ude80 Python-3.10.14 torch-2.4.0 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\nSetup complete \u2705 (4 CPUs, 31.4 GB RAM, 6095.9/8062.4 GB disk)\n\nOS                  Linux-6.6.56+-x86_64-with-glibc2.35\nEnvironment         Kaggle\nPython              3.10.14\nInstall             pip\nRAM                 31.35 GB\nDisk                6095.9/8062.4 GB\nCPU                 Intel Xeon 2.00GHz\nCPU count           4\nGPU                 Tesla P100-PCIE-16GB, 16269MiB\nGPU count           1\nCUDA                12.3\n\nnumpy               \u2705 1.26.4>=1.23.0\nnumpy               \u2705 1.26.4<2.0.0; sys_platform == \"darwin\"\nmatplotlib          \u2705 3.7.5>=3.3.0\nopencv-python       \u2705 4.10.0.84>=4.6.0\npillow              \u2705 11.0.0>=7.1.2\npyyaml              \u2705 6.0.2>=5.3.1\nrequests            \u2705 2.32.3>=2.23.0\nscipy               \u2705 1.14.1>=1.4.1\ntorch               \u2705 2.4.0>=1.8.0\ntorch               \u2705 2.4.0!=2.4.0,>=1.8.0; sys_platform == \"win32\"\ntorchvision         \u2705 0.19.0>=0.9.0\ntqdm                \u2705 4.66.4>=4.64.0\npsutil              \u2705 5.9.3\npy-cpuinfo          \u2705 9.0.0\npandas              \u2705 2.2.3>=1.1.4\nseaborn             \u2705 0.12.2>=0.11.0\nultralytics-thop    \u2705 2.0.14>=2.0.0\n```\n\n### Minimal Reproducible Example\n\ndataset:\n```\n%%writefile face-detection-dataset.yaml\n# CC0: Public Domain license\n# Face-Detection-Dataset dataset by Fares Elmenshawii\n# Documentation: https://www.kaggle.com/datasets/fareselmenshawii/face-detection-dataset\n\n# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\npath: /kaggle/input/face-detection-dataset # dataset root dir\ntrain: images/train # train images (relative to 'path')\nval: images/val # val images (relative to 'path')\ntest: # test images (optional)\n\n# Classes\nnames:\n  0: face\n\n# Download script/URL (optional)\ndownload: https://storage.googleapis.com/kaggle-data-sets/3345370/5891144/bundle/archive.zip\n```\n\nmodel:\n```\n%%writefile yolov6-face.yaml\n# Ultralytics YOLO \ud83d\ude80, AGPL-3.0 license\n# YOLOv6 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/models/yolov6\n\n# Parameters\nnc: 1 # number of classes\nactivation: nn.ReLU() # (optional) model default activation function\nscales: # model compound scaling constants, i.e. 'model=yolov6n.yaml' will call yolov8.yaml with scale 'n'\n  # [depth, width, max_channels]\n  p: [0.33, 0.25, 8] # nano is [0.33, 0.25, 1024]\n\n# YOLOv6-3.0s backbone\nbackbone:\n  # [from, repeats, module, args]\n  - [-1, 1, Conv, [64, 3, 2]] # 0-P1/2\n  - [-1, 1, Conv, [128, 3, 2]] # 1-P2/4\n  - [-1, 6, Conv, [128, 3, 1]]\n  - [-1, 1, Conv, [256, 3, 2]] # 3-P3/8\n  - [-1, 12, Conv, [256, 3, 1]]\n  - [-1, 1, Conv, [512, 3, 2]] # 5-P4/16\n  - [-1, 18, Conv, [512, 3, 1]]\n  - [-1, 1, Conv, [1024, 3, 2]] # 7-P5/32\n  - [-1, 6, Conv, [1024, 3, 1]]\n  - [-1, 1, SPPF, [1024, 5]] # 9\n\n# YOLOv6-3.0s head\nhead:\n  - [-1, 1, Conv, [256, 1, 1]]\n  - [-1, 1, nn.ConvTranspose2d, [256, 2, 2, 0]]\n  - [[-1, 6], 1, Concat, [1]] # cat backbone P4\n  - [-1, 1, Conv, [256, 3, 1]]\n  - [-1, 9, Conv, [256, 3, 1]] # 14\n\n  - [-1, 1, Conv, [128, 1, 1]]\n  - [-1, 1, nn.ConvTranspose2d, [128, 2, 2, 0]]\n  - [[-1, 4], 1, Concat, [1]] # cat backbone P3\n  - [-1, 1, Conv, [128, 3, 1]]\n  - [-1, 9, Conv, [128, 3, 1]] # 19\n\n  - [[14, 19], 1, Detect, [nc]] # Detect(P3, P4, P5)\n```\n\ncode:\n```\nmodel = YOLO(\"./yolov6-face.yaml\")\n\nr = model.train(data=\"face-detection-dataset.yaml\", epochs=1, imgsz='192,320', single_cls=True, plots=True, batch=500)\n\nfrom ultralytics.utils.benchmarks import benchmark\nmodel.benchmark(data=\"face-detection-dataset.yaml\", imgsz='192,320', device=\"cpu\")\n```\n\n### Additional\n\nExporting works just fine.\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!\n", "hints_text": "\ud83d\udc4b Hello @EmmanuelMess, thank you for your interest in Ultralytics \ud83d\ude80! We appreciate your detailed report and enthusiasm for using YOLO models. It looks like you're encountering issues while benchmarking your customized YOLOv6 model. Let's work together to address this!\n\nWe recommend starting with the following steps:\n\n- Ensure you're using the latest version of `ultralytics` since updates often resolve existing issues. Upgrade with:\n  ```bash\n  pip install -U ultralytics\n  ```\n\n- Verify that all dependencies have been correctly installed and meet the compatibility requirements. The full list of dependencies is available in our [pyproject.toml](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml).\n\n- If possible, try running your setup in a verified environment to isolate the issue. YOLO runs seamlessly in the following environments:\n  - **Notebooks**: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n  - **Google Cloud**: Deep Learning VM ([GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/))\n  - **Amazon AWS**: Deep Learning AMI ([AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/))\n  - **Docker Image**: ([Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/)) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\nIf this issue is a \ud83d\udc1b **Bug**, it would be extremely helpful if you could provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it more effectively. You've already shared many details (thank you!) but please confirm if these are sufficient to reproduce the error as-is.\n\nIf this is a training or benchmarking-related \u2753 **Question**, consider reviewing our [Training Tips](https://docs.ultralytics.com/guides/model-training-tips/) or join the community for further assistance.\n\n## Community Support \u2764\ufe0f\nMeanwhile, feel free to join the Ultralytics community where you can engage with other users while awaiting an engineer's input:\n- **[Discord](https://discord.com/invite/ultralytics)** for real-time support and chats \ud83c\udfa7\n- **[Discourse](https://community.ultralytics.com/)** for in-depth discussions \ud83d\udcd6\n- **[Subreddit](https://reddit.com/r/Ultralytics)** to discuss and share project insights \ud83e\udd16\n\n## Status Information\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>  \nThis badge indicates the status of our Continuous Integration (CI) tests. A green badge ensures all tests are passing for verified operations across platforms.\n\nAn Ultralytics engineer will review this issue and assist you shortly. Thank you for your patience! \ud83d\ude80\nYou should load the best.pt or last.pt after training.\n> You should load the best.pt or last.pt after training.\n\nI did, and I get a lot of NaN results, even if the exports work. I mention it in the issue.", "created_at": "2025-01-25T23:24:01Z"}
{"repo": "ultralytics/ultralytics", "pull_number": 18772, "instance_id": "ultralytics__ultralytics-18772", "issue_numbers": ["18771"], "base_commit": "673b43ce17e010101c311eb2b02fce8136d4b373", "patch": "diff --git a/ultralytics/data/build.py b/ultralytics/data/build.py\nindex 33b31ba4fb3..468238308b7 100644\n--- a/ultralytics/data/build.py\n+++ b/ultralytics/data/build.py\n@@ -49,11 +49,15 @@ def __iter__(self):\n \n     def __del__(self):\n         \"\"\"Ensure that workers are terminated.\"\"\"\n-        if hasattr(self.iterator, \"_workers\"):\n+        try:\n+            if not hasattr(self.iterator, \"_workers\"):\n+                return\n             for w in self.iterator._workers:  # force terminate\n                 if w.is_alive():\n                     w.terminate()\n             self.iterator._shutdown_workers()  # cleanup\n+        except Exception:\n+            pass\n \n     def reset(self):\n         \"\"\"\n", "test_patch": "", "problem_statement": "yolov11 train\n### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nException ignored in: <function InfiniteDataLoader.__del__ at 0x000001CC205E6700>\nTraceback (most recent call last):\n  File \"G:\\Learning\\yolo\\yolov11_ws\\ultralytics\\ultralytics\\data\\build.py\", line 52, in __del__\n    if hasattr(self.iterator, \"_workers\"):\nAttributeError: 'InfiniteDataLoader' object has no attribute 'iterator'\nAttributeError: 'InfiniteDataLoader' object has no attribute 'iterator'\n\n### Additional\n\n_No response_\n", "hints_text": "\ud83d\udc4b Hello @wuliu-G, thank you for your interest in Ultralytics \ud83d\ude80! We appreciate your detailed report. An Ultralytics engineer will review and assist you soon.\n\nIn the meantime, here are some steps that might help you troubleshoot the issue:\n\nIf this is a \ud83d\udc1b Bug Report, the error you've shared indicates an issue with the `InfiniteDataLoader`. To help us better understand and debug the problem, could you please provide a [minimum reproducible example (MRE)](https://docs.ultralytics.com/help/minimum-reproducible-example/)? This can include details like:\n\n- The version of the `ultralytics` package you are using (`pip show ultralytics`).\n- Your operating environment (OS, Python version, GPU/CPU details, etc.).\n- A minimal piece of code that replicates this error.\n\nIf you haven\u2019t already, please ensure you're running the latest version of the `ultralytics` package and its dependencies by upgrading your environment:\n\n```bash\npip install -U ultralytics\n```\n\nWe also recommend running YOLO tasks in one of our pre-verified environments to rule out configuration issues:\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. Refer to the [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. Refer to the [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See the [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\nFor general questions, tips, or further assistance, you can also engage with our community:\n- [\ud83d\udcda Docs](https://docs.ultralytics.com/)\n- [\ud83c\udfa7 Discord](https://discord.com/invite/ultralytics)\n- [\ud83d\udcac Discourse](https://community.ultralytics.com/)\n- [\ud83d\udce2 Subreddit](https://reddit.com/r/Ultralytics)\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>  \nThis badge shows the status of our Continuous Integration (CI) tests. When green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests have passed, ensuring correct operation of YOLO across tasks and platforms.\n\nWe look forward to assisting you further and hope this helps clarify the issue. \ud83d\ude0a\nWhen does it occur?", "created_at": "2025-01-20T09:33:48Z"}
{"repo": "ultralytics/ultralytics", "pull_number": 18649, "instance_id": "ultralytics__ultralytics-18649", "issue_numbers": ["18646"], "base_commit": "9045d8feccd27667ae5ca3eba1063eca83e86693", "patch": "diff --git a/ultralytics/__init__.py b/ultralytics/__init__.py\nindex ec8851ed2f6..d96c5e83d14 100644\n--- a/ultralytics/__init__.py\n+++ b/ultralytics/__init__.py\n@@ -1,6 +1,6 @@\n # Ultralytics YOLO \ud83d\ude80, AGPL-3.0 license\n \n-__version__ = \"8.3.59\"\n+__version__ = \"8.3.60\"\n \n import os\n \ndiff --git a/ultralytics/nn/autobackend.py b/ultralytics/nn/autobackend.py\nindex 70962e8ed8c..a89c8cfa2f3 100644\n--- a/ultralytics/nn/autobackend.py\n+++ b/ultralytics/nn/autobackend.py\n@@ -617,10 +617,9 @@ def callback(request, userdata):\n                 # box = xywh2xyxy(y['coordinates'] * [[w, h, w, h]])  # xyxy pixels\n                 # conf, cls = y['confidence'].max(1), y['confidence'].argmax(1).astype(np.float32)\n                 # y = np.concatenate((box, conf.reshape(-1, 1), cls.reshape(-1, 1)), 1)\n-            elif len(y) == 1:  # classification model\n-                y = list(y.values())\n-            elif len(y) == 2:  # segmentation model\n-                y = list(reversed(y.values()))  # reversed for segmentation models (pred, proto)\n+            y = list(y.values())\n+            if len(y) == 2 and len(y[1].shape) != 4:  # segmentation model\n+                y = list(reversed(y))  # reversed for segmentation models (pred, proto)\n \n         # PaddlePaddle\n         elif self.paddle:\n", "test_patch": "", "problem_statement": "CoreML Yolov11 Seg models not predicting masks correctly?\n### Search before asking\n\n- [X] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\nExport\n\n### Bug\n\nHello\r\n\r\nUsing both latest release and git pip installs of Ultralytics, i have either prediction or subtle export errors exporting to coreML that cause predictions to fail.\r\n\r\nNote, I use the Yolo built in exporter, but then I annotate the ML Model with the supplied labels and annotate the mask output, allowing for preview in CoreML / XCode segmentation flows.\r\n\r\nI've ensured I use Pytorch 2.4.0 along with torchvision 0.19 which is noted as compatible with CoreML 8.1. I am not annotating NMS, which ive seen in other issues is unsupported.\r\n\r\nHow do I ensure masking output is correct?\r\n\r\nPredictions for Yolov11 L Seg in XCode using the export method is attached below:\r\n\r\n\r\n\r\n\n\n### Environment\n\n**Ultralytics 8.3.59 \ud83d\ude80 Python-3.12.0 torch-2.4.0 CPU (Apple M1 Max)\r\nSetup complete \u2705 (10 CPUs, 64.0 GB RAM, 1673.6/1858.2 GB disk)\r\n\r\nOS                  macOS-15.1.1-arm64-arm-64bit\r\nEnvironment         Darwin\r\nPython              3.12.0\r\nInstall             pip\r\nRAM                 64.00 GB\r\nDisk                1673.6/1858.2 GB\r\nCPU                 Apple M1 Max\r\nCPU count           10\r\nGPU                 None\r\nGPU count           None\r\nCUDA                None\r\n\r\nnumpy               \u2705 1.26.4>=1.23.0\r\nnumpy               \u2705 1.26.4<2.0.0; sys_platform == \"darwin\"\r\nmatplotlib          \u2705 3.10.0>=3.3.0\r\nopencv-python       \u2705 4.10.0.84>=4.6.0\r\npillow              \u2705 11.1.0>=7.1.2\r\npyyaml              \u2705 6.0.2>=5.3.1\r\nrequests            \u2705 2.32.3>=2.23.0\r\nscipy               \u2705 1.15.1>=1.4.1\r\ntorch               \u2705 2.4.0>=1.8.0\r\ntorch               \u2705 2.4.0!=2.4.0,>=1.8.0; sys_platform == \"win32\"\r\ntorchvision         \u2705 0.19.0>=0.9.0\r\ntqdm                \u2705 4.67.1>=4.64.0\r\npsutil              \u2705 6.1.1\r\npy-cpuinfo          \u2705 9.0.0\r\npandas              \u2705 2.2.3>=1.1.4\r\nseaborn             \u2705 0.13.2>=0.11.0\r\nultralytics-thop    \u2705 2.0.13>=2.0.0**\n\n### Minimal Reproducible Example\n\n```python\r\n#\r\n#  Yolov11-Seg-Convert.py\r\n#  v\r\n#\r\n#  Created by Anton Marini on 1/11/25.\r\n#\r\n\r\n\r\nfrom ultralytics import YOLO\r\nimport coremltools as ct\r\nimport json\r\n\r\n## Load the YOLO11 model\r\nmodel = YOLO(\"yolo11l-seg.pt\")\r\n\r\n# Export the model to CoreML format\r\nmodel.export(format=\"coreml\")# imgsz=640, int8=False, nms=False)  # creates 'yolo11n.mlpackage'\r\n#model.export(format=\"mlmodel\", imgsz=640, int8=True, nms=True)  # creates 'yolo11n.mlpackage'\r\n\r\n# Load the exported CoreML model\r\ncoreml_model = YOLO(\"yolo11l-seg.mlpackage\")\r\n\r\n# Run inference\r\n#results = coreml_model(\"https://ultralytics.com/images/bus.jpg\")\r\n\r\n# from https://apple.github.io/coremltools/docs-guides/source/convert-a-pytorch-xx-model.html\r\nmlmodel = ct.models.MLModel(\"yolo11l-seg.mlpackage\")\r\n\r\nlabels_json = {\"labels\": [\"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\", \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]}\r\n\r\nmlmodel.user_defined_metadata[\"com.apple.coreml.model.preview.type\"] = \"imageSegmenter\"\r\nmlmodel.user_defined_metadata[\"com.apple.coreml.model.preview.params\"] = json.dumps(labels_json)\r\n\r\nmlmodel.save(\"yolov8l-seg-updated.mlpackage\")\r\n```\n\n### Additional\n\nexport command output\r\n\r\n```\r\n(ultralytics) vade@MacBookPro ModelConversion % python Yolov11-Seg-Convert.py\r\nscikit-learn version 1.6.1 is not supported. Minimum required version: 0.17. Maximum required version: 1.5.1. Disabling scikit-learn conversion API.\r\nUltralytics 8.3.59 \ud83d\ude80 Python-3.12.0 torch-2.4.0 CPU (Apple M1 Max)\r\nYOLO11l-seg summary (fused): 491 layers, 27,646,272 parameters, 0 gradients, 142.2 GFLOPs\r\n\r\nPyTorch: starting from 'yolo11l-seg.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) ((1, 116, 8400), (1, 32, 160, 160)) (53.5 MB)\r\n\r\nCoreML: starting export with coremltools 8.1...\r\nTuple detected at graph output. This will be flattened in the converted model.\r\nConverting PyTorch Frontend ==> MIL Ops: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 1465/1467 [00:00<00:00, 7494.57 ops/s]\r\nRunning MIL frontend_pytorch pipeline: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 75.37 passes/s]\r\nRunning MIL default pipeline: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 89/89 [00:02<00:00, 37.42 passes/s]\r\nRunning MIL backend_mlprogram pipeline: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:00<00:00, 80.84 passes/s]\r\nCoreML: export success \u2705 8.6s, saved as 'yolo11l-seg.mlpackage' (53.1 MB)\r\n\r\nExport complete (9.4s)\r\nResults saved to /Users/vade/Documents/Repositories/v/v/ModelConversion\r\nPredict:         yolo predict task=segment model=yolo11l-seg.mlpackage imgsz=640  \r\nValidate:        yolo val task=segment model=yolo11l-seg.mlpackage imgsz=640 data=/ultralytics/ultralytics/cfg/datasets/coco.yaml  \r\nVisualize:       https://netron.app\r\n(ultralytics) vade@MacBookPro ModelConversion % \r\n\r\n```\r\n\r\n<img width=\"1939\" alt=\"image\" src=\"https://github.com/user-attachments/assets/6561b407-10e1-40ab-bf05-b933bb686bd5\" />\r\n\r\n\r\nThank you for any insight,\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!\n", "hints_text": "\ud83d\udc4b Hello @vade, thank you for your interest in Ultralytics \ud83d\ude80! We recommend a visit to the [Docs](https://docs.ultralytics.com) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a \ud83d\udc1b Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) to help us debug it. From your MRE, it seems you're exporting a YOLOv11-L Seg model to CoreML and encountering masking issues on the predictions. To help us further investigate, could you confirm:\n- The exact predictions you're obtaining versus expected results.\n- Any additional specific errors or logs (other than those already provided in your post).\n- Steps to replicate, including any changes you've made when processing annotations or adjusting the exported CoreML model.\n\nIf this is a \u2753 Question, providing additional insight or detailing the adjustments you are making to the exported model may help us or the community assist you faster.\n\nAdditionally, ensure you are using the latest version of the `ultralytics` package, dependencies, and environments. You can upgrade everything with:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO models may be run or exported in any of the following up-to-date verified environments (with all key dependencies preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Community and Support\n\nJoin our vibrant Ultralytics community for additional support:\n- [Discord](https://discord.com/invite/ultralytics) \ud83c\udfa7 for real-time discussions.\n- [Discourse](https://community.ultralytics.com) for in-depth questions.\n- [Subreddit](https://reddit.com/r/Ultralytics) for ideas and collaboration.\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are currently passing, meaning core functionality is verified daily on macOS, Windows, and Ubuntu.\n\nThis is an automated response to ensure you receive timely feedback \u23f3. An Ultralytics engineer will review your issue in detail and assist you further soon. Thank you for bringing this to our attention! \ud83d\ude0a\nDoes it work with Ultralytics?\n@Y-T-G \r\n\r\nIs there a way to validate results easily using the bus.jpg sample image using coreml as the runtime within the Yolo framework? Ive kept the original Yolo export as well as my variant with the CoreML annotations to allow preview of masks. New to Yolo so apologies if this is obvious. \r\n\r\nThanks!\nI tried with the following:\r\n\r\n```python\r\n\r\nfrom ultralytics import YOLO\r\nimport coremltools as ct\r\nimport json\r\n\r\n\r\n## Load the YOLO11 model\r\nmodel = YOLO(\"yolo11l-seg.pt\")\r\n\r\n# Load the exported CoreML model\r\ncoreml_model = YOLO(\"yolo11l-seg.mlpackage\")\r\n\r\n# Load the exported CoreML model\r\ncoreml_model_annotated = YOLO(\"yolo11l-seg-updated.mlpackage\")\r\n\r\n# Run inference\r\nresults = model(\"https://ultralytics.com/images/bus.jpg\")\r\nresults_coreml = coreml_model(\"https://ultralytics.com/images/bus.jpg\")\r\nresults_coreml_annotated = coreml_model_annotated(\"https://ultralytics.com/images/bus.jpg\")\r\n\r\nprint(results)\r\nprint(results_coreml)\r\nprint(results_coreml_annotated)\r\n\r\nif results == results_coreml:\r\n    print(\"coreml matches yolo\")\r\nelse:\r\n    print(\"coreml and yolo do not match\")\r\n\r\n\r\nif results == results_coreml_annotated:\r\n    print(\"coreml annotated matches yolo\")\r\nelse:\r\n    print(\"coreml annoted and yolo do not match\")\r\n```\r\n\r\nbut received:\r\n\r\n```\r\n\r\nFound https://ultralytics.com/images/bus.jpg locally at bus.jpg\r\nTraceback (most recent call last):\r\n  File \"/Users/vade/Documents/Repositories/v/v/ModelConversion/Yolov11-Seg-Validation.py\", line 25, in <module>\r\n    results_coreml = coreml_model(\"https://ultralytics.com/images/bus.jpg\")\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/vade/miniforge3/envs/ultralytics/lib/python3.12/site-packages/ultralytics/engine/model.py\", line 180, in __call__\r\n    return self.predict(source, stream, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/vade/miniforge3/envs/ultralytics/lib/python3.12/site-packages/ultralytics/engine/model.py\", line 558, in predict\r\n    return self.predictor.predict_cli(source=source) if is_cli else self.predictor(source=source, stream=stream)\r\n                                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/vade/miniforge3/envs/ultralytics/lib/python3.12/site-packages/ultralytics/engine/predictor.py\", line 173, in __call__\r\n    return list(self.stream_inference(source, model, *args, **kwargs))  # merge list of Result into one\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/vade/miniforge3/envs/ultralytics/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\r\n    response = gen.send(None)\r\n               ^^^^^^^^^^^^^^\r\n  File \"/Users/vade/miniforge3/envs/ultralytics/lib/python3.12/site-packages/ultralytics/engine/predictor.py\", line 266, in stream_inference\r\n    self.results = self.postprocess(preds, im, im0s)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/vade/miniforge3/envs/ultralytics/lib/python3.12/site-packages/ultralytics/models/yolo/segment/predict.py\", line 30, in postprocess\r\n    p = ops.non_max_suppression(\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/vade/miniforge3/envs/ultralytics/lib/python3.12/site-packages/ultralytics/utils/ops.py\", line 249, in non_max_suppression\r\n    output = [torch.zeros((0, 6 + nm), device=prediction.device)] * bs\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: Trying to create tensor with negative dimension -46: [0, -46]\r\n```\r\n\r\n\nCheck the warning\nRunning the same test with the standard Yolo models (non segmented) with and a standard conversion, I get:\r\n\r\n```\r\n(ultralytics) vade@MacBookPro ModelConversion % python Yolov11-Seg-Validation.py\r\nscikit-learn version 1.6.1 is not supported. Minimum required version: 0.17. Maximum required version: 1.5.1. Disabling scikit-learn conversion API.\r\nWARNING \u26a0\ufe0f Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\r\n\r\nFound https://ultralytics.com/images/bus.jpg locally at bus.jpg\r\nimage 1/1 /Users/vade/Documents/Repositories/v/v/ModelConversion/bus.jpg: 640x480 4 persons, 1 bus, 58.7ms\r\nSpeed: 2.0ms preprocess, 58.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\r\n[ultralytics.engine.results.Results object with attributes:\r\n\r\nboxes: ultralytics.engine.results.Boxes object\r\nkeypoints: None\r\nmasks: None\r\nnames: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\r\nobb: None\r\norig_img: array([[[119, 146, 172],\r\n        [121, 148, 174],\r\n        [122, 152, 177],\r\n        ...,\r\n        [161, 171, 188],\r\n        [160, 170, 187],\r\n        [160, 170, 187]],\r\n\r\n       [[120, 147, 173],\r\n        [122, 149, 175],\r\n        [123, 153, 178],\r\n        ...,\r\n        [161, 171, 188],\r\n        [160, 170, 187],\r\n        [160, 170, 187]],\r\n\r\n       [[123, 150, 176],\r\n        [124, 151, 177],\r\n        [125, 155, 180],\r\n        ...,\r\n        [161, 171, 188],\r\n        [160, 170, 187],\r\n        [160, 170, 187]],\r\n\r\n       ...,\r\n\r\n       [[183, 182, 186],\r\n        [179, 178, 182],\r\n        [180, 179, 183],\r\n        ...,\r\n        [121, 111, 117],\r\n        [113, 103, 109],\r\n        [115, 105, 111]],\r\n\r\n       [[165, 164, 168],\r\n        [173, 172, 176],\r\n        [187, 186, 190],\r\n        ...,\r\n        [102,  92,  98],\r\n        [101,  91,  97],\r\n        [103,  93,  99]],\r\n\r\n       [[123, 122, 126],\r\n        [145, 144, 148],\r\n        [176, 175, 179],\r\n        ...,\r\n        [ 95,  85,  91],\r\n        [ 96,  86,  92],\r\n        [ 98,  88,  94]]], dtype=uint8)\r\norig_shape: (1080, 810)\r\npath: '/Users/vade/Documents/Repositories/v/v/ModelConversion/bus.jpg'\r\nprobs: None\r\nsave_dir: 'runs/detect/predict'\r\nspeed: {'preprocess': 2.001047134399414, 'inference': 58.69293212890625, 'postprocess': 0.46706199645996094}]\r\nLoading yolo11s.mlpackage for CoreML inference...\r\n\r\nFound https://ultralytics.com/images/bus.jpg locally at bus.jpg\r\nimage 1/1 /Users/vade/Documents/Repositories/v/v/ModelConversion/bus.jpg: 640x640 4 persons, 1 bus, 47.0ms\r\nSpeed: 1.6ms preprocess, 47.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\r\n[ultralytics.engine.results.Results object with attributes:\r\n\r\nboxes: ultralytics.engine.results.Boxes object\r\nkeypoints: None\r\nmasks: None\r\nnames: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\r\nobb: None\r\norig_img: array([[[119, 146, 172],\r\n        [121, 148, 174],\r\n        [122, 152, 177],\r\n        ...,\r\n        [161, 171, 188],\r\n        [160, 170, 187],\r\n        [160, 170, 187]],\r\n\r\n       [[120, 147, 173],\r\n        [122, 149, 175],\r\n        [123, 153, 178],\r\n        ...,\r\n        [161, 171, 188],\r\n        [160, 170, 187],\r\n        [160, 170, 187]],\r\n\r\n       [[123, 150, 176],\r\n        [124, 151, 177],\r\n        [125, 155, 180],\r\n        ...,\r\n        [161, 171, 188],\r\n        [160, 170, 187],\r\n        [160, 170, 187]],\r\n\r\n       ...,\r\n\r\n       [[183, 182, 186],\r\n        [179, 178, 182],\r\n        [180, 179, 183],\r\n        ...,\r\n        [121, 111, 117],\r\n        [113, 103, 109],\r\n        [115, 105, 111]],\r\n\r\n       [[165, 164, 168],\r\n        [173, 172, 176],\r\n        [187, 186, 190],\r\n        ...,\r\n        [102,  92,  98],\r\n        [101,  91,  97],\r\n        [103,  93,  99]],\r\n\r\n       [[123, 122, 126],\r\n        [145, 144, 148],\r\n        [176, 175, 179],\r\n        ...,\r\n        [ 95,  85,  91],\r\n        [ 96,  86,  92],\r\n        [ 98,  88,  94]]], dtype=uint8)\r\norig_shape: (1080, 810)\r\npath: '/Users/vade/Documents/Repositories/v/v/ModelConversion/bus.jpg'\r\nprobs: None\r\nsave_dir: 'runs/detect/predict'\r\nspeed: {'preprocess': 1.6429424285888672, 'inference': 46.9970703125, 'postprocess': 0.6887912750244141}]\r\ncoreml and yolo do not match\r\n```\n> Check the warning\r\n\r\nHi - thanks for the quick reply!\r\n\r\nAre you referring to the SciKit warning, or some other? Trying with ,`task=\"segment\"` makes no difference for the CoreML execution path:\r\n\r\n```\r\n(ultralytics) vade@MacBookPro ModelConversion % python Yolov11-Seg-Validation.py\r\nscikit-learn version 1.6.1 is not supported. Minimum required version: 0.17. Maximum required version: 1.5.1. Disabling scikit-learn conversion API.\r\n\r\nFound https://ultralytics.com/images/bus.jpg locally at bus.jpg\r\nimage 1/1 /Users/vade/Documents/Repositories/v/v/ModelConversion/bus.jpg: 640x480 4 persons, 1 bus, 87.2ms\r\nSpeed: 2.0ms preprocess, 87.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\r\n[ultralytics.engine.results.Results object with attributes:\r\n\r\nboxes: ultralytics.engine.results.Boxes object\r\nkeypoints: None\r\nmasks: ultralytics.engine.results.Masks object\r\nnames: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\r\nobb: None\r\norig_img: array([[[119, 146, 172],\r\n        [121, 148, 174],\r\n        [122, 152, 177],\r\n        ...,\r\n        [161, 171, 188],\r\n        [160, 170, 187],\r\n        [160, 170, 187]],\r\n\r\n       [[120, 147, 173],\r\n        [122, 149, 175],\r\n        [123, 153, 178],\r\n        ...,\r\n        [161, 171, 188],\r\n        [160, 170, 187],\r\n        [160, 170, 187]],\r\n\r\n       [[123, 150, 176],\r\n        [124, 151, 177],\r\n        [125, 155, 180],\r\n        ...,\r\n        [161, 171, 188],\r\n        [160, 170, 187],\r\n        [160, 170, 187]],\r\n\r\n       ...,\r\n\r\n       [[183, 182, 186],\r\n        [179, 178, 182],\r\n        [180, 179, 183],\r\n        ...,\r\n        [121, 111, 117],\r\n        [113, 103, 109],\r\n        [115, 105, 111]],\r\n\r\n       [[165, 164, 168],\r\n        [173, 172, 176],\r\n        [187, 186, 190],\r\n        ...,\r\n        [102,  92,  98],\r\n        [101,  91,  97],\r\n        [103,  93,  99]],\r\n\r\n       [[123, 122, 126],\r\n        [145, 144, 148],\r\n        [176, 175, 179],\r\n        ...,\r\n        [ 95,  85,  91],\r\n        [ 96,  86,  92],\r\n        [ 98,  88,  94]]], dtype=uint8)\r\norig_shape: (1080, 810)\r\npath: '/Users/vade/Documents/Repositories/v/v/ModelConversion/bus.jpg'\r\nprobs: None\r\nsave_dir: 'runs/segment/predict'\r\nspeed: {'preprocess': 2.0170211791992188, 'inference': 87.2199535369873, 'postprocess': 1.4388561248779297}]\r\nLoading yolo11l-seg.mlpackage for CoreML inference...\r\n\r\nFound https://ultralytics.com/images/bus.jpg locally at bus.jpg\r\nTraceback (most recent call last):\r\n  File \"/Users/vade/Documents/Repositories/v/v/ModelConversion/Yolov11-Seg-Validation.py\", line 27, in <module>\r\n    results_coreml = coreml_model(\"https://ultralytics.com/images/bus.jpg\")\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/vade/miniforge3/envs/ultralytics/lib/python3.12/site-packages/ultralytics/engine/model.py\", line 180, in __call__\r\n    return self.predict(source, stream, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/vade/miniforge3/envs/ultralytics/lib/python3.12/site-packages/ultralytics/engine/model.py\", line 558, in predict\r\n    return self.predictor.predict_cli(source=source) if is_cli else self.predictor(source=source, stream=stream)\r\n                                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/vade/miniforge3/envs/ultralytics/lib/python3.12/site-packages/ultralytics/engine/predictor.py\", line 173, in __call__\r\n    return list(self.stream_inference(source, model, *args, **kwargs))  # merge list of Result into one\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/vade/miniforge3/envs/ultralytics/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\r\n    response = gen.send(None)\r\n               ^^^^^^^^^^^^^^\r\n  File \"/Users/vade/miniforge3/envs/ultralytics/lib/python3.12/site-packages/ultralytics/engine/predictor.py\", line 266, in stream_inference\r\n    self.results = self.postprocess(preds, im, im0s)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/vade/miniforge3/envs/ultralytics/lib/python3.12/site-packages/ultralytics/models/yolo/segment/predict.py\", line 30, in postprocess\r\n    p = ops.non_max_suppression(\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/vade/miniforge3/envs/ultralytics/lib/python3.12/site-packages/ultralytics/utils/ops.py\", line 249, in non_max_suppression\r\n    output = [torch.zeros((0, 6 + nm), device=prediction.device)] * bs\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: Trying to create tensor with negative dimension -46: [0, -46]\r\n\r\n```\nFixing the SciKit learn version removes the Scikit warning, but i have the same errors running Yolo / Ultralytics prediction via the MLPackage models (the standard export as well as mine)\nWhat's the output of `print(coreml_model.names)`?\n\r\n```python\r\nmodel = YOLO(\"yolo11l-seg.pt\")\r\nmodel.export(format=\"coreml\") #creates 'yolo11l-seg.mlpackage'\r\ncoreml_model = YOLO(\"yolo11l-seg.mlpackage\")\r\nprint(coreml_model.names)\r\n```\r\n\r\n```\r\n{0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\r\n```\nWhat's the output of:\n```python\nresults = coreml_model(\"bus.jpg\", embed=[-1])\nprint(results[0].shape, results[-1].shape)\n```\nThanks for all the help!\r\n\r\n```python\r\nfrom ultralytics import YOLO\r\nimport coremltools as ct\r\nimport json\r\n\r\ncoreml_model = YOLO(\"yolo11l-seg.mlpackage\", task=\"segment\")\r\nresults_coreml = coreml_model(\"https://ultralytics.com/images/bus.jpg\",  embed=[-1])\r\nprint(results_coreml[0].shape, results_coreml[-1].shape)\r\n```\r\n\r\n```\r\nLoading yolo11l-seg.mlpackage for CoreML inference...\r\n\r\nFound https://ultralytics.com/images/bus.jpg locally at bus.jpg\r\ntorch.Size([1, 32, 160, 160]) torch.Size([1, 116, 8400])\r\n```", "created_at": "2025-01-12T23:10:04Z"}
{"repo": "ultralytics/ultralytics", "pull_number": 18579, "instance_id": "ultralytics__ultralytics-18579", "issue_numbers": ["18539"], "base_commit": "02e4e984a1cc3e89d305410617db02590c20a83b", "patch": "diff --git a/ultralytics/utils/metrics.py b/ultralytics/utils/metrics.py\nindex 01d5e6280c1..4772f9b39ba 100644\n--- a/ultralytics/utils/metrics.py\n+++ b/ultralytics/utils/metrics.py\n@@ -73,11 +73,16 @@ def box_iou(box1, box2, eps=1e-7):\n \n def bbox_iou(box1, box2, xywh=True, GIoU=False, DIoU=False, CIoU=False, eps=1e-7):\n     \"\"\"\n-    Calculate Intersection over Union (IoU) of box1(1, 4) to box2(n, 4).\n+    Calculates the Intersection over Union (IoU) between bounding boxes.\n+\n+    This function supports various shapes for `box1` and `box2` as long as the last dimension is 4.\n+    For instance, you may pass tensors shaped like (4,), (N, 4), (B, N, 4), or (B, N, 1, 4).\n+    Internally, the code will split the last dimension into (x, y, w, h) if `xywh=True`,\n+    or (x1, y1, x2, y2) if `xywh=False`.\n \n     Args:\n-        box1 (torch.Tensor): A tensor representing a single bounding box with shape (1, 4).\n-        box2 (torch.Tensor): A tensor representing n bounding boxes with shape (n, 4).\n+        box1 (torch.Tensor): A tensor representing one or more bounding boxes, with the last dimension being 4.\n+        box2 (torch.Tensor): A tensor representing one or more bounding boxes, with the last dimension being 4.\n         xywh (bool, optional): If True, input boxes are in (x, y, w, h) format. If False, input boxes are in\n                                (x1, y1, x2, y2) format. Defaults to True.\n         GIoU (bool, optional): If True, calculate Generalized IoU. Defaults to False.\n", "test_patch": "", "problem_statement": "Misleading documentation in bbox_iou function for bounding box input\n### Search before asking\n\n- [X] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\nOther\n\n### Bug\n\nHi\r\n\r\nI noticed that the docstring in the `bbox_iou` function appears to contain an incorrect description of the box1 argument. Specifically, the docstring says:\r\n\r\nhttps://github.com/ultralytics/ultralytics/blob/00aefd795c13bb6a5364b63798c383adc15248d4/ultralytics/utils/metrics.py#L79\r\n\r\nHowever, this doesn't align with the actual implementation. The function supports a batch of bounding boxes for `box1`, and `box1` can have a shape of `(n, 4)`, where `n >= 1.`\r\n\r\nFor example, this call works as expected:\r\n```python\r\nbox1 = torch.tensor([[0.5, 0.5, 1.0, 1.0], [0.3, 0.3, 0.8, 0.8]])  # Shape (2, 4)\r\nbox2 = torch.tensor([[0.4, 0.4, 0.9, 0.9]])  # Shape (1, 4)\r\niou = bbox_iou(box1, box2, xywh=False)\r\nprint(iou)  # Outputs IoU values for both bounding boxes in `box1`\r\n```\r\nPlus, \r\nthis is confirmed in the `TaskAlignedAssigner` class in `TAL.py`, where the `_forward` method uses `gt_bboxes` (which corresponds to `box1` in `bbox_iou`) with shapes like `(bs, max_num_object, 4)`\u2014clearly indicating that box1 can hold multiple bounding boxes:\r\n\r\nhttps://github.com/ultralytics/ultralytics/blob/00aefd795c13bb6a5364b63798c383adc15248d4/ultralytics/utils/tal.py#L155\r\n\r\n\r\nIf I\u2019ve misunderstood anything, please let me know. Thanks again for the amazing work you\u2019re doing!\r\n\n\n### Environment\n\nUltralytics 8.3.56 \ud83d\ude80 Python-3.10.12 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\r\nSetup complete \u2705 (12 CPUs, 31.2 GB RAM, 39.1/250.9 GB disk)\r\n\r\nOS                  Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\r\nEnvironment         Linux\r\nPython              3.10.12\r\nInstall             pip\r\nRAM                 31.24 GB\r\nDisk                39.1/250.9 GB\r\nCPU                 12th Gen Intel Core(TM) i5-12600\r\nCPU count           12\r\nGPU                 NVIDIA GeForce RTX 3060, 12288MiB\r\nGPU count           1\r\nCUDA                12.4\r\n\r\nnumpy               \u2705 1.26.4>=1.23.0\r\nnumpy               \u2705 1.26.4<2.0.0; sys_platform == \"darwin\"\r\nmatplotlib          \u2705 3.8.3>=3.3.0\r\nopencv-python       \u2705 4.10.0.84>=4.6.0\r\npillow              \u2705 10.2.0>=7.1.2\r\npyyaml              \u2705 5.4.1>=5.3.1\r\nrequests            \u2705 2.32.3>=2.23.0\r\nscipy               \u2705 1.14.1>=1.4.1\r\ntorch               \u2705 2.5.1>=1.8.0\r\ntorch               \u2705 2.5.1!=2.4.0,>=1.8.0; sys_platform == \"win32\"\r\ntorchvision         \u2705 0.20.1>=0.9.0\r\ntqdm                \u2705 4.66.2>=4.64.0\r\npsutil              \u2705 6.0.0\r\npy-cpuinfo          \u2705 9.0.0\r\npandas              \u2705 2.2.3>=1.1.4\r\nseaborn             \u2705 0.13.2>=0.11.0\r\nultralytics-thop    \u2705 2.0.13>=2.0.0\r\n{'OS': 'Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'Environment': 'Linux', 'Python': '3.10.12', 'Install': 'pip', 'RAM': '31.24 GB', 'Disk': '39.1/250.9 GB', 'CPU': '12th Gen Intel Core(TM) i5-12600', 'CPU count': 12, 'GPU': 'NVIDIA GeForce RTX 3060, 12288MiB', 'GPU count': 1, 'CUDA': '12.4', 'Package Info': {'numpy': '\u2705 1.26.4<2.0.0; sys_platform == \"darwin\"', 'matplotlib': '\u2705 3.8.3>=3.3.0', 'opencv-python': '\u2705 4.10.0.84>=4.6.0', 'pillow': '\u2705 10.2.0>=7.1.2', 'pyyaml': '\u2705 5.4.1>=5.3.1', 'requests': '\u2705 2.32.3>=2.23.0', 'scipy': '\u2705 1.14.1>=1.4.1', 'torch': '\u2705 2.5.1!=2.4.0,>=1.8.0; sys_platform == \"win32\"', 'torchvision': '\u2705 0.20.1>=0.9.0', 'tqdm': '\u2705 4.66.2>=4.64.0', 'psutil': '\u2705 6.0.0', 'py-cpuinfo': '\u2705 9.0.0', 'pandas': '\u2705 2.2.3>=1.1.4', 'seaborn': '\u2705 0.13.2>=0.11.0', 'ultralytics-thop': '\u2705 2.0.13>=2.0.0'}}\n\n### Minimal Reproducible Example\n\n```python\r\nimport torch \r\nfrom .metrics import bbox_iou\r\n\r\nbox1 = torch.tensor([[0.5, 0.5, 1.0, 1.0], [0.3, 0.3, 0.8, 0.8]])  # Shape (2, 4)\r\nbox2 = torch.tensor([[0.4, 0.4, 0.9, 0.9]])  # Shape (1, 4)\r\niou = bbox_iou(box1, box2, xywh=False)\r\nprint(iou)  # Outputs IoU values for both bounding boxes in `box1`\r\n```\n\n### Additional\n\nno\n\n### Are you willing to submit a PR?\n\n- [X] Yes I'd like to help by submitting a PR!\n", "hints_text": "\ud83d\udc4b Hello @visionNoob, thank you for your interest in Ultralytics \ud83d\ude80! We appreciate you taking the time to report this issue and dig into the specifics of the `bbox_iou` function. Your detailed explanation and example are super helpful! \ud83d\udd0e\n\nWe recommend a visit to the [Docs](https://docs.ultralytics.com), where you might find relevant information or clarifications. For future reference, if this issue is related to a \ud83d\udc1b bug, please ensure you provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/), which you've already done an excellent job of including here\u2014thank you!\n\nIf this is a \u2753 documentation-related query and you\u2019re willing to help with improvements, your offer to submit a PR is very welcome! Please ensure you follow our [Contribution Guide](https://github.com/ultralytics/ultralytics/blob/main/CONTRIBUTING.md) for submitting changes smoothly.\n\n### Upgrade \n\nJust a quick note to ensure you're running the latest version of Ultralytics and its dependencies for consistency across implementations and verification. You can upgrade using:\n```bash\npip install -U ultralytics\n```\n\n### Community\n\nYou can also explore our Ultralytics community to share knowledge or seek additional support:\n- [Discord](https://discord.com/invite/ultralytics) \ud83c\udfa7 for real-time support and chats \n- [Discourse](https://community.ultralytics.com) for in-depth discussions \n- [Subreddit](https://reddit.com/r/Ultralytics) to interact with the larger Ultralytics community\n\n### Environments \n\nYou may want to test your work in our verified environments to ensure consistency:\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n  \n### Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is \ud83d\udc9a green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are passing. CI tests verify the operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) across macOS, Windows, and Ubuntu.\n\nThis is an automated response\u2014an Ultralytics engineer will be with you shortly to assist further \ud83d\ude0a. Thank you for contributing!\nYou can open a PR to update the docstring ", "created_at": "2025-01-08T01:40:01Z"}
{"repo": "ultralytics/ultralytics", "pull_number": 18576, "instance_id": "ultralytics__ultralytics-18576", "issue_numbers": ["18571"], "base_commit": "47759012247d9dcf2385661b0d16a6fd791974c9", "patch": "diff --git a/ultralytics/data/converter.py b/ultralytics/data/converter.py\nindex d3d3d706216..21ab0a952f1 100644\n--- a/ultralytics/data/converter.py\n+++ b/ultralytics/data/converter.py\n@@ -377,7 +377,7 @@ def convert_segment_masks_to_yolo_seg(masks_dir, output_dir, classes):\n     \"\"\"\n     pixel_to_class_mapping = {i + 1: i for i in range(classes)}\n     for mask_path in Path(masks_dir).iterdir():\n-        if mask_path.suffix == \".png\":\n+        if mask_path.suffix in {\".png\", \".jpg\"}:\n             mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)  # Read the mask image in grayscale\n             img_height, img_width = mask.shape  # Get image dimensions\n             LOGGER.info(f\"Processing {mask_path} imgsz = {img_height} x {img_width}\")\n", "test_patch": "", "problem_statement": "convert_segment_masks_to_yolo_seg jpg not included\n### Search before asking\n\n- [X] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\nOther\n\n### Bug\n\n# convert_segment_masks_to_yolo_seg - Supports only png\r\n\r\n## Function\r\n\r\n `ultralytics.data.converter.convert_segment_masks_to_yolo_seg`\r\n\r\n## Description\r\n\r\nThe docs say that png and jpg are supported but from the code snippet below (first lines of the function source code without comments) we see only .png will work.\r\nThe function loops over all mask files and, if it is a .png, it creates the .txt corresponding file.\r\nSo, the .jpg files are ignored.  \r\n\r\n\r\n\n\n### Environment\n\nUltralytics 8.3.51 \ud83d\ude80 Python-3.11.7 torch-2.5.1+cpu CPU (13th Gen Intel Core(TM) i7-1355U)\r\nSetup complete \u2705 (12 CPUs, 15.7 GB RAM, 293.8/332.0 GB disk)\n\n### Minimal Reproducible Example\n\ndef convert_segment_masks_to_yolo_seg(masks_dir, output_dir, classes):\r\n    pixel_to_class_mapping = {i + 1: i for i in range(classes)}\r\n    for mask_path in Path(masks_dir).iterdir():\r\n        if mask_path.suffix == \".png\":\r\n...\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!\n", "hints_text": "\ud83d\udc4b Hello @acoletti-33100, thank you for bringing this to our attention and for your detailed report \ud83d\ude80! \n\nWe recommend checking out the [Docs](https://docs.ultralytics.com) for details about the `ultralytics` library, and specifically for related conversion workflows. The specific function you\u2019ve mentioned, `convert_segment_masks_to_yolo_seg`, may also be outlined there. Additionally, the [Python Usage section](https://docs.ultralytics.com/usage/python/) contains many helpful code examples.\n\nFor \ud83d\udc1b Bug Reports like this, it\u2019s helpful to provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) when possible. If you've already given the function snippet and environment where this is happening, that\u2019s a solid starting point. Could you expand with input files and the full step-by-step process to replicate the issue? This will help us debug more effectively and ensure clarity.\n\nTo ensure your environment is set up correctly and not affected by prior releases, please upgrade to the latest version of `ultralytics`. Ensure you're in a [Python>=3.8](https://www.python.org/) environment with the latest [PyTorch>=1.8](https://pytorch.org/get-started/locally/) installed:\n\n```bash\npip install -U ultralytics\n```\n\nIf you\u2019re considering adding support for `.jpg` files or adjusting functionality, feel free to share ideas with the community or even raise a Pull Request. At Ultralytics, we're keen on user-driven feedback and contributions \ud83d\udca1.\n\n## Environments\n\nYOLO can be used in the following verified setups:\n\n- **Notebooks** with GPU accelerations: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM (see [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/))\n- **Amazon** Deep Learning AMI (see [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/))\n- **Docker Image** (see [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/)) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green \u2705, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are currently passing. CI tests validate all core YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) across platforms like macOS, Windows, and Ubuntu.\n\nFeel free to join our community for more insights or discussions:\n- [Discord](https://discord.com/invite/ultralytics) \ud83c\udfa7 for real-time chat\n- [Discourse](https://community.ultralytics.com) for broader discussions\n- [Subreddit](https://reddit.com/r/Ultralytics) for community sharing\n\nThis is an automated response to assist as quickly as possible. Rest assured that an Ultralytics engineer will review the issue soon and provide direct assistance \ud83d\ude0a.", "created_at": "2025-01-07T19:27:38Z"}
{"repo": "ultralytics/ultralytics", "pull_number": 18546, "instance_id": "ultralytics__ultralytics-18546", "issue_numbers": ["18543"], "base_commit": "00aefd795c13bb6a5364b63798c383adc15248d4", "patch": "diff --git a/docs/en/integrations/weights-biases.md b/docs/en/integrations/weights-biases.md\nindex e1f5eff18ee..45c74dc86ba 100644\n--- a/docs/en/integrations/weights-biases.md\n+++ b/docs/en/integrations/weights-biases.md\n@@ -44,6 +44,9 @@ To install the required packages, run:\n         ```bash\n         # Install the required packages for Ultralytics YOLO and Weights & Biases\n         pip install -U ultralytics wandb\n+\n+        # Enable W&B logging for Ultralytics\n+        yolo settings wandb=True\n         ```\n \n For detailed instructions and best practices related to the installation process, be sure to check our [YOLO11 Installation guide](../quickstart.md). While installing the required packages for YOLO11, if you encounter any difficulties, consult our [Common Issues guide](../guides/yolo-common-issues.md) for solutions and tips.\n@@ -108,16 +111,16 @@ Before diving into the usage instructions for YOLO11 model training with Weights\n \n !!! tip \"Enable or Disable Weights & Biases\"\n \n-    If you want to enable or disable Weights & Biases logging, you can use the `wandb` command. By default, Weights & Biases logging is enabled.\n+    If you want to enable or disable Weights & Biases logging in Ultralytics, you can use the `yolo settings` command. By default, Weights & Biases logging is disabled.\n \n     === \"CLI\"\n \n         ```bash\n         # Enable Weights & Biases logging\n-        wandb enabled\n+        yolo settings wandb=True\n \n         # Disable Weights & Biases logging\n-        wandb disabled\n+        yolo settings wandb=False\n         ```\n \n ### Understanding the Output\n@@ -170,6 +173,7 @@ To integrate Weights & Biases with Ultralytics YOLO11:\n \n     ```bash\n     pip install -U ultralytics wandb\n+    yolo settings wandb=True\n     ```\n \n 2. Log in to your Weights & Biases account:\n@@ -220,13 +224,13 @@ The dashboard offers insights into your model's training process, allowing you t\n Yes, you can disable W&B logging using the following command:\n \n ```bash\n-wandb disabled\n+yolo settings wandb=True\n ```\n \n To re-enable logging, use:\n \n ```bash\n-wandb enabled\n+yolo settings wandb=False\n ```\n \n This allows you to control when you want to use W&B logging without modifying your training scripts.\n", "test_patch": "", "problem_statement": "can't train from scratch\n### Search before asking\n\n- [X] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\nTrain\n\n### Bug\n\nI keep getting this error:\r\n\r\n> TypeError: model='yolo11m.yaml' is not a supported model format. Ultralytics supports: ('PyTorch', 'TorchScript', 'ONNX', 'OpenVINO', 'TensorRT', 'CoreML', 'TensorFlow SavedModel', 'TensorFlow GraphDef', 'TensorFlow Lite', 'TensorFlow Edge TPU', 'TensorFlow.js', 'PaddlePaddle', 'MNN', 'NCNN', 'IMX')\r\nSee https://docs.ultralytics.com/modes/predict for help.\r\n\r\nWhen I try to train a yolov11 model from scratch following the offical doc:\r\n```\r\nfrom ultralytics import YOLO\r\n# Load a model\r\nmodel = YOLO(\"yolo11m.yaml\")  # build a new model from YAML\r\nresults = model.train(data=\"wider_face.yaml\", project=\"from_scratch\", name=\"yolov11m\", pretrained=False,\r\n                      epochs=100, batch=32, imgsz=640, deterministic=False, degrees=90)\r\n```\r\n no matter I add the pretrained=False para or not. \r\nAnd it happens right after the validation after the first training epoch.\n\n### Environment\n\nUltralytics 8.3.58 \ud83d\ude80 Python-3.11.10 torch-2.5.1+cu124 CUDA:0 (NVIDIA RTX A6000, 48577MiB)\r\nSetup complete \u2705 (64 CPUs, 503.8 GB RAM, 4.6/16.0 GB disk)\r\n\r\nOS                  Linux-5.15.0-122-generic-x86_64-with-glibc2.35\r\nEnvironment         Docker\r\nPython              3.11.10\r\nInstall             pip\r\nRAM                 503.76 GB\r\nDisk                4.6/16.0 GB\r\nCPU                 Intel Xeon E5-2683 v4 2.10GHz\r\nCPU count           64\r\nGPU                 NVIDIA RTX A6000, 48577MiB\r\nGPU count           1\r\nCUDA                12.4\r\n\r\nnumpy               \u2705 2.1.2>=1.23.0\r\nnumpy               \u2705 2.1.2<2.0.0; sys_platform == \"darwin\"\r\nmatplotlib          \u2705 3.10.0>=3.3.0\r\nopencv-python       \u2705 4.10.0.84>=4.6.0\r\npillow              \u2705 10.2.0>=7.1.2\r\npyyaml              \u2705 6.0.2>=5.3.1\r\nrequests            \u2705 2.32.3>=2.23.0\r\nscipy               \u2705 1.15.0>=1.4.1\r\ntorch               \u2705 2.5.1+cu124>=1.8.0\r\ntorch               \u2705 2.5.1+cu124!=2.4.0,>=1.8.0; sys_platform == \"win32\"\r\ntorchvision         \u2705 0.20.1+cu124>=0.9.0\r\ntqdm                \u2705 4.66.5>=4.64.0\r\npsutil              \u2705 6.1.0\r\npy-cpuinfo          \u2705 9.0.0\r\npandas              \u2705 2.2.3>=1.1.4\r\nseaborn             \u2705 0.13.2>=0.11.0\r\nultralytics-thop    \u2705 2.0.13>=2.0.0\n\n### Minimal Reproducible Example\n\n```\r\nfrom ultralytics import YOLO\r\n# Load a model\r\nmodel = YOLO(\"yolo11m.yaml\")  # build a new model from YAML\r\nresults = model.train(data=\"wider_face.yaml\", project=\"from_scratch\", name=\"yolov11m\", pretrained=False,\r\n                      epochs=100, batch=32, imgsz=640, deterministic=False, degrees=90)\r\n```\n\n### Additional\n\nThe full error message:\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[6], line 11\r\n      8 # Add WandB callback for logging\r\n      9 add_wandb_callback(model)\r\n---> 11 results = model.train(data=\"wider_face.yaml\", project=\"from_scratch\", name=\"yolov11m\", pretrained=False,\r\n     12                       epochs=100, batch=32, imgsz=640, deterministic=False, degrees=90)\r\n     14 # Mark the run as finished\r\n     15 wandb.finish()\r\n\r\nFile [/opt/conda/lib/python3.11/site-packages/ultralytics/engine/model.py:806](https://209.137.198.14:44227/lab/tree/workspace/opt/conda/lib/python3.11/site-packages/ultralytics/engine/model.py#line=805), in Model.train(self, trainer, **kwargs)\r\n    803     self.model = self.trainer.model\r\n    805 self.trainer.hub_session = self.session  # attach optional HUB session\r\n--> 806 self.trainer.train()\r\n    807 # Update model and cfg after training\r\n    808 if RANK in {-1, 0}:\r\n\r\nFile [/opt/conda/lib/python3.11/site-packages/ultralytics/engine/trainer.py:207](https://209.137.198.14:44227/lab/tree/workspace/opt/conda/lib/python3.11/site-packages/ultralytics/engine/trainer.py#line=206), in BaseTrainer.train(self)\r\n    204         ddp_cleanup(self, str(file))\r\n    206 else:\r\n--> 207     self._do_train(world_size)\r\n\r\nFile [/opt/conda/lib/python3.11/site-packages/ultralytics/engine/trainer.py:432](https://209.137.198.14:44227/lab/tree/workspace/opt/conda/lib/python3.11/site-packages/ultralytics/engine/trainer.py#line=431), in BaseTrainer._do_train(self, world_size)\r\n    430 # Validation\r\n    431 if self.args.val or final_epoch or self.stopper.possible_stop or self.stop:\r\n--> 432     self.metrics, self.fitness = self.validate()\r\n    433 self.save_metrics(metrics={**self.label_loss_items(self.tloss), **self.metrics, **self.lr})\r\n    434 self.stop |= self.stopper(epoch + 1, self.fitness) or final_epoch\r\n\r\nFile [/opt/conda/lib/python3.11/site-packages/ultralytics/engine/trainer.py:605](https://209.137.198.14:44227/lab/tree/workspace/opt/conda/lib/python3.11/site-packages/ultralytics/engine/trainer.py#line=604), in BaseTrainer.validate(self)\r\n    599 def validate(self):\r\n    600     \"\"\"\r\n    601     Runs validation on test set using self.validator.\r\n    602 \r\n    603     The returned dict is expected to contain \"fitness\" key.\r\n    604     \"\"\"\r\n--> 605     metrics = self.validator(self)\r\n    606     fitness = metrics.pop(\"fitness\", -self.loss.detach().cpu().numpy())  # use loss as fitness measure if not found\r\n    607     if not self.best_fitness or self.best_fitness < fitness:\r\n\r\nFile [/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py:116](https://209.137.198.14:44227/lab/tree/workspace/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py#line=115), in context_decorator.<locals>.decorate_context(*args, **kwargs)\r\n    113 @functools.wraps(func)\r\n    114 def decorate_context(*args, **kwargs):\r\n    115     with ctx_factory():\r\n--> 116         return func(*args, **kwargs)\r\n\r\nFile [/opt/conda/lib/python3.11/site-packages/ultralytics/engine/validator.py:202](https://209.137.198.14:44227/lab/tree/workspace/opt/conda/lib/python3.11/site-packages/ultralytics/engine/validator.py#line=201), in BaseValidator.__call__(self, trainer, model)\r\n    200 self.finalize_metrics()\r\n    201 self.print_results()\r\n--> 202 self.run_callbacks(\"on_val_end\")\r\n    203 if self.training:\r\n    204     model.float()\r\n\r\nFile [/opt/conda/lib/python3.11/site-packages/ultralytics/engine/validator.py:271](https://209.137.198.14:44227/lab/tree/workspace/opt/conda/lib/python3.11/site-packages/ultralytics/engine/validator.py#line=270), in BaseValidator.run_callbacks(self, event)\r\n    269 \"\"\"Runs all callbacks associated with a specified event.\"\"\"\r\n    270 for callback in self.callbacks.get(event, []):\r\n--> 271     callback(self)\r\n\r\nFile [/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py:116](https://209.137.198.14:44227/lab/tree/workspace/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py#line=115), in context_decorator.<locals>.decorate_context(*args, **kwargs)\r\n    113 @functools.wraps(func)\r\n    114 def decorate_context(*args, **kwargs):\r\n    115     with ctx_factory():\r\n--> 116         return func(*args, **kwargs)\r\n\r\nFile [/opt/conda/lib/python3.11/site-packages/wandb/integration/ultralytics/callback.py:362](https://209.137.198.14:44227/lab/tree/workspace/opt/conda/lib/python3.11/site-packages/wandb/integration/ultralytics/callback.py#line=361), in WandBUltralyticsCallback.on_val_end(self, trainer)\r\n    353     self.validation_table = plot_segmentation_validation_results(\r\n    354         dataloader=dataloader,\r\n    355         class_label_map=class_label_map,\r\n   (...)\r\n    359         max_validation_batches=self.max_validation_batches,\r\n    360     )\r\n    361 elif self.task == \"detect\":\r\n--> 362     self.validation_table = plot_detection_validation_results(\r\n    363         dataloader=dataloader,\r\n    364         class_label_map=class_label_map,\r\n    365         model_name=self.model_name,\r\n    366         predictor=self.predictor,\r\n    367         table=self.validation_table,\r\n    368         max_validation_batches=self.max_validation_batches,\r\n    369     )\r\n    370 elif self.task == \"classify\":\r\n    371     self.validation_table = plot_classification_validation_results(\r\n    372         dataloader=dataloader,\r\n    373         model_name=self.model_name,\r\n   (...)\r\n    376         max_validation_batches=self.max_validation_batches,\r\n    377     )\r\n\r\nFile [/opt/conda/lib/python3.11/site-packages/wandb/integration/ultralytics/bbox_utils.py:172](https://209.137.198.14:44227/lab/tree/workspace/opt/conda/lib/python3.11/site-packages/wandb/integration/ultralytics/bbox_utils.py#line=171), in plot_detection_validation_results(dataloader, class_label_map, model_name, predictor, table, max_validation_batches, epoch)\r\n    170 max_validation_batches = min(max_validation_batches, num_dataloader_batches)\r\n    171 for batch_idx, batch in enumerate(dataloader):\r\n--> 172     prediction_results = predictor(batch[\"im_file\"])\r\n    173     progress_bar_result_iterable = tqdm(\r\n    174         enumerate(prediction_results),\r\n    175         total=len(prediction_results),\r\n    176         desc=f\"Generating Visualizations for batch-{batch_idx + 1}[/](https://209.137.198.14:44227/lab/tree/workspace/remote_train.ipynb){max_validation_batches}\",\r\n    177     )\r\n    178     for img_idx, prediction_result in progress_bar_result_iterable:\r\n\r\nFile [/opt/conda/lib/python3.11/site-packages/ultralytics/engine/predictor.py:173](https://209.137.198.14:44227/lab/tree/workspace/opt/conda/lib/python3.11/site-packages/ultralytics/engine/predictor.py#line=172), in BasePredictor.__call__(self, source, model, stream, *args, **kwargs)\r\n    171     return self.stream_inference(source, model, *args, **kwargs)\r\n    172 else:\r\n--> 173     return list(self.stream_inference(source, model, *args, **kwargs))\r\n\r\nFile [/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py:36](https://209.137.198.14:44227/lab/tree/workspace/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py#line=35), in _wrap_generator.<locals>.generator_context(*args, **kwargs)\r\n     33 try:\r\n     34     # Issuing `None` to a generator fires it up\r\n     35     with ctx_factory():\r\n---> 36         response = gen.send(None)\r\n     38     while True:\r\n     39         try:\r\n     40             # Forward the response to our caller and get its next request\r\n\r\nFile [/opt/conda/lib/python3.11/site-packages/ultralytics/engine/predictor.py:227](https://209.137.198.14:44227/lab/tree/workspace/opt/conda/lib/python3.11/site-packages/ultralytics/engine/predictor.py#line=226), in BasePredictor.stream_inference(self, source, model, *args, **kwargs)\r\n    225 # Setup model\r\n    226 if not self.model:\r\n--> 227     self.setup_model(model)\r\n    229 with self._lock:  # for thread-safe inference\r\n    230     # Setup source every time predict is called\r\n    231     self.setup_source(source if source is not None else self.args.source)\r\n\r\nFile [/opt/conda/lib/python3.11/site-packages/ultralytics/engine/predictor.py:308](https://209.137.198.14:44227/lab/tree/workspace/opt/conda/lib/python3.11/site-packages/ultralytics/engine/predictor.py#line=307), in BasePredictor.setup_model(self, model, verbose)\r\n    306 def setup_model(self, model, verbose=True):\r\n    307     \"\"\"Initialize YOLO model with given parameters and set it to evaluation mode.\"\"\"\r\n--> 308     self.model = AutoBackend(\r\n    309         weights=model or self.args.model,\r\n    310         device=select_device(self.args.device, verbose=verbose),\r\n    311         dnn=self.args.dnn,\r\n    312         data=self.args.data,\r\n    313         fp16=self.args.half,\r\n    314         batch=self.args.batch,\r\n    315         fuse=True,\r\n    316         verbose=verbose,\r\n    317     )\r\n    319     self.device = self.model.device  # update device\r\n    320     self.args.half = self.model.fp16  # update half\r\n\r\nFile [/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py:116](https://209.137.198.14:44227/lab/tree/workspace/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py#line=115), in context_decorator.<locals>.decorate_context(*args, **kwargs)\r\n    113 @functools.wraps(func)\r\n    114 def decorate_context(*args, **kwargs):\r\n    115     with ctx_factory():\r\n--> 116         return func(*args, **kwargs)\r\n\r\nFile [/opt/conda/lib/python3.11/site-packages/ultralytics/nn/autobackend.py:468](https://209.137.198.14:44227/lab/tree/workspace/opt/conda/lib/python3.11/site-packages/ultralytics/nn/autobackend.py#line=467), in AutoBackend.__init__(self, weights, device, dnn, data, fp16, batch, fuse, verbose)\r\n    464 # Any other format (unsupported)\r\n    465 else:\r\n    466     from ultralytics.engine.exporter import export_formats\r\n--> 468     raise TypeError(\r\n    469         f\"model='{w}' is not a supported model format. Ultralytics supports: {export_formats()['Format']}\\n\"\r\n    470         f\"See https://docs.ultralytics.com/modes/predict for help.\"\r\n    471     )\r\n    473 # Load external metadata YAML\r\n    474 if isinstance(metadata, (str, Path)) and Path(metadata).exists():\r\n\r\nTypeError: model='yolo11m.yaml' is not a supported model format. Ultralytics supports: ('PyTorch', 'TorchScript', 'ONNX', 'OpenVINO', 'TensorRT', 'CoreML', 'TensorFlow SavedModel', 'TensorFlow GraphDef', 'TensorFlow Lite', 'TensorFlow Edge TPU', 'TensorFlow.js', 'PaddlePaddle', 'MNN', 'NCNN', 'IMX')\r\nSee https://docs.ultralytics.com/modes/predict for help.\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!\n", "hints_text": "\ud83d\udc4b Hello @ZhaoqunZhong, thank you for your interest in Ultralytics \ud83d\ude80! We recommend a visit to the [Docs](https://docs.ultralytics.com) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a \ud83d\udc1b Bug Report, thank you for providing details! To help us debug the issue faster, please make sure your [minimum reproducible example (MRE)](https://docs.ultralytics.com/help/minimum_reproducible_example/) is sufficiently clear and concise so we can reproduce the exact error.\n\nYour error suggests the model format provided (`yolo11m.yaml`) is not supported. Please ensure you are using a supported model format or configuration. For reference, Ultralytics supports the following formats:  \n('PyTorch', 'TorchScript', 'ONNX', 'OpenVINO', 'TensorRT', 'CoreML', 'TensorFlow SavedModel', 'TensorFlow GraphDef', 'TensorFlow Lite', 'TensorFlow Edge TPU', 'TensorFlow.js', 'PaddlePaddle', 'MNN', 'NCNN', 'IMX').  \nConsult the [Predict Modes Documentation](https://docs.ultralytics.com/modes/predict) for assistance in selecting a proper model.\n\nIf this is a custom model training \u2753 Question, please share additional information like your `yolo11m.yaml` content, dataset examples, and training logs. Additionally, verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nYou can also upgrade to the latest version of the `ultralytics` package to confirm the issue still exists in the latest release. Use the following command to upgrade:\n\n```bash\npip install -U ultralytics\n```\n\n### Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n### Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are currently passing. CI tests verify the correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nLastly, this is an automated response \ud83d\ude0a. An Ultralytics engineer will review your query and provide additional support soon! Thank you for your patience and let us know if you have any further details to add in the meantime.\nI found out it's related to the wandb callback, if I remove this line:\r\n```\r\n# Add WandB callback for logging\r\n# add_wandb_callback(model)\r\n```\r\nThen it doesn't crush at the error mentioned above.\nCan you try restarting the notebook? Or restarting your PC/server?\n> I found out it's related to the wandb callback, if I remove this line:\n> ```\n> # Add WandB callback for logging\n> # add_wandb_callback(model)\n> ```\n> Then it doesn't crush at the error mentioned above.\n\nThis shouldn't be used.\nCan you open an issue at https://github.com/wandb/wandb to update their docs?\n> > I found out it's related to the wandb callback, if I remove this line:\r\n> > ```\r\n> > # Add WandB callback for logging\r\n> > # add_wandb_callback(model)\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > Then it doesn't crush at the error mentioned above.\r\n> \r\n> This shouldn't be used.\r\n\r\nHi, I found this callback usage from the comment section of the wandb integration doc from Ultralytics not wandb(https://docs.ultralytics.com/integrations/weights-biases/#how-do-i-integrate-weights-biases-with-ultralytics-yolo11). I had to use it like this because the steps from the doc actually doesn't work, it doesn't start the wandb logging at all. \n> Can you try restarting the notebook? Or restarting your PC/server?\r\n\r\nI restarted the notebook kernal multiple times, the error persists. But I didn't try restarting the server, I doubt it will make any difference since it's not environment or dependency issue, but rather a file format issue related to the integration of wandb?\nI guess I can close this issue now, since I can actually train from scratch without wandb logging?\n> > > I found out it's related to the wandb callback, if I remove this line:\n> > > ```\n> > > # Add WandB callback for logging\n> > > # add_wandb_callback(model)\n> > > ```\n> > > \n> > > \n> > >     \n> > >       \n> > >     \n> > > \n> > >       \n> > >     \n> > > \n> > >     \n> > >   \n> > > Then it doesn't crush at the error mentioned above.\n> > \n> > This shouldn't be used.\n> \n> Hi, I found this callback usage from the comment section of the wandb integration doc from Ultralytics not wandb(https://docs.ultralytics.com/integrations/weights-biases/#how-do-i-integrate-weights-biases-with-ultralytics-yolo11). I had to use it like this because the steps from the doc actually doesn't work, it doesn't start the wandb logging at all. \n\nThat comment is outdated.\n\nYou need to enable it with `!yolo settings wandb=True`\n> > > > I found out it's related to the wandb callback, if I remove this line:\r\n> > > > ```\r\n> > > > # Add WandB callback for logging\r\n> > > > # add_wandb_callback(model)\r\n> > > > ```\r\n> > > > \r\n> > > > \r\n> > > >     \r\n> > > >       \r\n> > > >     \r\n> > > > \r\n> > > >       \r\n> > > >     \r\n> > > > \r\n> > > >     \r\n> > > >   \r\n> > > > Then it doesn't crush at the error mentioned above.\r\n> > > \r\n> > > \r\n> > > This shouldn't be used.\r\n> > \r\n> > \r\n> > Hi, I found this callback usage from the comment section of the wandb integration doc from Ultralytics not wandb(https://docs.ultralytics.com/integrations/weights-biases/#how-do-i-integrate-weights-biases-with-ultralytics-yolo11). I had to use it like this because the steps from the doc actually doesn't work, it doesn't start the wandb logging at all.\r\n> \r\n> That comment is outdated.\r\n> \r\n> You need to enable it with `!yolo settings wandb=True`\r\n\r\nThanks, will try this. But is this left out in the doc(the link I provided)? The doc only mentioned wandb.login().", "created_at": "2025-01-06T10:05:29Z"}
{"repo": "ultralytics/ultralytics", "pull_number": 18377, "instance_id": "ultralytics__ultralytics-18377", "issue_numbers": ["18375"], "base_commit": "51026a9a4a0b5e1ea8a1c76b16f8de12b9a71530", "patch": "diff --git a/docs/en/usage/callbacks.md b/docs/en/usage/callbacks.md\nindex 16c47187862..57472143f85 100644\n--- a/docs/en/usage/callbacks.md\n+++ b/docs/en/usage/callbacks.md\n@@ -129,20 +129,22 @@ for result, frame in model.predict():\n \n To customize your Ultralytics training routine using callbacks, you can inject your logic at specific stages of the training process. Ultralytics YOLO provides a variety of training callbacks such as `on_train_start`, `on_train_end`, and `on_train_batch_end`. These allow you to add custom metrics, processing, or logging.\n \n-Here's an example of how to log additional metrics at the end of each training epoch:\n+Here's an example of how to freeze BatchNorm statistics when freezing layers with callbacks:\n \n ```python\n from ultralytics import YOLO\n \n-\n-def on_train_epoch_end(trainer):\n-    \"\"\"Custom logic for additional metrics logging at the end of each training epoch.\"\"\"\n-    additional_metric = compute_additional_metric(trainer)\n-    trainer.log({\"additional_metric\": additional_metric})\n-\n+# Add a callback to put the frozen layers in eval mode to prevent BN values from changing\n+def put_in_eval_mode(trainer):\n+  n_layers = trainer.args.freeze\n+  if not isinstance(n_layers, int): return\n+\u00a0 for i, (name, module) in enumerate(trainer.model.named_modules()):\n+\u00a0 \u00a0 if name.endswith(\"bn\") and int(name.split('.')[1]) < n_layers:\n+\u00a0 \u00a0 \u00a0 module.eval()\n+\u00a0 \u00a0 \u00a0 module.track_running_stats = False\n \n model = YOLO(\"yolo11n.pt\")\n-model.add_callback(\"on_train_epoch_end\", on_train_epoch_end)\n+model.add_callback(\"on_train_epoch_start\", put_in_eval_mode)\n model.train(data=\"coco.yaml\", epochs=10)\n ```\n \n@@ -152,20 +154,23 @@ Refer to the [Training Guide](../modes/train.md) for more details on how to effe\n \n Using **callbacks during validation** in Ultralytics YOLO can enhance model evaluation by allowing custom processing, logging, or metrics calculation. Callbacks such as `on_val_start`, `on_val_batch_end`, and `on_val_end` provide entry points to inject custom logic, ensuring detailed and comprehensive validation processes.\n \n-For instance, you might want to log additional validation metrics or save intermediate results for further analysis. Here's an example of how to log custom metrics at the end of validation:\n+For instance, you might want to plot all the validation batches, instead of just the first 3. Here's how you can do that:\n \n ```python\n+import inspect\n+\n from ultralytics import YOLO\n \n \n-def on_val_end(validator):\n-    \"\"\"Log custom metrics at end of validation.\"\"\"\n-    custom_metric = compute_custom_metric(validator)\n-    validator.log({\"custom_metric\": custom_metric})\n+def plot_samples(validator):\n+    frame = inspect.currentframe().f_back.f_back\n+    v = frame.f_locals\n+    validator.plot_val_samples(v[\"batch\"], v[\"batch_i\"])\n+    validator.plot_predictions(v[\"batch\"], v[\"preds\"], v[\"batch_i\"])\n \n \n model = YOLO(\"yolo11n.pt\")\n-model.add_callback(\"on_val_end\", on_val_end)\n+model.add_callback(\"on_val_batch_end\", plot_samples)\n model.val(data=\"coco.yaml\")\n ```\n \n@@ -175,21 +180,29 @@ Check out the [Validation Guide](../modes/val.md) for further insights on incorp\n \n To attach a custom callback for the **prediction mode** in Ultralytics YOLO, you define a callback function and register it with the prediction process. Common prediction callbacks include `on_predict_start`, `on_predict_batch_end`, and `on_predict_end`. These allow for modification of prediction outputs and integration of additional functionalities like data logging or result transformation.\n \n-Here is an example where a custom callback is used to log predictions:\n+Here is an example where a custom callback is used to save predictions based on whether an object of a particular class is present:\n \n ```python\n from ultralytics import YOLO\n \n+model = YOLO(\"yolo11n.pt\")\n \n-def on_predict_end(predictor):\n-    \"\"\"Log predictions at the end of prediction.\"\"\"\n-    for result in predictor.results:\n-        log_prediction(result)\n+class_id = 2\n \n \n-model = YOLO(\"yolo11n.pt\")\n-model.add_callback(\"on_predict_end\", on_predict_end)\n-results = model.predict(source=\"image.jpg\")\n+def save_on_object(predictor):\n+    r = predictor.results[0]\n+    if class_id in r.boxes.cls:\n+        predictor.args.save = True\n+    else:\n+        predictor.args.save = False\n+\n+\n+model.add_callback(\"on_predict_postprocess_end\", save_on_object)\n+results = model(\"pedestrians.mp4\", stream=True, save=True)\n+\n+for results in results:\n+    pass\n ```\n \n For more comprehensive usage, refer to the [Prediction Guide](../modes/predict.md) which includes detailed instructions and additional customization options.\n", "test_patch": "", "problem_statement": "Failed to log metrics while customizing callbacks function for validator\n### Search before asking\n\n- [X] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHere is what introduced in the [ultralytics documents](https://docs.ultralytics.com/usage/callbacks/#how-can-i-customize-ultralytics-training-routine-using-callbacks):\r\n```\r\nfrom ultralytics import YOLO\r\n\r\n\r\ndef on_val_end(validator):\r\n    \"\"\"Log custom metrics at end of validation.\"\"\"\r\n    custom_metric = compute_custom_metric(validator)\r\n    validator.log({\"custom_metric\": custom_metric})\r\n\r\n\r\nmodel = YOLO(\"yolo11n.pt\")\r\nmodel.add_callback(\"on_val_end\", on_val_end)\r\nmodel.val(data=\"coco.yaml\")\r\n```\r\n\r\nHere is my code:\r\n```\r\nfrom ultralytics import YOLO\r\n\r\ndef on_val_end(validator):\r\n    # Your dummy metric implementation\r\n    dummy_metric = 0.0  # Replace with your computation\r\n    validator.log({\"custom_metric\": 0.})\r\n\r\nmodel = YOLO('yolo11l.pt')\r\nmodel.add_callback(\"on_val_end\", on_val_end)\r\nmodel.val(data=\"cfg.yaml\")\r\n```\r\n\r\nFailed with the following errors reported:\r\n```\r\nFile \"xxx/cb_sp0.py\", line 8, in on_val_end\r\n    validator.log({\"fbeta:\": 0.})\r\n    ^^^^^^^^^^^^^\r\nAttributeError: 'DetectionValidator' object has no attribute 'log'\r\n```\r\n\r\nWhat's the difference between mine and the docs'?\n\n### Additional\n\n_No response_\n", "hints_text": "\ud83d\udc4b Hello @ForcewithMe66, thank you for your interest in Ultralytics \ud83d\ude80! We appreciate you raising this question and for referencing the relevant documentation.\n\nFrom your description, it seems you're encountering a challenge with customizing callbacks for the validator. To assist you better, we'd like to confirm you are using the latest version of the `ultralytics` package, as callback behavior and features may have been updated recently. Please upgrade your environment if needed using the command below:\n\n```bash\npip install -U ultralytics\n```\n\nIf this is a \ud83d\udc1b Bug Report, kindly provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) that includes all the necessary steps and context to replicate the issue, such as the complete script, data configuration, and error trace.\n\nAdditionally, here are some next steps to try:\n- Double-check that the `validator` object in your callback function contains the attributes and methods you are attempting to use. If any discrepancy exists compared to the example in the [documentation](https://docs.ultralytics.com/usage/callbacks/), it could indicate a version mismatch or specific behavior tied to your model setup.\n- If you'd like, share additional details (e.g., your `cfg.yaml` content and system environment) to help us debug the issue.\n\nIn the meantime, join the Ultralytics community for more support and discussions:\n- For real-time assistance, head to [Discord](https://discord.com/invite/ultralytics) \ud83c\udfa7.\n- Prefer in-depth discussions? Check out [Discourse](https://community.ultralytics.com).\n- Share your findings and ask for advice on our [Subreddit](https://reddit.com/r/Ultralytics).\n\n## Environment Suggestions\n\nTo ensure consistency and reliability, consider running your code in one of the verified YOLO environments:\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status Check\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>  \nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are passing, confirming the correct operation across environments.  \n\nThis is an automated response to ensure you receive help as quickly as possible. An Ultralytics engineer will review your issue and provide additional insights soon \ud83d\ude0a", "created_at": "2024-12-24T19:13:29Z"}
{"repo": "ultralytics/ultralytics", "pull_number": 18212, "instance_id": "ultralytics__ultralytics-18212", "issue_numbers": ["18206"], "base_commit": "626e42ef253b5c20fa83412e7daf9b713484a866", "patch": "diff --git a/ultralytics/engine/model.py b/ultralytics/engine/model.py\nindex db8d87ebc2f..8affd958f27 100644\n--- a/ultralytics/engine/model.py\n+++ b/ultralytics/engine/model.py\n@@ -115,7 +115,7 @@ def __init__(\n         self.predictor = None  # reuse predictor\n         self.model = None  # model object\n         self.trainer = None  # trainer object\n-        self.ckpt = None  # if loaded from *.pt\n+        self.ckpt = {}  # if loaded from *.pt\n         self.cfg = None  # if loaded from *.yaml\n         self.ckpt_path = None\n         self.overrides = {}  # overrides for trainer object\n@@ -807,7 +807,7 @@ def train(\n         # Update model and cfg after training\n         if RANK in {-1, 0}:\n             ckpt = self.trainer.best if self.trainer.best.exists() else self.trainer.last\n-            self.model, _ = attempt_load_one_weight(ckpt)\n+            self.model, self.ckpt = attempt_load_one_weight(ckpt)\n             self.overrides = self.model.args\n             self.metrics = getattr(self.trainer.validator, \"metrics\", None)  # TODO: no metrics returned by DDP\n         return self.metrics\n", "test_patch": "", "problem_statement": "Saving yolov6n crashes\n### Search before asking\n\n- [X] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\nExport\n\n### Bug\n\nCrash on \"yolov6n.yaml\" save.\r\n\r\n```\r\nNew https://pypi.org/project/ultralytics/8.3.49 available \ud83d\ude03 Update with 'pip install -U ultralytics'\r\nUltralytics 8.3.44 \ud83d\ude80 Python-3.10.12 torch-2.5.1+cu124 CPU (AMD Ryzen 3 3200G with Radeon Vega Graphics)\r\nengine/trainer: task=detect, mode=train, model=yolov6n.yaml, data=coco8.yaml, epochs=1, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train11, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train11\r\nactivation: nn.ReLU()\r\n\r\n                   from  n    params  module                                       arguments                     \r\n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \r\n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \r\n  2                  -1  2     18560  ultralytics.nn.modules.conv.Conv             [32, 32, 3, 1]                \r\n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \r\n  4                  -1  4    147968  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 1]                \r\n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \r\n  6                  -1  6    886272  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 1]              \r\n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \r\n  8                  -1  2   1180672  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 1]              \r\n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \r\n 10                  -1  1     16512  ultralytics.nn.modules.conv.Conv             [256, 64, 1, 1]               \r\n 11                  -1  1     16448  torch.nn.modules.conv.ConvTranspose2d        [64, 64, 2, 2, 0]             \r\n 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \r\n 13                  -1  1    110720  ultralytics.nn.modules.conv.Conv             [192, 64, 3, 1]               \r\n 14                  -1  3    110976  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 1]                \r\n 15                  -1  1      2112  ultralytics.nn.modules.conv.Conv             [64, 32, 1, 1]                \r\n 16                  -1  1      4128  torch.nn.modules.conv.ConvTranspose2d        [32, 32, 2, 2, 0]             \r\n 17             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \r\n 18                  -1  1     27712  ultralytics.nn.modules.conv.Conv             [96, 32, 3, 1]                \r\n 19                  -1  3     27840  ultralytics.nn.modules.conv.Conv             [32, 32, 3, 1]                \r\n 20                  -1  1      9280  ultralytics.nn.modules.conv.Conv             [32, 32, 3, 2]                \r\n 21            [-1, 15]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \r\n 22                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 1]                \r\n 23                  -1  3    110976  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 1]                \r\n 24                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \r\n 25            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \r\n 26                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 1]              \r\n 27                  -1  3    443136  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 1]              \r\n 28        [19, 23, 27]  1    607360  ultralytics.nn.modules.head.Detect           [80, [32, 64, 128]]           \r\nYOLOv6n summary: 195 layers, 4,500,080 parameters, 4,500,064 gradients, 13.1 GFLOPs\r\n\r\nTensorBoard: Start with 'tensorboard --logdir runs/detect/train11', view at http://localhost:6006/\r\nFreezing layer 'model.28.dfl.conv.weight'\r\ntrain: Scanning datasets/coco8/labels/train.cache... 4 images, 0 backgrounds, 0 corrupt: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<?, ?it/s]\r\nval: Scanning datasets/coco8/labels/val.cache... 4 images, 0 backgrounds, 0 corrupt: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<?, ?it/s]\r\nPlotting labels to runs/detect/train11/labels.jpg... \r\noptimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \r\noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 53 weight(decay=0.0), 62 weight(decay=0.0005), 61 bias(decay=0.0)\r\nTensorBoard: model graph visualization added \u2705\r\nImage sizes 640 train, 640 val\r\nUsing 0 dataloader workers\r\nLogging results to runs/detect/train11\r\nStarting training for 1 epochs...\r\n\r\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\r\n        1/1         0G      3.483      5.686      4.311         22        640: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:05<00:00,  5.91s/it]\r\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02<00:00,  2.16s/it]\r\n                   all          4         17          0          0          0          0\r\n\r\n1 epochs completed in 0.006 hours.\r\nOptimizer stripped from runs/detect/train11/weights/last.pt, 9.2MB\r\nOptimizer stripped from runs/detect/train11/weights/best.pt, 9.2MB\r\n\r\nValidating runs/detect/train11/weights/best.pt...\r\nWARNING \u26a0\ufe0f validating an untrained model YAML will result in 0 mAP.\r\nUltralytics 8.3.44 \ud83d\ude80 Python-3.10.12 torch-2.5.1+cu124 CPU (AMD Ryzen 3 3200G with Radeon Vega Graphics)\r\nYOLOv6n summary (fused): 142 layers, 4,495,392 parameters, 0 gradients, 13.0 GFLOPs\r\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.31s/it]\r\n                   all          4         17          0          0          0          0\r\nSpeed: 4.7ms preprocess, 314.3ms inference, 0.0ms loss, 2.3ms postprocess per image\r\nResults saved to runs/detect/train11\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 91, in <module>\r\n    detectImageWithYolo()\r\n  File \"main.py\", line 70, in detectImageWithYolo\r\n    model.save(\"yolov6coco.pt\")\r\n  File \".venv/lib/python3.10/site-packages/ultralytics/engine/model.py\", line 414, in save\r\n    torch.save({**self.ckpt, **updates}, filename)\r\nTypeError: 'NoneType' object is not a mapping\r\n```\n\n### Environment\n\nUltralytics 8.3.44 \ud83d\ude80 Python-3.10.12 torch-2.5.1+cu124 CPU (AMD Ryzen 3 3200G with Radeon Vega Graphics)\r\nSetup complete \u2705 (4 CPUs, 23.3 GB RAM, 2769.5/3185.4 GB disk)\r\nOS                  Linux-6.8.0-49-generic-x86_64-with-glibc2.35\r\nEnvironment         Linux\r\nPython              3.10.12\r\nInstall             pip\r\nRAM                 23.35 GB\r\nDisk                2769.5/3185.4 GB\r\nCPU                 AMD Ryzen 3 3200G with Radeon Vega Graphics\r\nCPU count           4\r\nGPU                 None\r\nGPU count           None\r\nCUDA                None\r\nnumpy               \u2705 2.0.2>=1.23.0\r\nnumpy               \u2705 2.0.2<2.0.0; sys_platform == \"darwin\"\r\nmatplotlib          \u2705 3.9.2>=3.3.0\r\nopencv-python       \u2705 4.10.0.84>=4.6.0\r\npillow              \u2705 11.0.0>=7.1.2\r\npyyaml              \u2705 6.0.2>=5.3.1\r\nrequests            \u2705 2.32.3>=2.23.0\r\nscipy               \u2705 1.14.1>=1.4.1\r\ntorch               \u2705 2.5.1>=1.8.0\r\ntorch               \u2705 2.5.1!=2.4.0,>=1.8.0; sys_platform == \"win32\"\r\ntorchvision         \u2705 0.20.1>=0.9.0\r\ntqdm                \u2705 4.67.1>=4.64.0\r\npsutil              \u2705 6.1.0\r\npy-cpuinfo          \u2705 9.0.0\r\npandas              \u2705 2.2.3>=1.1.4\r\nseaborn             \u2705 0.13.2>=0.11.0\r\nultralytics-thop    \u2705 2.0.12>=2.0.0\r\n{'OS': 'Linux-6.8.0-49-generic-x86_64-with-glibc2.35', 'Environment': 'Linux', 'Python': '3.10.12', 'Install': 'pip', 'RAM': '23.35 GB', 'Disk': '2769.5/3185.4 GB', 'CPU': 'AMD Ryzen 3 3200G with Radeon Vega Graphics', 'CPU count': 4, 'GPU': None, 'GPU count': None, 'CUDA': None, 'Package Info': {'numpy': '\u2705 2.0.2<2.0.0; sys_platform == \"darwin\"', 'matplotlib': '\u2705 3.9.2>=3.3.0', 'opencv-python': '\u2705 4.10.0.84>=4.6.0', 'pillow': '\u2705 11.0.0>=7.1.2', 'pyyaml': '\u2705 6.0.2>=5.3.1', 'requests': '\u2705 2.32.3>=2.23.0', 'scipy': '\u2705 1.14.1>=1.4.1', 'torch': '\u2705 2.5.1!=2.4.0,>=1.8.0; sys_platform == \"win32\"', 'torchvision': '\u2705 0.20.1>=0.9.0', 'tqdm': '\u2705 4.67.1>=4.64.0', 'psutil': '\u2705 6.1.0', 'py-cpuinfo': '\u2705 9.0.0', 'pandas': '\u2705 2.2.3>=1.1.4', 'seaborn': '\u2705 0.13.2>=0.11.0', 'ultralytics-thop': '\u2705 2.0.12>=2.0.0'}}\r\n\n\n### Minimal Reproducible Example\n\n```\r\nmodel = YOLO(\"yolov6n.yaml\")\r\nmodel.train(data=\"coco8.yaml\", epochs=1, imgsz=640)\r\nmodel.save(\"yolov6coco.pt\")\r\n```\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!\n", "hints_text": "\ud83d\udc4b Hello @EmmanuelMess, thank you for your interest in Ultralytics \ud83d\ude80! We appreciate you taking the time to report this issue. \n\nTo help us investigate further, could you please confirm the reproducibility of this issue using the latest version of Ultralytics? You can upgrade with the command below:\n\n```bash\npip install -U ultralytics\n```\n\nWe also noticed that you've shared a reproducible example (thank you for that!). If the issue persists in the most recent version, this example will be very helpful for us to debug. Please also ensure that your `ultralytics` environment is aligned with our recommended setups:\n\n### Environments\n\nYOLO can be run in any of the following up-to-date verified environments (pre-installed with all necessary dependencies such as [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/)):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n### Join the Community\n\n- For real-time debugging or discussions, join our [Discord](https://discord.com/invite/ultralytics) server \ud83c\udfa7.\n- Share your insights or questions on [Discourse](https://community.ultralytics.com) or [Reddit](https://reddit.com/r/Ultralytics) to interact with the thriving community.\n\n### Helpful Resources \n\nIf you'd like to further explore concepts or troubleshoot other issues, check out our comprehensive [Docs](https://docs.ultralytics.com), including:\n- [Model Training Tips](https://docs.ultralytics.com/guides/model-training-tips/)\n- [Minimum Reproducible Example Guide](https://docs.ultralytics.com/help/minimum_reproducible_example/)\n\nIf this is a \ud83d\udc1b bug as appears to be the case, an Ultralytics engineer will assist you soon to look deeper into the root cause. We'll stay on top of resolving this for you! \ud83d\udd0d\ud83d\ude0a\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a> \n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are currently passing. CI tests run on macOS, Windows, and Ubuntu frequently to maintain performance and reliability.\n\nLet us know how it goes! \ud83d\ude80\nThe model is automatically saved during training inside the runs/detect folder. You don't need to use `model.save()`.\n> The model is automatically saved during training inside the runs/detect folder. You don't need to use `model.save()`.\r\n\r\nBut I want to explicitly save it to a file. It works with YOLOv11, why not with v6?\nIt doesn't work if you load from yaml.\n> It doesn't work if you load from yaml.\r\n\r\nThanks for the explanation, but it probably shouldn't crash. Maybe add an error message?", "created_at": "2024-12-13T03:40:07Z"}
{"repo": "ultralytics/ultralytics", "pull_number": 18077, "instance_id": "ultralytics__ultralytics-18077", "issue_numbers": ["18073"], "base_commit": "eb3783763dae921ada76bbb4d01a96ec5d412334", "patch": "diff --git a/docs/en/macros/augmentation-args.md b/docs/en/macros/augmentation-args.md\nindex b4d6c9df6d4..bee27ddd0da 100644\n--- a/docs/en/macros/augmentation-args.md\n+++ b/docs/en/macros/augmentation-args.md\n@@ -13,7 +13,7 @@\n | `bgr`             | `float` | `0.0`         | `0.0 - 1.0`   | Flips the image channels from RGB to BGR with the specified probability, useful for increasing robustness to incorrect channel ordering.                                  |\n | `mosaic`          | `float` | `1.0`         | `0.0 - 1.0`   | Combines four training images into one, simulating different scene compositions and object interactions. Highly effective for complex scene understanding.                |\n | `mixup`           | `float` | `0.0`         | `0.0 - 1.0`   | Blends two images and their labels, creating a composite image. Enhances the model's ability to generalize by introducing label noise and visual variability.             |\n-| `copy_paste`      | `float` | `0.0`         | `0.0 - 1.0`   | Copies objects from one image and pastes them onto another, useful for increasing object instances and learning object occlusion.                                         |\n+| `copy_paste`      | `float` | `0.0`         | `0.0 - 1.0`   | Copies and pastes objects across images, useful for increasing object instances and learning object occlusion. Requires segmentation labels.                              |\n | `copy_paste_mode` | `str`   | `flip`        | -             | Copy-Paste augmentation method selection among the options of (`\"flip\"`, `\"mixup\"`).                                                                                      |\n | `auto_augment`    | `str`   | `randaugment` | -             | Automatically applies a predefined augmentation policy (`randaugment`, `autoaugment`, `augmix`), optimizing for classification tasks by diversifying the visual features. |\n | `erasing`         | `float` | `0.4`         | `0.0 - 0.9`   | Randomly erases a portion of the image during classification training, encouraging the model to focus on less obvious features for recognition.                           |\n", "test_patch": "", "problem_statement": "how to use  copy-paste for   object detection tasks\n### Search before asking\n\n- [X] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI have a question regarding the use of the copy-paste method. Although YOLO has a new implementation, when I debug, I find that the segments in the labels are empty. To use this method, do I need to add additional mask data? I have experimented with adding and not adding the mask, but the results seem to be the same. Could you please teach me how to effectively use the copy-paste data augmentation method in YOLO for object detection tasks? Thank you so much.\n\n### Additional\n\n_No response_\n", "hints_text": "\ud83d\udc4b Hello @ZZxiaoxiao, thank you for your interest in Ultralytics \ud83d\ude80! We recommend taking a look at the [Docs](https://docs.ultralytics.com), where you'll find resources and examples for using advanced techniques such as data augmentation. You may also find useful details about available augmentations in [Model Training Tips](https://docs.ultralytics.com/guides/model-training-tips/).\n\nIf this is a \ud83d\udc1b Bug Report (e.g., related to segments being empty), it would be helpful if you could provide a [minimum reproducible example (MRE)](https://docs.ultralytics.com/help/minimum_reproducible_example/) showcasing the issue. This will allow us to investigate effectively and expedite the debugging process.\n\nIf this is a \u2753 Question about custom training or specific augmentations like copy-paste, please share more details about your setup. In particular:\n- The dataset configuration, including whether masks are included\n- Relevant portions of your code or training script\n- Results or logs from your experiments\n\nYou can also explore and run YOLO experiments for free in the following environments:\n- <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a>\n- <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n- <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n\n## Upgrade\n\nEnsure you are running on the latest version of `ultralytics` with all dependencies updated. Use the following command in a [Python>=3.8](https://www.python.org/) environment:\n```bash\npip install -U ultralytics\n```\nThis ensures you have access to the latest features, including any updates to copy-paste functionality.\n\n## Community Support\n\nJoin our vibrant community for further discussions! \n- Real-time advice: [Discord](https://discord.com/invite/ultralytics) \ud83c\udfa7\n- Long-form Q&A: [Discourse](https://community.ultralytics.com)\n- Engage with others: [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>  \nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are currently passing. This confirms the expected function of YOLO modes and tasks across various environments.\n\nThis is an automated response to help streamline support \ud83d\udd04. An Ultralytics engineer will follow up with additional assistance soon. Thank you for your patience and understanding! \ud83d\ude0a\nI'm also interested in this issue and look forward to the explanation\nIt only supports segmentation task\n![image](https://github.com/user-attachments/assets/be2b5975-6b22-4990-aed0-4a3f0ca0f461)\r\n\nwhy there object detection use copy paste?\nIt is for YOLO11-seg models.", "created_at": "2024-12-06T14:28:29Z"}
{"repo": "ultralytics/ultralytics", "pull_number": 17921, "instance_id": "ultralytics__ultralytics-17921", "issue_numbers": ["17919"], "base_commit": "530e6b934256017b518fe312d57f5700f7438f3f", "patch": "diff --git a/docs/en/guides/triton-inference-server.md b/docs/en/guides/triton-inference-server.md\nindex 0151cc078de..67d419bf523 100644\n--- a/docs/en/guides/triton-inference-server.md\n+++ b/docs/en/guides/triton-inference-server.md\n@@ -48,6 +48,16 @@ from ultralytics import YOLO\n # Load a model\n model = YOLO(\"yolo11n.pt\")  # load an official model\n \n+# Retreive metadata during export\n+metadata = []\n+\n+\n+def export_cb(exporter):\n+    metadata.append(exporter.metadata)\n+\n+\n+model.add_callback(\"on_export_end\", export_cb)\n+\n # Export the model\n onnx_file = model.export(format=\"onnx\", dynamic=True)\n ```\n@@ -107,7 +117,13 @@ The Triton Model Repository is a storage location where Triton can access and lo\n         }\n       }\n     }\n-    \"\"\"\n+    parameters {\n+      key: \"metadata\"\n+      value: {\n+        string_value: \"%s\"\n+      }\n+    }\n+    \"\"\" % metadata[0]\n \n     with open(triton_model_path / \"config.pbtxt\", \"w\") as f:\n         f.write(data)\ndiff --git a/ultralytics/__init__.py b/ultralytics/__init__.py\nindex 790bb406d46..9d19f6abfd4 100644\n--- a/ultralytics/__init__.py\n+++ b/ultralytics/__init__.py\n@@ -1,6 +1,6 @@\n # Ultralytics YOLO \ud83d\ude80, AGPL-3.0 license\n \n-__version__ = \"8.3.43\"\n+__version__ = \"8.3.44\"\n \n import os\n \ndiff --git a/ultralytics/nn/autobackend.py b/ultralytics/nn/autobackend.py\nindex 8e9b74ebc5c..b6df3753ec3 100644\n--- a/ultralytics/nn/autobackend.py\n+++ b/ultralytics/nn/autobackend.py\n@@ -462,6 +462,7 @@ def torch_to_mnn(x):\n             from ultralytics.utils.triton import TritonRemoteModel\n \n             model = TritonRemoteModel(w)\n+            metadata = model.metadata\n \n         # Any other format (unsupported)\n         else:\ndiff --git a/ultralytics/utils/triton.py b/ultralytics/utils/triton.py\nindex 3f873a6fafc..cc53ed57149 100644\n--- a/ultralytics/utils/triton.py\n+++ b/ultralytics/utils/triton.py\n@@ -66,6 +66,7 @@ def __init__(self, url: str, endpoint: str = \"\", scheme: str = \"\"):\n         self.np_input_formats = [type_map[x] for x in self.input_formats]\n         self.input_names = [x[\"name\"] for x in config[\"input\"]]\n         self.output_names = [x[\"name\"] for x in config[\"output\"]]\n+        self.metadata = eval(config.get(\"parameters\", {}).get(\"metadata\", {}).get(\"string_value\", \"None\"))\n \n     def __call__(self, *inputs: np.ndarray) -> List[np.ndarray]:\n         \"\"\"\n", "test_patch": "", "problem_statement": "YoloV11 Seg Model Exported to ONNX and Loaded on Triton 24.11 opset=19 gives Mat Multiplication error on Predict ?\n### Search before asking\n\n- [X] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\nPredict, Export\n\n### Bug\n\n### **Issue Description**\r\n\r\nI trained a YOLOv11 segmentation model and exported it to ONNX for deployment on Triton Inference Server. While the ONNX model works as expected locally (using `onnxruntime`), inference through Triton results in errors due to incompatible shapes.\r\n\r\nTriton Version: 24.11 \r\nopset: 19\r\n\r\n---\r\n\r\n### **Error Logs**\r\n\r\nWhen running inference via Triton, I encounter the following error during postprocessing:\r\n\r\n```plaintext\r\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (300x0 and 32x4096)\r\n```\r\n\r\n- **Shape Inspection**:\r\n  - `pred.shape`: `[300, 6]`\r\n  - `masks_in.shape`: `[300, 0]` )\r\n  - `protos.shape`: `[32, 64, 64]`\r\n\r\nThe issue arises because `masks_in` is empty, making it incompatible for matrix multiplication with `protos`. This issue does not occur when running inference directly with the exported ONNX model locally. \r\n\r\nHere is the snippet example code i use for predict:\r\n\r\n```python\r\nmodel =  YOLO(\"http://localhost:8000/seg_model\",task=\"segment\")\r\n## prepare  my image as a Numpy array with shape : [256,256,3]\r\nresults = model.predict(input_image, conf=0.7)\r\n```\r\n\r\n---\r\n\r\n### **Additional Context**\r\n\r\n- **Local ONNX Inference**: Works as expected with the following shapes:\r\n  - `pred.shape`: `[2, 38]`\r\n  - `masks_in.shape`: `[2, 32]`\r\n  - `protos.shape`: `[32, 64, 64]`\r\n\r\n- **Triton Inference**: Produces incorrect shapes:\r\n  - `pred.shape`: `[300, 6]`\r\n  - `masks_in.shape`: `[300, 0]`\r\n  - `protos.shape`: `[32, 64, 64]`\r\n\r\nThis suggests that YOLO model.predict for Triton may not correctly handle the model\u2019s intermediate outputs, or there is an issue with how the ONNX model is exported.\r\n\r\n---\r\n\r\n### **Request for Assistance**\r\n\r\n1. Is this a known issue with ONNX export for YOLO segmentation models?\r\n2. Should any specific postprocessing steps or additional configuration be included during ONNX export to make the model Triton-compatible?\r\n\n\n### Environment\n\nUltralytics 8.3.38 \ud83d\ude80 Python-3.8.16 torch-2.4.0+cu121 CUDA:0 (NVIDIA RTX A6000, 48577MiB)\r\nSetup complete \u2705 (64 CPUs, 125.7 GB RAM, 612.3/1844.4 GB disk)\r\n\r\nOS                  Linux-4.18.0-553.5.1.el8_10.x86_64-x86_64-with-glibc2.17\r\nEnvironment         Linux\r\nPython              3.8.16\r\nInstall             pip\r\nRAM                 125.74 GB\r\nDisk                612.3/1844.4 GB\r\nCPU                 AMD EPYC 7543 32-Core Processor\r\nCPU count           64\r\nGPU                 NVIDIA RTX A6000, 48577MiB\r\nGPU count           2\r\nCUDA                12.1\r\n\r\nnumpy               \u2705 1.24.4>=1.23.0\r\nnumpy               \u2705 1.24.4<2.0.0; sys_platform == \"darwin\"\r\nmatplotlib          \u2705 3.7.5>=3.3.0\r\nopencv-python       \u2705 4.10.0.84>=4.6.0\r\npillow              \u2705 10.4.0>=7.1.2\r\npyyaml              \u2705 6.0.1>=5.3.1\r\nrequests            \u2705 2.32.3>=2.23.0\r\nscipy               \u2705 1.10.1>=1.4.1\r\ntorch               \u2705 2.4.0>=1.8.0\r\ntorch               \u2705 2.4.0!=2.4.0,>=1.8.0; sys_platform == \"win32\"\r\ntorchvision         \u2705 0.19.0>=0.9.0\r\ntqdm                \u2705 4.66.4>=4.64.0\r\npsutil              \u2705 6.0.0\r\npy-cpuinfo          \u2705 9.0.0\r\npandas              \u2705 2.0.3>=1.1.4\r\nseaborn             \u2705 0.13.2>=0.11.0\r\nultralytics-thop    \u2705 2.0.0>=2.0.0\n\n### Minimal Reproducible Example\n\n### **Steps to Reproduce**\r\n\r\n1. Train a YOLOv11 segmentation model: `yolov11m-seg model`\r\n   `\r\n2. Export the trained model to ONNX:\r\n   ```python\r\n   model = YOLO(weights_path,task=\"segment\")\r\n   model.export(format='onnx', dynamic=True )\r\n   ```\r\n3. Deploy the ONNX model to Triton Inference Server.\r\n4. Run inference: \r\n```python\r\nmodel =  YOLO(\"http://localhost:8000/seg_model\",task=\"segment\")\r\ninput_image = ## prepare  my image as a Numpy array with shape : [256,256,3]\r\nresults = model.predict(input_image, conf=0.7)\r\n```\r\n6. Run inference on an image through Triton using the following configuration:\r\n   - Input size: `(256, 256)`\r\n   - Confidence threshold: `0.7`\r\n7. Observe the error during predict. \r\n\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [x] Yes I'd like to help by submitting a PR!\n", "hints_text": "\ud83d\udc4b Hello @julian-zaya, thank you for your interest in Ultralytics \ud83d\ude80! We appreciate the detailed report and your efforts to provide comprehensive information.\n\nTo assist you better:\n\n- If this is a \ud83d\udc1b Bug Report, please provide a [minimum reproducible example (MRE)](https://docs.ultralytics.com/help/minimum_reproducible_example/) in the form of minimal code and steps that reproduce the error. This will help our team debug it more efficiently.\n- Since you are deploying your YOLOv11 segmentation model using Triton, ensure that the exported ONNX model's dynamic shapes align with Triton's input requirements. It appears the issue may stem from a mismatch in expected shapes during inference.\n- Please verify that you are using the latest version of our library and dependencies by upgrading with the command below:\n\n```bash\npip install -U ultralytics\n```\n\nYou can also refer to our general [ONNX Export Guide](https://docs.ultralytics.com/modes/export/) to ensure all model export steps are followed correctly.\n\nFor your specific usage case:\n- Double-check that your ONNX export is using `dynamic=True` as documented, which supports varying input sizes.\n\n## Environment Verification\n\nYOLO can be run in the following tested and verified environments to ensure compatibility:\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM: Refer to the [Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/).\n- **Amazon AWS** Deep Learning AMI: Refer to the [Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/).\n- **Docker Image**: See our [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>.\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf the CI badge above is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are passing and the issue may stem from your custom environment or inputs. CI tests validate all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) across macOS, Windows, and Ubuntu.\n\nWe recommend joining the vibrant Ultralytics community to discuss your implementation if you need additional support. For real-time discussions, visit us on [Discord](https://discord.com/invite/ultralytics) \ud83c\udfa7. Alternatively, engage in meaningful technical exchanges on our [Discourse](https://community.ultralytics.com) or share ideas and troubleshoot with others on our [Subreddit](https://reddit.com/r/Ultralytics).\n\nThis is an automated response. An Ultralytics engineer will review and address your issue as soon as possible. Thank you for your patience and for contributing to our growing community! \ud83d\ude0a\nYou should pass `data=data.yaml` while loading Triton segmentation model because otherwise it can't know the metadata.\n> You should pass `data=data.yaml` while loading Triton segmentation model because otherwise it can't know the metadata.\r\n\r\nI am sorry where should i pass it ? Should i use the data.yaml from training ? \nIn the `predict()` function. Yes, the data.yaml that you used.\n> In the `predict()` function. Yes, the data.yaml that you used.\r\n\r\nThank you that works. \r\n\r\nI used the `data.yaml` from training. and used it with the `model.predict` function as `data=data.yaml`, this should be mentioned in the documentation \r\n", "created_at": "2024-12-02T13:59:02Z"}
{"repo": "ultralytics/ultralytics", "pull_number": 17872, "instance_id": "ultralytics__ultralytics-17872", "issue_numbers": ["17850"], "base_commit": "21162bd870444550286983a601afbfb142f4c198", "patch": "diff --git a/ultralytics/engine/predictor.py b/ultralytics/engine/predictor.py\nindex c28e1895d07..c5250166e9e 100644\n--- a/ultralytics/engine/predictor.py\n+++ b/ultralytics/engine/predictor.py\n@@ -155,7 +155,7 @@ def pre_transform(self, im):\n         same_shapes = len({x.shape for x in im}) == 1\n         letterbox = LetterBox(\n             self.imgsz,\n-            auto=same_shapes and (self.model.pt or getattr(self.model, \"dynamic\", False)),\n+            auto=same_shapes and (self.model.pt or (getattr(self.model, \"dynamic\", False) and not self.model.imx)),\n             stride=self.model.stride,\n         )\n         return [letterbox(image=x) for x in im]\n", "test_patch": "", "problem_statement": "Imx500 usage example error\n### Search before asking\n\n- [X] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\nExport\n\n### Bug\n\nI encountered an error when running the example code from the [Sony IMX500 usage examples](https://docs.ultralytics.com/integrations/sony-imx500/#usage-examples). The image is resized to 480x640 instead of the expected 640x640, despite both the ONNX model input and the packerOut description specifying a 640x640 input.\r\n\r\nThe model is exported successfully, but unable to run the inference. \r\n\r\n\r\n```\r\nExport complete (298.0s)\r\nResults saved to /home/magi/mido/ultralytics\r\nPredict:         yolo predict task=detect model=yolov8n_imx_model imgsz=640 int8 \r\nValidate:        yolo val task=detect model=yolov8n_imx_model imgsz=640 data=coco.yaml int8 \r\nVisualize:       https://netron.app\r\nWARNING \u26a0\ufe0f Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\r\nLoading yolov8n_imx_model for ONNX Runtime inference...\r\nPreferring ONNX Runtime AzureExecutionProvider\r\nLoading yolov8n_imx_model/yolov8n_imx.onnx for ONNX IMX inference...\r\n\r\nFound https://ultralytics.com/images/bus.jpg locally at bus.jpg\r\nTraceback (most recent call last):\r\n  File \"/home/magi/mido/ultralytics/yolo_v8_playground.py\", line 13, in <module>\r\n    results = imx_model(\"https://ultralytics.com/images/bus.jpg\")\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/magi/mido/ultralytics/.venv/lib/python3.11/site-packages/ultralytics/engine/model.py\", line 176, in __call__\r\n    return self.predict(source, stream, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/magi/mido/ultralytics/.venv/lib/python3.11/site-packages/ultralytics/engine/model.py\", line 554, in predict\r\n    return self.predictor.predict_cli(source=source) if is_cli else self.predictor(source=source, stream=stream)\r\n                                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/magi/mido/ultralytics/.venv/lib/python3.11/site-packages/ultralytics/engine/predictor.py\", line 173, in __call__\r\n    return list(self.stream_inference(source, model, *args, **kwargs))  # merge list of Result into one\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/magi/mido/ultralytics/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\r\n    response = gen.send(None)\r\n               ^^^^^^^^^^^^^^\r\n  File \"/home/magi/mido/ultralytics/.venv/lib/python3.11/site-packages/ultralytics/engine/predictor.py\", line 259, in stream_inference\r\n    preds = self.inference(im, *args, **kwargs)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/magi/mido/ultralytics/.venv/lib/python3.11/site-packages/ultralytics/engine/predictor.py\", line 143, in inference\r\n    return self.model(im, augment=self.args.augment, visualize=visualize, embed=self.args.embed, *args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/magi/mido/ultralytics/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/magi/mido/ultralytics/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/magi/mido/ultralytics/.venv/lib/python3.11/site-packages/ultralytics/nn/autobackend.py\", line 542, in forward\r\n    y = self.session.run(self.output_names, {self.session.get_inputs()[0].name: im})\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/magi/mido/ultralytics/.venv/lib/python3.11/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\", line 220, in run\r\n    return self._sess.run(output_names, input_feed, run_options)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nonnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: input for the following indices\r\n index: 3 Got: 480 Expected: 640\r\n Please fix either the inputs/outputs or the model.\r\n```\r\n\r\nI tested resizing the image to 640x640 before inference, and it worked properly. However, I assume that the inference call should handle the resizing automatically without requiring manual adjustment beforehand.\n\n### Environment\n\nUltralytics 8.3.37 \ud83d\ude80 Python-3.11.10 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce GTX 1650 Ti, 3906MiB)\r\nSetup complete \u2705 (12 CPUs, 31.0 GB RAM, 444.7/913.8 GB disk)\r\n\r\nOS                  Linux-6.8.0-49-generic-x86_64-with-glibc2.35\r\nEnvironment         Linux\r\nPython              3.11.10\r\nInstall             pip\r\nRAM                 30.97 GB\r\nDisk                444.7/913.8 GB\r\nCPU                 Intel Core(TM) i7-10750H 2.60GHz\r\nCPU count           12\r\nGPU                 NVIDIA GeForce GTX 1650 Ti, 3906MiB\r\nGPU count           1\r\nCUDA                12.4\r\n\r\nnumpy               \u2705 1.26.4>=1.23.0\r\nnumpy               \u2705 1.26.4<2.0.0; sys_platform == \"darwin\"\r\nmatplotlib          \u2705 3.9.2>=3.3.0\r\nopencv-python       \u2705 4.10.0.84>=4.6.0\r\npillow              \u2705 11.0.0>=7.1.2\r\npyyaml              \u2705 6.0.2>=5.3.1\r\nrequests            \u2705 2.32.3>=2.23.0\r\nscipy               \u2705 1.14.1>=1.4.1\r\ntorch               \u2705 2.5.1>=1.8.0\r\ntorch               \u2705 2.5.1!=2.4.0,>=1.8.0; sys_platform == \"win32\"\r\ntorchvision         \u2705 0.20.1>=0.9.0\r\ntqdm                \u2705 4.67.1>=4.64.0\r\npsutil              \u2705 6.1.0\r\npy-cpuinfo          \u2705 9.0.0\r\npandas              \u2705 2.2.3>=1.1.4\r\nseaborn             \u2705 0.13.2>=0.11.0\r\nultralytics-thop    \u2705 2.0.12>=2.0.0\r\n{'OS': 'Linux-6.8.0-49-generic-x86_64-with-glibc2.35', 'Environment': 'Linux', 'Python': '3.11.10', 'Install': 'pip', 'RAM': '30.97 GB', 'Disk': '444.7/913.8 GB', 'CPU': 'Intel Core(TM) i7-10750H 2.60GHz', 'CPU count': 12, 'GPU': 'NVIDIA GeForce GTX 1650 Ti, 3906MiB', 'GPU count': 1, 'CUDA': '12.4', 'Package Info': {'numpy': '\u2705 1.26.4<2.0.0; sys_platform == \"darwin\"', 'matplotlib': '\u2705 3.9.2>=3.3.0', 'opencv-python': '\u2705 4.10.0.84>=4.6.0', 'pillow': '\u2705 11.0.0>=7.1.2', 'pyyaml': '\u2705 6.0.2>=5.3.1', 'requests': '\u2705 2.32.3>=2.23.0', 'scipy': '\u2705 1.14.1>=1.4.1', 'torch': '\u2705 2.5.1!=2.4.0,>=1.8.0; sys_platform == \"win32\"', 'torchvision': '\u2705 0.20.1>=0.9.0', 'tqdm': '\u2705 4.67.1>=4.64.0', 'psutil': '\u2705 6.1.0', 'py-cpuinfo': '\u2705 9.0.0', 'pandas': '\u2705 2.2.3>=1.1.4', 'seaborn': '\u2705 0.13.2>=0.11.0', 'ultralytics-thop': '\u2705 2.0.12>=2.0.0'}}\n\n### Minimal Reproducible Example\n\n```python\r\nfrom ultralytics import YOLO\r\n\r\n# Load a YOLOv8n PyTorch model\r\nmodel = YOLO(\"yolov8n.pt\")\r\n\r\n# # # Export the model\r\nmodel.export(format=\"imx\")  # exports with PTQ quantization by default\r\n\r\n# Load the exported model\r\nimx_model = YOLO(\"yolov8n_imx_model\")\r\n\r\n# Run inference\r\nresults = imx_model(\"https://ultralytics.com/images/bus.jpg\")\r\n\r\n```\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!\n", "hints_text": "\ud83d\udc4b Hello @Magitoneu, thank you for your interest in Ultralytics \ud83d\ude80! We appreciate you taking the time to report this issue. Here\u2019s a quick guide to help us investigate this further:\n\nIt seems like you\u2019re experiencing an error related to image resizing when running the Sony IMX500 example. If this is indeed a \ud83d\udc1b Bug Report, we kindly request you to confirm the behavior by sharing a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/). This will help ensure we can reproduce and debug the issue effectively.\n\nIn the meantime, please ensure that you\u2019ve updated to the latest version of the `ultralytics` package and all related requirements. You can do so by running the following command in your terminal:\n\n```bash\npip install -U ultralytics\n```\n\nWe also recommend reviewing the [Sony IMX500 integration documentation](https://docs.ultralytics.com/integrations/sony-imx500/#usage-examples) to double-check the expected behavior of the example code. It\u2019s possible that some additional steps may be required for handling image preprocessing.\n\n## Resources\n\nFor troubleshooting tips and to learn more ways of using the library, visit the [Ultralytics Docs](https://docs.ultralytics.com). Additional examples for Python and CLI-based workflows are available to guide you:\n- [Python](https://docs.ultralytics.com/usage/python/)\n- [CLI](https://docs.ultralytics.com/usage/cli/)\n\nIf you\u2019re trying to improve your training results or explore new features, don\u2019t miss our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Join the Community\n\nFor real-time discussions with other Ultralytics users, join our [Discord](https://discord.com/invite/ultralytics) server \ud83c\udfa7. We also host conversations on [Discourse](https://community.ultralytics.com) and our [Subreddit](https://reddit.com/r/Ultralytics) for deeper discussions.\n\n## Verified Environments\n\nIf possible, try running your code in one of our tested environments for a consistent experience:\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. Check the [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are currently passing, verifying that core functionalities across different environments are operating correctly.\n\nThis is an automated response \ud83d\udca1. An Ultralytics engineer will review your report and reach out with further assistance soon. Thank you for helping us improve! \ud83d\ude0a\n@Magitoneu thank you for reporting this issue. It seems that the input dimensions mismatch is due to the image size not being adjusted automatically during inference with the IMX500 export. While resizing the input image to 640x640 manually resolves the issue, automatic resizing is currently not implemented for IMX500-exported models.\n\nAs a workaround, ensure your input images are pre-processed to 640x640 before inference. For improvements in this behavior, feel free to provide additional insights, or you can open a feature request. Let us know if you need further assistance!", "created_at": "2024-11-29T06:38:32Z"}
{"repo": "ultralytics/ultralytics", "pull_number": 17856, "instance_id": "ultralytics__ultralytics-17856", "issue_numbers": ["17855"], "base_commit": "c1554935e6568ec4b27fc0c25d7e36bb32aaa24c", "patch": "diff --git a/docs/en/macros/train-args.md b/docs/en/macros/train-args.md\nindex 924bd313456..e9e026e6402 100644\n--- a/docs/en/macros/train-args.md\n+++ b/docs/en/macros/train-args.md\n@@ -20,6 +20,7 @@\n | `seed`            | `0`      | Sets the random seed for training, ensuring reproducibility of results across runs with the same configurations.                                                                                                                                             |\n | `deterministic`   | `True`   | Forces deterministic algorithm use, ensuring reproducibility but may affect performance and speed due to the restriction on non-deterministic algorithms.                                                                                                    |\n | `single_cls`      | `False`  | Treats all classes in multi-class datasets as a single class during training. Useful for binary classification tasks or when focusing on object presence rather than classification.                                                                         |\n+| `classes`         | `None`   | Specifies a list of class IDs to train on. Useful for filtering out and focusing only on certain classes during training.                                                                                                                                    |\n | `rect`            | `False`  | Enables rectangular training, optimizing batch composition for minimal padding. Can improve efficiency and speed but may affect model accuracy.                                                                                                              |\n | `cos_lr`          | `False`  | Utilizes a cosine [learning rate](https://www.ultralytics.com/glossary/learning-rate) scheduler, adjusting the learning rate following a cosine curve over epochs. Helps in managing learning rate for better convergence.                                   |\n | `close_mosaic`    | `10`     | Disables mosaic [data augmentation](https://www.ultralytics.com/glossary/data-augmentation) in the last N epochs to stabilize training before completion. Setting to 0 disables this feature.                                                                |\n", "test_patch": "", "problem_statement": "How to ignore some class while training model ?\n### Search before asking\n\n- [X] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI have 14 class in my dataset. I want to ignore some class form this 14 class. How i can skip that classes while training new model.? \n\n### Additional\n\n_No response_\n", "hints_text": "\ud83d\udc4b Hello @jayprajapati-inventyv, thank you for your interest in Ultralytics \ud83d\ude80! We recommend a visit to the [Docs](https://docs.ultralytics.com) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nTo address your \u2753 question about ignoring specific classes during training: while this is not directly configurable in a single-step, there are approaches to achieve this by modifying the dataset or your training configuration. To better assist you, could you provide more details about your use case (e.g., how classes are labeled in your dataset, and how you'd like the remaining classes trained)? You can include examples or additional context so we can guide you more effectively.\n\nJoin the Ultralytics community where it suits you best:\n- For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) \ud83c\udfa7.\n- Prefer in-depth discussions? Check out [Discourse](https://community.ultralytics.com).\n- Or dive into threads on our [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge with the community.\n\n## Upgrade\nIf you haven\u2019t already, ensure you\u2019re using the latest version of the `ultralytics` package. This ensures access to the newest features, fixes, and enhancements. Update in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Additional Resources\nFor further insights and tips, check these resources:\n- [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/)\n- [Help with Dataset Preparation](https://docs.ultralytics.com/datasets/)\n\n## Environments\nFor the best experience, YOLO can be run in any of these verified environments:\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM ([GCP Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/))\n- **Amazon** Deep Learning AMI ([AWS Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/))\n- **Docker Image** ([Docker Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/)) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n\ud83d\udca1 This is an automated response, but an Ultralytics engineer will review your issue and provide assistance as soon as possible.\nYou can specify the classes to train using the `classes` argument.\n\n`model.train(classes=[0,3,5,8])`", "created_at": "2024-11-28T13:52:36Z"}
{"repo": "ultralytics/ultralytics", "pull_number": 17810, "instance_id": "ultralytics__ultralytics-17810", "issue_numbers": ["17796"], "base_commit": "d8c43874ae830a36d2adeac4a44a8ce5697e972c", "patch": "diff --git a/ultralytics/utils/ops.py b/ultralytics/utils/ops.py\nindex 25e83c61c3a..ac53546ed1b 100644\n--- a/ultralytics/utils/ops.py\n+++ b/ultralytics/utils/ops.py\n@@ -75,9 +75,8 @@ def segment2box(segment, width=640, height=640):\n         (np.ndarray): the minimum and maximum x and y values of the segment.\n     \"\"\"\n     x, y = segment.T  # segment xy\n-    inside = (x >= 0) & (y >= 0) & (x <= width) & (y <= height)\n-    x = x[inside]\n-    y = y[inside]\n+    x = x.clip(0, width)\n+    y = y.clip(0, height)\n     return (\n         np.array([x.min(), y.min(), x.max(), y.max()], dtype=segment.dtype)\n         if any(x)\n", "test_patch": "", "problem_statement": "Training labels not applied properly to training data\n### Search before asking\r\n\r\n- [X] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\r\n\r\n\r\n### Ultralytics YOLO Component\r\n\r\nTrain\r\n\r\n### Bug\r\n\r\n# Bug\r\nLabels are not included in the generated train_batch**X**.jpg images during training of a segmentation model.\r\nCode to reproduce at bottom of section including the example training data.\r\n\r\n## Likely cause of bug\r\nI am not familiar with how the training label images are generated, however I highly suspect the issue is that if there are no points that define the polygon (label) in the image. This is caused when Yolo performs augmentation such as crop, resize, stretch, etc as it can morph the label such that all points defining the label are outside the image. This causes the mask to encompress up to the entire image but still not be included\r\n### I do not know if this affects anything other than segmentation!\r\n### This may actually affect the training data itself and not just the generated image examples, but I am not sure!\r\n\r\n## Examples\r\n- All white parts of the images are included in the label, thus if they are unlabelled the bug has occured\r\n![train_batch41](https://github.com/user-attachments/assets/ff8243c4-badb-4ea9-a5c0-64b9c28fbef6)\r\n![train_batch42](https://github.com/user-attachments/assets/17895e1b-a967-4c6d-8a18-39b59962893d)\r\n\r\n### Code to reproduce, instuctions in other section\r\n[GitIssues.zip](https://github.com/user-attachments/files/17916419/GitIssues.zip)\r\n\r\n\r\n### Environment\r\n\r\n```\r\nUltralytics 8.3.29 \ud83d\ude80 Python-3.10.12 torch-2.4.1+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\r\nSetup complete \u2705 (32 CPUs, 15.5 GB RAM, 23.5/251.0 GB disk)\r\n\r\nOS                  Linux-5.10.102.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\r\nEnvironment         Linux\r\nPython              3.10.12\r\nInstall             pip\r\nRAM                 15.47 GB\r\nDisk                23.5/251.0 GB\r\nCPU                 13th Gen Intel Core(TM) i9-13900\r\nCPU count           32\r\nGPU                 NVIDIA GeForce RTX 4090, 24564MiB\r\nGPU count           1\r\nCUDA                12.1\r\n\r\nnumpy               \u2705 2.1.2>=1.23.0\r\nmatplotlib          \u2705 3.9.2>=3.3.0\r\nopencv-python       \u2705 4.10.0.84>=4.6.0\r\npillow              \u2705 10.4.0>=7.1.2\r\npyyaml              \u2705 5.4.1>=5.3.1\r\nrequests            \u2705 2.32.3>=2.23.0\r\nscipy               \u2705 1.14.1>=1.4.1\r\ntorch               \u2705 2.4.1>=1.8.0\r\ntorchvision         \u2705 0.19.1>=0.9.0\r\ntqdm                \u2705 4.66.5>=4.64.0\r\npsutil              \u2705 6.0.0\r\npy-cpuinfo          \u2705 9.0.0\r\npandas              \u2705 2.2.3>=1.1.4\r\nseaborn             \u2705 0.13.2>=0.11.0\r\nultralytics-thop    \u2705 2.0.11>=2.0.0\r\nnumpy               \u2705 2.1.2<2.0.0; sys_platform == \"darwin\"\r\ntorch               \u2705 2.4.1!=2.4.0,>=1.8.0; sys_platform == \"win32\"\r\n```\r\n\r\n### Minimal Reproducible Example\r\n\r\n# How to reproduce\r\n1. Download & Extract provided training images, config (.yaml) and test_yolo.py file \r\n2. Edit .yaml file such that the folder path is correct\r\n3. Run test_yolo.py \r\n4. Examine the generated train_batch**X**.jpg images to see if the bug occured (You may need to train more than once)\r\n\r\n## What to look for\r\n- Any part that is white is labelled, so if any white pixels are unlabelled this bug has occured\r\n\r\n### Examples\r\n![train_batch0](https://github.com/user-attachments/assets/fe7f5b3f-1b00-4004-beb1-a50b5d5413b0)\r\n- In this case the bottom left image is clearly white, but unlabelled\r\n\r\n![train_batch2](https://github.com/user-attachments/assets/25cd0a90-8e46-48e8-ba99-0d15cf620719)\r\n- Top right image does has white, but it isn't labelled\r\n\r\n\r\n### Additional\r\n\r\n_No response_\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [ ] Yes I'd like to help by submitting a PR!\n", "hints_text": "\ud83d\udc4b Hello @TheOfficialOzone, thank you for bringing this to our attention \ud83d\ude80! We understand that you're encountering an issue with labels not being applied correctly during the training of a segmentation model on the Ultralytics repository.\n\nFor us to assist you effectively, please ensure that you've provided a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) if it's not already included in your report. This will help us understand and address the issue more efficiently. It seems like you've already attached a code example and some test images, which is great!\n\n\ud83d\udcc4 In the meantime, we suggest ensuring all your dependencies are up-to-date. Upgrade to the latest `ultralytics` package, including all requirements, within a Python >=3.8 environment using PyTorch >=1.8 to see if the issue persists:\n\n```bash\npip install -U ultralytics\n```\n\nFor further tips and tricks regarding custom training, please refer to our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin our Ultralytics community for real-time support or discussions:\n\n- Head over to [Discord](https://ultralytics.com/discord) for chat support \ud83c\udfa7\n- Visit [Discourse](https://community.ultralytics.com) for deeper discussions\n- Share experiences or get insightful feedback on our [Subreddit](https://reddit.com/r/ultralytics)\n\nFinally, an Ultralytics engineer will review the details of your issue soon and follow up with you for additional help. Thank you for your patience and cooperation!\n\n## Environments\n\nIn case you wish to shift your work to a more verified environment, you might consider:\n\n- **Notebooks** with free GPU access: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n\nFor more details on different environments, please refer to the [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/), [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/), or the [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/).\n\nWe appreciate your engagement with the Ultralytics repository and hope to resolve your issue soon! \ud83c\udf1f\nThis issue persists after running `pip install -U ultralytics`.\r\nThe version that was upgraded to was ultralytics 8.3.37.\n@TheOfficialOzone Thanks for reporting! I'm able to reproduce this with our dataset. I'll look into it!", "created_at": "2024-11-26T14:44:25Z"}
{"repo": "ultralytics/ultralytics", "pull_number": 17728, "instance_id": "ultralytics__ultralytics-17728", "issue_numbers": ["17727"], "base_commit": "426879d80d49d0180b525c4fc2484772f9f6f8cc", "patch": "diff --git a/ultralytics/data/augment.py b/ultralytics/data/augment.py\nindex d092e3c3703..bd821de28de 100644\n--- a/ultralytics/data/augment.py\n+++ b/ultralytics/data/augment.py\n@@ -1591,7 +1591,7 @@ def __call__(self, labels=None, image=None):\n             labels[\"ratio_pad\"] = (labels[\"ratio_pad\"], (left, top))  # for evaluation\n \n         if len(labels):\n-            labels = self._update_labels(labels, ratio, dw, dh)\n+            labels = self._update_labels(labels, ratio, left, top)\n             labels[\"img\"] = img\n             labels[\"resized_shape\"] = new_shape\n             return labels\n", "test_patch": "", "problem_statement": "Significant mAP Drop When Using Bottom-Right Padding Instead of Center Padding in YOLOv8 Training\n### Search before asking\r\n\r\n- [X] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\r\n\r\n\r\n### Question\r\n\r\nHi, I'm training a YOLOv8 model on the same dataset, but noticed a significant difference in mAP when changing the padding strategy.\r\n\r\n    When using center padding (default), the mAP@50 after the first epoch is around 0.88.\r\n    When using bottom-right padding instead, the mAP@50 drops to 0.0001.\r\n\r\nI ensured that:\r\n\r\n    The same data augmentation and other settings were used in both cases.\r\n\r\nHowever, the bottom-right padding leads to poor performance. What could be causing such a drastic performance drop? Is it related to padding affecting feature distribution, anchor design, or augmentation strategies? Any suggestions for improving performance in this case would be appreciated!\r\n\r\nThanks!\r\nI changed \"center=True\" to \"center=False\",augment.py line1500\r\n![2024-11-23 16-09-01 \u7684\u5c4f\u5e55\u622a\u56fe](https://github.com/user-attachments/assets/b2c6c27d-2518-45cf-819b-40e6e6de87ce)\r\n![2024-11-23 16-09-40 \u7684\u5c4f\u5e55\u622a\u56fe](https://github.com/user-attachments/assets/fa10d8b8-2a61-4212-b709-2c213133bac4)\r\n![2024-11-23 16-09-29 \u7684\u5c4f\u5e55\u622a\u56fe](https://github.com/user-attachments/assets/2b2c9fd2-f53d-47ca-977f-7db6cd00f1c8)\r\n\r\n\r\n### Additional\r\n\r\n_No response_\n", "hints_text": "\ud83d\udc4b Hello @Gebbap, thank you for bringing your findings to the Ultralytics community's attention \ud83d\ude80!\n\nWe recommend checking out our [Docs](https://docs.ultralytics.com), where you can find comprehensive information on [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage, which may offer some insights into your padding strategy question.\n\nGiven this is a deep technical question related to padding strategies, we would appreciate if you could provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/). This will greatly assist our team in diagnosing and addressing the issue more effectively.\n\nMeanwhile, please ensure that you're running the most up-to-date version of the `ultralytics` package. You can upgrade using the following command:\n\n```bash\npip install -U ultralytics\n```\n\nThis ensures that any recent fixes or improvements are integrated into your environment.\n\nFor additional support and insights from both engineers and experienced community members, feel free to join the conversation on our [Discord](https://ultralytics.com/discord) \ud83c\udfa7, or start a discussion on [Discourse](https://community.ultralytics.com) or our [Subreddit](https://reddit.com/r/ultralytics).\n\nAn Ultralytics engineer will soon review your question to provide further assistance. Thank you for your patience and support \ud83d\udc4d!\n\n## Environments\n\nYOLO can be executed in various verified environments, which come pre-installed with all dependencies, including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nA green badge indicates all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are passing. CI tests ensure the correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and upon each commit.\nDid you check if the training labels are correct in the plots?\nyes,it is correct.two results used the same dateset and labels\nI checked the labels after setting `center=False` and they're wrong. So this modification is incorrect and breaks the training labels which is why you're getting low scores.\n\n![train_batch0.jpg](https://github.com/user-attachments/assets/7ec598d9-8140-4dd7-bd7f-fa2764061ce9)\n\n\nThank you! Simply modifying the value of \"center\" only changes the position of the plot but doesn\u2019t adjust the labels correctly. How can I change the padding method? Should I modify the source code, or is there an official way to achieve this?", "created_at": "2024-11-23T11:08:18Z"}
{"repo": "ultralytics/ultralytics", "pull_number": 17544, "instance_id": "ultralytics__ultralytics-17544", "issue_numbers": ["17543"], "base_commit": "a132920476b2d38bdd58c7a232888f425f476977", "patch": "diff --git a/ultralytics/utils/callbacks/wb.py b/ultralytics/utils/callbacks/wb.py\nindex b82b8d85ec3..22bbc347566 100644\n--- a/ultralytics/utils/callbacks/wb.py\n+++ b/ultralytics/utils/callbacks/wb.py\n@@ -138,7 +138,7 @@ def on_train_end(trainer):\n         art.add_file(trainer.best)\n         wb.run.log_artifact(art, aliases=[\"best\"])\n     # Check if we actually have plots to save\n-    if trainer.args.plots:\n+    if trainer.args.plots and hasattr(trainer.validator.metrics, \"curves_results\"):\n         for curve_name, curve_values in zip(trainer.validator.metrics.curves, trainer.validator.metrics.curves_results):\n             x, y, x_title, y_title = curve_values\n             _plot_curve(\n", "test_patch": "", "problem_statement": "wandb callback reporting fails if no positive examples in validator\n### Search before asking\n\n- [X] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\nPredict\n\n### Bug\n\nWhen using the `wandb` callback, the following error occurs if there are no positive examples:\r\n```\r\nTraceback (most recent call last):\r\n  File \"pipeline.py\", line 97, in <module>\r\n    main()\r\n  File \"pipeline.py\", line 84, in main\r\n    output_path = train_stage(stage_config)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"train_new.py\", line 57, in train_stage\r\n    results = model.train(**train_args)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"site-packages/ultralytics/engine/model.py\", line 802, in train\r\n    self.trainer.train()\r\n  File \"site-packages/ultralytics/engine/trainer.py\", line 207, in train\r\n    self._do_train(world_size)\r\n  File \"site-packages/ultralytics/engine/trainer.py\", line 477, in _do_train\r\n    self.run_callbacks(\"on_train_end\")\r\n  File \"site-packages/ultralytics/engine/trainer.py\", line 168, in run_callbacks\r\n    callback(self)\r\n  File \"site-packages/ultralytics/utils/callbacks/wb.py\", line 141, in on_train_end\r\n    if trainer.args.plots and trainer.validator.metrics.curves and trainer.validator.metrics.curves_results:\r\n                                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"site-packages/ultralytics/utils/__init__.py\", line 221, in __getattr__\r\n    raise AttributeError(f\"'{name}' object has no attribute '{attr}'. See valid attributes below.\\n{self.__doc__}\")\r\nAttributeError: 'DetMetrics' object has no attribute 'curves_results'. See valid attributes below.\r\n    Utility class for computing detection metrics such as precision, recall, and mean average precision (mAP) of an\r\n    object detection model.\r\n    Args:\r\n        save_dir (Path): A path to the directory where the output plots will be saved. Defaults to current directory.\r\n        plot (bool): A flag that indicates whether to plot precision-recall curves for each class. Defaults to False.\r\n        on_plot (func): An optional callback to pass plots path and data when they are rendered. Defaults to None.\r\n        names (dict of str): A dict of strings that represents the names of the classes. Defaults to an empty tuple.\r\n    Attributes:\r\n        save_dir (Path): A path to the directory where the output plots will be saved.\r\n        plot (bool): A flag that indicates whether to plot the precision-recall curves for each class.\r\n        on_plot (func): An optional callback to pass plots path and data when they are rendered.\r\n        names (dict of str): A dict of strings that represents the names of the classes.\r\n        box (Metric): An instance of the Metric class for storing the results of the detection metrics.\r\n        speed (dict): A dictionary for storing the execution time of different parts of the detection process.\r\n    Methods:\r\n        process(tp, conf, pred_cls, target_cls): Updates the metric results with the latest batch of predictions.\r\n        keys: Returns a list of keys for accessing the computed detection metrics.\r\n        mean_results: Returns a list of mean values for the computed detection metrics.\r\n        class_result(i): Returns a list of values for the computed detection metrics for a specific class.\r\n        maps: Returns a dictionary of mean average precision (mAP) values for different IoU thresholds.\r\n        fitness: Computes the fitness score based on the computed detection metrics.\r\n        ap_class_index: Returns a list of class indices sorted by their average precision (AP) values.\r\n        results_dict: Returns a dictionary that maps detection metric keys to their computed values.\r\n        curves: TODO\r\n        curves_results: TODO\r\n```\n\n### Environment\n\n```\r\nUltralytics 8.3.30 \ud83d\ude80 Python-3.11.5 torch-2.1.2+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24111MiB)\r\nSetup complete \u2705 (32 CPUs, 251.5 GB RAM, 6725.0/7096.0 GB disk)\r\n\r\nOS                  Linux-6.5.0-45-generic-x86_64-with-glibc2.35\r\nEnvironment         Linux\r\nPython              3.11.5\r\nInstall             pip\r\nRAM                 251.52 GB\r\nDisk                6725.0/7096.0 GB\r\nCPU                 AMD Ryzen Threadripper PRO 5955WX 16-Cores\r\nCPU count           32\r\nGPU                 NVIDIA GeForce RTX 4090, 24111MiB\r\nGPU count           3\r\nCUDA                12.1\r\n\r\nnumpy               \u2705 1.24.3>=1.23.0\r\nmatplotlib          \u2705 3.7.2>=3.3.0\r\nopencv-python       \u2705 4.8.1.78>=4.6.0\r\npillow              \u2705 10.1.0>=7.1.2\r\npyyaml              \u2705 6.0>=5.3.1\r\nrequests            \u2705 2.31.0>=2.23.0\r\nscipy               \u2705 1.11.1>=1.4.1\r\ntorch               \u2705 2.1.2>=1.8.0\r\ntorchvision         \u2705 0.16.2>=0.9.0\r\ntqdm                \u2705 4.65.0>=4.64.0\r\npsutil              \u2705 5.9.0\r\npy-cpuinfo          \u2705 8.0.0\r\npandas              \u2705 2.0.3>=1.1.4\r\nseaborn             \u2705 0.12.2>=0.11.0\r\nultralytics-thop    \u2705 2.0.0>=2.0.0\r\nnumpy               \u2705 1.24.3<2.0.0; sys_platform == \"darwin\"\r\ntorch               \u2705 2.1.2!=2.4.0,>=1.8.0; sys_platform == \"win32\"\r\n```\n\n### Minimal Reproducible Example\n\n```\r\nimport wandb\r\nfrom ultralytics import YOLO\r\nfrom wandb.integration.ultralytics import add_wandb_callback\r\n\r\ndef train_yolo():\r\n    # Initialize wandb\r\n    wandb.init(\r\n        project=\"yolo-example\",\r\n        name=\"training-run\",\r\n        job_type=\"training\"\r\n    )\r\n    \r\n    # Initialize YOLO model\r\n    model = YOLO('yolov8n.yaml')  # or 'yolov8n.pt' for pretrained\r\n    \r\n    # Add wandb callback to log metrics\r\n    add_wandb_callback(model)\r\n    \r\n    # Train the model\r\n    results = model.train(\r\n        data='coco128_negative.yaml',  # path to data config file with negatives\r\n        epochs=3,\r\n        batch=16,\r\n        imgsz=640\r\n    )\r\n    \r\n    # Close wandb run\r\n    wandb.finish()\r\n\r\nif __name__ == \"__main__\":\r\n    train_yolo()\r\n```\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [X] Yes I'd like to help by submitting a PR!\n", "hints_text": "", "created_at": "2024-11-14T23:06:49Z"}
{"repo": "ultralytics/ultralytics", "pull_number": 17499, "instance_id": "ultralytics__ultralytics-17499", "issue_numbers": ["17497"], "base_commit": "496e6a3b8680e4ccd4f190e30841748aee2cb89c", "patch": "diff --git a/ultralytics/engine/results.py b/ultralytics/engine/results.py\nindex 029e4471e04..8de0a2e6a1c 100644\n--- a/ultralytics/engine/results.py\n+++ b/ultralytics/engine/results.py\n@@ -750,7 +750,7 @@ def save_crop(self, save_dir, file_name=Path(\"im.jpg\")):\n             save_one_box(\n                 d.xyxy,\n                 self.orig_img.copy(),\n-                file=Path(save_dir) / self.names[int(d.cls)] / f\"{Path(file_name)}.jpg\",\n+                file=Path(save_dir) / self.names[int(d.cls)] / Path(file_name).with_suffix(\".jpg\"),\n                 BGR=True,\n             )\n \n", "test_patch": "", "problem_statement": "Save_crop method from Results with default params results in double file extension\n### Search before asking\r\n\r\n- [X] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\r\n\r\n\r\n### Ultralytics YOLO Component\r\n\r\nPredict\r\n\r\n### Bug\r\n\r\nSave_crop method in the Results object already adds file extension to the saved image (.jpg) and default value is set as 'im.jpg' so when using default behaviour we get a file named **\"im.jpg.jpg\"**\r\nWith this few examples\r\n`results[0].save_crop(save_dir='../out')`\r\n`results[0].save_crop(save_dir='../out', file_name='img.png')`\r\n`results[0].save_crop(save_dir='../out', file_name='img')`\r\nHere are the outputs in a File Explorer, only one without double extension is when you add file_name without any extension\r\n![image](https://github.com/user-attachments/assets/873bb127-ca4e-439d-8f74-8aece55fd3fa)\r\n\r\n\r\n### Environment\r\n\r\nSetup complete \u2705 (8 CPUs, 15.9 GB RAM, 477.1/931.5 GB disk) \r\n\r\nOS                  Windows-10-10.0.17763-SP0\r\nEnvironment         Windows\r\nPython              3.10.15\r\nInstall             pip\r\nRAM                 15.94 GB\r\nDisk                477.1/931.5 GB\r\nCPU                 Intel Core(TM) i7-9700 3.00GHz\r\nCPU count           8\r\nGPU                 NVIDIA GeForce GTX 1660, 6144MiB\r\nGPU count           1\r\nCUDA                11.8\r\n\r\nnumpy               \u2705 1.26.3>=1.23.0                                                                                                                                                                                               \r\nmatplotlib          \u2705 3.9.2>=3.3.0 \r\nopencv-python       \u2705 4.10.0.84>=4.6.0                                                                                                                                                                                             \r\npillow              \u2705 10.2.0>=7.1.2                                                                                                                                                                                                \r\npyyaml              \u2705 6.0.2>=5.3.1                                                                                                                                                                                                 \r\nrequests            \u2705 2.32.3>=2.23.0                                                                                                                                                                                               \r\nscipy               \u2705 1.14.1>=1.4.1 \r\ntorch               \u2705 2.5.1+cu118>=1.8.0                                                                                                                                                                                           \r\ntorchvision         \u2705 0.20.1+cu118>=0.9.0                                                                                                                                                                                          \r\ntqdm                \u2705 4.67.0>=4.64.0                                                                                                                                                                                               \r\npsutil              \u2705 6.1.0                                                                                                                                                                                                        \r\npy-cpuinfo          \u2705 9.0.0                                                                                                                                                                                                        \r\npandas              \u2705 2.2.3>=1.1.4                                                                                                                                                                                                 \r\nseaborn             \u2705 0.13.2>=0.11.0                                                                                                                                                                                               \r\nultralytics-thop    \u2705 2.0.11>=2.0.0                                                                                                                                                                                                \r\nnumpy               \u2705 1.26.3<2.0.0; sys_platform == \"darwin\"                                                                                                                                                                       \r\ntorch               \u2705 2.5.1+cu118!=2.4.0,>=1.8.0; sys_platform == \"win32\"   \r\n\r\n### Minimal Reproducible Example\r\n\r\n```\r\nmodel = YOLO('../model/yolov8s.pt')\r\nresults = model.predict(frame, conf=0.5)\r\nresults[0].save_crop(save_dir='../out')\r\n```\r\n\r\n### Additional\r\n\r\n_No response_\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [X] Yes I'd like to help by submitting a PR!\n", "hints_text": "\ud83d\udc4b Hello @M3nxudo, thank you for bringing this to our attention! We're excited to assist you \ud83d\ude80 and appreciate your proactive approach to contribute with a PR.\n\nFor anyone facing similar issues, we highly recommend checking out our [Docs](https://docs.ultralytics.com) for guidance on both [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage.\n\nSince this seems to be a \ud83d\udc1b Bug Report, please ensure that the provided [minimal reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) accurately reflects the issue you're facing. This will help us diagnose the problem more efficiently.\n\nFor real-time help or to engage with our vibrant community, you can join us on [Discord](https://ultralytics.com/discord) \ud83c\udfa7. For more in-depth discussions, consider visiting our [Discourse](https://community.ultralytics.com) or share insights with members on our [Subreddit](https://reddit.com/r/ultralytics).\n\n## Upgrade\n\nMake sure you're using the latest version of the `ultralytics` package, along with all its [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml). Verify this in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYou can run YOLO in various verified environments that include all necessary dependencies, such as [CUDA](https://developer.nvidia.com/cuda), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM: Check the [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI: See the [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**: Refer to the [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nOur [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests run every 24 hours and on all commits to ensure smooth operation across different environments and verify correct functionality for all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/).\n\nThis is an automated response and an Ultralytics engineer will review your issue shortly. Thank you for your understanding and collaboration! \ud83c\udf1f\nEasiest solution would be to change the default value of the file_name param from  line 721 of results.py:\r\nhttps://github.com/ultralytics/ultralytics/blob/main/ultralytics/engine/results.py\r\n`def save_crop(self, save_dir, file_name=Path(\"im.jpg\")):` \r\nto\r\n **file_name=Path(\"im\")**\r\nsince on line 753 we already add the file extension:\r\n`file=Path(save_dir) / self.names[int(d.cls)] / f\"{Path(file_name)}.jpg\",`", "created_at": "2024-11-12T13:20:43Z"}
{"repo": "ultralytics/ultralytics", "pull_number": 17172, "instance_id": "ultralytics__ultralytics-17172", "issue_numbers": ["14821"], "base_commit": "da1bf9d79d4063eb9f389f9e7509cd936e21a9cc", "patch": "diff --git a/ultralytics/cfg/trackers/botsort.yaml b/ultralytics/cfg/trackers/botsort.yaml\nindex 01cebb64789..c15fbcd8952 100644\n--- a/ultralytics/cfg/trackers/botsort.yaml\n+++ b/ultralytics/cfg/trackers/botsort.yaml\n@@ -2,9 +2,9 @@\n # Default YOLO tracker settings for BoT-SORT tracker https://github.com/NirAharon/BoT-SORT\n \n tracker_type: botsort # tracker type, ['botsort', 'bytetrack']\n-track_high_thresh: 0.5 # threshold for the first association\n+track_high_thresh: 0.25 # threshold for the first association\n track_low_thresh: 0.1 # threshold for the second association\n-new_track_thresh: 0.6 # threshold for init new track if the detection does not match any tracks\n+new_track_thresh: 0.25 # threshold for init new track if the detection does not match any tracks\n track_buffer: 30 # buffer to calculate the time when to remove tracks\n match_thresh: 0.8 # threshold for matching tracks\n fuse_score: True # Whether to fuse confidence scores with the iou distances before matching\ndiff --git a/ultralytics/cfg/trackers/bytetrack.yaml b/ultralytics/cfg/trackers/bytetrack.yaml\nindex 49ab3f697bb..7cdec59b330 100644\n--- a/ultralytics/cfg/trackers/bytetrack.yaml\n+++ b/ultralytics/cfg/trackers/bytetrack.yaml\n@@ -2,9 +2,9 @@\n # Default YOLO tracker settings for ByteTrack tracker https://github.com/ifzhang/ByteTrack\n \n tracker_type: bytetrack # tracker type, ['botsort', 'bytetrack']\n-track_high_thresh: 0.5 # threshold for the first association\n+track_high_thresh: 0.25 # threshold for the first association\n track_low_thresh: 0.1 # threshold for the second association\n-new_track_thresh: 0.6 # threshold for init new track if the detection does not match any tracks\n+new_track_thresh: 0.25 # threshold for init new track if the detection does not match any tracks\n track_buffer: 30 # buffer to calculate the time when to remove tracks\n match_thresh: 0.8 # threshold for matching tracks\n fuse_score: True # Whether to fuse confidence scores with the iou distances before matching\n", "test_patch": "", "problem_statement": "track error\n### Search before asking\n\n- [X] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nIn a video with dense small objects, I can predict the objects normally using model.predict. But when I use model.track, only the first frame of multiple objects are correctly identified, and only a few objects in subsequent frames are identified. Please help, thx\r\n\r\nresults = model.track(file_path, show=True)\r\nresults = model.predict(file_path, show=True)\n\n### Additional\n\n_No response_\n", "hints_text": "\ud83d\udc4b Hello @peanutpaste, thank you for your interest in Ultralytics YOLOv8 \ud83d\ude80! We recommend a visit to the [Docs](https://docs.ultralytics.com) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a \ud83d\udc1b Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) to help us debug it.\n\nIf this is a custom training \u2753 Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\nJoin the vibrant [Ultralytics Discord](https://ultralytics.com/discord) \ud83c\udfa7 community for real-time conversations and collaborations. This platform offers a perfect space to inquire, showcase your work, and connect with fellow Ultralytics users.\n\n## Install\n\nPip install the `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/).\n\n```bash\npip install ultralytics\n```\n\n## Environments\n\nYOLOv8 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov8\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLOv8 [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\nLower the threshold:\r\n\r\nhttps://github.com/ultralytics/ultralytics/blob/80f699ae218f08d7f7846a7991c014456a81ae96/ultralytics/cfg/trackers/botsort.yaml#L7", "created_at": "2024-10-26T00:55:07Z"}
{"repo": "ultralytics/ultralytics", "pull_number": 17162, "instance_id": "ultralytics__ultralytics-17162", "issue_numbers": ["17148"], "base_commit": "f6c378835b0904b785cf19cdc492bdcfbd35bbf2", "patch": "diff --git a/ultralytics/utils/checks.py b/ultralytics/utils/checks.py\nindex 60faef2c4b4..9591d3dea20 100644\n--- a/ultralytics/utils/checks.py\n+++ b/ultralytics/utils/checks.py\n@@ -335,7 +335,7 @@ def check_font(font=\"Arial.ttf\"):\n         return file\n \n \n-def check_python(minimum: str = \"3.8.0\", hard: bool = True, verbose: bool = True) -> bool:\n+def check_python(minimum: str = \"3.8.0\", hard: bool = True, verbose: bool = False) -> bool:\n     \"\"\"\n     Check current python version against the required minimum version.\n \n", "test_patch": "", "problem_statement": "Python version error when using YOLOv8 on VSCode\n### Search before asking\n\n- [X] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nTrying to train my own model with YOLOv8, but even if I installed python 3.11, there will always be a warning: \"WARNING \u26a0\ufe0f Python>=3.10 is required, but Python==3.8.1 is currently installed\" \r\n\r\nI was using VSCode to run .ipynb file. I have chosen 3.11.3 kernel and checked that interpreter is also python 3.11, but it keep telling me that currently installed version is 3.8.1.\r\n![b91304aab00ba4223cebc7aa06c8db4](https://github.com/user-attachments/assets/8840d142-b010-4a6c-8377-049e5008eb58)\r\n\r\nThen I tried with my own environment in cmd terminal of VSCode, but even if the version it returned is 3.11.10, I was not able to start the training task, telling me that currently 3.8.1 is installed.\r\n![image](https://github.com/user-attachments/assets/c84002eb-62a7-4211-85c3-59913e072d5a)\r\n\r\nI was trying to do a class project, and I forked another github repository and edited it a bit. Below is how the .ipynb file looks like now:\r\n![image](https://github.com/user-attachments/assets/a1fa1d64-ac11-46f3-8d33-5ab355afa6e8)\r\n![image](https://github.com/user-attachments/assets/91088663-82ee-4c44-8290-8c357d86cd65)\r\n* output is:\r\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufde8\ufffd\ufffd\ufffd\ufffd\u0237\ufffd\ufffd\r\n\u03f5\u0373\ufffd\u04b2\ufffd\ufffd\ufffd\u05b8\ufffd\ufffd\ufffd\ufffd\u00b7\ufffd\ufffd\ufffd\ufffd\r\n\r\n[notice] A new release of pip available: 22.3.1 -> 24.2\r\n[notice] To update, run: python.exe -m pip install --upgrade pip\r\nRequirement already satisfied: roboflow in [e:\\software\\python311\\lib\\site-packages](file:///E:/software/python311/lib/site-packages) (1.1.48)\r\nRequirement already satisfied: certifi in [e:\\software\\python311\\lib\\site-packages](file:///E:/software/python311/lib/site-packages) (from roboflow) (2024.8.30)\r\nRequirement already satisfied: idna==3.7 in [e:\\software\\python311\\lib\\site-packages](file:///E:/software/python311/lib/site-packages) (from roboflow) (3.7)\r\nRequirement already satisfied: cycler in [e:\\software\\python311\\lib\\site-packages](file:///E:/software/python311/lib/site-packages) (from roboflow) (0.12.1)\r\nRequirement already satisfied: kiwisolver>=1.3.1 in [e:\\software\\python311\\lib\\site-packages](file:///E:/software/python311/lib/site-packages) (from roboflow) (1.4.5)\r\nRequirement already satisfied: matplotlib in [e:\\software\\python311\\lib\\site-packages](file:///E:/software/python311/lib/site-packages) (from roboflow) (3.8.0)\r\nRequirement already satisfied: numpy>=1.18.5 in [e:\\software\\python311\\lib\\site-packages](file:///E:/software/python311/lib/site-packages) (from roboflow) (1.25.2)\r\nRequirement already satisfied: opencv-python-headless==4.10.0.84 in [e:\\software\\python311\\lib\\site-packages](file:///E:/software/python311/lib/site-packages) (from roboflow) (4.10.0.84)\r\nRequirement already satisfied: Pillow>=7.1.2 in [e:\\software\\python311\\lib\\site-packages](file:///E:/software/python311/lib/site-packages) (from roboflow) (10.1.0)\r\nRequirement already satisfied: python-dateutil in [e:\\software\\python311\\lib\\site-packages](file:///E:/software/python311/lib/site-packages) (from roboflow) (2.8.2)\r\nRequirement already satisfied: python-dotenv in [e:\\software\\python311\\lib\\site-packages](file:///E:/software/python311/lib/site-packages) (from roboflow) (1.0.1)\r\nRequirement already satisfied: requests in [e:\\software\\python311\\lib\\site-packages](file:///E:/software/python311/lib/site-packages) (from roboflow) (2.32.3)\r\nRequirement already satisfied: six in [e:\\software\\python311\\lib\\site-packages](file:///E:/software/python311/lib/site-packages) (from roboflow) (1.16.0)\r\nRequirement already satisfied: urllib3>=1.26.6 in [e:\\software\\python311\\lib\\site-packages](file:///E:/software/python311/lib/site-packages) (from roboflow) (2.2.3)\r\nRequirement already satisfied: tqdm>=4.41.0 in [e:\\software\\python311\\lib\\site-packages](file:///E:/software/python311/lib/site-packages) (from roboflow) (4.66.5)\r\nRequirement already satisfied: PyYAML>=5.3.1 in [e:\\software\\python311\\lib\\site-packages](file:///E:/software/python311/lib/site-packages) (from roboflow) (6.0.2)\r\nRequirement already satisfied: requests-toolbelt in [e:\\software\\python311\\lib\\site-packages](file:///E:/software/python311/lib/site-packages) (from roboflow) (1.0.0)\r\nRequirement already satisfied: filetype in [e:\\software\\python311\\lib\\site-packages](file:///E:/software/python311/lib/site-packages) (from roboflow) (1.2.0)\r\nRequirement already satisfied: colorama in [e:\\software\\python311\\lib\\site-packages](file:///E:/software/python311/lib/site-packages) (from tqdm>=4.41.0->roboflow) (0.4.6)\r\nRequirement already satisfied: contourpy>=1.0.1 in [e:\\software\\python311\\lib\\site-packages](file:///E:/software/python311/lib/site-packages) (from matplotlib->roboflow) (1.1.1)\r\nRequirement already satisfied: fonttools>=4.22.0 in [e:\\software\\python311\\lib\\site-packages](file:///E:/software/python311/lib/site-packages) (from matplotlib->roboflow) (4.43.1)\r\nRequirement already satisfied: packaging>=20.0 in [e:\\software\\python311\\lib\\site-packages](file:///E:/software/python311/lib/site-packages) (from matplotlib->roboflow) (23.2)\r\nRequirement already satisfied: pyparsing>=2.3.1 in [e:\\software\\python311\\lib\\site-packages](file:///E:/software/python311/lib/site-packages) (from matplotlib->roboflow) (3.1.1)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in [e:\\software\\python311\\lib\\site-packages](file:///E:/software/python311/lib/site-packages) (from requests->roboflow) (3.4.0)\r\nNote: you may need to restart the kernel to use updated packages.\r\nloading Roboflow workspace...\r\nloading Roboflow project...\r\n\r\nVersion export complete for YOLOv8 format\r\n* end output\r\n![image](https://github.com/user-attachments/assets/7e35f564-126d-452a-bc5d-317a210fb98d)\r\nI encountered the problem at the first block in this pic, so I did not show other parts of the notebook.\r\n\r\nSo I was wondering if this has something to do with YOLO in this repository? Or maybe it is due to some wrong setting of my VSCode environment, but where should I go to so that I can fix the problem and finish my class project?\r\n\r\nPlease reply and help me if you have any clue, thanks!\n\n### Additional\n\n_No response_\n", "hints_text": "\ud83d\udc4b Hello @baili-b7ack, thank you for reaching out to us with your issue \ud83c\udf1f! This is an automated response, and an Ultralytics engineer will assist you shortly.\n\nWe recommend checking out our [Docs](https://docs.ultralytics.com) where you can find various [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples, which might help resolve your issue.\n\nIf this is a \ud83d\udc1b Bug Report, please ensure you provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) to help us effectively debug the problem.\n\nIn the meantime, ensure you are using the latest version of the Ultralytics package. You can upgrade by running the following command:\n\n```bash\npip install -U ultralytics\n```\n\nEnsure that your Python environment is set up correctly, as the warning indicates a potential issue with the Python version being detected. You can use a command like `which python` in your VSCode terminal to confirm the correct Python interpreter is being used.\n\nFor further assistance, feel free to connect with the Ultralytics community on [Discord](https://ultralytics.com/discord) \ud83c\udfa7 for real-time help, visit [Discourse](https://community.ultralytics.com) for more detailed discussions, or explore our [Subreddit](https://reddit.com/r/ultralytics) for community-driven insights.\n\n## Available Environments\n\nYou can also consider using other environments where everything is pre-configured:\n\n- Notebooks with free GPU: [Run on Gradient](https://console.paperspace.com/github/ultralytics/ultralytics) <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> [Open In Kaggle](https://www.kaggle.com/models/ultralytics/yolo11)\n- Google Cloud Deep Learning VM: [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- Amazon Deep Learning AMI: [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- Docker Image: [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\nCheck our continuous integration status here: <a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>. These tests ensure all YOLO features are operational.\n\nThank you for your patience and understanding! \ud83c\udf89\nCaused by https://github.com/ultralytics/ultralytics/commit/02d5c290e6199fa88b820f59190286f550750863\n> Caused by [02d5c29](https://github.com/ultralytics/ultralytics/commit/02d5c290e6199fa88b820f59190286f550750863)\r\n\r\nSo\u2026\u2026does it mean that temporarily I cannot use YOLO from ultralytics due to the edition of the code?\nYou can ignore the warning ", "created_at": "2024-10-25T10:33:51Z"}
{"repo": "ultralytics/ultralytics", "pull_number": 16790, "instance_id": "ultralytics__ultralytics-16790", "issue_numbers": ["16788"], "base_commit": "8154a27e9b332dd2dac7a107003462f0e20f6179", "patch": "diff --git a/ultralytics/nn/tasks.py b/ultralytics/nn/tasks.py\nindex 2cfaa4dd7c9..407021c82a6 100644\n--- a/ultralytics/nn/tasks.py\n+++ b/ultralytics/nn/tasks.py\n@@ -1061,10 +1061,10 @@ def parse_model(d, ch, verbose=True):  # model_dict, input_channels(3)\n \n         m_ = nn.Sequential(*(m(*args) for _ in range(n))) if n > 1 else m(*args)  # module\n         t = str(m)[8:-2].replace(\"__main__.\", \"\")  # module type\n-        m.np = sum(x.numel() for x in m_.parameters())  # number params\n+        m_.np = sum(x.numel() for x in m_.parameters())  # number params\n         m_.i, m_.f, m_.type = i, f, t  # attach index, 'from' index, type\n         if verbose:\n-            LOGGER.info(f\"{i:>3}{str(f):>20}{n_:>3}{m.np:10.0f}  {t:<45}{str(args):<30}\")  # print\n+            LOGGER.info(f\"{i:>3}{str(f):>20}{n_:>3}{m_.np:10.0f}  {t:<45}{str(args):<30}\")  # print\n         save.extend(x % i for x in ([f] if isinstance(f, int) else f) if x != -1)  # append to savelist\n         layers.append(m_)\n         if i == 0:\n", "test_patch": "", "problem_statement": "Why does the parameter attribute not get set to the Sequential version of the module\n### Search before asking\n\n- [X] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI am working with testing out various latency within networks and I was using the `_predict_one_layer()` function of the `BaseModel` when I noticed that the number of parameters when parsing the model is much larger than expected. Upon further investigation, I noticed that, while most attributes of the model's modules are set after the model is converted to a `nn.Sequential` in `parse_model()`, the number of parameters, `np` is tied to the original module. \r\n\r\nMy work revolves around calculations of latency per parameter per module type and therefore this parameter representation is important to me. I was wondering if there was a reason for this or if I should proceed with the larger parameter count or the smaller count?\r\n\r\nBelow I have added my code to run `_predict_once()` immediately following the model parsing and the outputs showing the difference in parameter count\r\n\r\n```\r\n# Parse model\r\nself.model, self.save = parse_model(deepcopy(self.yaml), ch=ch, verbose=verbose)  # model, savelist\r\nself._predict_once(x=torch.empty(1, 3, 640, 640), profile=True)\r\n```\r\n\r\n\r\n\r\n```\r\n                   from  n    params  module                                       arguments                     \r\n  0                  -1  1       464  znt.ultralytics.nn.modules.conv.Conv         [3, 16, 3, 2]                 \r\n  1                  -1  1      4672  znt.ultralytics.nn.modules.conv.Conv         [16, 32, 3, 2]                \r\n  2                  -1  1      7360  znt.ultralytics.nn.modules.block.C2f         [32, 32, 1, True]             \r\n  3                  -1  1     18560  znt.ultralytics.nn.modules.conv.Conv         [32, 64, 3, 2]                \r\n  4                  -1  2     49664  znt.ultralytics.nn.modules.block.C2f         [64, 64, 2, True]             \r\n  5                  -1  1     73984  znt.ultralytics.nn.modules.conv.Conv         [64, 128, 3, 2]               \r\n  6                  -1  2    197632  znt.ultralytics.nn.modules.block.C2f         [128, 128, 2, True]           \r\n  7                  -1  1    295424  znt.ultralytics.nn.modules.conv.Conv         [128, 256, 3, 2]              \r\n  8                  -1  1    460288  znt.ultralytics.nn.modules.block.C2f         [256, 256, 1, True]           \r\n  9                  -1  1    164608  znt.ultralytics.nn.modules.block.SPPF        [256, 256, 5]                 \r\n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \r\n 11             [-1, 6]  1         0  znt.ultralytics.nn.modules.conv.Concat       [1]                           \r\n 12                  -1  1    148224  znt.ultralytics.nn.modules.block.C2f         [384, 128, 1]                 \r\n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \r\n 14             [-1, 4]  1         0  znt.ultralytics.nn.modules.conv.Concat       [1]                           \r\n 15                  -1  1     37248  znt.ultralytics.nn.modules.block.C2f         [192, 64, 1]                  \r\n 16                  -1  1     36992  znt.ultralytics.nn.modules.conv.Conv         [64, 64, 3, 2]                \r\n 17            [-1, 12]  1         0  znt.ultralytics.nn.modules.conv.Concat       [1]                           \r\n 18                  -1  1    123648  znt.ultralytics.nn.modules.block.C2f         [192, 128, 1]                 \r\n 19                  -1  1    147712  znt.ultralytics.nn.modules.conv.Conv         [128, 128, 3, 2]              \r\n 20             [-1, 9]  1         0  znt.ultralytics.nn.modules.conv.Concat       [1]                           \r\n 21                  -1  1    493056  znt.ultralytics.nn.modules.block.C2f         [384, 256, 1]                 \r\n 22        [15, 18, 21]  1    751897  znt.ultralytics.nn.modules.head.Detect       [3, [64, 128, 256]]           \r\n time (ms)     GFLOPs     params  module\r\n      3.93       0.10     147712  znt.ultralytics.nn.modules.conv.Conv\r\n      3.22       0.24     147712  znt.ultralytics.nn.modules.conv.Conv\r\n      9.09       0.39     493056  znt.ultralytics.nn.modules.block.C2f\r\n      2.27       0.24     147712  znt.ultralytics.nn.modules.conv.Conv\r\n      7.79       0.64     493056  znt.ultralytics.nn.modules.block.C2f\r\n      1.86       0.24     147712  znt.ultralytics.nn.modules.conv.Conv\r\n      5.73       0.64     493056  znt.ultralytics.nn.modules.block.C2f\r\n      1.66       0.24     147712  znt.ultralytics.nn.modules.conv.Conv\r\n      3.05       0.37     493056  znt.ultralytics.nn.modules.block.C2f\r\n      4.04       0.13     164608  znt.ultralytics.nn.modules.block.SPPF\r\n      0.69       0.00          0  torch.nn.modules.upsampling.Upsample\r\n      0.05       0.00          0  znt.ultralytics.nn.modules.conv.Concat\r\n      4.49       0.48     493056  znt.ultralytics.nn.modules.block.C2f\r\n      0.38       0.00          0  torch.nn.modules.upsampling.Upsample\r\n      0.11       0.00          0  znt.ultralytics.nn.modules.conv.Concat\r\n      6.31       0.48     493056  znt.ultralytics.nn.modules.block.C2f\r\n      1.01       0.12     147712  znt.ultralytics.nn.modules.conv.Conv\r\n      0.03       0.00          0  znt.ultralytics.nn.modules.conv.Concat\r\n      3.92       0.40     493056  znt.ultralytics.nn.modules.block.C2f\r\n      0.88       0.12     147712  znt.ultralytics.nn.modules.conv.Conv\r\n      0.01       0.00          0  znt.ultralytics.nn.modules.conv.Concat\r\n      3.24       0.40     493056  znt.ultralytics.nn.modules.block.C2f\r\n     31.79       2.98     751897  znt.ultralytics.nn.modules.head.Detect\r\n     95.54 8.20         5894937            Total\r\nYOLOv8n summary: 225 layers, 3011433 parameters, 3011417 gradients, 8.2 GFLOPs\r\n```\n\n### Additional\n\n_No response_\n", "hints_text": "", "created_at": "2024-10-09T05:51:36Z"}
{"repo": "ultralytics/ultralytics", "pull_number": 16734, "instance_id": "ultralytics__ultralytics-16734", "issue_numbers": ["16659"], "base_commit": "d88e57f143d48f8e6a2f963abce1cf0dd2d76d66", "patch": "diff --git a/ultralytics/__init__.py b/ultralytics/__init__.py\nindex ce089ca6b5f..7ecb2c05afa 100644\n--- a/ultralytics/__init__.py\n+++ b/ultralytics/__init__.py\n@@ -1,6 +1,6 @@\n # Ultralytics YOLO \ud83d\ude80, AGPL-3.0 license\n \n-__version__ = \"8.3.6\"\n+__version__ = \"8.3.7\"\n \n import os\n \ndiff --git a/ultralytics/engine/model.py b/ultralytics/engine/model.py\nindex c4db53426ab..43b7ccfae3c 100644\n--- a/ultralytics/engine/model.py\n+++ b/ultralytics/engine/model.py\n@@ -544,6 +544,8 @@ def predict(\n \n         if not self.predictor:\n             self.predictor = predictor or self._smart_load(\"predictor\")(overrides=args, _callbacks=self.callbacks)\n+            if predictor:\n+                self.predictor.args = get_cfg(self.predictor.args, args)\n             self.predictor.setup_model(model=self.model, verbose=is_cli)\n         else:  # only update args if predictor is already setup\n             self.predictor.args = get_cfg(self.predictor.args, args)\n", "test_patch": "", "problem_statement": "Args in custom predictor\n### Search before asking\r\n\r\n- [X] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\r\n\r\n\r\n### Ultralytics YOLO Component\r\n\r\nPredict\r\n\r\n### Bug\r\n\r\nThe first time you call `model.predict()` with a custom predictor, the predictor uses the wrong arguments.\r\n\r\n### Environment\r\n\r\nUltralytics 8.3.0 \ud83d\ude80 Python-3.10.14 torch-2.4.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24037MiB)\r\nSetup complete \u2705 (32 CPUs, 62.7 GB RAM, 524.0/915.3 GB disk)\r\n\r\nOS                  Linux-6.8.0-45-generic-x86_64-with-glibc2.35\r\nEnvironment         Linux\r\nPython              3.10.14\r\nInstall             pip\r\nRAM                 62.71 GB\r\nCPU                 AMD Ryzen 9 5950X 16-Core Processor\r\nCUDA                12.4\r\n\r\nnumpy               \u2705 1.26.4<2.0.0,>=1.23.0\r\nmatplotlib          \u2705 3.9.2>=3.3.0\r\nopencv-python       \u2705 4.10.0>=4.6.0\r\npillow              \u2705 10.4.0>=7.1.2\r\npyyaml              \u2705 6.0.1>=5.3.1\r\nrequests            \u2705 2.32.3>=2.23.0\r\nscipy               \u2705 1.14.1>=1.4.1\r\ntorch               \u2705 2.4.1>=1.8.0\r\ntorchvision         \u2705 0.19.1>=0.9.0\r\ntqdm                \u2705 4.66.5>=4.64.0\r\npsutil              \u2705 6.0.0\r\npy-cpuinfo          \u2705 9.0.0\r\npandas              \u2705 2.2.3>=1.1.4\r\nseaborn             \u2705 0.13.2>=0.11.0\r\nultralytics-thop    \u2705 2.0.8>=2.0.0\r\ntorch               \u2705 2.4.1!=2.4.0,>=1.8.0; sys_platform == \"win32\"\r\n\r\n### Minimal Reproducible Example\r\n\r\n```python\r\nfrom ultralytics import YOLO\r\nfrom ultralytics.models.yolo.detect.predict import DetectionPredictor\r\nfrom ultralytics.utils import DEFAULT_CFG\r\n\r\nclass MyPredictor(DetectionPredictor):\r\n    def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):\r\n        super().__init__(cfg, overrides, _callbacks)\r\n\r\n    def postprocess(self, preds, img, orig_imgs):\r\n        # Do custom post-processing\r\n        return super().postprocess(preds, img, orig_imgs)\r\n\r\nimage = \"<path_to_image>\"\r\n\r\nmodel = YOLO(\"<path_to_weights>\")\r\npredictor = MyPredictor()\r\nresults = model.predict(image, predictor=predictor, imgsz=2048, conf=0.1)\r\nresults = model.predict(image, predictor=predictor, imgsz=2048, conf=0.1)\r\n```\r\n\r\nThis uses a wrong image size for the first prediction (448x640 instead of 1376x2048):\r\n\r\n```\r\nimage 1/1 XXX.JPG: 448x640 (no detections), 30.1ms\r\nSpeed: 5.1ms preprocess, 30.1ms inference, 14.8ms postprocess per image at shape (1, 3, 448, 640)\r\nResults saved to runs/detect/train2144\r\n\r\nimage 1/1 XXX.JPG: 1376x2048 2 Fs, 28.3ms\r\nSpeed: 15.9ms preprocess, 28.3ms inference, 74.5ms postprocess per image at shape (1, 3, 1376, 2048)\r\nultralytics.engine.results.Boxes object with attributes:\r\n```\r\n\r\nThis is caused by the following lines in `model.py`:\r\n\r\nhttps://github.com/ultralytics/ultralytics/blob/05fd3a3e739f6438e3a77e6fafab49d58cec7cf0/ultralytics/engine/model.py#L545-L551\r\n\r\nwhere the arguments are only updated the second time you call `model.predict` in case of a custom predictor. Is this done intentional? Adding\r\n\r\n```python\r\nif predictor is not None:\r\n    predictor.args = get_cfg(predictor.args, args)\r\n```\r\nbefore line 546 would solve this issue for custom predictors. I can make a pull request if needed.\r\n\r\n### Additional\r\n\r\n_No response_\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [X] Yes I'd like to help by submitting a PR!\n", "hints_text": "", "created_at": "2024-10-07T08:05:09Z"}
{"repo": "ultralytics/ultralytics", "pull_number": 16708, "instance_id": "ultralytics__ultralytics-16708", "issue_numbers": ["16704"], "base_commit": "e45e3b16ad2191b927ccea2cd788e7017232460d", "patch": "diff --git a/docs/en/datasets/pose/hand-keypoints.md b/docs/en/datasets/pose/hand-keypoints.md\nindex 86548a0233b..dd3c19b1a46 100644\n--- a/docs/en/datasets/pose/hand-keypoints.md\n+++ b/docs/en/datasets/pose/hand-keypoints.md\n@@ -8,7 +8,7 @@ keywords: Hand KeyPoints, pose estimation, dataset, keypoints, MediaPipe, YOLO,\n \n ## Introduction\n \n-The hand-keypoints dataset contains 26,768 images of hands annotated with keypoints, making it suitable for training models like Ultralytics YOLO for pose estimation tasks. The annotations were generated using the Google MediaPipe library, ensuring high accuracy and consistency, and the dataset is compatible [Ultralytics YOLO11](https://github.com/ultralytics/ultralytics) formats.\n+The hand-keypoints dataset contains 26,768 images of hands annotated with keypoints, making it suitable for training models like Ultralytics YOLO for pose estimation tasks. The annotations were generated using the Google MediaPipe library, ensuring high [accuracy](https://www.ultralytics.com/glossary/accuracy) and consistency, and the dataset is compatible [Ultralytics YOLO11](https://github.com/ultralytics/ultralytics) formats.\n \n ## Hand Landmarks\n \ndiff --git a/docs/en/guides/security-alarm-system.md b/docs/en/guides/security-alarm-system.md\nindex a9523dd61c1..e8562485715 100644\n--- a/docs/en/guides/security-alarm-system.md\n+++ b/docs/en/guides/security-alarm-system.md\n@@ -8,7 +8,7 @@ keywords: YOLO11, Security Alarm System, real-time object detection, Ultralytics\n \n <img src=\"https://github.com/ultralytics/docs/releases/download/0/security-alarm-system-ultralytics-yolov8.avif\" alt=\"Security Alarm System\">\n \n-The Security Alarm System Project utilizing Ultralytics YOLO11 integrates advanced [computer vision](https://www.ultralytics.com/glossary/computer-vision-cv) capabilities to enhance security measures. YOLO11, developed by Ultralytics, provides real-time object detection, allowing the system to identify and respond to potential security threats promptly. This project offers several advantages:\n+The Security Alarm System Project utilizing Ultralytics YOLO11 integrates advanced [computer vision](https://www.ultralytics.com/glossary/computer-vision-cv) capabilities to enhance security measures. YOLO11, developed by Ultralytics, provides real-time [object detection](https://www.ultralytics.com/glossary/object-detection), allowing the system to identify and respond to potential security threats promptly. This project offers several advantages:\n \n - **Real-time Detection:** YOLO11's efficiency enables the Security Alarm System to detect and respond to security incidents in real-time, minimizing response time.\n - **[Accuracy](https://www.ultralytics.com/glossary/accuracy):** YOLO11 is known for its accuracy in object detection, reducing false positives and enhancing the reliability of the security alarm system.\ndiff --git a/docs/en/guides/steps-of-a-cv-project.md b/docs/en/guides/steps-of-a-cv-project.md\nindex b0f03c1eac5..ca067547dad 100644\n--- a/docs/en/guides/steps-of-a-cv-project.md\n+++ b/docs/en/guides/steps-of-a-cv-project.md\n@@ -147,7 +147,7 @@ It's important to keep in mind that proper dataset management is vital for effic\n \n It's important to assess your model's performance using various metrics and refine it to improve [accuracy](https://www.ultralytics.com/glossary/accuracy). [Evaluating](../modes/val.md) helps identify areas where the model excels and where it may need improvement. Fine-tuning ensures the model is optimized for the best possible performance.\n \n-- **[Performance Metrics](./yolo-performance-metrics.md):** Use metrics like accuracy, [precision](https://www.ultralytics.com/glossary/precision), recall, and F1-score to evaluate your model's performance. These metrics provide insights into how well your model is making predictions.\n+- **[Performance Metrics](./yolo-performance-metrics.md):** Use metrics like accuracy, [precision](https://www.ultralytics.com/glossary/precision), [recall](https://www.ultralytics.com/glossary/recall), and F1-score to evaluate your model's performance. These metrics provide insights into how well your model is making predictions.\n - **[Hyperparameter Tuning](./hyperparameter-tuning.md):** Adjust hyperparameters to optimize model performance. Techniques like grid search or random search can help find the best hyperparameter values.\n \n - Fine-Tuning: Make small adjustments to the model architecture or training process to enhance performance. This might involve tweaking [learning rates](https://www.ultralytics.com/glossary/learning-rate), [batch sizes](https://www.ultralytics.com/glossary/batch-size), or other model parameters.\ndiff --git a/docs/en/integrations/weights-biases.md b/docs/en/integrations/weights-biases.md\nindex 9f2cbb2fa00..06e071bfa34 100644\n--- a/docs/en/integrations/weights-biases.md\n+++ b/docs/en/integrations/weights-biases.md\n@@ -6,7 +6,7 @@ keywords: YOLO11, Weights & Biases, model training, experiment tracking, Ultraly\n \n # Enhancing YOLO11 Experiment Tracking and Visualization with Weights & Biases\n \n-[Object detection](https://www.ultralytics.com/glossary/object-detection) models like [Ultralytics YOLO11](https://github.com/ultralytics/ultralytics) have become integral to many [computer vision](https://www.ultralytics.com/glossary/computer-vision-cv) applications. However, training, evaluating, and deploying these complex models introduces several challenges. Tracking key training metrics, comparing model variants, analyzing model behavior, and detecting issues require substantial instrumentation and experiment management.\n+[Object detection](https://www.ultralytics.com/glossary/object-detection) models like [Ultralytics YOLO11](https://github.com/ultralytics/ultralytics) have become integral to many [computer vision](https://www.ultralytics.com/glossary/computer-vision-cv) applications. However, training, evaluating, and deploying these complex models introduce several challenges. Tracking key training metrics, comparing model variants, analyzing model behavior, and detecting issues require significant instrumentation and experiment management.\n \n <p align=\"center\">\n   <br>\n@@ -19,7 +19,7 @@ keywords: YOLO11, Weights & Biases, model training, experiment tracking, Ultraly\n   <strong>Watch:</strong> How to use Ultralytics YOLO11 with Weights and Biases\n </p>\n \n-This guide showcases Ultralytics YOLO11 integration with Weights & Biases' for enhanced experiment tracking, model-checkpointing, and visualization of model performance. It also includes instructions for setting up the integration, training, fine-tuning, and visualizing results using Weights & Biases' interactive features.\n+This guide showcases Ultralytics YOLO11 integration with Weights & Biases for enhanced experiment tracking, model-checkpointing, and visualization of model performance. It also includes instructions for setting up the integration, training, fine-tuning, and visualizing results using Weights & Biases' interactive features.\n \n ## Weights & Biases\n \n@@ -42,8 +42,8 @@ To install the required packages, run:\n     === \"CLI\"\n \n         ```bash\n-        # Install the required packages for YOLO11 and Weights & Biases\n-        pip install --upgrade ultralytics==8.0.186 wandb\n+        # Install the required packages for Ultralytics YOLO and Weights & Biases\n+        pip install -U ultralytics wandb\n         ```\n \n For detailed instructions and best practices related to the installation process, be sure to check our [YOLO11 Installation guide](../quickstart.md). While installing the required packages for YOLO11, if you encounter any difficulties, consult our [Common Issues guide](../guides/yolo-common-issues.md) for solutions and tips.\n@@ -56,12 +56,20 @@ Start by initializing the Weights & Biases environment in your workspace. You ca\n \n !!! tip \"Initial SDK Setup\"\n \n+    === \"Python\"\n+\n+        ```python\n+        import wandb\n+\n+        # Initialize your Weights & Biases environment\n+        wandb.login(key=\"<API_KEY>\")\n+        ```\n+\n     === \"CLI\"\n \n         ```bash\n         # Initialize your Weights & Biases environment\n-        import wandb\n-        wandb.login()\n+        wandb login <API_KEY>\n         ```\n \n Navigate to the Weights & Biases authorization page to create and retrieve your API key. Use this key to authenticate your environment with W&B.\n@@ -75,50 +83,39 @@ Before diving into the usage instructions for YOLO11 model training with Weights\n     === \"Python\"\n \n         ```python\n-        import wandb\n-        from wandb.integration.ultralytics import add_wandb_callback\n-\n         from ultralytics import YOLO\n \n-        # Initialize a Weights & Biases run\n-        wandb.init(project=\"ultralytics\", job_type=\"training\")\n-\n         # Load a YOLO model\n         model = YOLO(\"yolo11n.pt\")\n \n-        # Add W&B Callback for Ultralytics\n-        add_wandb_callback(model, enable_model_checkpointing=True)\n-\n         # Train and Fine-Tune the Model\n-        model.train(project=\"ultralytics\", data=\"coco8.yaml\", epochs=5, imgsz=640)\n-\n-        # Validate the Model\n-        model.val()\n-\n-        # Perform Inference and Log Results\n-        model([\"path/to/image1\", \"path/to/image2\"])\n-\n-        # Finalize the W&B Run\n-        wandb.finish()\n+        model.train(data=\"coco8.yaml\", epochs=5, project=\"ultralytics\", name=\"yolo11n\")\n         ```\n \n-### Understanding the Code\n-\n-Let's understand the steps showcased in the usage code snippet above.\n-\n-- **Step 1: Initialize a Weights & Biases Run**: Start by initializing a Weights & Biases run, specifying the project name and the job type. This run will track and manage the training and validation processes of your model.\n+    === \"CLI\"\n \n-- **Step 2: Define the YOLO11 Model and Dataset**: Specify the model variant and the dataset you wish to use. The YOLO model is then initialized with the specified model file.\n+        ```bash\n+        # Train a YOLO11 model with Weights & Biases\n+        yolo train data=coco8.yaml epochs=5 project=ultralytics name=yolo11n\n+        ```\n \n-- **Step 3: Add Weights & Biases Callback for Ultralytics**: This step is crucial as it enables the automatic logging of training metrics and validation results to Weights & Biases, providing a detailed view of the model's performance.\n+### W&B Arguments\n \n-- **Step 4: Train and Fine-Tune the Model**: Begin training the model with the specified dataset, number of epochs, and image size. The training process includes logging of metrics and predictions at the end of each [epoch](https://www.ultralytics.com/glossary/epoch), offering a comprehensive view of the model's learning progress.\n+| Argument | Default | Description                                                                                                        |\n+| -------- | ------- | ------------------------------------------------------------------------------------------------------------------ |\n+| project  | `None`  | Specifies the name of the project logged locally and in W&B. This way you can group multiple runs together.        |\n+| name     | `None`  | The name of the training run. This determines the name used to create subfolders and the name used for W&B logging |\n \n-- **Step 5: Validate the Model**: After training, the model is validated. This step is crucial for assessing the model's performance on unseen data and ensuring its generalizability.\n+!!! Tip \"Enable or Disable Weights & Biases\"\n+If you want to enable or disable Weights & Biases logging, you can use the `wandb` command. By default, Weights & Biases logging is enabled.\n \n-- **Step 6: Perform Inference and Log Results**: The model performs predictions on specified images. These predictions, along with visual overlays and insights, are automatically logged in a W&B Table for interactive exploration.\n+    ```bash\n+    # Enable Weights & Biases logging\n+    wandb enabled\n \n-- **Step 7: Finalize the W&B Run**: This step marks the end of data logging and saves the final state of your model's training and validation process in the W&B dashboard.\n+    # Disable Weights & Biases logging\n+    wandb disabled\n+    ```\n \n ### Understanding the Output\n \n@@ -126,7 +123,7 @@ Upon running the usage code snippet above, you can expect the following key outp\n \n - The setup of a new run with its unique ID, indicating the start of the training process.\n - A concise summary of the model's structure, including the number of layers and parameters.\n-- Regular updates on important metrics such as box loss, cls loss, dfl loss, [precision](https://www.ultralytics.com/glossary/precision), [recall](https://www.ultralytics.com/glossary/recall), and mAP scores during each training epoch.\n+- Regular updates on important metrics such as box loss, cls loss, dfl loss, [precision](https://www.ultralytics.com/glossary/precision), [recall](https://www.ultralytics.com/glossary/recall), and mAP scores during each training [epoch](https://www.ultralytics.com/glossary/epoch).\n - At the end of training, detailed metrics including the model's inference speed, and overall [accuracy](https://www.ultralytics.com/glossary/accuracy) metrics are displayed.\n - Links to the Weights & Biases dashboard for in-depth analysis and visualization of the training process, along with information on local log file locations.\n \n@@ -138,7 +135,7 @@ After running the usage code snippet, you can access the Weights & Biases (W&B)\n \n - **Real-Time Metrics Tracking**: Observe metrics like loss, accuracy, and validation scores as they evolve during the training, offering immediate insights for model tuning. [See how experiments are tracked using Weights & Biases](https://imgur.com/D6NVnmN).\n \n-- **Hyperparameter Optimization**: Weights & Biases aids in fine-tuning critical parameters such as [learning rate](https://www.ultralytics.com/glossary/learning-rate), batch size, and more, enhancing the performance of YOLO11.\n+- **Hyperparameter Optimization**: Weights & Biases aids in fine-tuning critical parameters such as [learning rate](https://www.ultralytics.com/glossary/learning-rate), [batch size](https://www.ultralytics.com/glossary/batch-size), and more, enhancing the performance of YOLO11.\n \n - **Comparative Analysis**: The platform allows side-by-side comparisons of different training runs, essential for assessing the impact of various model configurations.\n \n@@ -154,7 +151,7 @@ By using these features, you can effectively track, analyze, and optimize your Y\n \n ## Summary\n \n-This guide helped you explore Ultralytics' YOLO11 integration with Weights & Biases. It illustrates the ability of this integration to efficiently track and visualize model training and prediction results.\n+This guide helped you explore the Ultralytics YOLO integration with Weights & Biases. It illustrates the ability of this integration to efficiently track and visualize model training and prediction results.\n \n For further details on usage, visit [Weights & Biases' official documentation](https://docs.wandb.ai/guides/integrations/ultralytics/).\n \n@@ -162,83 +159,83 @@ Also, be sure to check out the [Ultralytics integration guide page](../integrati\n \n ## FAQ\n \n-### How do I install the required packages for YOLO11 and Weights & Biases?\n+### How do I integrate Weights & Biases with Ultralytics YOLO11?\n \n-To install the required packages for YOLO11 and Weights & Biases, open your command line interface and run:\n+To integrate Weights & Biases with Ultralytics YOLO11:\n+\n+1. Install the required packages:\n \n ```bash\n-pip install --upgrade ultralytics==8.0.186 wandb\n+pip install -U ultralytics wandb\n ```\n \n-For further guidance on installation steps, refer to our [YOLO11 Installation guide](../quickstart.md). If you encounter issues, consult the [Common Issues guide](../guides/yolo-common-issues.md) for troubleshooting tips.\n+2. Log in to your Weights & Biases account:\n \n-### What are the benefits of integrating Ultralytics YOLO11 with Weights & Biases?\n+```python\n+import wandb\n \n-Integrating Ultralytics YOLO11 with Weights & Biases offers several benefits including:\n+wandb.login(key=\"<API_KEY>\")\n+```\n \n-- **Real-Time Metrics Tracking:** Observe metric changes during training for immediate insights.\n-- **Hyperparameter Optimization:** Improve model performance by fine-tuning learning rate, [batch size](https://www.ultralytics.com/glossary/batch-size), etc.\n-- **Comparative Analysis:** Side-by-side comparison of different training runs.\n-- **Resource Monitoring:** Keep track of CPU, GPU, and memory usage.\n-- **Model Artifacts Management:** Easy access and sharing of model checkpoints.\n+3. Train your YOLO11 model with W&B logging enabled:\n \n-Explore these features in detail in the Weights & Biases Dashboard section above.\n+```python\n+from ultralytics import YOLO\n \n-### How can I configure Weights & Biases for YOLO11 training?\n+model = YOLO(\"yolo11n.pt\")\n+model.train(data=\"coco8.yaml\", epochs=5, project=\"ultralytics\", name=\"yolo11n\")\n+```\n \n-To configure Weights & Biases for YOLO11 training, follow these steps:\n+This will automatically log metrics, hyperparameters, and model artifacts to your W&B project.\n \n-1. Run the command to initialize Weights & Biases:\n-    ```bash\n-    import wandb\n-    wandb.login()\n-    ```\n-2. Retrieve your API key from the Weights & Biases website.\n-3. Use the API key to authenticate your development environment.\n+### What are the key features of Weights & Biases integration with YOLO11?\n \n-Detailed setup instructions can be found in the Configuring Weights & Biases section above.\n+The key features include:\n \n-### How do I train a YOLO11 model using Weights & Biases?\n+- Real-time metrics tracking during training\n+- Hyperparameter optimization tools\n+- Comparative analysis of different training runs\n+- Visualization of training progress through graphs\n+- Resource monitoring (CPU, GPU, memory usage)\n+- Model artifacts management and sharing\n+- Viewing inference results with image overlays\n \n-For training a YOLO11 model using Weights & Biases, use the following steps in a Python script:\n+These features help in tracking experiments, optimizing models, and collaborating more effectively on YOLO11 projects.\n \n-```python\n-import wandb\n-from wandb.integration.ultralytics import add_wandb_callback\n+### How can I view the Weights & Biases dashboard for my YOLO11 training?\n \n-from ultralytics import YOLO\n+After running your training script with W&B integration:\n \n-# Initialize a Weights & Biases run\n-wandb.init(project=\"ultralytics\", job_type=\"training\")\n+1. A link to your W&B dashboard will be provided in the console output.\n+2. Click on the link or go to [wandb.ai](https://wandb.ai) and log in to your account.\n+3. Navigate to your project to view detailed metrics, visualizations, and model performance data.\n \n-# Load a YOLO model\n-model = YOLO(\"yolo11n.pt\")\n+The dashboard offers insights into your model's training process, allowing you to analyze and improve your YOLO11 models effectively.\n \n-# Add W&B Callback for Ultralytics\n-add_wandb_callback(model, enable_model_checkpointing=True)\n+### Can I disable Weights & Biases logging for YOLO11 training?\n \n-# Train and Fine-Tune the Model\n-model.train(project=\"ultralytics\", data=\"coco8.yaml\", epochs=5, imgsz=640)\n+Yes, you can disable W&B logging using the following command:\n \n-# Validate the Model\n-model.val()\n+```bash\n+wandb disabled\n+```\n \n-# Perform Inference and Log Results\n-model([\"path/to/image1\", \"path/to/image2\"])\n+To re-enable logging, use:\n \n-# Finalize the W&B Run\n-wandb.finish()\n+```bash\n+wandb enabled\n ```\n \n-This script initializes Weights & Biases, sets up the model, trains it, and logs results. For more details, visit the Usage section above.\n+This allows you to control when you want to use W&B logging without modifying your training scripts.\n \n-### Why should I use Ultralytics YOLO11 with Weights & Biases over other platforms?\n+### How does Weights & Biases help in optimizing YOLO11 models?\n \n-Ultralytics YOLO11 integrated with Weights & Biases offers several unique advantages:\n+Weights & Biases helps optimize YOLO11 models by:\n \n-- **High Efficiency:** Real-time tracking of training metrics and performance optimization.\n-- **Scalability:** Easily manage large-scale training jobs with robust resource monitoring and utilization tools.\n-- **Interactivity:** A user-friendly interactive UI for [data visualization](https://www.ultralytics.com/glossary/data-visualization) and model management.\n-- **Community and Support:** Strong integration documentation and community support with flexible customization and enhancement options.\n+1. Providing detailed visualizations of training metrics\n+2. Enabling easy comparison between different model versions\n+3. Offering tools for [hyperparameter tuning](https://www.ultralytics.com/glossary/hyperparameter-tuning)\n+4. Allowing for collaborative analysis of model performance\n+5. Facilitating easy sharing of model artifacts and results\n \n-For comparisons with other platforms like Comet and ClearML, refer to [Ultralytics integrations](../integrations/index.md).\n+These features help researchers and developers iterate faster and make data-driven decisions to improve their YOLO11 models.\ndiff --git a/docs/mkdocs_github_authors.yaml b/docs/mkdocs_github_authors.yaml\nindex 039caa382d8..0e0423c2485 100644\n--- a/docs/mkdocs_github_authors.yaml\n+++ b/docs/mkdocs_github_authors.yaml\n@@ -112,6 +112,9 @@ lakshantha@ultralytics.com:\n lakshanthad@yahoo.com:\n   avatar: https://avatars.githubusercontent.com/u/20147381?v=4\n   username: lakshanthad\n+makei05@outlook.de:\n+  avatar: https://avatars.githubusercontent.com/u/78843978?v=4\n+  username: Skillnoob\n matthewnoyce@icloud.com:\n   avatar: https://avatars.githubusercontent.com/u/131261051?v=4\n   username: MatthewNoyce\n", "test_patch": "", "problem_statement": "Duplicated Wandb runs with multiple gpus \n### Search before asking\n\n- [X] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\nMulti-GPU\n\n### Bug\n\nFor one actual training run multiple Wandb runs are spawned when I use multiple gpus. I guess that problem because of use of `subprocess.run`, so no global `wandb.run` will be propagated to new process.\r\n\r\n![image](https://github.com/user-attachments/assets/2fcc2c32-fd13-4ad7-a036-d68bc6ba45d7)\r\n![image](https://github.com/user-attachments/assets/069c60ab-96d3-4ded-8556-7ea76b5388aa)\r\n\r\n\r\nI suggest to propagate wandb run's project, name, and id to subprocess through [Environment](https://docs.wandb.ai/guides/track/environment-variables/) or ddp file (hard way), that should fix the issue.\n\n### Environment\n\n```\r\nUltralytics 8.3.5 \ud83d\ude80 Python-3.10.14 torch-2.4.0 CUDA:0 (Tesla T4, 15095MiB)\r\nSetup complete \u2705 (4 CPUs, 31.4 GB RAM, 5934.0/8062.4 GB disk)\r\n```\r\n\r\nKaggle with GPU T4 x 2\n\n### Minimal Reproducible Example\n\nhttps://www.kaggle.com/code/dantetemplarwhowas/notebook915070977e\n\n### Additional\n\nIt's hard for me to debug actual behavior because I have only one gpu on my local machine. Problem appears in Kaggle with GPU T4 x 2 accelerator.\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!\n", "hints_text": "", "created_at": "2024-10-05T11:39:52Z"}
{"repo": "ultralytics/ultralytics", "pull_number": 16025, "instance_id": "ultralytics__ultralytics-16025", "issue_numbers": ["16023"], "base_commit": "93b80552fcaf7aa86ca7f561fd2a3e9723ad0ea6", "patch": "diff --git a/ultralytics/models/yolo/classify/train.py b/ultralytics/models/yolo/classify/train.py\nindex a1e465d73f5..e51349fa989 100644\n--- a/ultralytics/models/yolo/classify/train.py\n+++ b/ultralytics/models/yolo/classify/train.py\n@@ -1,5 +1,7 @@\n # Ultralytics YOLO \ud83d\ude80, AGPL-3.0 license\n \n+from copy import copy\n+\n import torch\n \n from ultralytics.data import ClassificationDataset, build_dataloader\n@@ -107,7 +109,9 @@ def progress_string(self):\n     def get_validator(self):\n         \"\"\"Returns an instance of ClassificationValidator for validation.\"\"\"\n         self.loss_names = [\"loss\"]\n-        return yolo.classify.ClassificationValidator(self.test_loader, self.save_dir, _callbacks=self.callbacks)\n+        return yolo.classify.ClassificationValidator(\n+            self.test_loader, self.save_dir, args=copy(self.args), _callbacks=self.callbacks\n+        )\n \n     def label_loss_items(self, loss_items=None, prefix=\"train\"):\n         \"\"\"\n", "test_patch": "", "problem_statement": "final_eval does not pick the right imgsz\n### Search before asking\n\n- [X] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\nTrain\n\n### Bug\n\nLooks like that during in the last phase of the training, the `final_eval()` step wants to use the default image size of 640, rather than the size passed to the `model.train`. \r\n\r\nI realized it when building a custom model with a few fully connected layers in the head. I train the model with the MNIST dataset using an image size of 32.\r\nThe train and the model work fine when going trough each of the epochs, \r\nbut at the final validation I get the error message below.\r\nAs you can see, the problem is a multiplication of two matrices that do not match, \r\nwhich is most probably caused by the input image size larger than 32.\r\nI think the problem is that the `model.warmup` (validator.py:157) does not get the right `imgsz`.\r\n\r\nSince the training goes trough the epochs, it saves the `best.pt` and `last.pt` weights.\r\nWhen loading these weights and predicting an MNIST image, everything works fine.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/notebooks/shared/andrea/train_custom_model.py\", line 7, in <module>\r\n    results = model.train(data=\"mnist\", epochs=3, imgsz=32)\r\n  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/engine/model.py\", line 673, in train\r\n    self.trainer.train()\r\n  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py\", line 199, in train\r\n    self._do_train(world_size)\r\n  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py\", line 459, in _do_train\r\n    self.final_eval()\r\n  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/models/yolo/classify/train.py\", line 146, in final_eval\r\n    self.metrics = self.validator(model=f)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/engine/validator.py\", line 157, in __call__\r\n    model.warmup(imgsz=(1 if pt else self.args.batch, 3, imgsz, imgsz))  # warmup\r\n  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/nn/autobackend.py\", line 627, in warmup\r\n    self.forward(im)  # warmup\r\n  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/nn/autobackend.py\", line 453, in forward\r\n    y = self.model(im, augment=augment, visualize=visualize, embed=embed)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/nn/tasks.py\", line 89, in forward\r\n    return self.predict(x, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/nn/tasks.py\", line 107, in predict\r\n    return self._predict_once(x, profile, visualize, embed)\r\n  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/nn/tasks.py\", line 128, in _predict_once\r\n    x = m(x)  # run\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\", line 117, in forward\r\n    return F.linear(input, self.weight, self.bias)\r\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (1x394384 and 400x120)\r\n```\r\n\n\n### Environment\n\nUltralytics YOLOv8.2.2 \ud83d\ude80 Python-3.10.12 torch-2.4.0+cu121 CUDA:0 (NVIDIA A100-SXM4-80GB, 81051MiB)\r\nSetup complete \u2705 (64 CPUs, 503.8 GB RAM, 373.5/862.4 GB disk)\n\n### Minimal Reproducible Example\n\nTraining\r\n```\r\nfrom ultralytics import YOLO\r\n\r\n\r\nmodel = YOLO(\"./custom_model.yaml\", task='classify')\r\nprint(model.info(detailed=True))\r\n\r\nresults = model.train(data=\"mnist\", epochs=10, imgsz=32)\r\n```\r\n\\\r\nPrediction\r\n```\r\nimport cv2\r\n\r\n\r\nmnist_image = \"path_to/mnist/test/0/10.png\"\r\nimg = cv2.imread(mnist_image)\r\n\r\nmodel = YOLO(\"./custom_model.yaml\", task='classify').load(\"./runs/classify/train/weights/best.pt\")\r\nresults = model.predict(img)\r\nresults[0].probs\r\n```\r\n\r\n\\\r\ncustom_model.yaml\r\n```\r\n# Simple image classifier as in https://pytorch.org/tutorials/beginner/onnx/export_simple_model_to_onnx_tutorial.html\r\n\r\nnc: 10\r\nbackbone:\r\n  # [from, repeats, module, args]\r\n  - [-1, 1, nn.Conv2d, [3, 6, 5]]\r\n  - [-1, 1, nn.MaxPool2d, [2]]  \r\n  - [-1, 1, nn.Conv2d, [6, 16, 5]]\r\n  - [-1, 1, nn.MaxPool2d, [2]]\r\n  - [-1, 1, nn.Flatten, [1, -1]]\r\n\r\nhead:\r\n  - [-1, 1, nn.Linear, [400, 120]]\r\n  - [-1, 1, nn.Linear, [120, 84]]\r\n  - [-1, 1, nn.Linear, [84, 10]]\r\n```\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!\n", "hints_text": "Probably because `args` is not passed when creating the validator for classification:\n\nhttps://github.com/ultralytics/ultralytics/blob/88102eb5085a668fff0171fdaa6297af65c2d010/ultralytics/models/yolo/classify/train.py#L110", "created_at": "2024-09-05T06:46:31Z"}
{"repo": "ultralytics/ultralytics", "pull_number": 15874, "instance_id": "ultralytics__ultralytics-15874", "issue_numbers": ["15856"], "base_commit": "7053169fd0802cba6e1984671f97678131619a4b", "patch": "diff --git a/ultralytics/engine/exporter.py b/ultralytics/engine/exporter.py\nindex a4c81e24e90..891fa8bbd72 100644\n--- a/ultralytics/engine/exporter.py\n+++ b/ultralytics/engine/exporter.py\n@@ -204,8 +204,9 @@ def __call__(self, model=None) -> str:\n             self.args.half = False\n             assert not self.args.dynamic, \"half=True not compatible with dynamic=True, i.e. use only one.\"\n         self.imgsz = check_imgsz(self.args.imgsz, stride=model.stride, min_dim=2)  # check image size\n-        if self.args.int8 and (engine or xml):\n+        if self.args.int8 and not self.args.dynamic and (engine or xml):\n             self.args.dynamic = True  # enforce dynamic to export TensorRT INT8; ensures ONNX is dynamic\n+            LOGGER.warning(\"WARNING \u26a0\ufe0f INT8 export requires dynamic image sizes, setting dynamic=True.\")\n         if self.args.optimize:\n             assert not ncnn, \"optimize=True not compatible with format='ncnn', i.e. use optimize=False\"\n             assert self.device.type == \"cpu\", \"optimize=True not compatible with cuda devices, i.e. use device='cpu'\"\n", "test_patch": "", "problem_statement": "Exporting engine int8 quantization model, setting dynamic=False, batch=1 does not take effect?\n### Search before asking\n\n- [X] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\n_No response_\n\n### Bug\n\nyolo export model=yolov8n.pt format=engine imgsz=640,640 batch=1 int8=True dynamic=False\r\n\r\n> **log**\r\n```\r\nWARNING \u26a0\ufe0f TensorRT requires GPU export, automatically assigning device=0\r\nUltralytics YOLOv8.2.82 \ud83d\ude80 Python-3.8.16 torch-2.0.0 CUDA:0 (NVIDIA TITAN V, 12050MiB)\r\nWARNING \u26a0\ufe0f INT8 export requires a missing 'data' arg for calibration. Using default 'data=coco8.yaml'.\r\nYOLOv8n summary (fused): 168 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\r\n\r\nPyTorch: starting from 'yolov8n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\r\n\r\nONNX: starting export with onnx 1.16.1 opset 17...\r\n================ Diagnostic Run torch.onnx.export version 2.0.0 ================\r\nverbose: False, log level: Level.ERROR\r\n======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\r\n\r\nONNX: export success \u2705 0.6s, saved as 'yolov8n.onnx' (12.1 MB)\r\n\r\nTensorRT: starting export with TensorRT 8.6.0...\r\n[08/28/2024-16:11:40] [TRT] [I] [MemUsageChange] Init CUDA: CPU +2, GPU +0, now: CPU 2447, GPU 619 (MiB)\r\n[08/28/2024-16:11:44] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +430, GPU +72, now: CPU 2953, GPU 691 (MiB)\r\n[08/28/2024-16:11:44] [TRT] [I] ----------------------------------------------------------------\r\n[08/28/2024-16:11:44] [TRT] [I] Input filename:   yolov8n.onnx\r\n[08/28/2024-16:11:44] [TRT] [I] ONNX IR version:  0.0.8\r\n[08/28/2024-16:11:44] [TRT] [I] Opset version:    17\r\n[08/28/2024-16:11:44] [TRT] [I] Producer name:    pytorch\r\n[08/28/2024-16:11:44] [TRT] [I] Producer version: 2.0.0\r\n[08/28/2024-16:11:44] [TRT] [I] Domain:           \r\n[08/28/2024-16:11:44] [TRT] [I] Model version:    0\r\n[08/28/2024-16:11:44] [TRT] [I] Doc string:       \r\n[08/28/2024-16:11:44] [TRT] [I] ----------------------------------------------------------------\r\n[08/28/2024-16:11:44] [TRT] [W] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\r\nTensorRT: input \"images\" with shape(-1, 3, -1, -1) DataType.FLOAT\r\nTensorRT: output \"output0\" with shape(-1, 84, -1) DataType.FLOAT\r\nTensorRT: WARNING \u26a0\ufe0f 'dynamic=True' model requires max batch size, i.e. 'batch=16'\r\nTensorRT: building INT8 engine as yolov8n.engine\r\nTensorRT: collecting INT8 calibration images from 'data=coco8.yaml'\r\nScanning /home/zhang/pythonworkspace/deeplabv3-plus-pytorch/datasets/coco8/label\r\nTensorRT: WARNING \u26a0\ufe0f >300 images recommended for INT8 calibration, found 4 images.\r\n```\r\n\r\n> I don't quite understand the TensorRT part in the log information. I have already set the static size and set 'dynamic=False'. The model is from the official YOLOV8n.\r\n\r\n```\r\nTensorRT: input \"images\" with shape(-1, 3, -1, -1) DataType.FLOAT\r\nTensorRT: output \"output0\" with shape(-1, 84, -1) DataType.FLOAT\r\nTensorRT: WARNING \u26a0\ufe0f 'dynamic=True' model requires max batch size, i.e. 'batch=16'\r\n\r\n```\r\n\r\n\r\n\n\n### Environment\n\nUltralytics YOLOv8.2.82 \ud83d\ude80 Python-3.8.16 torch-2.0.0 CUDA:0 (NVIDIA TITAN V, 12050MiB)\r\nSetup complete \u2705 (8 CPUs, 31.3 GB RAM, 34.8/93.3 GB disk)\r\n\r\nOS                  Linux-6.8.0-40-generic-x86_64-with-glibc2.17\r\nEnvironment         Linux\r\nPython              3.8.16\r\nInstall             git\r\nRAM                 31.27 GB\r\nCPU                 Intel Core(TM) i7-9700 3.00GHz\r\nCUDA                11.7\r\n\r\nnumpy               \u2705 1.23.5<2.0.0,>=1.23.0\r\nmatplotlib          \u2705 3.4.3>=3.3.0\r\nopencv-python       \u2705 4.10.0.84>=4.6.0\r\npillow              \u2705 9.4.0>=7.1.2\r\npyyaml              \u2705 6.0>=5.3.1\r\nrequests            \u2705 2.28.1>=2.23.0\r\nscipy               \u2705 1.10.1>=1.4.1\r\ntorch               \u2705 2.0.0>=1.8.0\r\ntorchvision         \u2705 0.15.0>=0.9.0\r\ntqdm                \u2705 4.66.1>=4.64.0\r\npsutil              \u2705 5.9.0\r\npy-cpuinfo          \u2705 9.0.0\r\npandas              \u2705 2.0.3>=1.1.4\r\nseaborn             \u2705 0.12.2>=0.11.0\r\nultralytics-thop    \u2705 2.0.0>=2.0.0\r\n\n\n### Minimal Reproducible Example\n\nyolo export model=yolov8n.pt format=engine imgsz=640,640 batch=1 int8=True dynamic=False\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!\n", "hints_text": "`dynamic=True` is enforced for `int8` TensorRT exports:\nhttps://github.com/ultralytics/ultralytics/blob/4fe046e2d59c0377fcc4ab01634a2e71aa02069b/ultralytics/engine/exporter.py#L208\n\n@Burhan-Q Probably should have a warning.", "created_at": "2024-08-29T00:17:26Z"}
{"repo": "ultralytics/ultralytics", "pull_number": 15719, "instance_id": "ultralytics__ultralytics-15719", "issue_numbers": ["12798"], "base_commit": "5f93df6fca2927617360a16cd8366520c51800fe", "patch": "diff --git a/ultralytics/__init__.py b/ultralytics/__init__.py\nindex 93ca50b1ac5..a03cb559e0a 100644\n--- a/ultralytics/__init__.py\n+++ b/ultralytics/__init__.py\n@@ -1,6 +1,6 @@\n # Ultralytics YOLO \ud83d\ude80, AGPL-3.0 license\n \n-__version__ = \"8.2.79\"\n+__version__ = \"8.2.80\"\n \n import os\n \ndiff --git a/ultralytics/engine/model.py b/ultralytics/engine/model.py\nindex 7e215319f4e..ddbfa9de5e1 100644\n--- a/ultralytics/engine/model.py\n+++ b/ultralytics/engine/model.py\n@@ -6,6 +6,7 @@\n \n import numpy as np\n import torch\n+from PIL import Image\n \n from ultralytics.cfg import TASK2DATA, get_cfg, get_save_dir\n from ultralytics.engine.results import Results\n@@ -143,7 +144,7 @@ def __init__(\n \n     def __call__(\n         self,\n-        source: Union[str, Path, int, list, tuple, np.ndarray, torch.Tensor] = None,\n+        source: Union[str, Path, int, Image.Image, list, tuple, np.ndarray, torch.Tensor] = None,\n         stream: bool = False,\n         **kwargs,\n     ) -> list:\n@@ -504,7 +505,7 @@ def embed(\n \n     def predict(\n         self,\n-        source: Union[str, Path, int, list, tuple, np.ndarray, torch.Tensor] = None,\n+        source: Union[str, Path, int, Image.Image, list, tuple, np.ndarray, torch.Tensor] = None,\n         stream: bool = False,\n         predictor=None,\n         **kwargs,\n@@ -517,7 +518,7 @@ def predict(\n         types of image sources and can operate in a streaming mode.\n \n         Args:\n-            source (str | Path | int | List[str] | List[Path] | List[int] | np.ndarray | torch.Tensor): The source\n+            source (str | Path | int | PIL.Image | np.ndarray | torch.Tensor | List | Tuple): The source\n                 of the image(s) to make predictions on. Accepts various types including file paths, URLs, PIL\n                 images, numpy arrays, and torch tensors.\n             stream (bool): If True, treats the input source as a continuous stream for predictions.\n", "test_patch": "", "problem_statement": "Incorrect type hint\n### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### YOLOv8 Component\n\n_No response_\n\n### Bug\n\nthe PIL.Image.Image type hint is not defined in the predict method of the model class model\n\n### Environment\n\nnot required\n\n### Minimal Reproducible Example\n\nnot required\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [X] Yes I'd like to help by submitting a PR!\n", "hints_text": "", "created_at": "2024-08-20T22:30:12Z"}
{"repo": "ultralytics/ultralytics", "pull_number": 14700, "instance_id": "ultralytics__ultralytics-14700", "issue_numbers": ["14694"], "base_commit": "fc7f1f3aeacd78749591d7d25a2f33990a77f241", "patch": "diff --git a/ultralytics/engine/trainer.py b/ultralytics/engine/trainer.py\nindex 71840822409..9d71810bb5f 100644\n--- a/ultralytics/engine/trainer.py\n+++ b/ultralytics/engine/trainer.py\n@@ -507,7 +507,7 @@ def save_model(self):\n         self.last.write_bytes(serialized_ckpt)  # save last.pt\n         if self.best_fitness == self.fitness:\n             self.best.write_bytes(serialized_ckpt)  # save best.pt\n-        if (self.save_period > 0) and (self.epoch > 0) and (self.epoch % self.save_period == 0):\n+        if (self.save_period > 0) and (self.epoch >= 0) and (self.epoch % self.save_period == 0):\n             (self.wdir / f\"epoch{self.epoch}.pt\").write_bytes(serialized_ckpt)  # save epoch, i.e. 'epoch3.pt'\n \n     def get_dataset(self):\n", "test_patch": "", "problem_statement": "Yolo v8.0 & v8.2 - save_period parameter not working as expected\n### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### YOLOv8 Component\n\nTrain\n\n### Bug\n\nWhen I train a model with for example EPOCH 20 and the parameter save_period=1 I am missing the very first EPOCH 0 in the weights folder.\r\n\r\nBelow you see the saved *.pt files with their corresponding timestamp. \r\nThis is repeadable on my ubuntu system with yolo v8.2 installed and on a windows machine with yolo v8.0 installed.\r\n\r\nModel Path\t\t\t\t\t\t\t\t\t\tTimestamp\r\n/mnt/data_2/05_mushroom/mushroom_training/20240724_mushroomm2/weights/epoch1.pt\t\t24.07.2024 19:20\r\n/mnt/data_2/05_mushroom/mushroom_training/20240724_mushroomm2/weights/epoch2.pt\t\t24.07.2024 19:57\r\n/mnt/data_2/05_mushroom/mushroom_training/20240724_mushroomm2/weights/epoch3.pt\t\t24.07.2024 20:34\r\n/mnt/data_2/05_mushroom/mushroom_training/20240724_mushroomm2/weights/epoch4.pt\t\t24.07.2024 21:12\r\n/mnt/data_2/05_mushroom/mushroom_training/20240724_mushroomm2/weights/epoch5.pt\t\t24.07.2024 21:49\r\n/mnt/data_2/05_mushroom/mushroom_training/20240724_mushroomm2/weights/epoch6.pt\t\t24.07.2024 22:27\r\n/mnt/data_2/05_mushroom/mushroom_training/20240724_mushroomm2/weights/epoch7.pt\t\t24.07.2024 23:04\r\n/mnt/data_2/05_mushroom/mushroom_training/20240724_mushroomm2/weights/epoch8.pt\t\t24.07.2024 23:41\r\n/mnt/data_2/05_mushroom/mushroom_training/20240724_mushroomm2/weights/epoch9.pt\t\t25.07.2024 00:17\r\n/mnt/data_2/05_mushroom/mushroom_training/20240724_mushroomm2/weights/epoch10.pt\t25.07.2024 00:53\r\n/mnt/data_2/05_mushroom/mushroom_training/20240724_mushroomm2/weights/epoch11.pt\t25.07.2024 01:29\r\n/mnt/data_2/05_mushroom/mushroom_training/20240724_mushroomm2/weights/epoch12.pt\t25.07.2024 02:05\r\n/mnt/data_2/05_mushroom/mushroom_training/20240724_mushroomm2/weights/epoch13.pt\t25.07.2024 02:40\r\n/mnt/data_2/05_mushroom/mushroom_training/20240724_mushroomm2/weights/epoch14.pt\t25.07.2024 03:16\r\n/mnt/data_2/05_mushroom/mushroom_training/20240724_mushroomm2/weights/epoch15.pt\t25.07.2024 03:52\r\n/mnt/data_2/05_mushroom/mushroom_training/20240724_mushroomm2/weights/epoch16.pt\t25.07.2024 04:27\r\n/mnt/data_2/05_mushroom/mushroom_training/20240724_mushroomm2/weights/epoch17.pt\t25.07.2024 05:03\r\n/mnt/data_2/05_mushroom/mushroom_training/20240724_mushroomm2/weights/epoch18.pt\t25.07.2024 05:39\r\n/mnt/data_2/05_mushroom/mushroom_training/20240724_mushroomm2/weights/last.pt\t\t25.07.2024 06:15\r\n/mnt/data_2/05_mushroom/mushroom_training/20240724_mushroomm2/weights/epoch19.pt\t25.07.2024 06:15\r\n/mnt/data_2/05_mushroom/mushroom_training/20240724_mushroomm2/weights/best.pt\t\t25.07.2024 06:15\r\n\r\nAny idea if I have a missunderstanding here or maybe it is a bug?\n\n### Environment\n\nUltralytics YOLOv8.2.10 \ud83d\ude80 Python-3.10.12 torch-2.0.1+cu117 CUDA:0 (NVIDIA RTX A6000, 48682MiB)\r\nSetup complete \u2705 (32 CPUs, 62.7 GB RAM, 694.6/915.3 GB disk)\r\n\r\nOS                  Linux-6.2.0-39-generic-x86_64-with-glibc2.35\r\nEnvironment         Linux\r\nPython              3.10.12\r\nInstall             pip\r\nRAM                 62.71 GB\r\nCPU                 AMD Ryzen 9 3950X 16-Core Processor\r\nCUDA                11.7\r\n\r\nmatplotlib          \u2705 3.8.2>=3.3.0\r\nopencv-python       \u2705 4.8.1.78>=4.6.0\r\npillow              \u2705 10.2.0>=7.1.2\r\npyyaml              \u2705 6.0.1>=5.3.1\r\nrequests            \u2705 2.31.0>=2.23.0\r\nscipy               \u2705 1.11.4>=1.4.1\r\ntorch               \u2705 2.0.1>=1.8.0\r\ntorchvision         \u2705 0.15.2>=0.9.0\r\ntqdm                \u2705 4.66.1>=4.64.0\r\npsutil              \u2705 5.9.5\r\npy-cpuinfo          \u2705 9.0.0\r\nthop                \u2705 0.1.1-2209072238>=0.1.1\r\npandas              \u2705 2.1.3>=1.1.4\r\nseaborn             \u2705 0.12.2>=0.11.0\r\n\n\n### Minimal Reproducible Example\n\nfrom ultralytics import YOLO\r\n \r\nimport os\r\nos.environ['WANDB_MODE'] = 'disabled'\r\n \r\nif __name__ == '__main__':\r\n    model = YOLO('yolov8m.pt')\r\n\r\n    results = model.train(\r\n\r\n# Ingmar Hyperparameters\r\n        data = '/mnt/data_2/05_mushroom/mushroom_yolov8_20240724.yaml',\r\n        save_dir = '/mnt/data_2/05_mushroom/',\r\n        epochs=20,\r\n        batch=16,\r\n        imgsz=1280,\r\n        lr0=0.01,\r\n        lrf=0.01,\r\n        momentum=0.937,\r\n        weight_decay=0.0005,\r\n        warmup_epochs=3,\r\n        warmup_momentum=0.8,\r\n        box=7.5,\r\n        save_period=1,\r\n        cls=0.5,\r\n        dfl=1.5,\r\n        hsv_h=0.015,\r\n        hsv_s=0.7,\r\n        hsv_v=0.4,\r\n        degrees=0.0,\r\n        translate=0.1,\r\n        scale=0.5,\r\n        shear=0.0,\r\n        perspective=0.0,\r\n        flipud=0.0,\r\n        fliplr=0.5,\r\n        bgr=0,\r\n        mosaic=1,\r\n        mixup=0.0,\r\n        copy_paste=0.0,\r\n        project='mushroom_training',\r\n        name='20240724_mushroom_m'\r\n\r\n    )\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!\n", "hints_text": "", "created_at": "2024-07-25T18:27:57Z"}
{"repo": "ultralytics/ultralytics", "pull_number": 14551, "instance_id": "ultralytics__ultralytics-14551", "issue_numbers": ["14548"], "base_commit": "dcde8bd23d12bbb4867ebf45f936dd37c2445974", "patch": "diff --git a/ultralytics/engine/model.py b/ultralytics/engine/model.py\nindex f8bc495f64a..e626f8cabec 100644\n--- a/ultralytics/engine/model.py\n+++ b/ultralytics/engine/model.py\n@@ -588,7 +588,13 @@ def export(\n         self._check_is_pytorch_model()\n         from .exporter import Exporter\n \n-        custom = {\"imgsz\": self.model.args[\"imgsz\"], \"batch\": 1, \"data\": None, \"verbose\": False}  # method defaults\n+        custom = {\n+            \"imgsz\": self.model.args[\"imgsz\"],\n+            \"batch\": 1,\n+            \"data\": None,\n+            \"device\": None,  # reset to avoid multi-GPU errors\n+            \"verbose\": False,\n+        }  # method defaults\n         args = {**self.overrides, **custom, **kwargs, \"mode\": \"export\"}  # highest priority args on the right\n         return Exporter(overrides=args, _callbacks=self.callbacks)(model=self.model)\n \n", "test_patch": "", "problem_statement": "Exporting after training on YoloV10 raise a ValueError with MultiGPU\n### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### YOLOv8 Component\n\nTrain, Export\n\n### Bug\n\nValueError: AutoBatch with batch<1 not supported for Multi-GPU training, please specify a valid batch size, i.e. batch=16.\n\n### Environment\n\n```\r\nUltralytics YOLOv8.2.58 \ud83d\ude80 Python-3.10.12 torch-2.0.0+cu117 CUDA:0 (NVIDIA GeForce RTX 2080 Ti, 11012MiB)\r\nSetup complete \u2705 (20 CPUs, 125.5 GB RAM, 848.9/1832.2 GB disk)\r\n\r\nOS                  Linux-5.4.0-182-generic-x86_64-with-glibc2.31\r\nEnvironment         Docker\r\nPython              3.10.12\r\nInstall             pip\r\nRAM                 125.48 GB\r\nCPU                 Intel Core(TM) i9-9820X 3.30GHz\r\nCUDA                11.7\r\n\r\nnumpy               \u2705 1.23.5<2.0.0,>=1.23.0\r\nmatplotlib          \u2705 3.9.1>=3.3.0\r\nopencv-python       \u2705 4.10.0.84>=4.6.0\r\npillow              \u2705 10.4.0>=7.1.2\r\npyyaml              \u2705 6.0.1>=5.3.1\r\nrequests            \u2705 2.32.3>=2.23.0\r\nscipy               \u2705 1.14.0>=1.4.1\r\ntorch               \u2705 2.0.0>=1.8.0\r\ntorchvision         \u2705 0.15.1>=0.9.0\r\ntqdm                \u2705 4.66.4>=4.64.0\r\npsutil              \u2705 6.0.0\r\npy-cpuinfo          \u2705 9.0.0\r\npandas              \u2705 1.5.3>=1.1.4\r\nseaborn             \u2705 0.13.2>=0.11.0\r\nultralytics-thop    \u2705 2.0.0>=2.0.0\r\n```\n\n### Minimal Reproducible Example\n\n```\r\nfrom ultralytics import YOLO\r\n\r\nmodel = YOLO(\"./yolov10n.pt\")\r\nmodel.train(\r\n  data=...,\r\n  epochs=1,\r\n  batch=16,\r\n  device=[0,1])\r\n\r\nmodel.export() # raises ValueError: AutoBatch with batch<1 not supported for Multi-GPU training, please specify a valid batch size, i.e. batch=16.\r\n\r\n```\n\n### Additional\n\nAfter analyzing the code, it just can't work actually.\r\nThe call stack is coming from exporter.py, line 193 : \r\n```\r\n        self.device = select_device(\"cpu\" if self.args.device is None else self.args.device)\r\n```\r\nThis does not pass any batch size information, which is a requiement for select_device as written :\r\n```\r\ndef select_device(device=\"\", batch=0, newline=False, verbose=True): # notice batch=0 by default\r\n    ...\r\n    if not cpu and not mps and torch.cuda.is_available():  # prefer GPU if available\r\n        devices = device.split(\",\") if device else \"0\"  # range(torch.cuda.device_count())  # i.e. 0,1,6,7\r\n        n = len(devices)  # device count\r\n        if n > 1:  # multi-GPU # We are using multiGPU\r\n            if batch < 1: # batch is 0 by default, this will always happen\r\n                raise ValueError(\r\n                    \"AutoBatch with batch<1 not supported for Multi-GPU training, \"\r\n                    \"please specify a valid batch size, i.e. batch=16.\"\r\n                )\r\n```\r\n\r\n\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!\n", "hints_text": "", "created_at": "2024-07-19T18:35:28Z"}
{"repo": "ultralytics/ultralytics", "pull_number": 14197, "instance_id": "ultralytics__ultralytics-14197", "issue_numbers": ["14194"], "base_commit": "6962df8a45df71f48f0782c336c2c5ca3a2fdf59", "patch": "diff --git a/ultralytics/nn/tasks.py b/ultralytics/nn/tasks.py\nindex fd7d4028c87..68d4ee653ae 100644\n--- a/ultralytics/nn/tasks.py\n+++ b/ultralytics/nn/tasks.py\n@@ -276,7 +276,7 @@ def loss(self, batch, preds=None):\n             batch (dict): Batch to compute loss on\n             preds (torch.Tensor | List[torch.Tensor]): Predictions.\n         \"\"\"\n-        if not hasattr(self, \"criterion\"):\n+        if getattr(self, \"criterion\", None) is None:\n             self.criterion = self.init_criterion()\n \n         preds = self.forward(batch[\"img\"]) if preds is None else preds\n", "test_patch": "", "problem_statement": "Error: \u2018NoneType\u2019 object is not callable during YOLOv8 Classification Training with Multi-GPU\n### Search before asking\r\n\r\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\r\n\r\n\r\n### YOLOv8 Component\r\n\r\nTrain, Multi-GPU\r\n\r\n### Bug\r\n\r\nHi Ultralytics team,\r\n\r\nI encountered an issue while training a YOLOv8 classification model where the `criterion` (loss function) was `None`, causing a `TypeError`.\r\n\r\n```\r\nroot@C.11372351:~$ yolo task=classify mode=train model=yolov8x-cls.pt data=project-5-latest epochs=100 patience=20 batch=64 imgsz=224 augment=True optimizer=SGD cache=memory device=0,1\r\nUltralytics YOLOv8.2.48 \ud83d\ude80 Python-3.10.14 torch-2.2.2 CUDA:0 (NVIDIA GeForce RTX 4090, 24217MiB)\r\n                                                      CUDA:1 (NVIDIA GeForce RTX 4090, 24217MiB)\r\nengine/trainer: task=classify, mode=train, model=yolov8x-cls.pt, data=project-5-latest, epochs=100, time=None, patience=20, batch=64, imgsz=224, save=True, save_period=-1, cache=memory, device=(0, 1), workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=True, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/train\r\ntrain: /root/project-5-latest/train... found 54064 images in 5 classes \u2705\r\nval: /root/project-5-latest/val... found 13686 images in 5 classes \u2705\r\ntest: None...\r\nOverriding model.yaml nc=1000 with nc=5\r\n\r\n                   from  n    params  module                                       arguments\r\n  0                  -1  1      2320  ultralytics.nn.modules.conv.Conv             [3, 80, 3, 2]\r\n  1                  -1  1    115520  ultralytics.nn.modules.conv.Conv             [80, 160, 3, 2]\r\n  2                  -1  3    436800  ultralytics.nn.modules.block.C2f             [160, 160, 3, True]\r\n  3                  -1  1    461440  ultralytics.nn.modules.conv.Conv             [160, 320, 3, 2]\r\n  4                  -1  6   3281920  ultralytics.nn.modules.block.C2f             [320, 320, 6, True]\r\n  5                  -1  1   1844480  ultralytics.nn.modules.conv.Conv             [320, 640, 3, 2]\r\n  6                  -1  6  13117440  ultralytics.nn.modules.block.C2f             [640, 640, 6, True]\r\n  7                  -1  1   7375360  ultralytics.nn.modules.conv.Conv             [640, 1280, 3, 2]\r\n  8                  -1  3  27865600  ultralytics.nn.modules.block.C2f             [1280, 1280, 3, True]\r\n  9                  -1  1   1647365  ultralytics.nn.modules.head.Classify         [1280, 5]\r\nYOLOv8x-cls summary: 183 layers, 56148245 parameters, 56148245 gradients, 154.3 GFLOPs\r\nTransferred 300/302 items from pretrained weights\r\nDDP: debug command /opt/conda/bin/python3 -m torch.distributed.run --nproc_per_node 2 --master_port 35381 /root/.config/Ultralytics/DDP/_temp_vhsexeb0140059747641488.py\r\nUltralytics YOLOv8.2.48 \ud83d\ude80 Python-3.10.14 torch-2.2.2 CUDA:0 (NVIDIA GeForce RTX 4090, 24217MiB)\r\n                                                      CUDA:1 (NVIDIA GeForce RTX 4090, 24217MiB)\r\ntrain: /root/project-5-latest/train... found 54064 images in 5 classes \u2705\r\nval: /root/project-5-latest/val... found 13686 images in 5 classes \u2705\r\ntest: None...\r\nwandb: (1) Create a W&B account\r\nwandb: (2) Use an existing W&B account\r\nwandb: (3) Don't visualize my results\r\nwandb: Enter your choice: 3\r\nwandb: You chose \"Don't visualize my results\"\r\nwandb: Tracking run with wandb version 0.17.3\r\nwandb: W&B syncing is set to `offline` in this directory.\r\nwandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\r\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\r\nAMP: checks passed \u2705\r\ntrain: Scanning /root/project-5-latest/train... 54064 images, 0 corrupt: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 54064/54064 [00:19<00:00, 2\r\ntrain: New cache created: /root/project-5-latest/train.cache\r\nval: Scanning /root/project-5-latest/val... 13686 images, 0 corrupt: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13686/13686 [00:04<00:00, 2746.\r\nval: New cache created: /root/project-5-latest/val.cache\r\noptimizer: SGD(lr=0.01, momentum=0.937) with parameter groups 50 weight(decay=0.0), 51 weight(decay=0.0005), 51 bias(decay=0.0)\r\nImage sizes 224 train, 224 val\r\nUsing 16 dataloader workers\r\nLogging results to runs/classify/train\r\nStarting training for 100 epochs...\r\n\r\n      Epoch    GPU_mem       loss  Instances       Size\r\n  0%|          | 0/845 [00:00<?, ?it/s]\r\nTraceback (most recent call last):\r\n  File \"/root/.config/Ultralytics/DDP/_temp_vhsexeb0140059747641488.py\", line 13, in <module>\r\n    results = trainer.train()\r\n  File \"/opt/conda/lib/python3.10/site-packages/ultralytics/engine/trainer.py\", line 204, in train\r\n    self._do_train(world_size)\r\n  File \"/opt/conda/lib/python3.10/site-packages/ultralytics/engine/trainer.py\", line 381, in _do_train\r\n    self.loss, self.loss_items = self.model(batch)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1523, in forward\r\n    else self._run_ddp_forward(*inputs, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1359, in _run_ddp_forward\r\n    return self.module(*inputs, **kwargs)  # type: ignore[index]\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/ultralytics/nn/tasks.py\", line 101, in forward\r\n    return self.loss(x, *args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/ultralytics/nn/tasks.py\", line 283, in loss\r\n    return self.criterion(preds, batch)\r\nTypeError: 'NoneType' object is not callable\r\nTraceback (most recent call last):\r\n  File \"/root/.config/Ultralytics/DDP/_temp_vhsexeb0140059747641488.py\", line 13, in <module>\r\n    results = trainer.train()\r\n  File \"/opt/conda/lib/python3.10/site-packages/ultralytics/engine/trainer.py\", line 204, in train\r\n    self._do_train(world_size)\r\n  File \"/opt/conda/lib/python3.10/site-packages/ultralytics/engine/trainer.py\", line 381, in _do_train\r\n    self.loss, self.loss_items = self.model(batch)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1523, in forward\r\n    else self._run_ddp_forward(*inputs, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1359, in _run_ddp_forward\r\n    return self.module(*inputs, **kwargs)  # type: ignore[index]\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/ultralytics/nn/tasks.py\", line 101, in forward\r\n    return self.loss(x, *args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/ultralytics/nn/tasks.py\", line 283, in loss\r\n    return self.criterion(preds, batch)\r\nTypeError: 'NoneType' object is not callable\r\nTraceback (most recent call last):\r\n  File \"/root/.config/Ultralytics/DDP/_temp_vhsexeb0140059747641488.py\", line 13, in <module>\r\n    results = trainer.train()\r\n  File \"/opt/conda/lib/python3.10/site-packages/ultralytics/engine/trainer.py\", line 204, in train\r\n    self._do_train(world_size)\r\n  File \"/opt/conda/lib/python3.10/site-packages/ultralytics/engine/trainer.py\", line 381, in _do_train\r\n    self.loss, self.loss_items = self.model(batch)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1523, in forward\r\n    else self._run_ddp_forward(*inputs, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1359, in _run_ddp_forward\r\n    return self.module(*inputs, **kwargs)  # type: ignore[index]\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/ultralytics/nn/tasks.py\", line 101, in forward\r\n    return self.loss(x, *args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/ultralytics/nn/tasks.py\", line 283, in loss\r\n    return self.criterion(preds, batch)\r\nTypeError: 'NoneType' object is not callable\r\nwandb: You can sync this run to the cloud by running:\r\nwandb: wandb sync /root/wandb/offline-run-20240703_171208-6f7ymrhl\r\nwandb: Find logs at: ./wandb/offline-run-20240703_171208-6f7ymrhl/logs\r\nwandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\r\n[2024-07-03 17:12:44,764] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1959 closing signal SIGTERM\r\n[2024-07-03 17:12:45,229] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 1 (pid: 1960) of binary: /opt/conda/bin/python3\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 816, in <module>\r\n    main()\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 812, in main\r\n    run(args)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 803, in run\r\n    elastic_launch(\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 135, in __call__\r\n    return launch_agent(self._config, self._entrypoint, list(args))\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\r\n    raise ChildFailedError(\r\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError:\r\n============================================================\r\n/root/.config/Ultralytics/DDP/_temp_vhsexeb0140059747641488.py FAILED\r\n------------------------------------------------------------\r\nFailures:\r\n  <NO_OTHER_FAILURES>\r\n------------------------------------------------------------\r\nRoot Cause (first observed failure):\r\n[0]:\r\n  time      : 2024-07-03_17:12:44\r\n  host      : 14ea1c502083\r\n  rank      : 1 (local_rank: 1)\r\n  exitcode  : 1 (pid: 1960)\r\n  error_file: <N/A>\r\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n============================================================\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/bin/yolo\", line 8, in <module>\r\n    sys.exit(entrypoint())\r\n  File \"/opt/conda/lib/python3.10/site-packages/ultralytics/cfg/__init__.py\", line 591, in entrypoint\r\n    getattr(model, mode)(**overrides)  # default args from model\r\n  File \"/opt/conda/lib/python3.10/site-packages/ultralytics/engine/model.py\", line 650, in train\r\n    self.trainer.train()\r\n  File \"/opt/conda/lib/python3.10/site-packages/ultralytics/engine/trainer.py\", line 199, in train\r\n    raise e\r\n  File \"/opt/conda/lib/python3.10/site-packages/ultralytics/engine/trainer.py\", line 197, in train\r\n    subprocess.run(cmd, check=True)\r\n  File \"/opt/conda/lib/python3.10/subprocess.py\", line 526, in run\r\n    raise CalledProcessError(retcode, process.args,\r\nsubprocess.CalledProcessError: Command '['/opt/conda/bin/python3', '-m', 'torch.distributed.run', '--nproc_per_node', '2', '--master_port', '35381', '/root/.config/Ultralytics/DDP/_temp_vhsexeb0140059747641488.py']' returned non-zero exit status 1.\r\n```\r\n\r\n### Environment\r\n\r\n```\r\nroot@C.11372351:~$ yolo checks\r\nUltralytics YOLOv8.2.48 \ud83d\ude80 Python-3.10.14 torch-2.2.2 CUDA:0 (NVIDIA GeForce RTX 4090, 24217MiB)\r\nSetup complete \u2705 (192 CPUs, 503.7 GB RAM, 3.8/16.0 GB disk)\r\n\r\nOS                  Linux-5.4.0-150-generic-x86_64-with-glibc2.35\r\nEnvironment         Docker\r\nPython              3.10.14\r\nInstall             pip\r\nRAM                 503.72 GB\r\nCPU                 AMD EPYC 7R12 48-Core Processor\r\nCUDA                12.1\r\n\r\nnumpy               \u2705 1.23.5<2.0.0,>=1.23.0\r\nmatplotlib          \u2705 3.9.0>=3.3.0\r\nopencv-python       \u2705 4.10.0.84>=4.6.0\r\npillow              \u2705 10.2.0>=7.1.2\r\npyyaml              \u2705 6.0.1>=5.3.1\r\nrequests            \u2705 2.31.0>=2.23.0\r\nscipy               \u2705 1.14.0>=1.4.1\r\ntorch               \u2705 2.2.2>=1.8.0\r\ntorchvision         \u2705 0.17.2>=0.9.0\r\ntqdm                \u2705 4.65.0>=4.64.0\r\npsutil              \u2705 5.9.0\r\npy-cpuinfo          \u2705 9.0.0\r\npandas              \u2705 2.2.2>=1.1.4\r\nseaborn             \u2705 0.13.2>=0.11.0\r\n```\r\n\r\n### Minimal Reproducible Example\r\n```\r\nyolo task=classify mode=train model=yolov8x-cls.pt data=project-5-latest epochs=100 patience=20 batch=64 imgsz=224 augment=True optimizer=SGD cache=memory device=0,1\r\n```\r\n\r\n### Additional\r\n\r\n\r\nThe error was resolved by modifying the `loss` method in the `ultralytics/nn/tasks.py` file.\r\n\r\n**Original Code:**\r\n```python\r\ndef loss(self, batch, preds=None):\r\n    \"\"\"\r\n    Compute loss.\r\n\r\n    Args:\r\n        batch (dict): Batch to compute loss on\r\n        preds (torch.Tensor | List[torch.Tensor]): Predictions.\r\n    \"\"\"\r\n    if not hasattr(self, \"criterion\"):\r\n        self.criterion = self.init_criterion()\r\n```\r\n\r\n**Modified Code:**\r\n```python\r\ndef loss(self, batch, preds=None):\r\n    \"\"\"\r\n    Compute loss.\r\n\r\n    Args:\r\n        batch (dict): Batch to compute loss on\r\n        preds (torch.Tensor | List[torch.Tensor]): Predictions.\r\n    \"\"\"\r\n    self.criterion = self.init_criterion()\r\n```\r\n\r\nTo ensure that the `criterion` is always initialized correctly, I removed the `if not hasattr(self, \"criterion\")` check. However, I understand that this check might be necessary in certain contexts.\r\n\r\n**Proposed Code Change for PR:**\r\nI would like to submit a pull request to modify the code to ensure that the `criterion` is initialized while keeping the original condition. Here is the proposed change:\r\n\r\n```python\r\ndef loss(self, batch, preds=None):\r\n    \"\"\"\r\n    Compute loss.\r\n\r\n    Args:\r\n        batch (dict): Batch to compute loss on\r\n        preds (torch.Tensor | List[torch.Tensor]): Predictions.\r\n    \"\"\"\r\n    if not hasattr(self, \"criterion\"):\r\n        self.criterion = self.init_criterion()\r\n    else:\r\n        # Re-initialize criterion to ensure it's not None\r\n        self.criterion = self.init_criterion()\r\n```\r\n\r\nThis change ensures that the criterion is always initialized, preventing the NoneType error, while retaining the original condition.\r\n\r\nPlease let me know if this solution is acceptable, or if there are any other suggestions or best practices I should follow.\r\n\r\nThank you!\r\n\r\nBest regards,\r\nAlyetama\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [X] Yes I'd like to help by submitting a PR!\n", "hints_text": "@Alyetama hi Alyetama,\n\nThank you for your detailed report and for proposing a solution! It\u2019s great to see your initiative in addressing the issue. The `NoneType` error you're encountering during multi-GPU training is indeed a critical one.\n\nBefore proceeding, could you please verify if the issue persists with the latest versions of the Ultralytics YOLO repository and all dependencies? This ensures that any recent fixes or updates are taken into account.\n\nAdditionally, providing a minimal reproducible example is crucial for us to replicate and diagnose the issue effectively. You can refer to our [Minimum Reproducible Example](https://docs.ultralytics.com/help/minimum_reproducible_example) guide for more details on how to create one.\n\nRegarding your proposed code change, it seems like a reasonable approach to ensure the `criterion` is always initialized. However, re-initializing the `criterion` within the `else` block might be redundant. Instead, we could ensure the `criterion` is initialized correctly during the model setup phase. Here\u2019s a refined version of your proposed change:\n\n```python\ndef loss(self, batch, preds=None):\n    \"\"\"\n    Compute loss.\n\n    Args:\n        batch (dict): Batch to compute loss on\n        preds (torch.Tensor | List[torch.Tensor]): Predictions.\n    \"\"\"\n    if not hasattr(self, \"criterion\") or self.criterion is None:\n        self.criterion = self.init_criterion()\n    return self.criterion(preds, batch)\n```\n\nThis ensures that `self.criterion` is initialized if it doesn't exist or if it is `None`, preventing the `NoneType` error.\n\nFeel free to submit a Pull Request (PR) with this change. The community and the Ultralytics team will review it and provide feedback. Your contribution is highly appreciated! \ud83d\ude80\n\nIf you have any further questions or need additional assistance, please don't hesitate to ask.\n@Alyetama I am able to reproduce this issue using our `imagnet10` dataset.", "created_at": "2024-07-03T22:02:14Z"}
{"repo": "ultralytics/ultralytics", "pull_number": 12774, "instance_id": "ultralytics__ultralytics-12774", "issue_numbers": ["12723"], "base_commit": "e26036d9654845ef8e1df6c1f15bd674167eec53", "patch": "diff --git a/ultralytics/engine/results.py b/ultralytics/engine/results.py\nindex 5cfd9c5a225..0c2161b4d6a 100644\n--- a/ultralytics/engine/results.py\n+++ b/ultralytics/engine/results.py\n@@ -94,7 +94,9 @@ class Results(SimpleClass):\n         tojson(normalize=False): Converts detection results to JSON format.\n     \"\"\"\n \n-    def __init__(self, orig_img, path, names, boxes=None, masks=None, probs=None, keypoints=None, obb=None) -> None:\n+    def __init__(\n+        self, orig_img, path, names, boxes=None, masks=None, probs=None, keypoints=None, obb=None, speed=None\n+    ) -> None:\n         \"\"\"\n         Initialize the Results class.\n \n@@ -115,7 +117,7 @@ def __init__(self, orig_img, path, names, boxes=None, masks=None, probs=None, ke\n         self.probs = Probs(probs) if probs is not None else None\n         self.keypoints = Keypoints(keypoints, self.orig_shape) if keypoints is not None else None\n         self.obb = OBB(obb, self.orig_shape) if obb is not None else None\n-        self.speed = {\"preprocess\": None, \"inference\": None, \"postprocess\": None}  # milliseconds per image\n+        self.speed = speed if speed is not None else {\"preprocess\": None, \"inference\": None, \"postprocess\": None}\n         self.names = names\n         self.path = path\n         self.save_dir = None\n@@ -180,8 +182,8 @@ def to(self, *args, **kwargs):\n         return self._apply(\"to\", *args, **kwargs)\n \n     def new(self):\n-        \"\"\"Return a new Results object with the same image, path, and names.\"\"\"\n-        return Results(orig_img=self.orig_img, path=self.path, names=self.names)\n+        \"\"\"Return a new Results object with the same image, path, names and speed.\"\"\"\n+        return Results(orig_img=self.orig_img, path=self.path, names=self.names, speed=self.speed)\n \n     def plot(\n         self,\n", "test_patch": "", "problem_statement": "Inference speed is None after transferring results to CPU\n### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### YOLOv8 Component\n\nPredict\n\n### Bug\n\nI use the `speed` attribute to check the inference time of the network. However, I noticed that the speeds are becoming `None` after transferring the detection results to the CPU.\n\n### Environment\n\nUltralytics YOLOv8.2.5 \ud83d\ude80 Python-3.10.13 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3090, 24257MiB)\r\nSetup complete \u2705 (32 CPUs, 62.7 GB RAM, 345.5/915.3 GB disk)\r\n\r\nOS                  Linux-6.5.0-28-generic-x86_64-with-glibc2.35\r\nEnvironment         Linux\r\nPython              3.10.13\r\nInstall             pip\r\nRAM                 62.71 GB\r\nCPU                 AMD Ryzen 9 5950X 16-Core Processor\r\nCUDA                12.1\r\n\r\nmatplotlib          \u2705 3.8.2>=3.3.0\r\nopencv-python       \u2705 4.8.1.78>=4.6.0\r\npillow              \u2705 10.1.0>=7.1.2\r\npyyaml              \u2705 6.0>=5.3.1\r\nrequests            \u2705 2.31.0>=2.23.0\r\nscipy               \u2705 1.11.4>=1.4.1\r\ntorch               \u2705 2.1.1>=1.8.0\r\ntorchvision         \u2705 0.16.1>=0.9.0\r\ntqdm                \u2705 4.66.1>=4.64.0\r\npsutil              \u2705 5.9.1\r\npy-cpuinfo          \u2705 9.0.0\r\nthop                \u2705 0.1.1-2209072238>=0.1.1\r\npandas              \u2705 1.4.2>=1.1.4\r\nseaborn             \u2705 0.13.0>=0.11.0\n\n### Minimal Reproducible Example\n\n```python\r\nfrom ultralytics import YOLO\r\n\r\npath_to_weights = ...\r\npath_to_image = ...\r\n\r\nmodel = YOLO(path_to_weights)\r\n\r\nresults = model.predict(path_to_image)[0]\r\n\r\nprint(results.speed)\r\n\r\nresults = results.cpu()\r\n\r\nprint(results.speed)\r\n```\r\n\r\nThis results in:\r\n\r\n```\r\n{'preprocess': 21.38376235961914, 'inference': 49.85451698303223, 'postprocess': 200.82473754882812}\r\n{'preprocess': None, 'inference': None, 'postprocess': None}\r\n```\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [x] Yes I'd like to help by submitting a PR!\n", "hints_text": "", "created_at": "2024-05-17T11:15:57Z"}
{"repo": "ultralytics/ultralytics", "pull_number": 12652, "instance_id": "ultralytics__ultralytics-12652", "issue_numbers": ["9835"], "base_commit": "b3dfbdda0239037e83b9949ebd4fb9758a75032e", "patch": "diff --git a/ultralytics/__init__.py b/ultralytics/__init__.py\nindex 0bb5c41e8be..5e1364da9aa 100644\n--- a/ultralytics/__init__.py\n+++ b/ultralytics/__init__.py\n@@ -1,6 +1,6 @@\n # Ultralytics YOLO \ud83d\ude80, AGPL-3.0 license\n \n-__version__ = \"8.2.14\"\n+__version__ = \"8.2.15\"\n \n from ultralytics.data.explorer.explorer import Explorer\n from ultralytics.models import RTDETR, SAM, YOLO, YOLOWorld\ndiff --git a/ultralytics/utils/torch_utils.py b/ultralytics/utils/torch_utils.py\nindex f859cd5f859..919fee07a20 100644\n--- a/ultralytics/utils/torch_utils.py\n+++ b/ultralytics/utils/torch_utils.py\n@@ -399,8 +399,13 @@ def copy_attr(a, b, include=(), exclude=()):\n \n \n def get_latest_opset():\n-    \"\"\"Return second-most (for maturity) recently supported ONNX opset by this version of torch.\"\"\"\n-    return max(int(k[14:]) for k in vars(torch.onnx) if \"symbolic_opset\" in k) - 1  # opset\n+    \"\"\"Return the second-most recent ONNX opset version supported by this version of PyTorch, adjusted for maturity.\"\"\"\n+    if TORCH_1_13:\n+        # If the PyTorch>=1.13, dynamically compute the latest opset minus one using 'symbolic_opset'\n+        return max(int(k[14:]) for k in vars(torch.onnx) if \"symbolic_opset\" in k) - 1\n+    # Otherwise for PyTorch<=1.12 return the corresponding predefined opset\n+    version = torch.onnx.producer_version.rsplit(\".\", 1)[0]  # i.e. '2.3'\n+    return {\"1.12\": 15, \"1.11\": 14, \"1.10\": 13, \"1.9\": 12, \"1.8\": 12}.get(version, 12)\n \n \n def intersect_dicts(da, db, exclude=()):\n", "test_patch": "", "problem_statement": "\u6a21\u578b\u8f6c\u6362\u9519\u8bef :TensorRT: export failure \u274c 0.0s: max() arg is an empty sequence    \n### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### YOLOv8 Component\n\nExport\n\n### Bug\n\n(yolov8_seg) wudengchao@wudengchao:~/ultralytics-main$ python\r\nPython 3.8.18 (default, Sep 11 2023, 13:40:15) \r\n[GCC 11.2.0] :: Anaconda, Inc. on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from ultralytics import YOLO\r\n>>> model = YOLO('yolov8m.pt')\r\n>>> model.export(format='engine')\r\nWARNING \u26a0\ufe0f TensorRT requires GPU export, automatically assigning device=0\r\nUltralytics YOLOv8.1.44 \ud83d\ude80 Python-3.8.18 torch-1.12.1+cu116 CUDA:0 (NVIDIA RTX A2000 12GB, 12036MiB)\r\nYOLOv8m summary (fused): 218 layers, 25886080 parameters, 0 gradients, 78.9 GFLOPs\r\n\r\nPyTorch: starting from 'yolov8m.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (49.7 MB)\r\nONNX: export failure \u274c 0.0s: max() arg is an empty sequence\r\nTensorRT: export failure \u274c 0.0s: max() arg is an empty sequence\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/wudengchao/ultralytics-main/ultralytics/engine/model.py\", line 601, in export\r\n    return Exporter(overrides=args, _callbacks=self.callbacks)(model=self.model)\r\n  File \"/home/wudengchao/.conda/envs/yolov8_seg/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/wudengchao/ultralytics-main/ultralytics/engine/exporter.py\", line 286, in __call__\r\n    f[1], _ = self.export_engine()\r\n  File \"/home/wudengchao/ultralytics-main/ultralytics/engine/exporter.py\", line 139, in outer_func\r\n    raise e\r\n  File \"/home/wudengchao/ultralytics-main/ultralytics/engine/exporter.py\", line 134, in outer_func\r\n    f, model = inner_func(*args, **kwargs)\r\n  File \"/home/wudengchao/ultralytics-main/ultralytics/engine/exporter.py\", line 657, in export_engine\r\n    f_onnx, _ = self.export_onnx()  # run before trt import https://github.com/ultralytics/ultralytics/issues/7016\r\n  File \"/home/wudengchao/ultralytics-main/ultralytics/engine/exporter.py\", line 139, in outer_func\r\n    raise e\r\n  File \"/home/wudengchao/ultralytics-main/ultralytics/engine/exporter.py\", line 134, in outer_func\r\n    f, model = inner_func(*args, **kwargs)\r\n  File \"/home/wudengchao/ultralytics-main/ultralytics/engine/exporter.py\", line 362, in export_onnx\r\n    opset_version = self.args.opset or get_latest_opset()\r\n  File \"/home/wudengchao/ultralytics-main/ultralytics/utils/torch_utils.py\", line 393, in get_latest_opset\r\n    return max(int(k[14:]) for k in vars(torch.onnx) if \"symbolic_opset\" in k) - 1  # opset\r\nValueError: max() arg is an empty sequence\n\n### Environment\n\nUltralytics YOLOv8.1.44 \ud83d\ude80 Python-3.8.18 torch-1.12.1+cu116 CUDA:0 (NVIDIA RTX A2000 12GB, 12036MiB)\n\n### Minimal Reproducible Example\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!\n", "hints_text": "", "created_at": "2024-05-13T07:34:35Z"}
{"repo": "ultralytics/ultralytics", "pull_number": 10423, "instance_id": "ultralytics__ultralytics-10423", "issue_numbers": ["10397"], "base_commit": "299797ff9eb5b191481e04c1d4a0076927810b6c", "patch": "diff --git a/ultralytics/__init__.py b/ultralytics/__init__.py\nindex 4c98c5c08f5..1909d745146 100644\n--- a/ultralytics/__init__.py\n+++ b/ultralytics/__init__.py\n@@ -1,6 +1,6 @@\n # Ultralytics YOLO \ud83d\ude80, AGPL-3.0 license\n \n-__version__ = \"8.2.8\"\n+__version__ = \"8.2.9\"\n \n from ultralytics.data.explorer.explorer import Explorer\n from ultralytics.models import RTDETR, SAM, YOLO, YOLOWorld\ndiff --git a/ultralytics/data/augment.py b/ultralytics/data/augment.py\nindex 43a7c9ec98c..08b7f0f2ca4 100644\n--- a/ultralytics/data/augment.py\n+++ b/ultralytics/data/augment.py\n@@ -10,6 +10,7 @@\n import torch\n from PIL import Image\n \n+from ultralytics.data.utils import polygons2masks, polygons2masks_overlap\n from ultralytics.utils import LOGGER, colorstr\n from ultralytics.utils.checks import check_version\n from ultralytics.utils.instance import Instances\n@@ -17,8 +18,6 @@\n from ultralytics.utils.ops import segment2box, xyxyxyxy2xywhr\n from ultralytics.utils.torch_utils import TORCHVISION_0_10, TORCHVISION_0_11, TORCHVISION_0_13\n \n-from .utils import polygons2masks, polygons2masks_overlap\n-\n DEFAULT_MEAN = (0.0, 0.0, 0.0)\n DEFAULT_STD = (1.0, 1.0, 1.0)\n DEFAULT_CROP_FRACTION = 1.0\ndiff --git a/ultralytics/data/base.py b/ultralytics/data/base.py\nindex 6f0ca7fe153..2218d31f362 100644\n--- a/ultralytics/data/base.py\n+++ b/ultralytics/data/base.py\n@@ -14,10 +14,9 @@\n import psutil\n from torch.utils.data import Dataset\n \n+from ultralytics.data.utils import FORMATS_HELP_MSG, HELP_URL, IMG_FORMATS\n from ultralytics.utils import DEFAULT_CFG, LOCAL_RANK, LOGGER, NUM_THREADS, TQDM\n \n-from .utils import FORMATS_HELP_MSG, HELP_URL, IMG_FORMATS\n-\n \n class BaseDataset(Dataset):\n     \"\"\"\ndiff --git a/ultralytics/data/build.py b/ultralytics/data/build.py\nindex 6fa611cc402..157266eb3d9 100644\n--- a/ultralytics/data/build.py\n+++ b/ultralytics/data/build.py\n@@ -9,6 +9,7 @@\n from PIL import Image\n from torch.utils.data import dataloader, distributed\n \n+from ultralytics.data.dataset import GroundingDataset, YOLODataset, YOLOMultiModalDataset\n from ultralytics.data.loaders import (\n     LOADERS,\n     LoadImagesAndVideos,\n@@ -19,13 +20,10 @@\n     SourceTypes,\n     autocast_list,\n )\n-from ultralytics.data.utils import IMG_FORMATS, VID_FORMATS\n+from ultralytics.data.utils import IMG_FORMATS, PIN_MEMORY, VID_FORMATS\n from ultralytics.utils import LINUX, NUM_THREADS, RANK, colorstr\n from ultralytics.utils.checks import check_file\n \n-from .dataset import GroundingDataset, YOLODataset, YOLOMultiModalDataset\n-from .utils import PIN_MEMORY\n-\n \n class InfiniteDataLoader(dataloader.DataLoader):\n     \"\"\"\ndiff --git a/ultralytics/engine/exporter.py b/ultralytics/engine/exporter.py\nindex d2536542428..227a76d5fda 100644\n--- a/ultralytics/engine/exporter.py\n+++ b/ultralytics/engine/exporter.py\n@@ -64,9 +64,10 @@\n import numpy as np\n import torch\n \n-from ultralytics.cfg import get_cfg\n+from ultralytics.cfg import TASK2DATA, get_cfg\n+from ultralytics.data import build_dataloader\n from ultralytics.data.dataset import YOLODataset\n-from ultralytics.data.utils import check_det_dataset\n+from ultralytics.data.utils import check_cls_dataset, check_det_dataset\n from ultralytics.nn.autobackend import check_class_names, default_class_names\n from ultralytics.nn.modules import C2f, Detect, RTDETRDecoder\n from ultralytics.nn.tasks import DetectionModel, SegmentationModel, WorldModel\n@@ -169,7 +170,7 @@ def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):\n         callbacks.add_integration_callbacks(self)\n \n     @smart_inference_mode()\n-    def __call__(self, model=None):\n+    def __call__(self, model=None) -> str:\n         \"\"\"Returns list of exported files/dirs after running callbacks.\"\"\"\n         self.run_callbacks(\"on_export_start\")\n         t = time.time()\n@@ -211,7 +212,12 @@ def __call__(self, model=None):\n                 \"(torchscript, onnx, openvino, engine, coreml) formats. \"\n                 \"See https://docs.ultralytics.com/models/yolo-world for details.\"\n             )\n-\n+        if self.args.int8 and not self.args.data:\n+            self.args.data = DEFAULT_CFG.data or TASK2DATA[getattr(model, \"task\", \"detect\")]  # assign default data\n+            LOGGER.warning(\n+                \"WARNING \u26a0\ufe0f INT8 export requires a missing 'data' arg for calibration. \"\n+                f\"Using default 'data={self.args.data}'.\"\n+            )\n         # Input\n         im = torch.zeros(self.args.batch, 3, *self.imgsz).to(self.device)\n         file = Path(\n@@ -333,6 +339,23 @@ def __call__(self, model=None):\n         self.run_callbacks(\"on_export_end\")\n         return f  # return list of exported files/dirs\n \n+    def get_int8_calibration_dataloader(self, prefix=\"\"):\n+        \"\"\"Build and return a dataloader suitable for calibration of INT8 models.\"\"\"\n+        LOGGER.info(f\"{prefix} collecting INT8 calibration images from 'data={self.args.data}'\")\n+        data = (check_cls_dataset if self.model.task == \"classify\" else check_det_dataset)(self.args.data)\n+        dataset = YOLODataset(\n+            data[self.args.split or \"val\"],\n+            data=data,\n+            task=self.model.task,\n+            imgsz=self.imgsz[0],\n+            augment=False,\n+            batch_size=self.args.batch,\n+        )\n+        n = len(dataset)\n+        if n < 300:\n+            LOGGER.warning(f\"{prefix} WARNING \u26a0\ufe0f >300 images recommended for INT8 calibration, found {n} images.\")\n+        return build_dataloader(dataset, batch=self.args.batch, workers=0)  # required for batch loading\n+\n     @try_export\n     def export_torchscript(self, prefix=colorstr(\"TorchScript:\")):\n         \"\"\"YOLOv8 TorchScript model export.\"\"\"\n@@ -442,37 +465,21 @@ def serialize(ov_model, file):\n         if self.args.int8:\n             fq = str(self.file).replace(self.file.suffix, f\"_int8_openvino_model{os.sep}\")\n             fq_ov = str(Path(fq) / self.file.with_suffix(\".xml\").name)\n-            if not self.args.data:\n-                self.args.data = DEFAULT_CFG.data or \"coco128.yaml\"\n-                LOGGER.warning(\n-                    f\"{prefix} WARNING \u26a0\ufe0f INT8 export requires a missing 'data' arg for calibration. \"\n-                    f\"Using default 'data={self.args.data}'.\"\n-                )\n             check_requirements(\"nncf>=2.8.0\")\n             import nncf\n \n-            def transform_fn(data_item):\n+            def transform_fn(data_item) -> np.ndarray:\n                 \"\"\"Quantization transform function.\"\"\"\n-                assert (\n-                    data_item[\"img\"].dtype == torch.uint8\n-                ), \"Input image must be uint8 for the quantization preprocessing\"\n-                im = data_item[\"img\"].numpy().astype(np.float32) / 255.0  # uint8 to fp16/32 and 0 - 255 to 0.0 - 1.0\n+                data_item: torch.Tensor = data_item[\"img\"] if isinstance(data_item, dict) else data_item\n+                assert data_item.dtype == torch.uint8, \"Input image must be uint8 for the quantization preprocessing\"\n+                im = data_item.numpy().astype(np.float32) / 255.0  # uint8 to fp16/32 and 0 - 255 to 0.0 - 1.0\n                 return np.expand_dims(im, 0) if im.ndim == 3 else im\n \n             # Generate calibration data for integer quantization\n-            LOGGER.info(f\"{prefix} collecting INT8 calibration images from 'data={self.args.data}'\")\n-            data = check_det_dataset(self.args.data)\n-            dataset = YOLODataset(data[\"val\"], data=data, task=self.model.task, imgsz=self.imgsz[0], augment=False)\n-            n = len(dataset)\n-            if n < 300:\n-                LOGGER.warning(f\"{prefix} WARNING \u26a0\ufe0f >300 images recommended for INT8 calibration, found {n} images.\")\n-            quantization_dataset = nncf.Dataset(dataset, transform_fn)\n-\n             ignored_scope = None\n             if isinstance(self.model.model[-1], Detect):\n                 # Includes all Detect subclasses like Segment, Pose, OBB, WorldDetect\n                 head_module_name = \".\".join(list(self.model.named_modules())[-1][0].split(\".\")[:2])\n-\n                 ignored_scope = nncf.IgnoredScope(  # ignore operations\n                     patterns=[\n                         f\".*{head_module_name}/.*/Add\",\n@@ -485,7 +492,10 @@ def transform_fn(data_item):\n                 )\n \n             quantized_ov_model = nncf.quantize(\n-                ov_model, quantization_dataset, preset=nncf.QuantizationPreset.MIXED, ignored_scope=ignored_scope\n+                model=ov_model,\n+                calibration_dataset=nncf.Dataset(self.get_int8_calibration_dataloader(prefix), transform_fn),\n+                preset=nncf.QuantizationPreset.MIXED,\n+                ignored_scope=ignored_scope,\n             )\n             serialize(quantized_ov_model, fq_ov)\n             return fq, None\n@@ -787,11 +797,9 @@ def export_saved_model(self, prefix=colorstr(\"TensorFlow SavedModel:\")):\n             verbosity = \"info\"\n             if self.args.data:\n                 # Generate calibration data for integer quantization\n-                LOGGER.info(f\"{prefix} collecting INT8 calibration images from 'data={self.args.data}'\")\n-                data = check_det_dataset(self.args.data)\n-                dataset = YOLODataset(data[\"val\"], data=data, imgsz=self.imgsz[0], augment=False)\n+                dataloader = self.get_int8_calibration_dataloader(prefix)\n                 images = []\n-                for i, batch in enumerate(dataset):\n+                for i, batch in enumerate(dataloader):\n                     if i >= 100:  # maximum number of calibration images\n                         break\n                     im = batch[\"img\"].permute(1, 2, 0)[None]  # list to nparray, CHW to BHWC\ndiff --git a/ultralytics/engine/model.py b/ultralytics/engine/model.py\nindex 677361e2501..1d88b2c5dcc 100644\n--- a/ultralytics/engine/model.py\n+++ b/ultralytics/engine/model.py\n@@ -572,7 +572,7 @@ def benchmark(\n     def export(\n         self,\n         **kwargs,\n-    ):\n+    ) -> str:\n         \"\"\"\n         Exports the model to a different format suitable for deployment.\n \n@@ -588,7 +588,7 @@ def export(\n                 model's overrides and method defaults.\n \n         Returns:\n-            (object): The exported model in the specified format, or an object related to the export process.\n+            (str): The exported model filename in the specified format, or an object related to the export process.\n \n         Raises:\n             AssertionError: If the model is not a PyTorch model.\n", "test_patch": "diff --git a/tests/__init__.py b/tests/__init__.py\nnew file mode 100644\nindex 00000000000..3356f1cadbf\n--- /dev/null\n+++ b/tests/__init__.py\n@@ -0,0 +1,22 @@\n+# Ultralytics YOLO \ud83d\ude80, AGPL-3.0 license\n+\n+from ultralytics.utils import ASSETS, ROOT, WEIGHTS_DIR, checks, is_dir_writeable\n+\n+# Constants used in tests\n+MODEL = WEIGHTS_DIR / \"path with spaces\" / \"yolov8n.pt\"  # test spaces in path\n+CFG = \"yolov8n.yaml\"\n+SOURCE = ASSETS / \"bus.jpg\"\n+TMP = (ROOT / \"../tests/tmp\").resolve()  # temp directory for test files\n+IS_TMP_WRITEABLE = is_dir_writeable(TMP)\n+CUDA_IS_AVAILABLE = checks.cuda_is_available()\n+CUDA_DEVICE_COUNT = checks.cuda_device_count()\n+\n+__all__ = (\n+    \"MODEL\",\n+    \"CFG\",\n+    \"SOURCE\",\n+    \"TMP\",\n+    \"IS_TMP_WRITEABLE\",\n+    \"CUDA_IS_AVAILABLE\",\n+    \"CUDA_DEVICE_COUNT\",\n+)\ndiff --git a/tests/test_cli.py b/tests/test_cli.py\nindex eeeab0727d9..9a31db24e15 100644\n--- a/tests/test_cli.py\n+++ b/tests/test_cli.py\n@@ -4,24 +4,14 @@\n \n import pytest\n \n+from ultralytics.cfg import TASK2DATA, TASK2MODEL, TASKS\n from ultralytics.utils import ASSETS, WEIGHTS_DIR, checks\n \n-CUDA_IS_AVAILABLE = checks.cuda_is_available()\n-CUDA_DEVICE_COUNT = checks.cuda_device_count()\n-TASK_ARGS = [\n-    (\"detect\", \"yolov8n\", \"coco8.yaml\"),\n-    (\"segment\", \"yolov8n-seg\", \"coco8-seg.yaml\"),\n-    (\"classify\", \"yolov8n-cls\", \"imagenet10\"),\n-    (\"pose\", \"yolov8n-pose\", \"coco8-pose.yaml\"),\n-    (\"obb\", \"yolov8n-obb\", \"dota8.yaml\"),\n-]  # (task, model, data)\n-EXPORT_ARGS = [\n-    (\"yolov8n\", \"torchscript\"),\n-    (\"yolov8n-seg\", \"torchscript\"),\n-    (\"yolov8n-cls\", \"torchscript\"),\n-    (\"yolov8n-pose\", \"torchscript\"),\n-    (\"yolov8n-obb\", \"torchscript\"),\n-]  # (model, format)\n+from . import CUDA_DEVICE_COUNT, CUDA_IS_AVAILABLE\n+\n+# Constants\n+TASK_MODEL_DATA = [(task, WEIGHTS_DIR / TASK2MODEL[task], TASK2DATA[task]) for task in TASKS]\n+MODELS = [WEIGHTS_DIR / TASK2MODEL[task] for task in TASKS]\n \n \n def run(cmd):\n@@ -38,28 +28,28 @@ def test_special_modes():\n     run(\"yolo cfg\")\n \n \n-@pytest.mark.parametrize(\"task,model,data\", TASK_ARGS)\n+@pytest.mark.parametrize(\"task,model,data\", TASK_MODEL_DATA)\n def test_train(task, model, data):\n     \"\"\"Test YOLO training for a given task, model, and data.\"\"\"\n-    run(f\"yolo train {task} model={model}.yaml data={data} imgsz=32 epochs=1 cache=disk\")\n+    run(f\"yolo train {task} model={model} data={data} imgsz=32 epochs=1 cache=disk\")\n \n \n-@pytest.mark.parametrize(\"task,model,data\", TASK_ARGS)\n+@pytest.mark.parametrize(\"task,model,data\", TASK_MODEL_DATA)\n def test_val(task, model, data):\n     \"\"\"Test YOLO validation for a given task, model, and data.\"\"\"\n-    run(f\"yolo val {task} model={WEIGHTS_DIR / model}.pt data={data} imgsz=32 save_txt save_json\")\n+    run(f\"yolo val {task} model={model} data={data} imgsz=32 save_txt save_json\")\n \n \n-@pytest.mark.parametrize(\"task,model,data\", TASK_ARGS)\n+@pytest.mark.parametrize(\"task,model,data\", TASK_MODEL_DATA)\n def test_predict(task, model, data):\n     \"\"\"Test YOLO prediction on sample assets for a given task and model.\"\"\"\n-    run(f\"yolo predict model={WEIGHTS_DIR / model}.pt source={ASSETS} imgsz=32 save save_crop save_txt\")\n+    run(f\"yolo predict model={model} source={ASSETS} imgsz=32 save save_crop save_txt\")\n \n \n-@pytest.mark.parametrize(\"model,format\", EXPORT_ARGS)\n-def test_export(model, format):\n+@pytest.mark.parametrize(\"model\", MODELS)\n+def test_export(model):\n     \"\"\"Test exporting a YOLO model to different formats.\"\"\"\n-    run(f\"yolo export model={WEIGHTS_DIR / model}.pt format={format} imgsz=32\")\n+    run(f\"yolo export model={model} format=torchscript imgsz=32\")\n \n \n def test_rtdetr(task=\"detect\", model=\"yolov8n-rtdetr.yaml\", data=\"coco8.yaml\"):\n@@ -129,10 +119,10 @@ def test_mobilesam():\n \n # Slow Tests -----------------------------------------------------------------------------------------------------------\n @pytest.mark.slow\n-@pytest.mark.parametrize(\"task,model,data\", TASK_ARGS)\n+@pytest.mark.parametrize(\"task,model,data\", TASK_MODEL_DATA)\n @pytest.mark.skipif(not CUDA_IS_AVAILABLE, reason=\"CUDA is not available\")\n @pytest.mark.skipif(CUDA_DEVICE_COUNT < 2, reason=\"DDP is not available\")\n def test_train_gpu(task, model, data):\n     \"\"\"Test YOLO training on GPU(s) for various tasks and models.\"\"\"\n-    run(f\"yolo train {task} model={model}.yaml data={data} imgsz=32 epochs=1 device=0\")  # single GPU\n-    run(f\"yolo train {task} model={model}.pt data={data} imgsz=32 epochs=1 device=0,1\")  # multi GPU\n+    run(f\"yolo train {task} model={model} data={data} imgsz=32 epochs=1 device=0\")  # single GPU\n+    run(f\"yolo train {task} model={model} data={data} imgsz=32 epochs=1 device=0,1\")  # multi GPU\ndiff --git a/tests/test_cuda.py b/tests/test_cuda.py\nindex b66860623eb..11e8f4e5820 100644\n--- a/tests/test_cuda.py\n+++ b/tests/test_cuda.py\n@@ -4,14 +4,9 @@\n import torch\n \n from ultralytics import YOLO\n-from ultralytics.utils import ASSETS, WEIGHTS_DIR, checks\n+from ultralytics.utils import ASSETS, WEIGHTS_DIR\n \n-CUDA_IS_AVAILABLE = checks.cuda_is_available()\n-CUDA_DEVICE_COUNT = checks.cuda_device_count()\n-\n-MODEL = WEIGHTS_DIR / \"path with spaces\" / \"yolov8n.pt\"  # test spaces in path\n-DATA = \"coco8.yaml\"\n-BUS = ASSETS / \"bus.jpg\"\n+from . import CUDA_DEVICE_COUNT, CUDA_IS_AVAILABLE, MODEL, SOURCE\n \n \n def test_checks():\n@@ -25,14 +20,14 @@ def test_checks():\n def test_export_engine():\n     \"\"\"Test exporting the YOLO model to NVIDIA TensorRT format.\"\"\"\n     f = YOLO(MODEL).export(format=\"engine\", device=0)\n-    YOLO(f)(BUS, device=0)\n+    YOLO(f)(SOURCE, device=0)\n \n \n @pytest.mark.skipif(not CUDA_IS_AVAILABLE, reason=\"CUDA is not available\")\n def test_train():\n     \"\"\"Test model training on a minimal dataset.\"\"\"\n     device = 0 if CUDA_DEVICE_COUNT == 1 else [0, 1]\n-    YOLO(MODEL).train(data=DATA, imgsz=64, epochs=1, device=device)  # requires imgsz>=64\n+    YOLO(MODEL).train(data=\"coco8.yaml\", imgsz=64, epochs=1, device=device)  # requires imgsz>=64\n \n \n @pytest.mark.slow\n@@ -42,22 +37,22 @@ def test_predict_multiple_devices():\n     model = YOLO(\"yolov8n.pt\")\n     model = model.cpu()\n     assert str(model.device) == \"cpu\"\n-    _ = model(BUS)  # CPU inference\n+    _ = model(SOURCE)  # CPU inference\n     assert str(model.device) == \"cpu\"\n \n     model = model.to(\"cuda:0\")\n     assert str(model.device) == \"cuda:0\"\n-    _ = model(BUS)  # CUDA inference\n+    _ = model(SOURCE)  # CUDA inference\n     assert str(model.device) == \"cuda:0\"\n \n     model = model.cpu()\n     assert str(model.device) == \"cpu\"\n-    _ = model(BUS)  # CPU inference\n+    _ = model(SOURCE)  # CPU inference\n     assert str(model.device) == \"cpu\"\n \n     model = model.cuda()\n     assert str(model.device) == \"cuda:0\"\n-    _ = model(BUS)  # CUDA inference\n+    _ = model(SOURCE)  # CUDA inference\n     assert str(model.device) == \"cuda:0\"\n \n \n@@ -93,10 +88,10 @@ def test_predict_sam():\n     model.info()\n \n     # Run inference\n-    model(BUS, device=0)\n+    model(SOURCE, device=0)\n \n     # Run inference with bboxes prompt\n-    model(BUS, bboxes=[439, 437, 524, 709], device=0)\n+    model(SOURCE, bboxes=[439, 437, 524, 709], device=0)\n \n     # Run inference with points prompt\n     model(ASSETS / \"zidane.jpg\", points=[900, 370], labels=[1], device=0)\ndiff --git a/tests/test_engine.py b/tests/test_engine.py\nindex 31088d971d6..73a3b744113 100644\n--- a/tests/test_engine.py\n+++ b/tests/test_engine.py\n@@ -9,11 +9,7 @@\n from ultralytics.models.yolo import classify, detect, segment\n from ultralytics.utils import ASSETS, DEFAULT_CFG, WEIGHTS_DIR\n \n-CFG_DET = \"yolov8n.yaml\"\n-CFG_SEG = \"yolov8n-seg.yaml\"\n-CFG_CLS = \"yolov8n-cls.yaml\"  # or 'squeezenet1_0'\n-CFG = get_cfg(DEFAULT_CFG)\n-MODEL = WEIGHTS_DIR / \"yolov8n\"\n+from . import MODEL\n \n \n def test_func(*args):  # noqa\n@@ -26,15 +22,16 @@ def test_export():\n     exporter = Exporter()\n     exporter.add_callback(\"on_export_start\", test_func)\n     assert test_func in exporter.callbacks[\"on_export_start\"], \"callback test failed\"\n-    f = exporter(model=YOLO(CFG_DET).model)\n+    f = exporter(model=YOLO(\"yolov8n.yaml\").model)\n     YOLO(f)(ASSETS)  # exported model inference\n \n \n def test_detect():\n     \"\"\"Test object detection functionality.\"\"\"\n-    overrides = {\"data\": \"coco8.yaml\", \"model\": CFG_DET, \"imgsz\": 32, \"epochs\": 1, \"save\": False}\n-    CFG.data = \"coco8.yaml\"\n-    CFG.imgsz = 32\n+    overrides = {\"data\": \"coco8.yaml\", \"model\": \"yolov8n.yaml\", \"imgsz\": 32, \"epochs\": 1, \"save\": False}\n+    cfg = get_cfg(DEFAULT_CFG)\n+    cfg.data = \"coco8.yaml\"\n+    cfg.imgsz = 32\n \n     # Trainer\n     trainer = detect.DetectionTrainer(overrides=overrides)\n@@ -43,7 +40,7 @@ def test_detect():\n     trainer.train()\n \n     # Validator\n-    val = detect.DetectionValidator(args=CFG)\n+    val = detect.DetectionValidator(args=cfg)\n     val.add_callback(\"on_val_start\", test_func)\n     assert test_func in val.callbacks[\"on_val_start\"], \"callback test failed\"\n     val(model=trainer.best)  # validate best.pt\n@@ -54,7 +51,7 @@ def test_detect():\n     assert test_func in pred.callbacks[\"on_predict_start\"], \"callback test failed\"\n     # Confirm there is no issue with sys.argv being empty.\n     with mock.patch.object(sys, \"argv\", []):\n-        result = pred(source=ASSETS, model=f\"{MODEL}.pt\")\n+        result = pred(source=ASSETS, model=MODEL)\n         assert len(result), \"predictor test failed\"\n \n     overrides[\"resume\"] = trainer.last\n@@ -70,9 +67,10 @@ def test_detect():\n \n def test_segment():\n     \"\"\"Test image segmentation functionality.\"\"\"\n-    overrides = {\"data\": \"coco8-seg.yaml\", \"model\": CFG_SEG, \"imgsz\": 32, \"epochs\": 1, \"save\": False}\n-    CFG.data = \"coco8-seg.yaml\"\n-    CFG.imgsz = 32\n+    overrides = {\"data\": \"coco8-seg.yaml\", \"model\": \"yolov8n-seg.yaml\", \"imgsz\": 32, \"epochs\": 1, \"save\": False}\n+    cfg = get_cfg(DEFAULT_CFG)\n+    cfg.data = \"coco8-seg.yaml\"\n+    cfg.imgsz = 32\n     # YOLO(CFG_SEG).train(**overrides)  # works\n \n     # Trainer\n@@ -82,7 +80,7 @@ def test_segment():\n     trainer.train()\n \n     # Validator\n-    val = segment.SegmentationValidator(args=CFG)\n+    val = segment.SegmentationValidator(args=cfg)\n     val.add_callback(\"on_val_start\", test_func)\n     assert test_func in val.callbacks[\"on_val_start\"], \"callback test failed\"\n     val(model=trainer.best)  # validate best.pt\n@@ -91,7 +89,7 @@ def test_segment():\n     pred = segment.SegmentationPredictor(overrides={\"imgsz\": [64, 64]})\n     pred.add_callback(\"on_predict_start\", test_func)\n     assert test_func in pred.callbacks[\"on_predict_start\"], \"callback test failed\"\n-    result = pred(source=ASSETS, model=f\"{MODEL}-seg.pt\")\n+    result = pred(source=ASSETS, model=WEIGHTS_DIR / \"yolov8n-seg.pt\")\n     assert len(result), \"predictor test failed\"\n \n     # Test resume\n@@ -108,9 +106,10 @@ def test_segment():\n \n def test_classify():\n     \"\"\"Test image classification functionality.\"\"\"\n-    overrides = {\"data\": \"imagenet10\", \"model\": CFG_CLS, \"imgsz\": 32, \"epochs\": 1, \"save\": False}\n-    CFG.data = \"imagenet10\"\n-    CFG.imgsz = 32\n+    overrides = {\"data\": \"imagenet10\", \"model\": \"yolov8n-cls.yaml\", \"imgsz\": 32, \"epochs\": 1, \"save\": False}\n+    cfg = get_cfg(DEFAULT_CFG)\n+    cfg.data = \"imagenet10\"\n+    cfg.imgsz = 32\n     # YOLO(CFG_SEG).train(**overrides)  # works\n \n     # Trainer\n@@ -120,7 +119,7 @@ def test_classify():\n     trainer.train()\n \n     # Validator\n-    val = classify.ClassificationValidator(args=CFG)\n+    val = classify.ClassificationValidator(args=cfg)\n     val.add_callback(\"on_val_start\", test_func)\n     assert test_func in val.callbacks[\"on_val_start\"], \"callback test failed\"\n     val(model=trainer.best)\ndiff --git a/tests/test_exports.py b/tests/test_exports.py\nnew file mode 100644\nindex 00000000000..76207230386\n--- /dev/null\n+++ b/tests/test_exports.py\n@@ -0,0 +1,128 @@\n+# Ultralytics YOLO \ud83d\ude80, AGPL-3.0 license\n+\n+import shutil\n+import uuid\n+from itertools import product\n+from pathlib import Path\n+\n+import pytest\n+\n+from ultralytics import YOLO\n+from ultralytics.cfg import TASK2DATA, TASK2MODEL, TASKS\n+from ultralytics.utils import (\n+    IS_RASPBERRYPI,\n+    LINUX,\n+    MACOS,\n+    WINDOWS,\n+    Retry,\n+    checks,\n+)\n+from ultralytics.utils.torch_utils import TORCH_1_9, TORCH_1_13\n+from . import MODEL, SOURCE\n+\n+# Constants\n+EXPORT_PARAMETERS_LIST = [  # generate all combinations but exclude those where both int8 and half are True\n+    (task, dynamic, int8, half, batch)\n+    for task, dynamic, int8, half, batch in product(TASKS, [True, False], [True, False], [True, False], [1, 2])\n+    if not (int8 and half)  # exclude cases where both int8 and half are True\n+]\n+\n+\n+def test_export_torchscript():\n+    \"\"\"Test exporting the YOLO model to TorchScript format.\"\"\"\n+    f = YOLO(MODEL).export(format=\"torchscript\", optimize=False, imgsz=32)\n+    YOLO(f)(SOURCE, imgsz=32)  # exported model inference\n+\n+\n+def test_export_onnx():\n+    \"\"\"Test exporting the YOLO model to ONNX format.\"\"\"\n+    f = YOLO(MODEL).export(format=\"onnx\", dynamic=True, imgsz=32)\n+    YOLO(f)(SOURCE, imgsz=32)  # exported model inference\n+\n+\n+@pytest.mark.skipif(checks.IS_PYTHON_3_12, reason=\"OpenVINO not supported in Python 3.12\")\n+@pytest.mark.skipif(not TORCH_1_13, reason=\"OpenVINO requires torch>=1.13\")\n+def test_export_openvino():\n+    \"\"\"Test exporting the YOLO model to OpenVINO format.\"\"\"\n+    f = YOLO(MODEL).export(format=\"openvino\", imgsz=32)\n+    YOLO(f)(SOURCE, imgsz=32)  # exported model inference\n+\n+\n+@pytest.mark.slow\n+@pytest.mark.skipif(checks.IS_PYTHON_3_12, reason=\"OpenVINO not supported in Python 3.12\")\n+@pytest.mark.skipif(not TORCH_1_13, reason=\"OpenVINO requires torch>=1.13\")\n+@pytest.mark.parametrize(\"task, dynamic, int8, half, batch\", EXPORT_PARAMETERS_LIST)\n+def test_export_openvino_matrix(task, dynamic, int8, half, batch):\n+    \"\"\"Test exporting the YOLO model to OpenVINO format.\"\"\"\n+    file = YOLO(TASK2MODEL[task]).export(\n+        format=\"openvino\",\n+        imgsz=32,\n+        dynamic=dynamic,\n+        int8=int8,\n+        half=half,\n+        batch=batch,\n+        data=TASK2DATA[task],\n+    )\n+    if WINDOWS:\n+        # Use unique filenames due to Windows file permissions bug possibly due to latent threaded use\n+        # See https://github.com/ultralytics/ultralytics/actions/runs/8957949304/job/24601616830?pr=10423\n+        file = Path(file)\n+        file = file.rename(file.with_stem(f\"{file.stem}-{uuid.uuid4()}\"))\n+    YOLO(file)([SOURCE] * batch, imgsz=64 if dynamic else 32)  # exported model inference\n+    with Retry(times=3, delay=1):  # retry in case of potential lingering multi-threaded file usage errors\n+        shutil.rmtree(file)\n+\n+\n+@pytest.mark.skipif(not TORCH_1_9, reason=\"CoreML>=7.2 not supported with PyTorch<=1.8\")\n+@pytest.mark.skipif(WINDOWS, reason=\"CoreML not supported on Windows\")  # RuntimeError: BlobWriter not loaded\n+@pytest.mark.skipif(IS_RASPBERRYPI, reason=\"CoreML not supported on Raspberry Pi\")\n+@pytest.mark.skipif(checks.IS_PYTHON_3_12, reason=\"CoreML not supported in Python 3.12\")\n+def test_export_coreml():\n+    \"\"\"Test exporting the YOLO model to CoreML format.\"\"\"\n+    if MACOS:\n+        f = YOLO(MODEL).export(format=\"coreml\", imgsz=32)\n+        YOLO(f)(SOURCE, imgsz=32)  # model prediction only supported on macOS for nms=False models\n+    else:\n+        YOLO(MODEL).export(format=\"coreml\", nms=True, imgsz=32)\n+\n+\n+@pytest.mark.skipif(not LINUX, reason=\"Test disabled as TF suffers from install conflicts on Windows and macOS\")\n+def test_export_tflite():\n+    \"\"\"\n+    Test exporting the YOLO model to TFLite format.\n+\n+    Note TF suffers from install conflicts on Windows and macOS.\n+    \"\"\"\n+    model = YOLO(MODEL)\n+    f = model.export(format=\"tflite\", imgsz=32)\n+    YOLO(f)(SOURCE, imgsz=32)\n+\n+\n+@pytest.mark.skipif(True, reason=\"Test disabled\")\n+@pytest.mark.skipif(not LINUX, reason=\"TF suffers from install conflicts on Windows and macOS\")\n+def test_export_pb():\n+    \"\"\"\n+    Test exporting the YOLO model to *.pb format.\n+\n+    Note TF suffers from install conflicts on Windows and macOS.\n+    \"\"\"\n+    model = YOLO(MODEL)\n+    f = model.export(format=\"pb\", imgsz=32)\n+    YOLO(f)(SOURCE, imgsz=32)\n+\n+\n+@pytest.mark.skipif(True, reason=\"Test disabled as Paddle protobuf and ONNX protobuf requirementsk conflict.\")\n+def test_export_paddle():\n+    \"\"\"\n+    Test exporting the YOLO model to Paddle format.\n+\n+    Note Paddle protobuf requirements conflicting with onnx protobuf requirements.\n+    \"\"\"\n+    YOLO(MODEL).export(format=\"paddle\", imgsz=32)\n+\n+\n+@pytest.mark.slow\n+def test_export_ncnn():\n+    \"\"\"Test exporting the YOLO model to NCNN format.\"\"\"\n+    f = YOLO(MODEL).export(format=\"ncnn\", imgsz=32)\n+    YOLO(f)(SOURCE, imgsz=32)  # exported model inference\ndiff --git a/tests/test_integrations.py b/tests/test_integrations.py\nindex f255b889322..d0126a0f1ad 100644\n--- a/tests/test_integrations.py\n+++ b/tests/test_integrations.py\n@@ -1,18 +1,18 @@\n # Ultralytics YOLO \ud83d\ude80, AGPL-3.0 license\n \n import contextlib\n+import os\n+import subprocess\n+import time\n from pathlib import Path\n \n import pytest\n \n from ultralytics import YOLO, download\n-from ultralytics.utils import ASSETS, DATASETS_DIR, ROOT, SETTINGS, WEIGHTS_DIR\n+from ultralytics.utils import DATASETS_DIR, SETTINGS\n from ultralytics.utils.checks import check_requirements\n \n-MODEL = WEIGHTS_DIR / \"path with spaces\" / \"yolov8n.pt\"  # test spaces in path\n-CFG = \"yolov8n.yaml\"\n-SOURCE = ASSETS / \"bus.jpg\"\n-TMP = (ROOT / \"../tests/tmp\").resolve()  # temp directory for test files\n+from . import MODEL, SOURCE, TMP\n \n \n @pytest.mark.skipif(not check_requirements(\"ray\", install=False), reason=\"ray[tune] not installed\")\n@@ -33,8 +33,6 @@ def test_mlflow():\n @pytest.mark.skipif(True, reason=\"Test failing in scheduled CI https://github.com/ultralytics/ultralytics/pull/8868\")\n @pytest.mark.skipif(not check_requirements(\"mlflow\", install=False), reason=\"mlflow not installed\")\n def test_mlflow_keep_run_active():\n-    import os\n-\n     import mlflow\n \n     \"\"\"Test training with MLflow tracking enabled.\"\"\"\n@@ -67,9 +65,6 @@ def test_mlflow_keep_run_active():\n def test_triton():\n     \"\"\"Test NVIDIA Triton Server functionalities.\"\"\"\n     check_requirements(\"tritonclient[all]\")\n-    import subprocess\n-    import time\n-\n     from tritonclient.http import InferenceServerClient  # noqa\n \n     # Create variables\ndiff --git a/tests/test_python.py b/tests/test_python.py\nindex b084385b182..86ed2eee5fd 100644\n--- a/tests/test_python.py\n+++ b/tests/test_python.py\n@@ -18,25 +18,17 @@\n     ASSETS,\n     DEFAULT_CFG,\n     DEFAULT_CFG_PATH,\n-    LINUX,\n-    MACOS,\n     ONLINE,\n     ROOT,\n     WEIGHTS_DIR,\n     WINDOWS,\n     Retry,\n     checks,\n-    is_dir_writeable,\n-    IS_RASPBERRYPI,\n )\n from ultralytics.utils.downloads import download\n-from ultralytics.utils.torch_utils import TORCH_1_9, TORCH_1_13\n+from ultralytics.utils.torch_utils import TORCH_1_9\n \n-MODEL = WEIGHTS_DIR / \"path with spaces\" / \"yolov8n.pt\"  # test spaces in path\n-CFG = \"yolov8n.yaml\"\n-SOURCE = ASSETS / \"bus.jpg\"\n-TMP = (ROOT / \"../tests/tmp\").resolve()  # temp directory for test files\n-IS_TMP_WRITEABLE = is_dir_writeable(TMP)\n+from . import CFG, IS_TMP_WRITEABLE, MODEL, SOURCE, TMP\n \n \n def test_model_forward():\n@@ -202,81 +194,6 @@ def test_train_pretrained():\n     model(SOURCE)\n \n \n-def test_export_torchscript():\n-    \"\"\"Test exporting the YOLO model to TorchScript format.\"\"\"\n-    f = YOLO(MODEL).export(format=\"torchscript\", optimize=False)\n-    YOLO(f)(SOURCE)  # exported model inference\n-\n-\n-def test_export_onnx():\n-    \"\"\"Test exporting the YOLO model to ONNX format.\"\"\"\n-    f = YOLO(MODEL).export(format=\"onnx\", dynamic=True)\n-    YOLO(f)(SOURCE)  # exported model inference\n-\n-\n-@pytest.mark.skipif(checks.IS_PYTHON_3_12, reason=\"OpenVINO not supported in Python 3.12\")\n-@pytest.mark.skipif(not TORCH_1_13, reason=\"OpenVINO requires torch>=1.13\")\n-def test_export_openvino():\n-    \"\"\"Test exporting the YOLO model to OpenVINO format.\"\"\"\n-    f = YOLO(MODEL).export(format=\"openvino\")\n-    YOLO(f)(SOURCE)  # exported model inference\n-\n-\n-@pytest.mark.skipif(not TORCH_1_9, reason=\"CoreML>=7.2 not supported with PyTorch<=1.8\")\n-@pytest.mark.skipif(WINDOWS, reason=\"CoreML not supported on Windows\")  # RuntimeError: BlobWriter not loaded\n-@pytest.mark.skipif(IS_RASPBERRYPI, reason=\"CoreML not supported on Raspberry Pi\")\n-@pytest.mark.skipif(checks.IS_PYTHON_3_12, reason=\"CoreML not supported in Python 3.12\")\n-def test_export_coreml():\n-    \"\"\"Test exporting the YOLO model to CoreML format.\"\"\"\n-    if MACOS:\n-        f = YOLO(MODEL).export(format=\"coreml\")\n-        YOLO(f)(SOURCE)  # model prediction only supported on macOS for nms=False models\n-    else:\n-        YOLO(MODEL).export(format=\"coreml\", nms=True)\n-\n-\n-@pytest.mark.skipif(not LINUX, reason=\"Test disabled as TF suffers from install conflicts on Windows and macOS\")\n-def test_export_tflite():\n-    \"\"\"\n-    Test exporting the YOLO model to TFLite format.\n-\n-    Note TF suffers from install conflicts on Windows and macOS.\n-    \"\"\"\n-    model = YOLO(MODEL)\n-    f = model.export(format=\"tflite\")\n-    YOLO(f)(SOURCE)\n-\n-\n-@pytest.mark.skipif(True, reason=\"Test disabled\")\n-@pytest.mark.skipif(not LINUX, reason=\"TF suffers from install conflicts on Windows and macOS\")\n-def test_export_pb():\n-    \"\"\"\n-    Test exporting the YOLO model to *.pb format.\n-\n-    Note TF suffers from install conflicts on Windows and macOS.\n-    \"\"\"\n-    model = YOLO(MODEL)\n-    f = model.export(format=\"pb\")\n-    YOLO(f)(SOURCE)\n-\n-\n-@pytest.mark.skipif(True, reason=\"Test disabled as Paddle protobuf and ONNX protobuf requirementsk conflict.\")\n-def test_export_paddle():\n-    \"\"\"\n-    Test exporting the YOLO model to Paddle format.\n-\n-    Note Paddle protobuf requirements conflicting with onnx protobuf requirements.\n-    \"\"\"\n-    YOLO(MODEL).export(format=\"paddle\")\n-\n-\n-@pytest.mark.slow\n-def test_export_ncnn():\n-    \"\"\"Test exporting the YOLO model to NCNN format.\"\"\"\n-    f = YOLO(MODEL).export(format=\"ncnn\")\n-    YOLO(f)(SOURCE)  # exported model inference\n-\n-\n def test_all_model_yamls():\n     \"\"\"Test YOLO model creation for all available YAML configurations.\"\"\"\n     for m in (ROOT / \"cfg\" / \"models\").rglob(\"*.yaml\"):\n@@ -293,7 +210,7 @@ def test_workflow():\n     model.train(data=\"coco8.yaml\", epochs=1, imgsz=32, optimizer=\"SGD\")\n     model.val(imgsz=32)\n     model.predict(SOURCE, imgsz=32)\n-    model.export(format=\"onnx\")  # export a model to ONNX format\n+    model.export(format=\"torchscript\")\n \n \n def test_predict_callback_and_setup():\n@@ -641,7 +558,7 @@ def test_yolo_world():\n     \"\"\"Tests YOLO world models with different configurations, including classes, detection, and training scenarios.\"\"\"\n     model = YOLO(\"yolov8s-world.pt\")  # no YOLOv8n-world model yet\n     model.set_classes([\"tree\", \"window\"])\n-    model(ASSETS / \"bus.jpg\", conf=0.01)\n+    model(SOURCE, conf=0.01)\n \n     model = YOLO(\"yolov8s-worldv2.pt\")  # no YOLOv8n-world model yet\n     # Training from a pretrained model. Eval is included at the final stage of training.\n@@ -651,11 +568,7 @@ def test_yolo_world():\n         epochs=1,\n         imgsz=32,\n         cache=\"disk\",\n-        batch=4,\n         close_mosaic=1,\n-        name=\"yolo-world\",\n-        save_txt=True,\n-        save_json=True,\n     )\n \n     # test WorWorldTrainerFromScratch\n@@ -667,8 +580,6 @@ def test_yolo_world():\n         epochs=1,\n         imgsz=32,\n         cache=\"disk\",\n-        batch=4,\n         close_mosaic=1,\n-        name=\"yolo-world\",\n         trainer=WorldTrainerFromScratch,\n     )\n", "problem_statement": "YOLOv8 Classification Model from FP32 to FP16 and INT8\n### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHi,\r\n\r\nAfter trained YOLOv8 Classification model (FP32), how can I convert it into FP16 and INT8 ? \n\n### Additional\n\n_No response_\n", "hints_text": "Hello! \ud83d\ude0a \n\nTo convert your YOLOv8 Classification model from FP32 to FP16 and INT8, you can use the export functionality provided by the Ultralytics YOLO framework. Here's how you can perform the conversions:\n\nFor **FP16**:\n```bash\nyolo export model=path/to/your/model.pt format=onnx half=True\n```\n\nFor **INT8** quantization, currently, exporting directly to INT8 is not supported directly via the Ultralytics export command for all models. However, once you have the ONNX model, you might consider using tools like the ONNX Runtime or TensorRT (for NVIDIA GPUs) which can further quantize the model to INT8 and provide inference capabilities.\n\nAssuming you've already exported to ONNX as shown above, for NVIDIA GPUs, you can use TensorRT's quantization tooling to process the ONNX model into INT8, focusing on creating a calibration dataset and running the model optimizer with INT8 options. Please refer to the [TensorRT documentation](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html) for the specifics on this process.\n\nRemember, moving to FP16 and especially INT8 may lead to a loss in accuracy but could provide significant improvements in inference speed and memory usage.\n\nHope this helps! If you have more questions regarding the process, we\u2019re here to assist. \ud83d\ude80\nLooking at the export documentation, int8 is supported by TF Lite and OpenVINO.\r\n\r\nWhen trying to export with OpenVINO it is necessary to supply calibration data (not sure about the other methods).\r\n\r\nA problem I ran into is that providing the dataset directory for classification task is not accepted by the export command:\r\n\r\n```\r\nyolo export model=best.pt format=openvino imgsz=320 int8=True batch=1 data=./calibration_images\r\n\r\nUltralytics YOLOv8.2.3 \ud83d\ude80 Python-3.11.8 torch-2.2.1+cu121 CPU (AMD Ryzen 7 7800X3D 8-Core Processor)\r\nYOLOv8n-cls summary (fused): 73 layers, 1438723 parameters, 0 gradients, 3.3 GFLOPs\r\n\r\nPyTorch: starting from 'best.pt' with input shape (1, 3, 320, 320) BCHW and output shape(s) (1, 3) (2.8 MB)\r\n\r\nOpenVINO: starting export with openvino 2024.1.0-15008-f4afc983258-releases/2024/1...\r\nINFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, tensorflow, onnx, openvino\r\nOpenVINO: collecting INT8 calibration images from 'data=./calibration_images/'\r\nOpenVINO: export failure \u274c 0.7s: [Errno 21] Is a directory: './calibration_images/'\r\n```\r\n\r\nIs this because classification is not fully supported yet, or is there a way to supply calibration images for classification model? It seems that the model is actually working with OpenVINO format after exporting without calibration data without too much loss in accuracy.\r\n\nHey there! \ud83d\udc4b\n\nGreat to hear you've been diving into the export options with YOLOv8, especially exploring INT8 quantization with OpenVINO! \ud83d\ude80 When it comes to providing calibration data for the INT8 quantization process in OpenVINO, it indeed expects a specific format or path handling that may differ slightly from expected. \n\nFor calibration data during an OpenVINO export, the expected input isn\u2019t a direct folder path through the `data` argument but rather integrated as part of a more extensive configuration process. Unfortunately, directly specifying a dataset directory like this isn\u2019t currently supported for the `yolo export` command for classification tasks in the manner you've tried.\n\nAs a workaround, after exporting your model to the OpenVINO format without INT8 calibration (as you've noted, with possibly minimal accuracy loss), you could manually run the model through the OpenVINO Model Optimizer with calibration data following the steps in their [calibration tools documentation](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Int_8_Quantization.html).\n\nIt sounds like you've got a good handle on things even without the INT8 calibration dataset, which is awesome! Keep experimenting and stay tuned for any updates on our side that might streamline this process in the future.\n\nIf you discover any more insights or if there's anything else we can help with, feel free to reach out. Happy coding! \ud83d\ude0a", "created_at": "2024-04-29T23:40:43Z"}
