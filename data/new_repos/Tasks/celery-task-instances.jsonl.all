{"repo": "celery/celery", "pull_number": 9580, "instance_id": "celery__celery-9580", "issue_numbers": ["9107"], "base_commit": "821e6557ae18a5ec163f7e4da5a1b50f3e047937", "patch": "diff --git a/celery/canvas.py b/celery/canvas.py\nindex 9f4d2f0ce74..da395c1390e 100644\n--- a/celery/canvas.py\n+++ b/celery/canvas.py\n@@ -2307,6 +2307,13 @@ def link_error(self, errback):\n                 CPendingDeprecationWarning\n             )\n \n+        # Edge case for nested chords in the header\n+        for task in maybe_list(self.tasks) or []:\n+            if isinstance(task, chord):\n+                # Let the nested chord do the error linking itself on its\n+                # header and body where needed, based on the current configuration\n+                task.link_error(errback)\n+\n         self.body.link_error(errback)\n         return errback\n \n", "test_patch": "diff --git a/t/integration/test_canvas.py b/t/integration/test_canvas.py\nindex d2474fa2351..ed838dc6730 100644\n--- a/t/integration/test_canvas.py\n+++ b/t/integration/test_canvas.py\n@@ -2862,6 +2862,60 @@ def test_chord_body_chain_child_replaced_with_chain_last(self, manager):\n         res_obj = orig_sig.delay()\n         assert res_obj.get(timeout=TIMEOUT) == [42]\n \n+    def test_nested_chord_header_link_error(self, manager, subtests):\n+        try:\n+            manager.app.backend.ensure_chords_allowed()\n+        except NotImplementedError as e:\n+            raise pytest.skip(e.args[0])\n+\n+        if not manager.app.conf.result_backend.startswith(\"redis\"):\n+            raise pytest.skip(\"Requires redis result backend.\")\n+        redis_connection = get_redis_connection()\n+\n+        errback_msg = \"errback called\"\n+        errback_key = \"echo_errback\"\n+        errback_sig = redis_echo.si(errback_msg, redis_key=errback_key)\n+\n+        body_msg = \"chord body called\"\n+        body_key = \"echo_body\"\n+        body_sig = redis_echo.si(body_msg, redis_key=body_key)\n+\n+        redis_connection.delete(errback_key, body_key)\n+\n+        manager.app.conf.task_allow_error_cb_on_chord_header = False\n+\n+        chord_inner = chord(\n+            [identity.si(\"t1\"), fail.si()],\n+            identity.si(\"t2 (body)\"),\n+        )\n+        chord_outer = chord(\n+            group(\n+                [\n+                    identity.si(\"t3\"),\n+                    chord_inner,\n+                ],\n+            ),\n+            body_sig,\n+        )\n+        chord_outer.link_error(errback_sig)\n+        chord_outer.delay()\n+\n+        with subtests.test(msg=\"Confirm the body was not executed\"):\n+            with pytest.raises(TimeoutError):\n+                # confirm the chord body was not called\n+                await_redis_echo((body_msg,), redis_key=body_key, timeout=10)\n+            # Double check\n+            assert not redis_connection.exists(body_key), \"Chord body was called when it should have not\"\n+\n+        with subtests.test(msg=\"Confirm only one errback was called\"):\n+            await_redis_echo((errback_msg,), redis_key=errback_key, timeout=10)\n+            with pytest.raises(TimeoutError):\n+                # Double check\n+                await_redis_echo((errback_msg,), redis_key=errback_key, timeout=10)\n+\n+        # Cleanup\n+        redis_connection.delete(errback_key)\n+\n     def test_enabling_flag_allow_error_cb_on_chord_header(self, manager, subtests):\n         \"\"\"\n         Test that the flag allow_error_callback_on_chord_header works as\ndiff --git a/t/unit/tasks/test_canvas.py b/t/unit/tasks/test_canvas.py\nindex 1f901376205..6d287848c31 100644\n--- a/t/unit/tasks/test_canvas.py\n+++ b/t/unit/tasks/test_canvas.py\n@@ -1746,7 +1746,7 @@ def test_flag_allow_error_cb_on_chord_header_various_header_types(self):\n             group(signature('t'), signature('t'))\n         ]\n         for chord_header in headers:\n-            c = chord(chord_header, signature('t'))\n+            c = chord(chord_header, signature('t'), app=self.app)\n             sig = signature('t')\n             errback = c.link_error(sig)\n             assert errback == sig\n@@ -1754,7 +1754,7 @@ def test_flag_allow_error_cb_on_chord_header_various_header_types(self):\n     @pytest.mark.usefixtures('depends_on_current_app')\n     def test_flag_allow_error_cb_on_chord_header_with_dict_callback(self):\n         self.app.conf.task_allow_error_cb_on_chord_header = True\n-        c = chord(group(signature('th1'), signature('th2')), signature('tbody'))\n+        c = chord(group(signature('th1'), signature('th2')), signature('tbody'), app=self.app)\n         errback_dict = dict(signature('tcb'))\n         errback = c.link_error(errback_dict)\n         assert errback == errback_dict\n@@ -1783,7 +1783,7 @@ def test_chord_upgrade_on_chaining(self):\n     def test_link_error_on_chord_header(self, header):\n         \"\"\" Test that link_error on a chord also links the header \"\"\"\n         self.app.conf.task_allow_error_cb_on_chord_header = True\n-        c = chord(header, signature('body'))\n+        c = chord(header, signature('body'), app=self.app)\n         err = signature('err')\n         errback = c.link_error(err)\n         assert errback == err\ndiff --git a/t/unit/tasks/test_stamping.py b/t/unit/tasks/test_stamping.py\nindex 3d139abb9e9..1c8da859dd7 100644\n--- a/t/unit/tasks/test_stamping.py\n+++ b/t/unit/tasks/test_stamping.py\n@@ -1300,13 +1300,13 @@ def tasks():\n \n         with subtests.test(\"chord header\"):\n             self.app.conf.task_allow_error_cb_on_chord_header = True\n-            canvas = chord(tasks(), self.identity.si(\"body\"))\n+            canvas = chord(tasks(), self.identity.si(\"body\"), app=self.app)\n             canvas.link_error(s(\"group_link_error\"))\n             canvas.stamp(CustomStampingVisitor())\n \n         with subtests.test(\"chord body\"):\n             self.app.conf.task_allow_error_cb_on_chord_header = False\n-            canvas = chord(tasks(), self.identity.si(\"body\"))\n+            canvas = chord(tasks(), self.identity.si(\"body\"), app=self.app)\n             canvas.link_error(s(\"group_link_error\"))\n             canvas.stamp(CustomStampingVisitor())\n \n", "problem_statement": "Chord with a chord in header doesn't invoke on_error on header failure\n<!--\r\nPlease fill this template entirely and do not erase parts of it.\r\nWe reserve the right to close without a response\r\nbug reports which are incomplete.\r\n-->\r\n# Checklist\r\n<!--\r\nTo check an item on the list replace [ ] with [x].\r\n-->\r\n- [x] I have verified that the issue exists against the `main` branch of Celery.\r\n- [x] This has already been asked to the [discussions forum](https://github.com/celery/celery/discussions) first.\r\n- [x] I have read the relevant section in the\r\n  [contribution guide](https://docs.celeryq.dev/en/main/contributing.html#other-bugs)\r\n  on reporting bugs.\r\n- [x] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Bug+Report%22+-label%3A%22Category%3A+Documentation%22)\r\n  for similar or identical bug reports.\r\n- [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22PR+Type%3A+Bugfix%22+-label%3A%22Category%3A+Documentation%22)\r\n  for existing proposed fixes.\r\n- [x] I have checked the [commit log](https://github.com/celery/celery/commits/main)\r\n  to find out if the bug was already fixed in the main branch.\r\n- [x] I have included all related issues and possible duplicate issues\r\n  in this issue (If there are none, check this box anyway).\r\n- [ ] I have tried to reproduce the issue with [pytest-celery](https://docs.celeryq.dev/projects/pytest-celery/en/latest/userguide/celery-bug-report.html) and added the reproduction script below.\r\n\r\n## Mandatory Debugging Information\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n    (if you are not able to do this, then at least specify the Celery\r\n     version affected).\r\n- [x] I have verified that the issue exists against the `main` branch of Celery.\r\n- [x] I have included the contents of ``pip freeze`` in the issue.\r\n- [x] I have included all the versions of all the external dependencies required\r\n  to reproduce this bug.\r\n\r\n## Optional Debugging Information\r\n<!--\r\nTry some of the below if you think they are relevant.\r\nIt will help us figure out the scope of the bug and how many users it affects.\r\n-->\r\n- [ ] I have tried reproducing the issue on more than one Python version\r\n  and/or implementation.\r\n- [ ] I have tried reproducing the issue on more than one message broker and/or\r\n  result backend.\r\n- [ ] I have tried reproducing the issue on more than one version of the message\r\n  broker and/or result backend.\r\n- [ ] I have tried reproducing the issue on more than one operating system.\r\n- [ ] I have tried reproducing the issue on more than one workers pool.\r\n- [ ] I have tried reproducing the issue with autoscaling, retries,\r\n  ETA/Countdown & rate limits disabled.\r\n- [ ] I have tried reproducing the issue after downgrading\r\n  and/or upgrading Celery and its dependencies.\r\n\r\n## Related Issues and Possible Duplicates\r\n<!--\r\nPlease make sure to search and mention any related issues\r\nor possible duplicates to this issue as requested by the checklist above.\r\n\r\nThis may or may not include issues in other repositories that the Celery project\r\nmaintains or other repositories that are dependencies of Celery.\r\n\r\nIf you don't know how to mention issues, please refer to Github's documentation\r\non the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests\r\n-->\r\n\r\n#### Related Issues\r\n\r\n- None\r\n\r\n#### Possible Duplicates\r\n\r\n- None\r\n\r\n## Environment & Settings\r\n<!-- Include the contents of celery --version below -->\r\n**Celery version**:\r\n<!-- Include the output of celery -A proj report below -->\r\n<details>\r\n<summary><b><code>celery report</code> Output:</b></summary>\r\n<p>\r\n\r\n```\r\nsoftware -> celery:5.4.0 (opalescent) kombu:5.3.7 py:3.12.3\r\n            billiard:4.2.0 py-amqp:5.2.0\r\nplatform -> system:Darwin arch:64bit\r\n            kernel version:23.5.0 imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:amqp results:redis://localhost:6379/0\r\n\r\nbroker_url: 'amqp://guest:********@localhost:5672//'\r\nresult_backend: 'redis://localhost:6379/0'\r\ntask_acks_late: True\r\ntask_reject_on_worker_lost: True\r\ndeprecated_settings: None\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n# Steps to Reproduce\r\n\r\n## Required Dependencies\r\n<!-- Please fill the required dependencies to reproduce this issue -->\r\n- **Minimal Python Version**: N/A or Unknown\r\n- **Minimal Celery Version**: N/A or Unknown\r\n- **Minimal Kombu Version**: N/A or Unknown\r\n- **Minimal Broker Version**: N/A or Unknown\r\n- **Minimal Result Backend Version**: N/A or Unknown\r\n- **Minimal OS and/or Kernel Version**: N/A or Unknown\r\n- **Minimal Broker Client Version**: N/A or Unknown\r\n- **Minimal Result Backend Client Version**: N/A or Unknown\r\n\r\n### Python Packages\r\n<!-- Please fill the contents of pip freeze below -->\r\n<details>\r\n<summary><b><code>pip freeze</code> Output:</b></summary>\r\n<p>\r\n\r\n```\r\namqp==5.2.0\r\nbilliard==4.2.0\r\ncelery @ git+https://github.com/celery/celery.git@50732166b645013626b28fc015ddd95895b6c5a3\r\nclick==8.1.7\r\nclick-didyoumean==0.3.1\r\nclick-plugins==1.1.1\r\nclick-repl==0.3.0\r\nkombu==5.3.7\r\nprompt_toolkit==3.0.47\r\npython-dateutil==2.9.0.post0\r\nredis==5.0.7\r\nsix==1.16.0\r\ntzdata==2024.1\r\nvine==5.1.0\r\nwcwidth==0.2.13\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n### Other Dependencies\r\n<!--\r\nPlease provide system dependencies, configuration files\r\nand other dependency information if applicable\r\n-->\r\n<details>\r\n<p>\r\nN/A\r\n</p>\r\n</details>\r\n\r\n## Minimally Reproducible Test Case\r\n<!--\r\nPlease provide a reproducible test case.\r\nRefer to the Reporting Bugs section in our contribution guide.\r\n\r\nWe prefer submitting test cases in the form of a PR to our integration test suite.\r\nIf you can provide one, please mention the PR number below.\r\nIf not, please attach the most minimal code example required to reproduce the issue below.\r\nIf the test case is too large, please include a link to a gist or a repository below.\r\n\r\nAlternatively, the pytest-celery plugin can be used to create standalone reproduction scripts\r\nthat can be added to this report. See the pytest-celery documentation for more information at\r\npytest-celery.readthedocs.io\r\n-->\r\n\r\n<details>\r\n<p>\r\n\r\n```python\r\napp = Celery(\r\n    'hello',\r\n    broker='amqp://localhost//',\r\n    backend='redis://localhost:6379/0',\r\n    task_acks_late=True,\r\n    task_reject_on_worker_lost=True,\r\n)\r\n\r\n\r\n@app.task(name='ok')\r\ndef ok(*args, **kwargs):\r\n    print('ok')\r\n    return 'ok'\r\n\r\n\r\n@app.task(name='bad')\r\ndef bad(*args, **kwargs):\r\n    print('bad')\r\n    raise Exception('bad')\r\n\r\n\r\n@app.task(name='on_error_handler')\r\ndef on_error_handler(request, exc, traceback):\r\n    print('error detected')\r\n\r\n\r\n@app.task(name='graph')\r\ndef graph():\r\n    print('running graph')\r\n    chord_inner = chord(\r\n        (\r\n            bad.s(),\r\n            ok.s(),\r\n        ),\r\n        ok.s(),\r\n    )\r\n\r\n    chord_outer = chord(\r\n        (\r\n            bad.s(),\r\n            chord_inner,\r\n        ),\r\n        ok.s(),\r\n    )\r\n    chord_outer.on_error(on_error_handler.s())\r\n    chord_outer.delay()\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n# Expected Behavior\r\n<!-- Describe in detail what you expect to happen -->\r\nShould call `on_error_handler` and print \"error detected\".\r\n\r\n# Actual Behavior\r\n<!--\r\nDescribe in detail what actually happened.\r\nPlease include a backtrace and surround it with triple backticks (```).\r\nIn addition, include the Celery daemon logs, the broker logs,\r\nthe result backend logs and system logs below if they will help us debug\r\nthe issue.\r\n-->\r\nNever calls `on_error_handler`.\r\n\r\nImportantly, note that if I replace `chord_inner` with `bad.s()` in the header, then `on_error_handler` is called. Meaning, `on_error(...)` works as intended with regular signatures in the header failing, but not nested `chord` signatures failing.\r\n\r\nSo, this is fine:\r\n```\r\nchord_outer = chord(\r\n    (\r\n        bad.s(),\r\n        bad.s(),\r\n    ),\r\n    ok.s(),\r\n)\r\nchord_outer.on_error(on_error_handler.s())\r\n```\r\n\r\nBut this is not:\r\n```\r\nchord_outer = chord(\r\n    (\r\n        bad.s(),\r\n        chord_inner,\r\n    ),\r\n    ok.s(),\r\n)\r\nchord_outer.on_error(on_error_handler.s())\r\n```\r\n\r\n```\r\n[2024-06-27 12:39:39,979: INFO/MainProcess] Task bad[d285c927-aa7c-4167-9732-d914c9094498] received\r\n[2024-06-27 12:39:39,980: WARNING/ForkPoolWorker-1] bad\r\n[2024-06-27 12:39:39,989: INFO/MainProcess] Task bad[b3e91204-905c-41e5-ae34-d398e69ce1a9] received\r\n[2024-06-27 12:39:39,990: INFO/MainProcess] Task ok[9053fb18-f333-417c-bc7b-04c95dedfb1a] received\r\n[2024-06-27 12:39:39,991: WARNING/ForkPoolWorker-9] bad\r\n[2024-06-27 12:39:39,991: INFO/ForkPoolWorker-8] Task graph[a728c96b-5935-48ef-ad71-71cb6b50a58c] succeeded in 0.04279591701924801s: None\r\n[2024-06-27 12:39:39,992: WARNING/ForkPoolWorker-2] ok\r\n[2024-06-27 12:39:39,992: ERROR/ForkPoolWorker-1] Task bad[d285c927-aa7c-4167-9732-d914c9094498] raised unexpected: Exception('bad')\r\nTraceback (most recent call last):\r\n  File \"/Users/foo/code/test-celery/venv/lib/python3.12/site-packages/celery/app/trace.py\", line 453, in trace_task\r\n    R = retval = fun(*args, **kwargs)\r\n                 ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/foo/code/test-celery/venv/lib/python3.12/site-packages/celery/app/trace.py\", line 736, in __protected_call__\r\n    return self.run(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/foo/code/test-celery/tasks.py\", line 24, in bad\r\n    raise Exception('bad')\r\nException: bad\r\n[2024-06-27 12:39:39,998: ERROR/ForkPoolWorker-9] Task bad[b3e91204-905c-41e5-ae34-d398e69ce1a9] raised unexpected: Exception('bad')\r\nTraceback (most recent call last):\r\n  File \"/Users/foo/code/test-celery/venv/lib/python3.12/site-packages/celery/app/trace.py\", line 453, in trace_task\r\n    R = retval = fun(*args, **kwargs)\r\n                 ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/foo/code/test-celery/venv/lib/python3.12/site-packages/celery/app/trace.py\", line 736, in __protected_call__\r\n    return self.run(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/foo/code/test-celery/tasks.py\", line 24, in bad\r\n    raise Exception('bad')\r\nException: bad\r\n[2024-06-27 12:39:39,999: ERROR/ForkPoolWorker-2] Chord '85ae8187-16fe-458e-b9ea-a49690a6e825' raised: ChordError(\"Dependency b3e91204-905c-41e5-ae34-d398e69ce1a9 raised Exception('bad')\")\r\nTraceback (most recent call last):\r\n  File \"/Users/foo/code/test-celery/venv/lib/python3.12/site-packages/celery/backends/redis.py\", line 528, in on_chord_part_return\r\n    resl = [unpack(tup, decode) for tup in resl]\r\n            ^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/foo/code/test-celery/venv/lib/python3.12/site-packages/celery/backends/redis.py\", line 434, in _unpack_chord_result\r\n    raise ChordError(f'Dependency {tid} raised {retval!r}')\r\ncelery.exceptions.ChordError: Dependency b3e91204-905c-41e5-ae34-d398e69ce1a9 raised Exception('bad')\r\n[2024-06-27 12:39:40,000: INFO/ForkPoolWorker-2] Task ok[9053fb18-f333-417c-bc7b-04c95dedfb1a] succeeded in 0.008979458012618124s: 'ok'\r\n```\r\n\n", "hints_text": "I also am seeing this issue, but also if the header tasks contain chains or groups.\r\n\r\nThis is a major issue for us in our production code, as we have a long-running group of tasks that when running are acquiring an external lock, but we have no consistent way to release that lock if any of the tasks fail (which happen sometimes due to intermittent issues).\n> Is `task_allow_error_cb_on_chord_header` turned on? See [docs](https://docs.celeryq.dev/en/stable/userguide/configuration.html#task-allow-error-cb-on-chord-header).\r\n\r\nIf I understand correctly, the docs say this about the default behaviour (`task_allow_error_cb_on_chord_header` turned off):\r\n```\r\nheader = group([t1, t2])\r\nbody = t3\r\nc = chord(header, body)\r\nc.link_error(error_callback_sig)\r\n```\r\n> If any of the header tasks failed (t1 or t2), by default, the chord body (t3) would not execute, and error_callback_sig will be called once (for the body).\r\n\r\nWhich sounds like exactly what I need, that is on any header failure:\r\n1. `error_callback_sig` will be called once,\r\n2. chord body will not execute.\r\n\r\nHowever, in my example `error_callback_sig` is not called, contrary to the above definition.\r\n\r\nJust to map to the example of the default behavior (with that flag off) in the documentation, I can rewrite the main part of my minimal reproducible example (included in the task description) as below.\r\n\r\nThis should call `error_callback_sig` once but, contrary to documentation, never calls it:\r\n```\r\napp.conf.task_allow_error_cb_on_chord_header = False\r\n\r\nt1 = ok.s()\r\nt2 = chord_inner\r\n\r\nheader = group([t1, t2])\r\nbody = ok.s()\r\nchord_outer = chord(header, body)\r\nchord_outer.link_error(on_error_handler.s())\r\n```\r\n\r\nThis seems to call `error_callback_sig` once if you call `chord_outer.apply_async()` for the first time, and twice for any repeated call:\r\n```\r\napp.conf.task_allow_error_cb_on_chord_header = False\r\n\r\nt1 = ok.s()\r\nt2 = bad.s()\r\n\r\nheader = group([t1, t2])\r\nbody = ok.s()\r\nchord_outer = chord(header, body)\r\nchord_outer.link_error(on_error_handler.s())\r\n```\nI am also affected on 5.5.0rc2. To be very specific, it seems to me that if at least one chord with at least one failure exists in the header of a chord, the error callback on the body of that chord will fail to run. If the failure is elsewhere, it will run.\r\n\r\nSo this works; `error_callback()` is called:\r\n\r\n```\r\n        c = (\r\n            group(\r\n                [\r\n                    chain(add.s(4, 4), add.s(3)),\r\n                    group(add.s(4, 4), add.s(3, 3)),\r\n                    group(add.s(4, 4), add.s(3, 3)) | tsum.s(),\r\n                    raising_task.s(),\r\n                    raising_task.s(),\r\n                    add.s(8, 8),\r\n                ]\r\n            )\r\n            | tsum.s().on_error(error_callback.s())\r\n        ).delay()\r\n ```\r\n \r\n But the callback is not called here:\r\n \r\n ```\r\n        c = (\r\n            group(\r\n                [\r\n                    chain(add.s(4, 4), add.s(3)),\r\n                    group(add.s(4, 4), add.s(3, 3)),\r\n                    group(add.s(4, 4), raising_task.s(), add.s(3, 3)) | tsum.s(),\r\n                    raising_task.s(),\r\n                    raising_task.s(),\r\n                    add.s(8, 8),\r\n                ]\r\n            )\r\n            | tsum.s().on_error(error_callback.s())\r\n        ).delay()\r\n ```", "created_at": "2025-02-24T20:14:23Z"}
{"repo": "celery/celery", "pull_number": 9578, "instance_id": "celery__celery-9578", "issue_numbers": ["9451"], "base_commit": "35cca09a8bf7fc076b81dbb182867e96284fd025", "patch": "diff --git a/celery/app/task.py b/celery/app/task.py\nindex 951c75824b..90ba8552d4 100644\n--- a/celery/app/task.py\n+++ b/celery/app/task.py\n@@ -535,6 +535,8 @@ def apply_async(self, args=None, kwargs=None, task_id=None, producer=None,\n             publisher (kombu.Producer): Deprecated alias to ``producer``.\n \n             headers (Dict): Message headers to be included in the message.\n+                The headers can be used as an overlay for custom labeling\n+                using the :ref:`canvas-stamping` feature.\n \n         Returns:\n             celery.result.AsyncResult: Promise of future evaluation.\ndiff --git a/docs/userguide/canvas.rst b/docs/userguide/canvas.rst\nindex 3268e93367..8d510a9c2a 100644\n--- a/docs/userguide/canvas.rst\n+++ b/docs/userguide/canvas.rst\n@@ -1174,6 +1174,8 @@ of one:\n This means that the first task will have a countdown of one second, the second\n task a countdown of two seconds, and so on.\n \n+.. _canvas-stamping:\n+\n Stamping\n ========\n \ndiff --git a/docs/userguide/workers.rst b/docs/userguide/workers.rst\nindex 1f2cef97c8..01d6491d72 100644\n--- a/docs/userguide/workers.rst\n+++ b/docs/userguide/workers.rst\n@@ -613,13 +613,13 @@ Note that remote control commands must be working for revokes to work.\n Remote control commands are only supported by the RabbitMQ (amqp) and Redis\n at this point.\n \n-.. control:: revoke_by_stamped_header\n+.. control:: revoke_by_stamped_headers\n \n-``revoke_by_stamped_header``: Revoking tasks by their stamped headers\n----------------------------------------------------------------------\n+``revoke_by_stamped_headers``: Revoking tasks by their stamped headers\n+----------------------------------------------------------------------\n :pool support: all, terminate only supported by prefork and eventlet\n :broker support: *amqp, redis*\n-:command: :program:`celery -A proj control revoke_by_stamped_header <header=value>`\n+:command: :program:`celery -A proj control revoke_by_stamped_headers <header=value>`\n \n This command is similar to :meth:`~@control.revoke`, but instead of\n specifying the task id(s), you specify the stamped header(s) as key-value pair(s),\n@@ -641,11 +641,11 @@ and each task that has a stamped header matching the key-value pair(s) will be r\n \n .. code-block:: pycon\n \n-    >>> app.control.revoke_by_stamped_header({'header': 'value'})\n+    >>> app.control.revoke_by_stamped_headers({'header': 'value'})\n \n-    >>> app.control.revoke_by_stamped_header({'header': 'value'}, terminate=True)\n+    >>> app.control.revoke_by_stamped_headers({'header': 'value'}, terminate=True)\n \n-    >>> app.control.revoke_by_stamped_header({'header': 'value'}, terminate=True, signal='SIGKILL')\n+    >>> app.control.revoke_by_stamped_headers({'header': 'value'}, terminate=True, signal='SIGKILL')\n \n \n Revoking multiple tasks by stamped headers\n@@ -653,14 +653,14 @@ Revoking multiple tasks by stamped headers\n \n .. versionadded:: 5.3\n \n-The ``revoke_by_stamped_header`` method also accepts a list argument, where it will revoke\n+The ``revoke_by_stamped_headers`` method also accepts a list argument, where it will revoke\n by several headers or several values.\n \n **Example**\n \n .. code-block:: pycon\n \n-    >> app.control.revoke_by_stamped_header({\n+    >> app.control.revoke_by_stamped_headers({\n     ...    'header_A': 'value_1',\n     ...    'header_B': ['value_2', 'value_3'],\n     })\n@@ -672,11 +672,11 @@ and all of the tasks that have a stamped header ``header_B`` with values ``value\n \n .. code-block:: console\n \n-    $ celery -A proj control revoke_by_stamped_header stamped_header_key_A=stamped_header_value_1 stamped_header_key_B=stamped_header_value_2\n+    $ celery -A proj control revoke_by_stamped_headers stamped_header_key_A=stamped_header_value_1 stamped_header_key_B=stamped_header_value_2\n \n-    $ celery -A proj control revoke_by_stamped_header stamped_header_key_A=stamped_header_value_1 stamped_header_key_B=stamped_header_value_2 --terminate\n+    $ celery -A proj control revoke_by_stamped_headers stamped_header_key_A=stamped_header_value_1 stamped_header_key_B=stamped_header_value_2 --terminate\n \n-    $ celery -A proj control revoke_by_stamped_header stamped_header_key_A=stamped_header_value_1 stamped_header_key_B=stamped_header_value_2 --terminate --signal=SIGKILL\n+    $ celery -A proj control revoke_by_stamped_headers stamped_header_key_A=stamped_header_value_1 stamped_header_key_B=stamped_header_value_2 --terminate --signal=SIGKILL\n \n .. _worker-time-limits:\n \n", "test_patch": "", "problem_statement": "Documentation Issues: revoke_by_stamped_headers Function Name and Incomplete Stamped Headers Documentation\n# Checklist\r\n- [x] I have checked the issues list for similar or identical bug reports.\r\n- [x] I have checked the pull requests list for existing proposed fixes.\r\n- [x] I have checked the commit log to find out if the bug was already fixed in the main branch.\r\n- [x] I have included all related issues and possible duplicate issues in this issue.\r\n\r\n## Related Issues and Possible Duplicates\r\n#### Related Issues\r\n- None\r\n\r\n#### Possible Duplicates\r\n- None\r\n\r\n# Description\r\nThere are several issues with the documentation regarding stamped headers:\r\n\r\n1. Function Name Mismatch:\r\n   - Documentation refers to `revoke_by_stamped_header`\r\n   - Actual function name is `revoke_by_stamped_headers` (plural)\r\n\r\n2. Unclear Documentation for `apply_async`:\r\n   - Current docs only mention \"headers (Dict) \u2013 Message headers to be included in the message\"\r\n   - stamps and stamped headers are not mentioned\r\n   - No comprehensive example of how to use stamped headers for task revocation\r\n\r\n# Suggestions\r\n\r\n1. Fix the function name in documentation to `revoke_by_stamped_headers`\r\n2. Update the `apply_async` documentation to include stamped headers:\r\n   - Current: only mentions \"headers (Dict) \u2013 Message headers to be included in the message\"\r\n   - Should add:\r\n     - `stamped_headers` (List) \u2013 Names of headers to be stamped for task tracking/revocation\r\n     - stamps \u2013 Values for the stamped headers specified in stamped_headers\r\n3. Add clear explanation of stamped headers concepts:\r\n```python\r\n# Example implementation\r\ndef start_process():\r\n    process_id = generate_process_id()\r\n    timestamp = timezone.now()\r\n    \r\n    # Start task with stamped headers\r\n    task = process_task.apply_async(\r\n        kwargs={'process_id': process_id},\r\n        stamped_headers=['batch_id'],    # List of header names to stamp\r\n        batch_id=str(timestamp)          # The actual stamp value\r\n    )\r\ndef stop_process(batch_id):\r\n    # Revoke all tasks with matching stamp\r\n    result = app.control.revoke_by_stamped_headers(\r\n        {'batch_id': ['batch_id']},\r\n        terminate=True\r\n    )\n", "hints_text": "", "created_at": "2025-02-24T15:34:56Z"}
{"repo": "celery/celery", "pull_number": 9575, "instance_id": "celery__celery-9575", "issue_numbers": ["9446"], "base_commit": "22f7d1f86fcfeb6e5b9301f2ee7d3e6ba1f9f03f", "patch": "diff --git a/celery/app/control.py b/celery/app/control.py\nindex 73b5162e851..603d930a542 100644\n--- a/celery/app/control.py\n+++ b/celery/app/control.py\n@@ -527,7 +527,8 @@ def revoke_by_stamped_headers(self, headers, destination=None, terminate=False,\n         if result:\n             for host in result:\n                 for response in host.values():\n-                    task_ids.update(response['ok'])\n+                    if isinstance(response['ok'], set):\n+                        task_ids.update(response['ok'])\n \n         if task_ids:\n             return self.revoke(list(task_ids), destination=destination, terminate=terminate, signal=signal, **kwargs)\n", "test_patch": "diff --git a/t/integration/test_tasks.py b/t/integration/test_tasks.py\nindex 76c46fd3f65..1b175a01320 100644\n--- a/t/integration/test_tasks.py\n+++ b/t/integration/test_tasks.py\n@@ -318,6 +318,16 @@ def on_signature(self, sig, **headers) -> dict:\n                     assert result.successful() is False\n             worker_state.revoked_stamps.clear()\n \n+    def test_revoke_by_stamped_headers_no_match(self, manager):\n+        response = manager.app.control.revoke_by_stamped_headers(\n+            {\"myheader\": [\"myvalue\"]},\n+            terminate=False,\n+            reply=True,\n+        )\n+\n+        expected_response = \"headers {'myheader': ['myvalue']} flagged as revoked, but not terminated\"\n+        assert response[0][list(response[0].keys())[0]][\"ok\"] == expected_response\n+\n     @flaky\n     def test_wrong_arguments(self, manager):\n         \"\"\"Tests that proper exceptions are raised when task is called with wrong arguments.\"\"\"\n", "problem_statement": "Bug in `revoke_by_stamped_headers` when no current worker tasks match the header\nThese lines here: https://github.com/celery/celery/blob/e3eaa675ee1e48d03a018f0abe763cc1dfb380a5/celery/app/control.py#L527-L530\r\n\r\nare problematic, because `host.values()` contains something like `{'ok': \"headers {'myheader': ['myvalue']} flagged as revoked, but not terminated\"}` if no current worker tasks match the header. \r\n\r\nWhen calling `revoke_by_stamped_headers` with\r\n\r\n```python\r\napp.control.revoke_by_stamped_headers(\r\n        {\"myheader\": [\"myvalue\"]},\r\n        terminate=False,\r\n        reply=True,\r\n        timeout=5,\r\n    )\r\n```\r\n\r\nthis leads to an attempt to revoke invalid task ids and, moreover, a gibberish response of the `app.control.revoke_by_stamped_headers` call:\r\n\r\n```\r\n      \"ok\": \"tasks {'o', \\\"'\\\", 'v', 'r', 'n', 's', ':', 'd', ' ', ',', '[', '_', '}', 'f', 'e', 'h', 'k', 'm', 'l', 'u', 't', '{', 'i', 'a', ']', 'w', 'g', 'b'} flagged as revoked\"\r\n```\r\n\r\ndue to these lines: https://github.com/celery/celery/blob/e3eaa675ee1e48d03a018f0abe763cc1dfb380a5/celery/app/control.py#L532-L533\n", "hints_text": "", "created_at": "2025-02-23T23:55:51Z"}
{"repo": "celery/celery", "pull_number": 9509, "instance_id": "celery__celery-9509", "issue_numbers": ["9508"], "base_commit": "fe761416f4d9269b780a13cc1131e2a16945937f", "patch": "diff --git a/docs/includes/resources.txt b/docs/includes/resources.txt\nindex 4bfbfd1792..91ef547e9d 100644\n--- a/docs/includes/resources.txt\n+++ b/docs/includes/resources.txt\n@@ -21,7 +21,7 @@ IRC\n Come chat with us on IRC. The **#celery** channel is located at the `Libera Chat`_\n network.\n \n-.. _`Libera Chat`: https://freenode.net\n+.. _`Libera Chat`: https://libera.chat/\n \n .. _bug-tracker:\n \n", "test_patch": "", "problem_statement": "IRC channel linked incorrectly\n# Checklist\n\n- [x] I have checked the [issues list](https://github.com/celery/celery/issues?utf8=%E2%9C%93&q=is%3Aissue+label%3A%22Category%3A+Documentation%22+)\n  for similar or identical bug reports.\n- [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22Category%3A+Documentation%22)\n  for existing proposed fixes.\n- [x] I have checked the [commit log](https://github.com/celery/celery/commits/main)\n  to find out if the bug was already fixed in the main branch.\n- [x] I have included all related issues and possible duplicate issues in this issue\n      (If there are none, check this box anyway).\n\n## Related Issues and Possible Duplicates\n\n#### Related Issues\n\n- #6811\n\n#### Possible Duplicates\n\n- None\n\n# Description\n\nCelery IRC channel has moved to libera.chat from Freenode. This was discussed in #6811 and partially resolved in #6837. However, `docs/includes/resources.txt` mentions Libera Chat in text, but links to Freenode. \n\nI should note that, at the documentation homepage (https://docs.celeryq.dev/en/stable/), clicking on \"join us [on IRC](https://docs.celeryq.dev/en/stable/getting-started/resources.html#irc-channel)\" then Libera Chat currently links to Freenode due to this bug.\n\n# Suggestions\n\nUpdating the link at `docs/includes/resources.txt` to point to Libera Chat as `README.rst` does.\n\n", "hints_text": "", "created_at": "2025-01-22T13:33:16Z"}
{"repo": "celery/celery", "pull_number": 9465, "instance_id": "celery__celery-9465", "issue_numbers": ["5226"], "base_commit": "1129272c3264b6c3e152e699b4a3ef49d185f2c8", "patch": "diff --git a/celery/backends/cassandra.py b/celery/backends/cassandra.py\nindex 0eb37f31ba..4ca071d2d0 100644\n--- a/celery/backends/cassandra.py\n+++ b/celery/backends/cassandra.py\n@@ -86,7 +86,7 @@ class CassandraBackend(BaseBackend):\n     supports_autoexpire = True      # autoexpire supported via entry_ttl\n \n     def __init__(self, servers=None, keyspace=None, table=None, entry_ttl=None,\n-                 port=9042, bundle_path=None, **kwargs):\n+                 port=None, bundle_path=None, **kwargs):\n         super().__init__(**kwargs)\n \n         if not cassandra:\n@@ -96,7 +96,7 @@ def __init__(self, servers=None, keyspace=None, table=None, entry_ttl=None,\n         self.servers = servers or conf.get('cassandra_servers', None)\n         self.bundle_path = bundle_path or conf.get(\n             'cassandra_secure_bundle_path', None)\n-        self.port = port or conf.get('cassandra_port', None)\n+        self.port = port or conf.get('cassandra_port', None) or 9042\n         self.keyspace = keyspace or conf.get('cassandra_keyspace', None)\n         self.table = table or conf.get('cassandra_table', None)\n         self.cassandra_options = conf.get('cassandra_options', {})\n", "test_patch": "diff --git a/t/unit/backends/test_cassandra.py b/t/unit/backends/test_cassandra.py\nindex 9bf8a480f3..b51b51d056 100644\n--- a/t/unit/backends/test_cassandra.py\n+++ b/t/unit/backends/test_cassandra.py\n@@ -267,4 +267,12 @@ def test_options(self):\n             'cql_version': '3.2.1',\n             'protocol_version': 3\n         }\n-        mod.CassandraBackend(app=self.app)\n+        self.app.conf.cassandra_port = None\n+        x = mod.CassandraBackend(app=self.app)\n+        # Default port is 9042\n+        assert x.port == 9042\n+\n+        # Valid options with port specified\n+        self.app.conf.cassandra_port = 1234\n+        x = mod.CassandraBackend(app=self.app)\n+        assert x.port == 1234\n", "problem_statement": "Config cassandra backend port config not work\n## Checklist\r\n\r\n- [ ] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n```\r\nsoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:2.7.12\r\n            billiard:3.5.0.3 py-amqp:2.2.2\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:amqp results:cassandra\r\n\r\nbroker_url: u'amqp://guest:********@localhost:32781//'\r\ncassandra_entry_ttl: 86400\r\ncassandra_table: 'tasks_result'\r\ncassandra_port: '61003'\r\ncassandra_write_consistency: 'ONE'\r\ncassandra_read_consistency: 'ONE'\r\ninclude: [u'celery_for_medium.tasks']\r\nresult_backend: 'cassandra'\r\ncassandra_keyspace: u'********'\r\ncassandra_servers: ['localhost']\r\n```\r\n- [ ] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\n\r\n## Expected behavior\r\nResult data should write correctly to cassandra database at: **localhost:61003**\r\n## Actual behavior\r\n```\r\n[2018-12-07 14:28:17,815: ERROR/ForkPoolWorker-1] Control connection failed to connect, shutting down Cluster:\r\nTraceback (most recent call last):\r\n  File \"cassandra/cluster.py\", line 1301, in cassandra.cluster.Cluster.connect\r\n    self.control_connection.connect()\r\n  File \"cassandra/cluster.py\", line 2802, in cassandra.cluster.ControlConnection.connect\r\n    self._set_new_connection(self._reconnect_internal())\r\n  File \"cassandra/cluster.py\", line 2845, in cassandra.cluster.ControlConnection._reconnect_internal\r\n    raise NoHostAvailable(\"Unable to connect to any servers\", errors)\r\nNoHostAvailable: ('Unable to connect to any servers', {'localhost': error(111, \"Tried connecting to [('localhost', 9042)]. Last error: Connection refused\")})\r\n```\r\n\n", "hints_text": "", "created_at": "2024-12-13T13:29:23Z"}
{"repo": "celery/celery", "pull_number": 9393, "instance_id": "celery__celery-9393", "issue_numbers": ["9363"], "base_commit": "71519d80f2a5ce77055541324dd545a015dca7d8", "patch": "diff --git a/celery/app/base.py b/celery/app/base.py\nindex 833818344d..7af07de941 100644\n--- a/celery/app/base.py\n+++ b/celery/app/base.py\n@@ -548,7 +548,7 @@ def _task_from_fun(\n         base=None,\n         bind=False,\n         pydantic: bool = False,\n-        pydantic_strict: bool = True,\n+        pydantic_strict: bool = False,\n         pydantic_context: typing.Optional[typing.Dict[str, typing.Any]] = None,\n         pydantic_dump_kwargs: typing.Optional[typing.Dict[str, typing.Any]] = None,\n         **options,\ndiff --git a/docs/userguide/tasks.rst b/docs/userguide/tasks.rst\nindex 505522b3cf..60c5e89f25 100644\n--- a/docs/userguide/tasks.rst\n+++ b/docs/userguide/tasks.rst\n@@ -893,7 +893,7 @@ There are a few more options influencing Pydantic behavior:\n .. attribute:: Task.pydantic_strict\n \n    By default, `strict mode <https://docs.pydantic.dev/dev/concepts/strict_mode/>`_\n-   is enabled. You can pass ``False`` to disable strict model validation.\n+   is disabled. You can pass ``True`` to enable strict model validation.\n \n .. attribute:: Task.pydantic_context\n \n", "test_patch": "diff --git a/t/unit/app/test_app.py b/t/unit/app/test_app.py\nindex 4bf1887b23..4d132a537d 100644\n--- a/t/unit/app/test_app.py\n+++ b/t/unit/app/test_app.py\n@@ -615,6 +615,28 @@ def foo(arg: ArgModel, kwarg: KwargModel = kwarg_default) -> ReturnModel:\n             assert foo(arg={'arg_value': 5}, kwarg={'kwarg_value': 6}) == {'ret_value': 2}\n             check.assert_called_once_with(ArgModel(arg_value=5), kwarg=KwargModel(kwarg_value=6))\n \n+    def test_task_with_pydantic_with_non_strict_validation(self):\n+        \"\"\"Test a pydantic task with where Pydantic has to apply non-strict validation.\"\"\"\n+\n+        class Model(BaseModel):\n+            value: timedelta\n+\n+        with self.Celery() as app:\n+            check = Mock()\n+\n+            @app.task(pydantic=True)\n+            def foo(arg: Model) -> Model:\n+                check(arg)\n+                return Model(value=timedelta(days=arg.value.days * 2))\n+\n+            assert foo({'value': timedelta(days=1)}) == {'value': 'P2D'}\n+            check.assert_called_once_with(Model(value=timedelta(days=1)))\n+            check.reset_mock()\n+\n+            # Pass a serialized value to the task\n+            assert foo({'value': 'P3D'}) == {'value': 'P6D'}\n+            check.assert_called_once_with(Model(value=timedelta(days=3)))\n+\n     def test_task_with_pydantic_with_optional_pydantic_args(self):\n         \"\"\"Test pydantic task receiving and returning an optional argument.\"\"\"\n         class ArgModel(BaseModel):\n", "problem_statement": "Pydantic model containing UUID4 between tasks in chain() fails validation \n<!--\r\nPlease fill this template entirely and do not erase parts of it.\r\nWe reserve the right to close without a response\r\nbug reports which are incomplete.\r\n-->\r\n# Checklist\r\n<!--\r\nTo check an item on the list replace [ ] with [x].\r\n-->\r\n- [x] I have verified that the issue exists against the `main` branch of Celery.\r\n- [ ] This has already been asked to the [discussions forum](https://github.com/celery/celery/discussions) first.\r\n- [x] I have read the relevant section in the\r\n  [contribution guide](https://docs.celeryq.dev/en/main/contributing.html#other-bugs)\r\n  on reporting bugs.\r\n- [x] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Bug+Report%22+-label%3A%22Category%3A+Documentation%22)\r\n  for similar or identical bug reports.\r\n- [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22PR+Type%3A+Bugfix%22+-label%3A%22Category%3A+Documentation%22)\r\n  for existing proposed fixes.\r\n- [x] I have checked the [commit log](https://github.com/celery/celery/commits/main)\r\n  to find out if the bug was already fixed in the main branch.\r\n- [x] I have included all related issues and possible duplicate issues\r\n  in this issue (If there are none, check this box anyway).\r\n- [ ] I have tried to reproduce the issue with [pytest-celery](https://docs.celeryq.dev/projects/pytest-celery/en/latest/userguide/celery-bug-report.html) and added the reproduction script below.\r\n\r\n## Mandatory Debugging Information\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n    (if you are not able to do this, then at least specify the Celery\r\n     version affected).\r\n- [x] I have verified that the issue exists against the `main` branch of Celery.\r\n- [x] I have included the contents of ``pip freeze`` in the issue.\r\n- [x] I have included all the versions of all the external dependencies required\r\n  to reproduce this bug.\r\n\r\n## Optional Debugging Information\r\n<!--\r\nTry some of the below if you think they are relevant.\r\nIt will help us figure out the scope of the bug and how many users it affects.\r\n-->\r\n- [ ] I have tried reproducing the issue on more than one Python version\r\n  and/or implementation.\r\n- [ ] I have tried reproducing the issue on more than one message broker and/or\r\n  result backend.\r\n- [ ] I have tried reproducing the issue on more than one version of the message\r\n  broker and/or result backend.\r\n- [ ] I have tried reproducing the issue on more than one operating system.\r\n- [ ] I have tried reproducing the issue on more than one workers pool.\r\n- [ ] I have tried reproducing the issue with autoscaling, retries,\r\n  ETA/Countdown & rate limits disabled.\r\n- [ ] I have tried reproducing the issue after downgrading\r\n  and/or upgrading Celery and its dependencies.\r\n\r\n## Related Issues and Possible Duplicates\r\n<!--\r\nPlease make sure to search and mention any related issues\r\nor possible duplicates to this issue as requested by the checklist above.\r\n\r\nThis may or may not include issues in other repositories that the Celery project\r\nmaintains or other repositories that are dependencies of Celery.\r\n\r\nIf you don't know how to mention issues, please refer to Github's documentation\r\non the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests\r\n-->\r\n\r\n#### Related Issues\r\n\r\n- None\r\n\r\n#### Possible Duplicates\r\n\r\n- None\r\n\r\n## Environment & Settings\r\n<!-- Include the contents of celery --version below -->\r\n**Celery version**:\r\n<!-- Include the output of celery -A proj report below -->\r\n<details>\r\n<summary><b><code>celery report</code> Output:</b></summary>\r\n<p>\r\n\r\n```\r\nsoftware -> celery:5.5.0rc1 (immunity) kombu:5.4.2 py:3.11.2\r\n            billiard:4.2.1 redis:5.1.1\r\nplatform -> system:Linux arch:64bit, ELF\r\n            kernel version:6.1.0-25-amd64 imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:redis results:redis\r\n\r\nbroker_url: 'redis://localhost:6379/0'\r\nresult_backend: 'redis'\r\ndeprecated_settings: None\r\n\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n# Steps to Reproduce\r\n\r\n## Required Dependencies\r\n<!-- Please fill the required dependencies to reproduce this issue -->\r\n- **Minimal Python Version**: py3.11\r\n- **Minimal Celery Version**: 5.5.0b4\r\n- **Minimal Kombu Version**: N/A or Unknown\r\n- **Minimal Broker Version**: N/A or Unknown\r\n- **Minimal Result Backend Version**: N/A or Unknown\r\n- **Minimal OS and/or Kernel Version**: N/A or Unknown\r\n- **Minimal Broker Client Version**: N/A or Unknown\r\n- **Minimal Result Backend Client Version**: N/A or Unknown\r\n\r\n### Python Packages\r\n<!-- Please fill the contents of pip freeze below -->\r\n<details>\r\n<summary><b><code>pip freeze</code> Output:</b></summary>\r\n<p>\r\n\r\n```\r\namqp==5.2.0\r\nannotated-types==0.7.0\r\nasync-timeout==4.0.3\r\nbilliard==4.2.1\r\ncelery==5.5.0rc1\r\nclick==8.1.7\r\nclick-didyoumean==0.3.1\r\nclick-plugins==1.1.1\r\nclick-repl==0.3.0\r\nkombu==5.4.2\r\nprompt_toolkit==3.0.48\r\npydantic==2.9.2\r\npydantic_core==2.23.4\r\npython-dateutil==2.9.0.post0\r\nredis==5.1.1\r\nsix==1.16.0\r\ntyping_extensions==4.12.2\r\ntzdata==2024.2\r\nvine==5.1.0\r\nwcwidth==0.2.13\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n### Other Dependencies\r\n<!--\r\nPlease provide system dependencies, configuration files\r\nand other dependency information if applicable\r\n-->\r\n<details>\r\n<p>\r\nN/A\r\n</p>\r\n</details>\r\n\r\n## Minimally Reproducible Test Case\r\n<!--\r\nPlease provide a reproducible test case.\r\nRefer to the Reporting Bugs section in our contribution guide.\r\n\r\nWe prefer submitting test cases in the form of a PR to our integration test suite.\r\nIf you can provide one, please mention the PR number below.\r\nIf not, please attach the most minimal code example required to reproduce the issue below.\r\nIf the test case is too large, please include a link to a gist or a repository below.\r\n\r\nAlternatively, the pytest-celery plugin can be used to create standalone reproduction scripts\r\nthat can be added to this report. See the pytest-celery documentation for more information at\r\npytest-celery.readthedocs.io\r\n-->\r\n\r\nFull test case: https://github.com/dromer/celery_pydantic_test\r\n\r\n\r\n# Expected Behavior\r\nI would expect the `return payload` in `task1` to not cause errors on the `uuid` property.\r\n\r\n# Actual Behavior\r\nWhen we chain the commands we get a failed validation if we rely on the pydantic return annotation:\r\n\r\n```python\r\npydantic_core._pydantic_core.ValidationError: 1 validation error for TestModel\r\nuuid\r\n  Input should be an instance of UUID [type=is_instance_of, input_value='53b3eab5-dbbd-4706-9635-5eec447fbb0c', input_type=str]\r\n    For further information visit https://errors.pydantic.dev/2.9/v/is_instance_of\r\n```\r\n\n", "hints_text": "@mathiasertl \nAm I right to assume that this error happens when loading the parameter for the second task?\nIndeed.\r\n\r\nWhen the first task returns the pydantic model this fails validation.\r\nIf it does a `.model_dump()` and returns a dict it works correctly.\r\n\r\nI've only seen this on attributes with type UUID, but it's likely there are other types where this also fails.\nThe issue is that `strict=True` by default in model validation (perhaps a debatable default?).\r\n\r\nEssentially, this is what is happening:\r\n\r\n```\r\nimport uuid\r\nfrom pydantic import BaseModel, UUID4\r\n\r\nclass Model(BaseModel):\r\n    uuid: UUID4\r\n\r\n# This is what the first task returns:\r\nm = Model(uuid=uuid.uuid4())\r\n\r\n# This is the serialization -> UUID becomes a string:\r\nm_dumped = m1.model_dump(mode='json')\r\n\r\n# This is what the parameter for the second task does:\r\nModel.model_validate(m_dumped, strict=True)\r\n```\r\n\r\nBottom line: passing `pydantic_strict=False` to the second task decorator should solve the issue.\n@mathiasertl I see, I was not aware of this flag.\r\n\r\nUnfortunate to have to use this, but it does work!\r\nI will likely have to add this to all my tasks from now on, since passing UUID inside the task payloads is quite essential for us.\r\n\r\nCheers and thanks for the help!\nI'm not sure strict validation is a good default to begin with. \n\nWithout using chains, the above example model works fine, but just because it's the return type annotation, where the model is only serialized, not *validated*\n\nBut argument annotations are validated. Strict validation means that you cannot send various types, like UUID, Timedeltas and datetimes.\n\nI think a different default (as in pydantic itself) is probably better.\n\nKr, mat\nYeah maybe having the default on False and being able to force strict validation when required (and possible) makes more sense.\nThank you for resolving this discussion quickly!", "created_at": "2024-11-01T11:01:31Z"}
{"repo": "celery/celery", "pull_number": 9392, "instance_id": "celery__celery-9392", "issue_numbers": ["9310"], "base_commit": "49f8f712f2e395ab32244b231d319e2484efea7e", "patch": "diff --git a/celery/fixups/django.py b/celery/fixups/django.py\nindex 5a8ca1b993a..b35499493a6 100644\n--- a/celery/fixups/django.py\n+++ b/celery/fixups/django.py\n@@ -16,6 +16,7 @@\n     from types import ModuleType\n     from typing import Protocol\n \n+    from django.db.backends.base.base import BaseDatabaseWrapper\n     from django.db.utils import ConnectionHandler\n \n     from celery.app.base import Celery\n@@ -164,15 +165,16 @@ def on_worker_process_init(self, **kwargs: Any) -> None:\n         # network IO that close() might cause.\n         for c in self._db.connections.all():\n             if c and c.connection:\n-                self._maybe_close_db_fd(c.connection)\n+                self._maybe_close_db_fd(c)\n \n         # use the _ version to avoid DB_REUSE preventing the conn.close() call\n         self._close_database(force=True)\n         self.close_cache()\n \n-    def _maybe_close_db_fd(self, fd: IO) -> None:\n+    def _maybe_close_db_fd(self, c: \"BaseDatabaseWrapper\") -> None:\n         try:\n-            _maybe_close_fd(fd)\n+            with c.wrap_database_errors:\n+                _maybe_close_fd(c.connection)\n         except self.interface_errors:\n             pass\n \n", "test_patch": "diff --git a/t/unit/fixups/test_django.py b/t/unit/fixups/test_django.py\nindex 72b4d60d873..c09ba61642c 100644\n--- a/t/unit/fixups/test_django.py\n+++ b/t/unit/fixups/test_django.py\n@@ -1,5 +1,5 @@\n from contextlib import contextmanager\n-from unittest.mock import Mock, patch\n+from unittest.mock import MagicMock, Mock, patch\n \n import pytest\n \n@@ -156,6 +156,10 @@ def test_on_worker_init(self):\n                 assert f._worker_fixup is DWF.return_value\n \n \n+class InterfaceError(Exception):\n+    pass\n+\n+\n class test_DjangoWorkerFixup(FixupCase):\n     Fixup = DjangoWorkerFixup\n \n@@ -180,14 +184,15 @@ def test_install(self):\n \n     def test_on_worker_process_init(self, patching):\n         with self.fixup_context(self.app) as (f, _, _):\n-            with patch('celery.fixups.django._maybe_close_fd') as mcf:\n+            with patch('celery.fixups.django._maybe_close_fd', side_effect=InterfaceError) as mcf:\n                 _all = f._db.connections.all = Mock()\n                 conns = _all.return_value = [\n-                    Mock(), Mock(),\n+                    Mock(), MagicMock(),\n                 ]\n                 conns[0].connection = None\n                 with patch.object(f, 'close_cache'):\n                     with patch.object(f, '_close_database'):\n+                        f.interface_errors = (InterfaceError, )\n                         f.on_worker_process_init()\n                         mcf.assert_called_with(conns[1].connection)\n                         f.close_cache.assert_called_with()\n", "problem_statement": "_maybe_close_db_fd in Django fixups does not correctly handle all exceptions\n# Details\r\n\r\nInside [`_maybe_close_fd`](https://github.com/celery/celery/blob/main/celery/fixups/django.py#L173-L177), Celery attempts to catch exceptions relating to closing the file descriptor. Currently it catches [`django.db.utils.InterfaceError`](https://github.com/celery/celery/blob/13830b18374d249e978ab0f4545569870e734202/celery/fixups/django.py#L128).\r\n\r\nHowever, when using `psycopg` or `psycopg2`, the exception thrown is _not_ `django.db.utils.InterfaceError`, it is `psycopg2.InterfaceError` or a `psycopg.OperationalError`:\r\n\r\n```python\r\nIn [1]: import django.db\r\nIn [2]: connection = django.db.connections.all()[0]\r\nIn [3]: connection.connection \r\nOut[3]: <psycopg.Connection [IDLE] (host=localhost database=postgres) at 0x1332340b0>\r\nIn [4]: connection.connection.fileno()\r\nOut[4]: 13\r\nIn [5]: connection.connection.close()\r\nIn [6]: connection.connection.fileno()\r\n... \r\npsycopg.OperationalError: the connection is closed\r\n```\r\n\r\nPsycopg2 throws this from [this macro](https://github.com/psycopg/psycopg2/blob/f9780aa0540434458ee933f46317f6879b9ebcf5/psycopg/connection.h#L190), called from [this C function](https://github.com/psycopg/psycopg2/blob/f9780aa0540434458ee933f46317f6879b9ebcf5/psycopg/connection_type.c#L1117) when `.fileno()` is accessed.\r\n\r\nThis means that if you have a connection that is already _closed_, [on_worker_process_init](https://github.com/celery/celery/blob/13830b18374d249e978ab0f4545569870e734202/celery/fixups/django.py#L151C9-L151C31) will throw an exception whilst attempting to close the connection:\r\n\r\n```\r\nSignal handler <bound method DjangoWorkerFixup.on_worker_process_init of <celery.fixups.django.DjangoWorkerFixup object at 0x7f0a3ac13f90>> raised: InterfaceError('connection already closed')\r\n```\n", "hints_text": "thanks for the report. would you mind contributing a fix?\nSure - I can do that! \r\n\r\nI'm not sure the best way to handle this though - do we want to catch _all_ exceptions here? At first I thought not, but maybe that's a fine thing to do? We could `logger.error()` if this happens, but otherwise continue?\r\n\r\nOtherwise we could try detecting if the connection is closed already, and only call `_maybe_close_fd` if it's not already closed? I feel this might be brittle, and would still require a general exception handler.\nNo problem, lets start with a draft POC? then we will see\nHey, I\u2019m on holiday for two weeks and so won\u2019t be able to tackle this. I\u2019ve added it to my queue and I\u2019ll take a look when I\u2019m back \ud83d\ude4f", "created_at": "2024-10-31T10:07:07Z"}
{"repo": "celery/celery", "pull_number": 9348, "instance_id": "celery__celery-9348", "issue_numbers": ["5663", "5663"], "base_commit": "fd27267c629b7c4d2bae1c2f375f4fe7089c21f2", "patch": "diff --git a/celery/worker/consumer/consumer.py b/celery/worker/consumer/consumer.py\nindex 8241a97602..d1b38232c6 100644\n--- a/celery/worker/consumer/consumer.py\n+++ b/celery/worker/consumer/consumer.py\n@@ -412,6 +412,7 @@ def register_with_event_loop(self, hub):\n         )\n \n     def shutdown(self):\n+        self.perform_pending_operations()\n         self.blueprint.shutdown(self)\n \n     def stop(self):\ndiff --git a/celery/worker/loops.py b/celery/worker/loops.py\nindex 0630e679fd..1f9e589eee 100644\n--- a/celery/worker/loops.py\n+++ b/celery/worker/loops.py\n@@ -119,8 +119,10 @@ def synloop(obj, connection, consumer, blueprint, hub, qos,\n \n     obj.on_ready()\n \n-    while blueprint.state == RUN and obj.connection:\n-        state.maybe_shutdown()\n+    def _loop_cycle():\n+        \"\"\"\n+        Perform one iteration of the blocking event loop.\n+        \"\"\"\n         if heartbeat_error[0] is not None:\n             raise heartbeat_error[0]\n         if qos.prev != qos.value:\n@@ -133,3 +135,9 @@ def synloop(obj, connection, consumer, blueprint, hub, qos,\n         except OSError:\n             if blueprint.state == RUN:\n                 raise\n+\n+    while blueprint.state == RUN and obj.connection:\n+        try:\n+            state.maybe_shutdown()\n+        finally:\n+            _loop_cycle()\n", "test_patch": "diff --git a/t/unit/test_loops.py b/t/unit/test_loops.py\nnew file mode 100644\nindex 0000000000..a203994199\n--- /dev/null\n+++ b/t/unit/test_loops.py\n@@ -0,0 +1,57 @@\n+from unittest.mock import Mock, patch\n+\n+import pytest\n+\n+from celery import bootsteps\n+from celery.worker.loops import synloop\n+\n+\n+def test_synloop_perform_pending_operations_on_system_exit():\n+    # Mock dependencies\n+    obj = Mock()\n+    connection = Mock()\n+    consumer = Mock()\n+    blueprint = Mock()\n+    hub = Mock()\n+    qos = Mock()\n+    heartbeat = Mock()\n+    clock = Mock()\n+\n+    # Set up the necessary attributes\n+    obj.create_task_handler.return_value = Mock()\n+    obj.perform_pending_operations = Mock()\n+    obj.on_ready = Mock()\n+    obj.pool.is_green = False\n+    obj.connection = True\n+\n+    blueprint.state = bootsteps.RUN  # Simulate RUN state\n+\n+    qos.prev = qos.value = Mock()\n+\n+    # Mock state.maybe_shutdown to raise SystemExit\n+    with patch(\"celery.worker.loops.state\") as mock_state:\n+        mock_state.maybe_shutdown.side_effect = SystemExit\n+\n+        # Call synloop and expect SystemExit to be raised\n+        with pytest.raises(SystemExit):\n+            synloop(\n+                obj,\n+                connection,\n+                consumer,\n+                blueprint,\n+                hub,\n+                qos,\n+                heartbeat,\n+                clock,\n+                hbrate=2.0,\n+            )\n+\n+    # Assert that perform_pending_operations was called even after SystemExit\n+    obj.perform_pending_operations.assert_called_once()\n+\n+    # Assert that connection.drain_events was called\n+    connection.drain_events.assert_called_with(timeout=2.0)\n+\n+    # Assert other important method calls\n+    obj.on_ready.assert_called_once()\n+    consumer.consume.assert_called_once()\ndiff --git a/t/unit/worker/test_consumer.py b/t/unit/worker/test_consumer.py\nindex ae677a7bfa..b43471134b 100644\n--- a/t/unit/worker/test_consumer.py\n+++ b/t/unit/worker/test_consumer.py\n@@ -47,6 +47,7 @@ def setup_method(self):\n         @self.app.task(shared=False)\n         def add(x, y):\n             return x + y\n+\n         self.add = add\n \n     def test_repr(self):\n@@ -147,6 +148,7 @@ def __enter__(self):\n \n             def __exit__(self, *args):\n                 pass\n+\n         c.qos._mutex = MutexMock()\n \n         assert c._restore_prefetch_count_after_connection_restart(None) is None\n@@ -266,6 +268,7 @@ def test_max_restarts_exceeded(self):\n         def se(*args, **kwargs):\n             c.blueprint.state = CLOSE\n             raise RestartFreqExceeded()\n+\n         c._restart_state.step.side_effect = se\n         c.blueprint.start.side_effect = socket.error()\n \n@@ -313,6 +316,7 @@ def test_too_many_open_files_raises_error(self):\n     def _closer(self, c):\n         def se(*args, **kwargs):\n             c.blueprint.state = CLOSE\n+\n         return se\n \n     @pytest.mark.parametrize(\"broker_connection_retry\", [True, False])\n@@ -531,6 +535,61 @@ def test_start_raises_connection_error(self,\n             assert expected_connection_retry_type in record.msg\n \n \n+class test_Consumer_PerformPendingOperations(ConsumerTestCase):\n+\n+    def test_perform_pending_operations_all_success(self):\n+        \"\"\"\n+        Test that all pending operations are processed successfully when `once=False`.\n+        \"\"\"\n+        c = self.get_consumer(no_hub=True)\n+\n+        # Create mock operations\n+        mock_operation_1 = Mock()\n+        mock_operation_2 = Mock()\n+\n+        # Add mock operations to _pending_operations\n+        c._pending_operations = [mock_operation_1, mock_operation_2]\n+\n+        # Call perform_pending_operations\n+        c.perform_pending_operations()\n+\n+        # Assert that all operations were called\n+        mock_operation_1.assert_called_once()\n+        mock_operation_2.assert_called_once()\n+\n+        # Ensure all pending operations are cleared\n+        assert len(c._pending_operations) == 0\n+\n+    def test_perform_pending_operations_with_exception(self):\n+        \"\"\"\n+        Test that pending operations are processed even if one raises an exception, and\n+        the exception is logged when `once=False`.\n+        \"\"\"\n+        c = self.get_consumer(no_hub=True)\n+\n+        # Mock operations: one failing, one successful\n+        mock_operation_fail = Mock(side_effect=Exception(\"Test Exception\"))\n+        mock_operation_success = Mock()\n+\n+        # Add operations to _pending_operations\n+        c._pending_operations = [mock_operation_fail, mock_operation_success]\n+\n+        # Patch logger to avoid logging during the test\n+        with patch('celery.worker.consumer.consumer.logger.exception') as mock_logger:\n+            # Call perform_pending_operations\n+            c.perform_pending_operations()\n+\n+            # Assert that both operations were attempted\n+            mock_operation_fail.assert_called_once()\n+            mock_operation_success.assert_called_once()\n+\n+            # Ensure the exception was logged\n+            mock_logger.assert_called_once()\n+\n+            # Ensure all pending operations are cleared\n+            assert len(c._pending_operations) == 0\n+\n+\n class test_Heart:\n \n     def test_start(self):\n", "problem_statement": "Completed tasks restore on shutdown causing duplicates for recursive tasks \n<!--\nPlease fill this template entirely and do not erase parts of it.\nWe reserve the right to close without a response\nbug reports which are incomplete.\n-->\n\n\n# Checklist\n<!--\nTo check an item on the list replace [ ] with [x].\n-->\n\n* [x] I have read the relevant section in the\n  [contribution guide](http://docs.celeryproject.org/en/latest/contributing.html#other-bugs)\n  on reporting bugs.\n* [x] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Bug+Report%22+-label%3A%22Category%3A+Documentation%22)\n  for similar or identical bug reports.\n* [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22PR+Type%3A+Bugfix%22+-label%3A%22Category%3A+Documentation%22)\n  for existing proposed fixes.\n* [x] I have checked the [commit log](https://github.com/celery/celery/commits/master)\n  to find out if the bug was already fixed in the master branch.\n* [x] I have included all related issues and possible duplicate issues\n  in this issue (If there are none, check this box anyway).\n\n## Mandatory Debugging Information\n* [x] I have included the output of ``celery -A proj report`` in the issue.\n    (if you are not able to do this, then at least specify the Celery\n     version affected).\n* [x] I have verified that the issue exists against the `master` branch of Celery.\n* [x] I have included the contents of ``pip freeze`` in the issue.\n* [x] I have included all the versions of all the external dependencies required\n  to reproduce this bug.\n\n## Optional Debugging Information\n<!--\nTry some of the below if you think they are relevant.\nIt will help us figure out the scope of the bug and how many users it affects.\n-->\n\n\n* [ ] I have tried reproducing the issue on more than one Python version\n  and/or implementation.\n* [ ] I have tried reproducing the issue on more than one message broker and/or\n  result backend.\n* [ ] I have tried reproducing the issue on more than one version of the message\n  broker and/or result backend.\n* [ ] I have tried reproducing the issue on more than one operating system.\n* [ ] I have tried reproducing the issue on more than one workers pool.\n* [ ] I have tried reproducing the issue with autoscaling, retries,\n  ETA/Countdown & rate limits disabled.\n* [x] I have tried reproducing the issue after downgrading\n  and/or upgrading Celery and its dependencies.\n\n## Related Issues and Possible Duplicates\n<!--\nPlease make sure to search and mention any related issues\nor possible duplicates to this issue as requested by the checklist above.\n\nThis may or may not include issues in other repositories that the Celery project\nmaintains or other repositories that are dependencies of Celery.\n\nIf you don't know how to mention issues, please refer to Github's documentation\non the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests\n-->\n\n#### Related Issues\n* None\n\n#### Possible Duplicates\n* None\n\n## Environment & Settings\n<!-- Include the contents of celery --version below -->\n**4.3.0 (rhubarb)**:\n<!-- Include the output of celery -A proj report below -->\n<details>\n<summary><b><code>celery report</code> Output:</b></summary>\n<p>\n\n```\n$ celery -A tasks report\n\nsoftware -> celery:4.3.0 (rhubarb) kombu:4.6.3 py:2.7.15rc1\n            billiard:3.6.0.0 redis:3.2.1\nplatform -> system:Linux arch:64bit\n            kernel version:4.15.0-55-generic imp:CPython\nloader   -> celery.loaders.app.AppLoader\nsettings -> transport:redis results:disabled\n\nbroker_url: u'redis://localhost:6379/0'\n```\n\n</p>\n</details>\n\n# Steps to Reproduce\n## Required Dependencies\n<!-- Please fill the required dependencies to reproduce this issue -->\n\n\n* **Minimal Python Version**: 2.7\n* **Minimal Celery Version**: 4.0\n* **Minimal Kombu Version**: N/A or Unknown\n* **Minimal Broker Version**: N/A or Unknown\n* **Minimal Result Backend Version**: N/A or Unknown\n* **Minimal OS and/or Kernel Version**: N/A or Unknown\n* **Minimal Broker Client Version**: N/A or Unknown\n* **Minimal Result Backend Client Version**: N/A or Unknown\n\n### Python Packages\n<!-- Please fill the contents of pip freeze below -->\n<details>\n<summary><b><code>pip freeze</code> Output:</b></summary>\n<p>\n\n```\namqp==2.5.0\nbilliard==3.6.0.0\ncelery==4.3.0\ngevent==1.3.7\ngreenlet==0.4.15\nkombu==4.6.3\npytz==2019.1\nredis==3.2.1\nvine==1.3.0\n```\n\n</p>\n</details>\n\n### Other Dependencies\n<!--\nPlease provide system dependencies, configuration files\nand other dependency information if applicable\n-->\n<details>\n<p>\nN/A\n</p>\n</details>\n\n## Minimally Reproducible Test Case\n<!--\nPlease provide a reproducible test case.\nRefer to the Reporting Bugs section in our contribution guide.\n\nWe prefer submitting test cases in the form of a PR to our integration test suite.\nIf you can provide one, please mention the PR number below.\nIf not, please attach the most minimal code example required to reproduce the issue below.\nIf the test case is too large, please include a link to a gist or a repository below.\n-->\n\n<details>\n<p>\n\n```python\nimport time\nfrom celery import Celery\n\napp = Celery('tasks', broker='redis://localhost:6379/0')\n\n@app.task\ndef recurse(num_times, sleep_seconds=1):\n    print(\"Recursing... num_times={}\".format(num_times))\n    if sleep_seconds > 0:\n        time.sleep(sleep_seconds)\n    if num_times > 0:\n        recurse.delay(num_times-1)\n    print(\"Recursed... num_times={}\".format(num_times))\n```\n\n</p>\n</details>\n\n# Expected Behavior\n<!-- Describe in detail what you expect to happen -->\n\nWhen I stop the worker all tasks will finish, acknowledging that they've finished.\n\n# Actual Behavior\n<!--\nDescribe in detail what actually happened.\nPlease include a backtrace and surround it with triple backticks (```).\nIn addition, include the Celery daemon logs, the broker logs,\nthe result backend logs and system logs below if they will help us debug\nthe issue.\n-->\n\nCelery will pick up more work after warm shutdown. That work will complete, in the case of recursive tasks will queue the next task, and then be restored because the task was never acked.\n\nThis seems to be because the synloop shuts down without flushing pending operations and the acks are handled here.\n\n```\n[celery]gary@gir:~/celery-test$ celery worker -l debug -A tasks -P gevent -c 10 -Q celery -n worker-2 --without-gossip --without-mingle\n[2019-07-31 20:14:28,058: DEBUG/MainProcess] | Worker: Preparing bootsteps.\n[2019-07-31 20:14:28,059: DEBUG/MainProcess] | Worker: Building graph...\n[2019-07-31 20:14:28,059: DEBUG/MainProcess] | Worker: New boot order: {StateDB, Timer, Hub, Pool, Autoscaler, Beat, Consumer}\n[2019-07-31 20:14:28,065: DEBUG/MainProcess] | Consumer: Preparing bootsteps.\n[2019-07-31 20:14:28,065: DEBUG/MainProcess] | Consumer: Building graph...\n[2019-07-31 20:14:28,067: DEBUG/MainProcess] | Consumer: New boot order: {Connection, Events, Mingle, Tasks, Control, Heart, Gossip, Agent, event loop}\n\n -------------- celery@worker-2 v4.3.0 (rhubarb)\n---- **** -----\n--- * ***  * -- Linux-4.15.0-55-generic-x86_64-with-Ubuntu-18.04-bionic 2019-07-31 20:14:28\n-- * - **** ---\n- ** ---------- [config]\n- ** ---------- .> app:         tasks:0x7f3c790fe4d0\n- ** ---------- .> transport:   redis://localhost:6379/0\n- ** ---------- .> results:     disabled://\n- *** --- * --- .> concurrency: 10 (gevent) \n-- ******* ---- .> task events: OFF (enable -E to monitor tasks in this worker)\n--- ***** -----\n -------------- [queues]\n                .> celery           exchange=celery(direct) key=celery\n\n[tasks]\n  . celery.accumulate\n  . celery.backend_cleanup\n  . celery.chain\n  . celery.chord\n  . celery.chord_unlock\n  . celery.chunks\n  . celery.group\n  . celery.map\n  . celery.starmap\n  . tasks.recurse\n\n[2019-07-31 20:14:28,071: DEBUG/MainProcess] | Worker: Starting Pool\n[2019-07-31 20:14:28,071: DEBUG/MainProcess] ^-- substep ok\n[2019-07-31 20:14:28,071: DEBUG/MainProcess] | Worker: Starting Consumer\n[2019-07-31 20:14:28,071: DEBUG/MainProcess] | Consumer: Starting Connection\n[2019-07-31 20:14:28,077: INFO/MainProcess] Connected to redis://localhost:6379/0\n[2019-07-31 20:14:28,077: DEBUG/MainProcess] ^-- substep ok\n[2019-07-31 20:14:28,077: DEBUG/MainProcess] | Consumer: Starting Events\n[2019-07-31 20:14:28,080: DEBUG/MainProcess] ^-- substep ok\n[2019-07-31 20:14:28,080: DEBUG/MainProcess] | Consumer: Starting Tasks\n[2019-07-31 20:14:28,082: DEBUG/MainProcess] ^-- substep ok\n[2019-07-31 20:14:28,082: DEBUG/MainProcess] | Consumer: Starting Control\n[2019-07-31 20:14:28,082: DEBUG/MainProcess] ^-- substep ok\n[2019-07-31 20:14:28,082: DEBUG/MainProcess] | Consumer: Starting Heart\n[2019-07-31 20:14:28,086: DEBUG/MainProcess] ^-- substep ok\n[2019-07-31 20:14:28,086: DEBUG/MainProcess] | Consumer: Starting event loop\n[2019-07-31 20:14:28,087: INFO/MainProcess] celery@worker-2 ready.\n[2019-07-31 20:14:28,087: DEBUG/MainProcess] basic.qos: prefetch_count->40\n[2019-07-31 20:14:28,088: INFO/MainProcess] pidbox: Connected to redis://localhost:6379/0.\n[2019-07-31 20:14:41,420: INFO/MainProcess] Received task: tasks.recurse[f0264bbb-dc78-448c-87d7-8df0e4f9a8b0]\n[2019-07-31 20:14:41,420: DEBUG/MainProcess] TaskPool: Apply <function _fast_trace_task at 0x7f3c790afaa0> (args:(u'tasks.recurse', u'f0264bbb-dc78-448c-87d7-8df0e4f9a8b0', {u'origin': u'gen3812@gir', u'lang': u'py', u'task': u'tasks.recurse', u'group': None, u'root_id': u'f0264bbb-dc78-448c-87d7-8df0e4f9a8b0', u'delivery_info': {u'priority': 0, u'redelivered': None, u'routing_key': u'celery', u'exchange': u''}, u'expires': None, u'correlation_id': u'f0264bbb-dc78-448c-87d7-8df0e4f9a8b0', u'retries': 0, u'timelimit': [None, None], u'argsrepr': u'()', u'eta': None, u'parent_id': None, u'reply_to': u'd3ba8668-d9ce-3926-aa3d-1570675b312d', u'shadow': None, u'id': u'f0264bbb-dc78-448c-87d7-8df0e4f9a8b0', u'kwargsrepr': u\"{'num_times': 10}\"}, '[[], {\"num_times\": 10}, {\"chord\": null, \"callbacks\": null, \"errbacks\": null, \"chain\": null}]', u'application/json', u'utf-8') kwargs:{})\n[2019-07-31 20:14:41,421: DEBUG/MainProcess] Task accepted: tasks.recurse[f0264bbb-dc78-448c-87d7-8df0e4f9a8b0] pid:3828\n[2019-07-31 20:14:41,421: WARNING/MainProcess] Recursing... num_times=10\n[2019-07-31 20:14:42,435: WARNING/MainProcess] Recursed... num_times=10\n[2019-07-31 20:14:42,435: INFO/MainProcess] Task tasks.recurse[f0264bbb-dc78-448c-87d7-8df0e4f9a8b0] succeeded in 1.01393432301s: None\n[2019-07-31 20:14:42,435: INFO/MainProcess] Received task: tasks.recurse[bccf82fb-bb24-453b-a680-a07f9119cb6d]\n[2019-07-31 20:14:42,436: DEBUG/MainProcess] TaskPool: Apply <function _fast_trace_task at 0x7f3c790afaa0> (args:(u'tasks.recurse', u'bccf82fb-bb24-453b-a680-a07f9119cb6d', {u'origin': u'gen3828@gir', u'lang': u'py', u'task': u'tasks.recurse', u'group': None, u'root_id': u'f0264bbb-dc78-448c-87d7-8df0e4f9a8b0', u'delivery_info': {u'priority': 0, u'redelivered': None, u'routing_key': u'celery', u'exchange': u''}, u'expires': None, u'correlation_id': u'bccf82fb-bb24-453b-a680-a07f9119cb6d', u'retries': 0, u'timelimit': [None, None], u'argsrepr': u'(9,)', u'eta': None, u'parent_id': u'f0264bbb-dc78-448c-87d7-8df0e4f9a8b0', u'reply_to': u'71e1ba16-86d5-3250-bd5d-e5f4c6688780', u'shadow': None, u'id': u'bccf82fb-bb24-453b-a680-a07f9119cb6d', u'kwargsrepr': u'{}'}, '[[9], {}, {\"chord\": null, \"callbacks\": null, \"errbacks\": null, \"chain\": null}]', u'application/json', u'utf-8') kwargs:{})\n[2019-07-31 20:14:42,436: DEBUG/MainProcess] Task accepted: tasks.recurse[bccf82fb-bb24-453b-a680-a07f9119cb6d] pid:3828\n[2019-07-31 20:14:42,436: WARNING/MainProcess] Recursing... num_times=9\n[2019-07-31 20:14:43,441: WARNING/MainProcess] Recursed... num_times=9\n[2019-07-31 20:14:43,442: INFO/MainProcess] Task tasks.recurse[bccf82fb-bb24-453b-a680-a07f9119cb6d] succeeded in 1.005564234s: None\n[2019-07-31 20:14:43,445: INFO/MainProcess] Received task: tasks.recurse[eac3b531-3919-46e6-b7d9-ef7fc2bd4270]\n[2019-07-31 20:14:43,446: DEBUG/MainProcess] TaskPool: Apply <function _fast_trace_task at 0x7f3c790afaa0> (args:(u'tasks.recurse', u'eac3b531-3919-46e6-b7d9-ef7fc2bd4270', {u'origin': u'gen3828@gir', u'lang': u'py', u'task': u'tasks.recurse', u'group': None, u'root_id': u'f0264bbb-dc78-448c-87d7-8df0e4f9a8b0', u'delivery_info': {u'priority': 0, u'redelivered': None, u'routing_key': u'celery', u'exchange': u''}, u'expires': None, u'correlation_id': u'eac3b531-3919-46e6-b7d9-ef7fc2bd4270', u'retries': 0, u'timelimit': [None, None], u'argsrepr': u'(8,)', u'eta': None, u'parent_id': u'bccf82fb-bb24-453b-a680-a07f9119cb6d', u'reply_to': u'71e1ba16-86d5-3250-bd5d-e5f4c6688780', u'shadow': None, u'id': u'eac3b531-3919-46e6-b7d9-ef7fc2bd4270', u'kwargsrepr': u'{}'}, '[[8], {}, {\"chord\": null, \"callbacks\": null, \"errbacks\": null, \"chain\": null}]', u'application/json', u'utf-8') kwargs:{})\n[2019-07-31 20:14:43,449: DEBUG/MainProcess] Task accepted: tasks.recurse[eac3b531-3919-46e6-b7d9-ef7fc2bd4270] pid:3828\n[2019-07-31 20:14:43,449: WARNING/MainProcess] Recursing... num_times=8\n[2019-07-31 20:14:44,454: WARNING/MainProcess] Recursed... num_times=8\n[2019-07-31 20:14:44,454: INFO/MainProcess] Task tasks.recurse[eac3b531-3919-46e6-b7d9-ef7fc2bd4270] succeeded in 1.005324692s: None\n[2019-07-31 20:14:44,457: INFO/MainProcess] Received task: tasks.recurse[3dba8041-c68e-4082-841f-3eb8cc1cdb66]\n[2019-07-31 20:14:44,458: DEBUG/MainProcess] TaskPool: Apply <function _fast_trace_task at 0x7f3c790afaa0> (args:(u'tasks.recurse', u'3dba8041-c68e-4082-841f-3eb8cc1cdb66', {u'origin': u'gen3828@gir', u'lang': u'py', u'task': u'tasks.recurse', u'group': None, u'root_id': u'f0264bbb-dc78-448c-87d7-8df0e4f9a8b0', u'delivery_info': {u'priority': 0, u'redelivered': None, u'routing_key': u'celery', u'exchange': u''}, u'expires': None, u'correlation_id': u'3dba8041-c68e-4082-841f-3eb8cc1cdb66', u'retries': 0, u'timelimit': [None, None], u'argsrepr': u'(7,)', u'eta': None, u'parent_id': u'eac3b531-3919-46e6-b7d9-ef7fc2bd4270', u'reply_to': u'71e1ba16-86d5-3250-bd5d-e5f4c6688780', u'shadow': None, u'id': u'3dba8041-c68e-4082-841f-3eb8cc1cdb66', u'kwargsrepr': u'{}'}, '[[7], {}, {\"chord\": null, \"callbacks\": null, \"errbacks\": null, \"chain\": null}]', u'application/json', u'utf-8') kwargs:{})\n[2019-07-31 20:14:44,460: DEBUG/MainProcess] Task accepted: tasks.recurse[3dba8041-c68e-4082-841f-3eb8cc1cdb66] pid:3828\n[2019-07-31 20:14:44,461: WARNING/MainProcess] Recursing... num_times=7\n^C\nworker: Hitting Ctrl+C again will terminate all running tasks!\n\nworker: Warm shutdown (MainProcess)\n[2019-07-31 20:14:45,463: WARNING/MainProcess] Recursed... num_times=7\n[2019-07-31 20:14:45,463: INFO/MainProcess] Task tasks.recurse[3dba8041-c68e-4082-841f-3eb8cc1cdb66] succeeded in 1.00242174501s: None\n[2019-07-31 20:14:45,464: INFO/MainProcess] Received task: tasks.recurse[1eaf15d1-f0c1-4a65-bd6f-f4b593c28a1d]\n[2019-07-31 20:14:45,464: DEBUG/MainProcess] TaskPool: Apply <function _fast_trace_task at 0x7f3c790afaa0> (args:(u'tasks.recurse', u'1eaf15d1-f0c1-4a65-bd6f-f4b593c28a1d', {u'origin': u'gen3828@gir', u'lang': u'py', u'task': u'tasks.recurse', u'group': None, u'root_id': u'f0264bbb-dc78-448c-87d7-8df0e4f9a8b0', u'delivery_info': {u'priority': 0, u'redelivered': None, u'routing_key': u'celery', u'exchange': u''}, u'expires': None, u'correlation_id': u'1eaf15d1-f0c1-4a65-bd6f-f4b593c28a1d', u'retries': 0, u'timelimit': [None, None], u'argsrepr': u'(6,)', u'eta': None, u'parent_id': u'3dba8041-c68e-4082-841f-3eb8cc1cdb66', u'reply_to': u'71e1ba16-86d5-3250-bd5d-e5f4c6688780', u'shadow': None, u'id': u'1eaf15d1-f0c1-4a65-bd6f-f4b593c28a1d', u'kwargsrepr': u'{}'}, '[[6], {}, {\"chord\": null, \"callbacks\": null, \"errbacks\": null, \"chain\": null}]', u'application/json', u'utf-8') kwargs:{})\n[2019-07-31 20:14:45,464: DEBUG/MainProcess] | Worker: Closing Pool...\n[2019-07-31 20:14:45,464: DEBUG/MainProcess] | Worker: Closing Consumer...\n[2019-07-31 20:14:45,464: DEBUG/MainProcess] | Worker: Stopping Consumer...\n[2019-07-31 20:14:45,464: DEBUG/MainProcess] | Consumer: Closing Connection...\n[2019-07-31 20:14:45,464: DEBUG/MainProcess] | Consumer: Closing Events...\n[2019-07-31 20:14:45,464: DEBUG/MainProcess] | Consumer: Closing Tasks...\n[2019-07-31 20:14:45,464: DEBUG/MainProcess] | Consumer: Closing Control...\n[2019-07-31 20:14:45,464: DEBUG/MainProcess] | Consumer: Closing Heart...\n[2019-07-31 20:14:45,464: DEBUG/MainProcess] | Consumer: Closing event loop...\n[2019-07-31 20:14:45,464: DEBUG/MainProcess] | Consumer: Stopping event loop...\n[2019-07-31 20:14:45,464: DEBUG/MainProcess] | Consumer: Stopping Heart...\n[2019-07-31 20:14:45,466: DEBUG/MainProcess] Task accepted: tasks.recurse[1eaf15d1-f0c1-4a65-bd6f-f4b593c28a1d] pid:3828\n[2019-07-31 20:14:45,466: WARNING/MainProcess] Recursing... num_times=6\n[2019-07-31 20:14:45,466: DEBUG/MainProcess] | Consumer: Stopping Control...\n[2019-07-31 20:14:45,466: DEBUG/MainProcess] Waiting for broadcast thread to shutdown...\n[2019-07-31 20:14:46,110: DEBUG/MainProcess] | Consumer: Stopping Tasks...\n[2019-07-31 20:14:46,110: DEBUG/MainProcess] Canceling task consumer...\n[2019-07-31 20:14:46,110: DEBUG/MainProcess] | Consumer: Stopping Events...\n[2019-07-31 20:14:46,110: DEBUG/MainProcess] | Consumer: Stopping Connection...\n[2019-07-31 20:14:46,110: DEBUG/MainProcess] | Worker: Stopping Pool...\n[2019-07-31 20:14:46,471: WARNING/MainProcess] Recursed... num_times=6\n[2019-07-31 20:14:46,471: INFO/MainProcess] Task tasks.recurse[1eaf15d1-f0c1-4a65-bd6f-f4b593c28a1d] succeeded in 1.00525897399s: None\n[2019-07-31 20:14:46,472: DEBUG/MainProcess] | Consumer: Shutdown Heart...\n[2019-07-31 20:14:46,472: DEBUG/MainProcess] | Consumer: Shutdown Control...\n[2019-07-31 20:14:46,472: DEBUG/MainProcess] | Consumer: Shutdown Tasks...\n[2019-07-31 20:14:46,473: DEBUG/MainProcess] Canceling task consumer...\n[2019-07-31 20:14:46,473: DEBUG/MainProcess] Closing consumer channel...\n[2019-07-31 20:14:46,473: DEBUG/MainProcess] | Consumer: Shutdown Events...\n[2019-07-31 20:14:46,474: DEBUG/MainProcess] | Consumer: Shutdown Connection...\n[2019-07-31 20:14:46,475: WARNING/MainProcess] Restoring 1 unacknowledged message(s)\n```\n\nYou can see if you run the worker again (sans debug this time for brevity) it starts running two tasks\n\n```\n[celery]gary@gir:~/celery-test$ celery worker -A tasks -P gevent -c 10 -Q celery -n worker-2 --without-gossip --without-mingle\n\n -------------- celery@worker-2 v4.3.0 (rhubarb)\n---- **** -----\n--- * ***  * -- Linux-4.15.0-55-generic-x86_64-with-Ubuntu-18.04-bionic 2019-07-31 20:34:06\n-- * - **** ---\n- ** ---------- [config]\n- ** ---------- .> app:         tasks:0x7f7b1ccad4d0\n- ** ---------- .> transport:   redis://localhost:6379/0\n- ** ---------- .> results:     disabled://\n- *** --- * --- .> concurrency: 10 (gevent)\n-- ******* ---- .> task events: OFF (enable -E to monitor tasks in this worker)\n--- ***** -----\n -------------- [queues]\n                .> celery           exchange=celery(direct) key=celery\n\n\n[2019-07-31 20:34:06,031: WARNING/MainProcess] Recursing... num_times=6\n[2019-07-31 20:34:06,033: WARNING/MainProcess] Recursing... num_times=5\n[2019-07-31 20:34:07,052: WARNING/MainProcess] Recursed... num_times=5\n[2019-07-31 20:34:07,052: WARNING/MainProcess] Recursed... num_times=6\n[2019-07-31 20:34:07,053: WARNING/MainProcess] Recursing... num_times=5\n[2019-07-31 20:34:07,054: WARNING/MainProcess] Recursing... num_times=4\n^C\nworker: Hitting Ctrl+C again will terminate all running tasks!\n\nworker: Warm shutdown (MainProcess)\n[2019-07-31 20:34:08,059: WARNING/MainProcess] Recursed... num_times=5\n[2019-07-31 20:34:08,060: WARNING/MainProcess] Recursed... num_times=4\n[2019-07-31 20:34:08,061: WARNING/MainProcess] Recursing... num_times=4\n[2019-07-31 20:34:09,065: WARNING/MainProcess] Recursed... num_times=4\n[2019-07-31 20:34:09,067: WARNING/MainProcess] Restoring 1 unacknowledged message(s)\n```\n\nThis behavior is new in 4.x. With the same code I'm unable to replicate in 3.1.26. This behavior is very reproducible in 4.x and causes very bad duplication when workers exist in autoscaling environments.\n\nI believe this regression was introduced in 7ec89a6bf0da853fc9f7e3e9911faf86880178d6\n\nThe following patch fixes this behavior but I'm not sure it is desirable as is and would like some guidance.\n\n```\n$ git diff\ndiff --git a/celery/worker/consumer/consumer.py b/celery/worker/consumer/consumer.py\nindex 60e64c298..81aa96b1a 100644\n--- a/celery/worker/consumer/consumer.py\n+++ b/celery/worker/consumer/consumer.py\n@@ -563,8 +563,8 @@ class Consumer(object):\n                 try:\n                     strategy(\n                         message, payload,\n-                        promise(call_soon, (message.ack_log_error,)),\n-                        promise(call_soon, (message.reject_log_error,)),\n+                        message.ack_log_error,\n+                        message.reject_log_error,\n                         callbacks,\n                     )\n                 except (InvalidTaskError, ContentDisallowed) as exc:\n```\n\n\nCompleted tasks restore on shutdown causing duplicates for recursive tasks \n<!--\nPlease fill this template entirely and do not erase parts of it.\nWe reserve the right to close without a response\nbug reports which are incomplete.\n-->\n\n\n# Checklist\n<!--\nTo check an item on the list replace [ ] with [x].\n-->\n\n* [x] I have read the relevant section in the\n  [contribution guide](http://docs.celeryproject.org/en/latest/contributing.html#other-bugs)\n  on reporting bugs.\n* [x] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Bug+Report%22+-label%3A%22Category%3A+Documentation%22)\n  for similar or identical bug reports.\n* [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22PR+Type%3A+Bugfix%22+-label%3A%22Category%3A+Documentation%22)\n  for existing proposed fixes.\n* [x] I have checked the [commit log](https://github.com/celery/celery/commits/master)\n  to find out if the bug was already fixed in the master branch.\n* [x] I have included all related issues and possible duplicate issues\n  in this issue (If there are none, check this box anyway).\n\n## Mandatory Debugging Information\n* [x] I have included the output of ``celery -A proj report`` in the issue.\n    (if you are not able to do this, then at least specify the Celery\n     version affected).\n* [x] I have verified that the issue exists against the `master` branch of Celery.\n* [x] I have included the contents of ``pip freeze`` in the issue.\n* [x] I have included all the versions of all the external dependencies required\n  to reproduce this bug.\n\n## Optional Debugging Information\n<!--\nTry some of the below if you think they are relevant.\nIt will help us figure out the scope of the bug and how many users it affects.\n-->\n\n\n* [ ] I have tried reproducing the issue on more than one Python version\n  and/or implementation.\n* [ ] I have tried reproducing the issue on more than one message broker and/or\n  result backend.\n* [ ] I have tried reproducing the issue on more than one version of the message\n  broker and/or result backend.\n* [ ] I have tried reproducing the issue on more than one operating system.\n* [ ] I have tried reproducing the issue on more than one workers pool.\n* [ ] I have tried reproducing the issue with autoscaling, retries,\n  ETA/Countdown & rate limits disabled.\n* [x] I have tried reproducing the issue after downgrading\n  and/or upgrading Celery and its dependencies.\n\n## Related Issues and Possible Duplicates\n<!--\nPlease make sure to search and mention any related issues\nor possible duplicates to this issue as requested by the checklist above.\n\nThis may or may not include issues in other repositories that the Celery project\nmaintains or other repositories that are dependencies of Celery.\n\nIf you don't know how to mention issues, please refer to Github's documentation\non the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests\n-->\n\n#### Related Issues\n* None\n\n#### Possible Duplicates\n* None\n\n## Environment & Settings\n<!-- Include the contents of celery --version below -->\n**4.3.0 (rhubarb)**:\n<!-- Include the output of celery -A proj report below -->\n<details>\n<summary><b><code>celery report</code> Output:</b></summary>\n<p>\n\n```\n$ celery -A tasks report\n\nsoftware -> celery:4.3.0 (rhubarb) kombu:4.6.3 py:2.7.15rc1\n            billiard:3.6.0.0 redis:3.2.1\nplatform -> system:Linux arch:64bit\n            kernel version:4.15.0-55-generic imp:CPython\nloader   -> celery.loaders.app.AppLoader\nsettings -> transport:redis results:disabled\n\nbroker_url: u'redis://localhost:6379/0'\n```\n\n</p>\n</details>\n\n# Steps to Reproduce\n## Required Dependencies\n<!-- Please fill the required dependencies to reproduce this issue -->\n\n\n* **Minimal Python Version**: 2.7\n* **Minimal Celery Version**: 4.0\n* **Minimal Kombu Version**: N/A or Unknown\n* **Minimal Broker Version**: N/A or Unknown\n* **Minimal Result Backend Version**: N/A or Unknown\n* **Minimal OS and/or Kernel Version**: N/A or Unknown\n* **Minimal Broker Client Version**: N/A or Unknown\n* **Minimal Result Backend Client Version**: N/A or Unknown\n\n### Python Packages\n<!-- Please fill the contents of pip freeze below -->\n<details>\n<summary><b><code>pip freeze</code> Output:</b></summary>\n<p>\n\n```\namqp==2.5.0\nbilliard==3.6.0.0\ncelery==4.3.0\ngevent==1.3.7\ngreenlet==0.4.15\nkombu==4.6.3\npytz==2019.1\nredis==3.2.1\nvine==1.3.0\n```\n\n</p>\n</details>\n\n### Other Dependencies\n<!--\nPlease provide system dependencies, configuration files\nand other dependency information if applicable\n-->\n<details>\n<p>\nN/A\n</p>\n</details>\n\n## Minimally Reproducible Test Case\n<!--\nPlease provide a reproducible test case.\nRefer to the Reporting Bugs section in our contribution guide.\n\nWe prefer submitting test cases in the form of a PR to our integration test suite.\nIf you can provide one, please mention the PR number below.\nIf not, please attach the most minimal code example required to reproduce the issue below.\nIf the test case is too large, please include a link to a gist or a repository below.\n-->\n\n<details>\n<p>\n\n```python\nimport time\nfrom celery import Celery\n\napp = Celery('tasks', broker='redis://localhost:6379/0')\n\n@app.task\ndef recurse(num_times, sleep_seconds=1):\n    print(\"Recursing... num_times={}\".format(num_times))\n    if sleep_seconds > 0:\n        time.sleep(sleep_seconds)\n    if num_times > 0:\n        recurse.delay(num_times-1)\n    print(\"Recursed... num_times={}\".format(num_times))\n```\n\n</p>\n</details>\n\n# Expected Behavior\n<!-- Describe in detail what you expect to happen -->\n\nWhen I stop the worker all tasks will finish, acknowledging that they've finished.\n\n# Actual Behavior\n<!--\nDescribe in detail what actually happened.\nPlease include a backtrace and surround it with triple backticks (```).\nIn addition, include the Celery daemon logs, the broker logs,\nthe result backend logs and system logs below if they will help us debug\nthe issue.\n-->\n\nCelery will pick up more work after warm shutdown. That work will complete, in the case of recursive tasks will queue the next task, and then be restored because the task was never acked.\n\nThis seems to be because the synloop shuts down without flushing pending operations and the acks are handled here.\n\n```\n[celery]gary@gir:~/celery-test$ celery worker -l debug -A tasks -P gevent -c 10 -Q celery -n worker-2 --without-gossip --without-mingle\n[2019-07-31 20:14:28,058: DEBUG/MainProcess] | Worker: Preparing bootsteps.\n[2019-07-31 20:14:28,059: DEBUG/MainProcess] | Worker: Building graph...\n[2019-07-31 20:14:28,059: DEBUG/MainProcess] | Worker: New boot order: {StateDB, Timer, Hub, Pool, Autoscaler, Beat, Consumer}\n[2019-07-31 20:14:28,065: DEBUG/MainProcess] | Consumer: Preparing bootsteps.\n[2019-07-31 20:14:28,065: DEBUG/MainProcess] | Consumer: Building graph...\n[2019-07-31 20:14:28,067: DEBUG/MainProcess] | Consumer: New boot order: {Connection, Events, Mingle, Tasks, Control, Heart, Gossip, Agent, event loop}\n\n -------------- celery@worker-2 v4.3.0 (rhubarb)\n---- **** -----\n--- * ***  * -- Linux-4.15.0-55-generic-x86_64-with-Ubuntu-18.04-bionic 2019-07-31 20:14:28\n-- * - **** ---\n- ** ---------- [config]\n- ** ---------- .> app:         tasks:0x7f3c790fe4d0\n- ** ---------- .> transport:   redis://localhost:6379/0\n- ** ---------- .> results:     disabled://\n- *** --- * --- .> concurrency: 10 (gevent) \n-- ******* ---- .> task events: OFF (enable -E to monitor tasks in this worker)\n--- ***** -----\n -------------- [queues]\n                .> celery           exchange=celery(direct) key=celery\n\n[tasks]\n  . celery.accumulate\n  . celery.backend_cleanup\n  . celery.chain\n  . celery.chord\n  . celery.chord_unlock\n  . celery.chunks\n  . celery.group\n  . celery.map\n  . celery.starmap\n  . tasks.recurse\n\n[2019-07-31 20:14:28,071: DEBUG/MainProcess] | Worker: Starting Pool\n[2019-07-31 20:14:28,071: DEBUG/MainProcess] ^-- substep ok\n[2019-07-31 20:14:28,071: DEBUG/MainProcess] | Worker: Starting Consumer\n[2019-07-31 20:14:28,071: DEBUG/MainProcess] | Consumer: Starting Connection\n[2019-07-31 20:14:28,077: INFO/MainProcess] Connected to redis://localhost:6379/0\n[2019-07-31 20:14:28,077: DEBUG/MainProcess] ^-- substep ok\n[2019-07-31 20:14:28,077: DEBUG/MainProcess] | Consumer: Starting Events\n[2019-07-31 20:14:28,080: DEBUG/MainProcess] ^-- substep ok\n[2019-07-31 20:14:28,080: DEBUG/MainProcess] | Consumer: Starting Tasks\n[2019-07-31 20:14:28,082: DEBUG/MainProcess] ^-- substep ok\n[2019-07-31 20:14:28,082: DEBUG/MainProcess] | Consumer: Starting Control\n[2019-07-31 20:14:28,082: DEBUG/MainProcess] ^-- substep ok\n[2019-07-31 20:14:28,082: DEBUG/MainProcess] | Consumer: Starting Heart\n[2019-07-31 20:14:28,086: DEBUG/MainProcess] ^-- substep ok\n[2019-07-31 20:14:28,086: DEBUG/MainProcess] | Consumer: Starting event loop\n[2019-07-31 20:14:28,087: INFO/MainProcess] celery@worker-2 ready.\n[2019-07-31 20:14:28,087: DEBUG/MainProcess] basic.qos: prefetch_count->40\n[2019-07-31 20:14:28,088: INFO/MainProcess] pidbox: Connected to redis://localhost:6379/0.\n[2019-07-31 20:14:41,420: INFO/MainProcess] Received task: tasks.recurse[f0264bbb-dc78-448c-87d7-8df0e4f9a8b0]\n[2019-07-31 20:14:41,420: DEBUG/MainProcess] TaskPool: Apply <function _fast_trace_task at 0x7f3c790afaa0> (args:(u'tasks.recurse', u'f0264bbb-dc78-448c-87d7-8df0e4f9a8b0', {u'origin': u'gen3812@gir', u'lang': u'py', u'task': u'tasks.recurse', u'group': None, u'root_id': u'f0264bbb-dc78-448c-87d7-8df0e4f9a8b0', u'delivery_info': {u'priority': 0, u'redelivered': None, u'routing_key': u'celery', u'exchange': u''}, u'expires': None, u'correlation_id': u'f0264bbb-dc78-448c-87d7-8df0e4f9a8b0', u'retries': 0, u'timelimit': [None, None], u'argsrepr': u'()', u'eta': None, u'parent_id': None, u'reply_to': u'd3ba8668-d9ce-3926-aa3d-1570675b312d', u'shadow': None, u'id': u'f0264bbb-dc78-448c-87d7-8df0e4f9a8b0', u'kwargsrepr': u\"{'num_times': 10}\"}, '[[], {\"num_times\": 10}, {\"chord\": null, \"callbacks\": null, \"errbacks\": null, \"chain\": null}]', u'application/json', u'utf-8') kwargs:{})\n[2019-07-31 20:14:41,421: DEBUG/MainProcess] Task accepted: tasks.recurse[f0264bbb-dc78-448c-87d7-8df0e4f9a8b0] pid:3828\n[2019-07-31 20:14:41,421: WARNING/MainProcess] Recursing... num_times=10\n[2019-07-31 20:14:42,435: WARNING/MainProcess] Recursed... num_times=10\n[2019-07-31 20:14:42,435: INFO/MainProcess] Task tasks.recurse[f0264bbb-dc78-448c-87d7-8df0e4f9a8b0] succeeded in 1.01393432301s: None\n[2019-07-31 20:14:42,435: INFO/MainProcess] Received task: tasks.recurse[bccf82fb-bb24-453b-a680-a07f9119cb6d]\n[2019-07-31 20:14:42,436: DEBUG/MainProcess] TaskPool: Apply <function _fast_trace_task at 0x7f3c790afaa0> (args:(u'tasks.recurse', u'bccf82fb-bb24-453b-a680-a07f9119cb6d', {u'origin': u'gen3828@gir', u'lang': u'py', u'task': u'tasks.recurse', u'group': None, u'root_id': u'f0264bbb-dc78-448c-87d7-8df0e4f9a8b0', u'delivery_info': {u'priority': 0, u'redelivered': None, u'routing_key': u'celery', u'exchange': u''}, u'expires': None, u'correlation_id': u'bccf82fb-bb24-453b-a680-a07f9119cb6d', u'retries': 0, u'timelimit': [None, None], u'argsrepr': u'(9,)', u'eta': None, u'parent_id': u'f0264bbb-dc78-448c-87d7-8df0e4f9a8b0', u'reply_to': u'71e1ba16-86d5-3250-bd5d-e5f4c6688780', u'shadow': None, u'id': u'bccf82fb-bb24-453b-a680-a07f9119cb6d', u'kwargsrepr': u'{}'}, '[[9], {}, {\"chord\": null, \"callbacks\": null, \"errbacks\": null, \"chain\": null}]', u'application/json', u'utf-8') kwargs:{})\n[2019-07-31 20:14:42,436: DEBUG/MainProcess] Task accepted: tasks.recurse[bccf82fb-bb24-453b-a680-a07f9119cb6d] pid:3828\n[2019-07-31 20:14:42,436: WARNING/MainProcess] Recursing... num_times=9\n[2019-07-31 20:14:43,441: WARNING/MainProcess] Recursed... num_times=9\n[2019-07-31 20:14:43,442: INFO/MainProcess] Task tasks.recurse[bccf82fb-bb24-453b-a680-a07f9119cb6d] succeeded in 1.005564234s: None\n[2019-07-31 20:14:43,445: INFO/MainProcess] Received task: tasks.recurse[eac3b531-3919-46e6-b7d9-ef7fc2bd4270]\n[2019-07-31 20:14:43,446: DEBUG/MainProcess] TaskPool: Apply <function _fast_trace_task at 0x7f3c790afaa0> (args:(u'tasks.recurse', u'eac3b531-3919-46e6-b7d9-ef7fc2bd4270', {u'origin': u'gen3828@gir', u'lang': u'py', u'task': u'tasks.recurse', u'group': None, u'root_id': u'f0264bbb-dc78-448c-87d7-8df0e4f9a8b0', u'delivery_info': {u'priority': 0, u'redelivered': None, u'routing_key': u'celery', u'exchange': u''}, u'expires': None, u'correlation_id': u'eac3b531-3919-46e6-b7d9-ef7fc2bd4270', u'retries': 0, u'timelimit': [None, None], u'argsrepr': u'(8,)', u'eta': None, u'parent_id': u'bccf82fb-bb24-453b-a680-a07f9119cb6d', u'reply_to': u'71e1ba16-86d5-3250-bd5d-e5f4c6688780', u'shadow': None, u'id': u'eac3b531-3919-46e6-b7d9-ef7fc2bd4270', u'kwargsrepr': u'{}'}, '[[8], {}, {\"chord\": null, \"callbacks\": null, \"errbacks\": null, \"chain\": null}]', u'application/json', u'utf-8') kwargs:{})\n[2019-07-31 20:14:43,449: DEBUG/MainProcess] Task accepted: tasks.recurse[eac3b531-3919-46e6-b7d9-ef7fc2bd4270] pid:3828\n[2019-07-31 20:14:43,449: WARNING/MainProcess] Recursing... num_times=8\n[2019-07-31 20:14:44,454: WARNING/MainProcess] Recursed... num_times=8\n[2019-07-31 20:14:44,454: INFO/MainProcess] Task tasks.recurse[eac3b531-3919-46e6-b7d9-ef7fc2bd4270] succeeded in 1.005324692s: None\n[2019-07-31 20:14:44,457: INFO/MainProcess] Received task: tasks.recurse[3dba8041-c68e-4082-841f-3eb8cc1cdb66]\n[2019-07-31 20:14:44,458: DEBUG/MainProcess] TaskPool: Apply <function _fast_trace_task at 0x7f3c790afaa0> (args:(u'tasks.recurse', u'3dba8041-c68e-4082-841f-3eb8cc1cdb66', {u'origin': u'gen3828@gir', u'lang': u'py', u'task': u'tasks.recurse', u'group': None, u'root_id': u'f0264bbb-dc78-448c-87d7-8df0e4f9a8b0', u'delivery_info': {u'priority': 0, u'redelivered': None, u'routing_key': u'celery', u'exchange': u''}, u'expires': None, u'correlation_id': u'3dba8041-c68e-4082-841f-3eb8cc1cdb66', u'retries': 0, u'timelimit': [None, None], u'argsrepr': u'(7,)', u'eta': None, u'parent_id': u'eac3b531-3919-46e6-b7d9-ef7fc2bd4270', u'reply_to': u'71e1ba16-86d5-3250-bd5d-e5f4c6688780', u'shadow': None, u'id': u'3dba8041-c68e-4082-841f-3eb8cc1cdb66', u'kwargsrepr': u'{}'}, '[[7], {}, {\"chord\": null, \"callbacks\": null, \"errbacks\": null, \"chain\": null}]', u'application/json', u'utf-8') kwargs:{})\n[2019-07-31 20:14:44,460: DEBUG/MainProcess] Task accepted: tasks.recurse[3dba8041-c68e-4082-841f-3eb8cc1cdb66] pid:3828\n[2019-07-31 20:14:44,461: WARNING/MainProcess] Recursing... num_times=7\n^C\nworker: Hitting Ctrl+C again will terminate all running tasks!\n\nworker: Warm shutdown (MainProcess)\n[2019-07-31 20:14:45,463: WARNING/MainProcess] Recursed... num_times=7\n[2019-07-31 20:14:45,463: INFO/MainProcess] Task tasks.recurse[3dba8041-c68e-4082-841f-3eb8cc1cdb66] succeeded in 1.00242174501s: None\n[2019-07-31 20:14:45,464: INFO/MainProcess] Received task: tasks.recurse[1eaf15d1-f0c1-4a65-bd6f-f4b593c28a1d]\n[2019-07-31 20:14:45,464: DEBUG/MainProcess] TaskPool: Apply <function _fast_trace_task at 0x7f3c790afaa0> (args:(u'tasks.recurse', u'1eaf15d1-f0c1-4a65-bd6f-f4b593c28a1d', {u'origin': u'gen3828@gir', u'lang': u'py', u'task': u'tasks.recurse', u'group': None, u'root_id': u'f0264bbb-dc78-448c-87d7-8df0e4f9a8b0', u'delivery_info': {u'priority': 0, u'redelivered': None, u'routing_key': u'celery', u'exchange': u''}, u'expires': None, u'correlation_id': u'1eaf15d1-f0c1-4a65-bd6f-f4b593c28a1d', u'retries': 0, u'timelimit': [None, None], u'argsrepr': u'(6,)', u'eta': None, u'parent_id': u'3dba8041-c68e-4082-841f-3eb8cc1cdb66', u'reply_to': u'71e1ba16-86d5-3250-bd5d-e5f4c6688780', u'shadow': None, u'id': u'1eaf15d1-f0c1-4a65-bd6f-f4b593c28a1d', u'kwargsrepr': u'{}'}, '[[6], {}, {\"chord\": null, \"callbacks\": null, \"errbacks\": null, \"chain\": null}]', u'application/json', u'utf-8') kwargs:{})\n[2019-07-31 20:14:45,464: DEBUG/MainProcess] | Worker: Closing Pool...\n[2019-07-31 20:14:45,464: DEBUG/MainProcess] | Worker: Closing Consumer...\n[2019-07-31 20:14:45,464: DEBUG/MainProcess] | Worker: Stopping Consumer...\n[2019-07-31 20:14:45,464: DEBUG/MainProcess] | Consumer: Closing Connection...\n[2019-07-31 20:14:45,464: DEBUG/MainProcess] | Consumer: Closing Events...\n[2019-07-31 20:14:45,464: DEBUG/MainProcess] | Consumer: Closing Tasks...\n[2019-07-31 20:14:45,464: DEBUG/MainProcess] | Consumer: Closing Control...\n[2019-07-31 20:14:45,464: DEBUG/MainProcess] | Consumer: Closing Heart...\n[2019-07-31 20:14:45,464: DEBUG/MainProcess] | Consumer: Closing event loop...\n[2019-07-31 20:14:45,464: DEBUG/MainProcess] | Consumer: Stopping event loop...\n[2019-07-31 20:14:45,464: DEBUG/MainProcess] | Consumer: Stopping Heart...\n[2019-07-31 20:14:45,466: DEBUG/MainProcess] Task accepted: tasks.recurse[1eaf15d1-f0c1-4a65-bd6f-f4b593c28a1d] pid:3828\n[2019-07-31 20:14:45,466: WARNING/MainProcess] Recursing... num_times=6\n[2019-07-31 20:14:45,466: DEBUG/MainProcess] | Consumer: Stopping Control...\n[2019-07-31 20:14:45,466: DEBUG/MainProcess] Waiting for broadcast thread to shutdown...\n[2019-07-31 20:14:46,110: DEBUG/MainProcess] | Consumer: Stopping Tasks...\n[2019-07-31 20:14:46,110: DEBUG/MainProcess] Canceling task consumer...\n[2019-07-31 20:14:46,110: DEBUG/MainProcess] | Consumer: Stopping Events...\n[2019-07-31 20:14:46,110: DEBUG/MainProcess] | Consumer: Stopping Connection...\n[2019-07-31 20:14:46,110: DEBUG/MainProcess] | Worker: Stopping Pool...\n[2019-07-31 20:14:46,471: WARNING/MainProcess] Recursed... num_times=6\n[2019-07-31 20:14:46,471: INFO/MainProcess] Task tasks.recurse[1eaf15d1-f0c1-4a65-bd6f-f4b593c28a1d] succeeded in 1.00525897399s: None\n[2019-07-31 20:14:46,472: DEBUG/MainProcess] | Consumer: Shutdown Heart...\n[2019-07-31 20:14:46,472: DEBUG/MainProcess] | Consumer: Shutdown Control...\n[2019-07-31 20:14:46,472: DEBUG/MainProcess] | Consumer: Shutdown Tasks...\n[2019-07-31 20:14:46,473: DEBUG/MainProcess] Canceling task consumer...\n[2019-07-31 20:14:46,473: DEBUG/MainProcess] Closing consumer channel...\n[2019-07-31 20:14:46,473: DEBUG/MainProcess] | Consumer: Shutdown Events...\n[2019-07-31 20:14:46,474: DEBUG/MainProcess] | Consumer: Shutdown Connection...\n[2019-07-31 20:14:46,475: WARNING/MainProcess] Restoring 1 unacknowledged message(s)\n```\n\nYou can see if you run the worker again (sans debug this time for brevity) it starts running two tasks\n\n```\n[celery]gary@gir:~/celery-test$ celery worker -A tasks -P gevent -c 10 -Q celery -n worker-2 --without-gossip --without-mingle\n\n -------------- celery@worker-2 v4.3.0 (rhubarb)\n---- **** -----\n--- * ***  * -- Linux-4.15.0-55-generic-x86_64-with-Ubuntu-18.04-bionic 2019-07-31 20:34:06\n-- * - **** ---\n- ** ---------- [config]\n- ** ---------- .> app:         tasks:0x7f7b1ccad4d0\n- ** ---------- .> transport:   redis://localhost:6379/0\n- ** ---------- .> results:     disabled://\n- *** --- * --- .> concurrency: 10 (gevent)\n-- ******* ---- .> task events: OFF (enable -E to monitor tasks in this worker)\n--- ***** -----\n -------------- [queues]\n                .> celery           exchange=celery(direct) key=celery\n\n\n[2019-07-31 20:34:06,031: WARNING/MainProcess] Recursing... num_times=6\n[2019-07-31 20:34:06,033: WARNING/MainProcess] Recursing... num_times=5\n[2019-07-31 20:34:07,052: WARNING/MainProcess] Recursed... num_times=5\n[2019-07-31 20:34:07,052: WARNING/MainProcess] Recursed... num_times=6\n[2019-07-31 20:34:07,053: WARNING/MainProcess] Recursing... num_times=5\n[2019-07-31 20:34:07,054: WARNING/MainProcess] Recursing... num_times=4\n^C\nworker: Hitting Ctrl+C again will terminate all running tasks!\n\nworker: Warm shutdown (MainProcess)\n[2019-07-31 20:34:08,059: WARNING/MainProcess] Recursed... num_times=5\n[2019-07-31 20:34:08,060: WARNING/MainProcess] Recursed... num_times=4\n[2019-07-31 20:34:08,061: WARNING/MainProcess] Recursing... num_times=4\n[2019-07-31 20:34:09,065: WARNING/MainProcess] Recursed... num_times=4\n[2019-07-31 20:34:09,067: WARNING/MainProcess] Restoring 1 unacknowledged message(s)\n```\n\nThis behavior is new in 4.x. With the same code I'm unable to replicate in 3.1.26. This behavior is very reproducible in 4.x and causes very bad duplication when workers exist in autoscaling environments.\n\nI believe this regression was introduced in 7ec89a6bf0da853fc9f7e3e9911faf86880178d6\n\nThe following patch fixes this behavior but I'm not sure it is desirable as is and would like some guidance.\n\n```\n$ git diff\ndiff --git a/celery/worker/consumer/consumer.py b/celery/worker/consumer/consumer.py\nindex 60e64c298..81aa96b1a 100644\n--- a/celery/worker/consumer/consumer.py\n+++ b/celery/worker/consumer/consumer.py\n@@ -563,8 +563,8 @@ class Consumer(object):\n                 try:\n                     strategy(\n                         message, payload,\n-                        promise(call_soon, (message.ack_log_error,)),\n-                        promise(call_soon, (message.reject_log_error,)),\n+                        message.ack_log_error,\n+                        message.reject_log_error,\n                         callbacks,\n                     )\n                 except (InvalidTaskError, ContentDisallowed) as exc:\n```\n\n\n", "hints_text": "To add to this, the problem isn't specific to recursive tasks it's just how i discover the behavior and breaks a pattern we use for sequencing tasks. You can see this undesirable behavior in this example as well\r\n\r\n```python\r\nimport time\r\nfrom celery import Celery\r\n\r\napp = Celery('tasks', broker='redis://localhost:6379/0')\r\n\r\n@app.task\r\ndef simple_task(id_, sleep_seconds=1):\r\n    print(\"{}: Sleeping {} seconds\".format(id_, sleep_seconds))\r\n    time.sleep(sleep_seconds)\r\n    print(\"{}: Slept {} seconds\".format(id_, sleep_seconds))\r\n```\r\n\r\nif you fire off 10 tasks with\r\n```python\r\nimport tasks\r\n\r\nfor i in range(10):\r\n    tasks.simple_task.delay(i)\r\n```\r\n\r\nyou get the same behavior:\r\n\r\n```\r\n[celery]gary@gir:~/celery-test$ celery worker -A tasks -P gevent -c 1\r\n[2019-08-01 05:56:38,558: WARNING/MainProcess] 0: Sleeping 1 seconds\r\n[2019-08-01 05:56:39,560: WARNING/MainProcess] 0: Slept 1 seconds\r\n[2019-08-01 05:56:39,561: WARNING/MainProcess] 1: Sleeping 1 seconds\r\n^C\r\nworker: Hitting Ctrl+C again will terminate all running tasks!\r\n\r\nworker: Warm shutdown (MainProcess)\r\n[2019-08-01 05:56:40,562: WARNING/MainProcess] 1: Slept 1 seconds\r\n[2019-08-01 05:56:40,564: WARNING/MainProcess] 2: Sleeping 1 seconds\r\n[2019-08-01 05:56:41,565: WARNING/MainProcess] 2: Slept 1 seconds\r\n[2019-08-01 05:56:41,567: WARNING/MainProcess] Restoring 1 unacknowledged message(s)\r\n```\r\n\r\nid's 0, 1, and 2 ran to completion, i don't have `acks_late` enabled and yet id 2 is restored and run again on a subsequent run\r\n\r\n```\r\n[celery]gary@gir:~/celery-test$ celery worker -A tasks -P gevent -c 1\r\n[2019-08-01 05:56:48,662: WARNING/MainProcess] 2: Sleeping 1 seconds\r\n[2019-08-01 05:56:49,664: WARNING/MainProcess] 2: Slept 1 seconds\r\n[2019-08-01 05:56:49,666: WARNING/MainProcess] 3: Sleeping 1 seconds\r\n[2019-08-01 05:56:50,667: WARNING/MainProcess] 3: Slept 1 seconds\r\n```\nis this also reproducible with celery 4.4.0rc3?\nSorry for the delay here but after finding multiple regressions that caused duplicate task bugs and issues with cpu spinning to 100% in celery 4.x and more recent kombu I've moved on to just patching celery 3.x to be py3.7 compatible instead. I've provided a very simple example to replicate the issue documented here but am no longer personally invested in this being fixed.\nI met the same problem and checked in celery 4.4.0rc4, still has this problem.\nhi guys, can you check the proposed fix for this?\nWe are using version 4.4.5 of Celery and kombu version 4.6.3.  We are still seeing this behavior.\r\n\ntry celery==4.4.6 with kombu==4.6.11\nThank you!  We will try that next week and I will let you know if it works.\r\n\nWe tried upgrading both, and the duplication was significantly worse so we rolled back.  The problem also happened when we upgraded Kombu without upgrading Celery.\ncan you provide a test case?\nHello, I had this error also. \r\n\r\nThe workaround I found is to deactivate the restore_at_shutdown : \r\n`kombu.transport.redis.QoS.restore_at_shutdown = False`\r\n\r\nThat doesn't correct the probl\u00e8me in which a gevent worker does not change the Status of the message when it is shutdown, but that prevent the risk to have task executed twice. \nHi @auvipy, I think a have a test case for this problem.\r\n\r\nI'm running celery with gevent pool with the next env and I have the same problem:\r\n```\r\nrabbitmq          3.8.12\r\n----\r\npython packages:\r\ngevent            21.1.2\r\namqp              5.0.5\r\nkombu             5.0.2\r\ncelery            5.0.5\r\n```\r\n\r\n# Expected Behavior\r\nAck was sent for any task that completed after warm shutdown.\r\n\r\n# Actual Behavior\r\nI have running background task with `acks_late=True`.\r\nWhen I press `ctrl-c` worker waiting for the pool to the last task.\r\nThen task successfully executes worker continue the shutdown process but ack for the tasks wasn't send.\r\nAfter consumer disconnection rabbitmq returns a message without ack back to a queue.\r\n\r\n\r\nI think the main reason for this problem is that after warm shutdown a consumer can't send ack for completed tasks because the `synloop` isn't running already. Gevent task just append promises to `Consumer._pending_operations` but `Consumer.perform_pending_operations` isn't called in shutdown process.\r\n\r\nCurrently, I fixed the problem by adding an extra bootstep on the consumer shutdown process:\r\n```python\r\nclass FixAck(bootsteps.StartStopStep):\r\n    requires = {\r\n        \"celery.worker.consumer.connection:Connection\",\r\n    }\r\n\r\n    def shutdown(self, c):\r\n        c.perform_pending_operations()\r\n        \r\napp.steps[\"consumer\"].add(FixAck)\r\n```\r\n\r\n\r\n# Steps to Reproduce\r\n\r\nI added two prints to debug the problem:\r\n* `celery.worker.request.Request.acknowledge` - `REQUEST ACK` to see when a task request call ack\r\n* `kombu.message.Message.ack` - `KOMBU ACK` to see when an actual ack was called\r\n\r\nMinimal example to reproduce the problem:\r\n```python\r\nfrom time import sleep\r\n\r\nfrom celery import Celery\r\nfrom gevent import getcurrent\r\n\r\napp = Celery(\"tasks\", broker=\"amqp://guest:guest@localhost:15672//\")\r\napp.conf.update(\r\n    result_backend=\"redis\",\r\n    result_persistent=True,\r\n    result_expires=3600 * 2,\r\n    redis_host=\"localhost\",\r\n    redis_port=\"16379\",\r\n    redis_db=\"7\",\r\n    redis_password=\"\",\r\n)\r\n\r\n\r\n@app.task(acks_late=True)\r\ndef add(x, y):\r\n    print(f\"ID {id(getcurrent())}\")\r\n    sleep(10)\r\n    return x + y\r\n```\r\n\r\nThen I run the celery worker:\r\n```bash\r\ncelery -A app worker --loglevel=debug  --pool gevent\r\n```\r\n\r\nThis a normal output, where the task was completed not in warm shutdown. You can see than ack was send as expected, `KOMBU ACK` was called.\r\n```bash\r\n[2021-02-19 12:06:12,890: INFO/MainProcess] Received task: app.add[abe149bb-93db-43ab-8183-f8ffc772fcf1]\r\n[2021-02-19 12:06:12,890: DEBUG/MainProcess] TaskPool: Apply <function _trace_task_ret at 0x7f13dac88790> (args:('app.add', 'abe149bb-93db-43ab-8183-f8ffc772fcf1', {'lang': 'py', 'task': 'app.add', 'id': 'abe149bb-93db-43ab-8183-f8ffc772fcf1', 'shadow': None, 'eta': None, 'expires': None, 'group': None, 'group_index': None, 'retries': 0, 'timelimit': [None, None], 'root_id': 'abe149bb-93db-43ab-8183-f8ffc772fcf1', 'parent_id': None, 'argsrepr': '(1, 2)', 'kwargsrepr': '{}', 'origin': 'gen859780@thinkpad', 'reply_to': '1357085d-e05c-304b-9ad2-ae1a17717b81', 'correlation_id': 'abe149bb-93db-43ab-8183-f8ffc772fcf1', 'hostname': 'celery@thinkpad', 'delivery_info': {'exchange': '', 'routing_key': 'celery', 'priority': 0, 'redelivered': True}, 'args': [1, 2], 'kwargs': {}}, '[[1, 2], {}, {\"callbacks\": null, \"errbacks\": null, \"chain\": null, \"chord\": null}]', 'application/json', 'utf-8') kwargs:{})\r\n[2021-02-19 12:06:12,890: DEBUG/MainProcess] Task accepted: app.add[abe149bb-93db-43ab-8183-f8ffc772fcf1] pid:880102\r\n[2021-02-19 12:06:12,890: WARNING/MainProcess] ID 139723247266256\r\n[2021-02-19 12:06:12,892: DEBUG/MainProcess] Start from server, version: 0.9, properties: {'capabilities': {'publisher_confirms': True, 'exchange_exchange_bindings': True, 'basic.nack': True, 'consumer_cancel_notify': True, 'connection.blocked': True, 'consumer_priorities': True, 'authentication_failure_close': True, 'per_consumer_qos': True, 'direct_reply_to': True}, 'cluster_name': 'rabbit@d695f948e14f', 'copyright': 'Copyright (c) 2007-2021 VMware, Inc. or its affiliates.', 'information': 'Licensed under the MPL 2.0. Website: https://rabbitmq.com', 'platform': 'Erlang/OTP 23.2.5', 'product': 'RabbitMQ', 'version': '3.8.12'}, mechanisms: [b'PLAIN', b'AMQPLAIN'], locales: ['en_US']\r\n[2021-02-19 12:06:12,894: INFO/MainProcess] pidbox: Connected to amqp://guest:**@127.0.0.1:15672//.\r\n[2021-02-19 12:06:12,894: DEBUG/MainProcess] using channel_id: 1\r\n[2021-02-19 12:06:12,895: DEBUG/MainProcess] Channel open\r\n[2021-02-19 12:06:22,893: INFO/MainProcess] Task app.add[abe149bb-93db-43ab-8183-f8ffc772fcf1] succeeded in 10.002478840004187s: 3\r\n[2021-02-19 12:06:22,893: WARNING/MainProcess] REQUEST ACK\r\n[2021-02-19 12:06:24,883: WARNING/MainProcess] KOMBU ACK\r\n\r\n\r\n^C\r\nworker: Hitting Ctrl+C again will terminate all running tasks!\r\n\r\nworker: Warm shutdown (MainProcess)\r\n```\r\n\r\nThis an output with the problem, where the task was completed in warm shutdown. `KOMBU ACK` wasn't called.\r\n```bash\r\n[2021-02-19 12:06:44,405: INFO/MainProcess] Received task: app.add[4844fd78-f439-4404-9422-2c97e96b7c59]\r\n[2021-02-19 12:06:44,406: DEBUG/MainProcess] TaskPool: Apply <function _trace_task_ret at 0x7fc080ae4820> (args:('app.add', '4844fd78-f439-4404-9422-2c97e96b7c59', {'lang': 'py', 'task': 'app.add', 'id': '4844fd78-f439-4404-9422-2c97e96b7c59', 'shadow': None, 'eta': None, 'expires': None, 'group': None, 'group_index': None, 'retries': 0, 'timelimit': [None, None], 'root_id': '4844fd78-f439-4404-9422-2c97e96b7c59', 'parent_id': None, 'argsrepr': '(1, 2)', 'kwargsrepr': '{}', 'origin': 'gen880833@thinkpad', 'reply_to': '117bddc4-c3ea-3ab2-b3ef-a93a447699e7', 'correlation_id': '4844fd78-f439-4404-9422-2c97e96b7c59', 'hostname': 'celery@thinkpad', 'delivery_info': {'exchange': '', 'routing_key': 'celery', 'priority': 0, 'redelivered': False}, 'args': [1, 2], 'kwargs': {}}, '[[1, 2], {}, {\"callbacks\": null, \"errbacks\": null, \"chain\": null, \"chord\": null}]', 'application/json', 'utf-8') kwargs:{})\r\n[2021-02-19 12:06:44,406: DEBUG/MainProcess] Task accepted: app.add[4844fd78-f439-4404-9422-2c97e96b7c59] pid:881208\r\n[2021-02-19 12:06:44,406: WARNING/MainProcess] ID 140464764963280\r\n^C\r\nworker: Hitting Ctrl+C again will terminate all running tasks!\r\n\r\nworker: Warm shutdown (MainProcess)\r\n[2021-02-19 12:06:46,914: DEBUG/MainProcess] | Worker: Closing Pool...\r\n[2021-02-19 12:06:46,914: DEBUG/MainProcess] | Worker: Closing Consumer...\r\n[2021-02-19 12:06:46,915: DEBUG/MainProcess] | Worker: Stopping Consumer...\r\n[2021-02-19 12:06:46,915: DEBUG/MainProcess] | Consumer: Closing Connection...\r\n[2021-02-19 12:06:46,915: DEBUG/MainProcess] | Consumer: Closing Events...\r\n[2021-02-19 12:06:46,915: DEBUG/MainProcess] | Consumer: Closing Mingle...\r\n[2021-02-19 12:06:46,916: DEBUG/MainProcess] | Consumer: Closing Tasks...\r\n[2021-02-19 12:06:46,916: DEBUG/MainProcess] | Consumer: Closing Control...\r\n[2021-02-19 12:06:46,916: DEBUG/MainProcess] | Consumer: Closing Heart...\r\n[2021-02-19 12:06:46,916: DEBUG/MainProcess] | Consumer: Closing Gossip...\r\n[2021-02-19 12:06:46,916: DEBUG/MainProcess] | Consumer: Closing event loop...\r\n[2021-02-19 12:06:46,917: DEBUG/MainProcess] | Consumer: Stopping event loop...\r\n[2021-02-19 12:06:46,917: DEBUG/MainProcess] | Consumer: Stopping Gossip...\r\n[2021-02-19 12:06:46,928: DEBUG/MainProcess] Closed channel #2\r\n[2021-02-19 12:06:46,928: DEBUG/MainProcess] | Consumer: Stopping Heart...\r\n[2021-02-19 12:06:46,931: DEBUG/MainProcess] | Consumer: Stopping Control...\r\n[2021-02-19 12:06:46,932: DEBUG/MainProcess] Waiting for broadcast thread to shutdown...\r\n[2021-02-19 12:06:47,415: DEBUG/MainProcess] Closed channel #1\r\n[2021-02-19 12:06:47,416: DEBUG/MainProcess] | Consumer: Stopping Tasks...\r\n[2021-02-19 12:06:47,417: DEBUG/MainProcess] Canceling task consumer...\r\n[2021-02-19 12:06:47,418: DEBUG/MainProcess] | Consumer: Stopping Mingle...\r\n[2021-02-19 12:06:47,419: DEBUG/MainProcess] | Consumer: Stopping Events...\r\n[2021-02-19 12:06:47,419: DEBUG/MainProcess] | Consumer: Stopping Connection...\r\n[2021-02-19 12:06:47,419: DEBUG/MainProcess] | Worker: Stopping Pool...\r\n[2021-02-19 12:06:54,421: INFO/MainProcess] Task app.add[4844fd78-f439-4404-9422-2c97e96b7c59] succeeded in 10.014907724049408s: 3\r\n[2021-02-19 12:06:54,422: WARNING/MainProcess] REQUEST ACK\r\n[2021-02-19 12:06:54,423: DEBUG/MainProcess] | Consumer: Shutdown Gossip...\r\n[2021-02-19 12:06:54,423: DEBUG/MainProcess] | Consumer: Shutdown Heart...\r\n[2021-02-19 12:06:54,423: DEBUG/MainProcess] | Consumer: Shutdown Control...\r\n[2021-02-19 12:06:54,423: DEBUG/MainProcess] | Consumer: Shutdown Tasks...\r\n[2021-02-19 12:06:54,424: DEBUG/MainProcess] Canceling task consumer...\r\n[2021-02-19 12:06:54,424: DEBUG/MainProcess] Closing consumer channel...\r\n[2021-02-19 12:06:54,424: DEBUG/MainProcess] | Consumer: Shutdown Events...\r\n[2021-02-19 12:06:54,425: DEBUG/MainProcess] Closed channel #1\r\n[2021-02-19 12:06:54,427: DEBUG/MainProcess] | Consumer: Shutdown Connection...\r\n[2021-02-19 12:06:54,439: DEBUG/MainProcess] Closed channel #1\r\n```\r\n\r\n\nhttps://github.com/celery/celery/commit/7ec89a6bf0da853fc9f7e3e9911faf86880178d6 can check this to find out the root of the problem as it's linked to the report\nI've seen this PR and it was a clue why it's not working on the shutdown process.\r\n\r\nHow do you think, it's possible to add `c.perform_pending_operations()` into `celery.worker.consumer.connection.shutdown` just before closing a connection?\n> I've seen this PR and it was a clue why it's not working on the shutdown process.\r\n> \r\n> How do you think, it's possible to add `c.perform_pending_operations()` into `celery.worker.consumer.connection.shutdown` just before closing a connection?\r\n\r\nyou could try this locally & see what happens\nHi @auvipy, it's me again ) \r\n\r\nIt looks like the problem is inside the `celery.worker.loops.synloop` and relevant only to the `gevent/eventlet` mode when celery use `promise` and `synloop` the operation is scheduled to been executed async. \r\n\r\nThen when celery receives the signal to shut down, it's just return the task in queue instead of acknowledge it properly.\r\n\r\nIt looks like we just need to change ordering of loop execution and set the https://github.com/celery/celery/blob/47552a7555ea513711ad11d42028dd2d1addd8ae/celery/worker/loops.py#L123 (maybe_shutdown) in the end of the while loop. \r\n\r\nI would like to test this on my end, and I can prepare a PR if that works. Alternatively, if you have the capacity to handle it, that would be great! :)\nTo add to this, the problem isn't specific to recursive tasks it's just how i discover the behavior and breaks a pattern we use for sequencing tasks. You can see this undesirable behavior in this example as well\r\n\r\n```python\r\nimport time\r\nfrom celery import Celery\r\n\r\napp = Celery('tasks', broker='redis://localhost:6379/0')\r\n\r\n@app.task\r\ndef simple_task(id_, sleep_seconds=1):\r\n    print(\"{}: Sleeping {} seconds\".format(id_, sleep_seconds))\r\n    time.sleep(sleep_seconds)\r\n    print(\"{}: Slept {} seconds\".format(id_, sleep_seconds))\r\n```\r\n\r\nif you fire off 10 tasks with\r\n```python\r\nimport tasks\r\n\r\nfor i in range(10):\r\n    tasks.simple_task.delay(i)\r\n```\r\n\r\nyou get the same behavior:\r\n\r\n```\r\n[celery]gary@gir:~/celery-test$ celery worker -A tasks -P gevent -c 1\r\n[2019-08-01 05:56:38,558: WARNING/MainProcess] 0: Sleeping 1 seconds\r\n[2019-08-01 05:56:39,560: WARNING/MainProcess] 0: Slept 1 seconds\r\n[2019-08-01 05:56:39,561: WARNING/MainProcess] 1: Sleeping 1 seconds\r\n^C\r\nworker: Hitting Ctrl+C again will terminate all running tasks!\r\n\r\nworker: Warm shutdown (MainProcess)\r\n[2019-08-01 05:56:40,562: WARNING/MainProcess] 1: Slept 1 seconds\r\n[2019-08-01 05:56:40,564: WARNING/MainProcess] 2: Sleeping 1 seconds\r\n[2019-08-01 05:56:41,565: WARNING/MainProcess] 2: Slept 1 seconds\r\n[2019-08-01 05:56:41,567: WARNING/MainProcess] Restoring 1 unacknowledged message(s)\r\n```\r\n\r\nid's 0, 1, and 2 ran to completion, i don't have `acks_late` enabled and yet id 2 is restored and run again on a subsequent run\r\n\r\n```\r\n[celery]gary@gir:~/celery-test$ celery worker -A tasks -P gevent -c 1\r\n[2019-08-01 05:56:48,662: WARNING/MainProcess] 2: Sleeping 1 seconds\r\n[2019-08-01 05:56:49,664: WARNING/MainProcess] 2: Slept 1 seconds\r\n[2019-08-01 05:56:49,666: WARNING/MainProcess] 3: Sleeping 1 seconds\r\n[2019-08-01 05:56:50,667: WARNING/MainProcess] 3: Slept 1 seconds\r\n```\nis this also reproducible with celery 4.4.0rc3?\nSorry for the delay here but after finding multiple regressions that caused duplicate task bugs and issues with cpu spinning to 100% in celery 4.x and more recent kombu I've moved on to just patching celery 3.x to be py3.7 compatible instead. I've provided a very simple example to replicate the issue documented here but am no longer personally invested in this being fixed.\nI met the same problem and checked in celery 4.4.0rc4, still has this problem.\nhi guys, can you check the proposed fix for this?\nWe are using version 4.4.5 of Celery and kombu version 4.6.3.  We are still seeing this behavior.\r\n\ntry celery==4.4.6 with kombu==4.6.11\nThank you!  We will try that next week and I will let you know if it works.\r\n\nWe tried upgrading both, and the duplication was significantly worse so we rolled back.  The problem also happened when we upgraded Kombu without upgrading Celery.\ncan you provide a test case?\nHello, I had this error also. \r\n\r\nThe workaround I found is to deactivate the restore_at_shutdown : \r\n`kombu.transport.redis.QoS.restore_at_shutdown = False`\r\n\r\nThat doesn't correct the probl\u00e8me in which a gevent worker does not change the Status of the message when it is shutdown, but that prevent the risk to have task executed twice. \nHi @auvipy, I think a have a test case for this problem.\r\n\r\nI'm running celery with gevent pool with the next env and I have the same problem:\r\n```\r\nrabbitmq          3.8.12\r\n----\r\npython packages:\r\ngevent            21.1.2\r\namqp              5.0.5\r\nkombu             5.0.2\r\ncelery            5.0.5\r\n```\r\n\r\n# Expected Behavior\r\nAck was sent for any task that completed after warm shutdown.\r\n\r\n# Actual Behavior\r\nI have running background task with `acks_late=True`.\r\nWhen I press `ctrl-c` worker waiting for the pool to the last task.\r\nThen task successfully executes worker continue the shutdown process but ack for the tasks wasn't send.\r\nAfter consumer disconnection rabbitmq returns a message without ack back to a queue.\r\n\r\n\r\nI think the main reason for this problem is that after warm shutdown a consumer can't send ack for completed tasks because the `synloop` isn't running already. Gevent task just append promises to `Consumer._pending_operations` but `Consumer.perform_pending_operations` isn't called in shutdown process.\r\n\r\nCurrently, I fixed the problem by adding an extra bootstep on the consumer shutdown process:\r\n```python\r\nclass FixAck(bootsteps.StartStopStep):\r\n    requires = {\r\n        \"celery.worker.consumer.connection:Connection\",\r\n    }\r\n\r\n    def shutdown(self, c):\r\n        c.perform_pending_operations()\r\n        \r\napp.steps[\"consumer\"].add(FixAck)\r\n```\r\n\r\n\r\n# Steps to Reproduce\r\n\r\nI added two prints to debug the problem:\r\n* `celery.worker.request.Request.acknowledge` - `REQUEST ACK` to see when a task request call ack\r\n* `kombu.message.Message.ack` - `KOMBU ACK` to see when an actual ack was called\r\n\r\nMinimal example to reproduce the problem:\r\n```python\r\nfrom time import sleep\r\n\r\nfrom celery import Celery\r\nfrom gevent import getcurrent\r\n\r\napp = Celery(\"tasks\", broker=\"amqp://guest:guest@localhost:15672//\")\r\napp.conf.update(\r\n    result_backend=\"redis\",\r\n    result_persistent=True,\r\n    result_expires=3600 * 2,\r\n    redis_host=\"localhost\",\r\n    redis_port=\"16379\",\r\n    redis_db=\"7\",\r\n    redis_password=\"\",\r\n)\r\n\r\n\r\n@app.task(acks_late=True)\r\ndef add(x, y):\r\n    print(f\"ID {id(getcurrent())}\")\r\n    sleep(10)\r\n    return x + y\r\n```\r\n\r\nThen I run the celery worker:\r\n```bash\r\ncelery -A app worker --loglevel=debug  --pool gevent\r\n```\r\n\r\nThis a normal output, where the task was completed not in warm shutdown. You can see than ack was send as expected, `KOMBU ACK` was called.\r\n```bash\r\n[2021-02-19 12:06:12,890: INFO/MainProcess] Received task: app.add[abe149bb-93db-43ab-8183-f8ffc772fcf1]\r\n[2021-02-19 12:06:12,890: DEBUG/MainProcess] TaskPool: Apply <function _trace_task_ret at 0x7f13dac88790> (args:('app.add', 'abe149bb-93db-43ab-8183-f8ffc772fcf1', {'lang': 'py', 'task': 'app.add', 'id': 'abe149bb-93db-43ab-8183-f8ffc772fcf1', 'shadow': None, 'eta': None, 'expires': None, 'group': None, 'group_index': None, 'retries': 0, 'timelimit': [None, None], 'root_id': 'abe149bb-93db-43ab-8183-f8ffc772fcf1', 'parent_id': None, 'argsrepr': '(1, 2)', 'kwargsrepr': '{}', 'origin': 'gen859780@thinkpad', 'reply_to': '1357085d-e05c-304b-9ad2-ae1a17717b81', 'correlation_id': 'abe149bb-93db-43ab-8183-f8ffc772fcf1', 'hostname': 'celery@thinkpad', 'delivery_info': {'exchange': '', 'routing_key': 'celery', 'priority': 0, 'redelivered': True}, 'args': [1, 2], 'kwargs': {}}, '[[1, 2], {}, {\"callbacks\": null, \"errbacks\": null, \"chain\": null, \"chord\": null}]', 'application/json', 'utf-8') kwargs:{})\r\n[2021-02-19 12:06:12,890: DEBUG/MainProcess] Task accepted: app.add[abe149bb-93db-43ab-8183-f8ffc772fcf1] pid:880102\r\n[2021-02-19 12:06:12,890: WARNING/MainProcess] ID 139723247266256\r\n[2021-02-19 12:06:12,892: DEBUG/MainProcess] Start from server, version: 0.9, properties: {'capabilities': {'publisher_confirms': True, 'exchange_exchange_bindings': True, 'basic.nack': True, 'consumer_cancel_notify': True, 'connection.blocked': True, 'consumer_priorities': True, 'authentication_failure_close': True, 'per_consumer_qos': True, 'direct_reply_to': True}, 'cluster_name': 'rabbit@d695f948e14f', 'copyright': 'Copyright (c) 2007-2021 VMware, Inc. or its affiliates.', 'information': 'Licensed under the MPL 2.0. Website: https://rabbitmq.com', 'platform': 'Erlang/OTP 23.2.5', 'product': 'RabbitMQ', 'version': '3.8.12'}, mechanisms: [b'PLAIN', b'AMQPLAIN'], locales: ['en_US']\r\n[2021-02-19 12:06:12,894: INFO/MainProcess] pidbox: Connected to amqp://guest:**@127.0.0.1:15672//.\r\n[2021-02-19 12:06:12,894: DEBUG/MainProcess] using channel_id: 1\r\n[2021-02-19 12:06:12,895: DEBUG/MainProcess] Channel open\r\n[2021-02-19 12:06:22,893: INFO/MainProcess] Task app.add[abe149bb-93db-43ab-8183-f8ffc772fcf1] succeeded in 10.002478840004187s: 3\r\n[2021-02-19 12:06:22,893: WARNING/MainProcess] REQUEST ACK\r\n[2021-02-19 12:06:24,883: WARNING/MainProcess] KOMBU ACK\r\n\r\n\r\n^C\r\nworker: Hitting Ctrl+C again will terminate all running tasks!\r\n\r\nworker: Warm shutdown (MainProcess)\r\n```\r\n\r\nThis an output with the problem, where the task was completed in warm shutdown. `KOMBU ACK` wasn't called.\r\n```bash\r\n[2021-02-19 12:06:44,405: INFO/MainProcess] Received task: app.add[4844fd78-f439-4404-9422-2c97e96b7c59]\r\n[2021-02-19 12:06:44,406: DEBUG/MainProcess] TaskPool: Apply <function _trace_task_ret at 0x7fc080ae4820> (args:('app.add', '4844fd78-f439-4404-9422-2c97e96b7c59', {'lang': 'py', 'task': 'app.add', 'id': '4844fd78-f439-4404-9422-2c97e96b7c59', 'shadow': None, 'eta': None, 'expires': None, 'group': None, 'group_index': None, 'retries': 0, 'timelimit': [None, None], 'root_id': '4844fd78-f439-4404-9422-2c97e96b7c59', 'parent_id': None, 'argsrepr': '(1, 2)', 'kwargsrepr': '{}', 'origin': 'gen880833@thinkpad', 'reply_to': '117bddc4-c3ea-3ab2-b3ef-a93a447699e7', 'correlation_id': '4844fd78-f439-4404-9422-2c97e96b7c59', 'hostname': 'celery@thinkpad', 'delivery_info': {'exchange': '', 'routing_key': 'celery', 'priority': 0, 'redelivered': False}, 'args': [1, 2], 'kwargs': {}}, '[[1, 2], {}, {\"callbacks\": null, \"errbacks\": null, \"chain\": null, \"chord\": null}]', 'application/json', 'utf-8') kwargs:{})\r\n[2021-02-19 12:06:44,406: DEBUG/MainProcess] Task accepted: app.add[4844fd78-f439-4404-9422-2c97e96b7c59] pid:881208\r\n[2021-02-19 12:06:44,406: WARNING/MainProcess] ID 140464764963280\r\n^C\r\nworker: Hitting Ctrl+C again will terminate all running tasks!\r\n\r\nworker: Warm shutdown (MainProcess)\r\n[2021-02-19 12:06:46,914: DEBUG/MainProcess] | Worker: Closing Pool...\r\n[2021-02-19 12:06:46,914: DEBUG/MainProcess] | Worker: Closing Consumer...\r\n[2021-02-19 12:06:46,915: DEBUG/MainProcess] | Worker: Stopping Consumer...\r\n[2021-02-19 12:06:46,915: DEBUG/MainProcess] | Consumer: Closing Connection...\r\n[2021-02-19 12:06:46,915: DEBUG/MainProcess] | Consumer: Closing Events...\r\n[2021-02-19 12:06:46,915: DEBUG/MainProcess] | Consumer: Closing Mingle...\r\n[2021-02-19 12:06:46,916: DEBUG/MainProcess] | Consumer: Closing Tasks...\r\n[2021-02-19 12:06:46,916: DEBUG/MainProcess] | Consumer: Closing Control...\r\n[2021-02-19 12:06:46,916: DEBUG/MainProcess] | Consumer: Closing Heart...\r\n[2021-02-19 12:06:46,916: DEBUG/MainProcess] | Consumer: Closing Gossip...\r\n[2021-02-19 12:06:46,916: DEBUG/MainProcess] | Consumer: Closing event loop...\r\n[2021-02-19 12:06:46,917: DEBUG/MainProcess] | Consumer: Stopping event loop...\r\n[2021-02-19 12:06:46,917: DEBUG/MainProcess] | Consumer: Stopping Gossip...\r\n[2021-02-19 12:06:46,928: DEBUG/MainProcess] Closed channel #2\r\n[2021-02-19 12:06:46,928: DEBUG/MainProcess] | Consumer: Stopping Heart...\r\n[2021-02-19 12:06:46,931: DEBUG/MainProcess] | Consumer: Stopping Control...\r\n[2021-02-19 12:06:46,932: DEBUG/MainProcess] Waiting for broadcast thread to shutdown...\r\n[2021-02-19 12:06:47,415: DEBUG/MainProcess] Closed channel #1\r\n[2021-02-19 12:06:47,416: DEBUG/MainProcess] | Consumer: Stopping Tasks...\r\n[2021-02-19 12:06:47,417: DEBUG/MainProcess] Canceling task consumer...\r\n[2021-02-19 12:06:47,418: DEBUG/MainProcess] | Consumer: Stopping Mingle...\r\n[2021-02-19 12:06:47,419: DEBUG/MainProcess] | Consumer: Stopping Events...\r\n[2021-02-19 12:06:47,419: DEBUG/MainProcess] | Consumer: Stopping Connection...\r\n[2021-02-19 12:06:47,419: DEBUG/MainProcess] | Worker: Stopping Pool...\r\n[2021-02-19 12:06:54,421: INFO/MainProcess] Task app.add[4844fd78-f439-4404-9422-2c97e96b7c59] succeeded in 10.014907724049408s: 3\r\n[2021-02-19 12:06:54,422: WARNING/MainProcess] REQUEST ACK\r\n[2021-02-19 12:06:54,423: DEBUG/MainProcess] | Consumer: Shutdown Gossip...\r\n[2021-02-19 12:06:54,423: DEBUG/MainProcess] | Consumer: Shutdown Heart...\r\n[2021-02-19 12:06:54,423: DEBUG/MainProcess] | Consumer: Shutdown Control...\r\n[2021-02-19 12:06:54,423: DEBUG/MainProcess] | Consumer: Shutdown Tasks...\r\n[2021-02-19 12:06:54,424: DEBUG/MainProcess] Canceling task consumer...\r\n[2021-02-19 12:06:54,424: DEBUG/MainProcess] Closing consumer channel...\r\n[2021-02-19 12:06:54,424: DEBUG/MainProcess] | Consumer: Shutdown Events...\r\n[2021-02-19 12:06:54,425: DEBUG/MainProcess] Closed channel #1\r\n[2021-02-19 12:06:54,427: DEBUG/MainProcess] | Consumer: Shutdown Connection...\r\n[2021-02-19 12:06:54,439: DEBUG/MainProcess] Closed channel #1\r\n```\r\n\r\n\nhttps://github.com/celery/celery/commit/7ec89a6bf0da853fc9f7e3e9911faf86880178d6 can check this to find out the root of the problem as it's linked to the report\nI've seen this PR and it was a clue why it's not working on the shutdown process.\r\n\r\nHow do you think, it's possible to add `c.perform_pending_operations()` into `celery.worker.consumer.connection.shutdown` just before closing a connection?\n> I've seen this PR and it was a clue why it's not working on the shutdown process.\r\n> \r\n> How do you think, it's possible to add `c.perform_pending_operations()` into `celery.worker.consumer.connection.shutdown` just before closing a connection?\r\n\r\nyou could try this locally & see what happens\nHi @auvipy, it's me again ) \r\n\r\nIt looks like the problem is inside the `celery.worker.loops.synloop` and relevant only to the `gevent/eventlet` mode when celery use `promise` and `synloop` the operation is scheduled to been executed async. \r\n\r\nThen when celery receives the signal to shut down, it's just return the task in queue instead of acknowledge it properly.\r\n\r\nIt looks like we just need to change ordering of loop execution and set the https://github.com/celery/celery/blob/47552a7555ea513711ad11d42028dd2d1addd8ae/celery/worker/loops.py#L123 (maybe_shutdown) in the end of the while loop. \r\n\r\nI would like to test this on my end, and I can prepare a PR if that works. Alternatively, if you have the capacity to handle it, that would be great! :)", "created_at": "2024-10-11T10:15:01Z"}
{"repo": "celery/celery", "pull_number": 9331, "instance_id": "celery__celery-9331", "issue_numbers": ["4777"], "base_commit": "b3cd4988467b14c61d42eaae691ad2ab04923eff", "patch": "diff --git a/celery/beat.py b/celery/beat.py\nindex 9656493ecb..86ad837f0d 100644\n--- a/celery/beat.py\n+++ b/celery/beat.py\n@@ -1,6 +1,7 @@\n \"\"\"The periodic task scheduler.\"\"\"\n \n import copy\n+import dbm\n import errno\n import heapq\n import os\n@@ -572,7 +573,7 @@ def _create_schedule(self):\n                 # new schedule db\n                 try:\n                     self._store['entries'] = {}\n-                except (KeyError, UnicodeDecodeError, TypeError) as exc:\n+                except (KeyError, UnicodeDecodeError, TypeError) + dbm.error as exc:\n                     self._store = self._destroy_open_corrupted_schedule(exc)\n                     continue\n             else:\n", "test_patch": "diff --git a/t/unit/app/test_beat.py b/t/unit/app/test_beat.py\nindex a95e8e4140..b81a11426e 100644\n--- a/t/unit/app/test_beat.py\n+++ b/t/unit/app/test_beat.py\n@@ -1,3 +1,4 @@\n+import dbm\n import errno\n import sys\n from datetime import datetime, timedelta, timezone\n@@ -688,6 +689,25 @@ def test_create_schedule_corrupted(self):\n         s._create_schedule()\n         s._destroy_open_corrupted_schedule.assert_called_with(expected_error)\n \n+    def test_create_schedule_corrupted_dbm_error(self):\n+        \"\"\"\n+        Test that any dbm.error that might happen when opening beat-schedule.db are caught\n+        \"\"\"\n+        s = create_persistent_scheduler()[0](app=self.app,\n+                                             schedule_filename='schedule')\n+        s._store = MagicMock()\n+        s._destroy_open_corrupted_schedule = Mock()\n+        s._destroy_open_corrupted_schedule.return_value = MagicMock()\n+\n+        # self._store['entries'] = {} will throw a KeyError\n+        s._store.__getitem__.side_effect = KeyError()\n+        # then, when _create_schedule tries to reset _store['entries'], throw another error, specifically dbm.error\n+        expected_error = dbm.error[0]()\n+        s._store.__setitem__.side_effect = expected_error\n+\n+        s._create_schedule()\n+        s._destroy_open_corrupted_schedule.assert_called_with(expected_error)\n+\n     def test_create_schedule_missing_entries(self):\n         \"\"\"\n         Test that if _create_schedule can't find the key \"entries\" in _store it will recreate it\n", "problem_statement": "Corrupted scheduler database\nHello,\r\n\r\nI'm using celery in a django project in production env and sometimes, after a release PersistentScheduler become corrupted and i can't figure why.\r\n\r\nThe last release it happen i updated celery from 4.1.0 to 4.1.1 and updated my CELERY_BEAT_SCHEDULE settings (adding two new beats). Maybe a clue ?\r\nCelery is running as a systemd service.\r\n\r\nThe error:\r\n```[2018-05-26 09:01:01,377: ERROR/Beat] Process Beat\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/sillonlorrain-media/lib/python3.6/site-packages/kombu/utils/objects.py\", line 42, in __get__\r\n    return obj.__dict__[self.__name__]\r\nKeyError: 'scheduler'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.6/shelve.py\", line 111, in __getitem__\r\n    value = self.cache[key]\r\nKeyError: 'entries'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/sillonlorrain-media/lib/python3.6/site-packages/celery/beat.py\", line 481, in _create_schedule\r\n    self._store[str('entries')]\r\n  File \"/usr/lib/python3.6/shelve.py\", line 113, in __getitem__\r\n    f = BytesIO(self.dict[key.encode(self.keyencoding)])\r\nKeyError: b'entries'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/sillonlorrain-media/lib/python3.6/site-packages/billiard/process.py\", line 327, in _bootstrap\r\n    self.run()\r\n  File \"/usr/lib/sillonlorrain-media/lib/python3.6/site-packages/celery/beat.py\", line 630, in run\r\n    self.service.start(embedded_process=True)\r\n  File \"/usr/lib/sillonlorrain-media/lib/python3.6/site-packages/celery/beat.py\", line 545, in start\r\n    humanize_seconds(self.scheduler.max_interval))\r\n  File \"/usr/lib/sillonlorrain-media/lib/python3.6/site-packages/kombu/utils/objects.py\", line 44, in __get__\r\n    value = obj.__dict__[self.__name__] = self.__get(obj)\r\n  File \"/usr/lib/sillonlorrain-media/lib/python3.6/site-packages/celery/beat.py\", line 589, in scheduler\r\n    return self.get_scheduler()\r\n  File \"/usr/lib/sillonlorrain-media/lib/python3.6/site-packages/celery/beat.py\", line 584, in get_scheduler\r\n    lazy=lazy,\r\n  File \"/usr/lib/sillonlorrain-media/lib/python3.6/site-packages/celery/beat.py\", line 424, in __init__\r\n    Scheduler.__init__(self, *args, **kwargs)\r\n  File \"/usr/lib/sillonlorrain-media/lib/python3.6/site-packages/celery/beat.py\", line 202, in __init__\r\n    self.setup_schedule()\r\n  File \"/usr/lib/sillonlorrain-media/lib/python3.6/site-packages/celery/beat.py\", line 452, in setup_schedule\r\n    self._create_schedule()\r\n  File \"/usr/lib/sillonlorrain-media/lib/python3.6/site-packages/celery/beat.py\", line 485, in _create_schedule\r\n    self._store[str('entries')] = {}\r\n  File \"/usr/lib/python3.6/shelve.py\", line 125, in __setitem__\r\n    self.dict[key.encode(self.keyencoding)] = f.getvalue()\r\n_dbm.error: cannot add item to database\r\n```\r\n\r\n## Checklist\r\nThis report is from my dev env but prod is the same except broker and cache access.\r\n```\r\nCELERY_ACCEPT_CONTENT: ['msgpack']\r\nCELERY_BEAT_SCHEDULE: {\r\n    'clear-sessions': {   'schedule': <crontab: 0 0 * * * (m/h/d/dM/MY)>,\r\n                          'task': 'sessions.tasks.clear_sessions'},\r\n    'flush-logs': {   'schedule': <crontab: 0 0 * * * (m/h/d/dM/MY)>,\r\n                      'task': 'clients.tasks.flush_logs'},\r\n    'harvest-presta': {   'schedule': <crontab: 0 0 * * * (m/h/d/dM/MY)>,\r\n                                     'task': 'feed_importers.presta.tasks.harvest_presta'},\r\n    'harvest-presta2': {   'schedule': <crontab: 0 1 * * * (m/h/d/dM/MY)>,\r\n                          'task': 'feed_importers.presta2.tasks.harvest_presta2'},\r\n    'harvest-presta3': {   'schedule': <crontab: 0 5,6,7,8,9,10,11 * * * (m/h/d/dM/MY)>,\r\n                           'task': 'feed_importers.presta3.tasks.harvest_presta3'},\r\n    'harvest-presta4': {   'schedule': <crontab: 30 7 * * * (m/h/d/dM/MY)>,\r\n                       'task': 'feed_importers.presta4.tasks.harvest_presta4'},\r\n    'import-users': {   'schedule': <crontab: 0 3 * * * (m/h/d/dM/MY)>,\r\n                        'task': 'clients.tasks.import_media_users'},\r\n    'update_bookings': {   'schedule': <crontab: 0 */1 * * * (m/h/d/dM/MY)>,\r\n                           'task': 'loans.tasks.update_bookings'},\r\n    'update_ended_book_loans': {   'schedule': <crontab: */5 * * * * (m/h/d/dM/MY)>,\r\n                                   'task': 'loans.tasks.update_ended_book_loans'},\r\n    'update_orders': {   'schedule': <crontab: 0 18 * * * (m/h/d/dM/MY)>,\r\n                         'task': 'loans.tasks.update_orders'}}\r\nCELERY_BROKER_URL: 'amqp://media:********@rabbitmq:5672/mediavhost'\r\nCELERY_BROKER_USE_SSL: False\r\nCELERY_RESULT_BACKEND: 'cache+memcached://memcached:11211/'\r\nCELERY_RESULT_EXCHANGE: 'media_results'\r\nCELERY_RESULT_EXPIRES: 86400\r\nCELERY_RESULT_PERSISTENT: True\r\nCELERY_RESULT_SERIALIZER: 'msgpack'\r\nCELERY_TASK_DEFAULT_QUEUE: 'media_celery'\r\nCELERY_TASK_ROUTES: {\r\n    'media.tasks.delete_list_by_id_presta': {'queue': 'indexation'},\r\n    'media.tasks.update_books_by_ean_13': {'queue': 'indexation'},\r\n    'media.tasks.update_list_by_slug': {'queue': 'indexation'},\r\n    'media.tasks.update_object_list_highlight': {'queue': 'indexation'},\r\n    'search.tasks.delete_object': {'queue': 'indexation'},\r\n    'search.tasks.delete_object_list': {'queue': 'indexation'},\r\n    'search.tasks.update_linked_medias': {'queue': 'indexation'},\r\n    'search.tasks.update_object': {'queue': 'indexation'},\r\n    'search.tasks.update_object_list': {'queue': 'indexation'}}\r\nCELERY_TASK_SERIALIZER: 'msgpack'\r\nCELERY_TIMEZONE: 'Europe/Paris'\r\n\r\n```\r\n## Steps to reproduce\r\nSorry i dunno.\r\n\r\n## Expected behavior\r\nI investiguated source code and there is a process to destroy and recreate corrupted scheduler database but it does not catch my exception.\r\n\r\nSo if we can't find my mistake, i think i'll fork to add a catch.\r\n\r\nMy actual \"fix process\" is to delete celerybeat-schedule file and restart my systemd service.\r\n\r\n## Actual behavior\r\nAfter release this error occur, not often but implies beats are not working unitl i delete the scheduler database by hand.\r\n\r\nThank you for your time !\r\n\r\n\n", "hints_text": "try latest versions and report again.\n@auvipy could we get this reopened? We are on celery 5.2.7 and experiencing the same issue. It is resolvable by deleting the schedule database, but it inevitably comes back up. It seems like a fairly common issue given the other public issues referencing it above. \r\n\r\nAs pointed out above, the functionality to catch these kinds of errors and delete the db already exists in celery (see below). The only problem is that celery only catches `KeyError`s. It seems like the fix might be a simple as expanding the catch here to include [`dbm.error`](https://docs.python.org/3/library/dbm.html#dbm.error). I'm not sure if there are other dbm errors that should be thrown or not, so it may need to be more specific.\r\n\r\nhttps://github.com/celery/celery/blob/848b8ad97d13ed97cd8a520cd64b459a79c37d96/celery/beat.py#L573-L577\r\n\r\nTo add a little bit more context, celery uses `shelve`, which uses `dbm`. [dbm documentation](https://docs.python.org/3/library/dbm.html#dbm.gnu.error) says that \"`KeyError` is raised for general mapping errors like specifying an incorrect key,\" while the DBM-variant-specific errors \"such as I/O errors\" will be raised as dbm errors. Evidently, these exceptions should also be considered indications of corruption.\nThis still seems like an issue with 5.4.0, based on https://github.com/paperless-ngx/paperless-ngx/discussions/7440, which reports:\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/celery/apps/beat.py\", line 113, in start_scheduler\r\n    service.start()\r\n  File \"/usr/local/lib/python3.10/dist-packages/celery/beat.py\", line 634, in start\r\n    humanize_seconds(self.scheduler.max_interval))\r\n  File \"/usr/local/lib/python3.10/dist-packages/kombu/utils/objects.py\", line 40, in __get__\r\n    return super().__get__(instance, owner)\r\n  File \"/usr/lib/python3.10/functools.py\", line 981, in __get__\r\n    val = self.func(instance)\r\n  File \"/usr/local/lib/python3.10/dist-packages/celery/beat.py\", line 677, in scheduler\r\n    return self.get_scheduler()\r\n  File \"/usr/local/lib/python3.10/dist-packages/celery/beat.py\", line 668, in get_scheduler\r\n    return symbol_by_name(self.scheduler_cls, aliases=aliases)(\r\n  File \"/usr/local/lib/python3.10/dist-packages/celery/beat.py\", line 513, in __init__\r\n    super().__init__(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/celery/beat.py\", line 264, in __init__\r\n    self.setup_schedule()\r\n  File \"/usr/local/lib/python3.10/dist-packages/celery/beat.py\", line 541, in setup_schedule\r\n    self._create_schedule()\r\n  File \"/usr/local/lib/python3.10/dist-packages/celery/beat.py\", line 574, in _create_schedule\r\n    self._store['entries'] = {}\r\n  File \"/usr/lib/python3.10/shelve.py\", line 125, in __setitem__\r\n    self.dict[key.encode(self.keyencoding)] = f.getvalue()\r\n_dbm.error: cannot add item to database\r\n```\r\n\r\nIt seems like this line should include `dbm.error`: https://github.com/celery/celery/blob/40dafda3ff49ea082613d975a850a374a6ac161e/celery/beat.py#L571\r\n\r\nIf there's no objection, I can probably find time to add it\nExperiencing the same problem:\r\n\r\nCPython 3.11.2\r\nCelery 5.3.6\r\n\r\n<details>\r\n<summary>Traceback (click to expand)</summary>\r\n\r\n```python\r\nKeyError: 'entries'\r\n  File \"shelve.py\", line 111, in __getitem__\r\n    value = self.cache[key]\r\n\r\nKeyError: b'entries'\r\n  File \"celery/beat.py\", line 570, in _create_schedule\r\n    self._store['entries']\r\n  File \"shelve.py\", line 113, in __getitem__\r\n    f = BytesIO(self.dict[key.encode(self.keyencoding)])\r\n\r\nerror: cannot add item to database\r\n  File \"celery\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"celery/__main__.py\", line 15, in main\r\n    sys.exit(_main())\r\n  File \"celery/bin/celery.py\", line 236, in main\r\n    return celery(auto_envvar_prefix=\"CELERY\")\r\n  File \"click/core.py\", line 1157, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"click/core.py\", line 1078, in main\r\n    rv = self.invoke(ctx)\r\n  File \"click/core.py\", line 1688, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"click/core.py\", line 1434, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"click/core.py\", line 783, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"click/decorators.py\", line 33, in new_func\r\n    return f(get_current_context(), *args, **kwargs)\r\n  File \"celery/bin/base.py\", line 134, in caller\r\n    return f(ctx, *args, **kwargs)\r\n  File \"celery/bin/beat.py\", line 72, in beat\r\n    return beat().run()\r\n  File \"celery/apps/beat.py\", line 84, in run\r\n    self.start_scheduler()\r\n  File \"celery/apps/beat.py\", line 113, in start_scheduler\r\n    service.start()\r\n  File \"celery/beat.py\", line 634, in start\r\n    humanize_seconds(self.scheduler.max_interval))\r\n  File \"kombu/utils/objects.py\", line 40, in __get__\r\n    return super().__get__(instance, owner)\r\n  File \"functools.py\", line 1001, in __get__\r\n    val = self.func(instance)\r\n  File \"celery/beat.py\", line 677, in scheduler\r\n    return self.get_scheduler()\r\n  File \"celery/beat.py\", line 668, in get_scheduler\r\n    return symbol_by_name(self.scheduler_cls, aliases=aliases)(\r\n  File \"celery/beat.py\", line 513, in __init__\r\n    super().__init__(*args, **kwargs)\r\n  File \"celery/beat.py\", line 264, in __init__\r\n    self.setup_schedule()\r\n  File \"celery/beat.py\", line 541, in setup_schedule\r\n    self._create_schedule()\r\n  File \"celery/beat.py\", line 574, in _create_schedule\r\n    self._store['entries'] = {}\r\n  File \"shelve.py\", line 125, in __setitem__\r\n    self.dict[key.encode(self.keyencoding)] = f.getvalue()\r\n```\r\n</details>", "created_at": "2024-10-01T14:44:37Z"}
{"repo": "celery/celery", "pull_number": 9247, "instance_id": "celery__celery-9247", "issue_numbers": ["9119"], "base_commit": "690d08e6c61a1c02e71542b7d5a19c6cccccef25", "patch": "diff --git a/celery/utils/dispatch/signal.py b/celery/utils/dispatch/signal.py\nindex 0cfa6127ed0..ad8047e6bd7 100644\n--- a/celery/utils/dispatch/signal.py\n+++ b/celery/utils/dispatch/signal.py\n@@ -54,6 +54,9 @@ def _boundmethod_safe_weakref(obj):\n def _make_lookup_key(receiver, sender, dispatch_uid):\n     if dispatch_uid:\n         return (dispatch_uid, _make_id(sender))\n+    # Issue #9119 - retry-wrapped functions use the underlying function for dispatch_uid\n+    elif hasattr(receiver, '_dispatch_uid'):\n+        return (receiver._dispatch_uid, _make_id(sender))\n     else:\n         return (_make_id(receiver), _make_id(sender))\n \n@@ -170,6 +173,7 @@ def on_error(exc, intervals, retries):\n                         # it up later with the original func id\n                         options['dispatch_uid'] = _make_id(fun)\n                     fun = _retry_receiver(fun)\n+                    fun._dispatch_uid = options['dispatch_uid']\n \n                 self._connect_signal(fun, sender, options['weak'],\n                                      options['dispatch_uid'])\n", "test_patch": "diff --git a/t/unit/utils/test_dispatcher.py b/t/unit/utils/test_dispatcher.py\nindex 07ee2216dc9..0de48531af0 100644\n--- a/t/unit/utils/test_dispatcher.py\n+++ b/t/unit/utils/test_dispatcher.py\n@@ -2,8 +2,6 @@\n import sys\n import time\n \n-import pytest\n-\n from celery.utils.dispatch import Signal\n \n if sys.platform.startswith('java'):\n@@ -185,7 +183,6 @@ def test_boundmethod(self):\n         garbage_collect()\n         self._testIsClean(a_signal)\n \n-    @pytest.mark.xfail(reason=\"Issue #9119\")\n     def test_disconnect_retryable_decorator(self):\n         # Regression test for https://github.com/celery/celery/issues/9119\n \n", "problem_statement": "Cannot disconnect a signal receiver after using Signal.connect(retry=True)\n<!--\r\nPlease fill this template entirely and do not erase parts of it.\r\nWe reserve the right to close without a response\r\nbug reports which are incomplete.\r\n-->\r\n# Checklist\r\n<!--\r\nTo check an item on the list replace [ ] with [x].\r\n-->\r\n- [x] I have verified that the issue exists against the `main` branch of Celery.\r\n- [ ] This has already been asked to the [discussions forum](https://github.com/celery/celery/discussions) first.\r\n- [x] I have read the relevant section in the\r\n  [contribution guide](https://docs.celeryq.dev/en/main/contributing.html#other-bugs)\r\n  on reporting bugs.\r\n- [x] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Bug+Report%22+-label%3A%22Category%3A+Documentation%22)\r\n  for similar or identical bug reports.\r\n- [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22PR+Type%3A+Bugfix%22+-label%3A%22Category%3A+Documentation%22)\r\n  for existing proposed fixes.\r\n- [x] I have checked the [commit log](https://github.com/celery/celery/commits/main)\r\n  to find out if the bug was already fixed in the main branch.\r\n- [x] I have included all related issues and possible duplicate issues\r\n  in this issue (If there are none, check this box anyway).\r\n- [ ] I have tried to reproduce the issue with [pytest-celery](https://docs.celeryq.dev/projects/pytest-celery/en/latest/userguide/celery-bug-report.html) and added the reproduction script below.\r\n\r\n## Mandatory Debugging Information\r\n\r\n- [ ] I have included the output of ``celery -A proj report`` in the issue.\r\n    (if you are not able to do this, then at least specify the Celery\r\n     version affected).\r\n- [x] I have verified that the issue exists against the `main` branch of Celery.\r\n- [ ] I have included the contents of ``pip freeze`` in the issue.\r\n- [x] I have included all the versions of all the external dependencies required\r\n  to reproduce this bug.\r\n\r\n## Optional Debugging Information\r\n<!--\r\nTry some of the below if you think they are relevant.\r\nIt will help us figure out the scope of the bug and how many users it affects.\r\n-->\r\n- [ ] I have tried reproducing the issue on more than one Python version\r\n  and/or implementation.\r\n- [ ] I have tried reproducing the issue on more than one message broker and/or\r\n  result backend.\r\n- [ ] I have tried reproducing the issue on more than one version of the message\r\n  broker and/or result backend.\r\n- [ ] I have tried reproducing the issue on more than one operating system.\r\n- [ ] I have tried reproducing the issue on more than one workers pool.\r\n- [ ] I have tried reproducing the issue with autoscaling, retries,\r\n  ETA/Countdown & rate limits disabled.\r\n- [ ] I have tried reproducing the issue after downgrading\r\n  and/or upgrading Celery and its dependencies.\r\n\r\n## Related Issues and Possible Duplicates\r\n<!--\r\nPlease make sure to search and mention any related issues\r\nor possible duplicates to this issue as requested by the checklist above.\r\n\r\nThis may or may not include issues in other repositories that the Celery project\r\nmaintains or other repositories that are dependencies of Celery.\r\n\r\nIf you don't know how to mention issues, please refer to Github's documentation\r\non the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests\r\n-->\r\n\r\n#### Related Issues\r\n\r\n- None\r\n\r\n#### Possible Duplicates\r\n\r\n- None\r\n\r\n## Environment & Settings\r\n<!-- Include the contents of celery --version below -->\r\n**Celery version**: 5.3.1\r\n<!-- Include the output of celery -A proj report below -->\r\n<details>\r\n<summary><b><code>celery report</code> Output:</b></summary>\r\n<p>\r\n\r\n```\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n# Steps to Reproduce\r\n\r\n## Required Dependencies\r\n<!-- Please fill the required dependencies to reproduce this issue -->\r\n- **Minimal Python Version**: N/A\r\n- **Minimal Celery Version**: Unknown\r\n- **Minimal Kombu Version**: N/A\r\n- **Minimal Broker Version**: N/A\r\n- **Minimal Result Backend Version**: N/A\r\n- **Minimal OS and/or Kernel Version**: N/A\r\n- **Minimal Broker Client Version**: N/A\r\n- **Minimal Result Backend Client Version**: N/A\r\n\r\n### Python Packages\r\n<!-- Please fill the contents of pip freeze below -->\r\n<details>\r\n<summary><b><code>pip freeze</code> Output:</b></summary>\r\n<p>\r\n\r\n```\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n### Other Dependencies\r\n<!--\r\nPlease provide system dependencies, configuration files\r\nand other dependency information if applicable\r\n-->\r\n<details>\r\n<p>\r\nN/A\r\n</p>\r\n</details>\r\n\r\n## Minimally Reproducible Test Case\r\n<!--\r\nPlease provide a reproducible test case.\r\nRefer to the Reporting Bugs section in our contribution guide.\r\n\r\nWe prefer submitting test cases in the form of a PR to our integration test suite.\r\nIf you can provide one, please mention the PR number below.\r\nIf not, please attach the most minimal code example required to reproduce the issue below.\r\nIf the test case is too large, please include a link to a gist or a repository below.\r\n\r\nAlternatively, the pytest-celery plugin can be used to create standalone reproduction scripts\r\nthat can be added to this report. See the pytest-celery documentation for more information at\r\npytest-celery.readthedocs.io\r\n-->\r\n\r\n<details>\r\n<p>\r\n\r\n```python\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n# Expected Behavior\r\n\r\nI expect to be able to use `worker_process_init.disconnect(my_handler)` after using\r\n\r\n```python\r\n@worker_process_init.connect(retry=True):\r\ndef my_handler(**kw):\r\n    ...\r\n```\r\n\r\nMore specifically, I use this in a pytest fixture in my conftest.py to disable my_handler which normally sets up database connections and everything.  The disconnect worker fine until I added `retry=True` to the connect.\r\n\r\n<!-- Describe in detail what you expect to happen -->\r\n\r\n# Actual Behavior\r\n<!--\r\nDescribe in detail what actually happened.\r\nPlease include a backtrace and surround it with triple backticks (```).\r\nIn addition, include the Celery daemon logs, the broker logs,\r\nthe result backend logs and system logs below if they will help us debug\r\nthe issue.\r\n-->\r\n\r\nThe disconnection doesn't work, my_handler gets called in the middle of my test suite and loops, retrying forever.\r\n\r\nLooking closer at the problem I see that it's caused by https://github.com/celery/celery/blob/bd3b3c6afb8b0c44b416f469b3db6a725d71b609/celery/utils/dispatch/signal.py#L165-L172\r\n\r\nwhich replaces fun = my_handler with a wrapper that does the retrying, but also remembers the id() of the original function as dispatch_uid so that the original function can be disconnected later.\r\n\r\nBut I no longer have the original function -- the Python decorator syntax means `my_handler` is replaced with the return value of the decorator, which is set by `return fun` executed _after_ the `fun = _retry_receiver(fun)` statement.  So when I call `.disconnect()`, I'm passing the wrapper, and it doesn't match the dispatch_uid computed from the original function.\r\n\r\n(I've planned to create a PR with a failing test case and complete all the checklists before submitting the issue, but this tab's already open for a couple of days I'm afraid of losing what I've already written to a browser restart or something.  I'll try to come back and complete it.)\n", "hints_text": "> I've planned to create a PR with a failing test case\r\n\r\nI now have a [failing unit test in a branch](https://github.com/celery/celery/compare/main...mgedmin:celery:failing-test-for-issue-9119), but some time has passed and I'm uncertain about the next step in the process.  Should I create a PR with the failing test?", "created_at": "2024-09-10T00:17:48Z"}
{"repo": "celery/celery", "pull_number": 9228, "instance_id": "celery__celery-9228", "issue_numbers": ["9218"], "base_commit": "89ff576eb9ae4efceeb169b6e956fdb57794f331", "patch": "diff --git a/CONTRIBUTORS.txt b/CONTRIBUTORS.txt\nindex c86f3c1d559..39b73c8a38a 100644\n--- a/CONTRIBUTORS.txt\n+++ b/CONTRIBUTORS.txt\n@@ -301,4 +301,5 @@ Johannes Faigle, 2024/06/18\n Giovanni Giampauli, 2024/06/26\n Shamil Abdulaev, 2024/08/05\n Nikos Atlas, 2024/08/26\n+Marc Bresson, 2024/09/02\n Narasux, 2024/09/09\ndiff --git a/celery/app/defaults.py b/celery/app/defaults.py\nindex 34fbe94bcec..04bc1927944 100644\n--- a/celery/app/defaults.py\n+++ b/celery/app/defaults.py\n@@ -249,6 +249,7 @@ def __repr__(self):\n         ),\n         table_schemas=Option(type='dict'),\n         table_names=Option(type='dict', old={'celery_result_db_tablenames'}),\n+        create_tables_at_setup=Option(True, type='bool'),\n     ),\n     task=Namespace(\n         __old__=OLD_NS,\ndiff --git a/celery/backends/database/__init__.py b/celery/backends/database/__init__.py\nindex 91080adc46a..df03db56d38 100644\n--- a/celery/backends/database/__init__.py\n+++ b/celery/backends/database/__init__.py\n@@ -98,11 +98,23 @@ def __init__(self, dburi=None, engine_options=None, url=None, **kwargs):\n                 'Missing connection string! Do you have the'\n                 ' database_url setting set to a real value?')\n \n+        self.session_manager = SessionManager()\n+\n+        create_tables_at_setup = conf.database_create_tables_at_setup\n+        if create_tables_at_setup is True:\n+            self._create_tables()\n+\n     @property\n     def extended_result(self):\n         return self.app.conf.find_value_for_key('extended', 'result')\n \n-    def ResultSession(self, session_manager=SessionManager()):\n+    def _create_tables(self):\n+        \"\"\"Create the task and taskset tables.\"\"\"\n+        self.ResultSession()\n+\n+    def ResultSession(self, session_manager=None):\n+        if session_manager is None:\n+            session_manager = self.session_manager\n         return session_manager.session_factory(\n             dburi=self.url,\n             short_lived_sessions=self.short_lived_sessions,\ndiff --git a/docs/userguide/configuration.rst b/docs/userguide/configuration.rst\nindex 23b2974f34a..ab17540ae6b 100644\n--- a/docs/userguide/configuration.rst\n+++ b/docs/userguide/configuration.rst\n@@ -987,6 +987,23 @@ strings (this is the part of the URI that comes after the ``db+`` prefix).\n .. _`Connection String`:\n     http://www.sqlalchemy.org/docs/core/engines.html#database-urls\n \n+.. setting:: database_create_tables_at_setup\n+\n+``database_create_tables_at_setup``\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+.. versionadded:: 5.5.0\n+\n+Default: True by default.\n+\n+- If `True`, Celery will create the tables in the database during setup.\n+- If `False`, Celery will create the tables lazily, i.e. wait for the first task\n+  to be executed before creating the tables.\n+\n+.. note::\n+    Before celery 5.5, the tables were created lazily i.e. it was equivalent to\n+    `database_create_tables_at_setup` set to False.\n+\n .. setting:: database_engine_options\n \n ``database_engine_options``\n", "test_patch": "diff --git a/t/unit/backends/test_database.py b/t/unit/backends/test_database.py\nindex a693f383f67..328ee0c9c02 100644\n--- a/t/unit/backends/test_database.py\n+++ b/t/unit/backends/test_database.py\n@@ -1,3 +1,4 @@\n+import os\n from datetime import datetime\n from pickle import dumps, loads\n from unittest.mock import Mock, patch\n@@ -15,6 +16,8 @@\n from celery.backends.database.session import PREPARE_MODELS_MAX_RETRIES, ResultModelBase, SessionManager  # noqa\n from t import skip  # noqa\n \n+DB_PATH = \"test.db\"\n+\n \n class SomeClass:\n \n@@ -45,8 +48,14 @@ def test_context_raises(self):\n @skip.if_pypy\n class test_DatabaseBackend:\n \n+    @pytest.fixture(autouse=True)\n+    def remmove_db(self):\n+        yield\n+        if os.path.exists(DB_PATH):\n+            os.remove(DB_PATH)\n+\n     def setup_method(self):\n-        self.uri = 'sqlite:///test.db'\n+        self.uri = 'sqlite:///' + DB_PATH\n         self.app.conf.result_serializer = 'pickle'\n \n     def test_retry_helper(self):\n@@ -73,6 +82,9 @@ def test_table_schema_config(self):\n             'task': 'foo',\n             'group': 'bar',\n         }\n+        # disable table creation because schema foo and bar do not exist\n+        # and aren't created if they don't exist.\n+        self.app.conf.database_create_tables_at_setup = False\n         tb = DatabaseBackend(self.uri, app=self.app)\n         assert tb.task_cls.__table__.schema == 'foo'\n         assert tb.task_cls.__table__.c.id.default.schema == 'foo'\n@@ -88,6 +100,14 @@ def test_table_name_config(self):\n         assert tb.task_cls.__table__.name == 'foo'\n         assert tb.taskset_cls.__table__.name == 'bar'\n \n+    def test_table_creation_at_setup_config(self):\n+        from sqlalchemy import inspect\n+        self.app.conf.database_create_tables_at_setup = True\n+        tb = DatabaseBackend(self.uri, app=self.app)\n+        engine = tb.session_manager.get_engine(tb.url)\n+        inspect(engine).has_table(\"celery_taskmeta\")\n+        inspect(engine).has_table(\"celery_tasksetmeta\")\n+\n     def test_missing_task_id_is_PENDING(self):\n         tb = DatabaseBackend(self.uri, app=self.app)\n         assert tb.get_state('xxx-does-not-exist') == states.PENDING\n@@ -220,7 +240,7 @@ def test_TaskSet__repr__(self):\n @skip.if_pypy\n class test_DatabaseBackend_result_extended():\n     def setup_method(self):\n-        self.uri = 'sqlite:///test.db'\n+        self.uri = 'sqlite:///' + DB_PATH\n         self.app.conf.result_serializer = 'pickle'\n         self.app.conf.result_extended = True\n \n", "problem_statement": "Add option for database backends: create tables on startup\n<!--\r\nPlease fill this template entirely and do not erase parts of it.\r\nWe reserve the right to close without a response\r\nfeature requests which are incomplete.\r\n-->\r\n# Checklist\r\n<!--\r\nTo check an item on the list replace [ ] with [x].\r\n-->\r\n\r\n- [x] I have checked the [issues list](https://github.com/celery/celery/issues?utf8=%E2%9C%93&q=is%3Aissue+label%3A%22Issue+Type%3A+Feature+Request%22+)\r\n  for similar or identical feature requests.\r\n- [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?utf8=%E2%9C%93&q=is%3Apr+label%3A%22PR+Type%3A+Feature%22+)\r\n  for existing proposed implementations of this feature.\r\n- [x] I have checked the [commit log](https://github.com/celery/celery/commits/main)\r\n  to find out if the same feature was already implemented in the\r\n  main branch.\r\n- [x] I have included all related issues and possible duplicate issues\r\n  in this issue (If there are none, check this box anyway).\r\n\r\n## Related Issues and Possible Duplicates\r\n<!--\r\nPlease make sure to search and mention any related issues\r\nor possible duplicates to this issue as requested by the checklist above.\r\n\r\nThis may or may not include issues in other repositories that the Celery project\r\nmaintains or other repositories that are dependencies of Celery.\r\n\r\nIf you don't know how to mention issues, please refer to Github's documentation\r\non the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests\r\n-->\r\n\r\n#### Related Issues\r\n\r\n- https://github.com/celery/celery/issues/4653\r\n\r\n#### Possible Duplicates\r\n\r\n- None\r\n\r\n# Brief Summary\r\n<!--\r\nPlease include a brief summary of what the feature does\r\nand why it is needed.\r\n-->\r\n\r\nthis option called `database_create_tables_on_startup` would not wait for the first task before creating tables.\r\n\r\n@Alexis-Jacob proposed a [workaround](https://github.com/celery/celery/issues/4653#issuecomment-400029147) but I believe it would make sense to have it as an option.\r\n\r\nOur issue was that right after creating the first task, we query celery's database. But because celery did not have time to create the table yet, we have an error \r\n\r\n```\r\npsycopg2.errors.UndefinedTable: relation \"celery_taskmeta\" does not exist\r\n```\r\n\r\n# Design\r\n\r\n## Architectural Considerations\r\n<!--\r\nIf more components other than Celery are involved,\r\ndescribe them here and the effect it would have on Celery.\r\n-->\r\nNone\r\n\r\n## Proposed Behavior\r\n<!--\r\nPlease describe in detail how this feature is going to behave.\r\nDescribe what happens in case of failures as well if applicable.\r\n-->\r\n\r\nis `True` in the config, Celery would attempt to create the tables as soon as it is initialized. \r\n\r\n## Proposed UI/UX\r\n<!--\r\nPlease provide your ideas for the API, CLI options,\r\nconfiguration key names etc. that will be introduced for this feature.\r\n-->\r\n\r\nIt would be part of the [configuration of the backend](https://docs.celeryq.dev/en/stable/userguide/configuration.html#database-backend-settings)\r\n\r\n## Diagrams\r\n<!--\r\nPlease include any diagrams that might be relevant\r\nto the implementation of this feature such as:\r\n* Class Diagrams\r\n* Sequence Diagrams\r\n* Activity Diagrams\r\nYou can drag and drop images into the text box to attach them to this issue.\r\n-->\r\nN/A\r\n\r\n## Alternatives\r\n<!--\r\nIf you have considered any alternative implementations\r\ndescribe them in detail below.\r\n-->\r\n\r\nAs suggested in [this message](https://github.com/celery/celery/issues/4653#issuecomment-400029147):\r\n\r\n```python\r\nfrom celery.backends.database import SessionManager\r\n\r\napp = Celery('celery', broker=BROKER_URL, backend=BACKEND_URL)\r\n\r\nsession = SessionManager()\r\nengine = session.get_engine(app.backend.url)\r\nsession.prepare_models(engine)\r\n```\r\n\n", "hints_text": "", "created_at": "2024-09-02T14:33:28Z"}
{"repo": "celery/celery", "pull_number": 9227, "instance_id": "celery__celery-9227", "issue_numbers": ["9225"], "base_commit": "11aef56ee930ddf9b6b7ba7e943bea14276f6892", "patch": "diff --git a/celery/worker/consumer/consumer.py b/celery/worker/consumer/consumer.py\nindex 551dfd586a7..8241a976021 100644\n--- a/celery/worker/consumer/consumer.py\n+++ b/celery/worker/consumer/consumer.py\n@@ -505,13 +505,14 @@ def _error_handler(exc, interval, next_step=CONNECTION_RETRY_STEP):\n             # to determine whether connection retries are disabled.\n             retry_disabled = not self.app.conf.broker_connection_retry\n \n-            warnings.warn(\n-                CPendingDeprecationWarning(\n-                    f\"The broker_connection_retry configuration setting will no longer determine\\n\"\n-                    f\"whether broker connection retries are made during startup in Celery 6.0 and above.\\n\"\n-                    f\"If you wish to retain the existing behavior for retrying connections on startup,\\n\"\n-                    f\"you should set broker_connection_retry_on_startup to {self.app.conf.broker_connection_retry}.\")\n-            )\n+            if retry_disabled:\n+                warnings.warn(\n+                    CPendingDeprecationWarning(\n+                        \"The broker_connection_retry configuration setting will no longer determine\\n\"\n+                        \"whether broker connection retries are made during startup in Celery 6.0 and above.\\n\"\n+                        \"If you wish to refrain from retrying connections on startup,\\n\"\n+                        \"you should set broker_connection_retry_on_startup to False instead.\")\n+                )\n         else:\n             if self.first_connection_attempt:\n                 retry_disabled = not self.app.conf.broker_connection_retry_on_startup\n", "test_patch": "diff --git a/t/unit/worker/test_consumer.py b/t/unit/worker/test_consumer.py\nindex 23933050780..a4c8ac6b196 100644\n--- a/t/unit/worker/test_consumer.py\n+++ b/t/unit/worker/test_consumer.py\n@@ -478,12 +478,12 @@ def test_ensure_connected(self, subtests, broker_connection_retry, broker_connec\n         c.app.conf.broker_connection_retry_on_startup = broker_connection_retry_on_startup\n         c.app.conf.broker_connection_retry = broker_connection_retry\n \n-        if broker_connection_retry_on_startup is None:\n-            with subtests.test(\"Deprecation warning when startup is None\"):\n-                with pytest.deprecated_call():\n-                    c.ensure_connected(Mock())\n-\n         if broker_connection_retry is False:\n+            if broker_connection_retry_on_startup is None:\n+                with subtests.test(\"Deprecation warning when startup is None\"):\n+                    with pytest.deprecated_call():\n+                        c.ensure_connected(Mock())\n+\n             with subtests.test(\"Does not retry when connect throws an error and retry is set to false\"):\n                 conn = Mock()\n                 conn.connect.side_effect = ConnectionError()\n", "problem_statement": "Confusing warning about `broker_connection_retry_on_startup`\n<!--\r\nPlease fill this template entirely and do not erase parts of it.\r\nWe reserve the right to close without a response\r\nbug reports which are incomplete.\r\n-->\r\n# Checklist\r\n<!--\r\nTo check an item on the list replace [ ] with [x].\r\n-->\r\n\r\n- [x] I have checked the [issues list](https://github.com/celery/celery/issues?utf8=%E2%9C%93&q=is%3Aissue+label%3A%22Category%3A+Documentation%22+)\r\n  for similar or identical bug reports.\r\n- [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22Category%3A+Documentation%22)\r\n  for existing proposed fixes.\r\n- [x] I have checked the [commit log](https://github.com/celery/celery/commits/main)\r\n  to find out if the bug was already fixed in the main branch.\r\n- [x] I have included all related issues and possible duplicate issues in this issue\r\n      (If there are none, check this box anyway).\r\n\r\n## Related Issues and Possible Duplicates\r\n<!--\r\nPlease make sure to search and mention any related issues\r\nor possible duplicates to this issue as requested by the checklist above.\r\n\r\nThis may or may not include issues in other repositories that the Celery project\r\nmaintains or other repositories that are dependencies of Celery.\r\n\r\nIf you don't know how to mention issues, please refer to Github's documentation\r\non the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests\r\n-->\r\n\r\n#### Related Issues\r\n\r\n- None\r\n\r\n#### Possible Duplicates\r\n\r\n- None\r\n\r\n# Description\r\n<!--\r\nPlease describe what's missing or incorrect about our documentation.\r\nInclude links and/or screenshots which will aid us to resolve the issue.\r\n-->\r\n\r\nWe just update Celery and we get this warning:\r\n\r\n```\r\npython-3.11/lib/python3.11/site-packages/celery/worker/consumer/consumer.py:508: \r\nCPendingDeprecationWarning: The broker_connection_retry configuration setting will no longer determine\r\nwhether broker connection retries are made during startup in Celery 6.0 and above.\r\nIf you wish to retain the existing behavior for retrying connections on startup,\r\nyou should set broker_connection_retry_on_startup to True.\r\n```\r\n\r\nSince in our configuration we didn't touch either (`broker_connection_retry` nor `broker_connection_retry_on_startup`), it took me some time to understand what is going on.\r\n\r\nAlso, from the message it looks like that the default for `broker_connection_retry_on_startup` will be `False`, but the [docs](https://docs.celeryq.dev/en/stable/userguide/configuration.html#broker-connection-retry-on-startup) specify that's enabled by default.\r\n\r\n# Suggestions\r\n<!-- Please provide us suggestions for how to fix the documentation -->\r\n\r\nAssuming that the default in Celery 6 will be to retry every time (both `True` by default), I think that the deprecation warning should not appear if `broker_connection_retry is True`, as no change would result from the deprecation going into action.\r\n\r\nTo recap, that's my proposition:\r\n- `broker_connection_retry` unset or `True`, `broker_connection_retry_on_startup` unset: no warning (everything evaluated as `True`) <= suggested change\r\n- `broker_connection_retry=False`, `broker_connection_retry_on_startup` unset: warning as it is now\r\n- both `broker_connection_retry` and `broker_connection_retry_on_startup` set: no warning, as it is now\r\n\r\nLet me know what are your thoughts, I can prepare a little PR if you agree.\n", "hints_text": "@thedrow ", "created_at": "2024-09-02T13:04:32Z"}
{"repo": "celery/celery", "pull_number": 9173, "instance_id": "celery__celery-9173", "issue_numbers": ["9125"], "base_commit": "498166793338e6b8bb62594a5d41e80252cffb3c", "patch": "diff --git a/CONTRIBUTORS.txt b/CONTRIBUTORS.txt\nindex 9c3534b3358..f6494360eeb 100644\n--- a/CONTRIBUTORS.txt\n+++ b/CONTRIBUTORS.txt\n@@ -299,3 +299,4 @@ Tomer Nosrati, 2022/17/07\n Andy Zickler, 2024/01/18\n Johannes Faigle, 2024/06/18\n Giovanni Giampauli, 2024/06/26\n+Shamil Abdulaev, 2024/08/05\ndiff --git a/celery/app/task.py b/celery/app/task.py\nindex 5d55a747b8c..78624655c4e 100644\n--- a/celery/app/task.py\n+++ b/celery/app/task.py\n@@ -543,6 +543,8 @@ def apply_async(self, args=None, kwargs=None, task_id=None, producer=None,\n             TypeError: If not enough arguments are passed, or too many\n                 arguments are passed.  Note that signature checks may\n                 be disabled by specifying ``@task(typing=False)``.\n+            ValueError: If soft_time_limit and time_limit are set,\n+                and soft_time_limit is less than time_limit\n             kombu.exceptions.OperationalError: If a connection to the\n                transport cannot be made, or if the connection is lost.\n \n@@ -550,6 +552,9 @@ def apply_async(self, args=None, kwargs=None, task_id=None, producer=None,\n             Also supports all keyword arguments supported by\n             :meth:`kombu.Producer.publish`.\n         \"\"\"\n+        if self.soft_time_limit and self.time_limit and self.soft_time_limit > self.time_limit:\n+            raise ValueError('soft_time_limit must be greater than or equal to time_limit')\n+\n         if self.typing:\n             try:\n                 check_arguments = self.__header__\ndiff --git a/t/integration/tasks.py b/t/integration/tasks.py\nindex 752db0278c3..227e3cb2917 100644\n--- a/t/integration/tasks.py\n+++ b/t/integration/tasks.py\n@@ -512,3 +512,8 @@ def replace_with_stamped_task(self: StampedTaskOnReplace, replace_with=None):\n         if replace_with is None:\n             replace_with = replaced_with_me.s()\n         self.replace(signature(replace_with))\n+\n+\n+@shared_task(soft_time_limit=2, time_limit=1)\n+def soft_time_limit_must_exceed_time_limit():\n+    pass\ndiff --git a/t/smoke/tasks.py b/t/smoke/tasks.py\nindex 6314dd11865..8250c650bca 100644\n--- a/t/smoke/tasks.py\n+++ b/t/smoke/tasks.py\n@@ -38,6 +38,16 @@ def long_running_task(seconds: float = 1, verbose: bool = False) -> bool:\n     return True\n \n \n+@shared_task(soft_time_limit=3, time_limit=5)\n+def soft_time_limit_lower_than_time_limit():\n+    sleep(4)\n+\n+\n+@shared_task(soft_time_limit=5, time_limit=3)\n+def soft_time_limit_must_exceed_time_limit():\n+    pass\n+\n+\n @shared_task(bind=True)\n def replace_with_task(self: Task, replace_with: Signature = None):\n     if replace_with is None:\n", "test_patch": "diff --git a/t/integration/test_tasks.py b/t/integration/test_tasks.py\nindex 060176e8b15..c6fc7476687 100644\n--- a/t/integration/test_tasks.py\n+++ b/t/integration/test_tasks.py\n@@ -18,7 +18,8 @@\n from .conftest import TEST_BACKEND, get_active_redis_channels, get_redis_connection\n from .tasks import (ClassBasedAutoRetryTask, ExpectedException, add, add_ignore_result, add_not_typed, add_pydantic,\n                     fail, fail_unpickleable, print_unicode, retry, retry_once, retry_once_headers,\n-                    retry_once_priority, retry_unpickleable, return_properties, second_order_replace1, sleeping)\n+                    retry_once_priority, retry_unpickleable, return_properties, second_order_replace1, sleeping,\n+                    soft_time_limit_must_exceed_time_limit)\n \n TIMEOUT = 10\n \n@@ -473,6 +474,15 @@ def test_properties(self, celery_session_worker):\n         res = return_properties.apply_async(app_id=\"1234\")\n         assert res.get(timeout=TIMEOUT)[\"app_id\"] == \"1234\"\n \n+    @flaky\n+    def test_soft_time_limit_exceeding_time_limit(self):\n+\n+        with pytest.raises(ValueError, match='soft_time_limit must be greater than or equal to time_limit'):\n+            result = soft_time_limit_must_exceed_time_limit.apply_async()\n+            result.get(timeout=5)\n+\n+            assert result.status == 'FAILURE'\n+\n \n class test_trace_log_arguments:\n     args = \"CUSTOM ARGS\"\ndiff --git a/t/smoke/tests/test_tasks.py b/t/smoke/tests/test_tasks.py\nindex e55a4b41f30..1878687ecca 100644\n--- a/t/smoke/tests/test_tasks.py\n+++ b/t/smoke/tests/test_tasks.py\n@@ -5,10 +5,11 @@\n from tenacity import retry, stop_after_attempt, wait_fixed\n \n from celery import Celery, signature\n-from celery.exceptions import TimeLimitExceeded, WorkerLostError\n+from celery.exceptions import SoftTimeLimitExceeded, TimeLimitExceeded, WorkerLostError\n from t.integration.tasks import add, identity\n from t.smoke.conftest import SuiteOperations, TaskTermination\n-from t.smoke.tasks import replace_with_task\n+from t.smoke.tasks import (replace_with_task, soft_time_limit_lower_than_time_limit,\n+                           soft_time_limit_must_exceed_time_limit)\n \n \n class test_task_termination(SuiteOperations):\n@@ -54,9 +55,7 @@ def wait_for_two_celery_processes():\n                 filters={\"name\": \"celery\"},\n             )\n             if len(pinfo_current) != 2:\n-                assert (\n-                    False\n-                ), f\"Child process did not respawn with method: {method.name}\"\n+                assert False, f\"Child process did not respawn with method: {method.name}\"\n \n         wait_for_two_celery_processes()\n \n@@ -85,7 +84,7 @@ def wait_for_two_celery_processes():\n             (\n                 TaskTermination.Method.DELAY_TIMEOUT,\n                 \"Hard time limit (2s) exceeded for t.smoke.tasks.self_termination_delay_timeout\",\n-                'TimeLimitExceeded(2,)',\n+                \"TimeLimitExceeded(2,)\",\n             ),\n             (\n                 TaskTermination.Method.EXHAUST_MEMORY,\n@@ -130,3 +129,16 @@ def test_sanity(self, celery_setup: CeleryTestSetup):\n         c = sig1 | sig2\n         r = c.apply_async(queue=queues[0])\n         assert r.get(timeout=RESULT_TIMEOUT) == 42\n+\n+\n+class test_time_limit:\n+    def test_soft_time_limit_lower_than_time_limit(self, celery_setup: CeleryTestSetup):\n+        sig = soft_time_limit_lower_than_time_limit.s()\n+        result = sig.apply_async(queue=celery_setup.worker.worker_queue)\n+        with pytest.raises(SoftTimeLimitExceeded):\n+            result.get(timeout=RESULT_TIMEOUT) is None\n+\n+    def test_soft_time_limit_must_exceed_time_limit(self, celery_setup: CeleryTestSetup):\n+        sig = soft_time_limit_must_exceed_time_limit.s()\n+        with pytest.raises(ValueError, match=\"soft_time_limit must be greater than or equal to time_limit\"):\n+            sig.apply_async(queue=celery_setup.worker.worker_queue)\ndiff --git a/t/unit/tasks/test_tasks.py b/t/unit/tasks/test_tasks.py\nindex 10a373ef54b..7d84f108de3 100644\n--- a/t/unit/tasks/test_tasks.py\n+++ b/t/unit/tasks/test_tasks.py\n@@ -1410,6 +1410,19 @@ def yyy5(self):\n \n         self.app.send_task = old_send_task\n \n+    def test_soft_time_limit_failure(self):\n+        @self.app.task(soft_time_limit=5, time_limit=3)\n+        def yyy():\n+            pass\n+\n+        try:\n+            yyy_result = yyy.apply_async()\n+            yyy_result.get(timeout=5)\n+\n+            assert yyy_result.state == 'FAILURE'\n+        except ValueError as e:\n+            assert str(e) == 'soft_time_limit must be greater than or equal to time_limit'\n+\n \n class test_apply_task(TasksCase):\n \n", "problem_statement": "`time_limit` is not respected if it is shorter than `soft_time_limit`\n<!--\r\nPlease fill this template entirely and do not erase parts of it.\r\nWe reserve the right to close without a response\r\nbug reports which are incomplete.\r\n-->\r\n# Checklist\r\n<!--\r\nTo check an item on the list replace [ ] with [x].\r\n-->\r\n- [x] I have verified that the issue exists against the `main` branch of Celery.\r\n- [ ] This has already been asked to the [discussions forum](https://github.com/celery/celery/discussions) first.\r\n- [x] I have read the relevant section in the\r\n  [contribution guide](https://docs.celeryq.dev/en/main/contributing.html#other-bugs)\r\n  on reporting bugs.\r\n- [x] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Bug+Report%22+-label%3A%22Category%3A+Documentation%22)\r\n  for similar or identical bug reports.\r\n- [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22PR+Type%3A+Bugfix%22+-label%3A%22Category%3A+Documentation%22)\r\n  for existing proposed fixes.\r\n- [x] I have checked the [commit log](https://github.com/celery/celery/commits/main)\r\n  to find out if the bug was already fixed in the main branch.\r\n- [x] I have included all related issues and possible duplicate issues\r\n  in this issue (If there are none, check this box anyway).\r\n- [x] I have tried to reproduce the issue with [pytest-celery](https://docs.celeryq.dev/projects/pytest-celery/en/latest/userguide/celery-bug-report.html) and added the reproduction script below.\r\n\r\n## Mandatory Debugging Information\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n    (if you are not able to do this, then at least specify the Celery\r\n     version affected).\r\n- [x] I have verified that the issue exists against the `main` branch of Celery.\r\n- [x] I have included the contents of ``pip freeze`` in the issue.\r\n- [x] I have included all the versions of all the external dependencies required\r\n  to reproduce this bug.\r\n\r\n## Optional Debugging Information\r\n<!--\r\nTry some of the below if you think they are relevant.\r\nIt will help us figure out the scope of the bug and how many users it affects.\r\n-->\r\n- [x] I have tried reproducing the issue on more than one Python version\r\n  and/or implementation.\r\n- [x] I have tried reproducing the issue on more than one message broker and/or\r\n  result backend.\r\n- [x] I have tried reproducing the issue on more than one version of the message\r\n  broker and/or result backend.\r\n- [x] I have tried reproducing the issue on more than one operating system.\r\n- [x] I have tried reproducing the issue on more than one workers pool.\r\n- [x] I have tried reproducing the issue with autoscaling, retries,\r\n  ETA/Countdown & rate limits disabled.\r\n- [x] I have tried reproducing the issue after downgrading\r\n  and/or upgrading Celery and its dependencies.\r\n\r\n## Related Issues and Possible Duplicates\r\n<!--\r\nPlease make sure to search and mention any related issues\r\nor possible duplicates to this issue as requested by the checklist above.\r\n\r\nThis may or may not include issues in other repositories that the Celery project\r\nmaintains or other repositories that are dependencies of Celery.\r\n\r\nIf you don't know how to mention issues, please refer to Github's documentation\r\non the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests\r\n-->\r\n\r\n#### Related Issues\r\n\r\n- None\r\n\r\n#### Possible Duplicates\r\n\r\n- None\r\n\r\n## Environment & Settings\r\n<!-- Include the contents of celery --version below -->\r\n**Celery version**:\r\n<!-- Include the output of celery -A proj report below -->\r\n<details>\r\n<summary><b><code>celery report</code> Output:</b></summary>\r\n<p>\r\n\r\n```\r\npoetry run celery --version\r\n5.4.0 (opalescent)\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n# Steps to Reproduce\r\n\r\n## Required Dependencies\r\n<!-- Please fill the required dependencies to reproduce this issue -->\r\n- **Minimal Python Version**: Observed with Python 3.11 and 3.12, using prefork worker and/or celery-pytest\r\n- **Minimal Celery Version**: N/A or Unknown\r\n- **Minimal Kombu Version**: N/A or Unknown\r\n- **Minimal Broker Version**: Observed with SQS, Redis and RabbitMQ\r\n- **Minimal Result Backend Version**: N/A or Unknown\r\n- **Minimal OS and/or Kernel Version**: Observed on Linux and MacOS\r\n- **Minimal Broker Client Version**: N/A or Unknown\r\n- **Minimal Result Backend Client Version**: N/A or Unknown\r\n\r\n### Python Packages\r\n<!-- Please fill the contents of pip freeze below -->\r\n<details>\r\n<summary><b><code>pip freeze</code> Output:</b></summary>\r\n<p>\r\n\r\n```\r\namqp==5.2.0\r\nbilliard==4.2.0\r\ncelery==5.4.0\r\ncertifi==2024.7.4\r\ncharset-normalizer==3.3.2\r\nclick==8.1.7\r\nclick-didyoumean==0.3.1\r\nclick-plugins==1.1.1\r\nclick-repl==0.3.0\r\ndebugpy==1.8.2\r\ndecorator==5.1.1\r\ndocker==7.1.0\r\nidna==3.7\r\niniconfig==2.0.0\r\nkombu==5.3.7\r\npackaging==24.1\r\npluggy==1.5.0\r\nprompt_toolkit==3.0.47\r\npsutil==6.0.0\r\npy==1.11.0\r\npytest==8.2.2\r\npytest-celery==1.0.0\r\npytest-docker-tools==3.1.3\r\npython-dateutil==2.9.0.post0\r\nredis==5.0.7\r\nrequests==2.32.3\r\nretry==0.9.2\r\nsix==1.16.0\r\ntzdata==2024.1\r\nurllib3==2.2.2\r\nvine==5.1.0\r\nwcwidth==0.2.13\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n### Other Dependencies\r\n<!--\r\nPlease provide system dependencies, configuration files\r\nand other dependency information if applicable\r\n-->\r\n<details>\r\n<p>\r\nN/A\r\n</p>\r\n</details>\r\n\r\n## Minimally Reproducible Test Case\r\n<!--\r\nPlease provide a reproducible test case.\r\nRefer to the Reporting Bugs section in our contribution guide.\r\n\r\nWe prefer submitting test cases in the form of a PR to our integration test suite.\r\nIf you can provide one, please mention the PR number below.\r\nIf not, please attach the most minimal code example required to reproduce the issue below.\r\nIf the test case is too large, please include a link to a gist or a repository below.\r\n\r\nAlternatively, the pytest-celery plugin can be used to create standalone reproduction scripts\r\nthat can be added to this report. See the pytest-celery documentation for more information at\r\npytest-celery.readthedocs.io\r\n-->\r\n\r\n\r\n```python\r\nfrom celery import shared_task\r\nimport time\r\n\r\n@shared_task(time_limit=5, soft_time_limit=15)\r\ndef long_task(*args, **kwargs) -> None:\r\n    time.sleep(10)\r\n    return \"OK\"\r\n```\r\n\r\n```python\r\nimport pytest \r\n\r\n@pytest.fixture\r\ndef default_worker_tasks(default_worker_tasks: set) -> set:\r\n    import tasks\r\n\r\n    default_worker_tasks.add(tasks)\r\n    return default_worker_tasks\r\n\r\ndef test_hello_world(celery_setup):\r\n    from tasks import long_task\r\n\r\n    assert long_task.s().apply_async().get() != \"OK\"\r\n```\r\n\r\n\r\n# Expected Behavior\r\n<!-- Describe in detail what you expect to happen -->\r\n\r\nTask is aborted with `TimeLimitExceeded` as `time_limit` was exceeded.\r\n\r\nWe discovered this behaviour due to a misconfiguration in our application. An alternative to raising `TimeLimitExceeded` could be that user is informed that the task has been improperly configured.\r\n\r\n# Actual Behavior\r\n<!--\r\nDescribe in detail what actually happened.\r\nPlease include a backtrace and surround it with triple backticks (```).\r\nIn addition, include the Celery daemon logs, the broker logs,\r\nthe result backend logs and system logs below if they will help us debug\r\nthe issue.\r\n-->\r\n\r\nTask exits successfully despite `time_limit` being exceeded.\r\n\n", "hints_text": "Can i work on this issue??", "created_at": "2024-08-04T17:22:44Z"}
{"repo": "celery/celery", "pull_number": 9159, "instance_id": "celery__celery-9159", "issue_numbers": ["9157"], "base_commit": "5e26553219da10f65f14d59573de18b5e366a693", "patch": "diff --git a/setup.py b/setup.py\nindex aef46a1a15f..324f6c0e607 100755\n--- a/setup.py\n+++ b/setup.py\n@@ -4,7 +4,6 @@\n import re\n \n import setuptools\n-import setuptools.command.test\n \n NAME = 'celery'\n \n", "test_patch": "", "problem_statement": "`setuptools` >= v72 removes `setuptools.command.test`\nhttps://setuptools.pypa.io/en/stable/history.html#v72-0-0\r\n\r\nhttps://github.com/celery/celery/blob/5e26553219da10f65f14d59573de18b5e366a693/setup.py#L7\r\n* pypa/setuptools#931\r\n    * pypa/setuptools#4458\r\n* celery/django-celery-beat#771\r\n* celery/django-celery-beat#772\r\n\n", "hints_text": "That import is also completely useless since currently in there are no other references to the test command of setuptools.\r\n\r\nThe usage of such element was removed in 1baca0ca90b5bd7f38e3d2ae2d513f24cc0613ea leaving just the unused import.", "created_at": "2024-07-29T10:01:54Z"}
{"repo": "celery/celery", "pull_number": 9121, "instance_id": "celery__celery-9121", "issue_numbers": ["6067"], "base_commit": "bf88b74edb17a389c4d2fb796a2947ff4f6abed2", "patch": "diff --git a/celery/app/amqp.py b/celery/app/amqp.py\nindex e6aae3f8b3c..575117d13e1 100644\n--- a/celery/app/amqp.py\n+++ b/celery/app/amqp.py\n@@ -249,9 +249,13 @@ def Queues(self, queues, create_missing=None,\n         if max_priority is None:\n             max_priority = conf.task_queue_max_priority\n         if not queues and conf.task_default_queue:\n+            queue_arguments = None\n+            if conf.task_default_queue_type == 'quorum':\n+                queue_arguments = {'x-queue-type': 'quorum'}\n             queues = (Queue(conf.task_default_queue,\n                             exchange=self.default_exchange,\n-                            routing_key=default_routing_key),)\n+                            routing_key=default_routing_key,\n+                            queue_arguments=queue_arguments),)\n         autoexchange = (self.autoexchange if autoexchange is None\n                         else autoexchange)\n         return self.queues_cls(\ndiff --git a/celery/app/defaults.py b/celery/app/defaults.py\nindex 523b56d72f6..b9aaf66ef65 100644\n--- a/celery/app/defaults.py\n+++ b/celery/app/defaults.py\n@@ -261,6 +261,7 @@ def __repr__(self):\n         inherit_parent_priority=Option(False, type='bool'),\n         default_delivery_mode=Option(2, type='string'),\n         default_queue=Option('celery'),\n+        default_queue_type=Option('classic', type='string'),\n         default_exchange=Option(None, type='string'),  # taken from queue\n         default_exchange_type=Option('direct'),\n         default_routing_key=Option(None, type='string'),  # taken from queue\n@@ -345,6 +346,7 @@ def __repr__(self):\n         task_log_format=Option(DEFAULT_TASK_LOG_FMT),\n         timer=Option(type='string'),\n         timer_precision=Option(1.0, type='float'),\n+        detect_quorum_queues=Option(True, type='bool'),\n     ),\n )\n \ndiff --git a/celery/worker/consumer/tasks.py b/celery/worker/consumer/tasks.py\nindex b4e4aee99ec..12f9b6a33b5 100644\n--- a/celery/worker/consumer/tasks.py\n+++ b/celery/worker/consumer/tasks.py\n@@ -1,7 +1,13 @@\n \"\"\"Worker Task Consumer Bootstep.\"\"\"\n+\n+from __future__ import annotations\n+\n+import warnings\n+\n from kombu.common import QoS, ignore_errors\n \n from celery import bootsteps\n+from celery.exceptions import CeleryWarning\n from celery.utils.log import get_logger\n \n from .mingle import Mingle\n@@ -12,6 +18,16 @@\n debug = logger.debug\n \n \n+ETA_TASKS_NO_GLOBAL_QOS_WARNING = \"\"\"\n+Detected quorum queue \"%r\", disabling global QoS.\n+With global QoS disabled, ETA tasks may not function as expected. Instead of adjusting\n+the prefetch count dynamically, ETA tasks will occupy the prefetch buffer, potentially\n+blocking other tasks from being consumed. To mitigate this, either set a high prefetch\n+count or avoid using quorum queues until the ETA mechanism is updated to support a\n+disabled global QoS, which is required for quorum queues.\n+\"\"\"\n+\n+\n class Tasks(bootsteps.StartStopStep):\n     \"\"\"Bootstep starting the task message consumer.\"\"\"\n \n@@ -25,10 +41,7 @@ def start(self, c):\n         \"\"\"Start task consumer.\"\"\"\n         c.update_strategies()\n \n-        # - RabbitMQ 3.3 completely redefines how basic_qos works...\n-        # This will detect if the new qos semantics is in effect,\n-        # and if so make sure the 'apply_global' flag is set on qos updates.\n-        qos_global = not c.connection.qos_semantics_matches_spec\n+        qos_global = self.qos_global(c)\n \n         # set initial prefetch count\n         c.connection.default_channel.basic_qos(\n@@ -63,3 +76,44 @@ def shutdown(self, c):\n     def info(self, c):\n         \"\"\"Return task consumer info.\"\"\"\n         return {'prefetch_count': c.qos.value if c.qos else 'N/A'}\n+\n+    def qos_global(self, c) -> bool:\n+        \"\"\"Determine if global QoS should be applied.\n+\n+        Additional information:\n+            https://www.rabbitmq.com/docs/consumer-prefetch\n+            https://www.rabbitmq.com/docs/quorum-queues#global-qos\n+        \"\"\"\n+        # - RabbitMQ 3.3 completely redefines how basic_qos works...\n+        # This will detect if the new qos semantics is in effect,\n+        # and if so make sure the 'apply_global' flag is set on qos updates.\n+        qos_global = not c.connection.qos_semantics_matches_spec\n+\n+        if c.app.conf.worker_detect_quorum_queues:\n+            using_quorum_queues, qname = self.detect_quorum_queues(c)\n+            if using_quorum_queues:\n+                qos_global = False\n+                # The ETA tasks mechanism requires additional work for Celery to fully support\n+                # quorum queues. Warn the user that ETA tasks may not function as expected until\n+                # this is done so we can at least support quorum queues partially for now.\n+                warnings.warn(ETA_TASKS_NO_GLOBAL_QOS_WARNING % (qname,), CeleryWarning)\n+\n+        return qos_global\n+\n+    def detect_quorum_queues(self, c) -> tuple[bool, str]:\n+        \"\"\"Detect if any of the queues are quorum queues.\n+\n+        Returns:\n+            tuple[bool, str]: A tuple containing a boolean indicating if any of the queues are quorum queues\n+            and the name of the first quorum queue found or an empty string if no quorum queues were found.\n+        \"\"\"\n+        is_rabbitmq_broker = c.app.conf.broker_url.startswith((\"amqp\", \"pyamqp\"))\n+\n+        if is_rabbitmq_broker:\n+            queues = c.app.amqp.queues\n+            for qname in queues:\n+                qarguments = queues[qname].queue_arguments or {}\n+                if qarguments.get(\"x-queue-type\") == \"quorum\":\n+                    return True, qname\n+\n+        return False, \"\"\ndiff --git a/docs/userguide/configuration.rst b/docs/userguide/configuration.rst\nindex f5c3f280aa4..1250f4ff16e 100644\n--- a/docs/userguide/configuration.rst\n+++ b/docs/userguide/configuration.rst\n@@ -137,6 +137,7 @@ have been moved into a new  ``task_`` prefix.\n ``CELERY_DEFAULT_EXCHANGE``                :setting:`task_default_exchange`\n ``CELERY_DEFAULT_EXCHANGE_TYPE``           :setting:`task_default_exchange_type`\n ``CELERY_DEFAULT_QUEUE``                   :setting:`task_default_queue`\n+``CELERY_DEFAULT_QUEUE_TYPE``              :setting:`task_default_queue_type`\n ``CELERY_DEFAULT_RATE_LIMIT``              :setting:`task_default_rate_limit`\n ``CELERY_DEFAULT_ROUTING_KEY``             :setting:`task_default_routing_key`\n ``CELERY_EAGER_PROPAGATES``                :setting:`task_eager_propagates`\n@@ -176,6 +177,7 @@ have been moved into a new  ``task_`` prefix.\n ``CELERY_WORKER_TASK_LOG_FORMAT``          :setting:`worker_task_log_format`\n ``CELERYD_TIMER``                          :setting:`worker_timer`\n ``CELERYD_TIMER_PRECISION``                :setting:`worker_timer_precision`\n+``CELERYD_DETECT_QUORUM_QUEUES``           :setting:`worker_detect_quorum_queues`\n ========================================== ==============================================\n \n Configuration Directives\n@@ -2606,6 +2608,42 @@ that queue.\n \n     :ref:`routing-changing-default-queue`\n \n+.. setting:: task_default_queue_type\n+\n+``task_default_queue_type``\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+.. versionadded:: 5.5\n+\n+Default: ``\"classic\"``.\n+\n+This setting is used to allow changing the default queue type for the\n+:setting:`task_default_queue` queue. The other viable option is ``\"quorum\"`` which\n+is only supported by RabbitMQ and sets the queue type to ``quorum`` using the ``x-queue-type``\n+queue argument.\n+\n+If the :setting:`worker_detect_quorum_queues` setting is enabled, the worker will\n+automatically detect the queue type and disable the global QoS accordingly.\n+\n+.. warning::\n+\n+    When using quorum queues, ETA tasks may not function as expected. Instead of adjusting\n+    the prefetch count dynamically, ETA tasks will occupy the prefetch buffer, potentially\n+    blocking other tasks from being consumed. To mitigate this, either set a high prefetch\n+    count or avoid using quorum queues until the ETA mechanism is updated to support a\n+    disabled global QoS, which is required for quorum queues.\n+\n+.. warning::\n+\n+    Quorum queues require confirm publish to be enabled.\n+    Use :setting:`broker_transport_options` to enable confirm publish by setting:\n+\n+    .. code-block:: python\n+\n+        broker_transport_options = {\"confirm_publish\": True}\n+\n+    For more information, see `RabbitMQ documentation <https://www.rabbitmq.com/docs/quorum-queues#use-cases>`_.\n+\n .. setting:: task_default_exchange\n \n ``task_default_exchange``\n@@ -3225,6 +3263,18 @@ are recorded as such in the result backend as long as :setting:`task_ignore_resu\n     will be set to ``True`` by default as the current behavior leads to more\n     problems than it solves.\n \n+.. setting:: worker_detect_quorum_queues\n+\n+``worker_detect_quorum_queues``\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+.. versionadded:: 5.5\n+\n+Default: Enabled.\n+\n+Automatically detect if any of the queues in :setting:`task_queues` are quorum queues\n+(including the :setting:`task_default_queue`) and disable the global QoS if any quorum queue is detected.\n+\n .. _conf-events:\n \n Events\ndiff --git a/examples/quorum-queues/declare_queue.py b/examples/quorum-queues/declare_queue.py\nnew file mode 100755\nindex 00000000000..4eaff0b88cb\n--- /dev/null\n+++ b/examples/quorum-queues/declare_queue.py\n@@ -0,0 +1,15 @@\n+\"\"\"Create a quorum queue using Kombu.\"\"\"\n+\n+from kombu import Connection, Exchange, Queue\n+\n+my_quorum_queue = Queue(\n+    \"my-quorum-queue\",\n+    Exchange(\"default\"),\n+    routing_key=\"default\",\n+    queue_arguments={\"x-queue-type\": \"quorum\"},\n+)\n+\n+with Connection(\"amqp://guest@localhost//\") as conn:\n+    channel = conn.channel()\n+    my_quorum_queue.maybe_bind(conn)\n+    my_quorum_queue.declare()\ndiff --git a/examples/quorum-queues/myapp.py b/examples/quorum-queues/myapp.py\nnew file mode 100644\nindex 00000000000..41698f3ce0f\n--- /dev/null\n+++ b/examples/quorum-queues/myapp.py\n@@ -0,0 +1,149 @@\n+\"\"\"myapp.py\n+\n+Usage::\n+\n+   (window1)$ python myapp.py worker -l INFO\n+\n+   (window2)$ celery shell\n+   >>> from myapp import example\n+   >>> example()\n+\n+\n+You can also specify the app to use with the `celery` command,\n+using the `-A` / `--app` option::\n+\n+    $ celery -A myapp worker -l INFO\n+\n+With the `-A myproj` argument the program will search for an app\n+instance in the module ``myproj``.  You can also specify an explicit\n+name using the fully qualified form::\n+\n+    $ celery -A myapp:app worker -l INFO\n+\n+\"\"\"\n+\n+import os\n+from datetime import UTC, datetime, timedelta\n+\n+from declare_queue import my_quorum_queue\n+\n+from celery import Celery\n+from celery.canvas import group\n+\n+app = Celery(\"myapp\", broker=\"amqp://guest@localhost//\")\n+\n+# Use custom queue (Optional) or set the default queue type to \"quorum\"\n+# app.conf.task_queues = (my_quorum_queue,)  # uncomment to use custom queue\n+app.conf.task_default_queue_type = \"quorum\"  # comment to use classic queue\n+\n+# Required by Quorum Queues: https://www.rabbitmq.com/docs/quorum-queues#use-cases\n+app.conf.broker_transport_options = {\"confirm_publish\": True}\n+\n+# Reduce qos to 4 (Optional, useful for testing)\n+app.conf.worker_prefetch_multiplier = 1\n+app.conf.worker_concurrency = 4\n+\n+# Reduce logs (Optional, useful for testing)\n+app.conf.worker_heartbeat = None\n+app.conf.broker_heartbeat = 0\n+\n+\n+def is_using_quorum_queues(app) -> bool:\n+    queues = app.amqp.queues\n+    for qname in queues:\n+        qarguments = queues[qname].queue_arguments or {}\n+        if qarguments.get(\"x-queue-type\") == \"quorum\":\n+            return True\n+\n+    return False\n+\n+\n+@app.task\n+def add(x, y):\n+    return x + y\n+\n+\n+@app.task\n+def identity(x):\n+    return x\n+\n+\n+def example():\n+    queue = my_quorum_queue.name if my_quorum_queue in (app.conf.task_queues or {}) else \"celery\"\n+\n+    while True:\n+        print(\"Celery Quorum Queue Example\")\n+        print(\"===========================\")\n+        print(\"1. Send a simple identity task\")\n+        print(\"1.1 Send an ETA identity task\")\n+        print(\"2. Send a group of add tasks\")\n+        print(\"3. Inspect the active queues\")\n+        print(\"4. Shutdown Celery worker\")\n+        print(\"Q. Quit\")\n+        print(\"Q! Exit\")\n+        choice = input(\"Enter your choice (1-4 or Q): \")\n+\n+        if choice == \"1\" or choice == \"1.1\":\n+            queue_type = \"Quorum\" if is_using_quorum_queues(app) else \"Classic\"\n+            payload = f\"Hello, {queue_type} Queue!\"\n+            eta = datetime.now(UTC) + timedelta(seconds=30)\n+            if choice == \"1.1\":\n+                result = identity.si(payload).apply_async(queue=queue, eta=eta)\n+            else:\n+                result = identity.si(payload).apply_async(queue=queue)\n+            print()\n+            print(f\"Task sent with ID: {result.id}\")\n+            print(\"Task type: identity\")\n+\n+            if choice == \"1.1\":\n+                print(f\"ETA: {eta}\")\n+\n+            print(f\"Payload: {payload}\")\n+\n+        elif choice == \"2\":\n+            tasks = [\n+                (1, 2),\n+                (3, 4),\n+                (5, 6),\n+            ]\n+            result = group(\n+                add.s(*tasks[0]),\n+                add.s(*tasks[1]),\n+                add.s(*tasks[2]),\n+            ).apply_async(queue=queue)\n+            print()\n+            print(\"Group of tasks sent.\")\n+            print(f\"Group result ID: {result.id}\")\n+            for i, task_args in enumerate(tasks, 1):\n+                print(f\"Task {i} type: add\")\n+                print(f\"Payload: {task_args}\")\n+\n+        elif choice == \"3\":\n+            active_queues = app.control.inspect().active_queues()\n+            print()\n+            print(\"Active queues:\")\n+            for worker, queues in active_queues.items():\n+                print(f\"Worker: {worker}\")\n+                for q in queues:\n+                    print(f\"  - {q['name']}\")\n+\n+        elif choice == \"4\":\n+            print(\"Shutting down Celery worker...\")\n+            app.control.shutdown()\n+\n+        elif choice.lower() == \"q\":\n+            print(\"Quitting test()\")\n+            break\n+\n+        elif choice.lower() == \"q!\":\n+            print(\"Exiting...\")\n+            os.abort()\n+\n+        else:\n+            print(\"Invalid choice. Please enter a number between 1 and 4 or Q to quit.\")\n+\n+        print(\"\\n\" + \"#\" * 80 + \"\\n\")\n+\n+\n+if __name__ == \"__main__\":\n+    app.start()\ndiff --git a/examples/quorum-queues/setup_cluster.sh b/examples/quorum-queues/setup_cluster.sh\nnew file mode 100755\nindex 00000000000..f59501e9277\n--- /dev/null\n+++ b/examples/quorum-queues/setup_cluster.sh\n@@ -0,0 +1,117 @@\n+#!/bin/bash\n+\n+ERLANG_COOKIE=\"MYSECRETCOOKIE\"\n+\n+cleanup() {\n+    echo \"Stopping and removing existing RabbitMQ containers...\"\n+    docker stop rabbit1 rabbit2 rabbit3 2>/dev/null\n+    docker rm rabbit1 rabbit2 rabbit3 2>/dev/null\n+\n+    echo \"Removing existing Docker network...\"\n+    docker network rm rabbitmq-cluster 2>/dev/null\n+}\n+\n+wait_for_container() {\n+    local container_name=$1\n+    local retries=20\n+    local count=0\n+\n+    until [ \"$(docker inspect -f {{.State.Running}} $container_name)\" == \"true\" ]; do\n+        sleep 1\n+        count=$((count + 1))\n+        if [ $count -ge $retries ]; then\n+            echo \"Error: Container $container_name did not start in time.\"\n+            exit 1\n+        fi\n+    done\n+}\n+\n+wait_for_rabbitmq() {\n+    local container_name=$1\n+    local retries=10\n+    local count=0\n+\n+    until docker exec -it $container_name rabbitmqctl status; do\n+        sleep 1\n+        count=$((count + 1))\n+        if [ $count -ge $retries ]; then\n+            echo \"Error: RabbitMQ in container $container_name did not start in time.\"\n+            exit 1\n+        fi\n+    done\n+}\n+\n+setup_cluster() {\n+    echo \"Creating Docker network for RabbitMQ cluster...\"\n+    docker network create rabbitmq-cluster\n+\n+    echo \"Starting rabbit1 container...\"\n+    docker run -d --rm --name rabbit1 --hostname rabbit1 --net rabbitmq-cluster \\\n+        -e RABBITMQ_NODENAME=rabbit@rabbit1 \\\n+        -e RABBITMQ_ERLANG_COOKIE=$ERLANG_COOKIE \\\n+        --net-alias rabbit1 \\\n+        -p 15672:15672 -p 5672:5672 rabbitmq:3-management\n+\n+    sleep 5\n+    wait_for_container rabbit1\n+    wait_for_rabbitmq rabbit1\n+\n+    # echo \"Installing netcat in rabbit1 for debugging purposes...\"\n+    # docker exec -it rabbit1 bash -c \"apt-get update && apt-get install -y netcat\"\n+\n+    echo \"Starting rabbit2 container...\"\n+    docker run -d --rm --name rabbit2 --hostname rabbit2 --net rabbitmq-cluster \\\n+        -e RABBITMQ_NODENAME=rabbit@rabbit2 \\\n+        -e RABBITMQ_ERLANG_COOKIE=$ERLANG_COOKIE \\\n+        --net-alias rabbit2 \\\n+        -p 15673:15672 -p 5673:5672 rabbitmq:3-management\n+\n+    sleep 5\n+    wait_for_container rabbit2\n+    wait_for_rabbitmq rabbit2\n+\n+    # echo \"Installing netcat in rabbit2 for debugging purposes...\"\n+    # docker exec -it rabbit2 bash -c \"apt-get update && apt-get install -y netcat\"\n+\n+    echo \"Starting rabbit3 container...\"\n+    docker run -d --rm --name rabbit3 --hostname rabbit3 --net rabbitmq-cluster \\\n+        -e RABBITMQ_NODENAME=rabbit@rabbit3 \\\n+        -e RABBITMQ_ERLANG_COOKIE=$ERLANG_COOKIE \\\n+        --net-alias rabbit3 \\\n+        -p 15674:15672 -p 5674:5672 rabbitmq:3-management\n+\n+    sleep 5\n+    wait_for_container rabbit3\n+    wait_for_rabbitmq rabbit3\n+\n+    # echo \"Installing netcat in rabbit3 for debugging purposes...\"\n+    # docker exec -it rabbit3 bash -c \"apt-get update && apt-get install -y netcat\"\n+\n+    echo \"Joining rabbit2 to the cluster...\"\n+    docker exec -it rabbit2 rabbitmqctl stop_app\n+    docker exec -it rabbit2 rabbitmqctl reset\n+    docker exec -it rabbit2 rabbitmqctl join_cluster rabbit@rabbit1\n+    if [ $? -ne 0 ]; then\n+        echo \"Error: Failed to join rabbit2 to the cluster.\"\n+        exit 1\n+    fi\n+    docker exec -it rabbit2 rabbitmqctl start_app\n+\n+    echo \"Joining rabbit3 to the cluster...\"\n+    docker exec -it rabbit3 rabbitmqctl stop_app\n+    docker exec -it rabbit3 rabbitmqctl reset\n+    docker exec -it rabbit3 rabbitmqctl join_cluster rabbit@rabbit1\n+    if [ $? -ne 0 ]; then\n+        echo \"Error: Failed to join rabbit3 to the cluster.\"\n+        exit 1\n+    fi\n+    docker exec -it rabbit3 rabbitmqctl start_app\n+\n+    echo \"Verifying cluster status from rabbit1...\"\n+    docker exec -it rabbit1 rabbitmqctl cluster_status\n+}\n+\n+cleanup\n+setup_cluster\n+\n+echo \"RabbitMQ cluster setup is complete.\"\n", "test_patch": "diff --git a/examples/quorum-queues/test_cluster.sh b/examples/quorum-queues/test_cluster.sh\nnew file mode 100755\nindex 00000000000..c0b36bce521\n--- /dev/null\n+++ b/examples/quorum-queues/test_cluster.sh\n@@ -0,0 +1,41 @@\n+#!/bin/bash\n+\n+QUEUE_NAME=\"my-quorum-queue\"\n+VHOST=\"/\"\n+\n+remove_existing_queue() {\n+  docker exec -it rabbit1 rabbitmqctl delete_queue $QUEUE_NAME\n+}\n+\n+create_quorum_queue() {\n+  docker exec -it rabbit1 rabbitmqadmin declare queue name=$QUEUE_NAME durable=true arguments='{\"x-queue-type\":\"quorum\"}'\n+}\n+\n+verify_quorum_queue() {\n+  docker exec -it rabbit1 rabbitmqctl list_queues name type durable auto_delete arguments | grep $QUEUE_NAME\n+}\n+\n+send_test_message() {\n+  docker exec -it rabbit1 rabbitmqadmin publish exchange=amq.default routing_key=$QUEUE_NAME payload='Hello, RabbitMQ!'\n+}\n+\n+receive_test_message() {\n+  docker exec -it rabbit1 rabbitmqadmin get queue=$QUEUE_NAME ackmode=ack_requeue_false\n+}\n+\n+echo \"Removing existing quorum queue if it exists...\"\n+remove_existing_queue\n+\n+echo \"Creating quorum queue...\"\n+create_quorum_queue\n+\n+echo \"Verifying quorum queue...\"\n+verify_quorum_queue\n+\n+echo \"Sending test message...\"\n+send_test_message\n+\n+echo \"Receiving test message...\"\n+receive_test_message\n+\n+echo \"Quorum queue setup and message test completed successfully.\"\ndiff --git a/t/smoke/tests/quorum_queues/__init__.py b/t/smoke/tests/quorum_queues/__init__.py\nnew file mode 100644\nindex 00000000000..e69de29bb2d\ndiff --git a/t/smoke/tests/quorum_queues/conftest.py b/t/smoke/tests/quorum_queues/conftest.py\nnew file mode 100644\nindex 00000000000..9111a97dd5a\n--- /dev/null\n+++ b/t/smoke/tests/quorum_queues/conftest.py\n@@ -0,0 +1,119 @@\n+from __future__ import annotations\n+\n+import os\n+\n+import pytest\n+from pytest_celery import RABBITMQ_PORTS, CeleryBrokerCluster, RabbitMQContainer, RabbitMQTestBroker, defaults\n+from pytest_docker_tools import build, container, fxtr\n+\n+from celery import Celery\n+from t.smoke.workers.dev import SmokeWorkerContainer\n+\n+###############################################################################\n+# RabbitMQ Management Broker\n+###############################################################################\n+\n+\n+class RabbitMQManagementBroker(RabbitMQTestBroker):\n+    def get_management_url(self) -> str:\n+        \"\"\"Opening this link during debugging allows you to see the\n+        RabbitMQ management UI in your browser.\n+\n+        Usage from a test:\n+        >>> celery_setup.broker.get_management_url()\n+\n+        Open from a browser and login with guest:guest.\n+        \"\"\"\n+        ports = self.container.attrs[\"NetworkSettings\"][\"Ports\"]\n+        ip = ports[\"15672/tcp\"][0][\"HostIp\"]\n+        port = ports[\"15672/tcp\"][0][\"HostPort\"]\n+        return f\"http://{ip}:{port}\"\n+\n+\n+@pytest.fixture\n+def default_rabbitmq_broker_image() -> str:\n+    return \"rabbitmq:management\"\n+\n+\n+@pytest.fixture\n+def default_rabbitmq_broker_ports() -> dict:\n+    # Expose the management UI port\n+    ports = RABBITMQ_PORTS.copy()\n+    ports.update({\"15672/tcp\": None})\n+    return ports\n+\n+\n+@pytest.fixture\n+def celery_rabbitmq_broker(default_rabbitmq_broker: RabbitMQContainer) -> RabbitMQTestBroker:\n+    broker = RabbitMQManagementBroker(default_rabbitmq_broker)\n+    yield broker\n+    broker.teardown()\n+\n+\n+@pytest.fixture\n+def celery_broker_cluster(celery_rabbitmq_broker: RabbitMQTestBroker) -> CeleryBrokerCluster:\n+    cluster = CeleryBrokerCluster(celery_rabbitmq_broker)\n+    yield cluster\n+    cluster.teardown()\n+\n+\n+###############################################################################\n+# Worker Configuration\n+###############################################################################\n+\n+\n+class QuorumWorkerContainer(SmokeWorkerContainer):\n+    @classmethod\n+    def log_level(cls) -> str:\n+        return \"INFO\"\n+\n+    @classmethod\n+    def worker_queue(cls) -> str:\n+        return \"celery\"\n+\n+\n+@pytest.fixture\n+def default_worker_container_cls() -> type[SmokeWorkerContainer]:\n+    return QuorumWorkerContainer\n+\n+\n+@pytest.fixture(scope=\"session\")\n+def default_worker_container_session_cls() -> type[SmokeWorkerContainer]:\n+    return QuorumWorkerContainer\n+\n+\n+celery_dev_worker_image = build(\n+    path=\".\",\n+    dockerfile=\"t/smoke/workers/docker/dev\",\n+    tag=\"t/smoke/worker:dev\",\n+    buildargs=QuorumWorkerContainer.buildargs(),\n+)\n+\n+\n+default_worker_container = container(\n+    image=\"{celery_dev_worker_image.id}\",\n+    ports=fxtr(\"default_worker_ports\"),\n+    environment=fxtr(\"default_worker_env\"),\n+    network=\"{default_pytest_celery_network.name}\",\n+    volumes={\n+        # Volume: Worker /app\n+        \"{default_worker_volume.name}\": defaults.DEFAULT_WORKER_VOLUME,\n+        # Mount: Celery source\n+        os.path.abspath(os.getcwd()): {\n+            \"bind\": \"/celery\",\n+            \"mode\": \"rw\",\n+        },\n+    },\n+    wrapper_class=QuorumWorkerContainer,\n+    timeout=defaults.DEFAULT_WORKER_CONTAINER_TIMEOUT,\n+    command=fxtr(\"default_worker_command\"),\n+)\n+\n+\n+@pytest.fixture\n+def default_worker_app(default_worker_app: Celery) -> Celery:\n+    app = default_worker_app\n+    app.conf.broker_transport_options = {\"confirm_publish\": True}\n+    app.conf.task_default_queue_type = \"quorum\"\n+\n+    return app\ndiff --git a/t/smoke/tests/quorum_queues/test_quorum_queues.py b/t/smoke/tests/quorum_queues/test_quorum_queues.py\nnew file mode 100644\nindex 00000000000..7748dce982d\n--- /dev/null\n+++ b/t/smoke/tests/quorum_queues/test_quorum_queues.py\n@@ -0,0 +1,36 @@\n+import requests\n+from pytest_celery import RESULT_TIMEOUT, CeleryTestSetup\n+from requests.auth import HTTPBasicAuth\n+\n+from celery.canvas import group\n+from t.integration.tasks import add, identity\n+from t.smoke.tests.quorum_queues.conftest import RabbitMQManagementBroker\n+\n+\n+class test_broker_configuration:\n+    def test_queue_type(self, celery_setup: CeleryTestSetup):\n+        broker: RabbitMQManagementBroker = celery_setup.broker\n+        api = broker.get_management_url() + \"/api/queues\"\n+        response = requests.get(api, auth=HTTPBasicAuth(\"guest\", \"guest\"))\n+        assert response.status_code == 200\n+        res = response.json()\n+        assert isinstance(res, list)\n+        worker_queue = next((queue for queue in res if queue[\"name\"] == celery_setup.worker.worker_queue), None)\n+        assert worker_queue is not None, f'\"{celery_setup.worker.worker_queue}\" queue not found'\n+        queue_type = worker_queue.get(\"type\")\n+        assert queue_type == \"quorum\", f'\"{celery_setup.worker.worker_queue}\" queue is not a quorum queue'\n+\n+\n+class test_quorum_queues:\n+    def test_signature(self, celery_setup: CeleryTestSetup):\n+        sig = identity.si(\"test_signature\").set(queue=celery_setup.worker.worker_queue)\n+        assert sig.delay().get(timeout=RESULT_TIMEOUT) == \"test_signature\"\n+\n+    def test_group(self, celery_setup: CeleryTestSetup):\n+        sig = group(\n+            group(add.si(1, 1), add.si(2, 2)),\n+            group([add.si(1, 1), add.si(2, 2)]),\n+            group(s for s in [add.si(1, 1), add.si(2, 2)]),\n+        )\n+        res = sig.apply_async(queue=celery_setup.worker.worker_queue)\n+        assert res.get(timeout=RESULT_TIMEOUT) == [2, 4, 2, 4, 2, 4]\ndiff --git a/t/unit/app/test_amqp.py b/t/unit/app/test_amqp.py\nindex acbeecea08a..1293eb5d15e 100644\n--- a/t/unit/app/test_amqp.py\n+++ b/t/unit/app/test_amqp.py\n@@ -137,17 +137,19 @@ def test_with_max_priority(self, queues_kwargs, qname, q, expected):\n \n class test_default_queues:\n \n+    @pytest.mark.parametrize('default_queue_type', ['classic', 'quorum'])\n     @pytest.mark.parametrize('name,exchange,rkey', [\n         ('default', None, None),\n         ('default', 'exchange', None),\n         ('default', 'exchange', 'routing_key'),\n         ('default', None, 'routing_key'),\n     ])\n-    def test_setting_default_queue(self, name, exchange, rkey):\n+    def test_setting_default_queue(self, name, exchange, rkey, default_queue_type):\n         self.app.conf.task_queues = {}\n         self.app.conf.task_default_exchange = exchange\n         self.app.conf.task_default_routing_key = rkey\n         self.app.conf.task_default_queue = name\n+        self.app.conf.task_default_queue_type = default_queue_type\n         assert self.app.amqp.queues.default_exchange.name == exchange or name\n         queues = dict(self.app.amqp.queues)\n         assert len(queues) == 1\n@@ -156,6 +158,11 @@ def test_setting_default_queue(self, name, exchange, rkey):\n         assert queue.exchange.type == 'direct'\n         assert queue.routing_key == rkey or name\n \n+        if default_queue_type == 'quorum':\n+            assert queue.queue_arguments == {'x-queue-type': 'quorum'}\n+        else:\n+            assert queue.queue_arguments is None\n+\n \n class test_default_exchange:\n \ndiff --git a/t/unit/worker/test_consumer.py b/t/unit/worker/test_consumer.py\nindex 6613bd2a40e..3b8cb2a8322 100644\n--- a/t/unit/worker/test_consumer.py\n+++ b/t/unit/worker/test_consumer.py\n@@ -9,14 +9,14 @@\n \n from celery import bootsteps\n from celery.contrib.testing.mocks import ContextMock\n-from celery.exceptions import WorkerShutdown, WorkerTerminate\n+from celery.exceptions import CeleryWarning, WorkerShutdown, WorkerTerminate\n from celery.utils.collections import LimitedSet\n from celery.worker.consumer.agent import Agent\n from celery.worker.consumer.consumer import CANCEL_TASKS_BY_DEFAULT, CLOSE, TERMINATE, Consumer\n from celery.worker.consumer.gossip import Gossip\n from celery.worker.consumer.heart import Heart\n from celery.worker.consumer.mingle import Mingle\n-from celery.worker.consumer.tasks import Tasks\n+from celery.worker.consumer.tasks import ETA_TASKS_NO_GLOBAL_QOS_WARNING, Tasks\n from celery.worker.state import active_requests\n \n \n@@ -543,8 +543,13 @@ def test_start_heartbeat_interval(self):\n \n class test_Tasks:\n \n+    def setup_method(self):\n+        self.c = Mock()\n+        self.c.app.conf.worker_detect_quorum_queues = True\n+        self.c.connection.qos_semantics_matches_spec = False\n+\n     def test_stop(self):\n-        c = Mock()\n+        c = self.c\n         tasks = Tasks(c)\n         assert c.task_consumer is None\n         assert c.qos is None\n@@ -553,10 +558,59 @@ def test_stop(self):\n         tasks.stop(c)\n \n     def test_stop_already_stopped(self):\n-        c = Mock()\n+        c = self.c\n         tasks = Tasks(c)\n         tasks.stop(c)\n \n+    def test_detect_quorum_queues_positive(self):\n+        c = self.c\n+        c.app.amqp.queues = {\"celery\": Mock(queue_arguments={\"x-queue-type\": \"quorum\"})}\n+        tasks = Tasks(c)\n+        result, name = tasks.detect_quorum_queues(c)\n+        assert result\n+        assert name == \"celery\"\n+\n+    def test_detect_quorum_queues_negative(self):\n+        c = self.c\n+        c.app.amqp.queues = {\"celery\": Mock(queue_arguments=None)}\n+        tasks = Tasks(c)\n+        result, name = tasks.detect_quorum_queues(c)\n+        assert not result\n+        assert name == \"\"\n+\n+    def test_detect_quorum_queues_not_rabbitmq(self):\n+        c = self.c\n+        c.app.conf.broker_url = \"redis://\"\n+        tasks = Tasks(c)\n+        result, name = tasks.detect_quorum_queues(c)\n+        assert not result\n+        assert name == \"\"\n+\n+    def test_qos_global_worker_detect_quorum_queues_false(self):\n+        c = self.c\n+        c.app.conf.worker_detect_quorum_queues = False\n+        tasks = Tasks(c)\n+        assert tasks.qos_global(c) is True\n+\n+    def test_qos_global_worker_detect_quorum_queues_true_no_quorum_queues(self):\n+        c = self.c\n+        c.app.amqp.queues = {\"celery\": Mock(queue_arguments=None)}\n+        tasks = Tasks(c)\n+        assert tasks.qos_global(c) is True\n+\n+    def test_qos_global_worker_detect_quorum_queues_true_with_quorum_queues(self):\n+        c = self.c\n+        c.app.amqp.queues = {\"celery\": Mock(queue_arguments={\"x-queue-type\": \"quorum\"})}\n+        tasks = Tasks(c)\n+        assert tasks.qos_global(c) is False\n+\n+    def test_qos_global_eta_warning(self):\n+        c = self.c\n+        c.app.amqp.queues = {\"celery\": Mock(queue_arguments={\"x-queue-type\": \"quorum\"})}\n+        tasks = Tasks(c)\n+        with pytest.warns(CeleryWarning, match=ETA_TASKS_NO_GLOBAL_QOS_WARNING % \"celery\"):\n+            tasks.qos_global(c)\n+\n \n class test_Agent:\n \n", "problem_statement": "support for quorum queues\n<!--\nPlease fill this template entirely and do not erase parts of it.\nWe reserve the right to close without a response\nfeature requests which are incomplete.\n-->\n\n\n# Checklist\n<!--\nTo check an item on the list replace [ ] with [x].\n-->\n\n* [x] I have checked the [issues list](https://github.com/celery/celery/issues?utf8=%E2%9C%93&q=is%3Aissue+label%3A%22Issue+Type%3A+Feature+Request%22+)\n  for similar or identical feature requests.\n* [ ] I have checked the [pull requests list](https://github.com/celery/celery/pulls?utf8=%E2%9C%93&q=is%3Apr+label%3A%22PR+Type%3A+Feature%22+)\n  for existing proposed implementations of this feature.\n* [x] I have checked the [commit log](https://github.com/celery/celery/commits/master)\n  to find out if the if the same feature was already implemented in the\n  master branch.\n* [x] I have included all related issues and possible duplicate issues\n  in this issue (If there are none, check this box anyway).\n\n## Related Issues and Possible Duplicates\n<!--\nPlease make sure to search and mention any related issues\nor possible duplicates to this issue as requested by the checklist above.\n\nThis may or may not include issues in other repositories that the Celery project\nmaintains or other repositories that are dependencies of Celery.\n\nIf you don't know how to mention issues, please refer to Github's documentation\non the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests\n-->\n\n#### Related Issues\n* [py-amqp](https://github.com/celery/py-amqp/issues/302)\n\n#### Possible Duplicates\n* None\n\n# Brief Summary\n<!--\nPlease include a brief summary of what the feature does\nand why it is needed.\n-->\nsupport for quorum queues.\n\n# Design\n## Architectural Considerations\n<!--\nIf more components other than Celery are involved,\ndescribe them here and the effect it would have on Celery.\n-->\nNone\n\n## Proposed Behavior\n<!--\nPlease describe in detail how this feature is going to behave.\nDescribe what happens in case of failures as well if applicable.\n-->\n\n## Proposed UI/UX\n<!--\nPlease provide your ideas for the API, CLI options,\nconfiguration key names etc. that will be introduced for this feature.\n-->\n\n## Diagrams\n<!--\nPlease include any diagrams that might be relevant\nto the implementation of this feature such as:\n\n\n* Class Diagrams\n* Sequence Diagrams\n* Activity Diagrams\nYou can drag and drop images into the text box to attach them to this issue.\n-->\nN/A\n\n## Alternatives\n<!--\nIf you have considered any alternative implementations\ndescribe them in detail below.\n-->\nNone\n\n\n", "hints_text": "there is same issue in Py-AMQP package!\ndo you mean the one that i mentioned on related issues?\n@auvipy the issue in Py-amqp package is not relevant. py-amqp should fully support quorum queues. The problem is that I was not able to find any way how to specify per-consumer QOS setting which is required for quorum queues. I have adviced @ashexpertVersion2  to create issue here - see https://github.com/celery/py-amqp/issues/302 for details.\nIt's worth reading over the feature matrix in the RMQ docs that compare mirrored with quorum queues: https://www.rabbitmq.com/quorum-queues.html#feature-comparison\r\n\r\nYou lose other features by moving to quorum queues, like task/message priorities.\nany updates on this?\nIt's a super dirty solution, but you can hack around this by hooking into the [boot steps subsystem](https://docs.celeryproject.org/en/stable/userguide/extending.html), in order to force a per-consumer QoS. I've been running something similar to [0] in production (currently being `4.4.7`) for the last few months with no issues so far, it's basically an override for [1].\r\n\r\n[0]\r\n```python\r\nfrom celery import bootsteps\r\n\r\nclass NoChannelGlobalQoS(bootsteps.StartStopStep):\r\n    requires = {'celery.worker.consumer.tasks:Tasks'}\r\n\r\n    def start(self, c):\r\n        qos_global = False\r\n\r\n        c.connection.default_channel.basic_qos(\r\n            0, c.initial_prefetch_count, qos_global,\r\n        )\r\n\r\n        def set_prefetch_count(prefetch_count):\r\n            return c.task_consumer.qos(\r\n                prefetch_count=prefetch_count,\r\n                apply_global=qos_global,\r\n            )\r\n        c.qos = QoS(set_prefetch_count, c.initial_prefetch_count)\r\n\r\n\r\napp.steps['consumer'].add(NoChannelGlobalQoS)\r\n```\r\n\r\n[1] https://github.com/celery/celery/blob/120770929f4a37c5373a378b75b5c41a99702af9/celery/worker/consumer/tasks.py#L33\nAnd obviously pass `'x-queue-type': 'quorum'` during queue declaration, but that's [pretty easy already](https://docs.celeryproject.org/en/latest/userguide/routing.html#rabbitmq-message-priorities), just put it in `queue_arguments`.\nThe py-amqp was closed a while ago. Is this blocked on anything? There seems to be a workaround but I think the expected behaviour is that a celery user, after having setup rabbitmq using [commonly advised queue configurations](https://www.cloudamqp.com/blog/reasons-you-should-switch-to-quorum-queues.html) should be able to connect to the rabbit hosts and start submitting and consuming tasks. If i understand this will currently fail?\nYes, it fails for me. Until celery is updated to support per-consumer QOS it won't work with quorum queues.\nAny update on this?\nIs there any update on this ?? I still get the error `amqp.exceptions.AMQPNotImplementedError: Basic.consume: (540) NOT_IMPLEMENTED - queue 'feeds' in vhost 'jarvis_vhost' does not support global qos` \nSuggested patch by @Asgavar causes that prefetch count is not incremented when a task with ETA is received.\r\n\r\nDo you have any idea on how to approach it?\r\n\r\n> When a task with an ETA is received the QoS prefetch count is also incremented, so another message can be reserved. When the ETA is met the prefetch count is decremented again, though this cannot happen immediately because amqplib doesn\u2019t support doing broker requests across threads. Instead the current prefetch count is kept as a shared counter, so as soon as consume_messages() detects that the value has changed it will send out the actual QoS event to the broker.\nI'm currently debugging the same issue @MichaelKubovic since we're in the process of moving Celery to Quorum queues.\r\nI found that the prefetch count does get incremented but only on a channel level. The `basic_qos` command gets ran with `global` set to `False` but that won't help consumers which have already launched with existing prefetch count. Every consumer that has already started by that point won't get its prefetch count updated. \r\n\r\nHere's an example:\r\n![](https://cdn.zappy.app/85011b8da4b8d01d48f0ec840e087364.jpeg)\r\n\r\nThe prefetch count at consumer launch was set to 192. Celery received a couple of ETA tasks and did send the qos command to change prefetch to 203 which was set. But the existing consumer has the prefetch still at 192 and the qos change will only impact new consumers that launch.\r\n\r\nWhat we need is some way of changing prefetch for existing consumers. I'm not sure something like this is even possible.\r\n\r\nEDIT: After chatting with some RabbitMQ experts, I've been told that you can't dynamically change prefetch count for existing consumers.\n> I'm currently debugging the same issue @MichaelKubovic since we're in the process of moving Celery to Quorum queues. I found that the prefetch count does get incremented but only on a channel level. The `basic_qos` command gets ran with `global` set to `False` but that won't help consumers which have already launched with existing prefetch count. Every consumer that has already started by that point won't get its prefetch count updated.\r\n> \r\n> Here's an example: ![](https://camo.githubusercontent.com/b875b06b9923fe2612355e1ed513e1ddb3ff418835cb4ce9872c0995c1e6fe24/68747470733a2f2f63646e2e7a617070792e6170702f38353031316238646134623864303164343866306563383430653038373336342e6a706567)\r\n> \r\n> The prefetch count at consumer launch was set to 192. Celery received a couple of ETA tasks and did send the qos command to change prefetch to 203 which was set. But the existing consumer has the prefetch still at 192 and the qos change will only impact new consumers that launch.\r\n> \r\n> What we need is some way of changing prefetch for existing consumers. I'm not sure something like this is even possible.\r\n> \r\n> EDIT: After chatting with some RabbitMQ experts, I've been told that you can't dynamically change prefetch count for existing consumers.\r\n\r\nthanks for sharing this, did you read https://github.com/celery/celery/issues/6067#issuecomment-623056428?\n > thanks for sharing this, did you read [#6067 (comment)](https://github.com/celery/celery/issues/6067#issuecomment-623056428)?\r\n\r\nYeah, luckily we don't use task priorities but do rely on ETA tasks a lot (combined with non ETA tasks in same queue) so we decided against migrating to quorum queues since consumer QoS is a blocker. I don't see any easy way of implementing quorum queues in Celery without completely refactoring how ETA tasks are implemented with AMQP (in Rabbit it could work with per message TTL in a queue (with no consumers) that dead letters to a another queue (that has consumers)).\n> It's a super dirty solution, but you can hack around this by hooking into the [boot steps subsystem](https://docs.celeryproject.org/en/stable/userguide/extending.html), in order to force a per-consumer QoS. I've been running something similar to [0] in production (currently being `4.4.7`) for the last few months with no issues so far, it's basically an override for [1].\r\n> \r\n> [0]\r\n> \r\n> ```python\r\n> from celery import bootsteps\r\n> \r\n> class NoChannelGlobalQoS(bootsteps.StartStopStep):\r\n>     requires = {'celery.worker.consumer.tasks:Tasks'}\r\n> \r\n>     def start(self, c):\r\n>         qos_global = False\r\n> \r\n>         c.connection.default_channel.basic_qos(\r\n>             0, c.initial_prefetch_count, qos_global,\r\n>         )\r\n> \r\n>         def set_prefetch_count(prefetch_count):\r\n>             return c.task_consumer.qos(\r\n>                 prefetch_count=prefetch_count,\r\n>                 apply_global=qos_global,\r\n>             )\r\n>         c.qos = QoS(set_prefetch_count, c.initial_prefetch_count)\r\n> \r\n> \r\n> app.steps['consumer'].add(NoChannelGlobalQoS)\r\n> ```\r\n> \r\n> [1]\r\n> \r\n> https://github.com/celery/celery/blob/120770929f4a37c5373a378b75b5c41a99702af9/celery/worker/consumer/tasks.py#L33\r\n\r\nHas there been any update on this issue? We deployed this fix on celery 5.2.7 and we are getting this error back. \r\n\r\n`amqp.exceptions.PreconditionFailed: Queue.declare: (406) PRECONDITION_FAILED - inequivalent arg 'x-queue-type' for queue 'helios-messages' in vhost 'helios': received the value 'quorum' of type 'longstr' but current is none`\r\n\n> \r\n> Has there been any update on this issue? We deployed this fix on celery 5.2.7 and we are getting this error back.\r\n> \r\n> `amqp.exceptions.PreconditionFailed: Queue.declare: (406) PRECONDITION_FAILED - inequivalent arg 'x-queue-type' for queue 'helios-messages' in vhost 'helios': received the value 'quorum' of type 'longstr' but current is none`\r\n\r\nThe definition of celery Queue in your code is missing the `x-queue-type: quorum` hence the error. Celery is saying the definition it got from RabbitMQ has a different value than the definition of queue you supplied.\r\n\r\nBut even when you get this working it's a half baked solution as I mentioned previously. Normal Celery tasks work but anything that involves any type of delay and countdown will block your entire task processing since it will use up the entire QoS prefetch count and the worker will stop processing new tasks.\n> `amqp.exceptions.PreconditionFailed: Queue.declare: (406) PRECONDITION_FAILED - inequivalent arg 'x-queue-type' for queue 'helios-messages' in vhost 'helios': received the value 'quorum' of type 'longstr' but current is none`\r\n\r\nThis one is actually unrelated, try removing this queue and just let Celery re-create it with the args you provide now\nIs there any update on this?\r\n\r\nSince RabitMQ itself will deprecate classic queues.\r\n\r\nhttps://blog.rabbitmq.com/posts/2021/08/4.0-deprecation-announcements/ \r\n\r\n\n> Is there any update on this?\r\n> \r\n> Since RabitMQ itself will deprecate classic queues.\r\n> \r\n> https://blog.rabbitmq.com/posts/2021/08/4.0-deprecation-announcements/\r\n\r\nwhat about supporting streams https://www.rabbitmq.com/streams.html?\n> RabitMQ itself will deprecate classic queues.\r\n\r\nWorse, it will remove mirrored queues. Thus means Celery + RMQ will not support some HA scenarios unless Quorum Queues or Streams are supported. \r\n\r\nSpecifically a key issue that prevents use of Quorum Queues with Celery, channel-wide QoS settings, will not be supported by RMQ:\r\n\r\n> Global QoS, where a single shared prefetch is used for an entire channel, is not recommended practice.\r\n\r\nIs anyone already working on this? How can we help?\n> It's a super dirty solution, but you can hack around this by hooking into the [boot steps subsystem](https://docs.celeryproject.org/en/stable/userguide/extending.html), in order to force a per-consumer QoS. I've been running something similar to [0] in production (currently being `4.4.7`) for the last few months with no issues so far, it's basically an override for [1].\r\n> \r\n> [0]\r\n> \r\n> ```python\r\n> from celery import bootsteps\r\n> \r\n> class NoChannelGlobalQoS(bootsteps.StartStopStep):\r\n>     requires = {'celery.worker.consumer.tasks:Tasks'}\r\n> \r\n>     def start(self, c):\r\n>         qos_global = False\r\n> \r\n>         c.connection.default_channel.basic_qos(\r\n>             0, c.initial_prefetch_count, qos_global,\r\n>         )\r\n> \r\n>         def set_prefetch_count(prefetch_count):\r\n>             return c.task_consumer.qos(\r\n>                 prefetch_count=prefetch_count,\r\n>                 apply_global=qos_global,\r\n>             )\r\n>         c.qos = QoS(set_prefetch_count, c.initial_prefetch_count)\r\n> \r\n> \r\n> app.steps['consumer'].add(NoChannelGlobalQoS)\r\n> ```\r\n> \r\n> [1]\r\n> \r\n> https://github.com/celery/celery/blob/120770929f4a37c5373a378b75b5c41a99702af9/celery/worker/consumer/tasks.py#L33\r\n\r\n\r\nactually, if you want all celery queue is HA, you need set control queue and event queue too.\r\n\r\n**notice**: untest and very very dirty\r\n\r\n```bash\r\nfrom kombu.transport import base\r\nfrom kombu import Queue as _Queue\r\n\r\n# monkey patch kombu, set  x-queue-type\uff0cquorum default\r\ndef set_args(func):  # type: ignore\r\n    def inner(self, *args, **kwargs):  # type: ignore\r\n        func(self, *args, **kwargs)\r\n        # quorum not support exclusive, auto-delete, and must durable\r\n        self.exclusive = False\r\n        self.auto_delete = False\r\n        self.durable = True\r\n    return inner\r\n\r\n\r\ndef monkey_kombu() -> None:\r\n    base.to_rabbitmq_queue_arguments = append_queue_type(\r\n        base.to_rabbitmq_queue_arguments\r\n    )\r\n    _Queue.__name__ = \"Queue\"\r\n    _Queue.__init__ = set_args(_Queue.__init__)\r\n\r\nmonkey_kombu()\r\n```\r\n\r\n\r\n\r\n\r\n\nWait, the only option to get highly available queues is to monkey-patch the queue implementation of Celery..? Has anything happened regarding streams or quorum queues?\n> Wait, the only option to get highly available queues is to monkey-patch the queue implementation of Celery..? Has anything happened regarding streams or quorum queues?\r\n\r\nClassic mirrored queues is the supported option for now. Agreed quorum queue support would be best.\nCelery now supports quorum queues with RabbitMQ(I am using 3.11-management), and celery(5.1.0). I am able to do it by defining:\r\n\r\nfrom celery import bootsteps\r\n\r\nclass NoChannelGlobalQoS(bootsteps.StartStopStep):\r\n    requires = {'celery.worker.consumer.tasks:Tasks'}\r\n\r\n    def start(self, c):\r\n        qos_global = False\r\n\r\n        c.connection.default_channel.basic_qos(\r\n            0, c.initial_prefetch_count, qos_global,\r\n        )\r\n\r\n        def set_prefetch_count(prefetch_count):\r\n            return c.task_consumer.qos(\r\n                prefetch_count=prefetch_count,\r\n                apply_global=qos_global,\r\n            )\r\n        c.qos = QoS(set_prefetch_count, c.initial_prefetch_count)\r\n\r\n\r\napp.steps['consumer'].add(NoChannelGlobalQoS)\r\n\r\n# after this I defined my celery queues with 'x-queue-type': 'quorum'. \r\nand it worked!\nUnless something changed (which I don't think is the case since version 5.10 is 2 years old) this won't work for ETA tasks. As soon as ETA tasks appear in the queue, the consumers will use up their prefetch count and block since dynamic updates of prefetch count are not possible with consumer QoS (non-global).\n@apolloFER may be, but it worked for me.\n> ```python\r\n> c.qos = QoS(set_prefetch_count, c.initial_prefetch_count)\r\n> ```\r\n\r\nI got error `QoS` is undefined. However, it seems there is no need to set `c.qos` since the global qos was already disabled earlier in the code. Using this code, I am able to make streams work with celery! So thanks @jbarnett-i4g \n> As soon as ETA tasks appear in the queue, the consumers will use up their prefetch count and block since dynamic updates of prefetch count are not possible with consumer QoS (non-global).\r\n\r\nPerhaps this should be its own issue? It seems that the logic for ETA tasks needs to be changed, e.g. use the worker broadcast mechanism to update the prefetch count (for ETA tasks) across all workers, or not fetch ETA tasks before the time is due. \r\n\r\n\n> > As soon as ETA tasks appear in the queue, the consumers will use up their prefetch count and block since dynamic updates of prefetch count are not possible with consumer QoS (non-global).\r\n> \r\n> Perhaps this should be its own issue? It seems that the logic for ETA tasks needs to be changed, e.g. use the worker broadcast mechanism to update the prefetch count (for ETA tasks) across all workers, or not fetch ETA tasks before the time is due.\r\n\r\nUnless something changed on the rabbit side, there is not way to change the prefetch count on existing consumers.\r\n\r\nI guess this [proposed](https://github.com/celery/celery/issues/6067#issuecomment-1154933510) implementation is the most straightforward way to do it.\r\n \r\n\nIs it possible to add a configuration to let users opt in to enable Quorum Queues at the expense of ETA tasks? Maybe as a short term solution till ETA tasks are re-architected.\r\n\r\nAlso, I believe with per-consumer QoS, the [autoscale feature](https://docs.celeryq.dev/en/main/internals/reference/celery.worker.autoscale.html) would not work since it also relies on [updating the prefetch count of the existing consumer](https://github.com/celery/celery/blob/2b3fde49576771975ec462243f9adf296938f616/celery/worker/autoscale.py#L136).\nAny update on support for quorum queues?\nAny updates?\nAlso waiting for the solution\n> Celery now supports quorum queues with RabbitMQ(I am using 3.11-management), and celery(5.1.0). I am able to do it by defining:\r\n> \r\n> from celery import bootsteps\r\n> \r\n> class NoChannelGlobalQoS(bootsteps.StartStopStep): requires = {'celery.worker.consumer.tasks:Tasks'}\r\n> \r\n> ```\r\n> def start(self, c):\r\n>     qos_global = False\r\n> \r\n>     c.connection.default_channel.basic_qos(\r\n>         0, c.initial_prefetch_count, qos_global,\r\n>     )\r\n> \r\n>     def set_prefetch_count(prefetch_count):\r\n>         return c.task_consumer.qos(\r\n>             prefetch_count=prefetch_count,\r\n>             apply_global=qos_global,\r\n>         )\r\n>     c.qos = QoS(set_prefetch_count, c.initial_prefetch_count)\r\n> ```\r\n> \r\n> app.steps['consumer'].add(NoChannelGlobalQoS)\r\n> \r\n> # after this I defined my celery queues with 'x-queue-type': 'quorum'.\r\n> and it worked!\r\n\r\nI am using celery with Django, so I am not quite sure how to use this example. Nonetheless, when I try and use quorum queues, I get this:\r\n```\r\namqp.exceptions.AMQPNotImplementedError: Basic.consume: (540) NOT_IMPLEMENTED - queue 'celery' in vhost '/' does not support global qos\r\n```\r\n\n> It's a super dirty solution, but you can hack around this by hooking into the [boot steps subsystem](https://docs.celeryproject.org/en/stable/userguide/extending.html), in order to force a per-consumer QoS. I've been running something similar to [0] in production (currently being `4.4.7`) for the last few months with no issues so far, it's basically an override for [1].\r\n> \r\n> [0]\r\n> \r\n> ```python\r\n> from celery import bootsteps\r\n> \r\n> class NoChannelGlobalQoS(bootsteps.StartStopStep):\r\n>     requires = {'celery.worker.consumer.tasks:Tasks'}\r\n> \r\n>     def start(self, c):\r\n>         qos_global = False\r\n> \r\n>         c.connection.default_channel.basic_qos(\r\n>             0, c.initial_prefetch_count, qos_global,\r\n>         )\r\n> \r\n>         def set_prefetch_count(prefetch_count):\r\n>             return c.task_consumer.qos(\r\n>                 prefetch_count=prefetch_count,\r\n>                 apply_global=qos_global,\r\n>             )\r\n>         c.qos = QoS(set_prefetch_count, c.initial_prefetch_count)\r\n> \r\n> \r\n> app.steps['consumer'].add(NoChannelGlobalQoS)\r\n> ```\r\n> \r\n> [1]\r\n> \r\n> https://github.com/celery/celery/blob/120770929f4a37c5373a378b75b5c41a99702af9/celery/worker/consumer/tasks.py#L33\r\n\r\nFor people using celery + django, add this code to your `celery.py` file or wherever you init celery like `app = Celery(\"...\")`\r\n\r\nWhile this solution works for us, we experienced a task loss issue with RabbitMQ cluster using quorum queue.\r\nWe did some digging and it turned out that we also want to add `app.conf.broker_transport_options = {\"confirm_publish\": True}` possibly due to the [implementation differences](https://groups.google.com/g/rabbitmq-users/c/LAflf1l4LtY#:~:text=A%20quorum%20queue%20ignores%20messages%20published%20by%20a%20closed%20channel%20and%20a%20classic%20queue%20does%20not.%20That%27s%20an%20implementation%20detail.) of quorum queue and classic queue when handling messages from closed channel.\r\n\r\nrefs - #5410, https://groups.google.com/g/rabbitmq-users/c/LAflf1l4LtY\r\n\n> Celery now supports quorum queues with RabbitMQ(I am using 3.11-management), and celery(5.1.0). I am able to do it by defining:\r\n> \r\n> from celery import bootsteps\r\n> \r\n> class NoChannelGlobalQoS(bootsteps.StartStopStep): requires = {'celery.worker.consumer.tasks:Tasks'}\r\n> \r\n> ```\r\n> def start(self, c):\r\n>     qos_global = False\r\n> \r\n>     c.connection.default_channel.basic_qos(\r\n>         0, c.initial_prefetch_count, qos_global,\r\n>     )\r\n> \r\n>     def set_prefetch_count(prefetch_count):\r\n>         return c.task_consumer.qos(\r\n>             prefetch_count=prefetch_count,\r\n>             apply_global=qos_global,\r\n>         )\r\n>     c.qos = QoS(set_prefetch_count, c.initial_prefetch_count)\r\n> ```\r\n> \r\n> app.steps['consumer'].add(NoChannelGlobalQoS)\r\n> \r\n> # after this I defined my celery queues with 'x-queue-type': 'quorum'.\r\n> and it worked!\r\n\r\nthe above worked for all the queues we create explicitly.\r\nhowever, is there a way to also start the \"celery@...pitbox\" queues (remote control queues created by celery) with quorum type?\nHello everyone, I\u2019ve been investigating the Quorum Queues feature for Celery and I will add official support for it based on the suggested solution by @Asgavar from https://github.com/celery/celery/issues/6067#issuecomment-724001426 and the details discussed in this discussion here. Thank you everyone for your comments and insights - it helped a lot \u2764\ufe0f!\r\n\r\nI\u2019ve created the following scripts to validate that it indeed works, and I can confirm Celery v5.4.0 was able to use quorum queues successfully. The next Celery version will include a slightly more refined implementation, but it will be based on this issue.\r\n\r\nEDIT: I understand there are some caveats discussed above that I didn\u2019t refer to in my demo, but we\u2019ll start with supporting via a feature flag and continue from there.\r\n\r\n### setup_rabbitmq_cluster.sh\r\nRun this script to create 3 rabbitmq nodes in a cluster with a single leader.\r\n```bash\r\n#!/bin/bash\r\n\r\nERLANG_COOKIE=\"MYSECRETCOOKIE\"\r\n\r\ncleanup() {\r\n    echo \"Stopping and removing existing RabbitMQ containers...\"\r\n    docker stop rabbit1 rabbit2 rabbit3 2>/dev/null\r\n    docker rm rabbit1 rabbit2 rabbit3 2>/dev/null\r\n\r\n    echo \"Removing existing Docker network...\"\r\n    docker network rm rabbitmq-cluster 2>/dev/null\r\n}\r\n\r\nwait_for_container() {\r\n    local container_name=$1\r\n    local retries=20\r\n    local count=0\r\n\r\n    until [ \"$(docker inspect -f {{.State.Running}} $container_name)\" == \"true\" ]; do\r\n        sleep 1\r\n        count=$((count + 1))\r\n        if [ $count -ge $retries ]; then\r\n            echo \"Error: Container $container_name did not start in time.\"\r\n            exit 1\r\n        fi\r\n    done\r\n}\r\n\r\nwait_for_rabbitmq() {\r\n    local container_name=$1\r\n    local retries=10\r\n    local count=0\r\n\r\n    until docker exec -it $container_name rabbitmqctl status; do\r\n        sleep 1\r\n        count=$((count + 1))\r\n        if [ $count -ge $retries ]; then\r\n            echo \"Error: RabbitMQ in container $container_name did not start in time.\"\r\n            exit 1\r\n        fi\r\n    done\r\n}\r\n\r\nsetup_cluster() {\r\n    echo \"Creating Docker network for RabbitMQ cluster...\"\r\n    docker network create rabbitmq-cluster\r\n\r\n    echo \"Starting rabbit1 container...\"\r\n    docker run -d --rm --name rabbit1 --hostname rabbit1 --net rabbitmq-cluster \\\r\n        -e RABBITMQ_NODENAME=rabbit@rabbit1 \\\r\n        -e RABBITMQ_ERLANG_COOKIE=$ERLANG_COOKIE \\\r\n        --net-alias rabbit1 \\\r\n        -p 15672:15672 -p 5672:5672 rabbitmq:3-management\r\n\r\n    sleep 5\r\n    wait_for_container rabbit1\r\n    wait_for_rabbitmq rabbit1\r\n\r\n    # echo \"Installing netcat in rabbit1 for debugging purposes...\"\r\n    # docker exec -it rabbit1 bash -c \"apt-get update && apt-get install -y netcat\"\r\n\r\n    echo \"Starting rabbit2 container...\"\r\n    docker run -d --rm --name rabbit2 --hostname rabbit2 --net rabbitmq-cluster \\\r\n        -e RABBITMQ_NODENAME=rabbit@rabbit2 \\\r\n        -e RABBITMQ_ERLANG_COOKIE=$ERLANG_COOKIE \\\r\n        --net-alias rabbit2 \\\r\n        -p 15673:15672 -p 5673:5672 rabbitmq:3-management\r\n\r\n    sleep 5\r\n    wait_for_container rabbit2\r\n    wait_for_rabbitmq rabbit2\r\n\r\n    # echo \"Installing netcat in rabbit2 for debugging purposes...\"\r\n    # docker exec -it rabbit2 bash -c \"apt-get update && apt-get install -y netcat\"\r\n\r\n    echo \"Starting rabbit3 container...\"\r\n    docker run -d --rm --name rabbit3 --hostname rabbit3 --net rabbitmq-cluster \\\r\n        -e RABBITMQ_NODENAME=rabbit@rabbit3 \\\r\n        -e RABBITMQ_ERLANG_COOKIE=$ERLANG_COOKIE \\\r\n        --net-alias rabbit3 \\\r\n        -p 15674:15672 -p 5674:5672 rabbitmq:3-management\r\n\r\n    sleep 5\r\n    wait_for_container rabbit3\r\n    wait_for_rabbitmq rabbit3\r\n\r\n    # echo \"Installing netcat in rabbit3 for debugging purposes...\"\r\n    # docker exec -it rabbit3 bash -c \"apt-get update && apt-get install -y netcat\"\r\n\r\n    echo \"Joining rabbit2 to the cluster...\"\r\n    docker exec -it rabbit2 rabbitmqctl stop_app\r\n    docker exec -it rabbit2 rabbitmqctl reset\r\n    docker exec -it rabbit2 rabbitmqctl join_cluster rabbit@rabbit1\r\n    if [ $? -ne 0 ]; then\r\n        echo \"Error: Failed to join rabbit2 to the cluster.\"\r\n        exit 1\r\n    fi\r\n    docker exec -it rabbit2 rabbitmqctl start_app\r\n\r\n    echo \"Joining rabbit3 to the cluster...\"\r\n    docker exec -it rabbit3 rabbitmqctl stop_app\r\n    docker exec -it rabbit3 rabbitmqctl reset\r\n    docker exec -it rabbit3 rabbitmqctl join_cluster rabbit@rabbit1\r\n    if [ $? -ne 0 ]; then\r\n        echo \"Error: Failed to join rabbit3 to the cluster.\"\r\n        exit 1\r\n    fi\r\n    docker exec -it rabbit3 rabbitmqctl start_app\r\n\r\n    echo \"Verifying cluster status from rabbit1...\"\r\n    docker exec -it rabbit1 rabbitmqctl cluster_status\r\n}\r\n\r\ncleanup\r\nsetup_cluster\r\n\r\necho \"RabbitMQ cluster setup is complete.\"\r\n\r\n```\r\n\r\n### setup_quorum_queue.sh\r\nThis is used to quickly test the cluster creation script, but it is not necessary as the next script will use Kombu directly instead of this script.\r\n```bash\r\n#!/bin/bash\r\n\r\nQUEUE_NAME=\"my-quorum-queue\"\r\nVHOST=\"/\"\r\n\r\nremove_existing_queue() {\r\n  docker exec -it rabbit1 rabbitmqctl delete_queue $QUEUE_NAME\r\n}\r\n\r\ncreate_quorum_queue() {\r\n  docker exec -it rabbit1 rabbitmqadmin declare queue name=$QUEUE_NAME durable=true arguments='{\"x-queue-type\":\"quorum\"}'\r\n}\r\n\r\nverify_quorum_queue() {\r\n  docker exec -it rabbit1 rabbitmqctl list_queues name type durable auto_delete arguments | grep $QUEUE_NAME\r\n}\r\n\r\nsend_test_message() {\r\n  docker exec -it rabbit1 rabbitmqadmin publish exchange=amq.default routing_key=$QUEUE_NAME payload='Hello, RabbitMQ!'\r\n}\r\n\r\nreceive_test_message() {\r\n  docker exec -it rabbit1 rabbitmqadmin get queue=$QUEUE_NAME ackmode=ack_requeue_false\r\n}\r\n\r\necho \"Removing existing quorum queue if it exists...\"\r\nremove_existing_queue\r\n\r\necho \"Creating quorum queue...\"\r\ncreate_quorum_queue\r\n\r\necho \"Verifying quorum queue...\"\r\nverify_quorum_queue\r\n\r\necho \"Sending test message...\"\r\nsend_test_message\r\n\r\necho \"Receiving test message...\"\r\nreceive_test_message\r\n\r\necho \"Quorum queue setup and message test completed successfully.\"\r\n\r\n```\r\n\r\n### create_quorum_queues.py\r\nThis script will create a quorum queue using Kombu on our setup.\r\n```python\r\nfrom kombu import Connection, Exchange, Queue\r\n\r\nmy_quorum_queue = Queue(\r\n    \"my-quorum-queue\",\r\n    Exchange(\"default\"),\r\n    routing_key=\"default\",\r\n    queue_arguments={\"x-queue-type\": \"quorum\"},\r\n)\r\n\r\nwith Connection(\"amqp://guest@localhost//\") as conn:\r\n    channel = conn.channel()\r\n    my_quorum_queue.maybe_bind(conn)\r\n    my_quorum_queue.declare()\r\n\r\n```\r\n\r\n### myapp.py\r\nYou can replace the `myapp.py` from `celery/examples/app` to run it locally for quick testing.\r\n```python\r\nfrom create_quorum_queues import my_quorum_queue\r\nfrom kombu.common import QoS\r\n\r\nfrom celery import Celery, bootsteps\r\nfrom celery.canvas import group\r\n\r\n\r\nclass NoChannelGlobalQoS(bootsteps.StartStopStep):\r\n    requires = {\"celery.worker.consumer.tasks:Tasks\"}\r\n\r\n    def start(self, c):\r\n        qos_global = False\r\n        c.connection.default_channel.basic_qos(\r\n            0,\r\n            c.initial_prefetch_count,\r\n            qos_global,\r\n        )\r\n\r\n        def set_prefetch_count(prefetch_count):\r\n            return c.task_consumer.qos(\r\n                prefetch_count=prefetch_count,\r\n                apply_global=qos_global,\r\n            )\r\n\r\n        c.qos = QoS(set_prefetch_count, c.initial_prefetch_count)\r\n\r\n\r\napp = Celery(\"myapp\", broker=\"amqp://guest@localhost:5672//\")\r\napp.steps[\"consumer\"].add(NoChannelGlobalQoS)\r\napp.conf.task_queues = (my_quorum_queue,)\r\n\r\n\r\n@app.task\r\ndef add(x, y):\r\n    return x + y\r\n\r\n\r\n@app.task\r\ndef identity(x):\r\n    return x\r\n\r\n\r\ndef test():\r\n    while True:\r\n        print(\"Celery Quorum Queue POC\")\r\n        print(\"=======================\")\r\n        print(\"1. Send a simple identity task to the quorum queue\")\r\n        print(\"2. Send a group of add tasks to the quorum queue\")\r\n        print(\"3. Inspect the active queues\")\r\n        print(\"4. Shutdown Celery worker\")\r\n        print(\"Q. Quit\")\r\n        choice = input(\"Enter your choice (1-4 or Q): \")\r\n\r\n        if choice == \"1\":\r\n            payload = \"Hello, Quorum Queue!\"\r\n            result = identity.si(payload).apply_async(queue=my_quorum_queue.name)\r\n            print()\r\n            print(f\"Task sent to the quorum queue with ID: {result.id}\")\r\n            print(\"Task type: identity\")\r\n            print(f\"Payload: {payload}\")\r\n\r\n        elif choice == \"2\":\r\n            tasks = [\r\n                (1, 2),\r\n                (3, 4),\r\n                (5, 6),\r\n            ]\r\n            result = group(\r\n                add.s(*tasks[0]),\r\n                add.s(*tasks[1]),\r\n                add.s(*tasks[2]),\r\n            ).apply_async(queue=my_quorum_queue.name)\r\n            print()\r\n            print(\"Group of tasks sent to the quorum queue.\")\r\n            print(f\"Group result ID: {result.id}\")\r\n            for i, task_args in enumerate(tasks, 1):\r\n                print(f\"Task {i} type: add\")\r\n                print(f\"Payload: {task_args}\")\r\n\r\n        elif choice == \"3\":\r\n            active_queues = app.control.inspect().active_queues()\r\n            print()\r\n            print(\"Active queues:\")\r\n            for worker, queues in active_queues.items():\r\n                print(f\"Worker: {worker}\")\r\n                for queue in queues:\r\n                    print(f\"  - {queue['name']}\")\r\n\r\n        elif choice == \"4\":\r\n            print(\"Shutting down Celery worker...\")\r\n            app.control.shutdown()\r\n\r\n        elif choice.lower() == \"q\":\r\n            print(\"Quitting test()\")\r\n            break\r\n\r\n        else:\r\n            print(\"Invalid choice. Please enter a number between 1 and 4 or Q to quit.\")\r\n\r\n        print('\\n' + '#' * 80 + '\\n')\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    app.start()\r\n\r\n```\r\n\r\nDon\u2019t forget to run `chmod +x` on the scripts and place them all in `examples/app`.\r\n\r\n## Demo\r\n\r\n### Creating the RabbitMQ cluster\r\n\r\n```console\r\n./setup_rabbitmq_cluster.sh\r\nStopping and removing existing RabbitMQ containers...\r\nRemoving existing Docker network...\r\nrabbitmq-cluster\r\nCreating Docker network for RabbitMQ cluster...\r\nd967a87f89766fda48fccca07dbc02f445e55d55c07a915d0548be5c93c71a6b\r\nStarting rabbit1 container...\r\ne27a94922247996056c9c40ea4e6b5c2a3e6fd27b319e7a44515a3e47b127b62\r\nStatus of node rabbit@rabbit1 ...\r\nRuntime\r\n\r\nOS PID: 20\r\nOS: Linux\r\nUptime (seconds): 5\r\nIs under maintenance?: false\r\nRabbitMQ version: 3.13.3\r\nRabbitMQ release series support status: supported\r\nNode name: rabbit@rabbit1\r\nErlang configuration: Erlang/OTP 26 [erts-14.2.5] [source] [64-bit] [smp:10:10] [ds:10:10:10] [async-threads:1] [jit]\r\nCrypto library: OpenSSL 3.1.6 4 Jun 2024\r\nErlang processes: 449 used, 1048576 limit\r\nScheduler run queue: 1\r\nCluster heartbeat timeout (net_ticktime): 60\r\n\r\nPlugins\r\n\r\nEnabled plugin file: /etc/rabbitmq/enabled_plugins\r\nEnabled plugins:\r\n\r\n * rabbitmq_prometheus\r\n * rabbitmq_federation\r\n * accept\r\n * prometheus\r\n * rabbitmq_management\r\n * rabbitmq_management_agent\r\n * rabbitmq_web_dispatch\r\n * amqp_client\r\n * cowboy\r\n * cowlib\r\n * oauth2_client\r\n * jose\r\n\r\nData directory\r\n\r\nNode data directory: /var/lib/rabbitmq/mnesia/rabbit@rabbit1\r\nRaft data directory: /var/lib/rabbitmq/mnesia/rabbit@rabbit1/quorum/rabbit@rabbit1\r\n\r\nConfig files\r\n\r\n * /etc/rabbitmq/conf.d/10-defaults.conf\r\n\r\nLog file(s)\r\n\r\n * <stdout>\r\n\r\nAlarms\r\n\r\n(none)\r\n\r\nMemory\r\n\r\nTotal memory used: 0.2188 gb\r\nCalculation strategy: rss\r\nMemory high watermark setting: 0.4 of available memory, computed to: 1.6453 gb\r\n\r\nreserved_unallocated: 0.1411 gb (64.48 %)\r\ncode: 0.0359 gb (16.41 %)\r\nother_system: 0.0278 gb (12.7 %)\r\nother_proc: 0.015 gb (6.88 %)\r\nother_ets: 0.0024 gb (1.12 %)\r\natom: 0.0018 gb (0.84 %)\r\nmetrics: 0.0012 gb (0.56 %)\r\nplugins: 0.0003 gb (0.12 %)\r\nmsg_index: 0.0003 gb (0.12 %)\r\nmgmt_db: 0.0001 gb (0.06 %)\r\nmnesia: 0.0001 gb (0.03 %)\r\nbinary: 0.0001 gb (0.03 %)\r\nmetadata_store: 0.0 gb (0.02 %)\r\nmetadata_store_ets: 0.0 gb (0.01 %)\r\nquorum_ets: 0.0 gb (0.01 %)\r\nconnection_other: 0.0 gb (0.0 %)\r\nquorum_queue_procs: 0.0 gb (0.0 %)\r\nquorum_queue_dlx_procs: 0.0 gb (0.0 %)\r\nstream_queue_procs: 0.0 gb (0.0 %)\r\nstream_queue_replica_reader_procs: 0.0 gb (0.0 %)\r\nconnection_readers: 0.0 gb (0.0 %)\r\nconnection_writers: 0.0 gb (0.0 %)\r\nconnection_channels: 0.0 gb (0.0 %)\r\nqueue_procs: 0.0 gb (0.0 %)\r\nqueue_slave_procs: 0.0 gb (0.0 %)\r\nstream_queue_coordinator_procs: 0.0 gb (0.0 %)\r\nallocated_unused: 0.0 gb (0.0 %)\r\n\r\nFile Descriptors\r\n\r\nTotal: 0, limit: 1048479\r\nSockets: 0, limit: 943629\r\n\r\nFree Disk Space\r\n\r\nLow free disk space watermark: 0.05 gb\r\nFree disk space: 23.0974 gb\r\n\r\nTotals\r\n\r\nConnection count: 0\r\nQueue count: 0\r\nVirtual host count: 1\r\n\r\nListeners\r\n\r\nInterface: [::], port: 15672, protocol: http, purpose: HTTP API\r\nInterface: [::], port: 15692, protocol: http/prometheus, purpose: Prometheus exporter API over HTTP\r\nInterface: [::], port: 25672, protocol: clustering, purpose: inter-node and CLI tool communication\r\nInterface: [::], port: 5672, protocol: amqp, purpose: AMQP 0-9-1 and AMQP 1.0\r\n\r\nWhat's next:\r\n    Try Docker Debug for seamless, persistent debugging tools in any container or image \u2192 docker debug rabbit1\r\n    Learn more at https://docs.docker.com/go/debug-cli/\r\nStarting rabbit2 container...\r\nf71cca72920bb6455024310119f4da68cfa24523eba87143166e792ccf377c6c\r\nStatus of node rabbit@rabbit2 ...\r\nRuntime\r\n\r\nOS PID: 19\r\nOS: Linux\r\nUptime (seconds): 5\r\nIs under maintenance?: false\r\nRabbitMQ version: 3.13.3\r\nRabbitMQ release series support status: supported\r\nNode name: rabbit@rabbit2\r\nErlang configuration: Erlang/OTP 26 [erts-14.2.5] [source] [64-bit] [smp:10:10] [ds:10:10:10] [async-threads:1] [jit]\r\nCrypto library: OpenSSL 3.1.6 4 Jun 2024\r\nErlang processes: 449 used, 1048576 limit\r\nScheduler run queue: 1\r\nCluster heartbeat timeout (net_ticktime): 60\r\n\r\nPlugins\r\n\r\nEnabled plugin file: /etc/rabbitmq/enabled_plugins\r\nEnabled plugins:\r\n\r\n * rabbitmq_prometheus\r\n * rabbitmq_federation\r\n * accept\r\n * prometheus\r\n * rabbitmq_management\r\n * rabbitmq_management_agent\r\n * rabbitmq_web_dispatch\r\n * amqp_client\r\n * cowboy\r\n * cowlib\r\n * oauth2_client\r\n * jose\r\n\r\nData directory\r\n\r\nNode data directory: /var/lib/rabbitmq/mnesia/rabbit@rabbit2\r\nRaft data directory: /var/lib/rabbitmq/mnesia/rabbit@rabbit2/quorum/rabbit@rabbit2\r\n\r\nConfig files\r\n\r\n * /etc/rabbitmq/conf.d/10-defaults.conf\r\n\r\nLog file(s)\r\n\r\n * <stdout>\r\n\r\nAlarms\r\n\r\n(none)\r\n\r\nMemory\r\n\r\nTotal memory used: 0.2208 gb\r\nCalculation strategy: rss\r\nMemory high watermark setting: 0.4 of available memory, computed to: 1.6453 gb\r\n\r\nreserved_unallocated: 0.1473 gb (66.71 %)\r\ncode: 0.0359 gb (16.26 %)\r\nother_system: 0.0278 gb (12.58 %)\r\nother_proc: 0.015 gb (6.79 %)\r\nother_ets: 0.0024 gb (1.11 %)\r\natom: 0.0018 gb (0.83 %)\r\nmetrics: 0.0012 gb (0.56 %)\r\nplugins: 0.0003 gb (0.12 %)\r\nmsg_index: 0.0003 gb (0.12 %)\r\nmgmt_db: 0.0001 gb (0.06 %)\r\nmnesia: 0.0001 gb (0.03 %)\r\nbinary: 0.0001 gb (0.03 %)\r\nmetadata_store: 0.0 gb (0.02 %)\r\nmetadata_store_ets: 0.0 gb (0.01 %)\r\nquorum_ets: 0.0 gb (0.01 %)\r\nconnection_other: 0.0 gb (0.0 %)\r\nquorum_queue_procs: 0.0 gb (0.0 %)\r\nquorum_queue_dlx_procs: 0.0 gb (0.0 %)\r\nstream_queue_procs: 0.0 gb (0.0 %)\r\nstream_queue_replica_reader_procs: 0.0 gb (0.0 %)\r\nconnection_readers: 0.0 gb (0.0 %)\r\nconnection_writers: 0.0 gb (0.0 %)\r\nconnection_channels: 0.0 gb (0.0 %)\r\nqueue_procs: 0.0 gb (0.0 %)\r\nqueue_slave_procs: 0.0 gb (0.0 %)\r\nstream_queue_coordinator_procs: 0.0 gb (0.0 %)\r\nallocated_unused: 0.0 gb (0.0 %)\r\n\r\nFile Descriptors\r\n\r\nTotal: 0, limit: 1048479\r\nSockets: 0, limit: 943629\r\n\r\nFree Disk Space\r\n\r\nLow free disk space watermark: 0.05 gb\r\nFree disk space: 23.097 gb\r\n\r\nTotals\r\n\r\nConnection count: 0\r\nQueue count: 0\r\nVirtual host count: 1\r\n\r\nListeners\r\n\r\nInterface: [::], port: 15672, protocol: http, purpose: HTTP API\r\nInterface: [::], port: 15692, protocol: http/prometheus, purpose: Prometheus exporter API over HTTP\r\nInterface: [::], port: 25672, protocol: clustering, purpose: inter-node and CLI tool communication\r\nInterface: [::], port: 5672, protocol: amqp, purpose: AMQP 0-9-1 and AMQP 1.0\r\n\r\nWhat's next:\r\n    Try Docker Debug for seamless, persistent debugging tools in any container or image \u2192 docker debug rabbit2\r\n    Learn more at https://docs.docker.com/go/debug-cli/\r\nStarting rabbit3 container...\r\n3ce7716154751b5a9dad78270ba3949f648f6f6b27a612c78cc429b054862226\r\nStatus of node rabbit@rabbit3 ...\r\nRuntime\r\n\r\nOS PID: 20\r\nOS: Linux\r\nUptime (seconds): 5\r\nIs under maintenance?: false\r\nRabbitMQ version: 3.13.3\r\nRabbitMQ release series support status: supported\r\nNode name: rabbit@rabbit3\r\nErlang configuration: Erlang/OTP 26 [erts-14.2.5] [source] [64-bit] [smp:10:10] [ds:10:10:10] [async-threads:1] [jit]\r\nCrypto library: OpenSSL 3.1.6 4 Jun 2024\r\nErlang processes: 451 used, 1048576 limit\r\nScheduler run queue: 1\r\nCluster heartbeat timeout (net_ticktime): 60\r\n\r\nPlugins\r\n\r\nEnabled plugin file: /etc/rabbitmq/enabled_plugins\r\nEnabled plugins:\r\n\r\n * rabbitmq_prometheus\r\n * rabbitmq_federation\r\n * accept\r\n * prometheus\r\n * rabbitmq_management\r\n * rabbitmq_management_agent\r\n * rabbitmq_web_dispatch\r\n * amqp_client\r\n * cowboy\r\n * cowlib\r\n * oauth2_client\r\n * jose\r\n\r\nData directory\r\n\r\nNode data directory: /var/lib/rabbitmq/mnesia/rabbit@rabbit3\r\nRaft data directory: /var/lib/rabbitmq/mnesia/rabbit@rabbit3/quorum/rabbit@rabbit3\r\n\r\nConfig files\r\n\r\n * /etc/rabbitmq/conf.d/10-defaults.conf\r\n\r\nLog file(s)\r\n\r\n * <stdout>\r\n\r\nAlarms\r\n\r\n(none)\r\n\r\nMemory\r\n\r\nTotal memory used: 0.2253 gb\r\nCalculation strategy: rss\r\nMemory high watermark setting: 0.4 of available memory, computed to: 1.6453 gb\r\n\r\nreserved_unallocated: 0.1492 gb (66.21 %)\r\ncode: 0.0359 gb (15.94 %)\r\nother_system: 0.0278 gb (12.33 %)\r\nother_proc: 0.015 gb (6.67 %)\r\nother_ets: 0.0024 gb (1.08 %)\r\natom: 0.0018 gb (0.81 %)\r\nmetrics: 0.0012 gb (0.55 %)\r\nplugins: 0.0003 gb (0.12 %)\r\nmsg_index: 0.0003 gb (0.11 %)\r\nmgmt_db: 0.0001 gb (0.06 %)\r\nmnesia: 0.0001 gb (0.03 %)\r\nbinary: 0.0001 gb (0.03 %)\r\nmetadata_store: 0.0 gb (0.02 %)\r\nmetadata_store_ets: 0.0 gb (0.01 %)\r\nquorum_ets: 0.0 gb (0.01 %)\r\nconnection_other: 0.0 gb (0.0 %)\r\nquorum_queue_procs: 0.0 gb (0.0 %)\r\nquorum_queue_dlx_procs: 0.0 gb (0.0 %)\r\nstream_queue_procs: 0.0 gb (0.0 %)\r\nstream_queue_replica_reader_procs: 0.0 gb (0.0 %)\r\nconnection_readers: 0.0 gb (0.0 %)\r\nconnection_writers: 0.0 gb (0.0 %)\r\nconnection_channels: 0.0 gb (0.0 %)\r\nqueue_procs: 0.0 gb (0.0 %)\r\nqueue_slave_procs: 0.0 gb (0.0 %)\r\nstream_queue_coordinator_procs: 0.0 gb (0.0 %)\r\nallocated_unused: 0.0 gb (0.0 %)\r\n\r\nFile Descriptors\r\n\r\nTotal: 0, limit: 1048479\r\nSockets: 0, limit: 943629\r\n\r\nFree Disk Space\r\n\r\nLow free disk space watermark: 0.05 gb\r\nFree disk space: 23.0965 gb\r\n\r\nTotals\r\n\r\nConnection count: 0\r\nQueue count: 0\r\nVirtual host count: 1\r\n\r\nListeners\r\n\r\nInterface: [::], port: 15672, protocol: http, purpose: HTTP API\r\nInterface: [::], port: 15692, protocol: http/prometheus, purpose: Prometheus exporter API over HTTP\r\nInterface: [::], port: 25672, protocol: clustering, purpose: inter-node and CLI tool communication\r\nInterface: [::], port: 5672, protocol: amqp, purpose: AMQP 0-9-1 and AMQP 1.0\r\n\r\nWhat's next:\r\n    Try Docker Debug for seamless, persistent debugging tools in any container or image \u2192 docker debug rabbit3\r\n    Learn more at https://docs.docker.com/go/debug-cli/\r\nJoining rabbit2 to the cluster...\r\nStopping rabbit application on node rabbit@rabbit2 ...\r\n\r\nWhat's next:\r\n    Try Docker Debug for seamless, persistent debugging tools in any container or image \u2192 docker debug rabbit2\r\n    Learn more at https://docs.docker.com/go/debug-cli/\r\nResetting node rabbit@rabbit2 ...\r\n\r\nWhat's next:\r\n    Try Docker Debug for seamless, persistent debugging tools in any container or image \u2192 docker debug rabbit2\r\n    Learn more at https://docs.docker.com/go/debug-cli/\r\nClustering node rabbit@rabbit2 with rabbit@rabbit1\r\n\r\nWhat's next:\r\n    Try Docker Debug for seamless, persistent debugging tools in any container or image \u2192 docker debug rabbit2\r\n    Learn more at https://docs.docker.com/go/debug-cli/\r\nStarting node rabbit@rabbit2 ...\r\n\r\nWhat's next:\r\n    Try Docker Debug for seamless, persistent debugging tools in any container or image \u2192 docker debug rabbit2\r\n    Learn more at https://docs.docker.com/go/debug-cli/\r\nJoining rabbit3 to the cluster...\r\nStopping rabbit application on node rabbit@rabbit3 ...\r\n\r\nWhat's next:\r\n    Try Docker Debug for seamless, persistent debugging tools in any container or image \u2192 docker debug rabbit3\r\n    Learn more at https://docs.docker.com/go/debug-cli/\r\nResetting node rabbit@rabbit3 ...\r\n\r\nWhat's next:\r\n    Try Docker Debug for seamless, persistent debugging tools in any container or image \u2192 docker debug rabbit3\r\n    Learn more at https://docs.docker.com/go/debug-cli/\r\nClustering node rabbit@rabbit3 with rabbit@rabbit1\r\n\r\nWhat's next:\r\n    Try Docker Debug for seamless, persistent debugging tools in any container or image \u2192 docker debug rabbit3\r\n    Learn more at https://docs.docker.com/go/debug-cli/\r\nStarting node rabbit@rabbit3 ...\r\n\r\nWhat's next:\r\n    Try Docker Debug for seamless, persistent debugging tools in any container or image \u2192 docker debug rabbit3\r\n    Learn more at https://docs.docker.com/go/debug-cli/\r\nVerifying cluster status from rabbit1...\r\nCluster status of node rabbit@rabbit1 ...\r\nBasics\r\n\r\nCluster name: rabbit@rabbit1\r\nTotal CPU cores available cluster-wide: 30\r\n\r\nDisk Nodes\r\n\r\nrabbit@rabbit1\r\nrabbit@rabbit2\r\nrabbit@rabbit3\r\n\r\nRunning Nodes\r\n\r\nrabbit@rabbit1\r\nrabbit@rabbit2\r\nrabbit@rabbit3\r\n\r\nVersions\r\n\r\nrabbit@rabbit1: RabbitMQ 3.13.3 on Erlang 26.2.5\r\nrabbit@rabbit2: RabbitMQ 3.13.3 on Erlang 26.2.5\r\nrabbit@rabbit3: RabbitMQ 3.13.3 on Erlang 26.2.5\r\n\r\nCPU Cores\r\n\r\nNode: rabbit@rabbit1, available CPU cores: 10\r\nNode: rabbit@rabbit2, available CPU cores: 10\r\nNode: rabbit@rabbit3, available CPU cores: 10\r\n\r\nMaintenance status\r\n\r\nNode: rabbit@rabbit1, status: not under maintenance\r\nNode: rabbit@rabbit2, status: not under maintenance\r\nNode: rabbit@rabbit3, status: not under maintenance\r\n\r\nAlarms\r\n\r\n(none)\r\n\r\nNetwork Partitions\r\n\r\n(none)\r\n\r\nListeners\r\n\r\nNode: rabbit@rabbit1, interface: [::], port: 15672, protocol: http, purpose: HTTP API\r\nNode: rabbit@rabbit1, interface: [::], port: 15692, protocol: http/prometheus, purpose: Prometheus exporter API over HTTP\r\nNode: rabbit@rabbit1, interface: [::], port: 25672, protocol: clustering, purpose: inter-node and CLI tool communication\r\nNode: rabbit@rabbit1, interface: [::], port: 5672, protocol: amqp, purpose: AMQP 0-9-1 and AMQP 1.0\r\nNode: rabbit@rabbit2, interface: [::], port: 15672, protocol: http, purpose: HTTP API\r\nNode: rabbit@rabbit2, interface: [::], port: 15692, protocol: http/prometheus, purpose: Prometheus exporter API over HTTP\r\nNode: rabbit@rabbit2, interface: [::], port: 25672, protocol: clustering, purpose: inter-node and CLI tool communication\r\nNode: rabbit@rabbit2, interface: [::], port: 5672, protocol: amqp, purpose: AMQP 0-9-1 and AMQP 1.0\r\nNode: rabbit@rabbit3, interface: [::], port: 15672, protocol: http, purpose: HTTP API\r\nNode: rabbit@rabbit3, interface: [::], port: 15692, protocol: http/prometheus, purpose: Prometheus exporter API over HTTP\r\nNode: rabbit@rabbit3, interface: [::], port: 25672, protocol: clustering, purpose: inter-node and CLI tool communication\r\nNode: rabbit@rabbit3, interface: [::], port: 5672, protocol: amqp, purpose: AMQP 0-9-1 and AMQP 1.0\r\n\r\nFeature flags\r\n\r\nFlag: classic_mirrored_queue_version, state: enabled\r\nFlag: classic_queue_type_delivery_support, state: enabled\r\nFlag: detailed_queues_endpoint, state: enabled\r\nFlag: direct_exchange_routing_v2, state: enabled\r\nFlag: drop_unroutable_metric, state: enabled\r\nFlag: empty_basic_get_metric, state: enabled\r\nFlag: feature_flags_v2, state: enabled\r\nFlag: implicit_default_bindings, state: enabled\r\nFlag: khepri_db, state: disabled\r\nFlag: listener_records_in_ets, state: enabled\r\nFlag: maintenance_mode_status, state: enabled\r\nFlag: message_containers, state: enabled\r\nFlag: message_containers_deaths_v2, state: enabled\r\nFlag: quorum_queue, state: enabled\r\nFlag: quorum_queue_non_voters, state: enabled\r\nFlag: restart_streams, state: enabled\r\nFlag: stream_filtering, state: enabled\r\nFlag: stream_queue, state: enabled\r\nFlag: stream_sac_coordinator_unblock_group, state: enabled\r\nFlag: stream_single_active_consumer, state: enabled\r\nFlag: stream_update_config_command, state: enabled\r\nFlag: tracking_records_in_ets, state: enabled\r\nFlag: user_limits, state: enabled\r\nFlag: virtual_host_metadata, state: enabled\r\n\r\nWhat's next:\r\n    Try Docker Debug for seamless, persistent debugging tools in any container or image \u2192 docker debug rabbit1\r\n    Learn more at https://docs.docker.com/go/debug-cli/\r\nRabbitMQ cluster setup is complete.\r\n```\r\n\r\n### Checking the Cluster is working\r\n\r\n```console\r\n./setup_quorum_queue.sh\r\nRemoving existing quorum queue if it exists...\r\nDeleting queue 'my-quorum-queue' on vhost '/' ...\r\nError:\r\nQueue not found\r\n\r\nWhat's next:\r\n    Try Docker Debug for seamless, persistent debugging tools in any container or image \u2192 docker debug rabbit1\r\n    Learn more at https://docs.docker.com/go/debug-cli/\r\nCreating quorum queue...\r\nqueue declared\r\n\r\nWhat's next:\r\n    Try Docker Debug for seamless, persistent debugging tools in any container or image \u2192 docker debug rabbit1\r\n    Learn more at https://docs.docker.com/go/debug-cli/\r\nVerifying quorum queue...\r\nmy-quorum-queue\tquorum\ttrue\tfalse\t[{\"x-queue-type\",\"quorum\"}]\r\nSending test message...\r\nMessage published\r\n\r\nWhat's next:\r\n    Try Docker Debug for seamless, persistent debugging tools in any container or image \u2192 docker debug rabbit1\r\n    Learn more at https://docs.docker.com/go/debug-cli/\r\nReceiving test message...\r\n+-----------------+----------+---------------+------------------+---------------+------------------+------------+-------------+\r\n|   routing_key   | exchange | message_count |     payload      | payload_bytes | payload_encoding | properties | redelivered |\r\n+-----------------+----------+---------------+------------------+---------------+------------------+------------+-------------+\r\n| my-quorum-queue |          | 0             | Hello, RabbitMQ! | 16            | string           |            | False       |\r\n+-----------------+----------+---------------+------------------+---------------+------------------+------------+-------------+\r\n\r\nWhat's next:\r\n    Try Docker Debug for seamless, persistent debugging tools in any container or image \u2192 docker debug rabbit1\r\n    Learn more at https://docs.docker.com/go/debug-cli/\r\nQuorum queue setup and message test completed successfully.\r\n```\r\n\r\n![CleanShot 2024-06-12 at 23 57 11@2x](https://github.com/celery/celery/assets/4662342/82c417f1-2c89-4067-bdac-0b72f0a9c4cd)\r\n\r\n![CleanShot 2024-06-12 at 23 57 48@2x](https://github.com/celery/celery/assets/4662342/7163abab-3442-4455-99e5-8a3a7261a9f2)\r\n\r\n### Running Kombu script\r\n\r\nDon\u2019t expect any output though..\r\n\r\n```console\r\npython create_quorum_queues.py\r\n```\r\n\r\n### Celery Shell\r\n\r\n```console\r\ncelery shell\r\nPython 3.12.1 (main, Dec 17 2023, 21:54:54) [Clang 15.0.0 (clang-1500.1.0.2.5)]\r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 8.24.0 -- An enhanced Interactive Python. Type '?' for help.\r\n\r\nIn [1]: from myapp import test\r\n\r\nIn [2]: test()\r\nCelery Quorum Queue POC\r\n=======================\r\n1. Send a simple identity task to the quorum queue\r\n2. Send a group of add tasks to the quorum queue\r\n3. Inspect the active queues\r\n4. Shutdown Celery worker\r\nQ. Quit\r\nEnter your choice (1-4 or Q): 1\r\n\r\nTask sent to the quorum queue with ID: 45f29e34-b2b9-4e4d-9872-242fcf227453\r\nTask type: identity\r\nPayload: Hello, Quorum Queue!\r\n\r\n################################################################################\r\n\r\nCelery Quorum Queue POC\r\n=======================\r\n1. Send a simple identity task to the quorum queue\r\n2. Send a group of add tasks to the quorum queue\r\n3. Inspect the active queues\r\n4. Shutdown Celery worker\r\nQ. Quit\r\nEnter your choice (1-4 or Q): 2\r\n\r\nGroup of tasks sent to the quorum queue.\r\nGroup result ID: 7a515989-7b86-440a-b736-9bb6672890c1\r\nTask 1 type: add\r\nPayload: (1, 2)\r\nTask 2 type: add\r\nPayload: (3, 4)\r\nTask 3 type: add\r\nPayload: (5, 6)\r\n\r\n################################################################################\r\n\r\nCelery Quorum Queue POC\r\n=======================\r\n1. Send a simple identity task to the quorum queue\r\n2. Send a group of add tasks to the quorum queue\r\n3. Inspect the active queues\r\n4. Shutdown Celery worker\r\nQ. Quit\r\nEnter your choice (1-4 or Q): 3\r\n\r\nActive queues:\r\nWorker: celery@Tomers-MacBook-Pro.local\r\n  - my-quorum-queue\r\n\r\n################################################################################\r\n\r\nCelery Quorum Queue POC\r\n=======================\r\n1. Send a simple identity task to the quorum queue\r\n2. Send a group of add tasks to the quorum queue\r\n3. Inspect the active queues\r\n4. Shutdown Celery worker\r\nQ. Quit\r\nEnter your choice (1-4 or Q): 4\r\nShutting down Celery worker...\r\n\r\n################################################################################\r\n\r\nCelery Quorum Queue POC\r\n=======================\r\n1. Send a simple identity task to the quorum queue\r\n2. Send a group of add tasks to the quorum queue\r\n3. Inspect the active queues\r\n4. Shutdown Celery worker\r\nQ. Quit\r\nEnter your choice (1-4 or Q): Q\r\nQuitting test()\r\n\r\nIn [3]:\r\n\r\n```\r\n\r\n### Celery Worker\r\n\r\n```console\r\ncelery -A myapp worker -l INFO\r\n\r\n -------------- celery@Tomers-MacBook-Pro.local v5.4.0 (opalescent)\r\n--- ***** -----\r\n-- ******* ---- macOS-14.5-arm64-arm-64bit 2024-06-13 00:00:19\r\n- *** --- * ---\r\n- ** ---------- [config]\r\n- ** ---------- .> app:         myapp:0x10333e600\r\n- ** ---------- .> transport:   amqp://guest:**@localhost:5672//\r\n- ** ---------- .> results:     disabled://\r\n- *** --- * --- .> concurrency: 10 (prefork)\r\n-- ******* ---- .> task events: OFF (enable -E to monitor tasks in this worker)\r\n--- ***** -----\r\n -------------- [queues]\r\n                .> my-quorum-queue  exchange=default(direct) key=default\r\n\r\n\r\n[tasks]\r\n  . myapp.add\r\n  . myapp.identity\r\n\r\n[2024-06-13 00:00:19,937: WARNING/MainProcess] /Users/nusnus/dev/GitHub/celery/celery/worker/consumer/consumer.py:508: CPendingDeprecationWarning: The broker_connection_retry configuration setting will no longer determine\r\nwhether broker connection retries are made during startup in Celery 6.0 and above.\r\nIf you wish to retain the existing behavior for retrying connections on startup,\r\nyou should set broker_connection_retry_on_startup to True.\r\n  warnings.warn(\r\n\r\n[2024-06-13 00:00:19,944: INFO/MainProcess] Connected to amqp://guest:**@127.0.0.1:5672//\r\n[2024-06-13 00:00:19,944: WARNING/MainProcess] /Users/nusnus/dev/GitHub/celery/celery/worker/consumer/consumer.py:508: CPendingDeprecationWarning: The broker_connection_retry configuration setting will no longer determine\r\nwhether broker connection retries are made during startup in Celery 6.0 and above.\r\nIf you wish to retain the existing behavior for retrying connections on startup,\r\nyou should set broker_connection_retry_on_startup to True.\r\n  warnings.warn(\r\n\r\n[2024-06-13 00:00:19,950: INFO/MainProcess] mingle: searching for neighbors\r\n[2024-06-13 00:00:20,980: INFO/MainProcess] mingle: all alone\r\n[2024-06-13 00:00:21,010: INFO/MainProcess] celery@Tomers-MacBook-Pro.local ready.\r\n[2024-06-13 00:00:34,790: INFO/MainProcess] Task myapp.identity[45f29e34-b2b9-4e4d-9872-242fcf227453] received\r\n[2024-06-13 00:00:34,791: INFO/ForkPoolWorker-8] Task myapp.identity[45f29e34-b2b9-4e4d-9872-242fcf227453] succeeded in 0.00043800007551908493s: 'Hello, Quorum Queue!'\r\n[2024-06-13 00:00:38,407: INFO/MainProcess] Task myapp.add[b9b525ba-f4bf-4711-9a27-f67deae69d69] received\r\n[2024-06-13 00:00:38,407: INFO/MainProcess] Task myapp.add[8a1c01f0-ed40-4c91-bd7a-7dc17475ae09] received\r\n[2024-06-13 00:00:38,408: INFO/ForkPoolWorker-8] Task myapp.add[b9b525ba-f4bf-4711-9a27-f67deae69d69] succeeded in 0.00014258408918976784s: 3\r\n[2024-06-13 00:00:38,408: INFO/MainProcess] Task myapp.add[8f4221a4-b716-4350-b0e2-6ab374bfe2e3] received\r\n[2024-06-13 00:00:38,408: INFO/ForkPoolWorker-8] Task myapp.add[8f4221a4-b716-4350-b0e2-6ab374bfe2e3] succeeded in 4.6083005145192146e-05s: 11\r\n[2024-06-13 00:00:38,408: INFO/ForkPoolWorker-1] Task myapp.add[8a1c01f0-ed40-4c91-bd7a-7dc17475ae09] succeeded in 0.00044000009074807167s: 7\r\n[2024-06-13 00:00:51,478: WARNING/MainProcess] Got shutdown from remote\r\n```\r\n\r\n@auvipy FYI\r\n\r\n## Relevant Docs\r\nI\u2019ll share some of the guides and documents I\u2019ve been reviewing while making the above demo, at least until it is officially implemented with proper documentation.\r\n\r\n- https://www.rabbitmq.com/docs/clustering\r\n- https://www.rabbitmq.com/docs/quorum-queues\r\n- https://docs.celeryq.dev/en/stable/userguide/extending.html\r\n- https://docs.celeryq.dev/en/latest/userguide/routing.html\n@Nusnus Fantastic and extensive! I was planning to try a less extensive version of this but you did really great!\n> @Nusnus Fantastic and extensive! I was planning to try a less extensive version of this but you did really great!\r\n\r\nThanks bro \u2764\ufe0f!\r\nNow that I have the most basic setup running locally, I\u2019m looking for bugs and issues when using quorum queues with Celery v5.4 + above patch.\r\n\r\nI see reports here about potential issues with ETA tasks and potential exceptions raised when trying to use quorum queues. I want to investigate it locally with the above setup and see what we are dealing with. I\u2019ll try to reproduce the issues so I can debug it further and learn what\u2019s going on.\r\n\r\nMy first priority is to uncover the unknown unknowns so we can make educated decisions.\r\n\r\nIf anyone has encountered any other issues or has anything else to add, feel free to report here and tag me! Thanks!\n> My first priority is to uncover the unknown unknowns so we can make educated decisions.\r\n\r\nI'd be happy to help with that, would you perhaps have a pip-installable package (could be from a github branch)? \n> > My first priority is to uncover the unknown unknowns so we can make educated decisions.\r\n> \r\n> I'd be happy to help with that, would you perhaps have a pip-installable package (could be from a github branch)?\r\n\r\nMuch appreciated - thank you @miraculixx !\r\nIt will be much more efficient to run against a branch though than to set up an installable package just for this.\r\n\r\nI\u2019ve created a branch in the upstream called `quorumq` - see changes in branch: https://github.com/celery/celery/compare/main...quorumq\r\nIt\u2019s patched for easy POC testing.\r\n\r\nI\u2019ve also made a small change to `myapp.py` to make it a bit easier to control it for this POC testing.\r\n```python\r\napp = Celery(\"myapp\", broker=\"amqp://guest@localhost:5672//\")\r\n\r\napp.conf.worker_quorumq = os.environ.get(\"WORKER_QUORUMQ\", \"False\").lower() == \"true\"\r\n\r\nif app.conf.worker_quorumq:\r\n    app.conf.task_queues = (my_quorum_queue,)\r\n```\r\n\r\n## Usage\r\n### With Quorum Queue (e.g., `qos_global` being False)\r\n1. cd `examples/app`\r\n2. `WORKER_QUORUMQ=True celery -A myapp worker -l INFO`\r\n3. `WORKER_QUORUMQ=True celery shell`\r\n\r\n### Without (e.g., Normally) - [Disabled by default](https://github.com/celery/celery/blob/e6bd433fe7a14779f117eca1d4ffaaeabccef18d/celery/app/defaults.py#L348)\r\nhttps://github.com/celery/celery/blob/e6bd433fe7a14779f117eca1d4ffaaeabccef18d/celery/worker/consumer/tasks.py#L31-L35\r\n1. cd `examples/app`\r\n2. `celery -A myapp worker -l INFO`\r\n3. `celery shell`\nObviously, there are many edge cases to check, but I\u2019ll try to be as simple as I can. Anyone who can double-check me or add more insights is more than welcome to join in!\r\n\r\nI\u2019m trying to see what\u2019s going on with ETA tasks. The most simple check is to run a simple scenario using classic queues and quorum queues, and see if anything catches the eye.\r\n\r\nI\u2019ve made some changes to add an ETA task and disable heartbeats to avoid cluttering the logs. I\u2019ve also set the prefetch multiplier and concurrency to lower values, just to make the numbers small and easier to handle.\r\nTake note everything is based on the `quorumq` branch described in my previous [comment](https://github.com/celery/celery/issues/6067#issuecomment-2177100729) and is running against a 3-nodes cluster, per my `setup_rabbitmq_cluster.sh` script above.\r\n\r\n## myapp.py\r\n```python\r\nimport os\r\nfrom datetime import UTC, datetime, timedelta\r\n\r\nfrom create_quorum_queues import my_quorum_queue\r\n\r\nfrom celery import Celery\r\nfrom celery.canvas import group\r\n\r\napp = Celery(\"myapp\", broker=\"amqp://guest@localhost:5672//\")\r\n\r\napp.conf.worker_quorumq = os.environ.get(\"WORKER_QUORUMQ\", \"False\").lower() == \"true\"\r\n\r\nif app.conf.worker_quorumq:\r\n    app.conf.task_queues = (my_quorum_queue,)\r\n\r\n# Reduce qos to 1*4=4\r\napp.conf.worker_prefetch_multiplier = 1\r\napp.conf.worker_concurrency = 4\r\n\r\n# Reduce logs\r\napp.conf.worker_heartbeat = None\r\napp.conf.broker_heartbeat = 0\r\n\r\n\r\n@app.task\r\ndef add(x, y):\r\n    return x + y\r\n\r\n\r\n@app.task\r\ndef identity(x):\r\n    return x\r\n\r\n\r\ndef test():\r\n    if app.conf.worker_quorumq:\r\n        queue = my_quorum_queue.name\r\n    else:\r\n        queue = \"celery\"\r\n\r\n    while True:\r\n        print(\"Celery Quorum Queue POC\")\r\n        print(\"=======================\")\r\n        print(\"1. Send a simple identity task\")\r\n        print(\"1.1 Send an ETA identity task\")\r\n        print(\"2. Send a group of add tasks\")\r\n        print(\"3. Inspect the active queues\")\r\n        print(\"4. Shutdown Celery worker\")\r\n        print(\"Q. Quit\")\r\n        print(\"Q! Exit\")\r\n        choice = input(\"Enter your choice (1-4 or Q): \")\r\n\r\n        if choice == \"1\" or choice == \"1.1\":\r\n            payload = f\"Hello, {\"Quorum\" if app.conf.worker_quorumq else \"Classic\"} Queue!\"\r\n            eta = datetime.now(UTC) + timedelta(seconds=30)\r\n            if choice == \"1.1\":\r\n                result = identity.si(payload).apply_async(queue=queue, eta=eta)\r\n            else:\r\n                result = identity.si(payload).apply_async(queue=queue)\r\n            print()\r\n            print(f\"Task sent with ID: {result.id}\")\r\n            print(\"Task type: identity\")\r\n\r\n            if choice == \"1.1\":\r\n                print(f\"ETA: {eta}\")\r\n\r\n            print(f\"Payload: {payload}\")\r\n\r\n        elif choice == \"2\":\r\n            tasks = [\r\n                (1, 2),\r\n                (3, 4),\r\n                (5, 6),\r\n            ]\r\n            result = group(\r\n                add.s(*tasks[0]),\r\n                add.s(*tasks[1]),\r\n                add.s(*tasks[2]),\r\n            ).apply_async(queue=queue)\r\n            print()\r\n            print(\"Group of tasks sent.\")\r\n            print(f\"Group result ID: {result.id}\")\r\n            for i, task_args in enumerate(tasks, 1):\r\n                print(f\"Task {i} type: add\")\r\n                print(f\"Payload: {task_args}\")\r\n\r\n        elif choice == \"3\":\r\n            active_queues = app.control.inspect().active_queues()\r\n            print()\r\n            print(\"Active queues:\")\r\n            for worker, queues in active_queues.items():\r\n                print(f\"Worker: {worker}\")\r\n                for queue in queues:\r\n                    print(f\"  - {queue['name']}\")\r\n\r\n        elif choice == \"4\":\r\n            print(\"Shutting down Celery worker...\")\r\n            app.control.shutdown()\r\n\r\n        elif choice.lower() == \"q\":\r\n            print(\"Quitting test()\")\r\n            break\r\n\r\n        elif choice.lower() == \"q!\":\r\n            print(\"Exiting...\")\r\n            os.abort()\r\n\r\n        else:\r\n            print(\"Invalid choice. Please enter a number between 1 and 4 or Q to quit.\")\r\n\r\n        print(\"\\n\" + \"#\" * 80 + \"\\n\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    app.start()\r\n\r\n```\r\n\r\n## Flow\r\n1. Run Celery worker.\r\n2. Send 2 ETA tasks.\r\n3. Wait for 1 of them to finish.\r\n4. Send 3rd ETA task.\r\n5. Observe `basic.qos`\r\n\r\n### Using classic/normal queues\r\n#### WORKER_QUORUMQ=False celery -A myapp worker -l DEBUG\r\n```console\r\nWORKER_QUORUMQ=False celery -A myapp worker -l DEBUG\r\n[2024-06-19 22:22:41,753: DEBUG/MainProcess] | Worker: Preparing bootsteps.\r\n[2024-06-19 22:22:41,754: DEBUG/MainProcess] | Worker: Building graph...\r\n[2024-06-19 22:22:41,754: DEBUG/MainProcess] | Worker: New boot order: {Beat, StateDB, Timer, Hub, Pool, Autoscaler, Consumer}\r\n[2024-06-19 22:22:41,756: DEBUG/MainProcess] | Consumer: Preparing bootsteps.\r\n[2024-06-19 22:22:41,756: DEBUG/MainProcess] | Consumer: Building graph...\r\n[2024-06-19 22:22:41,763: DEBUG/MainProcess] | Consumer: New boot order: {Connection, Events, Heart, Mingle, Tasks, Control, Gossip, Agent, event loop}\r\n\r\n -------------- celery@Tomers-MacBook-Pro.local v5.4.0 (opalescent)\r\n--- ***** -----\r\n-- ******* ---- macOS-14.5-arm64-arm-64bit 2024-06-19 22:22:41\r\n- *** --- * ---\r\n- ** ---------- [config]\r\n- ** ---------- .> app:         myapp:0x102fce240\r\n- ** ---------- .> transport:   amqp://guest:**@localhost:5672//\r\n- ** ---------- .> results:     disabled://\r\n- *** --- * --- .> concurrency: 4 (prefork)\r\n-- ******* ---- .> task events: OFF (enable -E to monitor tasks in this worker)\r\n--- ***** -----\r\n -------------- [queues]\r\n                .> celery           exchange=celery(direct) key=celery\r\n\r\n\r\n[tasks]\r\n  . celery.accumulate\r\n  . celery.backend_cleanup\r\n  . celery.chain\r\n  . celery.chord\r\n  . celery.chord_unlock\r\n  . celery.chunks\r\n  . celery.group\r\n  . celery.map\r\n  . celery.starmap\r\n  . myapp.add\r\n  . myapp.identity\r\n\r\n[2024-06-19 22:22:41,778: DEBUG/MainProcess] | Worker: Starting Hub\r\n[2024-06-19 22:22:41,779: DEBUG/MainProcess] ^-- substep ok\r\n[2024-06-19 22:22:41,779: DEBUG/MainProcess] | Worker: Starting Pool\r\n[2024-06-19 22:22:41,839: DEBUG/MainProcess] ^-- substep ok\r\n[2024-06-19 22:22:41,840: DEBUG/MainProcess] | Worker: Starting Consumer\r\n[2024-06-19 22:22:41,840: DEBUG/MainProcess] | Consumer: Starting Connection\r\n[2024-06-19 22:22:41,841: WARNING/MainProcess] /Users/nusnus/dev/GitHub/celery/celery/worker/consumer/consumer.py:508: CPendingDeprecationWarning: The broker_connection_retry configuration setting will no longer determine\r\nwhether broker connection retries are made during startup in Celery 6.0 and above.\r\nIf you wish to retain the existing behavior for retrying connections on startup,\r\nyou should set broker_connection_retry_on_startup to True.\r\n  warnings.warn(\r\n\r\n[2024-06-19 22:22:41,845: DEBUG/MainProcess] Start from server, version: 0.9, properties: {'capabilities': {'publisher_confirms': True, 'exchange_exchange_bindings': True, 'basic.nack': True, 'consumer_cancel_notify': True, 'connection.blocked': True, 'consumer_priorities': True, 'authentication_failure_close': True, 'per_consumer_qos': True, 'direct_reply_to': True}, 'cluster_name': 'rabbit@rabbit1', 'copyright': 'Copyright (c) 2007-2024 Broadcom Inc and/or its subsidiaries', 'information': 'Licensed under the MPL 2.0. Website: https://rabbitmq.com', 'platform': 'Erlang/OTP 26.2.5', 'product': 'RabbitMQ', 'version': '3.13.3'}, mechanisms: [b'AMQPLAIN', b'PLAIN'], locales: ['en_US']\r\n[2024-06-19 22:22:41,848: INFO/MainProcess] Connected to amqp://guest:**@127.0.0.1:5672//\r\n[2024-06-19 22:22:41,848: DEBUG/MainProcess] ^-- substep ok\r\n[2024-06-19 22:22:41,848: DEBUG/MainProcess] | Consumer: Starting Events\r\n[2024-06-19 22:22:41,848: WARNING/MainProcess] /Users/nusnus/dev/GitHub/celery/celery/worker/consumer/consumer.py:508: CPendingDeprecationWarning: The broker_connection_retry configuration setting will no longer determine\r\nwhether broker connection retries are made during startup in Celery 6.0 and above.\r\nIf you wish to retain the existing behavior for retrying connections on startup,\r\nyou should set broker_connection_retry_on_startup to True.\r\n  warnings.warn(\r\n\r\n[2024-06-19 22:22:41,850: DEBUG/MainProcess] Start from server, version: 0.9, properties: {'capabilities': {'publisher_confirms': True, 'exchange_exchange_bindings': True, 'basic.nack': True, 'consumer_cancel_notify': True, 'connection.blocked': True, 'consumer_priorities': True, 'authentication_failure_close': True, 'per_consumer_qos': True, 'direct_reply_to': True}, 'cluster_name': 'rabbit@rabbit1', 'copyright': 'Copyright (c) 2007-2024 Broadcom Inc and/or its subsidiaries', 'information': 'Licensed under the MPL 2.0. Website: https://rabbitmq.com', 'platform': 'Erlang/OTP 26.2.5', 'product': 'RabbitMQ', 'version': '3.13.3'}, mechanisms: [b'AMQPLAIN', b'PLAIN'], locales: ['en_US']\r\n[2024-06-19 22:22:41,851: DEBUG/MainProcess] ^-- substep ok\r\n[2024-06-19 22:22:41,851: DEBUG/MainProcess] | Consumer: Starting Heart\r\n[2024-06-19 22:22:41,851: DEBUG/MainProcess] using channel_id: 1\r\n[2024-06-19 22:22:41,852: DEBUG/MainProcess] Channel open\r\n[2024-06-19 22:22:41,853: DEBUG/MainProcess] ^-- substep ok\r\n[2024-06-19 22:22:41,853: DEBUG/MainProcess] | Consumer: Starting Mingle\r\n[2024-06-19 22:22:41,853: INFO/MainProcess] mingle: searching for neighbors\r\n[2024-06-19 22:22:41,853: DEBUG/MainProcess] using channel_id: 1\r\n[2024-06-19 22:22:41,854: DEBUG/MainProcess] Channel open\r\n[2024-06-19 22:22:41,865: DEBUG/MainProcess] Start from server, version: 0.9, properties: {'capabilities': {'publisher_confirms': True, 'exchange_exchange_bindings': True, 'basic.nack': True, 'consumer_cancel_notify': True, 'connection.blocked': True, 'consumer_priorities': True, 'authentication_failure_close': True, 'per_consumer_qos': True, 'direct_reply_to': True}, 'cluster_name': 'rabbit@rabbit1', 'copyright': 'Copyright (c) 2007-2024 Broadcom Inc and/or its subsidiaries', 'information': 'Licensed under the MPL 2.0. Website: https://rabbitmq.com', 'platform': 'Erlang/OTP 26.2.5', 'product': 'RabbitMQ', 'version': '3.13.3'}, mechanisms: [b'AMQPLAIN', b'PLAIN'], locales: ['en_US']\r\n[2024-06-19 22:22:41,866: DEBUG/MainProcess] using channel_id: 1\r\n[2024-06-19 22:22:41,867: DEBUG/MainProcess] Channel open\r\n[2024-06-19 22:22:42,879: INFO/MainProcess] mingle: all alone\r\n[2024-06-19 22:22:42,880: DEBUG/MainProcess] ^-- substep ok\r\n[2024-06-19 22:22:42,880: DEBUG/MainProcess] | Consumer: Starting Tasks\r\n[2024-06-19 22:22:42,888: DEBUG/MainProcess] ^-- substep ok\r\n[2024-06-19 22:22:42,888: DEBUG/MainProcess] | Consumer: Starting Control\r\n[2024-06-19 22:22:42,888: DEBUG/MainProcess] using channel_id: 2\r\n[2024-06-19 22:22:42,889: DEBUG/MainProcess] Channel open\r\n[2024-06-19 22:22:42,903: DEBUG/MainProcess] ^-- substep ok\r\n[2024-06-19 22:22:42,903: DEBUG/MainProcess] | Consumer: Starting Gossip\r\n[2024-06-19 22:22:42,903: DEBUG/MainProcess] using channel_id: 3\r\n[2024-06-19 22:22:42,904: DEBUG/MainProcess] Channel open\r\n[2024-06-19 22:22:42,917: DEBUG/MainProcess] ^-- substep ok\r\n[2024-06-19 22:22:42,917: DEBUG/MainProcess] | Consumer: Starting event loop\r\n[2024-06-19 22:22:42,917: DEBUG/MainProcess] | Worker: Hub.register Pool...\r\n[2024-06-19 22:22:42,918: INFO/MainProcess] celery@Tomers-MacBook-Pro.local ready.\r\n[2024-06-19 22:22:42,919: DEBUG/MainProcess] basic.qos: prefetch_count->4\r\n[2024-06-19 22:22:54,628: INFO/MainProcess] Task myapp.identity[90d05eca-4bf4-4207-84cd-0429b28bc54a] received\r\n[2024-06-19 22:22:54,628: DEBUG/MainProcess] basic.qos: prefetch_count->5\r\n[2024-06-19 22:23:03,604: INFO/MainProcess] Task myapp.identity[2c73d40d-f4e2-4dc5-9d21-d5de6606e739] received\r\n[2024-06-19 22:23:03,604: DEBUG/MainProcess] basic.qos: prefetch_count->6\r\n[2024-06-19 22:23:24,613: DEBUG/MainProcess] TaskPool: Apply <function fast_trace_task at 0x1039dd800> (args:('myapp.identity', '90d05eca-4bf4-4207-84cd-0429b28bc54a', {'argsrepr': \"('Hello, Classic Queue!',)\", 'eta': '2024-06-19T19:23:24.611430+00:00', 'expires': None, 'group': None, 'group_index': None, 'id': '90d05eca-4bf4-4207-84cd-0429b28bc54a', 'ignore_result': False, 'kwargsrepr': '{}', 'lang': 'py', 'origin': 'gen97472@Tomers-MacBook-Pro.local', 'parent_id': None, 'replaced_task_nesting': 0, 'retries': 0, 'root_id': '90d05eca-4bf4-4207-84cd-0429b28bc54a', 'shadow': None, 'stamped_headers': None, 'stamps': {}, 'task': 'myapp.identity', 'timelimit': [None, None], 'properties': {'content_type': 'application/json', 'content_encoding': 'utf-8', 'application_headers': {'argsrepr': \"('Hello, Classic Queue!',)\", 'eta': '2024-06-19T19:23:24.611430+00:00', 'expires': None, 'group': None, 'group_index': None, 'id': '90d05eca-4bf4-4207-84cd-0429b28bc54a', 'ignore_result': False, 'kwargsrepr': '{}', 'lang': 'py', 'origin': 'gen97472@Tomers-MacBook-Pro.local', 'parent_id': None, 'replaced_task_nesting': 0, 'retries': 0,... kwargs:{})\r\n[2024-06-19 22:23:24,614: DEBUG/MainProcess] basic.qos: prefetch_count->5\r\n[2024-06-19 22:23:24,619: INFO/ForkPoolWorker-2] Task myapp.identity[90d05eca-4bf4-4207-84cd-0429b28bc54a] succeeded in 0.00037004100158810616s: 'Hello, Classic Queue!'\r\n[2024-06-19 22:23:31,417: INFO/MainProcess] Task myapp.identity[ef9d8cf7-3b70-4a91-b24b-a260e21021b9] received\r\n[2024-06-19 22:23:31,417: DEBUG/MainProcess] basic.qos: prefetch_count->6\r\n[2024-06-19 22:23:33,590: DEBUG/MainProcess] TaskPool: Apply <function fast_trace_task at 0x1039dd800> (args:('myapp.identity', '2c73d40d-f4e2-4dc5-9d21-d5de6606e739', {'argsrepr': \"('Hello, Classic Queue!',)\", 'eta': '2024-06-19T19:23:33.588267+00:00', 'expires': None, 'group': None, 'group_index': None, 'id': '2c73d40d-f4e2-4dc5-9d21-d5de6606e739', 'ignore_result': False, 'kwargsrepr': '{}', 'lang': 'py', 'origin': 'gen97472@Tomers-MacBook-Pro.local', 'parent_id': None, 'replaced_task_nesting': 0, 'retries': 0, 'root_id': '2c73d40d-f4e2-4dc5-9d21-d5de6606e739', 'shadow': None, 'stamped_headers': None, 'stamps': {}, 'task': 'myapp.identity', 'timelimit': [None, None], 'properties': {'content_type': 'application/json', 'content_encoding': 'utf-8', 'application_headers': {'argsrepr': \"('Hello, Classic Queue!',)\", 'eta': '2024-06-19T19:23:33.588267+00:00', 'expires': None, 'group': None, 'group_index': None, 'id': '2c73d40d-f4e2-4dc5-9d21-d5de6606e739', 'ignore_result': False, 'kwargsrepr': '{}', 'lang': 'py', 'origin': 'gen97472@Tomers-MacBook-Pro.local', 'parent_id': None, 'replaced_task_nesting': 0, 'retries': 0,... kwargs:{})\r\n[2024-06-19 22:23:33,591: DEBUG/MainProcess] basic.qos: prefetch_count->5\r\n[2024-06-19 22:23:33,593: INFO/ForkPoolWorker-2] Task myapp.identity[2c73d40d-f4e2-4dc5-9d21-d5de6606e739] succeeded in 0.0004399591125547886s: 'Hello, Classic Queue!'\r\n[2024-06-19 22:24:01,407: DEBUG/MainProcess] TaskPool: Apply <function fast_trace_task at 0x1039dd800> (args:('myapp.identity', 'ef9d8cf7-3b70-4a91-b24b-a260e21021b9', {'argsrepr': \"('Hello, Classic Queue!',)\", 'eta': '2024-06-19T19:24:01.405553+00:00', 'expires': None, 'group': None, 'group_index': None, 'id': 'ef9d8cf7-3b70-4a91-b24b-a260e21021b9', 'ignore_result': False, 'kwargsrepr': '{}', 'lang': 'py', 'origin': 'gen97472@Tomers-MacBook-Pro.local', 'parent_id': None, 'replaced_task_nesting': 0, 'retries': 0, 'root_id': 'ef9d8cf7-3b70-4a91-b24b-a260e21021b9', 'shadow': None, 'stamped_headers': None, 'stamps': {}, 'task': 'myapp.identity', 'timelimit': [None, None], 'properties': {'content_type': 'application/json', 'content_encoding': 'utf-8', 'application_headers': {'argsrepr': \"('Hello, Classic Queue!',)\", 'eta': '2024-06-19T19:24:01.405553+00:00', 'expires': None, 'group': None, 'group_index': None, 'id': 'ef9d8cf7-3b70-4a91-b24b-a260e21021b9', 'ignore_result': False, 'kwargsrepr': '{}', 'lang': 'py', 'origin': 'gen97472@Tomers-MacBook-Pro.local', 'parent_id': None, 'replaced_task_nesting': 0, 'retries': 0,... kwargs:{})\r\n[2024-06-19 22:24:01,407: DEBUG/MainProcess] basic.qos: prefetch_count->4\r\n[2024-06-19 22:24:01,408: INFO/ForkPoolWorker-2] Task myapp.identity[ef9d8cf7-3b70-4a91-b24b-a260e21021b9] succeeded in 0.00010208413004875183s: 'Hello, Classic Queue!'\r\n^C\r\nworker: Hitting Ctrl+C again will terminate all running tasks!\r\n```\r\n\r\nLet\u2019s observe the interesting part:\r\n```console\r\n[2024-06-19 22:22:42,919: DEBUG/MainProcess] basic.qos: prefetch_count->4\r\n[2024-06-19 22:22:54,628: INFO/MainProcess] Task myapp.identity[90d05eca-4bf4-4207-84cd-0429b28bc54a] received\r\n[2024-06-19 22:22:54,628: DEBUG/MainProcess] basic.qos: prefetch_count->5\r\n[2024-06-19 22:23:03,604: INFO/MainProcess] Task myapp.identity[2c73d40d-f4e2-4dc5-9d21-d5de6606e739] received\r\n[2024-06-19 22:23:03,604: DEBUG/MainProcess] basic.qos: prefetch_count->6\r\n[2024-06-19 22:23:24,613: DEBUG/MainProcess] TaskPool: Apply <function fast_trace_task at 0x1039dd800> (args:('myapp.identity', '90d05eca-4bf4-4207-84cd-0429b28bc54a', {'argsrepr': \"('Hello, Classic Queue!',)\", 'eta': '2024-06-19T19:23:24.611430+00:00', 'expires': None, 'group': None, 'group_index': None, 'id': '90d05eca-4bf4-4207-84cd-0429b28bc54a', 'ignore_result': False, 'kwargsrepr': '{}', 'lang': 'py', 'origin': 'gen97472@Tomers-MacBook-Pro.local', 'parent_id': None, 'replaced_task_nesting': 0, 'retries': 0, 'root_id': '90d05eca-4bf4-4207-84cd-0429b28bc54a', 'shadow': None, 'stamped_headers': None, 'stamps': {}, 'task': 'myapp.identity', 'timelimit': [None, None], 'properties': {'content_type': 'application/json', 'content_encoding': 'utf-8', 'application_headers': {'argsrepr': \"('Hello, Classic Queue!',)\", 'eta': '2024-06-19T19:23:24.611430+00:00', 'expires': None, 'group': None, 'group_index': None, 'id': '90d05eca-4bf4-4207-84cd-0429b28bc54a', 'ignore_result': False, 'kwargsrepr': '{}', 'lang': 'py', 'origin': 'gen97472@Tomers-MacBook-Pro.local', 'parent_id': None, 'replaced_task_nesting': 0, 'retries': 0,... kwargs:{})\r\n[2024-06-19 22:23:24,614: DEBUG/MainProcess] basic.qos: prefetch_count->5\r\n[2024-06-19 22:23:24,619: INFO/ForkPoolWorker-2] Task myapp.identity[90d05eca-4bf4-4207-84cd-0429b28bc54a] succeeded in 0.00037004100158810616s: 'Hello, Classic Queue!'\r\n[2024-06-19 22:23:31,417: INFO/MainProcess] Task myapp.identity[ef9d8cf7-3b70-4a91-b24b-a260e21021b9] received\r\n[2024-06-19 22:23:31,417: DEBUG/MainProcess] basic.qos: prefetch_count->6\r\n[2024-06-19 22:23:33,590: DEBUG/MainProcess] TaskPool: Apply <function fast_trace_task at 0x1039dd800> (args:('myapp.identity', '2c73d40d-f4e2-4dc5-9d21-d5de6606e739', {'argsrepr': \"('Hello, Classic Queue!',)\", 'eta': '2024-06-19T19:23:33.588267+00:00', 'expires': None, 'group': None, 'group_index': None, 'id': '2c73d40d-f4e2-4dc5-9d21-d5de6606e739', 'ignore_result': False, 'kwargsrepr': '{}', 'lang': 'py', 'origin': 'gen97472@Tomers-MacBook-Pro.local', 'parent_id': None, 'replaced_task_nesting': 0, 'retries': 0, 'root_id': '2c73d40d-f4e2-4dc5-9d21-d5de6606e739', 'shadow': None, 'stamped_headers': None, 'stamps': {}, 'task': 'myapp.identity', 'timelimit': [None, None], 'properties': {'content_type': 'application/json', 'content_encoding': 'utf-8', 'application_headers': {'argsrepr': \"('Hello, Classic Queue!',)\", 'eta': '2024-06-19T19:23:33.588267+00:00', 'expires': None, 'group': None, 'group_index': None, 'id': '2c73d40d-f4e2-4dc5-9d21-d5de6606e739', 'ignore_result': False, 'kwargsrepr': '{}', 'lang': 'py', 'origin': 'gen97472@Tomers-MacBook-Pro.local', 'parent_id': None, 'replaced_task_nesting': 0, 'retries': 0,... kwargs:{})\r\n[2024-06-19 22:23:33,591: DEBUG/MainProcess] basic.qos: prefetch_count->5\r\n[2024-06-19 22:23:33,593: INFO/ForkPoolWorker-2] Task myapp.identity[2c73d40d-f4e2-4dc5-9d21-d5de6606e739] succeeded in 0.0004399591125547886s: 'Hello, Classic Queue!'\r\n[2024-06-19 22:24:01,407: DEBUG/MainProcess] TaskPool: Apply <function fast_trace_task at 0x1039dd800> (args:('myapp.identity', 'ef9d8cf7-3b70-4a91-b24b-a260e21021b9', {'argsrepr': \"('Hello, Classic Queue!',)\", 'eta': '2024-06-19T19:24:01.405553+00:00', 'expires': None, 'group': None, 'group_index': None, 'id': 'ef9d8cf7-3b70-4a91-b24b-a260e21021b9', 'ignore_result': False, 'kwargsrepr': '{}', 'lang': 'py', 'origin': 'gen97472@Tomers-MacBook-Pro.local', 'parent_id': None, 'replaced_task_nesting': 0, 'retries': 0, 'root_id': 'ef9d8cf7-3b70-4a91-b24b-a260e21021b9', 'shadow': None, 'stamped_headers': None, 'stamps': {}, 'task': 'myapp.identity', 'timelimit': [None, None], 'properties': {'content_type': 'application/json', 'content_encoding': 'utf-8', 'application_headers': {'argsrepr': \"('Hello, Classic Queue!',)\", 'eta': '2024-06-19T19:24:01.405553+00:00', 'expires': None, 'group': None, 'group_index': None, 'id': 'ef9d8cf7-3b70-4a91-b24b-a260e21021b9', 'ignore_result': False, 'kwargsrepr': '{}', 'lang': 'py', 'origin': 'gen97472@Tomers-MacBook-Pro.local', 'parent_id': None, 'replaced_task_nesting': 0, 'retries': 0,... kwargs:{})\r\n[2024-06-19 22:24:01,407: DEBUG/MainProcess] basic.qos: prefetch_count->4\r\n[2024-06-19 22:24:01,408: INFO/ForkPoolWorker-2] Task myapp.identity[ef9d8cf7-3b70-4a91-b24b-a260e21021b9] succeeded in 0.00010208413004875183s: 'Hello, Classic Queue!\u2019\r\n```\r\n\r\n### Using quorum queues\r\n#### WORKER_QUORUMQ=True celery -A myapp worker -l DEBUG\r\n```console\r\nWORKER_QUORUMQ=True celery -A myapp worker -l DEBUG\r\n[2024-06-19 22:41:34,837: DEBUG/MainProcess] | Worker: Preparing bootsteps.\r\n[2024-06-19 22:41:34,838: DEBUG/MainProcess] | Worker: Building graph...\r\n[2024-06-19 22:41:34,838: DEBUG/MainProcess] | Worker: New boot order: {Beat, Timer, Hub, Pool, Autoscaler, StateDB, Consumer}\r\n[2024-06-19 22:41:34,840: DEBUG/MainProcess] | Consumer: Preparing bootsteps.\r\n[2024-06-19 22:41:34,840: DEBUG/MainProcess] | Consumer: Building graph...\r\n[2024-06-19 22:41:34,848: DEBUG/MainProcess] | Consumer: New boot order: {Connection, Events, Mingle, Tasks, Control, Agent, Gossip, Heart, event loop}\r\n\r\n -------------- celery@Tomers-MacBook-Pro.local v5.4.0 (opalescent)\r\n--- ***** -----\r\n-- ******* ---- macOS-14.5-arm64-arm-64bit 2024-06-19 22:41:34\r\n- *** --- * ---\r\n- ** ---------- [config]\r\n- ** ---------- .> app:         myapp:0x1064115e0\r\n- ** ---------- .> transport:   amqp://guest:**@localhost:5672//\r\n- ** ---------- .> results:     disabled://\r\n- *** --- * --- .> concurrency: 4 (prefork)\r\n-- ******* ---- .> task events: OFF (enable -E to monitor tasks in this worker)\r\n--- ***** -----\r\n -------------- [queues]\r\n                .> my-quorum-queue  exchange=default(direct) key=default\r\n\r\n\r\n[tasks]\r\n  . celery.accumulate\r\n  . celery.backend_cleanup\r\n  . celery.chain\r\n  . celery.chord\r\n  . celery.chord_unlock\r\n  . celery.chunks\r\n  . celery.group\r\n  . celery.map\r\n  . celery.starmap\r\n  . myapp.add\r\n  . myapp.identity\r\n\r\n[2024-06-19 22:41:34,864: DEBUG/MainProcess] | Worker: Starting Hub\r\n[2024-06-19 22:41:34,864: DEBUG/MainProcess] ^-- substep ok\r\n[2024-06-19 22:41:34,864: DEBUG/MainProcess] | Worker: Starting Pool\r\n[2024-06-19 22:41:34,924: DEBUG/MainProcess] ^-- substep ok\r\n[2024-06-19 22:41:34,924: DEBUG/MainProcess] | Worker: Starting Consumer\r\n[2024-06-19 22:41:34,924: DEBUG/MainProcess] | Consumer: Starting Connection\r\n[2024-06-19 22:41:34,925: WARNING/MainProcess] /Users/nusnus/dev/GitHub/celery/celery/worker/consumer/consumer.py:508: CPendingDeprecationWarning: The broker_connection_retry configuration setting will no longer determine\r\nwhether broker connection retries are made during startup in Celery 6.0 and above.\r\nIf you wish to retain the existing behavior for retrying connections on startup,\r\nyou should set broker_connection_retry_on_startup to True.\r\n  warnings.warn(\r\n\r\n[2024-06-19 22:41:34,931: DEBUG/MainProcess] Start from server, version: 0.9, properties: {'capabilities': {'publisher_confirms': True, 'exchange_exchange_bindings': True, 'basic.nack': True, 'consumer_cancel_notify': True, 'connection.blocked': True, 'consumer_priorities': True, 'authentication_failure_close': True, 'per_consumer_qos': True, 'direct_reply_to': True}, 'cluster_name': 'rabbit@rabbit1', 'copyright': 'Copyright (c) 2007-2024 Broadcom Inc and/or its subsidiaries', 'information': 'Licensed under the MPL 2.0. Website: https://rabbitmq.com', 'platform': 'Erlang/OTP 26.2.5', 'product': 'RabbitMQ', 'version': '3.13.3'}, mechanisms: [b'AMQPLAIN', b'PLAIN'], locales: ['en_US']\r\n[2024-06-19 22:41:34,932: INFO/MainProcess] Connected to amqp://guest:**@127.0.0.1:5672//\r\n[2024-06-19 22:41:34,932: DEBUG/MainProcess] ^-- substep ok\r\n[2024-06-19 22:41:34,932: DEBUG/MainProcess] | Consumer: Starting Events\r\n[2024-06-19 22:41:34,932: WARNING/MainProcess] /Users/nusnus/dev/GitHub/celery/celery/worker/consumer/consumer.py:508: CPendingDeprecationWarning: The broker_connection_retry configuration setting will no longer determine\r\nwhether broker connection retries are made during startup in Celery 6.0 and above.\r\nIf you wish to retain the existing behavior for retrying connections on startup,\r\nyou should set broker_connection_retry_on_startup to True.\r\n  warnings.warn(\r\n\r\n[2024-06-19 22:41:34,934: DEBUG/MainProcess] Start from server, version: 0.9, properties: {'capabilities': {'publisher_confirms': True, 'exchange_exchange_bindings': True, 'basic.nack': True, 'consumer_cancel_notify': True, 'connection.blocked': True, 'consumer_priorities': True, 'authentication_failure_close': True, 'per_consumer_qos': True, 'direct_reply_to': True}, 'cluster_name': 'rabbit@rabbit1', 'copyright': 'Copyright (c) 2007-2024 Broadcom Inc and/or its subsidiaries', 'information': 'Licensed under the MPL 2.0. Website: https://rabbitmq.com', 'platform': 'Erlang/OTP 26.2.5', 'product': 'RabbitMQ', 'version': '3.13.3'}, mechanisms: [b'AMQPLAIN', b'PLAIN'], locales: ['en_US']\r\n[2024-06-19 22:41:34,935: DEBUG/MainProcess] ^-- substep ok\r\n[2024-06-19 22:41:34,935: DEBUG/MainProcess] | Consumer: Starting Mingle\r\n[2024-06-19 22:41:34,935: INFO/MainProcess] mingle: searching for neighbors\r\n[2024-06-19 22:41:34,936: DEBUG/MainProcess] using channel_id: 1\r\n[2024-06-19 22:41:34,936: DEBUG/MainProcess] Channel open\r\n[2024-06-19 22:41:34,947: DEBUG/MainProcess] Start from server, version: 0.9, properties: {'capabilities': {'publisher_confirms': True, 'exchange_exchange_bindings': True, 'basic.nack': True, 'consumer_cancel_notify': True, 'connection.blocked': True, 'consumer_priorities': True, 'authentication_failure_close': True, 'per_consumer_qos': True, 'direct_reply_to': True}, 'cluster_name': 'rabbit@rabbit1', 'copyright': 'Copyright (c) 2007-2024 Broadcom Inc and/or its subsidiaries', 'information': 'Licensed under the MPL 2.0. Website: https://rabbitmq.com', 'platform': 'Erlang/OTP 26.2.5', 'product': 'RabbitMQ', 'version': '3.13.3'}, mechanisms: [b'AMQPLAIN', b'PLAIN'], locales: ['en_US']\r\n[2024-06-19 22:41:34,948: DEBUG/MainProcess] using channel_id: 1\r\n[2024-06-19 22:41:34,949: DEBUG/MainProcess] Channel open\r\n[2024-06-19 22:41:35,965: INFO/MainProcess] mingle: all alone\r\n[2024-06-19 22:41:35,972: DEBUG/MainProcess] ^-- substep ok\r\n[2024-06-19 22:41:35,974: DEBUG/MainProcess] | Consumer: Starting Tasks\r\n[2024-06-19 22:41:35,984: DEBUG/MainProcess] ^-- substep ok\r\n[2024-06-19 22:41:35,984: DEBUG/MainProcess] | Consumer: Starting Control\r\n[2024-06-19 22:41:35,984: DEBUG/MainProcess] using channel_id: 2\r\n[2024-06-19 22:41:35,985: DEBUG/MainProcess] Channel open\r\n[2024-06-19 22:41:35,992: DEBUG/MainProcess] ^-- substep ok\r\n[2024-06-19 22:41:35,992: DEBUG/MainProcess] | Consumer: Starting Gossip\r\n[2024-06-19 22:41:35,992: DEBUG/MainProcess] using channel_id: 3\r\n[2024-06-19 22:41:35,993: DEBUG/MainProcess] Channel open\r\n[2024-06-19 22:41:36,000: DEBUG/MainProcess] ^-- substep ok\r\n[2024-06-19 22:41:36,000: DEBUG/MainProcess] | Consumer: Starting Heart\r\n[2024-06-19 22:41:36,000: DEBUG/MainProcess] using channel_id: 1\r\n[2024-06-19 22:41:36,001: DEBUG/MainProcess] Channel open\r\n[2024-06-19 22:41:36,002: DEBUG/MainProcess] ^-- substep ok\r\n[2024-06-19 22:41:36,002: DEBUG/MainProcess] | Consumer: Starting event loop\r\n[2024-06-19 22:41:36,002: DEBUG/MainProcess] | Worker: Hub.register Pool...\r\n[2024-06-19 22:41:36,006: INFO/MainProcess] celery@Tomers-MacBook-Pro.local ready.\r\n[2024-06-19 22:41:36,006: DEBUG/MainProcess] basic.qos: prefetch_count->4\r\n[2024-06-19 22:41:51,608: INFO/MainProcess] Task myapp.identity[43dfeb57-d6b3-4d10-8a86-3e7ecb077d88] received\r\n[2024-06-19 22:41:51,608: DEBUG/MainProcess] basic.qos: prefetch_count->5\r\n[2024-06-19 22:41:56,512: INFO/MainProcess] Task myapp.identity[1e92c894-b982-4f9e-85c8-defae15a2c24] received\r\n[2024-06-19 22:41:56,512: DEBUG/MainProcess] basic.qos: prefetch_count->6\r\n[2024-06-19 22:42:21,578: DEBUG/MainProcess] TaskPool: Apply <function fast_trace_task at 0x10638d800> (args:('myapp.identity', '43dfeb57-d6b3-4d10-8a86-3e7ecb077d88', {'argsrepr': \"('Hello, Quorum Queue!',)\", 'eta': '2024-06-19T19:42:21.576442+00:00', 'expires': None, 'group': None, 'group_index': None, 'id': '43dfeb57-d6b3-4d10-8a86-3e7ecb077d88', 'ignore_result': False, 'kwargsrepr': '{}', 'lang': 'py', 'origin': 'gen5583@Tomers-MacBook-Pro.local', 'parent_id': None, 'replaced_task_nesting': 0, 'retries': 0, 'root_id': '43dfeb57-d6b3-4d10-8a86-3e7ecb077d88', 'shadow': None, 'stamped_headers': None, 'stamps': {}, 'task': 'myapp.identity', 'timelimit': [None, None], 'properties': {'content_type': 'application/json', 'content_encoding': 'utf-8', 'application_headers': {'argsrepr': \"('Hello, Quorum Queue!',)\", 'eta': '2024-06-19T19:42:21.576442+00:00', 'expires': None, 'group': None, 'group_index': None, 'id': '43dfeb57-d6b3-4d10-8a86-3e7ecb077d88', 'ignore_result': False, 'kwargsrepr': '{}', 'lang': 'py', 'origin': 'gen5583@Tomers-MacBook-Pro.local', 'parent_id': None, 'replaced_task_nesting': 0, 'retries': 0,... kwargs:{})\r\n[2024-06-19 22:42:21,579: DEBUG/MainProcess] basic.qos: prefetch_count->5\r\n[2024-06-19 22:42:21,581: INFO/ForkPoolWorker-2] Task myapp.identity[43dfeb57-d6b3-4d10-8a86-3e7ecb077d88] succeeded in 0.0009212500881403685s: 'Hello, Quorum Queue!'\r\n[2024-06-19 22:42:22,484: INFO/MainProcess] Task myapp.identity[a43f8d54-c6e1-4bbb-b1e9-c80bae8c53d4] received\r\n[2024-06-19 22:42:22,484: DEBUG/MainProcess] basic.qos: prefetch_count->6\r\n[2024-06-19 22:42:26,506: DEBUG/MainProcess] TaskPool: Apply <function fast_trace_task at 0x10638d800> (args:('myapp.identity', '1e92c894-b982-4f9e-85c8-defae15a2c24', {'argsrepr': \"('Hello, Quorum Queue!',)\", 'eta': '2024-06-19T19:42:26.505187+00:00', 'expires': None, 'group': None, 'group_index': None, 'id': '1e92c894-b982-4f9e-85c8-defae15a2c24', 'ignore_result': False, 'kwargsrepr': '{}', 'lang': 'py', 'origin': 'gen5583@Tomers-MacBook-Pro.local', 'parent_id': None, 'replaced_task_nesting': 0, 'retries': 0, 'root_id': '1e92c894-b982-4f9e-85c8-defae15a2c24', 'shadow': None, 'stamped_headers': None, 'stamps': {}, 'task': 'myapp.identity', 'timelimit': [None, None], 'properties': {'content_type': 'application/json', 'content_encoding': 'utf-8', 'application_headers': {'argsrepr': \"('Hello, Quorum Queue!',)\", 'eta': '2024-06-19T19:42:26.505187+00:00', 'expires': None, 'group': None, 'group_index': None, 'id': '1e92c894-b982-4f9e-85c8-defae15a2c24', 'ignore_result': False, 'kwargsrepr': '{}', 'lang': 'py', 'origin': 'gen5583@Tomers-MacBook-Pro.local', 'parent_id': None, 'replaced_task_nesting': 0, 'retries': 0,... kwargs:{})\r\n[2024-06-19 22:42:26,511: DEBUG/MainProcess] basic.qos: prefetch_count->5\r\n[2024-06-19 22:42:26,511: INFO/ForkPoolWorker-2] Task myapp.identity[1e92c894-b982-4f9e-85c8-defae15a2c24] succeeded in 0.00017629098147153854s: 'Hello, Quorum Queue!'\r\n[2024-06-19 22:42:52,479: DEBUG/MainProcess] TaskPool: Apply <function fast_trace_task at 0x10638d800> (args:('myapp.identity', 'a43f8d54-c6e1-4bbb-b1e9-c80bae8c53d4', {'argsrepr': \"('Hello, Quorum Queue!',)\", 'eta': '2024-06-19T19:42:52.477317+00:00', 'expires': None, 'group': None, 'group_index': None, 'id': 'a43f8d54-c6e1-4bbb-b1e9-c80bae8c53d4', 'ignore_result': False, 'kwargsrepr': '{}', 'lang': 'py', 'origin': 'gen5583@Tomers-MacBook-Pro.local', 'parent_id': None, 'replaced_task_nesting': 0, 'retries': 0, 'root_id': 'a43f8d54-c6e1-4bbb-b1e9-c80bae8c53d4', 'shadow': None, 'stamped_headers': None, 'stamps': {}, 'task': 'myapp.identity', 'timelimit': [None, None], 'properties': {'content_type': 'application/json', 'content_encoding': 'utf-8', 'application_headers': {'argsrepr': \"('Hello, Quorum Queue!',)\", 'eta': '2024-06-19T19:42:52.477317+00:00', 'expires': None, 'group': None, 'group_index': None, 'id': 'a43f8d54-c6e1-4bbb-b1e9-c80bae8c53d4', 'ignore_result': False, 'kwargsrepr': '{}', 'lang': 'py', 'origin': 'gen5583@Tomers-MacBook-Pro.local', 'parent_id': None, 'replaced_task_nesting': 0, 'retries': 0,... kwargs:{})\r\n[2024-06-19 22:42:52,480: DEBUG/MainProcess] basic.qos: prefetch_count->4\r\n[2024-06-19 22:42:52,487: INFO/ForkPoolWorker-2] Task myapp.identity[a43f8d54-c6e1-4bbb-b1e9-c80bae8c53d4] succeeded in 0.00019058305770158768s: 'Hello, Quorum Queue!'\r\n^C\r\nworker: Hitting Ctrl+C again will terminate all running tasks!\r\n```\r\n\r\nLet\u2019s observe the interesting part:\r\n```console\r\n[2024-06-19 22:41:36,006: DEBUG/MainProcess] basic.qos: prefetch_count->4\r\n[2024-06-19 22:41:51,608: INFO/MainProcess] Task myapp.identity[43dfeb57-d6b3-4d10-8a86-3e7ecb077d88] received\r\n[2024-06-19 22:41:51,608: DEBUG/MainProcess] basic.qos: prefetch_count->5\r\n[2024-06-19 22:41:56,512: INFO/MainProcess] Task myapp.identity[1e92c894-b982-4f9e-85c8-defae15a2c24] received\r\n[2024-06-19 22:41:56,512: DEBUG/MainProcess] basic.qos: prefetch_count->6\r\n[2024-06-19 22:42:21,578: DEBUG/MainProcess] TaskPool: Apply <function fast_trace_task at 0x10638d800> (args:('myapp.identity', '43dfeb57-d6b3-4d10-8a86-3e7ecb077d88', {'argsrepr': \"('Hello, Quorum Queue!',)\", 'eta': '2024-06-19T19:42:21.576442+00:00', 'expires': None, 'group': None, 'group_index': None, 'id': '43dfeb57-d6b3-4d10-8a86-3e7ecb077d88', 'ignore_result': False, 'kwargsrepr': '{}', 'lang': 'py', 'origin': 'gen5583@Tomers-MacBook-Pro.local', 'parent_id': None, 'replaced_task_nesting': 0, 'retries': 0, 'root_id': '43dfeb57-d6b3-4d10-8a86-3e7ecb077d88', 'shadow': None, 'stamped_headers': None, 'stamps': {}, 'task': 'myapp.identity', 'timelimit': [None, None], 'properties': {'content_type': 'application/json', 'content_encoding': 'utf-8', 'application_headers': {'argsrepr': \"('Hello, Quorum Queue!',)\", 'eta': '2024-06-19T19:42:21.576442+00:00', 'expires': None, 'group': None, 'group_index': None, 'id': '43dfeb57-d6b3-4d10-8a86-3e7ecb077d88', 'ignore_result': False, 'kwargsrepr': '{}', 'lang': 'py', 'origin': 'gen5583@Tomers-MacBook-Pro.local', 'parent_id': None, 'replaced_task_nesting': 0, 'retries': 0,... kwargs:{})\r\n[2024-06-19 22:42:21,579: DEBUG/MainProcess] basic.qos: prefetch_count->5\r\n[2024-06-19 22:42:21,581: INFO/ForkPoolWorker-2] Task myapp.identity[43dfeb57-d6b3-4d10-8a86-3e7ecb077d88] succeeded in 0.0009212500881403685s: 'Hello, Quorum Queue!'\r\n[2024-06-19 22:42:22,484: INFO/MainProcess] Task myapp.identity[a43f8d54-c6e1-4bbb-b1e9-c80bae8c53d4] received\r\n[2024-06-19 22:42:22,484: DEBUG/MainProcess] basic.qos: prefetch_count->6\r\n[2024-06-19 22:42:26,506: DEBUG/MainProcess] TaskPool: Apply <function fast_trace_task at 0x10638d800> (args:('myapp.identity', '1e92c894-b982-4f9e-85c8-defae15a2c24', {'argsrepr': \"('Hello, Quorum Queue!',)\", 'eta': '2024-06-19T19:42:26.505187+00:00', 'expires': None, 'group': None, 'group_index': None, 'id': '1e92c894-b982-4f9e-85c8-defae15a2c24', 'ignore_result': False, 'kwargsrepr': '{}', 'lang': 'py', 'origin': 'gen5583@Tomers-MacBook-Pro.local', 'parent_id': None, 'replaced_task_nesting': 0, 'retries': 0, 'root_id': '1e92c894-b982-4f9e-85c8-defae15a2c24', 'shadow': None, 'stamped_headers': None, 'stamps': {}, 'task': 'myapp.identity', 'timelimit': [None, None], 'properties': {'content_type': 'application/json', 'content_encoding': 'utf-8', 'application_headers': {'argsrepr': \"('Hello, Quorum Queue!',)\", 'eta': '2024-06-19T19:42:26.505187+00:00', 'expires': None, 'group': None, 'group_index': None, 'id': '1e92c894-b982-4f9e-85c8-defae15a2c24', 'ignore_result': False, 'kwargsrepr': '{}', 'lang': 'py', 'origin': 'gen5583@Tomers-MacBook-Pro.local', 'parent_id': None, 'replaced_task_nesting': 0, 'retries': 0,... kwargs:{})\r\n[2024-06-19 22:42:26,511: DEBUG/MainProcess] basic.qos: prefetch_count->5\r\n[2024-06-19 22:42:26,511: INFO/ForkPoolWorker-2] Task myapp.identity[1e92c894-b982-4f9e-85c8-defae15a2c24] succeeded in 0.00017629098147153854s: 'Hello, Quorum Queue!'\r\n[2024-06-19 22:42:52,479: DEBUG/MainProcess] TaskPool: Apply <function fast_trace_task at 0x10638d800> (args:('myapp.identity', 'a43f8d54-c6e1-4bbb-b1e9-c80bae8c53d4', {'argsrepr': \"('Hello, Quorum Queue!',)\", 'eta': '2024-06-19T19:42:52.477317+00:00', 'expires': None, 'group': None, 'group_index': None, 'id': 'a43f8d54-c6e1-4bbb-b1e9-c80bae8c53d4', 'ignore_result': False, 'kwargsrepr': '{}', 'lang': 'py', 'origin': 'gen5583@Tomers-MacBook-Pro.local', 'parent_id': None, 'replaced_task_nesting': 0, 'retries': 0, 'root_id': 'a43f8d54-c6e1-4bbb-b1e9-c80bae8c53d4', 'shadow': None, 'stamped_headers': None, 'stamps': {}, 'task': 'myapp.identity', 'timelimit': [None, None], 'properties': {'content_type': 'application/json', 'content_encoding': 'utf-8', 'application_headers': {'argsrepr': \"('Hello, Quorum Queue!',)\", 'eta': '2024-06-19T19:42:52.477317+00:00', 'expires': None, 'group': None, 'group_index': None, 'id': 'a43f8d54-c6e1-4bbb-b1e9-c80bae8c53d4', 'ignore_result': False, 'kwargsrepr': '{}', 'lang': 'py', 'origin': 'gen5583@Tomers-MacBook-Pro.local', 'parent_id': None, 'replaced_task_nesting': 0, 'retries': 0,... kwargs:{})\r\n[2024-06-19 22:42:52,480: DEBUG/MainProcess] basic.qos: prefetch_count->4\r\n[2024-06-19 22:42:52,487: INFO/ForkPoolWorker-2] Task myapp.identity[a43f8d54-c6e1-4bbb-b1e9-c80bae8c53d4] succeeded in 0.00019058305770158768s: 'Hello, Quorum Queue!'\r\n```\r\n\r\n## Summary\r\nSo far, from this **very simple** test case, it appears that:\r\n1. ETA tasks seem to be working (taken with a grain of salt).\r\n2. Quorum queues (e.g., `qos_global = False`) seem to have similar logs to classic queues with ETA tasks, in regard to `basic.qos`.\r\n\r\nOn one hand, it\u2019s nice that \u201ceverything is working,\u201d but on the other hand, it makes me wonder what I am missing.\r\n\r\nI need to see how to \u201ccomplicate the environment\u201d, as to get closer to a production environment, to try and surface out the edge cases and bugs. @miraculixx @auvipy any idea how can we break it?\r\n\r\nAlso, generally, do I miss anything in the way I test/observe the details? I know it\u2019s been a while, but do you mind checking my recent insights and let me know if something doesn\u2019t add up? @MichaelKubovic @apolloFER\nOne issue was that ETA may saturate de prefetch queue and block incoming new job. \r\n\r\nYou should have a test that generate a lots of ETA tasks, and then still try to schedule new tasks to see if the prefetch count is updated accordingly and the task is executed  (which was what the global_qos was doing).\r\n\r\n", "created_at": "2024-07-07T20:21:07Z"}
{"repo": "celery/celery", "pull_number": 9099, "instance_id": "celery__celery-9099", "issue_numbers": ["9098"], "base_commit": "87f6893e4dab758c09e7eb16618129157753e734", "patch": "diff --git a/CONTRIBUTORS.txt b/CONTRIBUTORS.txt\nindex 184a2538e5a..9c3534b3358 100644\n--- a/CONTRIBUTORS.txt\n+++ b/CONTRIBUTORS.txt\n@@ -298,3 +298,4 @@ Jeremy Z. Othieno 2023/07/27\n Tomer Nosrati, 2022/17/07\n Andy Zickler, 2024/01/18\n Johannes Faigle, 2024/06/18\n+Giovanni Giampauli, 2024/06/26\ndiff --git a/examples/django/proj/celery.py b/examples/django/proj/celery.py\nindex ec3354dcdf3..182da54fb55 100644\n--- a/examples/django/proj/celery.py\n+++ b/examples/django/proj/celery.py\n@@ -1,5 +1,7 @@\n import os\n \n+from django.conf import settings\n+\n from celery import Celery\n \n # Set the default Django settings module for the 'celery' program.\n@@ -11,7 +13,7 @@\n # the configuration object to child processes.\n # - namespace='CELERY' means all celery-related configuration keys\n #   should have a `CELERY_` prefix.\n-app.config_from_object('django.conf:settings', namespace='CELERY')\n+app.config_from_object(f'django.conf:{settings.__name__}', namespace='CELERY')\n \n # Load task modules from all registered Django apps.\n app.autodiscover_tasks()\n", "test_patch": "", "problem_statement": "Missing Import in Documentation in Django tutorial\n**Description:**\r\n\r\nThe code snippet provided in the documentation needs an additional import to function correctly. Without this import, the code does not raise any errors, but it fails to discover `@shared_task`s in other files.\r\n\r\n**Link to Documentation:** [First steps with Django](https://docs.celeryq.dev/en/stable/django/first-steps-with-django.html)\r\n\r\n**Code:**\r\n```python\r\nimport os\r\n\r\nfrom celery import Celery\r\n\r\n# Set the default Django settings module for the 'celery' program.\r\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'proj.settings')\r\n\r\napp = Celery('proj')\r\n\r\n# Using a string here means the worker doesn't have to serialize\r\n# the configuration object to child processes.\r\n# - namespace='CELERY' means all celery-related configuration keys\r\n#   should have a `CELERY_` prefix.\r\napp.config_from_object('django.conf:settings', namespace='CELERY')\r\n\r\n# Load task modules from all registered Django apps.\r\napp.autodiscover_tasks()\r\n\r\n\r\n@app.task(bind=True, ignore_result=True)\r\ndef debug_task(self):\r\n    print(f'Request: {self.request!r}')\r\n```\r\n\r\n**Necessary Import:**\r\n```python\r\n# The documentation does not mention the need to import this module\r\nfrom django.conf import settings\r\n```\r\n\r\n**Problem:**\r\nWithout the `from django.conf import settings` import, the code does not produce any errors, but it fails to discover `@shared_task`s in other files.\r\n\r\n**Notes**\r\nI fixed this in [Django OAuth Toolkit Tutorial - Step 5](https://django-oauth-toolkit.readthedocs.io/en/latest/tutorial/tutorial_05.html)\r\n\r\n**Reported Issues:**\r\n- [Missing Import in Documentation](https://github.com/jazzband/django-oauth-toolkit/issues/1421)\r\n- [Stack Overflow: Celery does not discover tasks inside project](https://stackoverflow.com/questions/72628911/celery-does-not-discovers-tasks-inside-project/78501672#78501672)\r\n- [GitHub: cookiecutter-django Issue #1681](https://github.com/cookiecutter/cookiecutter-django/issues/1681)\r\n- [GitHub: django-celery-beat Issue #26](https://github.com/celery/django-celery-beat/issues/26)\r\n- [Django Forum: Celery does not register tasks](https://forum.djangoproject.com/t/celery-does-not-register-tasks/14560)\r\n\r\n**References:**\r\n- [Celery Documentation: First Steps with Django](https://docs.celeryq.dev/en/latest/django/first-steps-with-django.html#django-first-steps)\n", "hints_text": "", "created_at": "2024-06-26T03:12:54Z"}
{"repo": "celery/celery", "pull_number": 9023, "instance_id": "celery__celery-9023", "issue_numbers": ["8751"], "base_commit": "f12abdfa4dc5976d48869dd6772c44c64f07e150", "patch": "diff --git a/celery/app/base.py b/celery/app/base.py\nindex 63f3d54abec..c1bb9b790b5 100644\n--- a/celery/app/base.py\n+++ b/celery/app/base.py\n@@ -1,8 +1,11 @@\n \"\"\"Actual App instance implementation.\"\"\"\n+import functools\n+import importlib\n import inspect\n import os\n import sys\n import threading\n+import typing\n import warnings\n from collections import UserDict, defaultdict, deque\n from datetime import datetime\n@@ -43,6 +46,10 @@\n from .utils import (AppPickler, Settings, _new_key_to_old, _old_key_to_new, _unpickle_app, _unpickle_app_v2, appstr,\n                     bugreport, detect_settings)\n \n+if typing.TYPE_CHECKING:  # pragma: no cover  # codecov does not capture this\n+    # flake8 marks the BaseModel import as unused, because the actual typehint is quoted.\n+    from pydantic import BaseModel  # noqa: F401\n+\n __all__ = ('Celery',)\n \n logger = get_logger(__name__)\n@@ -92,6 +99,59 @@ def _after_fork_cleanup_app(app):\n         logger.info('after forker raised exception: %r', exc, exc_info=1)\n \n \n+def pydantic_wrapper(\n+    app: \"Celery\",\n+    task_fun: typing.Callable[..., typing.Any],\n+    task_name: str,\n+    strict: bool = True,\n+    context: typing.Optional[typing.Dict[str, typing.Any]] = None,\n+    dump_kwargs: typing.Optional[typing.Dict[str, typing.Any]] = None\n+):\n+    \"\"\"Wrapper to validate arguments and serialize return values using Pydantic.\"\"\"\n+    try:\n+        pydantic = importlib.import_module('pydantic')\n+    except ModuleNotFoundError as ex:\n+        raise ImproperlyConfigured('You need to install pydantic to use pydantic model serialization.') from ex\n+\n+    BaseModel: typing.Type['BaseModel'] = pydantic.BaseModel  # noqa: F811  # only defined when type checking\n+\n+    if context is None:\n+        context = {}\n+    if dump_kwargs is None:\n+        dump_kwargs = {}\n+    dump_kwargs.setdefault('mode', 'json')\n+\n+    task_signature = inspect.signature(task_fun)\n+\n+    @functools.wraps(task_fun)\n+    def wrapper(*task_args, **task_kwargs):\n+        # Validate task parameters if type hinted as BaseModel\n+        bound_args = task_signature.bind(*task_args, **task_kwargs)\n+        for arg_name, arg_value in bound_args.arguments.items():\n+            arg_annotation = task_signature.parameters[arg_name].annotation\n+            if issubclass(arg_annotation, BaseModel):\n+                bound_args.arguments[arg_name] = arg_annotation.model_validate(\n+                    arg_value,\n+                    strict=strict,\n+                    context={**context, 'celery_app': app, 'celery_task_name': task_name},\n+                )\n+\n+        # Call the task with (potentially) converted arguments\n+        returned_value = task_fun(*bound_args.args, **bound_args.kwargs)\n+\n+        # Dump Pydantic model if the returned value is an instance of pydantic.BaseModel *and* its\n+        # class matches the typehint\n+        if (\n+            isinstance(returned_value, BaseModel)\n+            and isinstance(returned_value, task_signature.return_annotation)\n+        ):\n+            return returned_value.model_dump(**dump_kwargs)\n+\n+        return returned_value\n+\n+    return wrapper\n+\n+\n class PendingConfiguration(UserDict, AttributeDictMixin):\n     # `app.conf` will be of this type before being explicitly configured,\n     # meaning the app can keep any configuration set directly\n@@ -469,13 +529,27 @@ def cons(app):\n     def type_checker(self, fun, bound=False):\n         return staticmethod(head_from_fun(fun, bound=bound))\n \n-    def _task_from_fun(self, fun, name=None, base=None, bind=False, **options):\n+    def _task_from_fun(\n+        self,\n+        fun,\n+        name=None,\n+        base=None,\n+        bind=False,\n+        pydantic: bool = False,\n+        pydantic_strict: bool = True,\n+        pydantic_context: typing.Optional[typing.Dict[str, typing.Any]] = None,\n+        pydantic_dump_kwargs: typing.Optional[typing.Dict[str, typing.Any]] = None,\n+        **options,\n+    ):\n         if not self.finalized and not self.autofinalize:\n             raise RuntimeError('Contract breach: app not finalized')\n         name = name or self.gen_task_name(fun.__name__, fun.__module__)\n         base = base or self.Task\n \n         if name not in self._tasks:\n+            if pydantic is True:\n+                fun = pydantic_wrapper(self, fun, name, pydantic_strict, pydantic_context, pydantic_dump_kwargs)\n+\n             run = fun if bind else staticmethod(fun)\n             task = type(fun.__name__, (base,), dict({\n                 'app': self,\ndiff --git a/docs/userguide/tasks.rst b/docs/userguide/tasks.rst\nindex 1fc99c39962..88d1b8022ed 100644\n--- a/docs/userguide/tasks.rst\n+++ b/docs/userguide/tasks.rst\n@@ -795,6 +795,62 @@ You can also set `autoretry_for`, `max_retries`, `retry_backoff`, `retry_backoff\n \tThis allows to exclude some exceptions that match `autoretry_for\n \t<Task.autoretry_for>`:attr: but for which you don't want a retry.\n \n+.. _task-pydantic:\n+\n+Argument validation with Pydantic\n+=================================\n+\n+.. versionadded:: 5.5.0\n+\n+You can use Pydantic_ to validate and convert arguments as well as serializing\n+results based on typehints by passing ``pydantic=True``. For example:\n+\n+.. code-block:: python\n+\n+    from pydantic import BaseModel\n+\n+    class ArgModel(BaseModel):\n+        value: int\n+\n+    class ReturnModel(BaseModel):\n+        value: str\n+\n+    @app.task(pydantic=True)\n+    def x(arg: ArgModel) -> ReturnModel:\n+        # args/kwargs type hinted as Pydantic model will be converted\n+        assert isinstance(arg, ArgModel)\n+\n+        # The returned model will be converted to a dict automatically\n+        return ReturnModel(value=f\"example: {arg.value}\")\n+\n+The task can then be called using a dict matching the model, and you'll receive\n+the returned model \"dumped\" (serialized using ``BaseModel.model_dump()``):\n+\n+.. code-block:: python\n+\n+   >>> result = x.delay({'value': 1})\n+   >>> result.get(timeout=1)\n+   {'value': 'example: 1'}\n+\n+There are a few more options influencing Pydantic behavior:\n+\n+.. attribute:: Task.pydantic_strict\n+\n+   By default, `strict mode <https://docs.pydantic.dev/dev/concepts/strict_mode/>`_\n+   is enabled. You can pass ``False`` to disable strict model validation.\n+\n+.. attribute:: Task.pydantic_context\n+\n+   Pass `additional validation context\n+   <https://docs.pydantic.dev/dev/concepts/validators/#validation-context>`_ during\n+   Pydantic model validation. The context already includes the application object as\n+   ``celery_app`` and the task name as ``celery_task_name`` by default.\n+\n+.. attribute:: Task.pydantic_dump_kwargs\n+\n+   When serializing a result, pass these additional arguments to ``dump_kwargs()``.\n+   By default, only ``mode='json'`` is passed.\n+\n \n .. _task-options:\n \n@@ -2091,3 +2147,4 @@ To make API calls to `Akismet`_ I use the `akismet.py`_ library written by\n .. _`Michael Foord`: http://www.voidspace.org.uk/\n .. _`exponential backoff`: https://en.wikipedia.org/wiki/Exponential_backoff\n .. _`jitter`: https://en.wikipedia.org/wiki/Jitter\n+.. _`Pydantic`: https://docs.pydantic.dev/\ndiff --git a/examples/pydantic/__init__.py b/examples/pydantic/__init__.py\nnew file mode 100644\nindex 00000000000..e69de29bb2d\ndiff --git a/examples/pydantic/tasks.py b/examples/pydantic/tasks.py\nnew file mode 100644\nindex 00000000000..70b821338c1\n--- /dev/null\n+++ b/examples/pydantic/tasks.py\n@@ -0,0 +1,21 @@\n+from pydantic import BaseModel\n+\n+from celery import Celery\n+\n+app = Celery('tasks', broker='amqp://')\n+\n+\n+class ArgModel(BaseModel):\n+    value: int\n+\n+\n+class ReturnModel(BaseModel):\n+    value: str\n+\n+\n+@app.task(pydantic=True)\n+def x(arg: ArgModel) -> ReturnModel:\n+    # args/kwargs type hinted as Pydantic model will be converted\n+    assert isinstance(arg, ArgModel)\n+    # The returned model will be converted to a dict automatically\n+    return ReturnModel(value=f\"example: {arg.value}\")\ndiff --git a/requirements/extras/pydantic.txt b/requirements/extras/pydantic.txt\nnew file mode 100644\nindex 00000000000..29ac1fa96c9\n--- /dev/null\n+++ b/requirements/extras/pydantic.txt\n@@ -0,0 +1,1 @@\n+pydantic>=2.4\ndiff --git a/setup.py b/setup.py\nindex 324f6c0e607..8cfc1749389 100755\n--- a/setup.py\n+++ b/setup.py\n@@ -30,6 +30,7 @@\n     'mongodb',\n     'msgpack',\n     'pymemcache',\n+    'pydantic',\n     'pyro',\n     'pytest',\n     'redis',\ndiff --git a/t/integration/tasks.py b/t/integration/tasks.py\nindex f09492f3fd5..752db0278c3 100644\n--- a/t/integration/tasks.py\n+++ b/t/integration/tasks.py\n@@ -2,6 +2,8 @@\n from collections.abc import Iterable\n from time import sleep\n \n+from pydantic import BaseModel\n+\n from celery import Signature, Task, chain, chord, group, shared_task\n from celery.canvas import signature\n from celery.exceptions import SoftTimeLimitExceeded\n@@ -475,6 +477,22 @@ def replaced_with_me():\n     return True\n \n \n+class AddParameterModel(BaseModel):\n+    x: int\n+    y: int\n+\n+\n+class AddResultModel(BaseModel):\n+    result: int\n+\n+\n+@shared_task(pydantic=True)\n+def add_pydantic(data: AddParameterModel) -> AddResultModel:\n+    \"\"\"Add two numbers, but with parameters and results using Pydantic model serialization.\"\"\"\n+    value = data.x + data.y\n+    return AddResultModel(result=value)\n+\n+\n if LEGACY_TASKS_DISABLED:\n     class StampOnReplace(StampingVisitor):\n         stamp = {\"StampOnReplace\": \"This is the replaced task\"}\ndiff --git a/t/smoke/workers/docker/dev b/t/smoke/workers/docker/dev\nindex 82427c19573..b932dd4b393 100644\n--- a/t/smoke/workers/docker/dev\n+++ b/t/smoke/workers/docker/dev\n@@ -38,7 +38,7 @@ WORKDIR /celery\n COPY --chown=test_user:test_user . /celery\n RUN pip install --no-cache-dir --upgrade \\\n     pip \\\n-    -e /celery[redis,pymemcache] \\\n+    -e /celery[redis,pymemcache,pydantic] \\\n     pytest-celery>=1.0.0\n \n # The workdir must be /app\ndiff --git a/t/smoke/workers/docker/pypi b/t/smoke/workers/docker/pypi\nindex 699f290e119..87344cee2ad 100644\n--- a/t/smoke/workers/docker/pypi\n+++ b/t/smoke/workers/docker/pypi\n@@ -38,7 +38,8 @@ EXPOSE 5678\n RUN pip install --no-cache-dir --upgrade \\\n     pip \\\n     celery[redis,pymemcache]${CELERY_VERSION:+==$CELERY_VERSION} \\\n-    pytest-celery>=1.0.0\n+    pytest-celery>=1.0.0 \\\n+    pydantic>=2.4\n \n # The workdir must be /app\n WORKDIR /app\n", "test_patch": "diff --git a/requirements/test.txt b/requirements/test.txt\nindex 58265c8cad9..bf569095bdb 100644\n--- a/requirements/test.txt\n+++ b/requirements/test.txt\n@@ -15,3 +15,4 @@ pre-commit>=3.8.0; python_version >= '3.9'\n -r extras/msgpack.txt\n -r extras/mongodb.txt\n -r extras/gcs.txt\n+-r extras/pydantic.txt\ndiff --git a/t/integration/test_tasks.py b/t/integration/test_tasks.py\nindex 87587119b15..060176e8b15 100644\n--- a/t/integration/test_tasks.py\n+++ b/t/integration/test_tasks.py\n@@ -16,9 +16,9 @@\n from celery.worker import state as worker_state\n \n from .conftest import TEST_BACKEND, get_active_redis_channels, get_redis_connection\n-from .tasks import (ClassBasedAutoRetryTask, ExpectedException, add, add_ignore_result, add_not_typed, fail,\n-                    fail_unpickleable, print_unicode, retry, retry_once, retry_once_headers, retry_once_priority,\n-                    retry_unpickleable, return_properties, second_order_replace1, sleeping)\n+from .tasks import (ClassBasedAutoRetryTask, ExpectedException, add, add_ignore_result, add_not_typed, add_pydantic,\n+                    fail, fail_unpickleable, print_unicode, retry, retry_once, retry_once_headers,\n+                    retry_once_priority, retry_unpickleable, return_properties, second_order_replace1, sleeping)\n \n TIMEOUT = 10\n \n@@ -128,6 +128,20 @@ def test_ignore_result(self, manager):\n         sleep(1)\n         assert result.result is None\n \n+    @flaky\n+    def test_pydantic_annotations(self, manager):\n+        \"\"\"Tests task call with Pydantic model serialization.\"\"\"\n+        results = []\n+        # Tests calling task only with args\n+        for i in range(10):\n+            results.append([i + i, add_pydantic.delay({'x': i, 'y': i})])\n+        for expected, result in results:\n+            value = result.get(timeout=10)\n+            assert value == {'result': expected}\n+            assert result.status == 'SUCCESS'\n+            assert result.ready() is True\n+            assert result.successful() is True\n+\n     @flaky\n     def test_timeout(self, manager):\n         \"\"\"Testing timeout of getting results from tasks.\"\"\"\ndiff --git a/t/unit/app/test_app.py b/t/unit/app/test_app.py\nindex 4c92f475d42..1ca508d89b3 100644\n--- a/t/unit/app/test_app.py\n+++ b/t/unit/app/test_app.py\n@@ -1,16 +1,19 @@\n import gc\n+import importlib\n import itertools\n import os\n import ssl\n import sys\n+import typing\n import uuid\n from copy import deepcopy\n from datetime import datetime, timedelta\n from datetime import timezone as datetime_timezone\n from pickle import dumps, loads\n-from unittest.mock import Mock, patch\n+from unittest.mock import DEFAULT, Mock, patch\n \n import pytest\n+from pydantic import BaseModel, ValidationInfo, model_validator\n from vine import promise\n \n from celery import Celery, _state\n@@ -505,6 +508,189 @@ def foo():\n                 pass\n             check.assert_called_with(foo)\n \n+    def test_task_with_pydantic_with_no_args(self):\n+        \"\"\"Test a pydantic task with no arguments or return value.\"\"\"\n+        with self.Celery() as app:\n+            check = Mock()\n+\n+            @app.task(pydantic=True)\n+            def foo():\n+                check()\n+\n+            assert foo() is None\n+            check.assert_called_once()\n+\n+    def test_task_with_pydantic_with_arg_and_kwarg(self):\n+        \"\"\"Test a pydantic task with simple (non-pydantic) arg/kwarg and return value.\"\"\"\n+        with self.Celery() as app:\n+            check = Mock()\n+\n+            @app.task(pydantic=True)\n+            def foo(arg: int, kwarg: bool = True) -> int:\n+                check(arg, kwarg=kwarg)\n+                return 1\n+\n+            assert foo(0) == 1\n+            check.assert_called_once_with(0, kwarg=True)\n+\n+    def test_task_with_pydantic_with_pydantic_arg_and_default_kwarg(self):\n+        \"\"\"Test a pydantic task with pydantic arg/kwarg and return value.\"\"\"\n+\n+        class ArgModel(BaseModel):\n+            arg_value: int\n+\n+        class KwargModel(BaseModel):\n+            kwarg_value: int\n+\n+        kwarg_default = KwargModel(kwarg_value=1)\n+\n+        class ReturnModel(BaseModel):\n+            ret_value: int\n+\n+        with self.Celery() as app:\n+            check = Mock()\n+\n+            @app.task(pydantic=True)\n+            def foo(arg: ArgModel, kwarg: KwargModel = kwarg_default) -> ReturnModel:\n+                check(arg, kwarg=kwarg)\n+                return ReturnModel(ret_value=2)\n+\n+            assert foo({'arg_value': 0}) == {'ret_value': 2}\n+            check.assert_called_once_with(ArgModel(arg_value=0), kwarg=kwarg_default)\n+            check.reset_mock()\n+\n+            # Explicitly pass kwarg (but as argument)\n+            assert foo({'arg_value': 3}, {'kwarg_value': 4}) == {'ret_value': 2}\n+            check.assert_called_once_with(ArgModel(arg_value=3), kwarg=KwargModel(kwarg_value=4))\n+            check.reset_mock()\n+\n+            # Explicitly pass all arguments as kwarg\n+            assert foo(arg={'arg_value': 5}, kwarg={'kwarg_value': 6}) == {'ret_value': 2}\n+            check.assert_called_once_with(ArgModel(arg_value=5), kwarg=KwargModel(kwarg_value=6))\n+\n+    def test_task_with_pydantic_with_task_name_in_context(self):\n+        \"\"\"Test that the task name is passed to as additional context.\"\"\"\n+\n+        class ArgModel(BaseModel):\n+            value: int\n+\n+            @model_validator(mode='after')\n+            def validate_context(self, info: ValidationInfo):\n+                context = info.context\n+                assert context\n+                assert context.get('celery_task_name') == 't.unit.app.test_app.task'\n+                return self\n+\n+        with self.Celery() as app:\n+            check = Mock()\n+\n+            @app.task(pydantic=True)\n+            def task(arg: ArgModel):\n+                check(arg)\n+                return 1\n+\n+            assert task({'value': 1}) == 1\n+\n+    def test_task_with_pydantic_with_strict_validation(self):\n+        \"\"\"Test a pydantic task with/without strict model validation.\"\"\"\n+\n+        class ArgModel(BaseModel):\n+            value: int\n+\n+        with self.Celery() as app:\n+            check = Mock()\n+\n+            @app.task(pydantic=True, pydantic_strict=True)\n+            def strict(arg: ArgModel):\n+                check(arg)\n+\n+            @app.task(pydantic=True, pydantic_strict=False)\n+            def loose(arg: ArgModel):\n+                check(arg)\n+\n+            # In Pydantic, passing an \"exact int\" as float works without strict validation\n+            assert loose({'value': 1.0}) is None\n+            check.assert_called_once_with(ArgModel(value=1))\n+            check.reset_mock()\n+\n+            # ... but a non-strict value will raise an exception\n+            with pytest.raises(ValueError):\n+                loose({'value': 1.1})\n+            check.assert_not_called()\n+\n+            # ... with strict validation, even an \"exact int\" will not work:\n+            with pytest.raises(ValueError):\n+                strict({'value': 1.0})\n+            check.assert_not_called()\n+\n+    def test_task_with_pydantic_with_extra_context(self):\n+        \"\"\"Test passing additional validation context to the model.\"\"\"\n+\n+        class ArgModel(BaseModel):\n+            value: int\n+\n+            @model_validator(mode='after')\n+            def validate_context(self, info: ValidationInfo):\n+                context = info.context\n+                assert context, context\n+                assert context.get('foo') == 'bar'\n+                return self\n+\n+        with self.Celery() as app:\n+            check = Mock()\n+\n+            @app.task(pydantic=True, pydantic_context={'foo': 'bar'})\n+            def task(arg: ArgModel):\n+                check(arg.value)\n+                return 1\n+\n+            assert task({'value': 1}) == 1\n+            check.assert_called_once_with(1)\n+\n+    def test_task_with_pydantic_with_dump_kwargs(self):\n+        \"\"\"Test passing keyword arguments to model_dump().\"\"\"\n+\n+        class ArgModel(BaseModel):\n+            value: int\n+\n+        class RetModel(BaseModel):\n+            value: datetime\n+            unset_value: typing.Optional[int] = 99  # this would be in the output, if exclude_unset weren't True\n+\n+        with self.Celery() as app:\n+            check = Mock()\n+\n+            @app.task(pydantic=True, pydantic_dump_kwargs={'mode': 'python', 'exclude_unset': True})\n+            def task(arg: ArgModel) -> RetModel:\n+                check(arg)\n+                return RetModel(value=datetime(2024, 5, 14, tzinfo=timezone.utc))\n+\n+            assert task({'value': 1}) == {'value': datetime(2024, 5, 14, tzinfo=timezone.utc)}\n+            check.assert_called_once_with(ArgModel(value=1))\n+\n+    def test_task_with_pydantic_with_pydantic_not_installed(self):\n+        \"\"\"Test configuring a task with Pydantic when pydantic is not installed.\"\"\"\n+\n+        with self.Celery() as app:\n+            @app.task(pydantic=True)\n+            def task():\n+                return\n+\n+            # mock function will raise ModuleNotFoundError only if pydantic is imported\n+            def import_module(name, *args, **kwargs):\n+                if name == 'pydantic':\n+                    raise ModuleNotFoundError('Module not found.')\n+                return DEFAULT\n+\n+            msg = r'^You need to install pydantic to use pydantic model serialization\\.$'\n+            with patch(\n+                'celery.app.base.importlib.import_module',\n+                side_effect=import_module,\n+                wraps=importlib.import_module\n+            ):\n+                with pytest.raises(ImproperlyConfigured, match=msg):\n+                    task()\n+\n     def test_task_sets_main_name_MP_MAIN_FILE(self):\n         from celery.utils import imports as _imports\n         _imports.MP_MAIN_FILE = __file__\n", "problem_statement": "Adding support for pydantic model serialization\n<!--\r\nPlease fill this template entirely and do not erase parts of it.\r\nWe reserve the right to close without a response\r\nfeature requests which are incomplete.\r\n-->\r\n# Checklist\r\n<!--\r\nTo check an item on the list replace [ ] with [x].\r\n-->\r\n\r\n- [x] I have checked the [issues list](https://github.com/celery/celery/issues?utf8=%E2%9C%93&q=is%3Aissue+label%3A%22Issue+Type%3A+Feature+Request%22+)\r\n  for similar or identical feature requests.\r\n- [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?utf8=%E2%9C%93&q=is%3Apr+label%3A%22PR+Type%3A+Feature%22+)\r\n  for existing proposed implementations of this feature.\r\n- [x] I have checked the [commit log](https://github.com/celery/celery/commits/main)\r\n  to find out if the same feature was already implemented in the\r\n  main branch.\r\n- [x] I have included all related issues and possible duplicate issues\r\n  in this issue (If there are none, check this box anyway).\r\n\r\n## Related Issues and Possible Duplicates\r\n<!--\r\nPlease make sure to search and mention any related issues\r\nor possible duplicates to this issue as requested by the checklist above.\r\n\r\nThis may or may not include issues in other repositories that the Celery project\r\nmaintains or other repositories that are dependencies of Celery.\r\n\r\nIf you don't know how to mention issues, please refer to Github's documentation\r\non the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests\r\n-->\r\n\r\n#### Related Issues\r\n\r\n- None\r\n\r\n#### Possible Duplicates\r\n\r\n- None\r\n\r\n# Brief Summary\r\n\r\nHello, since [Pydantic](https://github.com/pydantic/pydantic) is a growing data validation util within the Python community and has become the de-facto solution in some popular web frameworks, ilke [FastAPI](https://github.com/tiangolo/fastapi), I wonder why not allowing support to pass Pydantic models as task arguments. Since it supports json serialization out of the box, I guess that it should be fairly doable.\r\n\r\n# Design\r\n\r\n## Architectural Considerations\r\n\r\nPlease note that I haven't gone through the code base extensively, so I don't know how to best implement this, so this is just an idea:\r\n\r\nAdding a custom serializer that is a wrapper around Pydantic's base model methods`. model_dump_json()` (in V2) or `.json()` (in V1). \r\n\r\nOne could add something like\r\n\r\n\r\n```python\r\n# pydantic\r\nclass PydanticEncoder(json.JSONEncoder): \r\n     ....\r\n\r\n\r\ndef decode():\r\n    ...\r\n\r\n\r\ndef encode():\r\n     ...   \r\n````     \r\n\r\nand register it in kombu\r\n\r\n```python\r\n\r\nfrom kombu.serialization import register\r\nfrom .pydantic import encode, decode\r\n\r\nregister('pydantic-json', encode, decode, \r\n    content_type='application/json',\r\n    content_encoding='utf-8') \r\n```\r\n## Proposed Behavior\r\n\r\nGiven a pydantic model like\r\n\r\n```python\r\nfrom pydantic import BaseModel\r\n\r\nclass TaskParams(BaseModel):\r\n       foo: str\r\n       bar: int\r\n```\r\n\r\nand a celery task that is defined like:\r\n\r\n```python\r\n\r\n@shared_task\r\ndef my_task(foo: str, bar: int)\r\n      ...\r\n```\r\n\r\nwriting this like:\r\n\r\n```python\r\n@shared_task\r\ndef my_task(params: TaskParams)\r\n      ...\r\n```\r\n\r\nshould be passed to the broker correctly serialized as json out of the box by celery\r\n\r\n## Proposed UI/UX\r\n<!--\r\nPlease provide your ideas for the API, CLI options,\r\nconfiguration key names etc. that will be introduced for this feature.\r\n-->\r\n\r\nNone. \r\n\r\nWhen installing, this could be installed as an option\r\n\r\n```bash\r\npip install celery[pydantic]\r\n```\r\n\r\n## Diagrams\r\n<!--\r\nPlease include any diagrams that might be relevant\r\nto the implementation of this feature such as:\r\n* Class Diagrams\r\n* Sequence Diagrams\r\n* Activity Diagrams\r\nYou can drag and drop images into the text box to attach them to this issue.\r\n-->\r\nN/A\r\n\r\n## Alternatives\r\n\r\nI haven't personally tried  it out, but I guess there should be 2 options:\r\n\r\n1. Registering pydantic in kombu's json serializer, just like in the docs https://docs.celeryq.dev/projects/kombu/en/latest/userguide/serialization.html\n", "hints_text": "Would love this feature tbh. I've been working with FastAPI+Pydantic+SQLAlchemy for a few years now and, especially with recent developments in Pydantic V2, it's now so nice to use it for serialisation/deserialisation.\r\n\r\nMy current approach, since this isn't yet here, so to do `my_model.model_dump_json()` on one end, and `Model.model_validate_json(json_blob)` at the other end. Any nitty gritty of app-specific serialisation is defined directly in the `Model` Pydantic class and automatically syncs up between the Celery Worker and my API which is issuing the tasks.\r\n\r\nAlso, Pydantic has its own JSON parser integrated, and it handles types in a very specific and predefined ways (e.g. datetimes), so I prefer to use `model_dump_json()` which outputs a string, rather than `model_dump(mode=\"json\")` which outputs a dict and then let Celery/Kombu do the serialisation down to JSON. I think a kombu serialiser should follow a similar approach.\nI got a rudimentary solution to this working: https://gist.github.com/badge/926165ff21ce438c8ddb58aba87021e0\r\n\nI tried out @badge's solution and it inspired me to make my own solution to work for my use case.\r\nI specialized it to work with classes and generic inheritance. I think it could be made to work with decorators using the `base` argument. Here is a working example: <https://gist.github.com/martinkozle/37c7eef95f9bbb5ace8bc6e32f379673>\r\nIt works great with the `celery-types` type stubs and strict mypy type checking.\r\nIf Pydantic support isn't planned in Celery then I can make my `BasePydanticTask` implementation into a separate package if there is interest.\n@martinkozle can you please create a pull request for celery so we can upvote it?\n> @martinkozle can you please create a pull request for celery so we can upvote it?\r\n\r\nI do not believe that the current implementation that I provided is suitable for upstream celery as is. It was made for my use case and codebase. It has some questionable design decisions by me, and some edge cases:\r\n\r\n- It uses typing and generics for the TypeAdapter type, although this can be swapped for class level variables instead.\r\n- It creates side effects by registering different kombu serializers for every task. I don't quite like this approach but didn't find a better way, and maybe there isn't a better way `\u00af\\_(\u30c4)_/\u00af`.\r\n- I designed it to be mainly used with class style tasks and inheritance, while the docs everywhere suggest to use decorators. So the definition of the TypeAdapter type needs to be done in a different way than the Generic, which currently requires explicit inheritance. Also idk if other than that it would be compatible with the decorators.\r\n- Doesn't work if you want to pass an argument which is an Union of Pydantic models with the same fields, then the same json would map to multiple valid Pydantic models. Although this would just be a bad model design when working with Pydantic and serialization in general.\r\n\r\nIf anyone has any suggestions on how to better adapt this solution to fit inside celery, then I would be happy to try to implement it.\r\nIn the meantime I will explore some other possibilities.\nAny additional discussion on this? I woud love to have this.\nAlso consider this: https://benninger.ca/posts/celery-serializer-pydantic/ (an approach we're currently using).\r\n(not our code, just thought it should be mentioned here)\n> Also consider this: https://benninger.ca/posts/celery-serializer-pydantic/ (an approach we're currently using).\r\n(not our code, just thought it should be mentioned here)\r\n\r\nI had a look at this. It's a clever approach and I'm sure it works in most scenarios, but it relies on the caller defining the class (any class that implements a `parse_obj()` function, really) that the data will be loaded with. That just seems somehow \"wrong\" to me.\r\n\r\nAt the very least, the task still has to verify that this is the correct model, but this could theoretically even be a security issue, as the caller can trick the Celery worker into running different code, right?\nLooking at this more closely today, I believe this is not really possible just with Celery Serializers as they are now. They do receive just the object, but not the task they deserialize this for.\r\n\r\nI try a different approach: Create a decorator that wraps `shared_task()`. Here's a simple proof of concept:\r\n\r\nhttps://gist.github.com/mathiasertl/40432b4d653085441c20affd8a8764e6\r\n\r\nFor the logs, I get:\r\n\r\n```\r\n[2024-05-06 21:40:50,005: INFO/MainProcess] Task django_ca.tasks.test_task[e109b0b9-dcb3-40c8-b4fe-4f5ba479aa8a] received\r\n[INFO     2024-05-06 21:40:50] Found typehints: {'data': <class 'django_ca.tasks.TestModel'>, 'return': <class 'django_ca.tasks.TestReturnModel'>}\r\n[INFO     2024-05-06 21:40:50] Validated data: TestModel(value=3)\r\n[INFO     2024-05-06 21:40:50] Task received value: TestModel(value=3)\r\n[INFO     2024-05-06 21:40:50] Wrapper dumps model: TestReturnModel(value='returned value')\r\n[2024-05-06 21:40:50,007: INFO/ForkPoolWorker-16] Task django_ca.tasks.test_task[e109b0b9-dcb3-40c8-b4fe-4f5ba479aa8a] succeeded in 0.0012026669992337702s: {'\r\nvalue': 'returned value'}\r\n```\r\n\r\n... this certainly needs refinement. But I'd be willing to create a PR if a maintainer (e.g. @Nusnus) would consider this approach.\nI think that would be a really useful feature. @mathiasertl the combination of the two approaches would be great: Celery serializer should just .model_dump() all pydantic models and the wrapper should based on the typehint execute a model_validate for pydantic models. \nThis would indeed be a very interesting feature\nI don't think we want to use \"duck typing\" and just blindly call `.model_dump()` and `.model_validate()`. This is because then we won't have support for types like `list[Model]`, `tuple[Model, Model]`, `dict[str, Model]`, etc.\r\n\r\nFor serialization you can just use `from pydantic_core import to_json`, which works with any combination of nested Python data structures and Pydantic models. For de-serialization I really like the idea of using type hinting in order to get the types, and then you can construct a `TypeAdapter` for each argument.", "created_at": "2024-05-14T10:08:56Z"}
{"repo": "celery/celery", "pull_number": 9021, "instance_id": "celery__celery-9021", "issue_numbers": ["9020"], "base_commit": "53e96fa6eebd23aee782c2e23aa91a525151f65e", "patch": "diff --git a/celery/canvas.py b/celery/canvas.py\nindex cf322f3b8a1..9f4d2f0ce74 100644\n--- a/celery/canvas.py\n+++ b/celery/canvas.py\n@@ -974,9 +974,7 @@ def __or__(self, other):\n                 tasks, other), app=self._app)\n         elif isinstance(other, _chain):\n             # chain | chain -> chain\n-            # use type(self) for _chain subclasses\n-            return type(self)(seq_concat_seq(\n-                self.unchain_tasks(), other.unchain_tasks()), app=self._app)\n+            return reduce(operator.or_, other.unchain_tasks(), self)\n         elif isinstance(other, Signature):\n             if self.tasks and isinstance(self.tasks[-1], group):\n                 # CHAIN [last item is group] | TASK -> chord\n", "test_patch": "diff --git a/t/integration/test_canvas.py b/t/integration/test_canvas.py\nindex bb5b80ffa67..d2474fa2351 100644\n--- a/t/integration/test_canvas.py\n+++ b/t/integration/test_canvas.py\n@@ -1108,6 +1108,25 @@ def test_group_in_center_of_chain(self, manager):\n         res = t3.apply_async()  # should not raise\n         assert res.get(timeout=TIMEOUT) == 60\n \n+    def test_upgrade_to_chord_inside_chains(self, manager):\n+        if not manager.app.conf.result_backend.startswith(\"redis\"):\n+            raise pytest.skip(\"Requires redis result backend.\")\n+        try:\n+            manager.app.backend.ensure_chords_allowed()\n+        except NotImplementedError as e:\n+            raise pytest.skip(e.args[0])\n+\n+        redis_key = str(uuid.uuid4())\n+        group1 = group(redis_echo.si('a', redis_key), redis_echo.si('a', redis_key))\n+        group2 = group(redis_echo.si('a', redis_key), redis_echo.si('a', redis_key))\n+        chord1 = group1 | group2\n+        chain1 = chain(chord1, (redis_echo.si('a', redis_key) | redis_echo.si('b', redis_key)))\n+        chain1.apply_async().get(timeout=TIMEOUT)\n+        redis_connection = get_redis_connection()\n+        actual = redis_connection.lrange(redis_key, 0, -1)\n+        assert actual.count(b'b') == 1\n+        redis_connection.delete(redis_key)\n+\n \n class test_result_set:\n \ndiff --git a/t/smoke/tests/test_canvas.py b/t/smoke/tests/test_canvas.py\nindex 7ecf838af90..6590315f024 100644\n--- a/t/smoke/tests/test_canvas.py\n+++ b/t/smoke/tests/test_canvas.py\n@@ -1,8 +1,11 @@\n+import uuid\n+\n import pytest\n from pytest_celery import RESULT_TIMEOUT, CeleryTestSetup\n \n from celery.canvas import chain, chord, group, signature\n-from t.integration.tasks import ExpectedException, add, fail, identity\n+from t.integration.conftest import get_redis_connection\n+from t.integration.tasks import ExpectedException, add, fail, identity, redis_echo\n \n \n class test_signature:\n@@ -52,6 +55,22 @@ def test_chain_gets_last_task_id_with_failing_tasks_in_chain(self, celery_setup:\n         with pytest.raises(ExpectedException):\n             res.get(timeout=RESULT_TIMEOUT)\n \n+    def test_upgrade_to_chord_inside_chains(self, celery_setup: CeleryTestSetup):\n+        redis_key = str(uuid.uuid4())\n+        queue = celery_setup.worker.worker_queue\n+        group1 = group(redis_echo.si(\"a\", redis_key), redis_echo.si(\"a\", redis_key))\n+        group2 = group(redis_echo.si(\"a\", redis_key), redis_echo.si(\"a\", redis_key))\n+        chord1 = group1 | group2\n+        chain1 = chain(\n+            chord1, (redis_echo.si(\"a\", redis_key) | redis_echo.si(\"b\", redis_key).set(queue=queue))\n+        )\n+        chain1.apply_async(queue=queue).get(timeout=RESULT_TIMEOUT)\n+        redis_connection = get_redis_connection()\n+        actual = redis_connection.lrange(redis_key, 0, -1)\n+        assert actual.count(b\"a\") == 5\n+        assert actual.count(b\"b\") == 1\n+        redis_connection.delete(redis_key)\n+\n \n class test_chord:\n     def test_sanity(self, celery_setup: CeleryTestSetup):\ndiff --git a/t/unit/tasks/test_canvas.py b/t/unit/tasks/test_canvas.py\nindex 9bd4f6b75dd..1f901376205 100644\n--- a/t/unit/tasks/test_canvas.py\n+++ b/t/unit/tasks/test_canvas.py\n@@ -825,6 +825,16 @@ def test_group_in_center_of_chain(self):\n         t2 = chord([self.add.si(1, 1), self.add.si(1, 1)], t1)\n         t2.freeze()  # should not raise\n \n+    def test_upgrade_to_chord_on_chain(self):\n+        group1 = group(self.add.si(10, 10), self.add.si(10, 10))\n+        group2 = group(self.xsum.s(), self.xsum.s())\n+        chord1 = group1 | group2\n+        chain1 = (self.xsum.si([5]) | self.add.s(1))\n+        final_task = chain(chord1, chain1)\n+        assert len(final_task.tasks) == 1 and isinstance(final_task.tasks[0], chord)\n+        assert isinstance(final_task.tasks[0].body, chord)\n+        assert final_task.tasks[0].body.body == chain1\n+\n \n class test_group(CanvasCase):\n     def test_repr(self):\n", "problem_statement": "A task will run multiple times when chaining chains with groups\n<!--\r\nPlease fill this template entirely and do not erase parts of it.\r\nWe reserve the right to close without a response\r\nbug reports which are incomplete.\r\n-->\r\n# Checklist\r\n<!--\r\nTo check an item on the list replace [ ] with [x].\r\n-->\r\n- [x] I have verified that the issue exists against the `main` branch of Celery.\r\n- [ ] This has already been asked to the [discussions forum](https://github.com/celery/celery/discussions) first.\r\n- [x] I have read the relevant section in the\r\n  [contribution guide](https://docs.celeryq.dev/en/main/contributing.html#other-bugs)\r\n  on reporting bugs.\r\n- [x] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Bug+Report%22+-label%3A%22Category%3A+Documentation%22)\r\n  for similar or identical bug reports.\r\n- [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22PR+Type%3A+Bugfix%22+-label%3A%22Category%3A+Documentation%22)\r\n  for existing proposed fixes.\r\n- [x] I have checked the [commit log](https://github.com/celery/celery/commits/main)\r\n  to find out if the bug was already fixed in the main branch.\r\n- [x] I have included all related issues and possible duplicate issues\r\n  in this issue (If there are none, check this box anyway).\r\n- [x] I have tried to reproduce the issue with [pytest-celery](https://docs.celeryq.dev/projects/pytest-celery/en/latest/userguide/celery-bug-report.html) and added the reproduction script below.\r\n\r\n## Mandatory Debugging Information\r\n\r\n- [ ] I have included the output of ``celery -A proj report`` in the issue.\r\n    (if you are not able to do this, then at least specify the Celery\r\n     version affected).\r\n- [ ] I have verified that the issue exists against the `main` branch of Celery.\r\n- [ ] I have included the contents of ``pip freeze`` in the issue.\r\n- [ ] I have included all the versions of all the external dependencies required\r\n  to reproduce this bug.\r\n\r\n## Optional Debugging Information\r\n<!--\r\nTry some of the below if you think they are relevant.\r\nIt will help us figure out the scope of the bug and how many users it affects.\r\n-->\r\n- [ ] I have tried reproducing the issue on more than one Python version\r\n  and/or implementation.\r\n- [ ] I have tried reproducing the issue on more than one message broker and/or\r\n  result backend.\r\n- [ ] I have tried reproducing the issue on more than one version of the message\r\n  broker and/or result backend.\r\n- [ ] I have tried reproducing the issue on more than one operating system.\r\n- [ ] I have tried reproducing the issue on more than one workers pool.\r\n- [ ] I have tried reproducing the issue with autoscaling, retries,\r\n  ETA/Countdown & rate limits disabled.\r\n- [ ] I have tried reproducing the issue after downgrading\r\n  and/or upgrading Celery and its dependencies.\r\n\r\n## Related Issues and Possible Duplicates\r\n<!--\r\nPlease make sure to search and mention any related issues\r\nor possible duplicates to this issue as requested by the checklist above.\r\n\r\nThis may or may not include issues in other repositories that the Celery project\r\nmaintains or other repositories that are dependencies of Celery.\r\n\r\nIf you don't know how to mention issues, please refer to Github's documentation\r\non the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests\r\n-->\r\n\r\n#### Related Issues\r\n\r\n- None\r\n\r\n#### Possible Duplicates\r\n\r\n- None\r\n\r\n## Environment & Settings\r\n<!-- Include the contents of celery --version below -->\r\n**Celery version**:\r\n<!-- Include the output of celery -A proj report below -->\r\n<details>\r\n<summary><b><code>celery report</code> Output:</b></summary>\r\n<p>\r\n\r\n```\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n# Steps to Reproduce\r\n\r\n## Required Dependencies\r\n<!-- Please fill the required dependencies to reproduce this issue -->\r\n- **Minimal Python Version**: N/A or Unknown\r\n- **Minimal Celery Version**: N/A or Unknown\r\n- **Minimal Kombu Version**: N/A or Unknown\r\n- **Minimal Broker Version**: N/A or Unknown\r\n- **Minimal Result Backend Version**: N/A or Unknown\r\n- **Minimal OS and/or Kernel Version**: N/A or Unknown\r\n- **Minimal Broker Client Version**: N/A or Unknown\r\n- **Minimal Result Backend Client Version**: N/A or Unknown\r\n\r\n### Python Packages\r\n<!-- Please fill the contents of pip freeze below -->\r\n<details>\r\n<summary><b><code>pip freeze</code> Output:</b></summary>\r\n<p>\r\n\r\n```\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n### Other Dependencies\r\n<!--\r\nPlease provide system dependencies, configuration files\r\nand other dependency information if applicable\r\n-->\r\n<details>\r\n<p>\r\nN/A\r\n</p>\r\n</details>\r\n\r\n## Minimally Reproducible Test Case\r\n<!--\r\nPlease provide a reproducible test case.\r\nRefer to the Reporting Bugs section in our contribution guide.\r\n\r\nWe prefer submitting test cases in the form of a PR to our integration test suite.\r\nIf you can provide one, please mention the PR number below.\r\nIf not, please attach the most minimal code example required to reproduce the issue below.\r\nIf the test case is too large, please include a link to a gist or a repository below.\r\n\r\nAlternatively, the pytest-celery plugin can be used to create standalone reproduction scripts\r\nthat can be added to this report. See the pytest-celery documentation for more information at\r\npytest-celery.readthedocs.io\r\n-->\r\n\r\n<details>\r\n<p>\r\n\r\n```python\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n# Expected Behavior\r\n<!-- Describe in detail what you expect to happen -->\r\n\r\n# Actual Behavior\r\n<!--\r\nDescribe in detail what actually happened.\r\nPlease include a backtrace and surround it with triple backticks (```).\r\nIn addition, include the Celery daemon logs, the broker logs,\r\nthe result backend logs and system logs below if they will help us debug\r\nthe issue.\r\n-->\r\nHappens on the latest version of celery.\r\n\r\nOn some complex scenarios, when there is some chaining of chains that contain groups, a task that is chained after a group will run multiple times.\r\nFor example, here task_b and task_c should be printed only once, but each one is printed twice\r\n```\r\napp = Celery('tasks', broker='memory://', result_backend='cache+memory://')\r\n\r\n\r\n@app.task(bind=True)\r\ndef task_a(self):\r\n    print(\"task_a\")\r\n\r\n\r\n@app.task(bind=True)\r\ndef task_b(self):\r\n    print('task_b')\r\n\r\n\r\n@app.task(bind=True)\r\ndef task_c(self):\r\n    print(\"task_c\")\r\n\r\n\r\ngroup1 = celery.group(task_a.si(), task_a.si())\r\ngroup2 = celery.group(task_a.si(), task_a.si())\r\nchord1 = group1 | group2\r\nchord2 = celery.chain(chord1, (task_b.si() | task_c.si())).apply_async()\r\n```\r\n\r\nOutput:\r\n```\r\n[2024-05-13 10:12:37,405: INFO/MainProcess] Task celery.task_a[d947d22f-900a-454e-a3cd-dee12dedfb4a] succeeded in 0.5s: None\r\n[2024-05-13 10:12:37,405: INFO/MainProcess] Task celery.task_a[4dc6c550-70fb-46d6-8967-1c92783682b4] received\r\n[2024-05-13 10:12:37,405: WARNING/MainProcess] task_a\r\n[2024-05-13 10:12:37,407: INFO/MainProcess] Task celery.task_a[4dc6c550-70fb-46d6-8967-1c92783682b4] succeeded in 0.0s: None\r\n[2024-05-13 10:12:37,407: INFO/MainProcess] Task celery.task_a[eb8dc8d9-30e6-4b30-aa4f-b867d85b817c] received\r\n[2024-05-13 10:12:37,408: WARNING/MainProcess] task_a\r\n[2024-05-13 10:12:37,408: INFO/MainProcess] Task celery.task_a[eb8dc8d9-30e6-4b30-aa4f-b867d85b817c] succeeded in 0.0s: None\r\n[2024-05-13 10:12:37,409: INFO/MainProcess] Task celery.task_b[88f28e3b-a986-4f20-9ce0-b4a28b0c669b] received\r\n[2024-05-13 10:12:37,409: WARNING/MainProcess] task_b\r\n[2024-05-13 10:12:37,410: INFO/MainProcess] Task celery.task_b[88f28e3b-a986-4f20-9ce0-b4a28b0c669b] succeeded in 0.0s: None\r\n[2024-05-13 10:12:37,410: INFO/MainProcess] Task celery.task_b[88f28e3b-a986-4f20-9ce0-b4a28b0c669b] received\r\n[2024-05-13 10:12:37,410: WARNING/MainProcess] task_b\r\n[2024-05-13 10:12:37,411: INFO/MainProcess] Task celery.task_b[88f28e3b-a986-4f20-9ce0-b4a28b0c669b] succeeded in 0.0s: None\r\n[2024-05-13 10:12:37,411: INFO/MainProcess] Task celery.task_c[8bb0b7fc-c3f8-4fea-a2c9-13e8f9ae7dbd] received\r\n[2024-05-13 10:12:37,411: WARNING/MainProcess] task_c\r\n[2024-05-13 10:12:37,412: INFO/MainProcess] Task celery.task_c[8bb0b7fc-c3f8-4fea-a2c9-13e8f9ae7dbd] succeeded in 0.0s: None\r\n[2024-05-13 10:12:37,412: INFO/MainProcess] Task celery.task_c[8bb0b7fc-c3f8-4fea-a2c9-13e8f9ae7dbd] received\r\n[2024-05-13 10:12:37,412: WARNING/MainProcess] task_c\r\n[2024-05-13 10:12:37,412: INFO/MainProcess] Task celery.task_c[8bb0b7fc-c3f8-4fea-a2c9-13e8f9ae7dbd] succeeded in 0.0s: None\r\n```\r\n\n", "hints_text": "", "created_at": "2024-05-13T07:16:16Z"}
{"repo": "celery/celery", "pull_number": 8984, "instance_id": "celery__celery-8984", "issue_numbers": ["8976"], "base_commit": "04af085f6d21d85cecaeafc406f8c08cb12502e7", "patch": "diff --git a/celery/contrib/django/task.py b/celery/contrib/django/task.py\nindex eacc7c66471..b0dc6677553 100644\n--- a/celery/contrib/django/task.py\n+++ b/celery/contrib/django/task.py\n@@ -12,10 +12,10 @@ class DjangoTask(Task):\n     Provide a nicer API to trigger tasks at the end of the DB transaction.\n     \"\"\"\n \n-    def delay_on_commit(self, *args, **kwargs):\n+    def delay_on_commit(self, *args, **kwargs) -> None:\n         \"\"\"Call :meth:`~celery.app.task.Task.delay` with Django's ``on_commit()``.\"\"\"\n-        return transaction.on_commit(functools.partial(self.delay, *args, **kwargs))\n+        transaction.on_commit(functools.partial(self.delay, *args, **kwargs))\n \n-    def apply_async_on_commit(self, *args, **kwargs):\n+    def apply_async_on_commit(self, *args, **kwargs) -> None:\n         \"\"\"Call :meth:`~celery.app.task.Task.apply_async` with Django's ``on_commit()``.\"\"\"\n-        return transaction.on_commit(functools.partial(self.apply_async, *args, **kwargs))\n+        transaction.on_commit(functools.partial(self.apply_async, *args, **kwargs))\ndiff --git a/docs/django/first-steps-with-django.rst b/docs/django/first-steps-with-django.rst\nindex 7091e391c01..5f93fb3ec63 100644\n--- a/docs/django/first-steps-with-django.rst\n+++ b/docs/django/first-steps-with-django.rst\n@@ -206,6 +206,11 @@ This API takes care of wrapping the call into the `on_commit`_ hook for you.\n In rare cases where you want to trigger a task without waiting, the existing\n :meth:`~celery.app.task.Task.delay` API is still available.\n \n+One key difference compared to the ``delay`` method, is that ``delay_on_commit``\n+will NOT return the task ID back to the caller. The task is not sent to the broker\n+when you call the method, only when the Django transaction finishes. If you need the\n+task ID, best to stick to :meth:`~celery.app.task.Task.delay`.\n+\n This task class should be used automatically if you've follow the setup steps above.\n However, if your app :ref:`uses a custom task base class <task-custom-classes>`,\n you'll need inherit from :class:`~celery.contrib.django.task.DjangoTask` instead of\n", "test_patch": "diff --git a/t/unit/contrib/django/test_task.py b/t/unit/contrib/django/test_task.py\nindex 52b45b84bc4..d1efa591d2b 100644\n--- a/t/unit/contrib/django/test_task.py\n+++ b/t/unit/contrib/django/test_task.py\n@@ -25,8 +25,8 @@ def on_commit(self):\n \n     def test_delay_on_commit(self, task_instance, on_commit):\n         result = task_instance.delay_on_commit()\n-        assert result is not None\n+        assert result is None\n \n     def test_apply_async_on_commit(self, task_instance, on_commit):\n         result = task_instance.apply_async_on_commit()\n-        assert result is not None\n+        assert result is None\n", "problem_statement": "New Django task with transaction atomic, return `None` instead of the task UUID\n<!--\r\nPlease fill this template entirely and do not erase parts of it.\r\nWe reserve the right to close without a response\r\nbug reports which are incomplete.\r\n-->\r\n# Checklist\r\n<!--\r\nTo check an item on the list replace [ ] with [x].\r\n-->\r\n- [x] I have verified that the issue exists against the `main` branch of Celery.\r\n- [ ] This has already been asked to the [discussions forum](https://github.com/celery/celery/discussions) first.\r\n- [x] I have read the relevant section in the\r\n  [contribution guide](https://docs.celeryq.dev/en/main/contributing.html#other-bugs)\r\n  on reporting bugs.\r\n- [x] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Bug+Report%22+-label%3A%22Category%3A+Documentation%22)\r\n  for similar or identical bug reports.\r\n- [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22PR+Type%3A+Bugfix%22+-label%3A%22Category%3A+Documentation%22)\r\n  for existing proposed fixes.\r\n- [x] I have checked the [commit log](https://github.com/celery/celery/commits/main)\r\n  to find out if the bug was already fixed in the main branch.\r\n- [ ] I have included all related issues and possible duplicate issues\r\n  in this issue (If there are none, check this box anyway).\r\n- [ ] I have tried to reproduce the issue with [pytest-celery](https://docs.celeryq.dev/projects/pytest-celery/en/latest/userguide/celery-bug-report.html) and added the reproduction script below.\r\n\r\n## Mandatory Debugging Information\r\n\r\n- [ ] I have included the output of ``celery -A proj report`` in the issue.\r\n    (if you are not able to do this, then at least specify the Celery\r\n     version affected).\r\n- [x] I have verified that the issue exists against the `main` branch of Celery.\r\n- [ ] I have included the contents of ``pip freeze`` in the issue.\r\n- [ ] I have included all the versions of all the external dependencies required\r\n  to reproduce this bug.\r\n\r\n## Optional Debugging Information\r\n<!--\r\nTry some of the below if you think they are relevant.\r\nIt will help us figure out the scope of the bug and how many users it affects.\r\n-->\r\n- [ ] I have tried reproducing the issue on more than one Python version\r\n  and/or implementation.\r\n- [ ] I have tried reproducing the issue on more than one message broker and/or\r\n  result backend.\r\n- [ ] I have tried reproducing the issue on more than one version of the message\r\n  broker and/or result backend.\r\n- [ ] I have tried reproducing the issue on more than one operating system.\r\n- [ ] I have tried reproducing the issue on more than one workers pool.\r\n- [ ] I have tried reproducing the issue with autoscaling, retries,\r\n  ETA/Countdown & rate limits disabled.\r\n- [ ] I have tried reproducing the issue after downgrading\r\n  and/or upgrading Celery and its dependencies.\r\n\r\n## Related Issues and Possible Duplicates\r\n<!--\r\nPlease make sure to search and mention any related issues\r\nor possible duplicates to this issue as requested by the checklist above.\r\n\r\nThis may or may not include issues in other repositories that the Celery project\r\nmaintains or other repositories that are dependencies of Celery.\r\n\r\nIf you don't know how to mention issues, please refer to Github's documentation\r\non the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests\r\n-->\r\n\r\n#### Related Issues\r\n\r\n- None\r\n\r\n#### Possible Duplicates\r\n\r\n- None\r\n\r\n## Environment & Settings\r\n<!-- Include the contents of celery --version below -->\r\n**Celery version**:\r\n5.4.0\r\n<!-- Include the output of celery -A proj report below -->\r\n<details>\r\n<summary><b><code>celery report</code> Output:</b></summary>\r\n<p>\r\n\r\n```\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n# Steps to Reproduce\r\n\r\n## Required Dependencies\r\n<!-- Please fill the required dependencies to reproduce this issue -->\r\n- **Minimal Python Version**: N/A or Unknown\r\n- **Minimal Celery Version**:5.4.0\r\n- **Minimal Kombu Version**: N/A or Unknown\r\n- **Minimal Broker Version**: N/A or Unknown\r\n- **Minimal Result Backend Version**: N/A or Unknown\r\n- **Minimal OS and/or Kernel Version**: N/A or Unknown\r\n- **Minimal Broker Client Version**: N/A or Unknown\r\n- **Minimal Result Backend Client Version**: N/A or Unknown\r\n\r\n### Python Packages\r\n<!-- Please fill the contents of pip freeze below -->\r\n<details>\r\n<summary><b><code>pip freeze</code> Output:</b></summary>\r\n<p>\r\n\r\n```\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n### Other Dependencies\r\n<!--\r\nPlease provide system dependencies, configuration files\r\nand other dependency information if applicable\r\n-->\r\n<details>\r\n<p>\r\nN/A\r\n</p>\r\n</details>\r\n\r\n## Minimally Reproducible Test Case\r\n<!--\r\nPlease provide a reproducible test case.\r\nRefer to the Reporting Bugs section in our contribution guide.\r\n\r\nWe prefer submitting test cases in the form of a PR to our integration test suite.\r\nIf you can provide one, please mention the PR number below.\r\nIf not, please attach the most minimal code example required to reproduce the issue below.\r\nIf the test case is too large, please include a link to a gist or a repository below.\r\n\r\nAlternatively, the pytest-celery plugin can be used to create standalone reproduction scripts\r\nthat can be added to this report. See the pytest-celery documentation for more information at\r\npytest-celery.readthedocs.io\r\n-->\r\n\r\n<details>\r\n<p>\r\n\r\n```python\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n# Expected Behavior\r\n<!-- Describe in detail what you expect to happen -->\r\nReturn the correct task uuid like before the release 5.4.0\r\n# Actual Behavior\r\n<!--\r\nDescribe in detail what actually happened.\r\nPlease include a backtrace and surround it with triple backticks (```).\r\nIn addition, include the Celery daemon logs, the broker logs,\r\nthe result backend logs and system logs below if they will help us debug\r\nthe issue.\r\n-->\r\n## Issue \u26a0\ufe0f \r\n\r\n`AttributeError: 'NoneType' object has no attribute 'id'\r\n\r\n```python\r\n@app.task\r\ndef example_task(id):\r\n       try:\r\n            obj = Model.objects.get(id=id)\r\n            obj.number += 1\r\n            obj.save(update_fields=[\"number\"])\r\n       except ObjectDoesNotExist as e:\r\n            return \"str(e)\"\r\n\r\n\r\ntask = example_task.delay_on_commit(example_model_instance.id)\r\nexample_model_instance.task_uuid = task.id\r\n```\r\n\r\nLiterally basic minimal task \ud83d\ude04 , but the `delay_on_commit` not return the task uuid for `AsyncResult`, and in my case django raise excpetion (rightly) AttributeError -> maybe the task with `delay_on_commit` return `None`?\r\n\r\ndocs:\r\n```python\r\nclass DjangoTask(Task):\r\n    \"\"\"\r\n    Extend the base :class:`~celery.app.task.Task` for Django.\r\n\r\n    Provide a nicer API to trigger tasks at the end of the DB transaction.\r\n    \"\"\"\r\n\r\n    def delay_on_commit(self, *args, **kwargs):\r\n        \"\"\"Call :meth:`~celery.app.task.Task.delay` with Django's ``on_commit()``.\"\"\"\r\n        return transaction.on_commit(functools.partial(self.delay, *args, **kwargs))\r\n\r\n    def apply_async_on_commit(self, *args, **kwargs):\r\n        \"\"\"Call :meth:`~celery.app.task.Task.apply_async` with Django's ``on_commit()``.\"\"\"\r\n        return transaction.on_commit(functools.partial(self.apply_async, *args, **kwargs))\r\n```\n", "hints_text": "@browniebroke \n> [x] I have tried to reproduce the issue with pytest-celery and added the reproduction script below.\n\nWhere is it?\n> > [x] I have tried to reproduce the issue with pytest-celery and added the reproduction script below.\r\n> \r\n> Where is it?\r\n\r\noh no sorry... my fault\nI think that django `transaction.on_commit` should return `None`, but this is the source of  django `transaction,py`\r\n[transaction.py](https://github.com/django/django/blob/main/django/db/transaction.py)\r\n\r\nif use the decorator instead the on_commit function?\r\ndelay and apply_async like before but:\r\n\r\n```python\r\n@transaction.atomic\r\ndef delay(.....)\r\n\r\n@transaction.atomic\r\ndef apply_async(.....)\r\n```\nYes, that's expected behaviour, `transaction.on_commit(...)` delays the execution of the wrapped function to a later stage, when the Django DB transaction finishes, so you won't get a task UUID until then. If you need access to the task UUID, you should use `delay` or `apply_async`, NOT the `..._on_commit` variants. While I didn't forsee this issue, I'm glad we picked entirely new method names, which means there is an escape hatch for you. We should document this caveat better, though.\r\n\r\nYour solution to use `@transaction.atomic` decorator does not do the same thing. It will open either a new transaction, or [a savepoint (if you're already in a transaction)](https://docs.djangoproject.com/en/5.0/topics/db/transactions/#savepoints), which may or may not have the same effect. Consider this case, for example:\r\n\r\n```python\r\ndef create_user(request):\r\n    with transaction.atomic():\r\n        user = User.objects.create(first_name=\"bob\", last_name=\"doe\")\r\n        send_welcome_email.delay_on_commit(user.pk)\r\n        # later in the transaction, do something that cause the transaction to be rolledback\r\n        # e.g. set the email to a value that already exists, raising IntegrityError\r\n        user.email = \"bob.doe@example.com\"\r\n        user.save() # <- raises IntegrityError, and rollback the whole transaction\r\n```\r\n\r\nWhen the whole transaction is rolled back, the whole operation failed, and user is NOT created in the DB, so we shouldn't send the welcome email. Since the transaction wasn't commited, the task never triggered (and no UUID was generated).\r\n\r\nNow if we replace the `delay_on_commit` by a call to `delay` with the `@transaction.atomic`, then the task will be queued in a nested transaction, and we will try to send welcome email (although that will probably crash, since the user wasn't actually persistend in the DB).\nI confirm that `delay_on_commit` return None:\r\n\r\n```python\r\nIn [9]: @app.task\r\n   ...: def sum_number(n1, n2):\r\n   ...:     print(n1+n2)\r\n   ...: \r\n\r\nIn [10]: task = sum_number.delay(1,2)\r\n\r\nIn [11]: task\r\nOut[11]: <AsyncResult: 3a4138ec-35d7-4d46-808f-3f73a1dfee53>\r\n\r\nIn [12]: task = sum_number.delay_on_commit(1,2)\r\n\r\nIn [13]: task\r\n\r\nIn [14]: assert task is None\r\n\r\nIn [15]: task is None\r\nOut[15]: True\r\n```\n> Yes, that's expected behaviour, `transaction.on_commit(...)` delays the execution of the wrapped function to a later stage, when the Django DB transaction finishes, so you won't get a task UUID until then. If you need access to the task UUID, you should use `delay` or `apply_async`, NOT the `..._on_commit` variants. While I didn't forsee this issue, I'm glad we picked entirely new method names, which means there is an escape hatch for you. We should document this caveat better, though.\r\n> \r\n> Your solution to use `@transaction.atomic` decorator does not do the same thing. It will open either a new transaction, or [a savepoint (if you're already in a transaction)](https://docs.djangoproject.com/en/5.0/topics/db/transactions/#savepoints), which may or may not have the same effect. Consider this case, for example:\r\n> \r\n> ```python\r\n> def create_user(request):\r\n>     with transaction.atomic():\r\n>         user = User.objects.create(first_name=\"bob\", last_name=\"doe\")\r\n>         send_welcome_email.delay_on_commit(user.pk)\r\n>         # later in the transaction, do something that cause the transaction to be rolledback\r\n>         # e.g. set the email to a value that already exists, raising IntegrityError\r\n>         user.email = \"bob.doe@example.com\"\r\n>         user.save() # <- raises IntegrityError, and rollback the whole transaction\r\n> ```\r\n> \r\n> When the whole transaction is rolled back, the whole operation failed, and user is NOT created in the DB, so we shouldn't send the welcome email. Since the transaction wasn't commited, the task never triggered (and no UUID was generated).\r\n> \r\n> Now if we replace the `delay_on_commit` by `call to`delay`with the`@transaction.atomic`, then the task will be queued in a nested transaction, and we will try to send welcome email (although that will probably crash, since the user wasn't actually persistend in the DB).\r\n\r\nOhh correctly...so if I need the task UUID just use mandatory the `delay` or `apply_async` call task method, correctly?\n> so if I need the task UUID just use mandatory the `delay` or `apply_async` call task method, correctly?\r\n\r\nCorrect, yes\nwe can develop the way to wait for the end of the transaction and return the task UUID?\r\nor it's a bad idea?\nThat kind of defeat the whole purpose of the `on_commit` utility, which is a mechanism to delay execution and move on without waiting. Here is the source: \r\n\r\nhttps://github.com/django/django/blob/ec8552417df51df8482df61b8ad78a7002634011/django/db/transaction.py#L129-L134\r\n\r\nWhich essentially calls this:\r\n\r\nhttps://github.com/django/django/blob/ec8552417df51df8482df61b8ad78a7002634011/django/db/backends/base/base.py#L727-L750\r\n\r\nThe function is NOT executed at this point, it's basically added to a list of callbacks (line 732):\r\n\r\n```python\r\nself.run_on_commit.append((set(self.savepoint_ids), func, robust))\r\n```\r\n\r\nThis list is read and executed in `run_and_clear_commit_hooks` (defined below):\r\n\r\nhttps://github.com/django/django/blob/ec8552417df51df8482df61b8ad78a7002634011/django/db/backends/base/base.py#L752C9-L769\r\n\r\nThat may happen in a completely different context that where you trigger the task.\nLet me know if anything is unclear, it would be good to incorporate explanations to lift any doubts you may have in the Celery docs...\n> That kind of defeat the whole purpose of the `on_commit` utility, which is a mechanism to delay execution and move on without waiting. Here is the source:\r\n> \r\n> https://github.com/django/django/blob/ec8552417df51df8482df61b8ad78a7002634011/django/db/transaction.py#L129-L134\r\n> \r\n> Which essentially calls this:\r\n> \r\n> https://github.com/django/django/blob/ec8552417df51df8482df61b8ad78a7002634011/django/db/backends/base/base.py#L727-L750\r\n> \r\n> The function is NOT executed at this point, it's basically added to a list of callbacks (line 732):\r\n> \r\n> ```python\r\n> self.run_on_commit.append((set(self.savepoint_ids), func, robust))\r\n> ```\r\n> \r\n> This list is read and executed in `run_and_clear_commit_hooks` (defined below):\r\n> \r\n> https://github.com/django/django/blob/ec8552417df51df8482df61b8ad78a7002634011/django/db/backends/base/base.py#L752C9-L769\r\n> \r\n> That may happen in a completely different context that where you trigger the task.\r\n\r\nSo... it's a way for not waiting the finish of the transaction, correctly?\r\n\r\nIn this `while`, iterate and call `func` to execute for every function to execute on_commit\r\nhttps://github.com/django/django/blob/ec8552417df51df8482df61b8ad78a7002634011/django/db/backends/base/base.py#L756-L767", "created_at": "2024-04-27T11:49:14Z"}
{"repo": "celery/celery", "pull_number": 8982, "instance_id": "celery__celery-8982", "issue_numbers": ["8981"], "base_commit": "e9ebd657b0327dde2170706d8d6b81f01e7bdad0", "patch": "diff --git a/celery/security/serialization.py b/celery/security/serialization.py\nindex c58ef906542..937abe63c72 100644\n--- a/celery/security/serialization.py\n+++ b/celery/security/serialization.py\n@@ -29,7 +29,8 @@ def serialize(self, data):\n         assert self._cert is not None\n         with reraise_errors('Unable to serialize: {0!r}', (Exception,)):\n             content_type, content_encoding, body = dumps(\n-                bytes_to_str(data), serializer=self._serializer)\n+                data, serializer=self._serializer)\n+\n             # What we sign is the serialized body, not the body itself.\n             # this way the receiver doesn't have to decode the contents\n             # to verify the signature (and thus avoiding potential flaws\n@@ -48,7 +49,7 @@ def deserialize(self, data):\n                                        payload['signer'],\n                                        payload['body'])\n             self._cert_store[signer].verify(body, signature, self._digest)\n-        return loads(bytes_to_str(body), payload['content_type'],\n+        return loads(body, payload['content_type'],\n                      payload['content_encoding'], force=True)\n \n     def _pack(self, body, content_type, content_encoding, signer, signature,\n@@ -84,7 +85,7 @@ def _unpack(self, payload, sep=str_to_bytes('\\x00\\x01')):\n             'signature': signature,\n             'content_type': bytes_to_str(v[0]),\n             'content_encoding': bytes_to_str(v[1]),\n-            'body': bytes_to_str(v[2]),\n+            'body': v[2],\n         }\n \n \n", "test_patch": "diff --git a/t/unit/security/test_serialization.py b/t/unit/security/test_serialization.py\nindex 6caf3857b81..cb16d9f14fc 100644\n--- a/t/unit/security/test_serialization.py\n+++ b/t/unit/security/test_serialization.py\n@@ -16,15 +16,19 @@\n \n class test_secureserializer(SecurityCase):\n \n-    def _get_s(self, key, cert, certs):\n+    def _get_s(self, key, cert, certs, serializer=\"json\"):\n         store = CertStore()\n         for c in certs:\n             store.add_cert(Certificate(c))\n-        return SecureSerializer(PrivateKey(key), Certificate(cert), store)\n+        return SecureSerializer(\n+            PrivateKey(key), Certificate(cert), store, serializer=serializer\n+        )\n \n-    def test_serialize(self):\n-        s = self._get_s(KEY1, CERT1, [CERT1])\n-        assert s.deserialize(s.serialize('foo')) == 'foo'\n+    @pytest.mark.parametrize(\"data\", [1, \"foo\", b\"foo\", {\"foo\": 1}])\n+    @pytest.mark.parametrize(\"serializer\", [\"json\", \"pickle\"])\n+    def test_serialize(self, data, serializer):\n+        s = self._get_s(KEY1, CERT1, [CERT1], serializer=serializer)\n+        assert s.deserialize(s.serialize(data)) == data\n \n     def test_deserialize(self):\n         s = self._get_s(KEY1, CERT1, [CERT1])\n", "problem_statement": "SecureSerializer fails on certain types and binary serializers\n<!--\r\nPlease fill this template entirely and do not erase parts of it.\r\nWe reserve the right to close without a response\r\nbug reports which are incomplete.\r\n-->\r\n# Checklist\r\n<!--\r\nTo check an item on the list replace [ ] with [x].\r\n-->\r\n- [X] I have verified that the issue exists against the `main` branch of Celery.\r\n- [ ] This has already been asked to the [discussions forum](https://github.com/celery/celery/discussions) first.\r\n- [X] I have read the relevant section in the\r\n  [contribution guide](https://docs.celeryq.dev/en/main/contributing.html#other-bugs)\r\n  on reporting bugs.\r\n- [X] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Bug+Report%22+-label%3A%22Category%3A+Documentation%22)\r\n  for similar or identical bug reports.\r\n- [X] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22PR+Type%3A+Bugfix%22+-label%3A%22Category%3A+Documentation%22)\r\n  for existing proposed fixes.\r\n- [X] I have checked the [commit log](https://github.com/celery/celery/commits/main)\r\n  to find out if the bug was already fixed in the main branch.\r\n- [X] I have included all related issues and possible duplicate issues\r\n  in this issue (If there are none, check this box anyway).\r\n- [X] I have tried to reproduce the issue with [pytest-celery](https://docs.celeryq.dev/projects/pytest-celery/en/latest/userguide/celery-bug-report.html) and added the reproduction script below.\r\n\r\n## Mandatory Debugging Information\r\n\r\n- [ ] I have included the output of ``celery -A proj report`` in the issue.\r\n    (if you are not able to do this, then at least specify the Celery\r\n     version affected).\r\n- [X] I have verified that the issue exists against the `main` branch of Celery.\r\n- [ ] I have included the contents of ``pip freeze`` in the issue.\r\n- [ ] I have included all the versions of all the external dependencies required\r\n  to reproduce this bug.\r\n\r\n## Optional Debugging Information\r\n<!--\r\nTry some of the below if you think they are relevant.\r\nIt will help us figure out the scope of the bug and how many users it affects.\r\n-->\r\n- [ ] I have tried reproducing the issue on more than one Python version\r\n  and/or implementation.\r\n- [ ] I have tried reproducing the issue on more than one message broker and/or\r\n  result backend.\r\n- [ ] I have tried reproducing the issue on more than one version of the message\r\n  broker and/or result backend.\r\n- [ ] I have tried reproducing the issue on more than one operating system.\r\n- [ ] I have tried reproducing the issue on more than one workers pool.\r\n- [ ] I have tried reproducing the issue with autoscaling, retries,\r\n  ETA/Countdown & rate limits disabled.\r\n- [ ] I have tried reproducing the issue after downgrading\r\n  and/or upgrading Celery and its dependencies.\r\n\r\n## Related Issues and Possible Duplicates\r\n<!--\r\nPlease make sure to search and mention any related issues\r\nor possible duplicates to this issue as requested by the checklist above.\r\n\r\nThis may or may not include issues in other repositories that the Celery project\r\nmaintains or other repositories that are dependencies of Celery.\r\n\r\nIf you don't know how to mention issues, please refer to Github's documentation\r\non the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests\r\n-->\r\n\r\n#### Related Issues\r\n\r\n- None\r\n\r\n#### Possible Duplicates\r\n\r\n- None\r\n\r\n## Environment & Settings\r\n<!-- Include the contents of celery --version below -->\r\n**Celery version**:\r\n<!-- Include the output of celery -A proj report below -->\r\n<details>\r\n<summary><b><code>celery report</code> Output:</b></summary>\r\n<p>\r\n\r\n```\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n# Steps to Reproduce\r\n\r\n## Required Dependencies\r\n<!-- Please fill the required dependencies to reproduce this issue -->\r\n- **Minimal Python Version**: N/A or Unknown\r\n- **Minimal Celery Version**: N/A or Unknown\r\n- **Minimal Kombu Version**: N/A or Unknown\r\n- **Minimal Broker Version**: N/A or Unknown\r\n- **Minimal Result Backend Version**: N/A or Unknown\r\n- **Minimal OS and/or Kernel Version**: N/A or Unknown\r\n- **Minimal Broker Client Version**: N/A or Unknown\r\n- **Minimal Result Backend Client Version**: N/A or Unknown\r\n\r\n### Python Packages\r\n<!-- Please fill the contents of pip freeze below -->\r\n<details>\r\n<summary><b><code>pip freeze</code> Output:</b></summary>\r\n<p>\r\n\r\n```\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n### Other Dependencies\r\n<!--\r\nPlease provide system dependencies, configuration files\r\nand other dependency information if applicable\r\n-->\r\n<details>\r\n<p>\r\nN/A\r\n</p>\r\n</details>\r\n\r\n## Minimally Reproducible Test Case\r\n<!--\r\nPlease provide a reproducible test case.\r\nRefer to the Reporting Bugs section in our contribution guide.\r\n\r\nWe prefer submitting test cases in the form of a PR to our integration test suite.\r\nIf you can provide one, please mention the PR number below.\r\nIf not, please attach the most minimal code example required to reproduce the issue below.\r\nIf the test case is too large, please include a link to a gist or a repository below.\r\n\r\nAlternatively, the pytest-celery plugin can be used to create standalone reproduction scripts\r\nthat can be added to this report. See the pytest-celery documentation for more information at\r\npytest-celery.readthedocs.io\r\n-->\r\n\r\n<details>\r\n<p>\r\n\r\n```python\r\n\r\napp.conf.update(\r\n    security_key='/private/keys/celery/private.key',\r\n    security_certificate='/private/keys/celery/public.pem',\r\n    security_cert_store='/private/keys/celery/*.pem')\r\n\r\napp.setup_security()\r\n\r\n@app.task\r\ndef serializer_test_task(arg: Any) -> Any:\r\n    return arg\r\n\r\ndef test_serialize(data):\r\n    res = serializer_test_task.delay(data)\r\n    deserialized_value = res.get()\r\n    assert deserialized_value == data\r\n\r\ntest_serialize(data=b\"foo\") # fails to validate signature\r\n\r\n#############\r\n\r\napp.setup_security(serializer=\"pickle\")\r\ntest_serialize(data=\"foo\") # fails to serialize any value using pickle serializer\r\n\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n# Expected Behavior\r\n\r\n\r\n# Actual Behavior\r\n\r\nThere are several bugs in the SecureSerializer feature:\r\n - When using the 'json' serializer (default), it will always fail to validate the signature if the passed value is of type 'bytes'\r\n- When using the other binary serializer (and not a string-based serializer like 'json'), for example 'pickle', the serializer is completely broken. \r\n\n", "hints_text": "", "created_at": "2024-04-25T21:29:35Z"}
{"repo": "celery/celery", "pull_number": 8829, "instance_id": "celery__celery-8829", "issue_numbers": ["8809"], "base_commit": "58d2e67a0c0050e793c54928418749d68ee4e3bb", "patch": "diff --git a/docs/getting-started/backends-and-brokers/sqs.rst b/docs/getting-started/backends-and-brokers/sqs.rst\nindex a9f82686910..9017871b984 100644\n--- a/docs/getting-started/backends-and-brokers/sqs.rst\n+++ b/docs/getting-started/backends-and-brokers/sqs.rst\n@@ -15,7 +15,7 @@ the ``celery[sqs]`` :ref:`bundle <bundles>`:\n \n .. code-block:: console\n \n-    $ pip install celery[sqs]\n+    $ pip install \"celery[sqs]\"\n \n .. _broker-sqs-configuration:\n \n", "test_patch": "", "problem_statement": "celery[sqs] no longer available?\n\r\n## SQS Celery Issue\r\nI'm unable to download sqs version for celery via `pip install celery[sqs]` as shown [here](https://docs.celeryq.dev/en/v5.3.6/getting-started/backends-and-brokers/sqs.html) ,it returns:\r\n\r\n```bash\r\n\r\nzsh: no matches found: celery[sqs]\r\n\r\n```\r\n\n", "hints_text": "This is a doc bug.\r\n`pip install 'celery[sqs]'` works.", "created_at": "2024-01-31T11:30:07Z"}
{"repo": "celery/celery", "pull_number": 8826, "instance_id": "celery__celery-8826", "issue_numbers": ["8822"], "base_commit": "eff0b2fb2ae6579542e38fceae63f436bfe25b5e", "patch": "diff --git a/.github/workflows/codeql-analysis.yml b/.github/workflows/codeql-analysis.yml\nindex a1dcabfe893..d0b8564bb86 100644\n--- a/.github/workflows/codeql-analysis.yml\n+++ b/.github/workflows/codeql-analysis.yml\n@@ -17,7 +17,9 @@ on:\n   pull_request:\n     # The branches below must be a subset of the branches above\n     branches: [ main ]\n- \n+  workflow_dispatch:\n+\n+\n \n jobs:\n   analyze:\ndiff --git a/.github/workflows/docker.yml b/.github/workflows/docker.yml\nindex 65dd0914029..380a87c0eff 100644\n--- a/.github/workflows/docker.yml\n+++ b/.github/workflows/docker.yml\n@@ -18,6 +18,7 @@ on:\n           - '**.toml'\n           - '/docker/**'\n           - '.github/workflows/docker.yml'\n+    workflow_dispatch:\n \n \n jobs:\ndiff --git a/.github/workflows/linter.yml b/.github/workflows/linter.yml\nindex 31fa81f88cf..f12f0169627 100644\n--- a/.github/workflows/linter.yml\n+++ b/.github/workflows/linter.yml\n@@ -1,6 +1,6 @@\n name: Linter\n \n-on: [pull_request]\n+on: [pull_request, workflow_dispatch]\n \n jobs:\n   linter:\ndiff --git a/.github/workflows/python-package.yml b/.github/workflows/python-package.yml\nindex 3efa187bc3e..88f83caf71c 100644\n--- a/.github/workflows/python-package.yml\n+++ b/.github/workflows/python-package.yml\n@@ -20,6 +20,8 @@ on:\n         - '**.toml'\n         - '.github/workflows/python-package.yml'\n         - \"tox.ini\"\n+  workflow_dispatch:\n+\n \n permissions:\n   contents: read # to fetch code (actions/checkout)\ndiff --git a/.github/workflows/semgrep.yml b/.github/workflows/semgrep.yml\nindex 1352b65ae16..ddb065dbe48 100644\n--- a/.github/workflows/semgrep.yml\n+++ b/.github/workflows/semgrep.yml\n@@ -9,6 +9,8 @@ on:\n   schedule:\n   # random HH:MM to avoid a load spike on GitHub Actions at 00:00\n   - cron: 44 6 * * *\n+  workflow_dispatch:\n+\n name: Semgrep\n jobs:\n   semgrep:\n", "test_patch": "", "problem_statement": "Add `workflow_dispatch` event to CI for granular workflow control\n<!--\nPlease fill this template entirely and do not erase parts of it.\nWe reserve the right to close without a response\nenhancement requests which are incomplete.\n-->\n# Checklist\n<!--\nTo check an item on the list replace [ ] with [x].\n-->\n\n- [x] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Enhancement%22+-label%3A%22Category%3A+Documentation%22)\n  for similar or identical enhancement to an existing feature.\n- [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22Issue+Type%3A+Enhancement%22+-label%3A%22Category%3A+Documentation%22)\n  for existing proposed enhancements.\n- [x] I have checked the [commit log](https://github.com/celery/celery/commits/main)\n  to find out if the if the same enhancement was already implemented in the\n  main branch.\n- [x] I have included all related issues and possible duplicate issues in this issue\n      (If there are none, check this box anyway).\n\n## Related Issues and Possible Duplicates\n\n#### Related Issues\n\n- None\n\n#### Possible Duplicates\n\n- None\n\n# Brief Summary\n\nThis proposal aims to enhance the CI infrastructure by adding the workflow_dispatch event. This event will enable finer-grained control over workflow execution, allowing developers to trigger individual workflows directly from the CLI. This capability will be particularly beneficial for debugging purposes, as it will facilitate isolating and troubleshooting failing workflows more efficiently.\n\n# Design\n\n## Architectural Considerations\n\n* The workflow_dispatch event will be implemented using GitHub Actions.\n\n* A new workflow will be defined specifically for handling dispatch events.\n\n* The dispatch workflow will accept input parameters specifying the target workflow to be triggered.\n\n* Security measures will be implemented to restrict access to the dispatch workflow and prevent unauthorized executions.\n\n\n## Proposed Behavior\n\n* Developers will be able to trigger individual workflows using the gh workflow dispatch command with the target workflow name as an argument.\n\n* The dispatch workflow will validate the provided workflow name and trigger the corresponding workflow run.\n\n* The triggered workflow will run with the same context and environment as a regular CI run.\n\n## Proposed UI/UX\n\n* No UI changes are directly associated with this proposal.\n\n* The addition of the workflow_dispatch event will primarily impact the CLI experience.\n\n* Documentation will be updated to provide instructions on using the gh workflow dispatch command.\n\n## Diagrams\nN/A\n\n## Alternatives\nNone\n\n", "hints_text": "", "created_at": "2024-01-30T09:19:22Z"}
{"repo": "celery/celery", "pull_number": 8806, "instance_id": "celery__celery-8806", "issue_numbers": ["2907"], "base_commit": "8f389997887232500d4aa1a2b0ae0c7320c4c84a", "patch": "diff --git a/CONTRIBUTORS.txt b/CONTRIBUTORS.txt\nindex e0a8394bc6f..6159effcc3a 100644\n--- a/CONTRIBUTORS.txt\n+++ b/CONTRIBUTORS.txt\n@@ -295,4 +295,5 @@ JoonHwan Kim, 2022/08/01\n Kaustav Banerjee, 2022/11/10\n Austin Snoeyink 2022/12/06\n Jeremy Z. Othieno 2023/07/27\n-Tomer Nosrati, 2022/17/07\n\\ No newline at end of file\n+Tomer Nosrati, 2022/17/07\n+Andy Zickler, 2024/01/18\n\\ No newline at end of file\ndiff --git a/celery/beat.py b/celery/beat.py\nindex 76e44721e14..9656493ecbe 100644\n--- a/celery/beat.py\n+++ b/celery/beat.py\n@@ -568,11 +568,11 @@ def _create_schedule(self):\n         for _ in (1, 2):\n             try:\n                 self._store['entries']\n-            except KeyError:\n+            except (KeyError, UnicodeDecodeError, TypeError):\n                 # new schedule db\n                 try:\n                     self._store['entries'] = {}\n-                except KeyError as exc:\n+                except (KeyError, UnicodeDecodeError, TypeError) as exc:\n                     self._store = self._destroy_open_corrupted_schedule(exc)\n                     continue\n             else:\n", "test_patch": "diff --git a/t/unit/app/test_beat.py b/t/unit/app/test_beat.py\nindex fa163bb931e..a95e8e41409 100644\n--- a/t/unit/app/test_beat.py\n+++ b/t/unit/app/test_beat.py\n@@ -2,7 +2,7 @@\n import sys\n from datetime import datetime, timedelta, timezone\n from pickle import dumps, loads\n-from unittest.mock import Mock, call, patch\n+from unittest.mock import MagicMock, Mock, call, patch\n \n import pytest\n \n@@ -669,6 +669,38 @@ def test_remove_db(self, remove):\n         with pytest.raises(OSError):\n             s._remove_db()\n \n+    def test_create_schedule_corrupted(self):\n+        \"\"\"\n+        Test that any decoding errors that might happen when opening beat-schedule.db are caught\n+        \"\"\"\n+        s = create_persistent_scheduler()[0](app=self.app,\n+                                             schedule_filename='schedule')\n+        s._store = MagicMock()\n+        s._destroy_open_corrupted_schedule = Mock()\n+        s._destroy_open_corrupted_schedule.return_value = MagicMock()\n+\n+        # self._store['entries'] will throw a KeyError\n+        s._store.__getitem__.side_effect = KeyError()\n+        # then, when _create_schedule tries to reset _store['entries'], throw another error\n+        expected_error = UnicodeDecodeError(\"ascii\", b\"ordinal not in range(128)\", 0, 0, \"\")\n+        s._store.__setitem__.side_effect = expected_error\n+\n+        s._create_schedule()\n+        s._destroy_open_corrupted_schedule.assert_called_with(expected_error)\n+\n+    def test_create_schedule_missing_entries(self):\n+        \"\"\"\n+        Test that if _create_schedule can't find the key \"entries\" in _store it will recreate it\n+        \"\"\"\n+        s = create_persistent_scheduler()[0](app=self.app, schedule_filename=\"schedule\")\n+        s._store = MagicMock()\n+\n+        # self._store['entries'] will throw a KeyError\n+        s._store.__getitem__.side_effect = TypeError()\n+\n+        s._create_schedule()\n+        s._store.__setitem__.assert_called_with(\"entries\", {})\n+\n     def test_setup_schedule(self):\n         s = create_persistent_scheduler()[0](app=self.app,\n                                              schedule_filename='schedule')\n", "problem_statement": "Celery beat UnicodeDecodeError (Python 3.4) issue\nI am using Python 3.4 with Celery 3.1.19\n\nRunning without beat it works properly:\n\n```\ncelery worker --app worker --config=celeryconfig --loglevel=info\n```\n\nBut with celery beat:\n\n```\ncelery worker --app worker -B --config=celeryconfig --loglevel=info\n```\n\nI got this exception:\n\n```\nTraceback (most recent call last):\n  File \"/env/lib/python3.4/site-packages/kombu/utils/__init__.py\", line 320, in __get__\n    return obj.__dict__[self.__name__]\nKeyError: 'scheduler'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/shelve.py\", line 111, in __getitem__\n    value = self.cache[key]\nKeyError: 'entries'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/env/lib/python3.4/site-packages/billiard/process.py\", line 292, in _bootstrap\n    self.run()\n  File \"/env/lib/python3.4/site-packages/celery/beat.py\", line 530, in run\n    self.service.start(embedded_process=True)\n  File \"/env/lib/python3.4/site-packages/celery/beat.py\", line 454, in start\n    humanize_seconds(self.scheduler.max_interval))\n  File \"/env/lib/python3.4/site-packages/kombu/utils/__init__.py\", line 322, in __get__\n    value = obj.__dict__[self.__name__] = self.__get(obj)\n  File \"/env/lib/python3.4/site-packages/celery/beat.py\", line 494, in scheduler\n    return self.get_scheduler()\n  File \"/env/lib/python3.4/site-packages/celery/beat.py\", line 489, in get_scheduler\n    lazy=lazy)\n  File \"/env/lib/python3.4/site-packages/celery/utils/imports.py\", line 53, in instantiate\n    return symbol_by_name(name)(*args, **kwargs)\n  File \"/env/lib/python3.4/site-packages/celery/beat.py\", line 358, in __init__\n    Scheduler.__init__(self, *args, **kwargs)\n  File \"/env/lib/python3.4/site-packages/celery/beat.py\", line 185, in __init__\n    self.setup_schedule()\n  File \"/env/lib/python3.4/site-packages/celery/beat.py\", line 377, in setup_schedule\n    self._store['entries']\n  File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/shelve.py\", line 114, in __getitem__\n    value = Unpickler(f).load()\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xdf in position 1: ordinal not in range(128)\n```\n\nAny ideas? Thanks in advance\n\n", "hints_text": "It works for me here, maybe you could try removing the celerybeat-schedule file?\n\nIt worked! thanks! I removed the `celerybeat-schedule.db` file in the current directory.\n\nMe too, thaks @ask ! Just so you know I've migrated from `Python 2.7.12` to `Python 3.5.3` and it started happening.\nWorked for me also, what I did is rename the file to \"celerybeat-schedule.db.backup\"\nin dicrectory Delete (rm) celerybeat-schedule.db and celerybeat-schedule like this \r\n\r\n```\r\ncd project-directory\r\nrm celerybeat-schedule.db celerybeat-schedule\r\n```", "created_at": "2024-01-19T00:56:19Z"}
{"repo": "celery/celery", "pull_number": 8791, "instance_id": "celery__celery-8791", "issue_numbers": ["8748"], "base_commit": "dc49ec2a95da14ae3449491a4aa1e799b1415375", "patch": "diff --git a/celery/backends/base.py b/celery/backends/base.py\nindex 4216c3b343e..f7d62c3dbe4 100644\n--- a/celery/backends/base.py\n+++ b/celery/backends/base.py\n@@ -9,7 +9,7 @@\n import time\n import warnings\n from collections import namedtuple\n-from datetime import datetime, timedelta\n+from datetime import datetime, timedelta, timezone\n from functools import partial\n from weakref import WeakValueDictionary\n \n@@ -460,7 +460,7 @@ def _get_result_meta(self, result,\n                          state, traceback, request, format_date=True,\n                          encode=False):\n         if state in self.READY_STATES:\n-            date_done = datetime.utcnow()\n+            date_done = datetime.now(timezone.utc)\n             if format_date:\n                 date_done = date_done.isoformat()\n         else:\ndiff --git a/celery/backends/database/models.py b/celery/backends/database/models.py\nindex 1c766b51ca4..a5df8f4d341 100644\n--- a/celery/backends/database/models.py\n+++ b/celery/backends/database/models.py\n@@ -1,5 +1,5 @@\n \"\"\"Database models used by the SQLAlchemy result store backend.\"\"\"\n-from datetime import datetime\n+from datetime import datetime, timezone\n \n import sqlalchemy as sa\n from sqlalchemy.types import PickleType\n@@ -22,8 +22,8 @@ class Task(ResultModelBase):\n     task_id = sa.Column(sa.String(155), unique=True)\n     status = sa.Column(sa.String(50), default=states.PENDING)\n     result = sa.Column(PickleType, nullable=True)\n-    date_done = sa.Column(sa.DateTime, default=datetime.utcnow,\n-                          onupdate=datetime.utcnow, nullable=True)\n+    date_done = sa.Column(sa.DateTime, default=datetime.now(timezone.utc),\n+                          onupdate=datetime.now(timezone.utc), nullable=True)\n     traceback = sa.Column(sa.Text, nullable=True)\n \n     def __init__(self, task_id):\n@@ -84,7 +84,7 @@ class TaskSet(ResultModelBase):\n                    autoincrement=True, primary_key=True)\n     taskset_id = sa.Column(sa.String(155), unique=True)\n     result = sa.Column(PickleType, nullable=True)\n-    date_done = sa.Column(sa.DateTime, default=datetime.utcnow,\n+    date_done = sa.Column(sa.DateTime, default=datetime.now(timezone.utc),\n                           nullable=True)\n \n     def __init__(self, taskset_id, result):\ndiff --git a/celery/backends/elasticsearch.py b/celery/backends/elasticsearch.py\nindex cb4ca4da0fd..a97869bef52 100644\n--- a/celery/backends/elasticsearch.py\n+++ b/celery/backends/elasticsearch.py\n@@ -1,5 +1,5 @@\n \"\"\"Elasticsearch result store backend.\"\"\"\n-from datetime import datetime\n+from datetime import datetime, timezone\n \n from kombu.utils.encoding import bytes_to_str\n from kombu.utils.url import _parse_url\n@@ -129,7 +129,7 @@ def _set_with_state(self, key, value, state):\n         body = {\n             'result': value,\n             '@timestamp': '{}Z'.format(\n-                datetime.utcnow().isoformat()[:-3]\n+                datetime.now(timezone.utc).isoformat()[:-9]\n             ),\n         }\n         try:\ndiff --git a/celery/utils/time.py b/celery/utils/time.py\nindex c8fd0959336..d27615cc10e 100644\n--- a/celery/utils/time.py\n+++ b/celery/utils/time.py\n@@ -212,7 +212,7 @@ def remaining(\n             using :func:`delta_resolution` (i.e., rounded to the\n             resolution of `ends_in`).\n         now (Callable): Function returning the current time and date.\n-            Defaults to :func:`datetime.utcnow`.\n+            Defaults to :func:`datetime.now(timezone.utc)`.\n \n     Returns:\n         ~datetime.timedelta: Remaining time.\n", "test_patch": "diff --git a/t/unit/app/test_beat.py b/t/unit/app/test_beat.py\nindex 6b113df426e..fa163bb931e 100644\n--- a/t/unit/app/test_beat.py\n+++ b/t/unit/app/test_beat.py\n@@ -156,7 +156,10 @@ def is_due(self, *args, **kwargs):\n \n class mocked_schedule(schedule):\n \n-    def __init__(self, is_due, next_run_at, nowfun=datetime.utcnow):\n+    def now_func():\n+        return datetime.now(timezone.utc)\n+\n+    def __init__(self, is_due, next_run_at, nowfun=now_func):\n         self._is_due = is_due\n         self._next_run_at = next_run_at\n         self.run_every = timedelta(seconds=1)\n@@ -872,7 +875,7 @@ def test_maybe_make_aware(self):\n     def test_to_local(self):\n         x = schedule(10, app=self.app)\n         x.utc_enabled = True\n-        d = x.to_local(datetime.utcnow())  # datetime.utcnow() is deprecated in Python 3.12\n+        d = x.to_local(datetime.now())\n         assert d.tzinfo is None\n         x.utc_enabled = False\n         d = x.to_local(datetime.now(timezone.utc))\ndiff --git a/t/unit/backends/test_elasticsearch.py b/t/unit/backends/test_elasticsearch.py\nindex a53fe512984..a465cbcf501 100644\n--- a/t/unit/backends/test_elasticsearch.py\n+++ b/t/unit/backends/test_elasticsearch.py\n@@ -1,4 +1,4 @@\n-import datetime\n+from datetime import datetime, timezone\n from unittest.mock import Mock, call, patch, sentinel\n \n import pytest\n@@ -150,8 +150,8 @@ def test_backend_by_url(self, url='elasticsearch://localhost:9200/index'):\n \n     @patch('celery.backends.elasticsearch.datetime')\n     def test_index_conflict(self, datetime_mock):\n-        expected_dt = datetime.datetime(2020, 6, 1, 18, 43, 24, 123456, None)\n-        datetime_mock.utcnow.return_value = expected_dt\n+        expected_dt = datetime(2020, 6, 1, 18, 43, 24, 123456, timezone.utc)\n+        datetime_mock.now.return_value = expected_dt\n \n         x = ElasticsearchBackend(app=self.app)\n         x._server = Mock()\n@@ -178,20 +178,20 @@ def test_index_conflict(self, datetime_mock):\n         x._server.index.assert_called_once_with(\n             id=sentinel.task_id,\n             index=x.index,\n-            body={'result': sentinel.result, '@timestamp': expected_dt.isoformat()[:-3] + 'Z'},\n+            body={'result': sentinel.result, '@timestamp': expected_dt.isoformat()[:-9] + 'Z'},\n             params={'op_type': 'create'},\n         )\n         x._server.update.assert_called_once_with(\n             id=sentinel.task_id,\n             index=x.index,\n-            body={'doc': {'result': sentinel.result, '@timestamp': expected_dt.isoformat()[:-3] + 'Z'}},\n+            body={'doc': {'result': sentinel.result, '@timestamp': expected_dt.isoformat()[:-9] + 'Z'}},\n             params={'if_seq_no': 2, 'if_primary_term': 1}\n         )\n \n     @patch('celery.backends.elasticsearch.datetime')\n     def test_index_conflict_with_doctype(self, datetime_mock):\n-        expected_dt = datetime.datetime(2020, 6, 1, 18, 43, 24, 123456, None)\n-        datetime_mock.utcnow.return_value = expected_dt\n+        expected_dt = datetime(2020, 6, 1, 18, 43, 24, 123456, timezone.utc)\n+        datetime_mock.now.return_value = expected_dt\n \n         x = ElasticsearchBackend(app=self.app)\n         x._server = Mock()\n@@ -219,21 +219,21 @@ def test_index_conflict_with_doctype(self, datetime_mock):\n             id=sentinel.task_id,\n             index=x.index,\n             doc_type=x.doc_type,\n-            body={'result': sentinel.result, '@timestamp': expected_dt.isoformat()[:-3] + 'Z'},\n+            body={'result': sentinel.result, '@timestamp': expected_dt.isoformat()[:-9] + 'Z'},\n             params={'op_type': 'create'},\n         )\n         x._server.update.assert_called_once_with(\n             id=sentinel.task_id,\n             index=x.index,\n             doc_type=x.doc_type,\n-            body={'doc': {'result': sentinel.result, '@timestamp': expected_dt.isoformat()[:-3] + 'Z'}},\n+            body={'doc': {'result': sentinel.result, '@timestamp': expected_dt.isoformat()[:-9] + 'Z'}},\n             params={'if_seq_no': 2, 'if_primary_term': 1}\n         )\n \n     @patch('celery.backends.elasticsearch.datetime')\n     def test_index_conflict_without_state(self, datetime_mock):\n-        expected_dt = datetime.datetime(2020, 6, 1, 18, 43, 24, 123456, None)\n-        datetime_mock.utcnow.return_value = expected_dt\n+        expected_dt = datetime(2020, 6, 1, 18, 43, 24, 123456, timezone.utc)\n+        datetime_mock.now.return_value = expected_dt\n \n         x = ElasticsearchBackend(app=self.app)\n         x._server = Mock()\n@@ -260,13 +260,13 @@ def test_index_conflict_without_state(self, datetime_mock):\n         x._server.index.assert_called_once_with(\n             id=sentinel.task_id,\n             index=x.index,\n-            body={'result': sentinel.result, '@timestamp': expected_dt.isoformat()[:-3] + 'Z'},\n+            body={'result': sentinel.result, '@timestamp': expected_dt.isoformat()[:-9] + 'Z'},\n             params={'op_type': 'create'},\n         )\n         x._server.update.assert_called_once_with(\n             id=sentinel.task_id,\n             index=x.index,\n-            body={'doc': {'result': sentinel.result, '@timestamp': expected_dt.isoformat()[:-3] + 'Z'}},\n+            body={'doc': {'result': sentinel.result, '@timestamp': expected_dt.isoformat()[:-9] + 'Z'}},\n             params={'if_seq_no': 2, 'if_primary_term': 1}\n         )\n \n@@ -277,8 +277,8 @@ def test_index_conflict_with_ready_state_on_backend_without_state(self, datetime\n         so it cannot protect overriding a ready state by any other state.\n         As a result, server.update will be called no matter what.\n         \"\"\"\n-        expected_dt = datetime.datetime(2020, 6, 1, 18, 43, 24, 123456, None)\n-        datetime_mock.utcnow.return_value = expected_dt\n+        expected_dt = datetime(2020, 6, 1, 18, 43, 24, 123456, timezone.utc)\n+        datetime_mock.now.return_value = expected_dt\n \n         x = ElasticsearchBackend(app=self.app)\n         x._server = Mock()\n@@ -305,20 +305,20 @@ def test_index_conflict_with_ready_state_on_backend_without_state(self, datetime\n         x._server.index.assert_called_once_with(\n             id=sentinel.task_id,\n             index=x.index,\n-            body={'result': sentinel.result, '@timestamp': expected_dt.isoformat()[:-3] + 'Z'},\n+            body={'result': sentinel.result, '@timestamp': expected_dt.isoformat()[:-9] + 'Z'},\n             params={'op_type': 'create'},\n         )\n         x._server.update.assert_called_once_with(\n             id=sentinel.task_id,\n             index=x.index,\n-            body={'doc': {'result': sentinel.result, '@timestamp': expected_dt.isoformat()[:-3] + 'Z'}},\n+            body={'doc': {'result': sentinel.result, '@timestamp': expected_dt.isoformat()[:-9] + 'Z'}},\n             params={'if_seq_no': 2, 'if_primary_term': 1}\n         )\n \n     @patch('celery.backends.elasticsearch.datetime')\n     def test_index_conflict_with_existing_success(self, datetime_mock):\n-        expected_dt = datetime.datetime(2020, 6, 1, 18, 43, 24, 123456, None)\n-        datetime_mock.utcnow.return_value = expected_dt\n+        expected_dt = datetime(2020, 6, 1, 18, 43, 24, 123456, timezone.utc)\n+        datetime_mock.now.return_value = expected_dt\n \n         x = ElasticsearchBackend(app=self.app)\n         x._server = Mock()\n@@ -347,15 +347,15 @@ def test_index_conflict_with_existing_success(self, datetime_mock):\n         x._server.index.assert_called_once_with(\n             id=sentinel.task_id,\n             index=x.index,\n-            body={'result': sentinel.result, '@timestamp': expected_dt.isoformat()[:-3] + 'Z'},\n+            body={'result': sentinel.result, '@timestamp': expected_dt.isoformat()[:-9] + 'Z'},\n             params={'op_type': 'create'},\n         )\n         x._server.update.assert_not_called()\n \n     @patch('celery.backends.elasticsearch.datetime')\n     def test_index_conflict_with_existing_ready_state(self, datetime_mock):\n-        expected_dt = datetime.datetime(2020, 6, 1, 18, 43, 24, 123456, None)\n-        datetime_mock.utcnow.return_value = expected_dt\n+        expected_dt = datetime(2020, 6, 1, 18, 43, 24, 123456, timezone.utc)\n+        datetime_mock.now.return_value = expected_dt\n \n         x = ElasticsearchBackend(app=self.app)\n         x._server = Mock()\n@@ -382,7 +382,7 @@ def test_index_conflict_with_existing_ready_state(self, datetime_mock):\n         x._server.index.assert_called_once_with(\n             id=sentinel.task_id,\n             index=x.index,\n-            body={'result': sentinel.result, '@timestamp': expected_dt.isoformat()[:-3] + 'Z'},\n+            body={'result': sentinel.result, '@timestamp': expected_dt.isoformat()[:-9] + 'Z'},\n             params={'op_type': 'create'},\n         )\n         x._server.update.assert_not_called()\n@@ -390,11 +390,11 @@ def test_index_conflict_with_existing_ready_state(self, datetime_mock):\n     @patch('celery.backends.elasticsearch.datetime')\n     @patch('celery.backends.base.datetime')\n     def test_backend_concurrent_update(self, base_datetime_mock, es_datetime_mock):\n-        expected_dt = datetime.datetime(2020, 6, 1, 18, 43, 24, 123456, None)\n-        es_datetime_mock.utcnow.return_value = expected_dt\n+        expected_dt = datetime(2020, 6, 1, 18, 43, 24, 123456, timezone.utc)\n+        es_datetime_mock.now.return_value = expected_dt\n \n-        expected_done_dt = datetime.datetime(2020, 6, 1, 18, 45, 34, 654321, None)\n-        base_datetime_mock.utcnow.return_value = expected_done_dt\n+        expected_done_dt = datetime(2020, 6, 1, 18, 45, 34, 654321, timezone.utc)\n+        base_datetime_mock.now.return_value = expected_done_dt\n \n         self.app.conf.result_backend_always_retry, prev = True, self.app.conf.result_backend_always_retry\n         x_server_get_side_effect = [\n@@ -455,7 +455,7 @@ def test_backend_concurrent_update(self, base_datetime_mock, es_datetime_mock):\n                     index=x.index,\n                     body={\n                         'result': expected_result,\n-                        '@timestamp': expected_dt.isoformat()[:-3] + 'Z'\n+                        '@timestamp': expected_dt.isoformat()[:-9] + 'Z'\n                     },\n                     params={'op_type': 'create'}\n                 ),\n@@ -464,7 +464,7 @@ def test_backend_concurrent_update(self, base_datetime_mock, es_datetime_mock):\n                     index=x.index,\n                     body={\n                         'result': expected_result,\n-                        '@timestamp': expected_dt.isoformat()[:-3] + 'Z'\n+                        '@timestamp': expected_dt.isoformat()[:-9] + 'Z'\n                     },\n                     params={'op_type': 'create'}\n                 ),\n@@ -476,7 +476,7 @@ def test_backend_concurrent_update(self, base_datetime_mock, es_datetime_mock):\n                     body={\n                         'doc': {\n                             'result': expected_result,\n-                            '@timestamp': expected_dt.isoformat()[:-3] + 'Z'\n+                            '@timestamp': expected_dt.isoformat()[:-9] + 'Z'\n                         }\n                     },\n                     params={'if_seq_no': 2, 'if_primary_term': 1}\n@@ -487,7 +487,7 @@ def test_backend_concurrent_update(self, base_datetime_mock, es_datetime_mock):\n                     body={\n                         'doc': {\n                             'result': expected_result,\n-                            '@timestamp': expected_dt.isoformat()[:-3] + 'Z'\n+                            '@timestamp': expected_dt.isoformat()[:-9] + 'Z'\n                         }\n                     },\n                     params={'if_seq_no': 3, 'if_primary_term': 1}\n@@ -501,11 +501,11 @@ def test_backend_concurrent_update(self, base_datetime_mock, es_datetime_mock):\n     @patch('celery.backends.elasticsearch.datetime')\n     @patch('celery.backends.base.datetime')\n     def test_backend_index_conflicting_document_removed(self, base_datetime_mock, es_datetime_mock):\n-        expected_dt = datetime.datetime(2020, 6, 1, 18, 43, 24, 123456, None)\n-        es_datetime_mock.utcnow.return_value = expected_dt\n+        expected_dt = datetime(2020, 6, 1, 18, 43, 24, 123456, timezone.utc)\n+        es_datetime_mock.now.return_value = expected_dt\n \n-        expected_done_dt = datetime.datetime(2020, 6, 1, 18, 45, 34, 654321, None)\n-        base_datetime_mock.utcnow.return_value = expected_done_dt\n+        expected_done_dt = datetime(2020, 6, 1, 18, 45, 34, 654321, timezone.utc)\n+        base_datetime_mock.now.return_value = expected_done_dt\n \n         self.app.conf.result_backend_always_retry, prev = True, self.app.conf.result_backend_always_retry\n         try:\n@@ -550,7 +550,7 @@ def test_backend_index_conflicting_document_removed(self, base_datetime_mock, es\n                     index=x.index,\n                     body={\n                         'result': expected_result,\n-                        '@timestamp': expected_dt.isoformat()[:-3] + 'Z'\n+                        '@timestamp': expected_dt.isoformat()[:-9] + 'Z'\n                     },\n                     params={'op_type': 'create'}\n                 ),\n@@ -559,7 +559,7 @@ def test_backend_index_conflicting_document_removed(self, base_datetime_mock, es\n                     index=x.index,\n                     body={\n                         'result': expected_result,\n-                        '@timestamp': expected_dt.isoformat()[:-3] + 'Z'\n+                        '@timestamp': expected_dt.isoformat()[:-9] + 'Z'\n                     },\n                     params={'op_type': 'create'}\n                 ),\n@@ -572,11 +572,11 @@ def test_backend_index_conflicting_document_removed(self, base_datetime_mock, es\n     @patch('celery.backends.elasticsearch.datetime')\n     @patch('celery.backends.base.datetime')\n     def test_backend_index_conflicting_document_removed_not_throwing(self, base_datetime_mock, es_datetime_mock):\n-        expected_dt = datetime.datetime(2020, 6, 1, 18, 43, 24, 123456, None)\n-        es_datetime_mock.utcnow.return_value = expected_dt\n+        expected_dt = datetime(2020, 6, 1, 18, 43, 24, 123456, timezone.utc)\n+        es_datetime_mock.now.return_value = expected_dt\n \n-        expected_done_dt = datetime.datetime(2020, 6, 1, 18, 45, 34, 654321, None)\n-        base_datetime_mock.utcnow.return_value = expected_done_dt\n+        expected_done_dt = datetime(2020, 6, 1, 18, 45, 34, 654321, timezone.utc)\n+        base_datetime_mock.now.return_value = expected_done_dt\n \n         self.app.conf.result_backend_always_retry, prev = True, self.app.conf.result_backend_always_retry\n         try:\n@@ -618,7 +618,7 @@ def test_backend_index_conflicting_document_removed_not_throwing(self, base_date\n                     index=x.index,\n                     body={\n                         'result': expected_result,\n-                        '@timestamp': expected_dt.isoformat()[:-3] + 'Z'\n+                        '@timestamp': expected_dt.isoformat()[:-9] + 'Z'\n                     },\n                     params={'op_type': 'create'}\n                 ),\n@@ -627,7 +627,7 @@ def test_backend_index_conflicting_document_removed_not_throwing(self, base_date\n                     index=x.index,\n                     body={\n                         'result': expected_result,\n-                        '@timestamp': expected_dt.isoformat()[:-3] + 'Z'\n+                        '@timestamp': expected_dt.isoformat()[:-9] + 'Z'\n                     },\n                     params={'op_type': 'create'}\n                 ),\n@@ -640,11 +640,11 @@ def test_backend_index_conflicting_document_removed_not_throwing(self, base_date\n     @patch('celery.backends.elasticsearch.datetime')\n     @patch('celery.backends.base.datetime')\n     def test_backend_index_corrupted_conflicting_document(self, base_datetime_mock, es_datetime_mock):\n-        expected_dt = datetime.datetime(2020, 6, 1, 18, 43, 24, 123456, None)\n-        es_datetime_mock.utcnow.return_value = expected_dt\n+        expected_dt = datetime(2020, 6, 1, 18, 43, 24, 123456, timezone.utc)\n+        es_datetime_mock.now.return_value = expected_dt\n \n-        expected_done_dt = datetime.datetime(2020, 6, 1, 18, 45, 34, 654321, None)\n-        base_datetime_mock.utcnow.return_value = expected_done_dt\n+        expected_done_dt = datetime(2020, 6, 1, 18, 45, 34, 654321, timezone.utc)\n+        base_datetime_mock.now.return_value = expected_done_dt\n \n         # self.app.conf.result_backend_always_retry, prev = True, self.app.conf.result_backend_always_retry\n         # try:\n@@ -685,7 +685,7 @@ def test_backend_index_corrupted_conflicting_document(self, base_datetime_mock,\n             index=x.index,\n             body={\n                 'result': expected_result,\n-                '@timestamp': expected_dt.isoformat()[:-3] + 'Z'\n+                '@timestamp': expected_dt.isoformat()[:-9] + 'Z'\n             },\n             params={'op_type': 'create'}\n         )\n@@ -695,7 +695,7 @@ def test_backend_index_corrupted_conflicting_document(self, base_datetime_mock,\n             body={\n                 'doc': {\n                     'result': expected_result,\n-                    '@timestamp': expected_dt.isoformat()[:-3] + 'Z'\n+                    '@timestamp': expected_dt.isoformat()[:-9] + 'Z'\n                 }\n             },\n             params={'if_primary_term': 1, 'if_seq_no': 2}\ndiff --git a/t/unit/backends/test_mongodb.py b/t/unit/backends/test_mongodb.py\nindex 6f74b42125f..9ae340ee149 100644\n--- a/t/unit/backends/test_mongodb.py\n+++ b/t/unit/backends/test_mongodb.py\n@@ -563,7 +563,10 @@ def test_cleanup(self, mock_get_database):\n         mock_database.__getitem__ = Mock(name='MD.__getitem__')\n         mock_database.__getitem__.return_value = mock_collection\n \n-        self.backend.app.now = datetime.datetime.utcnow\n+        def now_func():\n+            return datetime.datetime.now(datetime.timezone.utc)\n+\n+        self.backend.app.now = now_func\n         self.backend.cleanup()\n \n         mock_get_database.assert_called_once_with()\ndiff --git a/t/unit/utils/test_time.py b/t/unit/utils/test_time.py\nindex 6b955e096e9..621769252a9 100644\n--- a/t/unit/utils/test_time.py\n+++ b/t/unit/utils/test_time.py\n@@ -212,14 +212,14 @@ def test_tz_when_zoneinfo(self):\n     def test_maybe_make_aware(self):\n         aware = datetime.now(_timezone.utc).replace(tzinfo=timezone.utc)\n         assert maybe_make_aware(aware)\n-        naive = datetime.utcnow()  # datetime.utcnow() is deprecated in Python 3.12\n+        naive = datetime.now()\n         assert maybe_make_aware(naive)\n         assert maybe_make_aware(naive).tzinfo is ZoneInfo(\"UTC\")\n \n         tz = ZoneInfo('US/Eastern')\n         eastern = datetime.now(_timezone.utc).replace(tzinfo=tz)\n         assert maybe_make_aware(eastern).tzinfo is tz\n-        utcnow = datetime.utcnow()  # datetime.utcnow() is deprecated in Python 3.12\n+        utcnow = datetime.now()\n         assert maybe_make_aware(utcnow, 'UTC').tzinfo is ZoneInfo(\"UTC\")\n \n \n", "problem_statement": "datetime.utcnow is deprecated in python3.12\n# Checklist\r\n- [x] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Enhancement%22+-label%3A%22Category%3A+Documentation%22)\r\n  for similar or identical enhancement to an existing feature.\r\n- [ ] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22Issue+Type%3A+Enhancement%22+-label%3A%22Category%3A+Documentation%22)\r\n  for existing proposed enhancements.\r\n- [x] I have checked the [commit log](https://github.com/celery/celery/commits/main)\r\n  to find out if the if the same enhancement was already implemented in the\r\n  main branch.\r\n- [x] I have included all related issues and possible duplicate issues in this issue\r\n      (If there are none, check this box anyway).\r\n\r\n## Related Issues and Possible Duplicates\r\n\r\n#### Related Issues\r\n\r\n- None\r\n\r\n#### Possible Duplicates\r\n\r\n- None\r\n\r\n# Brief Summary\r\nhttps://docs.python.org/3/library/datetime.html#datetime.datetime.utcnow\r\n\r\nAs of python 3.12, datetime.utcnow is deprecated (since it returns a naive datetime rather than a timezone-aware datetime).\r\nThe suggested fix is to use datetime.now with a specific timezone info of UTC instead.\r\n\r\n# Design\r\n\r\n## Architectural Considerations\r\n<!--\r\nIf more components other than Celery are involved,\r\ndescribe them here and the effect it would have on Celery.\r\n-->\r\nNone\r\n\r\n## Proposed Behavior\r\n<!--\r\nPlease describe in detail how this enhancement is going to change the behavior\r\nof an existing feature.\r\nDescribe what happens in case of failures as well if applicable.\r\n-->\r\n\r\n## Proposed UI/UX\r\n<!--\r\nPlease provide your ideas for the API, CLI options,\r\nconfiguration key names etc. that will be adjusted for this enhancement.\r\n-->\r\n\r\n## Diagrams\r\n<!--\r\nPlease include any diagrams that might be relevant\r\nto the implementation of this enhancement such as:\r\n* Class Diagrams\r\n* Sequence Diagrams\r\n* Activity Diagrams\r\nYou can drag and drop images into the text box to attach them to this issue.\r\n-->\r\nN/A\r\n\r\n## Alternatives\r\n<!--\r\nIf you have considered any alternative implementations\r\ndescribe them in detail below.\r\n-->\r\nNone\r\n\n", "hints_text": "* #8385 (tests fail)\r\n* #8726 (merged)\r\n* #8734 (tests fail)\r\n\r\nPlease uncheck the `I have checked the pull request list` above.", "created_at": "2024-01-14T17:38:15Z"}
{"repo": "celery/celery", "pull_number": 8702, "instance_id": "celery__celery-8702", "issue_numbers": ["8678"], "base_commit": "17631f7eda712b688294ecb8fa53e4769fe2b1f9", "patch": "diff --git a/celery/canvas.py b/celery/canvas.py\nindex a4007f0a27f..a32d3eea7e7 100644\n--- a/celery/canvas.py\n+++ b/celery/canvas.py\n@@ -2271,6 +2271,8 @@ def link_error(self, errback):\n             ``False`` (the current default), then the error callback will only be\n             applied to the body.\n         \"\"\"\n+        errback = maybe_signature(errback)\n+\n         if self.app.conf.task_allow_error_cb_on_chord_header:\n             for task in maybe_list(self.tasks) or []:\n                 task.link_error(errback.clone(immutable=True))\n", "test_patch": "diff --git a/t/unit/tasks/test_canvas.py b/t/unit/tasks/test_canvas.py\nindex 2c3f4f12f3e..53dc52e5cbb 100644\n--- a/t/unit/tasks/test_canvas.py\n+++ b/t/unit/tasks/test_canvas.py\n@@ -1688,6 +1688,14 @@ def test_flag_allow_error_cb_on_chord_header_various_header_types(self):\n             errback = c.link_error(sig)\n             assert errback == sig\n \n+    @pytest.mark.usefixtures('depends_on_current_app')\n+    def test_flag_allow_error_cb_on_chord_header_with_dict_callback(self):\n+        self.app.conf.task_allow_error_cb_on_chord_header = True\n+        c = chord(group(signature('th1'), signature('th2')), signature('tbody'))\n+        errback_dict = dict(signature('tcb'))\n+        errback = c.link_error(errback_dict)\n+        assert errback == errback_dict\n+\n     def test_chord__or__group_of_single_task(self):\n         \"\"\" Test chaining a chord to a group of a single task. \"\"\"\n         c = chord([signature('header')], signature('body'))\n", "problem_statement": "task with an errback throws `AttributeError` when replaced with a chord, when `task_allow_error_cb_on_chord_header` is set\n<!--\r\nPlease fill this template entirely and do not erase parts of it.\r\nWe reserve the right to close without a response\r\nbug reports which are incomplete.\r\n-->\r\n# Checklist\r\n<!--\r\nTo check an item on the list replace [ ] with [x].\r\n-->\r\n- [x] I have verified that the issue exists against the `main` branch of Celery.\r\n- [ ] This has already been asked to the [discussions forum](https://github.com/celery/celery/discussions) first.\r\n- [x] I have read the relevant section in the\r\n  [contribution guide](https://docs.celeryq.dev/en/main/contributing.html#other-bugs)\r\n  on reporting bugs.\r\n- [x] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Bug+Report%22+-label%3A%22Category%3A+Documentation%22)\r\n  for similar or identical bug reports.\r\n- [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22PR+Type%3A+Bugfix%22+-label%3A%22Category%3A+Documentation%22)\r\n  for existing proposed fixes.\r\n- [x] I have checked the [commit log](https://github.com/celery/celery/commits/main)\r\n  to find out if the bug was already fixed in the main branch.\r\n- [x] I have included all related issues and possible duplicate issues\r\n  in this issue (If there are none, check this box anyway).\r\n\r\n## Mandatory Debugging Information\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n    (if you are not able to do this, then at least specify the Celery\r\n     version affected).\r\n- [x] I have verified that the issue exists against the `main` branch of Celery.\r\n- [x] I have included the contents of ``pip freeze`` in the issue.\r\n- [x] I have included all the versions of all the external dependencies required\r\n  to reproduce this bug.\r\n\r\n## Optional Debugging Information\r\n<!--\r\nTry some of the below if you think they are relevant.\r\nIt will help us figure out the scope of the bug and how many users it affects.\r\n-->\r\n- [ ] I have tried reproducing the issue on more than one Python version\r\n  and/or implementation.\r\n- [ ] I have tried reproducing the issue on more than one message broker and/or\r\n  result backend.\r\n- [ ] I have tried reproducing the issue on more than one version of the message\r\n  broker and/or result backend.\r\n- [ ] I have tried reproducing the issue on more than one operating system.\r\n- [x] I have tried reproducing the issue on more than one workers pool.\r\n- [ ] I have tried reproducing the issue with autoscaling, retries,\r\n  ETA/Countdown & rate limits disabled.\r\n- [x] I have tried reproducing the issue after downgrading\r\n  and/or upgrading Celery and its dependencies.\r\n\r\n## Related Issues and Possible Duplicates\r\n<!--\r\nPlease make sure to search and mention any related issues\r\nor possible duplicates to this issue as requested by the checklist above.\r\n\r\nThis may or may not include issues in other repositories that the Celery project\r\nmaintains or other repositories that are dependencies of Celery.\r\n\r\nIf you don't know how to mention issues, please refer to Github's documentation\r\non the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests\r\n-->\r\n\r\n#### Related Issues\r\n\r\nhttps://github.com/celery/celery/issues/8456\r\n\r\n#### Possible Duplicates\r\n\r\n- None\r\n\r\n## Environment & Settings\r\n<!-- Include the contents of celery --version below -->\r\n**Celery version**: 5.3.6 (emerald-rush)\r\n<!-- Include the output of celery -A proj report below -->\r\n<details>\r\n<summary><b><code>celery report</code> Output:</b></summary>\r\n<p>\r\n\r\n```\r\nsoftware -> celery:5.3.6 (emerald-rush) kombu:5.3.4 py:3.8.10\r\n            billiard:4.2.0 redis:5.0.1\r\nplatform -> system:Darwin arch:64bit\r\n            kernel version:23.1.0 imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:redis results:redis://0.0.0.0:6479/\r\n\r\nbroker_url: 'redis://0.0.0.0:6479//'\r\nresult_backend: 'redis://0.0.0.0:6479/'\r\ndeprecated_settings: None\r\ntask_allow_error_cb_on_chord_header: True\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n# Steps to Reproduce\r\n\r\n## Required Dependencies\r\n<!-- Please fill the required dependencies to reproduce this issue -->\r\n- **Minimal Python Version**: N/A or Unknown\r\n- **Minimal Celery Version**: 5.3.1\r\n- **Minimal Kombu Version**: N/A or Unknown\r\n- **Minimal Broker Version**: N/A or Unknown\r\n- **Minimal Result Backend Version**: N/A or Unknown\r\n- **Minimal OS and/or Kernel Version**: N/A or Unknown\r\n- **Minimal Broker Client Version**: N/A or Unknown\r\n- **Minimal Result Backend Client Version**: N/A or Unknown\r\n\r\n### Python Packages\r\n<!-- Please fill the contents of pip freeze below -->\r\n<details>\r\n<summary><b><code>pip freeze</code> Output:</b></summary>\r\n<p>\r\n\r\n```\r\naiohttp==3.8.5\r\naiosignal==1.3.1\r\namqp==5.2.0\r\nasgiref==3.6.0\r\nastroid==2.15.2\r\nasync-timeout==4.0.3\r\nattrs==22.2.0\r\nautopep8==1.5.5\r\nbackports.zoneinfo==0.2.1\r\nbandit==1.7.5\r\nbilliard==4.2.0\r\nblack==23.7.0\r\nboto3-stubs==1.19.12.post1\r\nbotocore-stubs==1.29.107\r\ncelery==5.3.6\r\ncelery-stubs==0.1.3\r\ncertifi==2022.12.7\r\ncfgv==3.3.1\r\ncharset-normalizer==3.1.0\r\nclick==8.1.7\r\nclick-didyoumean==0.3.0\r\nclick-plugins==1.1.1\r\nclick-repl==0.3.0\r\ncoreapi==2.3.3\r\ncoreschema==0.0.4\r\ndata-science-types==0.2.23\r\ndill==0.3.6\r\ndistlib==0.3.6\r\nDjango==4.2\r\ndjango-filter-stubs==0.1.3\r\ndjango-stubs==1.15.0\r\ndjango-stubs-ext==0.8.0\r\ndjangorestframework==3.14.0\r\ndjangorestframework-stubs==1.9.1\r\ndrf-yasg==1.20.3\r\nfactory-boy==3.2.1\r\nFaker==18.3.4\r\nfilelock==3.10.7\r\nflake8==3.8.4\r\nfrozenlist==1.4.0\r\nfuzzywuzzy-stubs==0.0.1\r\ngitdb==4.0.10\r\nGitPython==3.1.31\r\ngraphene-stubs==0.15\r\nidentify==2.5.22\r\nidna==3.4\r\ninflection==0.5.1\r\nisort==5.12.0\r\nitypes==1.2.0\r\njedi==0.17.2\r\nJinja2==3.1.2\r\nkombu==5.3.4\r\nlazy-object-proxy==1.9.0\r\nmarkdown-it-py==2.2.0\r\nMarkupSafe==2.1.2\r\nmccabe==0.6.1\r\nmdurl==0.1.2\r\nmultidict==6.0.4\r\nmypy==1.1.1\r\nmypy-extensions==1.0.0\r\nnodeenv==1.7.0\r\npackaging==23.0\r\nparso==0.7.1\r\npathspec==0.11.1\r\npbr==5.11.1\r\npip-licenses==3.5.5\r\nplatformdirs==3.2.0\r\npluggy==1.0.0\r\npre-commit==2.7.1\r\nprompt-toolkit==3.0.41\r\nPTable==0.9.2\r\npycodestyle==2.6.0\r\npydocstyle==6.3.0\r\npyflakes==2.2.0\r\nPygments==2.14.0\r\npylint==2.17.2\r\npython-dateutil==2.8.2\r\npython-jsonrpc-server==0.4.0\r\npython-language-server==0.36.2\r\npytoolconfig==1.2.5\r\npytz==2023.3\r\nPyYAML==6.0\r\nratelimit-stubs==2.2.1\r\nredis==5.0.1\r\nregex==2019.11.1\r\nrequests==2.28.2\r\nrich==13.3.3\r\nrope==1.7.0\r\nruamel.yaml==0.17.21\r\nruamel.yaml.clib==0.2.7\r\nsix==1.16.0\r\nsmmap==5.0.0\r\nsnowballstemmer==2.2.0\r\nsqlparse==0.4.3\r\nstevedore==5.0.0\r\ntoml==0.10.2\r\ntomli==2.0.1\r\ntomlkit==0.11.7\r\ntypes-awscrt==0.16.13.post1\r\ntypes-beautifulsoup4==4.10.20\r\ntypes-docutils==0.19.1.7\r\ntypes-pytz==2023.3.0.0\r\ntypes-PyYAML==5.4.12\r\ntypes-requests==2.25.12\r\ntypes-setuptools==67.3.0.2\r\ntypes-six==0.1.9\r\ntyping_extensions==4.5.0\r\ntzdata==2023.3\r\nujson==5.7.0\r\nuritemplate==4.1.1\r\nurllib3==1.26.15\r\nvine==5.1.0\r\nvirtualenv==20.21.0\r\nwcwidth==0.2.12\r\nwrapt==1.15.0\r\nyapf==0.32.0\r\nyarl==1.8.2\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n### Other Dependencies\r\n<!--\r\nPlease provide system dependencies, configuration files\r\nand other dependency information if applicable\r\n-->\r\n<details>\r\n<p>\r\nN/A\r\n</p>\r\n</details>\r\n\r\n## Minimally Reproducible Test Case\r\n<!--\r\nPlease provide a reproducible test case.\r\nRefer to the Reporting Bugs section in our contribution guide.\r\n\r\nWe prefer submitting test cases in the form of a PR to our integration test suite.\r\nIf you can provide one, please mention the PR number below.\r\nIf not, please attach the most minimal code example required to reproduce the issue below.\r\nIf the test case is too large, please include a link to a gist or a repository below.\r\n-->\r\n\r\n<details>\r\n<p>\r\n\r\n```python\r\nfrom celery import Celery, chain, chord, signature, group\r\n\r\napp = Celery(\r\n    \"celery_bug\", backend=\"redis://0.0.0.0:6479/\", broker=\"redis://0.0.0.0:6479/\"\r\n)\r\n\r\napp.conf.task_allow_error_cb_on_chord_header = True\r\n\r\n\r\n@app.task(bind=True)\r\ndef orig_task(self, arg):\r\n    chord_headers = group([chord_header.s(arg=f\"arg{i}\") for i in range(5)])\r\n    replacement_chord = chord(chord_headers, chord_body.s())\r\n    return self.replace(replacement_chord)\r\n\r\n\r\n@app.task\r\ndef chord_header(arg):\r\n    return f\"header: {arg}\"\r\n\r\n\r\n@app.task\r\ndef chord_body(arg):\r\n    return f\"body: {arg}]\"\r\n\r\n\r\n@app.task\r\ndef handle_error(*args, **kwargs):\r\n    print(f\"handle error called with args {args} kwargs {kwargs}\")\r\n\r\n\r\ndef main():\r\n    print(f\"hello world\")\r\n    res = orig_task.apply_async(args=[\"spam\"], link_error=handle_error.s())\r\n    print(f\"RESULT: {res.get()}\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n# Expected Behavior\r\n<!-- Describe in detail what you expect to happen -->\r\nI would expect the `orig_task` to be replaced with `replacement_chord` and give expected output.\r\n\r\nThis is the expected output that I do see if `task_allow_error_cb_on_chord_header` is `False`, or if the `orig_task` is called without the `link_error=` callback:  \r\n\r\n```\r\n$ poetry run python celery_bug.py \r\nhello world\r\nRESULT: body: ['header: arg0', 'header: arg1', 'header: arg2', 'header: arg3', 'header: arg4']]\r\n```\r\n# Actual Behavior\r\n<!--\r\nDescribe in detail what actually happened.\r\nPlease include a backtrace and surround it with triple backticks (```).\r\nIn addition, include the Celery daemon logs, the broker logs,\r\nthe result backend logs and system logs below if they will help us debug\r\nthe issue.\r\n-->\r\nInstead, I get this `AttributeError`:\r\n```\r\n$ poetry run python celery_bug.py \r\nhello world\r\nTraceback (most recent call last):\r\n  File \"celery_bug.py\", line 39, in <module>\r\n    main()\r\n  File \"celery_bug.py\", line 35, in main\r\n    print(f\"RESULT: {res.get()}\")\r\n  File \"/Users/robertgalloway/Library/Caches/pypoetry/virtualenvs/rpg-play-aJQQ1jqR-py3.8/lib/python3.8/site-packages/celery/result.py\", line 251, in get\r\n    return self.backend.wait_for_pending(\r\n  File \"/Users/robertgalloway/Library/Caches/pypoetry/virtualenvs/rpg-play-aJQQ1jqR-py3.8/lib/python3.8/site-packages/celery/backends/asynchronous.py\", line 223, in wait_for_pending\r\n    return result.maybe_throw(callback=callback, propagate=propagate)\r\n  File \"/Users/robertgalloway/Library/Caches/pypoetry/virtualenvs/rpg-play-aJQQ1jqR-py3.8/lib/python3.8/site-packages/celery/result.py\", line 365, in maybe_throw\r\n    self.throw(value, self._to_remote_traceback(tb))\r\n  File \"/Users/robertgalloway/Library/Caches/pypoetry/virtualenvs/rpg-play-aJQQ1jqR-py3.8/lib/python3.8/site-packages/celery/result.py\", line 358, in throw\r\n    self.on_ready.throw(*args, **kwargs)\r\n  File \"/Users/robertgalloway/Library/Caches/pypoetry/virtualenvs/rpg-play-aJQQ1jqR-py3.8/lib/python3.8/site-packages/vine/promises.py\", line 235, in throw\r\n    reraise(type(exc), exc, tb)\r\n  File \"/Users/robertgalloway/Library/Caches/pypoetry/virtualenvs/rpg-play-aJQQ1jqR-py3.8/lib/python3.8/site-packages/vine/utils.py\", line 27, in reraise\r\n    raise value\r\nAttributeError: 'dict' object has no attribute 'clone'\r\n```\r\n\r\nI suspect the error is related to this code in the `_chord.link_error()` method:\r\n```python\r\n        if self.app.conf.task_allow_error_cb_on_chord_header:\r\n            for task in maybe_list(self.tasks) or []:\r\n                task.link_error(errback.clone(immutable=True))\r\n```\r\n\n", "hints_text": "previous related fix  https://github.com/celery/celery/pull/8463 ", "created_at": "2023-12-07T12:10:06Z"}
{"repo": "celery/celery", "pull_number": 8663, "instance_id": "celery__celery-8663", "issue_numbers": ["8662"], "base_commit": "ac16f239985cf9248155b95788c4b6227f7f1b94", "patch": "diff --git a/celery/canvas.py b/celery/canvas.py\nindex 909962c1639..70c7b139212 100644\n--- a/celery/canvas.py\n+++ b/celery/canvas.py\n@@ -958,6 +958,8 @@ def __or__(self, other):\n         if isinstance(other, group):\n             # unroll group with one member\n             other = maybe_unroll_group(other)\n+            if not isinstance(other, group):\n+                return self.__or__(other)\n             # chain | group() -> chain\n             tasks = self.unchain_tasks()\n             if not tasks:\n@@ -981,6 +983,13 @@ def __or__(self, other):\n                 sig = self.clone()\n                 sig.tasks[-1] = chord(\n                     sig.tasks[-1], other, app=self._app)\n+                # In the scenario where the second-to-last item in a chain is a chord,\n+                # it leads to a situation where two consecutive chords are formed.\n+                # In such cases, a further upgrade can be considered.\n+                # This would involve chaining the body of the second-to-last chord with the last chord.\"\n+                if len(sig.tasks) > 1 and isinstance(sig.tasks[-2], chord):\n+                    sig.tasks[-2].body = sig.tasks[-2].body | sig.tasks[-1]\n+                    sig.tasks = sig.tasks[:-1]\n                 return sig\n             elif self.tasks and isinstance(self.tasks[-1], chord):\n                 # CHAIN [last item is chord] -> chain with chord body.\n", "test_patch": "diff --git a/t/integration/test_canvas.py b/t/integration/test_canvas.py\nindex 7c78a98148b..45cd24f6949 100644\n--- a/t/integration/test_canvas.py\n+++ b/t/integration/test_canvas.py\n@@ -1037,6 +1037,65 @@ def test_freezing_chain_sets_id_of_last_task(self, manager):\n         c.freeze(last_task.id)\n         assert c.id == last_task.id\n \n+    @pytest.mark.parametrize(\n+        \"group_last_task\",\n+        [False, True],\n+    )\n+    def test_chaining_upgraded_chords_mixed_canvas_protocol_2(\n+            self, manager, subtests, group_last_task):\n+        \"\"\" This test is built to reproduce the github issue https://github.com/celery/celery/issues/8662\n+\n+        The issue describes a canvas where a chain of groups are executed multiple times instead of once.\n+        This test is built to reproduce the issue and to verify that the issue is fixed.\n+        \"\"\"\n+        try:\n+            manager.app.backend.ensure_chords_allowed()\n+        except NotImplementedError as e:\n+            raise pytest.skip(e.args[0])\n+\n+        if not manager.app.conf.result_backend.startswith('redis'):\n+            raise pytest.skip('Requires redis result backend.')\n+\n+        redis_connection = get_redis_connection()\n+        redis_key = 'echo_chamber'\n+\n+        c = chain(\n+            group([\n+                redis_echo.si('1', redis_key=redis_key),\n+                redis_echo.si('2', redis_key=redis_key)\n+            ]),\n+            group([\n+                redis_echo.si('3', redis_key=redis_key),\n+                redis_echo.si('4', redis_key=redis_key),\n+                redis_echo.si('5', redis_key=redis_key)\n+            ]),\n+            group([\n+                redis_echo.si('6', redis_key=redis_key),\n+                redis_echo.si('7', redis_key=redis_key),\n+                redis_echo.si('8', redis_key=redis_key),\n+                redis_echo.si('9', redis_key=redis_key)\n+            ]),\n+            redis_echo.si('Done', redis_key='Done') if not group_last_task else\n+            group(redis_echo.si('Done', redis_key='Done')),\n+        )\n+\n+        with subtests.test(msg='Run the chain and wait for completion'):\n+            redis_connection.delete(redis_key, 'Done')\n+            c.delay().get(timeout=TIMEOUT)\n+            await_redis_list_message_length(1, redis_key='Done', timeout=10)\n+\n+        with subtests.test(msg='All tasks are executed once'):\n+            actual = [\n+                sig.decode('utf-8')\n+                for sig in redis_connection.lrange(redis_key, 0, -1)\n+            ]\n+            expected = [str(i) for i in range(1, 10)]\n+            with subtests.test(msg='All tasks are executed once'):\n+                assert sorted(actual) == sorted(expected)\n+\n+        # Cleanup\n+        redis_connection.delete(redis_key, 'Done')\n+\n \n class test_result_set:\n \ndiff --git a/t/unit/tasks/test_canvas.py b/t/unit/tasks/test_canvas.py\nindex 5bed3d8ec51..b4d03a56e3c 100644\n--- a/t/unit/tasks/test_canvas.py\n+++ b/t/unit/tasks/test_canvas.py\n@@ -571,6 +571,36 @@ def test_chain_of_chord_upgrade_on_chaining(self):\n         assert isinstance(new_chain, _chain)\n         assert isinstance(new_chain.tasks[0].body, chord)\n \n+    @pytest.mark.parametrize(\n+        \"group_last_task\",\n+        [False, True],\n+    )\n+    def test_chain_of_chord_upgrade_on_chaining__protocol_2(\n+            self, group_last_task):\n+        c = chain(\n+            group([self.add.s(i, i) for i in range(5)], app=self.app),\n+            group([self.add.s(i, i) for i in range(10, 15)], app=self.app),\n+            group([self.add.s(i, i) for i in range(20, 25)], app=self.app),\n+            self.add.s(30) if not group_last_task else group(self.add.s(30),\n+                                                             app=self.app))\n+        assert isinstance(c, _chain)\n+        assert len(\n+            c.tasks\n+        ) == 1, \"Consecutive chords should be further upgraded to a single chord.\"\n+        assert isinstance(c.tasks[0], chord)\n+\n+    def test_chain_of_chord_upgrade_on_chaining__protocol_3(self):\n+        c = chain(\n+            chain([self.add.s(i, i) for i in range(5)]),\n+            group([self.add.s(i, i) for i in range(10, 15)], app=self.app),\n+            chord([signature('header')], signature('body'), app=self.app),\n+            group([self.add.s(i, i) for i in range(20, 25)], app=self.app))\n+        assert isinstance(c, _chain)\n+        assert isinstance(\n+            c.tasks[-1], chord\n+        ), \"Chord followed by a group should be upgraded to a single chord with chained body.\"\n+        assert len(c.tasks) == 6\n+\n     def test_apply_options(self):\n \n         class static(Signature):\n", "problem_statement": "Duplicate executions and django-celery-results backend exceptions when chaining 4x number of groups\n<!--\r\nPlease fill this template entirely and do not erase parts of it.\r\nWe reserve the right to close without a response\r\nbug reports which are incomplete.\r\n-->\r\n# Checklist\r\n<!--\r\nTo check an item on the list replace [ ] with [x].\r\n-->\r\n- [x] I have verified that the issue exists against the `main` branch of Celery.\r\n- [ ] This has already been asked to the [discussions forum](https://github.com/celery/celery/discussions) first.\r\n- [x] I have read the relevant section in the\r\n  [contribution guide](https://docs.celeryq.dev/en/main/contributing.html#other-bugs)\r\n  on reporting bugs.\r\n- [x] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Bug+Report%22+-label%3A%22Category%3A+Documentation%22)\r\n  for similar or identical bug reports.\r\n- [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22PR+Type%3A+Bugfix%22+-label%3A%22Category%3A+Documentation%22)\r\n  for existing proposed fixes.\r\n- [x] I have checked the [commit log](https://github.com/celery/celery/commits/main)\r\n  to find out if the bug was already fixed in the main branch.\r\n- [x] I have included all related issues and possible duplicate issues\r\n  in this issue (If there are none, check this box anyway).\r\n\r\n## Mandatory Debugging Information\r\n\r\n- [ ] I have included the output of ``celery -A proj report`` in the issue.\r\n    (if you are not able to do this, then at least specify the Celery\r\n     version affected).\r\n- [x] I have verified that the issue exists against the `main` branch of Celery.\r\n- [x] I have included the contents of ``pip freeze`` in the issue.\r\n- [x] I have included all the versions of all the external dependencies required\r\n  to reproduce this bug.\r\n\r\n## Optional Debugging Information\r\n<!--\r\nTry some of the below if you think they are relevant.\r\nIt will help us figure out the scope of the bug and how many users it affects.\r\n-->\r\n- [ ] I have tried reproducing the issue on more than one Python version\r\n  and/or implementation.\r\n- [ ] I have tried reproducing the issue on more than one message broker and/or\r\n  result backend.\r\n- [ ] I have tried reproducing the issue on more than one version of the message\r\n  broker and/or result backend.\r\n- [ ] I have tried reproducing the issue on more than one operating system.\r\n- [x] I have tried reproducing the issue on more than one workers pool.\r\n- [ ] I have tried reproducing the issue with autoscaling, retries,\r\n  ETA/Countdown & rate limits disabled.\r\n- [x] I have tried reproducing the issue after downgrading\r\n  and/or upgrading Celery and its dependencies.\r\n\r\n## Related Issues and Possible Duplicates\r\n<!--\r\nPlease make sure to search and mention any related issues\r\nor possible duplicates to this issue as requested by the checklist above.\r\n\r\nThis may or may not include issues in other repositories that the Celery project\r\nmaintains or other repositories that are dependencies of Celery.\r\n\r\nIf you don't know how to mention issues, please refer to Github's documentation\r\non the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests\r\n-->\r\n\r\n#### Related Issues\r\n\r\n- #5958 \r\n\r\n#### Possible Duplicates\r\n\r\n- None\r\n\r\n## Environment & Settings\r\n<!-- Include the contents of celery --version below -->\r\n**Celery version**: 5.3.0 - 5.3.5\r\n\r\n# Steps to Reproduce\r\n\r\n## Required Dependencies\r\n<!-- Please fill the required dependencies to reproduce this issue -->\r\n- **Minimal Python Version**: 3.11.4\r\n- **Minimal Celery Version**: 5.3.5\r\n- **Minimal Kombu Version**: 5.3.4\r\n- **Minimal Broker Version**: N/A or Unknown\r\n- **Minimal Result Backend Version**: django-celery-results 2.5.1\r\n- **Minimal OS and/or Kernel Version**: Debian 11\r\n- **Minimal Broker Client Version**: N/A or Unknown\r\n- **Minimal Result Backend Client Version**: N/A or Unknown\r\n\r\n### Python Packages\r\n<!-- Please fill the contents of pip freeze below -->\r\nDjango==4.2.7\r\n\r\n### Other Dependencies\r\n<!--\r\nPlease provide system dependencies, configuration files\r\nand other dependency information if applicable\r\n-->\r\n<details>\r\n<p>\r\n</p>\r\n</details>\r\n\r\n## Minimally Reproducible Test Case\r\nWhile working with a chain of groups in Celery with django-celery-results as result backend, I encountered an unexpected behavior leading to exceptions in the django-celery-results backend. The structure of my chain is as follows:\r\n\r\n```\r\ngroup1(task1, task2, task3, task4, task5, task6) |\r\ngroup2(task7, task8, task9, task10, task11, task12, task13) |\r\ngroup3(task14, task15, task16, task17) |\r\ngroup4(task18)\r\n```\r\n\r\n# Expected Behavior\r\nChain executed without exception.\r\n\r\n# Actual Behavior\r\nDuring execution, `apply_chord` in the django-celery-results database backend is called multiple times with the same group_id, triggering exceptions.\r\n\r\n# Attempts to Resolve\r\n\r\nI tried modifying django-celery-results (https://github.com/celery/django-celery-results/pull/413) to prevent updating the chord counter when the group_id is the same. This change, however, resulted in task18 not being triggered.\r\nFurther modifications were made to accumulate the count of the chord counter when the group_id is the same. This led to tasks in group3 being executed 7 times (equal to the number of tasks in group2).\r\nThis issue seems to bear resemblance to https://github.com/celery/celery/issues/5958.\r\n\r\n# Suggested Solution\r\n\r\nUnder the current behavior, these four groups are upgraded into two chords. A potential solution could be to connect the second chord to the end of the first chord's body in `prepare_steps`. This approach aligns with the following existing behaviors:\r\n\r\n- PR #7919 connects a group following a chord to the end of the chord body.\r\n- The `or` operator in class `_chord` (https://github.com/celery/celery/blob/main/celery/canvas.py#L2045) connects a task or chord following a chord to the end of the chord body.\r\n\r\nLooking forward to guidance or suggestions for resolving this issue.\r\n\n", "hints_text": "", "created_at": "2023-11-23T04:45:42Z"}
