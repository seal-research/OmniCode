{"repo": "ytdl-org/youtube-dl", "pull_number": 32987, "instance_id": "ytdl-org__youtube-dl-32987", "issue_numbers": ["32986"], "base_commit": "c5098961b04ce83f4615f2a846c84f803b072639", "patch": "diff --git a/youtube_dl/extractor/common.py b/youtube_dl/extractor/common.py\nindex 9b0016d07ec..78704b55718 100644\n--- a/youtube_dl/extractor/common.py\n+++ b/youtube_dl/extractor/common.py\n@@ -3170,7 +3170,7 @@ def _parse_jwplayer_formats(self, jwplayer_sources_data, video_id=None,\n                     # See com/longtailvideo/jwplayer/media/RTMPMediaProvider.as\n                     # of jwplayer.flash.swf\n                     rtmp_url_parts = re.split(\n-                        r'((?:mp4|mp3|flv):)', source_url, 1)\n+                        r'((?:mp4|mp3|flv):)', source_url, maxsplit=1)\n                     if len(rtmp_url_parts) == 3:\n                         rtmp_url, prefix, play_path = rtmp_url_parts\n                         a_format.update({\ndiff --git a/youtube_dl/extractor/youtube.py b/youtube_dl/extractor/youtube.py\nindex 6fe520e9a44..1f83acf7cbf 100644\n--- a/youtube_dl/extractor/youtube.py\n+++ b/youtube_dl/extractor/youtube.py\n@@ -3,11 +3,13 @@\n from __future__ import unicode_literals\n \n import collections\n+import hashlib\n import itertools\n import json\n import os.path\n import random\n import re\n+import time\n import traceback\n \n from .common import InfoExtractor, SearchInfoExtractor\n@@ -290,6 +292,33 @@ def _real_initialize(self):\n     _YT_INITIAL_PLAYER_RESPONSE_RE = r'ytInitialPlayerResponse\\s*=\\s*({.+?})\\s*;'\n     _YT_INITIAL_BOUNDARY_RE = r'(?:var\\s+meta|</script|\\n)'\n \n+    _SAPISID = None\n+\n+    def _generate_sapisidhash_header(self, origin='https://www.youtube.com'):\n+        time_now = round(time.time())\n+        if self._SAPISID is None:\n+            yt_cookies = self._get_cookies('https://www.youtube.com')\n+            # Sometimes SAPISID cookie isn't present but __Secure-3PAPISID is.\n+            # See: https://github.com/yt-dlp/yt-dlp/issues/393\n+            sapisid_cookie = dict_get(\n+                yt_cookies, ('__Secure-3PAPISID', 'SAPISID'))\n+            if sapisid_cookie and sapisid_cookie.value:\n+                self._SAPISID = sapisid_cookie.value\n+                self.write_debug('Extracted SAPISID cookie')\n+                # SAPISID cookie is required if not already present\n+                if not yt_cookies.get('SAPISID'):\n+                    self.write_debug('Copying __Secure-3PAPISID cookie to SAPISID cookie')\n+                    self._set_cookie(\n+                        '.youtube.com', 'SAPISID', self._SAPISID, secure=True, expire_time=time_now + 3600)\n+            else:\n+                self._SAPISID = False\n+        if not self._SAPISID:\n+            return None\n+        # SAPISIDHASH algorithm from https://stackoverflow.com/a/32065323\n+        sapisidhash = hashlib.sha1(\n+            '{0} {1} {2}'.format(time_now, self._SAPISID, origin).encode('utf-8')).hexdigest()\n+        return 'SAPISIDHASH {0}_{1}'.format(time_now, sapisidhash)\n+\n     def _call_api(self, ep, query, video_id, fatal=True, headers=None):\n         data = self._DEFAULT_API_DATA.copy()\n         data.update(query)\n@@ -1579,20 +1608,27 @@ def _genslice(start, end, step):\n         self.to_screen('Extracted signature function:\\n' + code)\n \n     def _parse_sig_js(self, jscode):\n+        # Examples where `sig` is funcname:\n+        # sig=function(a){a=a.split(\"\"); ... ;return a.join(\"\")};\n+        # ;c&&(c=sig(decodeURIComponent(c)),a.set(b,encodeURIComponent(c)));return a};\n+        # {var l=f,m=h.sp,n=sig(decodeURIComponent(h.s));l.set(m,encodeURIComponent(n))}\n+        # sig=function(J){J=J.split(\"\"); ... ;return J.join(\"\")};\n+        # ;N&&(N=sig(decodeURIComponent(N)),J.set(R,encodeURIComponent(N)));return J};\n+        # {var H=u,k=f.sp,v=sig(decodeURIComponent(f.s));H.set(k,encodeURIComponent(v))}\n         funcname = self._search_regex(\n-            (r'\\b[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*encodeURIComponent\\s*\\(\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'\\b[a-zA-Z0-9]+\\s*&&\\s*[a-zA-Z0-9]+\\.set\\([^,]+\\s*,\\s*encodeURIComponent\\s*\\(\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'\\bm=(?P<sig>[a-zA-Z0-9$]{2,})\\(decodeURIComponent\\(h\\.s\\)\\)',\n-             r'\\bc&&\\(c=(?P<sig>[a-zA-Z0-9$]{2,})\\(decodeURIComponent\\(c\\)\\)',\n-             r'(?:\\b|[^a-zA-Z0-9$])(?P<sig>[a-zA-Z0-9$]{2,})\\s*=\\s*function\\(\\s*a\\s*\\)\\s*{\\s*a\\s*=\\s*a\\.split\\(\\s*\"\"\\s*\\)(?:;[a-zA-Z0-9$]{2}\\.[a-zA-Z0-9$]{2}\\(a,\\d+\\))?',\n-             r'(?P<sig>[a-zA-Z0-9$]+)\\s*=\\s*function\\(\\s*a\\s*\\)\\s*{\\s*a\\s*=\\s*a\\.split\\(\\s*\"\"\\s*\\)',\n+            (r'\\b(?P<var>[\\w$]+)&&\\((?P=var)=(?P<sig>[\\w$]{2,})\\(decodeURIComponent\\((?P=var)\\)\\)',\n+             r'(?P<sig>[\\w$]+)\\s*=\\s*function\\(\\s*(?P<arg>[\\w$]+)\\s*\\)\\s*{\\s*(?P=arg)\\s*=\\s*(?P=arg)\\.split\\(\\s*\"\"\\s*\\)\\s*;\\s*[^}]+;\\s*return\\s+(?P=arg)\\.join\\(\\s*\"\"\\s*\\)',\n+             r'(?:\\b|[^\\w$])(?P<sig>[\\w$]{2,})\\s*=\\s*function\\(\\s*a\\s*\\)\\s*{\\s*a\\s*=\\s*a\\.split\\(\\s*\"\"\\s*\\)(?:;[\\w$]{2}\\.[\\w$]{2}\\(a,\\d+\\))?',\n+             # Old patterns\n+             r'\\b[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*encodeURIComponent\\s*\\(\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\b[\\w]+\\s*&&\\s*[\\w]+\\.set\\([^,]+\\s*,\\s*encodeURIComponent\\s*\\(\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\bm=(?P<sig>[\\w$]{2,})\\(decodeURIComponent\\(h\\.s\\)\\)',\n              # Obsolete patterns\n-             r'(\"|\\')signature\\1\\s*,\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'\\.sig\\|\\|(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'yt\\.akamaized\\.net/\\)\\s*\\|\\|\\s*.*?\\s*[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?:encodeURIComponent\\s*\\()?\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'\\b[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'\\b[a-zA-Z0-9]+\\s*&&\\s*[a-zA-Z0-9]+\\.set\\([^,]+\\s*,\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'\\bc\\s*&&\\s*[a-zA-Z0-9]+\\.set\\([^,]+\\s*,\\s*\\([^)]*\\)\\s*\\(\\s*(?P<sig>[a-zA-Z0-9$]+)\\('),\n+             r'(\"|\\')signature\\1\\s*,\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\.sig\\|\\|(?P<sig>[\\w$]+)\\(',\n+             r'yt\\.akamaized\\.net/\\)\\s*\\|\\|\\s*.*?\\s*[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?:encodeURIComponent\\s*\\()?\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\b[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?P<sig>[\\w$]+)\\(',\n+             r'\\bc\\s*&&\\s*[\\w]+\\.set\\([^,]+\\s*,\\s*\\([^)]*\\)\\s*\\(\\s*(?P<sig>[\\w$]+)\\('),\n             jscode, 'Initial JS player signature function name', group='sig')\n \n         jsi = JSInterpreter(jscode)\n@@ -1658,36 +1694,29 @@ def _decrypt_nsig(self, n, video_id, player_url):\n \n     def _extract_n_function_name(self, jscode):\n         func_name, idx = self._search_regex(\n-            # new: (b=String.fromCharCode(110),c=a.get(b))&&c=nfunc[idx](c)\n-            # or:  (b=\"nn\"[+a.D],c=a.get(b))&&(c=nfunc[idx](c)\n-            # or:  (PL(a),b=a.j.n||null)&&(b=nfunc[idx](b)\n+            # (y=NuD(),Mw(k),q=k.Z[y]||null)&&(q=narray[idx](q),k.set(y,q),k.V||NuD(''))}};\n+            # (R=\"nn\"[+J.Z],mW(J),N=J.K[R]||null)&&(N=narray[idx](N),J.set(R,N))}};\n+            # or:  (b=String.fromCharCode(110),c=a.get(b))&&c=narray[idx](c)\n+            # or:  (b=\"nn\"[+a.D],c=a.get(b))&&(c=narray[idx](c)\n+            # or:  (PL(a),b=a.j.n||null)&&(b=narray[idx](b)\n             # or:  (b=\"nn\"[+a.D],vL(a),c=a.j[b]||null)&&(c=narray[idx](c),a.set(b,c),narray.length||nfunc(\"\")\n-            # old: (b=a.get(\"n\"))&&(b=nfunc[idx](b)(?P<c>[a-z])\\s*=\\s*[a-z]\\s*\n+            # old: (b=a.get(\"n\"))&&(b=narray[idx](b)(?P<c>[a-z])\\s*=\\s*[a-z]\\s*\n             # older: (b=a.get(\"n\"))&&(b=nfunc(b)\n             r'''(?x)\n-                \\((?:[\\w$()\\s]+,)*?\\s*      # (\n-                (?P<b>[a-z])\\s*=\\s*         # b=\n-                (?:\n-                    (?:                     # expect ,c=a.get(b) (etc)\n-                        String\\s*\\.\\s*fromCharCode\\s*\\(\\s*110\\s*\\)|\n-                        \"n+\"\\[\\s*\\+?s*[\\w$.]+\\s*]\n-                    )\\s*(?:,[\\w$()\\s]+(?=,))*|\n-                       (?P<old>[\\w$]+)      # a (old[er])\n-                   )\\s*\n-                   (?(old)\n-                                            # b.get(\"n\")\n-                       (?:\\.\\s*[\\w$]+\\s*|\\[\\s*[\\w$]+\\s*]\\s*)*?\n-                       (?:\\.\\s*n|\\[\\s*\"n\"\\s*]|\\.\\s*get\\s*\\(\\s*\"n\"\\s*\\))\n-                       |                    # ,c=a.get(b)\n-                       ,\\s*(?P<c>[a-z])\\s*=\\s*[a-z]\\s*\n-                       (?:\\.\\s*[\\w$]+\\s*|\\[\\s*[\\w$]+\\s*]\\s*)*?\n-                       (?:\\[\\s*(?P=b)\\s*]|\\.\\s*get\\s*\\(\\s*(?P=b)\\s*\\))\n-                   )\n-                                            # interstitial junk\n-                   \\s*(?:\\|\\|\\s*null\\s*)?(?:\\)\\s*)?&&\\s*(?:\\(\\s*)?\n-               (?(c)(?P=c)|(?P=b))\\s*=\\s*   # [c|b]=\n-                                            # nfunc|nfunc[idx]\n-                   (?P<nfunc>[a-zA-Z_$][\\w$]*)(?:\\s*\\[(?P<idx>\\d+)\\])?\\s*\\(\\s*[\\w$]+\\s*\\)\n+                # (expr, ...,\n+                \\((?:(?:\\s*[\\w$]+\\s*=)?(?:[\\w$\"+\\.\\s(\\[]+(?:[)\\]]\\s*)?),)*\n+                  # b=...\n+                  (?P<b>[\\w$]+)\\s*=\\s*(?!(?P=b)[^\\w$])[\\w$]+\\s*(?:(?:\n+                    \\.\\s*[\\w$]+ |\n+                    \\[\\s*[\\w$]+\\s*\\] |\n+                    \\.\\s*get\\s*\\(\\s*[\\w$\"]+\\s*\\)\n+                  )\\s*){,2}(?:\\s*\\|\\|\\s*null(?=\\s*\\)))?\\s*\n+                \\)\\s*&&\\s*\\(        # ...)&&(\n+                # b = nfunc, b = narray[idx]\n+                (?P=b)\\s*=\\s*(?P<nfunc>[\\w$]+)\\s*\n+                    (?:\\[\\s*(?P<idx>[\\w$]+)\\s*\\]\\s*)?\n+                    # (...)\n+                    \\(\\s*[\\w$]+\\s*\\)\n             ''', jscode, 'Initial JS player n function name', group=('nfunc', 'idx'),\n             default=(None, None))\n         # thx bashonly: yt-dlp/yt-dlp/pull/10611\n@@ -1697,15 +1726,19 @@ def _extract_n_function_name(self, jscode):\n                 r'''(?xs)\n                     (?:(?<=[^\\w$])|^)       # instead of \\b, which ignores $\n                     (?P<name>(?!\\d)[a-zA-Z\\d_$]+)\\s*=\\s*function\\((?!\\d)[a-zA-Z\\d_$]+\\)\n-                    \\s*\\{(?:(?!};).)+?[\"']enhanced_except_\n+                    \\s*\\{(?:(?!};).)+?(?:\n+                        [\"']enhanced_except_ |\n+                        return\\s*(?P<q>\"|')[a-zA-Z\\d-]+_w8_(?P=q)\\s*\\+\\s*[\\w$]+\n+                    )\n                 ''', jscode, 'Initial JS player n function name', group='name')\n         if not idx:\n             return func_name\n \n-        return self._parse_json(self._search_regex(\n-            r'var\\s+{0}\\s*=\\s*(\\[.+?\\])\\s*[,;]'.format(re.escape(func_name)), jscode,\n-            'Initial JS player n function list ({0}.{1})'.format(func_name, idx)),\n-            func_name, transform_source=js_to_json)[int(idx)]\n+        return self._search_json(\n+            r'var\\s+{0}\\s*='.format(re.escape(func_name)), jscode,\n+            'Initial JS player n function list ({0}.{1})'.format(func_name, idx),\n+            func_name, contains_pattern=r'\\[[\\s\\S]+\\]', end_pattern='[,;]',\n+            transform_source=js_to_json)[int(idx)]\n \n     def _extract_n_function_code(self, video_id, player_url):\n         player_id = self._extract_player_info(player_url)\n@@ -1728,13 +1761,13 @@ def _extract_n_function_from_code(self, jsi, func_code):\n \n         def extract_nsig(s):\n             try:\n-                ret = func([s])\n+                ret = func([s], kwargs={'_ytdl_do_not_return': s})\n             except JSInterpreter.Exception:\n                 raise\n             except Exception as e:\n                 raise JSInterpreter.Exception(traceback.format_exc(), cause=e)\n \n-            if ret.startswith('enhanced_except_'):\n+            if ret.startswith('enhanced_except_') or ret.endswith(s):\n                 raise JSInterpreter.Exception('Signature function returned an exception')\n             return ret\n \n@@ -1910,9 +1943,50 @@ def _real_extract(self, url):\n             player_response = self._extract_yt_initial_variable(\n                 webpage, self._YT_INITIAL_PLAYER_RESPONSE_RE,\n                 video_id, 'initial player response')\n-        if not player_response:\n+        if False and not player_response:\n             player_response = self._call_api(\n                 'player', {'videoId': video_id}, video_id)\n+        if True or not player_response:\n+            origin = 'https://www.youtube.com'\n+            pb_context = {'html5Preference': 'HTML5_PREF_WANTS'}\n+\n+            player_url = self._extract_player_url(webpage)\n+            ytcfg = self._extract_ytcfg(video_id, webpage)\n+            sts = self._extract_signature_timestamp(video_id, player_url, ytcfg)\n+            if sts:\n+                pb_context['signatureTimestamp'] = sts\n+\n+            query = {\n+                'playbackContext': {\n+                    'contentPlaybackContext': pb_context,\n+                    'contentCheckOk': True,\n+                    'racyCheckOk': True,\n+                },\n+                'context': {\n+                    'client': {\n+                        'clientName': 'MWEB',\n+                        'clientVersion': '2.20241202.07.00',\n+                        'hl': 'en',\n+                        'userAgent': 'Mozilla/5.0 (iPad; CPU OS 16_7_10 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1,gzip(gfe)',\n+                        'timeZone': 'UTC',\n+                        'utcOffsetMinutes': 0,\n+                    },\n+                },\n+                'videoId': video_id,\n+            }\n+            headers = {\n+                'X-YouTube-Client-Name': '2',\n+                'X-YouTube-Client-Version': '2.20241202.07.00',\n+                'Origin': origin,\n+                'Sec-Fetch-Mode': 'navigate',\n+                'User-Agent': query['context']['client']['userAgent'],\n+            }\n+            auth = self._generate_sapisidhash_header(origin)\n+            if auth is not None:\n+                headers['Authorization'] = auth\n+                headers['X-Origin'] = origin\n+\n+            player_response = self._call_api('player', query, video_id, fatal=False, headers=headers)\n \n         def is_agegated(playability):\n             if not isinstance(playability, dict):\n@@ -2219,12 +2293,12 @@ def process_manifest_format(f, proto, client_name, itag, all_formats=False):\n                         formats.append(f)\n \n         playable_formats = [f for f in formats if not f.get('has_drm')]\n-        if formats and not playable_formats:\n-            # If there are no formats that definitely don't have DRM, all have DRM\n-            self.report_drm(video_id)\n-        formats[:] = playable_formats\n-\n-        if not formats:\n+        if formats:\n+            if not playable_formats:\n+                # If there are no formats that definitely don't have DRM, all have DRM\n+                self.report_drm(video_id)\n+            formats[:] = playable_formats\n+        else:\n             if streaming_data.get('licenseInfos'):\n                 raise ExtractorError(\n                     'This video is DRM protected.', expected=True)\ndiff --git a/youtube_dl/jsinterp.py b/youtube_dl/jsinterp.py\nindex a616ad070b2..7835187f5fa 100644\n--- a/youtube_dl/jsinterp.py\n+++ b/youtube_dl/jsinterp.py\n@@ -1,3 +1,4 @@\n+# coding: utf-8\n from __future__ import unicode_literals\n \n import itertools\n@@ -5,11 +6,12 @@\n import operator\n import re\n \n-from functools import update_wrapper\n+from functools import update_wrapper, wraps\n \n from .utils import (\n     error_to_compat_str,\n     ExtractorError,\n+    float_or_none,\n     js_to_json,\n     remove_quotes,\n     unified_timestamp,\n@@ -20,9 +22,11 @@\n     compat_basestring,\n     compat_chr,\n     compat_collections_chain_map as ChainMap,\n+    compat_contextlib_suppress,\n     compat_filter as filter,\n     compat_itertools_zip_longest as zip_longest,\n     compat_map as map,\n+    compat_numeric_types,\n     compat_str,\n )\n \n@@ -62,6 +66,10 @@ def update_and_rename_wrapper(w):\n _Infinity = float('inf')\n \n \n+class JS_Undefined(object):\n+    pass\n+\n+\n def _js_bit_op(op):\n \n     def zeroise(x):\n@@ -74,43 +82,114 @@ def wrapped(a, b):\n     return wrapped\n \n \n-def _js_arith_op(op):\n+def _js_arith_op(op, div=False):\n \n     @wraps_op(op)\n     def wrapped(a, b):\n         if JS_Undefined in (a, b):\n             return _NaN\n-        return op(a or 0, b or 0)\n+        # null, \"\" --> 0\n+        a, b = (float_or_none(\n+            (x.strip() if isinstance(x, compat_basestring) else x) or 0,\n+            default=_NaN) for x in (a, b))\n+        if _NaN in (a, b):\n+            return _NaN\n+        try:\n+            return op(a, b)\n+        except ZeroDivisionError:\n+            return _NaN if not (div and (a or b)) else _Infinity\n \n     return wrapped\n \n \n-def _js_div(a, b):\n-    if JS_Undefined in (a, b) or not (a or b):\n-        return _NaN\n-    return operator.truediv(a or 0, b) if b else _Infinity\n+_js_arith_add = _js_arith_op(operator.add)\n+\n+\n+def _js_add(a, b):\n+    if not (isinstance(a, compat_basestring) or isinstance(b, compat_basestring)):\n+        return _js_arith_add(a, b)\n+    if not isinstance(a, compat_basestring):\n+        a = _js_toString(a)\n+    elif not isinstance(b, compat_basestring):\n+        b = _js_toString(b)\n+    return operator.concat(a, b)\n \n \n-def _js_mod(a, b):\n-    if JS_Undefined in (a, b) or not b:\n-        return _NaN\n-    return (a or 0) % b\n+_js_mod = _js_arith_op(operator.mod)\n+__js_exp = _js_arith_op(operator.pow)\n \n \n def _js_exp(a, b):\n     if not b:\n         return 1  # even 0 ** 0 !!\n-    elif JS_Undefined in (a, b):\n-        return _NaN\n-    return (a or 0) ** b\n-\n-\n-def _js_eq_op(op):\n+    return __js_exp(a, b)\n+\n+\n+def _js_to_primitive(v):\n+    return (\n+        ','.join(map(_js_toString, v)) if isinstance(v, list)\n+        else '[object Object]' if isinstance(v, dict)\n+        else compat_str(v) if not isinstance(v, (\n+            compat_numeric_types, compat_basestring))\n+        else v\n+    )\n+\n+\n+def _js_toString(v):\n+    return (\n+        'undefined' if v is JS_Undefined\n+        else 'Infinity' if v == _Infinity\n+        else 'NaN' if v is _NaN\n+        else 'null' if v is None\n+        # bool <= int: do this first\n+        else ('false', 'true')[v] if isinstance(v, bool)\n+        else '{0:.7f}'.format(v).rstrip('.0') if isinstance(v, compat_numeric_types)\n+        else _js_to_primitive(v))\n+\n+\n+_nullish = frozenset((None, JS_Undefined))\n+\n+\n+def _js_eq(a, b):\n+    # NaN != any\n+    if _NaN in (a, b):\n+        return False\n+    # Object is Object\n+    if isinstance(a, type(b)) and isinstance(b, (dict, list)):\n+        return operator.is_(a, b)\n+    # general case\n+    if a == b:\n+        return True\n+    # null == undefined\n+    a_b = set((a, b))\n+    if a_b & _nullish:\n+        return a_b <= _nullish\n+    a, b = _js_to_primitive(a), _js_to_primitive(b)\n+    if not isinstance(a, compat_basestring):\n+        a, b = b, a\n+    # Number to String: convert the string to a number\n+    # Conversion failure results in ... false\n+    if isinstance(a, compat_basestring):\n+        return float_or_none(a) == b\n+    return a == b\n+\n+\n+def _js_neq(a, b):\n+    return not _js_eq(a, b)\n+\n+\n+def _js_id_op(op):\n \n     @wraps_op(op)\n     def wrapped(a, b):\n-        if set((a, b)) <= set((None, JS_Undefined)):\n-            return op(a, a)\n+        if _NaN in (a, b):\n+            return op(_NaN, None)\n+        if not isinstance(a, (compat_basestring, compat_numeric_types)):\n+            a, b = b, a\n+        # strings are === if ==\n+        # why 'a' is not 'a': https://stackoverflow.com/a/1504848\n+        if isinstance(a, (compat_basestring, compat_numeric_types)):\n+            return a == b if op(0, 0) else a != b\n         return op(a, b)\n \n     return wrapped\n@@ -138,25 +217,57 @@ def _js_ternary(cndn, if_true=True, if_false=False):\n     return if_true\n \n \n+def _js_unary_op(op):\n+\n+    @wraps_op(op)\n+    def wrapped(_, a):\n+        return op(a)\n+\n+    return wrapped\n+\n+\n+# https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/typeof\n+def _js_typeof(expr):\n+    with compat_contextlib_suppress(TypeError, KeyError):\n+        return {\n+            JS_Undefined: 'undefined',\n+            _NaN: 'number',\n+            _Infinity: 'number',\n+            True: 'boolean',\n+            False: 'boolean',\n+            None: 'object',\n+        }[expr]\n+    for t, n in (\n+        (compat_basestring, 'string'),\n+        (compat_numeric_types, 'number'),\n+    ):\n+        if isinstance(expr, t):\n+            return n\n+    if callable(expr):\n+        return 'function'\n+    # TODO: Symbol, BigInt\n+    return 'object'\n+\n+\n # (op, definition) in order of binding priority, tightest first\n # avoid dict to maintain order\n # definition None => Defined in JSInterpreter._operator\n _OPERATORS = (\n     ('>>', _js_bit_op(operator.rshift)),\n     ('<<', _js_bit_op(operator.lshift)),\n-    ('+', _js_arith_op(operator.add)),\n+    ('+', _js_add),\n     ('-', _js_arith_op(operator.sub)),\n     ('*', _js_arith_op(operator.mul)),\n     ('%', _js_mod),\n-    ('/', _js_div),\n+    ('/', _js_arith_op(operator.truediv, div=True)),\n     ('**', _js_exp),\n )\n \n _COMP_OPERATORS = (\n-    ('===', operator.is_),\n-    ('!==', operator.is_not),\n-    ('==', _js_eq_op(operator.eq)),\n-    ('!=', _js_eq_op(operator.ne)),\n+    ('===', _js_id_op(operator.is_)),\n+    ('!==', _js_id_op(operator.is_not)),\n+    ('==', _js_eq),\n+    ('!=', _js_neq),\n     ('<=', _js_comp_op(operator.le)),\n     ('>=', _js_comp_op(operator.ge)),\n     ('<', _js_comp_op(operator.lt)),\n@@ -176,6 +287,11 @@ def _js_ternary(cndn, if_true=True, if_false=False):\n     ('&&', None),\n )\n \n+_UNARY_OPERATORS_X = (\n+    ('void', _js_unary_op(lambda _: JS_Undefined)),\n+    ('typeof', _js_unary_op(_js_typeof)),\n+)\n+\n _OPERATOR_RE = '|'.join(map(lambda x: re.escape(x[0]), _OPERATORS + _LOG_OPERATORS))\n \n _NAME_RE = r'[a-zA-Z_$][\\w$]*'\n@@ -183,10 +299,6 @@ def _js_ternary(cndn, if_true=True, if_false=False):\n _QUOTES = '\\'\"/'\n \n \n-class JS_Undefined(object):\n-    pass\n-\n-\n class JS_Break(ExtractorError):\n     def __init__(self):\n         ExtractorError.__init__(self, 'Invalid break')\n@@ -242,6 +354,7 @@ def truncate_string(s, left, right=0):\n \n     @classmethod\n     def wrap_interpreter(cls, f):\n+        @wraps(f)\n         def interpret_statement(self, stmt, local_vars, allow_recursion, *args, **kwargs):\n             if cls.ENABLED and stmt.strip():\n                 cls.write(stmt, level=allow_recursion)\n@@ -255,7 +368,7 @@ def interpret_statement(self, stmt, local_vars, allow_recursion, *args, **kwargs\n                 raise\n             if cls.ENABLED and stmt.strip():\n                 if should_ret or repr(ret) != stmt:\n-                    cls.write(['->', '=>'][should_ret], repr(ret), '<-|', stmt, level=allow_recursion)\n+                    cls.write(['->', '=>'][bool(should_ret)], repr(ret), '<-|', stmt, level=allow_recursion)\n             return ret, should_ret\n         return interpret_statement\n \n@@ -284,6 +397,9 @@ class JS_RegExp(object):\n         RE_FLAGS = {\n             # special knowledge: Python's re flags are bitmask values, current max 128\n             # invent new bitmask values well above that for literal parsing\n+            # JS 'u' flag is effectively always set (surrogate pairs aren't seen),\n+            # but \\u{...} and \\p{...} escapes aren't handled); no additional JS 'v'\n+            # features are supported\n             # TODO: execute matches with these flags (remaining: d, y)\n             'd': 1024,  # Generate indices for substring matches\n             'g': 2048,  # Global search\n@@ -291,6 +407,7 @@ class JS_RegExp(object):\n             'm': re.M,  # Multi-line search\n             's': re.S,  # Allows . to match newline characters\n             'u': re.U,  # Treat a pattern as a sequence of unicode code points\n+            'v': re.U,  # Like 'u' with extended character class and \\p{} syntax\n             'y': 4096,  # Perform a \"sticky\" search that matches starting at the current position in the target string\n         }\n \n@@ -347,6 +464,8 @@ def regex_flags(cls, expr):\n     def __op_chars(cls):\n         op_chars = set(';,[')\n         for op in cls._all_operators():\n+            if op[0].isalpha():\n+                continue\n             op_chars.update(op[0])\n         return op_chars\n \n@@ -369,9 +488,18 @@ def _separate(cls, expr, delim=',', max_split=None, skip_delims=None):\n         skipping = 0\n         if skip_delims:\n             skip_delims = variadic(skip_delims)\n+        skip_txt = None\n         for idx, char in enumerate(expr):\n+            if skip_txt and idx <= skip_txt[1]:\n+                continue\n             paren_delta = 0\n             if not in_quote:\n+                if char == '/' and expr[idx:idx + 2] == '/*':\n+                    # skip a comment\n+                    skip_txt = expr[idx:].find('*/', 2)\n+                    skip_txt = [idx, idx + skip_txt + 1] if skip_txt >= 2 else None\n+                    if skip_txt:\n+                        continue\n                 if char in _MATCHING_PARENS:\n                     counters[_MATCHING_PARENS[char]] += 1\n                     paren_delta = 1\n@@ -404,12 +532,19 @@ def _separate(cls, expr, delim=',', max_split=None, skip_delims=None):\n             if pos < delim_len:\n                 pos += 1\n                 continue\n-            yield expr[start: idx - delim_len]\n+            if skip_txt and skip_txt[0] >= start and skip_txt[1] <= idx - delim_len:\n+                yield expr[start:skip_txt[0]] + expr[skip_txt[1] + 1: idx - delim_len]\n+            else:\n+                yield expr[start: idx - delim_len]\n+            skip_txt = None\n             start, pos = idx + 1, 0\n             splits += 1\n             if max_split and splits >= max_split:\n                 break\n-        yield expr[start:]\n+        if skip_txt and skip_txt[0] >= start:\n+            yield expr[start:skip_txt[0]] + expr[skip_txt[1] + 1:]\n+        else:\n+            yield expr[start:]\n \n     @classmethod\n     def _separate_at_paren(cls, expr, delim=None):\n@@ -425,7 +560,7 @@ def _all_operators(_cached=[]):\n         if not _cached:\n             _cached.extend(itertools.chain(\n                 # Ref: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Operator_Precedence\n-                _SC_OPERATORS, _LOG_OPERATORS, _COMP_OPERATORS, _OPERATORS))\n+                _SC_OPERATORS, _LOG_OPERATORS, _COMP_OPERATORS, _OPERATORS, _UNARY_OPERATORS_X))\n         return _cached\n \n     def _operator(self, op, left_val, right_expr, expr, local_vars, allow_recursion):\n@@ -449,13 +584,14 @@ def _operator(self, op, left_val, right_expr, expr, local_vars, allow_recursion)\n         except Exception as e:\n             raise self.Exception('Failed to evaluate {left_val!r:.50} {op} {right_val!r:.50}'.format(**locals()), expr, cause=e)\n \n-    def _index(self, obj, idx, allow_undefined=False):\n-        if idx == 'length':\n+    def _index(self, obj, idx, allow_undefined=True):\n+        if idx == 'length' and isinstance(obj, list):\n             return len(obj)\n         try:\n-            return obj[int(idx)] if isinstance(obj, list) else obj[idx]\n-        except Exception as e:\n+            return obj[int(idx)] if isinstance(obj, list) else obj[compat_str(idx)]\n+        except (TypeError, KeyError, IndexError) as e:\n             if allow_undefined:\n+                # when is not allowed?\n                 return JS_Undefined\n             raise self.Exception('Cannot get index {idx!r:.100}'.format(**locals()), expr=repr(obj), cause=e)\n \n@@ -467,7 +603,7 @@ def _dump(self, obj, namespace):\n \n     # used below\n     _VAR_RET_THROW_RE = re.compile(r'''(?x)\n-        (?P<var>(?:var|const|let)\\s)|return(?:\\s+|(?=[\"'])|$)|(?P<throw>throw\\s+)\n+        (?:(?P<var>var|const|let)\\s+|(?P<ret>return)(?:\\s+|(?=[\"'])|$)|(?P<throw>throw)\\s+)\n         ''')\n     _COMPOUND_RE = re.compile(r'''(?x)\n         (?P<try>try)\\s*\\{|\n@@ -479,6 +615,52 @@ def _dump(self, obj, namespace):\n     _FINALLY_RE = re.compile(r'finally\\s*\\{')\n     _SWITCH_RE = re.compile(r'switch\\s*\\(')\n \n+    def handle_operators(self, expr, local_vars, allow_recursion):\n+\n+        for op, _ in self._all_operators():\n+            # hackety: </> have higher priority than <</>>, but don't confuse them\n+            skip_delim = (op + op) if op in '<>*?' else None\n+            if op == '?':\n+                skip_delim = (skip_delim, '?.')\n+            separated = list(self._separate(expr, op, skip_delims=skip_delim))\n+            if len(separated) < 2:\n+                continue\n+\n+            right_expr = separated.pop()\n+            # handle operators that are both unary and binary, minimal BODMAS\n+            if op in ('+', '-'):\n+                # simplify/adjust consecutive instances of these operators\n+                undone = 0\n+                separated = [s.strip() for s in separated]\n+                while len(separated) > 1 and not separated[-1]:\n+                    undone += 1\n+                    separated.pop()\n+                if op == '-' and undone % 2 != 0:\n+                    right_expr = op + right_expr\n+                elif op == '+':\n+                    while len(separated) > 1 and set(separated[-1]) <= self.OP_CHARS:\n+                        right_expr = separated.pop() + right_expr\n+                    if separated[-1][-1:] in self.OP_CHARS:\n+                        right_expr = separated.pop() + right_expr\n+                # hanging op at end of left => unary + (strip) or - (push right)\n+                left_val = separated[-1] if separated else ''\n+                for dm_op in ('*', '%', '/', '**'):\n+                    bodmas = tuple(self._separate(left_val, dm_op, skip_delims=skip_delim))\n+                    if len(bodmas) > 1 and not bodmas[-1].strip():\n+                        expr = op.join(separated) + op + right_expr\n+                        if len(separated) > 1:\n+                            separated.pop()\n+                            right_expr = op.join((left_val, right_expr))\n+                        else:\n+                            separated = [op.join((left_val, right_expr))]\n+                            right_expr = None\n+                        break\n+                if right_expr is None:\n+                    continue\n+\n+            left_val = self.interpret_expression(op.join(separated), local_vars, allow_recursion)\n+            return self._operator(op, left_val, right_expr, expr, local_vars, allow_recursion), True\n+\n     @Debugger.wrap_interpreter\n     def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n         if allow_recursion < 0:\n@@ -501,7 +683,7 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n             expr = stmt[len(m.group(0)):].strip()\n             if m.group('throw'):\n                 raise JS_Throw(self.interpret_expression(expr, local_vars, allow_recursion))\n-            should_return = not m.group('var')\n+            should_return = 'return' if m.group('ret') else False\n         if not expr:\n             return None, should_return\n \n@@ -533,9 +715,15 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n             else:\n                 raise self.Exception('Unsupported object {obj:.100}'.format(**locals()), expr=expr)\n \n-        if expr.startswith('void '):\n-            left = self.interpret_expression(expr[5:], local_vars, allow_recursion)\n-            return None, should_return\n+        for op, _ in _UNARY_OPERATORS_X:\n+            if not expr.startswith(op):\n+                continue\n+            operand = expr[len(op):]\n+            if not operand or operand[0] != ' ':\n+                continue\n+            op_result = self.handle_operators(expr, local_vars, allow_recursion)\n+            if op_result:\n+                return op_result[0], should_return\n \n         if expr.startswith('{'):\n             inner, outer = self._separate_at_paren(expr)\n@@ -582,7 +770,7 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n                 if_expr, expr = self._separate_at_paren(expr)\n             else:\n                 # may lose ... else ... because of ll.368-374\n-                if_expr, expr = self._separate_at_paren(expr, delim=';')\n+                if_expr, expr = self._separate_at_paren(' %s;' % (expr,), delim=';')\n             else_expr = None\n             m = re.match(r'else\\s*(?P<block>\\{)?', expr)\n             if m:\n@@ -720,7 +908,7 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n             start, end = m.span()\n             sign = m.group('pre_sign') or m.group('post_sign')\n             ret = local_vars[var]\n-            local_vars[var] += 1 if sign[0] == '+' else -1\n+            local_vars[var] = _js_add(ret, 1 if sign[0] == '+' else -1)\n             if m.group('pre_sign'):\n                 ret = local_vars[var]\n             expr = expr[:start] + self._dump(ret, local_vars) + expr[end:]\n@@ -730,13 +918,13 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n \n         m = re.match(r'''(?x)\n             (?P<assign>\n-                (?P<out>{_NAME_RE})(?:\\[(?P<index>[^\\]]+?)\\])?\\s*\n+                (?P<out>{_NAME_RE})(?:\\[(?P<out_idx>(?:.+?\\]\\s*\\[)*.+?)\\])?\\s*\n                 (?P<op>{_OPERATOR_RE})?\n                 =(?!=)(?P<expr>.*)$\n             )|(?P<return>\n                 (?!if|return|true|false|null|undefined|NaN|Infinity)(?P<name>{_NAME_RE})$\n             )|(?P<indexing>\n-                (?P<in>{_NAME_RE})\\[(?P<idx>.+)\\]$\n+                (?P<in>{_NAME_RE})\\[(?P<in_idx>(?:.+?\\]\\s*\\[)*.+?)\\]$\n             )|(?P<attribute>\n                 (?P<var>{_NAME_RE})(?:(?P<nullish>\\?)?\\.(?P<member>[^(]+)|\\[(?P<member2>[^\\]]+)\\])\\s*\n             )|(?P<function>\n@@ -746,19 +934,23 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n         if md.get('assign'):\n             left_val = local_vars.get(m.group('out'))\n \n-            if not m.group('index'):\n+            if not m.group('out_idx'):\n                 local_vars[m.group('out')] = self._operator(\n                     m.group('op'), left_val, m.group('expr'), expr, local_vars, allow_recursion)\n                 return local_vars[m.group('out')], should_return\n             elif left_val in (None, JS_Undefined):\n                 raise self.Exception('Cannot index undefined variable ' + m.group('out'), expr=expr)\n \n-            idx = self.interpret_expression(m.group('index'), local_vars, allow_recursion)\n-            if not isinstance(idx, (int, float)):\n-                raise self.Exception('List index %s must be integer' % (idx, ), expr=expr)\n-            idx = int(idx)\n+            indexes = re.split(r'\\]\\s*\\[', m.group('out_idx'))\n+            for i, idx in enumerate(indexes, 1):\n+                idx = self.interpret_expression(idx, local_vars, allow_recursion)\n+                if i < len(indexes):\n+                    left_val = self._index(left_val, idx)\n+            if isinstance(idx, float):\n+                idx = int(idx)\n             left_val[idx] = self._operator(\n-                m.group('op'), self._index(left_val, idx), m.group('expr'), expr, local_vars, allow_recursion)\n+                m.group('op'), self._index(left_val, idx) if m.group('op') else None,\n+                m.group('expr'), expr, local_vars, allow_recursion)\n             return left_val[idx], should_return\n \n         elif expr.isdigit():\n@@ -776,63 +968,31 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n             return _Infinity, should_return\n \n         elif md.get('return'):\n-            return local_vars[m.group('name')], should_return\n+            ret = local_vars[m.group('name')]\n+            # challenge may try to force returning the original value\n+            # use an optional internal var to block this\n+            if should_return == 'return':\n+                if '_ytdl_do_not_return' not in local_vars:\n+                    return ret, True\n+                return (ret, True) if ret != local_vars['_ytdl_do_not_return'] else (ret, False)\n+            else:\n+                return ret, should_return\n \n-        try:\n+        with compat_contextlib_suppress(ValueError):\n             ret = json.loads(js_to_json(expr))  # strict=True)\n             if not md.get('attribute'):\n                 return ret, should_return\n-        except ValueError:\n-            pass\n \n         if md.get('indexing'):\n             val = local_vars[m.group('in')]\n-            idx = self.interpret_expression(m.group('idx'), local_vars, allow_recursion)\n-            return self._index(val, idx), should_return\n+            for idx in re.split(r'\\]\\s*\\[', m.group('in_idx')):\n+                idx = self.interpret_expression(idx, local_vars, allow_recursion)\n+                val = self._index(val, idx)\n+            return val, should_return\n \n-        for op, _ in self._all_operators():\n-            # hackety: </> have higher priority than <</>>, but don't confuse them\n-            skip_delim = (op + op) if op in '<>*?' else None\n-            if op == '?':\n-                skip_delim = (skip_delim, '?.')\n-            separated = list(self._separate(expr, op, skip_delims=skip_delim))\n-            if len(separated) < 2:\n-                continue\n-\n-            right_expr = separated.pop()\n-            # handle operators that are both unary and binary, minimal BODMAS\n-            if op in ('+', '-'):\n-                # simplify/adjust consecutive instances of these operators\n-                undone = 0\n-                separated = [s.strip() for s in separated]\n-                while len(separated) > 1 and not separated[-1]:\n-                    undone += 1\n-                    separated.pop()\n-                if op == '-' and undone % 2 != 0:\n-                    right_expr = op + right_expr\n-                elif op == '+':\n-                    while len(separated) > 1 and set(separated[-1]) <= self.OP_CHARS:\n-                        right_expr = separated.pop() + right_expr\n-                    if separated[-1][-1:] in self.OP_CHARS:\n-                        right_expr = separated.pop() + right_expr\n-                # hanging op at end of left => unary + (strip) or - (push right)\n-                left_val = separated[-1] if separated else ''\n-                for dm_op in ('*', '%', '/', '**'):\n-                    bodmas = tuple(self._separate(left_val, dm_op, skip_delims=skip_delim))\n-                    if len(bodmas) > 1 and not bodmas[-1].strip():\n-                        expr = op.join(separated) + op + right_expr\n-                        if len(separated) > 1:\n-                            separated.pop()\n-                            right_expr = op.join((left_val, right_expr))\n-                        else:\n-                            separated = [op.join((left_val, right_expr))]\n-                            right_expr = None\n-                        break\n-                if right_expr is None:\n-                    continue\n-\n-            left_val = self.interpret_expression(op.join(separated), local_vars, allow_recursion)\n-            return self._operator(op, left_val, right_expr, expr, local_vars, allow_recursion), should_return\n+        op_result = self.handle_operators(expr, local_vars, allow_recursion)\n+        if op_result:\n+            return op_result[0], should_return\n \n         if md.get('attribute'):\n             variable, member, nullish = m.group('var', 'member', 'nullish')\n@@ -877,7 +1037,7 @@ def eval_method(variable, member):\n \n                 # Member access\n                 if arg_str is None:\n-                    return self._index(obj, member, nullish)\n+                    return self._index(obj, member)\n \n                 # Function call\n                 argvals = [\n@@ -904,7 +1064,7 @@ def eval_method(variable, member):\n                 if obj is compat_str:\n                     if member == 'fromCharCode':\n                         assertion(argvals, 'takes one or more arguments')\n-                        return ''.join(map(compat_chr, argvals))\n+                        return ''.join(compat_chr(int(n)) for n in argvals)\n                     raise self.Exception('Unsupported string method ' + member, expr=expr)\n                 elif obj is float:\n                     if member == 'pow':\n@@ -913,13 +1073,47 @@ def eval_method(variable, member):\n                     raise self.Exception('Unsupported Math method ' + member, expr=expr)\n \n                 if member == 'split':\n-                    assertion(argvals, 'takes one or more arguments')\n-                    assertion(len(argvals) == 1, 'with limit argument is not implemented')\n-                    return obj.split(argvals[0]) if argvals[0] else list(obj)\n+                    assertion(len(argvals) <= 2, 'takes at most two arguments')\n+                    if len(argvals) > 1:\n+                        limit = argvals[1]\n+                        assertion(isinstance(limit, int) and limit >= 0, 'integer limit >= 0')\n+                        if limit == 0:\n+                            return []\n+                    else:\n+                        limit = 0\n+                    if len(argvals) == 0:\n+                        argvals = [JS_Undefined]\n+                    elif isinstance(argvals[0], self.JS_RegExp):\n+                        # avoid re.split(), similar but not enough\n+\n+                        def where():\n+                            for m in argvals[0].finditer(obj):\n+                                yield m.span(0)\n+                            yield (None, None)\n+\n+                        def splits(limit=limit):\n+                            i = 0\n+                            for j, jj in where():\n+                                if j == jj == 0:\n+                                    continue\n+                                if j is None and i >= len(obj):\n+                                    break\n+                                yield obj[i:j]\n+                                if jj is None or limit == 1:\n+                                    break\n+                                limit -= 1\n+                                i = jj\n+\n+                        return list(splits())\n+                    return (\n+                        obj.split(argvals[0], limit - 1) if argvals[0] and argvals[0] != JS_Undefined\n+                        else list(obj)[:limit or None])\n                 elif member == 'join':\n                     assertion(isinstance(obj, list), 'must be applied on a list')\n-                    assertion(len(argvals) == 1, 'takes exactly one argument')\n-                    return argvals[0].join(obj)\n+                    assertion(len(argvals) <= 1, 'takes at most one argument')\n+                    return (',' if len(argvals) == 0 else argvals[0]).join(\n+                        ('' if x in (None, JS_Undefined) else _js_toString(x))\n+                        for x in obj)\n                 elif member == 'reverse':\n                     assertion(not argvals, 'does not take any arguments')\n                     obj.reverse()\n@@ -941,37 +1135,31 @@ def eval_method(variable, member):\n                     index, how_many = map(int, (argvals + [len(obj)])[:2])\n                     if index < 0:\n                         index += len(obj)\n-                    add_items = argvals[2:]\n-                    res = []\n-                    for _ in range(index, min(index + how_many, len(obj))):\n-                        res.append(obj.pop(index))\n-                    for i, item in enumerate(add_items):\n-                        obj.insert(index + i, item)\n+                    res = [obj.pop(index)\n+                           for _ in range(index, min(index + how_many, len(obj)))]\n+                    obj[index:index] = argvals[2:]\n                     return res\n-                elif member == 'unshift':\n-                    assertion(isinstance(obj, list), 'must be applied on a list')\n-                    assertion(argvals, 'takes one or more arguments')\n-                    for item in reversed(argvals):\n-                        obj.insert(0, item)\n-                    return obj\n-                elif member == 'pop':\n+                elif member in ('shift', 'pop'):\n                     assertion(isinstance(obj, list), 'must be applied on a list')\n                     assertion(not argvals, 'does not take any arguments')\n-                    if not obj:\n-                        return\n-                    return obj.pop()\n+                    return obj.pop(0 if member == 'shift' else -1) if len(obj) > 0 else JS_Undefined\n+                elif member == 'unshift':\n+                    assertion(isinstance(obj, list), 'must be applied on a list')\n+                    # not enforced: assertion(argvals, 'takes one or more arguments')\n+                    obj[0:0] = argvals\n+                    return len(obj)\n                 elif member == 'push':\n-                    assertion(argvals, 'takes one or more arguments')\n+                    # not enforced: assertion(argvals, 'takes one or more arguments')\n                     obj.extend(argvals)\n-                    return obj\n+                    return len(obj)\n                 elif member == 'forEach':\n                     assertion(argvals, 'takes one or more arguments')\n-                    assertion(len(argvals) <= 2, 'takes at-most 2 arguments')\n+                    assertion(len(argvals) <= 2, 'takes at most 2 arguments')\n                     f, this = (argvals + [''])[:2]\n                     return [f((item, idx, obj), {'this': this}, allow_recursion) for idx, item in enumerate(obj)]\n                 elif member == 'indexOf':\n                     assertion(argvals, 'takes one or more arguments')\n-                    assertion(len(argvals) <= 2, 'takes at-most 2 arguments')\n+                    assertion(len(argvals) <= 2, 'takes at most 2 arguments')\n                     idx, start = (argvals + [0])[:2]\n                     try:\n                         return obj.index(idx, start)\n@@ -980,7 +1168,7 @@ def eval_method(variable, member):\n                 elif member == 'charCodeAt':\n                     assertion(isinstance(obj, compat_str), 'must be applied on a string')\n                     # assertion(len(argvals) == 1, 'takes exactly one argument') # but not enforced\n-                    idx = argvals[0] if isinstance(argvals[0], int) else 0\n+                    idx = argvals[0] if len(argvals) > 0 and isinstance(argvals[0], int) else 0\n                     if idx >= len(obj):\n                         return None\n                     return ord(obj[idx])\n@@ -1031,7 +1219,7 @@ def interpret_iter(self, list_txt, local_vars, allow_recursion):\n             yield self.interpret_expression(v, local_vars, allow_recursion)\n \n     def extract_object(self, objname):\n-        _FUNC_NAME_RE = r'''(?:[a-zA-Z$0-9]+|\"[a-zA-Z$0-9]+\"|'[a-zA-Z$0-9]+')'''\n+        _FUNC_NAME_RE = r'''(?:{n}|\"{n}\"|'{n}')'''.format(n=_NAME_RE)\n         obj = {}\n         fields = next(filter(None, (\n             obj_m.group('fields') for obj_m in re.finditer(\n@@ -1090,6 +1278,7 @@ def extract_function(self, funcname):\n \n     def extract_function_from_code(self, argnames, code, *global_stack):\n         local_vars = {}\n+\n         while True:\n             mobj = re.search(r'function\\((?P<args>[^)]*)\\)\\s*{', code)\n             if mobj is None:\n@@ -1100,10 +1289,11 @@ def extract_function_from_code(self, argnames, code, *global_stack):\n                 [x.strip() for x in mobj.group('args').split(',')],\n                 body, local_vars, *global_stack))\n             code = code[:start] + name + remaining\n+\n         return self.build_function(argnames, code, local_vars, *global_stack)\n \n-    def call_function(self, funcname, *args):\n-        return self.extract_function(funcname)(args)\n+    def call_function(self, funcname, *args, **kw_global_vars):\n+        return self.extract_function(funcname)(args, kw_global_vars)\n \n     @classmethod\n     def build_arglist(cls, arg_text):\n@@ -1122,8 +1312,9 @@ def build_function(self, argnames, code, *global_stack):\n         global_stack = list(global_stack) or [{}]\n         argnames = tuple(argnames)\n \n-        def resf(args, kwargs={}, allow_recursion=100):\n-            global_stack[0].update(zip_longest(argnames, args, fillvalue=None))\n+        def resf(args, kwargs=None, allow_recursion=100):\n+            kwargs = kwargs or {}\n+            global_stack[0].update(zip_longest(argnames, args, fillvalue=JS_Undefined))\n             global_stack[0].update(kwargs)\n             var_stack = LocalNameSpace(*global_stack)\n             ret, should_abort = self.interpret_statement(code.replace('\\n', ' '), var_stack, allow_recursion - 1)\n", "test_patch": "diff --git a/test/test_jsinterp.py b/test/test_jsinterp.py\nindex c7a4f2cbf23..12e7b9b9485 100644\n--- a/test/test_jsinterp.py\n+++ b/test/test_jsinterp.py\n@@ -1,4 +1,5 @@\n #!/usr/bin/env python\n+# coding: utf-8\n \n from __future__ import unicode_literals\n \n@@ -11,7 +12,7 @@\n import math\n import re\n \n-from youtube_dl.compat import compat_str\n+from youtube_dl.compat import compat_str as str\n from youtube_dl.jsinterp import JS_Undefined, JSInterpreter\n \n NaN = object()\n@@ -19,7 +20,7 @@\n \n class TestJSInterpreter(unittest.TestCase):\n     def _test(self, jsi_or_code, expected, func='f', args=()):\n-        if isinstance(jsi_or_code, compat_str):\n+        if isinstance(jsi_or_code, str):\n             jsi_or_code = JSInterpreter(jsi_or_code)\n         got = jsi_or_code.call_function(func, *args)\n         if expected is NaN:\n@@ -40,16 +41,27 @@ def test_add(self):\n         self._test('function f(){return 42 + 7;}', 49)\n         self._test('function f(){return 42 + undefined;}', NaN)\n         self._test('function f(){return 42 + null;}', 42)\n+        self._test('function f(){return 1 + \"\";}', '1')\n+        self._test('function f(){return 42 + \"7\";}', '427')\n+        self._test('function f(){return false + true;}', 1)\n+        self._test('function f(){return \"false\" + true;}', 'falsetrue')\n+        self._test('function f(){return '\n+                   '1 + \"2\" + [3,4] + {k: 56} + null + undefined + Infinity;}',\n+                   '123,4[object Object]nullundefinedInfinity')\n \n     def test_sub(self):\n         self._test('function f(){return 42 - 7;}', 35)\n         self._test('function f(){return 42 - undefined;}', NaN)\n         self._test('function f(){return 42 - null;}', 42)\n+        self._test('function f(){return 42 - \"7\";}', 35)\n+        self._test('function f(){return 42 - \"spam\";}', NaN)\n \n     def test_mul(self):\n         self._test('function f(){return 42 * 7;}', 294)\n         self._test('function f(){return 42 * undefined;}', NaN)\n         self._test('function f(){return 42 * null;}', 0)\n+        self._test('function f(){return 42 * \"7\";}', 294)\n+        self._test('function f(){return 42 * \"eggs\";}', NaN)\n \n     def test_div(self):\n         jsi = JSInterpreter('function f(a, b){return a / b;}')\n@@ -57,17 +69,26 @@ def test_div(self):\n         self._test(jsi, NaN, args=(JS_Undefined, 1))\n         self._test(jsi, float('inf'), args=(2, 0))\n         self._test(jsi, 0, args=(0, 3))\n+        self._test(jsi, 6, args=(42, 7))\n+        self._test(jsi, 0, args=(42, float('inf')))\n+        self._test(jsi, 6, args=(\"42\", 7))\n+        self._test(jsi, NaN, args=(\"spam\", 7))\n \n     def test_mod(self):\n         self._test('function f(){return 42 % 7;}', 0)\n         self._test('function f(){return 42 % 0;}', NaN)\n         self._test('function f(){return 42 % undefined;}', NaN)\n+        self._test('function f(){return 42 % \"7\";}', 0)\n+        self._test('function f(){return 42 % \"beans\";}', NaN)\n \n     def test_exp(self):\n         self._test('function f(){return 42 ** 2;}', 1764)\n         self._test('function f(){return 42 ** undefined;}', NaN)\n         self._test('function f(){return 42 ** null;}', 1)\n+        self._test('function f(){return undefined ** 0;}', 1)\n         self._test('function f(){return undefined ** 42;}', NaN)\n+        self._test('function f(){return 42 ** \"2\";}', 1764)\n+        self._test('function f(){return 42 ** \"spam\";}', NaN)\n \n     def test_calc(self):\n         self._test('function f(a){return 2*a+1;}', 7, args=[3])\n@@ -89,7 +110,35 @@ def test_operators(self):\n         self._test('function f(){return 19 & 21;}', 17)\n         self._test('function f(){return 11 >> 2;}', 2)\n         self._test('function f(){return []? 2+3: 4;}', 5)\n+        # equality\n+        self._test('function f(){return 1 == 1}', True)\n+        self._test('function f(){return 1 == 1.0}', True)\n+        self._test('function f(){return 1 == \"1\"}', True)\n         self._test('function f(){return 1 == 2}', False)\n+        self._test('function f(){return 1 != \"1\"}', False)\n+        self._test('function f(){return 1 != 2}', True)\n+        self._test('function f(){var x = {a: 1}; var y = x; return x == y}', True)\n+        self._test('function f(){var x = {a: 1}; return x == {a: 1}}', False)\n+        self._test('function f(){return NaN == NaN}', False)\n+        self._test('function f(){return null == undefined}', True)\n+        self._test('function f(){return \"spam, eggs\" == \"spam, eggs\"}', True)\n+        # strict equality\n+        self._test('function f(){return 1 === 1}', True)\n+        self._test('function f(){return 1 === 1.0}', True)\n+        self._test('function f(){return 1 === \"1\"}', False)\n+        self._test('function f(){return 1 === 2}', False)\n+        self._test('function f(){var x = {a: 1}; var y = x; return x === y}', True)\n+        self._test('function f(){var x = {a: 1}; return x === {a: 1}}', False)\n+        self._test('function f(){return NaN === NaN}', False)\n+        self._test('function f(){return null === undefined}', False)\n+        self._test('function f(){return null === null}', True)\n+        self._test('function f(){return undefined === undefined}', True)\n+        self._test('function f(){return \"uninterned\" === \"uninterned\"}', True)\n+        self._test('function f(){return 1 === 1}', True)\n+        self._test('function f(){return 1 === \"1\"}', False)\n+        self._test('function f(){return 1 !== 1}', False)\n+        self._test('function f(){return 1 !== \"1\"}', True)\n+        # expressions\n         self._test('function f(){return 0 && 1 || 2;}', 2)\n         self._test('function f(){return 0 ?? 42;}', 0)\n         self._test('function f(){return \"life, the universe and everything\" < 42;}', False)\n@@ -111,7 +160,6 @@ def test_assignments(self):\n         self._test('function f(){var x = 20; x += 30 + 1; return x;}', 51)\n         self._test('function f(){var x = 20; x -= 30 + 1; return x;}', -11)\n \n-    @unittest.skip('Not yet fully implemented')\n     def test_comments(self):\n         self._test('''\n             function f() {\n@@ -130,6 +178,15 @@ def test_comments(self):\n             }\n         ''', 3)\n \n+        self._test('''\n+            function f() {\n+                var x = ( /* 1 + */ 2 +\n+                          /* 30 * 40 */\n+                          50);\n+                return x;\n+            }\n+        ''', 52)\n+\n     def test_precedence(self):\n         self._test('''\n             function f() {\n@@ -266,7 +323,20 @@ def test_comma(self):\n         self._test('function f() { return (l=[0,1,2,3], function(a, b){return a+b})((l[1], l[2]), l[3]) }', 5)\n \n     def test_void(self):\n-        self._test('function f() { return void 42; }', None)\n+        self._test('function f() { return void 42; }', JS_Undefined)\n+\n+    def test_typeof(self):\n+        self._test('function f() { return typeof undefined; }', 'undefined')\n+        self._test('function f() { return typeof NaN; }', 'number')\n+        self._test('function f() { return typeof Infinity; }', 'number')\n+        self._test('function f() { return typeof true; }', 'boolean')\n+        self._test('function f() { return typeof null; }', 'object')\n+        self._test('function f() { return typeof \"a string\"; }', 'string')\n+        self._test('function f() { return typeof 42; }', 'number')\n+        self._test('function f() { return typeof 42.42; }', 'number')\n+        self._test('function f() { var g = function(){}; return typeof g; }', 'function')\n+        self._test('function f() { return typeof {key: \"value\"}; }', 'object')\n+        # not yet implemented: Symbol, BigInt\n \n     def test_return_function(self):\n         jsi = JSInterpreter('''\n@@ -283,7 +353,7 @@ def test_null(self):\n     def test_undefined(self):\n         self._test('function f() { return undefined === undefined; }', True)\n         self._test('function f() { return undefined; }', JS_Undefined)\n-        self._test('function f() {return undefined ?? 42; }', 42)\n+        self._test('function f() { return undefined ?? 42; }', 42)\n         self._test('function f() { let v; return v; }', JS_Undefined)\n         self._test('function f() { let v; return v**0; }', 1)\n         self._test('function f() { let v; return [v>42, v<=42, v&&42, 42&&v]; }',\n@@ -324,6 +394,16 @@ def test_object(self):\n         self._test('function f() { let a; return a?.qq; }', JS_Undefined)\n         self._test('function f() { let a = {m1: 42, m2: 0 }; return a?.qq; }', JS_Undefined)\n \n+    def test_indexing(self):\n+        self._test('function f() { return [1, 2, 3, 4][3]}', 4)\n+        self._test('function f() { return [1, [2, [3, [4]]]][1][1][1][0]}', 4)\n+        self._test('function f() { var o = {1: 2, 3: 4}; return o[3]}', 4)\n+        self._test('function f() { var o = {1: 2, 3: 4}; return o[\"3\"]}', 4)\n+        self._test('function f() { return [1, [2, {3: [4]}]][1][1][\"3\"][0]}', 4)\n+        self._test('function f() { return [1, 2, 3, 4].length}', 4)\n+        self._test('function f() { var o = {1: 2, 3: 4}; return o.length}', JS_Undefined)\n+        self._test('function f() { var o = {1: 2, 3: 4}; o[\"length\"] = 42; return o.length}', 42)\n+\n     def test_regex(self):\n         self._test('function f() { let a=/,,[/,913,/](,)}/; }', None)\n \n@@ -411,6 +491,13 @@ def test_join(self):\n             self._test(jsi, 't-e-s-t', args=[test_input, '-'])\n             self._test(jsi, '', args=[[], '-'])\n \n+        self._test('function f(){return '\n+                   '[1, 1.0, \"abc\", {a: 1}, null, undefined, Infinity, NaN].join()}',\n+                   '1,1,abc,[object Object],,,Infinity,NaN')\n+        self._test('function f(){return '\n+                   '[1, 1.0, \"abc\", {a: 1}, null, undefined, Infinity, NaN].join(\"~\")}',\n+                   '1~1~abc~[object Object]~~~Infinity~NaN')\n+\n     def test_split(self):\n         test_result = list('test')\n         tests = [\n@@ -424,6 +511,18 @@ def test_split(self):\n             self._test(jsi, test_result, args=['t-e-s-t', '-'])\n             self._test(jsi, [''], args=['', '-'])\n             self._test(jsi, [], args=['', ''])\n+        # RegExp split\n+        self._test('function f(){return \"test\".split(/(?:)/)}',\n+                   ['t', 'e', 's', 't'])\n+        self._test('function f(){return \"t-e-s-t\".split(/[es-]+/)}',\n+                   ['t', 't'])\n+        # from MDN: surrogate pairs aren't handled: case 1 fails\n+        # self._test('function f(){return \"\ud83d\ude04\ud83d\ude04\".split(/(?:)/)}',\n+        #            ['\\ud83d', '\\ude04', '\\ud83d', '\\ude04'])\n+        # case 2 beats Py3.2: it gets the case 1 result\n+        if sys.version_info >= (2, 6) and not ((3, 0) <= sys.version_info < (3, 3)):\n+            self._test('function f(){return \"\ud83d\ude04\ud83d\ude04\".split(/(?:)/u)}',\n+                       ['\ud83d\ude04', '\ud83d\ude04'])\n \n     def test_slice(self):\n         self._test('function f(){return [0, 1, 2, 3, 4, 5, 6, 7, 8].slice()}', [0, 1, 2, 3, 4, 5, 6, 7, 8])\n@@ -453,6 +552,40 @@ def test_slice(self):\n         self._test('function f(){return \"012345678\".slice(-1, 1)}', '')\n         self._test('function f(){return \"012345678\".slice(-3, -1)}', '67')\n \n+    def test_pop(self):\n+        # pop\n+        self._test('function f(){var a = [0, 1, 2, 3, 4, 5, 6, 7, 8]; return [a.pop(), a]}',\n+                   [8, [0, 1, 2, 3, 4, 5, 6, 7]])\n+        self._test('function f(){return [].pop()}', JS_Undefined)\n+        # push\n+        self._test('function f(){var a = [0, 1, 2]; return [a.push(3, 4), a]}',\n+                   [5, [0, 1, 2, 3, 4]])\n+        self._test('function f(){var a = [0, 1, 2]; return [a.push(), a]}',\n+                   [3, [0, 1, 2]])\n+\n+    def test_shift(self):\n+        # shift\n+        self._test('function f(){var a = [0, 1, 2, 3, 4, 5, 6, 7, 8]; return [a.shift(), a]}',\n+                   [0, [1, 2, 3, 4, 5, 6, 7, 8]])\n+        self._test('function f(){return [].shift()}', JS_Undefined)\n+        # unshift\n+        self._test('function f(){var a = [0, 1, 2]; return [a.unshift(3, 4), a]}',\n+                   [5, [3, 4, 0, 1, 2]])\n+        self._test('function f(){var a = [0, 1, 2]; return [a.unshift(), a]}',\n+                   [3, [0, 1, 2]])\n+\n+    def test_forEach(self):\n+        self._test('function f(){var ret = []; var l = [4, 2]; '\n+                   'var log = function(e,i,a){ret.push([e,i,a]);}; '\n+                   'l.forEach(log); '\n+                   'return [ret.length, ret[0][0], ret[1][1], ret[0][2]]}',\n+                   [2, 4, 1, [4, 2]])\n+        self._test('function f(){var ret = []; var l = [4, 2]; '\n+                   'var log = function(e,i,a){this.push([e,i,a]);}; '\n+                   'l.forEach(log, ret); '\n+                   'return [ret.length, ret[0][0], ret[1][1], ret[0][2]]}',\n+                   [2, 4, 1, [4, 2]])\n+\n \n if __name__ == '__main__':\n     unittest.main()\ndiff --git a/test/test_youtube_signature.py b/test/test_youtube_signature.py\nindex 56e92fac5df..fcbc9d7a813 100644\n--- a/test/test_youtube_signature.py\n+++ b/test/test_youtube_signature.py\n@@ -1,4 +1,5 @@\n #!/usr/bin/env python\n+# coding: utf-8\n \n from __future__ import unicode_literals\n \n@@ -12,6 +13,7 @@\n import string\n \n from youtube_dl.compat import (\n+    compat_contextlib_suppress,\n     compat_open as open,\n     compat_str,\n     compat_urlretrieve,\n@@ -50,23 +52,38 @@\n     (\n         'https://s.ytimg.com/yts/jsbin/html5player-en_US-vflBb0OQx.js',\n         84,\n-        '123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQ0STUVWXYZ!\"#$%&\\'()*+,@./:;<=>'\n+        '123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQ0STUVWXYZ!\"#$%&\\'()*+,@./:;<=>',\n     ),\n     (\n         'https://s.ytimg.com/yts/jsbin/html5player-en_US-vfl9FYC6l.js',\n         83,\n-        '123456789abcdefghijklmnopqr0tuvwxyzABCDETGHIJKLMNOPQRS>UVWXYZ!\"#$%&\\'()*+,-./:;<=F'\n+        '123456789abcdefghijklmnopqr0tuvwxyzABCDETGHIJKLMNOPQRS>UVWXYZ!\"#$%&\\'()*+,-./:;<=F',\n     ),\n     (\n         'https://s.ytimg.com/yts/jsbin/html5player-en_US-vflCGk6yw/html5player.js',\n         '4646B5181C6C3020DF1D9C7FCFEA.AD80ABF70C39BD369CCCAE780AFBB98FA6B6CB42766249D9488C288',\n-        '82C8849D94266724DC6B6AF89BBFA087EACCD963.B93C07FBA084ACAEFCF7C9D1FD0203C6C1815B6B'\n+        '82C8849D94266724DC6B6AF89BBFA087EACCD963.B93C07FBA084ACAEFCF7C9D1FD0203C6C1815B6B',\n     ),\n     (\n         'https://s.ytimg.com/yts/jsbin/html5player-en_US-vflKjOTVq/html5player.js',\n         '312AA52209E3623129A412D56A40F11CB0AF14AE.3EE09501CB14E3BCDC3B2AE808BF3F1D14E7FBF12',\n         '112AA5220913623229A412D56A40F11CB0AF14AE.3EE0950FCB14EEBCDC3B2AE808BF331D14E7FBF3',\n-    )\n+    ),\n+    (\n+        'https://www.youtube.com/s/player/6ed0d907/player_ias.vflset/en_US/base.js',\n+        '2aq0aqSyOoJXtK73m-uME_jv7-pT15gOFC02RFkGMqWpzEICs69VdbwQ0LDp1v7j8xx92efCJlYFYb1sUkkBSPOlPmXgIARw8JQ0qOAOAA',\n+        'AOq0QJ8wRAIgXmPlOPSBkkUs1bYFYlJCfe29xx8j7v1pDL2QwbdV96sCIEzpWqMGkFR20CFOg51Tp-7vj_EMu-m37KtXJoOySqa0',\n+    ),\n+    (\n+        'https://www.youtube.com/s/player/3bb1f723/player_ias.vflset/en_US/base.js',\n+        '2aq0aqSyOoJXtK73m-uME_jv7-pT15gOFC02RFkGMqWpzEICs69VdbwQ0LDp1v7j8xx92efCJlYFYb1sUkkBSPOlPmXgIARw8JQ0qOAOAA',\n+        'MyOSJXtKI3m-uME_jv7-pT12gOFC02RFkGoqWpzE0Cs69VdbwQ0LDp1v7j8xx92efCJlYFYb1sUkkBSPOlPmXgIARw8JQ0qOAOAA',\n+    ),\n+    (\n+        'https://www.youtube.com/s/player/2f1832d2/player_ias.vflset/en_US/base.js',\n+        '2aq0aqSyOoJXtK73m-uME_jv7-pT15gOFC02RFkGMqWpzEICs69VdbwQ0LDp1v7j8xx92efCJlYFYb1sUkkBSPOlPmXgIARw8JQ0qOAOAA',\n+        '0QJ8wRAIgXmPlOPSBkkUs1bYFYlJCfe29xxAj7v1pDL0QwbdV96sCIEzpWqMGkFR20CFOg51Tp-7vj_EMu-m37KtXJ2OySqa0q',\n+    ),\n ]\n \n _NSIG_TESTS = [\n@@ -142,6 +159,10 @@\n         'https://www.youtube.com/s/player/5a3b6271/player_ias.vflset/en_US/base.js',\n         'B2j7f_UPT4rfje85Lu_e', 'm5DmNymaGQ5RdQ',\n     ),\n+    (\n+        'https://www.youtube.com/s/player/7a062b77/player_ias.vflset/en_US/base.js',\n+        'NRcE3y3mVtm_cV-W', 'VbsCYUATvqlt5w',\n+    ),\n     (\n         'https://www.youtube.com/s/player/dac945fd/player_ias.vflset/en_US/base.js',\n         'o8BkRxXhuYsBCWi6RplPdP', '3Lx32v_hmzTm6A',\n@@ -154,6 +175,10 @@\n         'https://www.youtube.com/s/player/cfa9e7cb/player_ias.vflset/en_US/base.js',\n         'qO0NiMtYQ7TeJnfFG2', 'k9cuJDHNS5O7kQ',\n     ),\n+    (\n+        'https://www.youtube.com/s/player/8c7583ff/player_ias.vflset/en_US/base.js',\n+        '1wWCVpRR96eAmMI87L', 'KSkWAVv1ZQxC3A',\n+    ),\n     (\n         'https://www.youtube.com/s/player/b7910ca8/player_ias.vflset/en_US/base.js',\n         '_hXMCwMt9qE310D', 'LoZMgkkofRMCZQ',\n@@ -182,6 +207,18 @@\n         'https://www.youtube.com/s/player/b12cc44b/player_ias.vflset/en_US/base.js',\n         'keLa5R2U00sR9SQK', 'N1OGyujjEwMnLw',\n     ),\n+    (\n+        'https://www.youtube.com/s/player/3bb1f723/player_ias.vflset/en_US/base.js',\n+        'gK15nzVyaXE9RsMP3z', 'ZFFWFLPWx9DEgQ',\n+    ),\n+    (\n+        'https://www.youtube.com/s/player/f8f53e1a/player_ias.vflset/en_US/base.js',\n+        'VTQOUOv0mCIeJ7i8kZB', 'kcfD8wy0sNLyNQ',\n+    ),\n+    (\n+        'https://www.youtube.com/s/player/2f1832d2/player_ias.vflset/en_US/base.js',\n+        'YWt1qdbe8SAfkoPHW5d', 'RrRjWQOJmBiP',\n+    ),\n ]\n \n \n@@ -216,11 +253,9 @@ def setUp(self):\n             os.mkdir(self.TESTDATA_DIR)\n \n     def tearDown(self):\n-        try:\n+        with compat_contextlib_suppress(OSError):\n             for f in os.listdir(self.TESTDATA_DIR):\n                 os.remove(f)\n-        except OSError:\n-            pass\n \n \n def t_factory(name, sig_func, url_pattern):\n@@ -254,11 +289,12 @@ def signature(jscode, sig_input):\n \n def n_sig(jscode, sig_input):\n     funcname = YoutubeIE(FakeYDL())._extract_n_function_name(jscode)\n-    return JSInterpreter(jscode).call_function(funcname, sig_input)\n+    return JSInterpreter(jscode).call_function(\n+        funcname, sig_input, _ytdl_do_not_return=sig_input)\n \n \n make_sig_test = t_factory(\n-    'signature', signature, re.compile(r'.*-(?P<id>[a-zA-Z0-9_-]+)(?:/watch_as3|/html5player)?\\.[a-z]+$'))\n+    'signature', signature, re.compile(r'.*(?:-|/player/)(?P<id>[a-zA-Z0-9_-]+)(?:/.+\\.js|(?:/watch_as3|/html5player)?\\.[a-z]+)$'))\n for test_spec in _SIG_TESTS:\n     make_sig_test(*test_spec)\n \n", "problem_statement": "[YOUTUBE] ERROR: Unable to extract nsig function code (caused by RegexNotFoundError('Unable to extract Initial JS player n function name... \n<!--\r\n\r\n######################################################################\r\n  WARNING!\r\n  IGNORING THE FOLLOWING TEMPLATE WILL RESULT IN ISSUE CLOSED AS INCOMPLETE\r\n######################################################################\r\n\r\n-->\r\n\r\n\r\n## Checklist\r\n\r\n<!--\r\nCarefully read and work through this check list in order to prevent the most common mistakes and misuse of youtube-dl:\r\n- First of, make sure you are using the latest version of youtube-dl. Run `youtube-dl --version` and ensure your version is 2021.12.17. If it's not, see https://yt-dl.org/update on how to update. Issues with outdated version will be REJECTED.\r\n- Make sure that all provided video/audio/playlist URLs (if any) are alive and playable in a browser.\r\n- Make sure that all URLs and arguments with special characters are properly quoted or escaped as explained in http://yt-dl.org/escape.\r\n- Search the bugtracker for similar issues: http://yt-dl.org/search-issues. DO NOT post duplicates.\r\n- Finally, put x into all relevant boxes (like this [x])\r\n-->\r\n\r\n- [x] I'm reporting a broken site support\r\n- [x] I've verified that I'm running youtube-dl version **2021.12.17**\r\n- [x] I've checked that all provided URLs are alive and playable in a browser\r\n- [x] I've checked that all URLs and arguments with special characters are properly quoted or escaped\r\n- [x] I've searched the bugtracker for similar issues including closed ones\r\n\r\n\r\n## Verbose log\r\n\r\n<!--\r\nProvide the complete verbose output of youtube-dl that clearly demonstrates the problem.\r\nAdd the `-v` flag to your command line you run youtube-dl with (`youtube-dl -v <your command line>`), copy the WHOLE output and insert it below. It should look similar to this:\r\n [debug] System config: []\r\n [debug] User config: []\r\n [debug] Command-line args: [u'-v', u'http://www.youtube.com/watch?v=BaW_jenozKcj']\r\n [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251\r\n [debug] youtube-dl version 2021.12.17\r\n [debug] Python version 2.7.11 - Windows-2003Server-5.2.3790-SP2\r\n [debug] exe versions: ffmpeg N-75573-g1d0487f, ffprobe N-75573-g1d0487f, rtmpdump 2.4\r\n [debug] Proxy map: {}\r\n <more lines>\r\n-->\r\n\r\n```\r\n==========================\r\nTESTING NORMAL YOUTUBE-DL:\r\n==========================\r\n\r\n\r\n[debug] System config: []\r\n[debug] User config: ['--no-mtime', '--match-filter', '!is_live', '--retries', 'infinite', '--fragment-retries', '3', '--skip-unavailable-fragments', '--restrict-filenames', '-i', '-o', '/home/gregorius/home/pending/videos/%(title)s___%(id)s.webm', '-f', '(bestvideo[height<=360]+worstaudio/best[height<=360])[protocol!=http_dash_segments][container!^=dash]', '--console-title', '--hls-prefer-native', '--no-cache-dir', '--cookies', '/home/gregorius/home/scripts/video/youtube-dl-cookies']\r\n[debug] Custom config: []\r\n[debug] Command-line args: ['https://www.youtube.com/watch?v=eMNXpZZBWFE', '-vf', '(242+249/242+250/242+171/242+251)/(243+249/243+250/243+171/243+251)/18', '--no-playlist', '-o', '/home/gregorius/home/scripts/video/TEST_NORMAL_%(title)s___%(id)s.webm']\r\n[debug] Encodings: locale UTF-8, fs utf-8, out utf-8, pref UTF-8\r\n[debug] youtube-dl version 2021.12.17\r\n[debug] Single file build\r\n[debug] Python 3.10.12 (CPython x86_64 64bit) - Linux-5.15.0-124-generic-x86_64-with-glibc2.35 - OpenSSL 3.0.2 15 Mar 2022 - glibc 2.35\r\n[debug] exe versions: ffmpeg 4.4.2, ffprobe 4.4.2, rtmpdump 2.4\r\n[debug] Proxy map: {}\r\n[youtube] eMNXpZZBWFE: Downloading webpage\r\n[youtube] Downloading just video eMNXpZZBWFE because of --no-playlist\r\n[youtube] eMNXpZZBWFE: Downloading player 3bb1f723\r\nWARNING: [youtube] Falling back to generic n function search\r\nERROR: Unable to extract nsig function code (caused by RegexNotFoundError('Unable to extract \\x1b[0;34mInitial JS player n function name\\x1b[0m; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.')); please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\nTraceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1637, in _decrypt_nsig\r\n    jsi, player_id, func_code = self._extract_n_function_code(video_id, player_url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1719, in _extract_n_function_code\r\n    func_name = self._extract_n_function_name(jscode)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1696, in _extract_n_function_name\r\n    return self._search_regex(\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/common.py\", line 1101, in _search_regex\r\n    raise RegexNotFoundError('Unable to extract %s' % _name)\r\nyoutube_dl.utils.RegexNotFoundError: Unable to extract Initial JS player n function name; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\nTraceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1637, in _decrypt_nsig\r\n    jsi, player_id, func_code = self._extract_n_function_code(video_id, player_url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1719, in _extract_n_function_code\r\n    func_name = self._extract_n_function_name(jscode)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1696, in _extract_n_function_name\r\n    return self._search_regex(\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/common.py\", line 1101, in _search_regex\r\n    raise RegexNotFoundError('Unable to extract %s' % _name)\r\nyoutube_dl.utils.RegexNotFoundError: Unable to extract Initial JS player n function name; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/YoutubeDL.py\", line 875, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/YoutubeDL.py\", line 971, in __extract_info\r\n    ie_result = ie.extract(url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/common.py\", line 571, in extract\r\n    ie_result = self._real_extract(url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 2137, in _real_extract\r\n    self._unthrottle_format_urls(video_id, player_url, dct)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1754, in _unthrottle_format_urls\r\n    n_response = decrypt_nsig(n_param)(n_param, video_id, player_url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1614, in inner\r\n    raise ret\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1606, in inner\r\n    self._player_cache[cache_id] = func(*args, **kwargs)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1639, in _decrypt_nsig\r\n    raise ExtractorError('Unable to extract nsig function code', cause=e)\r\nyoutube_dl.utils.ExtractorError: Unable to extract nsig function code (caused by RegexNotFoundError('Unable to extract \\x1b[0;34mInitial JS player n function name\\x1b[0m; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.')); please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n\r\n\r\n===========================\r\nTESTING NIGHTLY YOUTUBE-DL:\r\n===========================\r\n\r\n\r\n[debug] System config: []\r\n[debug] User config: ['--no-mtime', '--match-filter', '!is_live', '--retries', 'infinite', '--fragment-retries', '3', '--skip-unavailable-fragments', '--restrict-filenames', '-i', '-o', '/home/gregorius/home/pending/videos/%(title)s___%(id)s.webm', '-f', '(bestvideo[height<=360]+worstaudio/best[height<=360])[protocol!=http_dash_segments][container!^=dash]', '--console-title', '--hls-prefer-native', '--no-cache-dir', '--cookies', '/home/gregorius/home/scripts/video/youtube-dl-cookies']\r\n[debug] Custom config: []\r\n[debug] Command-line args: ['https://www.youtube.com/watch?v=eMNXpZZBWFE', '-vf', '(242+249/242+250/242+171/242+251)/(243+249/243+250/243+171/243+251)/18', '--no-playlist', '-o', '/home/gregorius/home/scripts/video/TEST_NIGHTLY_%(title)s___%(id)s.webm']\r\n[debug] Encodings: locale UTF-8, fs utf-8, out utf-8, pref UTF-8\r\n[debug] youtube-dl version 2024.08.07 [c5098961b] (single file build)\r\n[debug] ** This version was built from the latest master code at https://github.com/ytdl-org/youtube-dl.\r\n[debug] ** For support, visit the main site.\r\n[debug] Python 3.10.12 (CPython x86_64 64bit) - Linux-5.15.0-124-generic-x86_64-with-glibc2.35 - OpenSSL 3.0.2 15 Mar 2022 - glibc 2.35\r\n[debug] exe versions: ffmpeg 4.4.2, ffprobe 4.4.2, rtmpdump 2.4\r\n[debug] Proxy map: {}\r\n[youtube] eMNXpZZBWFE: Downloading webpage\r\n[youtube] Downloading just video eMNXpZZBWFE because of --no-playlist\r\n[youtube] eMNXpZZBWFE: Downloading player 3bb1f723\r\nWARNING: [youtube] Falling back to generic n function search\r\nERROR: Unable to extract nsig function code (caused by RegexNotFoundError('Unable to extract \\x1b[0;34mInitial JS player n function name\\x1b[0m; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.')); please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\nTraceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1637, in _decrypt_nsig\r\n    jsi, player_id, func_code = self._extract_n_function_code(video_id, player_url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1719, in _extract_n_function_code\r\n    func_name = self._extract_n_function_name(jscode)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1696, in _extract_n_function_name\r\n    return self._search_regex(\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/common.py\", line 1101, in _search_regex\r\n    raise RegexNotFoundError('Unable to extract %s' % _name)\r\nyoutube_dl.utils.RegexNotFoundError: Unable to extract Initial JS player n function name; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\nTraceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1637, in _decrypt_nsig\r\n    jsi, player_id, func_code = self._extract_n_function_code(video_id, player_url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1719, in _extract_n_function_code\r\n    func_name = self._extract_n_function_name(jscode)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1696, in _extract_n_function_name\r\n    return self._search_regex(\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/common.py\", line 1101, in _search_regex\r\n    raise RegexNotFoundError('Unable to extract %s' % _name)\r\nyoutube_dl.utils.RegexNotFoundError: Unable to extract Initial JS player n function name; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/YoutubeDL.py\", line 879, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/YoutubeDL.py\", line 975, in __extract_info\r\n    ie_result = ie.extract(url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/common.py\", line 571, in extract\r\n    ie_result = self._real_extract(url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 2137, in _real_extract\r\n    self._unthrottle_format_urls(video_id, player_url, dct)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1754, in _unthrottle_format_urls\r\n    n_response = decrypt_nsig(n_param)(n_param, video_id, player_url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1614, in inner\r\n    raise ret\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1606, in inner\r\n    self._player_cache[cache_id] = func(*args, **kwargs)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1639, in _decrypt_nsig\r\n    raise ExtractorError('Unable to extract nsig function code', cause=e)\r\nyoutube_dl.utils.ExtractorError: Unable to extract nsig function code (caused by RegexNotFoundError('Unable to extract \\x1b[0;34mInitial JS player n function name\\x1b[0m; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.')); please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n\r\n\r\n\r\n```\r\n\r\n\r\n## Description\r\n\r\n<!--\r\nProvide an explanation of your issue in an arbitrary form. Provide any additional information, suggested solution and as much context and examples as possible.\r\nIf work on your issue requires account credentials please provide them or explain how one can obtain them.\r\n-->\r\n\r\nWell, youtube-dl stopped working entirely now.\r\n\r\nI updated both regular and nightly to what the GitHub Repos have, which was basically no update at all since months, despite other unrelated Youtube issues not being fixed yet (like inability to bulk download shorts because it cant parse a channels shorts page, or inability to download anything but simple formats like format 18 unless you run a workaround, or inability to specify maximum video length to be downloaded).\r\n\r\nAnyways, looks like Youtube changed their webpage layout again since it's the regex that fails, meaning you cannot download ANYTHING now!\n", "hints_text": "This is the same issue as yt-dlp/yt-dlp#11744. I have a fix similar to the PR applied in _yt-dlp_ that will be pushed as soon as QA.\nI should mention again that youtube-dl no longer works at all whatsoever for me, this is not just something i can workaround anymore, because it cant even parse the page of a direct video link.\r\n\r\nThis Issue has forced me to look into why youtube-dlp was not a drop-in replacement for youtube-dl on my setup, and I eventually found out that the Config File was ignored and that was the Issue I had, meaning I have switched to youtube-dlp and rewritten my Scripts now, and can no longer report Issues here in the future.\nPlease raise the config issue separately since an incompatibility such as you mention is not meant to exist as far as I know.\nThis is the error im getting on a AlmaLinux server...Maybe this will help.  This did work like a week ago.\r\n\r\n[youtube] SvVS1_hWiZk: Downloading webpage\r\n[youtube] SvVS1_hWiZk: Downloading player 3bb1f723\r\nWARNING: [youtube] Falling back to generic n function search\r\nERROR: Unable to extract nsig function code (caused by RegexNotFoundError('Unable to extract \\x1b[0;34mInitial JS player n function name\\x1b[0m; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; see  https://github.com/ytdl-org/youtube-dl/#user-content-installation  on how to update. Be sure to call youtube-dl with the --verbose option and include the complete output.')); please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; see  https://github.com/ytdl-org/youtube-dl/#user-content-installation  on how to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\nDownloads from Python_videos.txt completed.\r\nDownloading videos from /home/jeff/Desktop/Youtube//Computers/Docker/Docker_videos.txt...\r\nUsage: youtube-dl [OPTIONS] URL [URL...]\r\n\nHi same problem but in my case on download meta data form youtube. \r\n\r\nexample in jenkins file download wideo and audio works.\r\n\r\n![Screenshot at Dec 10 07-58-46](https://github.com/user-attachments/assets/4ac2d54d-1c4b-4f9f-b074-2055380988f2)\r\n\r\n\r\n\r\n```bash\r\nstage('Download Video') {\r\n            steps {\r\n                timestamps {\r\n                    ansiColor('xterm') {\r\n                        sh'''\r\n                            #!/bin/bash\r\n                            set -e\r\n\r\n                            # Sprawdzenie obecno\u015bci youtube-dl\r\n                            if ! command -v youtube-dl >/dev/null 2>&1; \r\n                            then\r\n                                echo \"youtube-dl nie jest zainstalowane. Zainstaluj za pomoc\u0105: sudo apt install yt-dlp (lub odpowiednio skonfiguruj alias)\"\r\n                                exit 1\r\n                            fi\r\n\r\n                            # Sprawdzenie obecno\u015bci ffmpeg w dowolnej lokalizacji\r\n                            if ! command -v ffmpeg >/dev/null 2>&1; then\r\n                                echo \"ffmpeg nie jest zainstalowany. Zainstaluj za pomoc\u0105: sudo apt install ffmpeg\"\r\n                                exit 1\r\n                            fi\r\n\r\n                            echo \"GET ALL youtube-dl format video\"\r\n                            echo \" \"\r\n                            youtube-dl -F \"${YOUTUBE_VIDEO_URL}\"\r\n                            echo \" \"\r\n\r\n                            echo \"Start download best video: 4K, HD, SD\"\r\n                            video_url=\"${YOUTUBE_VIDEO_URL}\"\r\n\r\n                            # Lista preferowanych format\u00f3w (priorytet: 337, 315, 335, 299, 298)\r\n                            preferred_formats=\"337+140/315+140/335+140/299+140/298+140\"\r\n\r\n                            # Pobierz wideo w najlepszym dost\u0119pnym formacie\r\n                            echo \"Downloading best available format...\"\r\n                            youtube-dl -f \"$preferred_formats\" -o \"%(title)s.%(ext)s\" \"$video_url\"\r\n\r\n                            # Konwersja plik\u00f3w MKV na MP4, je\u015bli s\u0105 dost\u0119pne\r\n                            for i in *.mkv; do\r\n                                if [ -f \"$i\" ]; then\r\n                                    echo \"Converting $i to MP4...\"\r\n                                    ffmpeg -i \"$i\" -c copy \"${i%.*}.mp4\"\r\n                                    rm \"$i\"\r\n                                fi\r\n                            done\r\n\r\n                            echo \"Download and conversion completed.\"\r\n                        '''\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        stage('Download audio') {\r\n            steps {\r\n                timestamps {\r\n                    ansiColor('xterm') {\r\n                        sh'''\r\n                            set -e\r\n                            youtube-dl -f m4a -o \"%(title)s.%(ext)s\" ${YOUTUBE_VIDEO_URL}\r\n                        '''\r\n                    }\r\n                }\r\n            }\r\n        }\r\n```\r\n\r\nwhen i try download meta data from youtube this error show:\r\n\r\n```bash\r\nstage('Download video descryption and metadata') {\r\n            steps {\r\n                timestamps {\r\n                    ansiColor('xterm') {\r\n                        sh'''\r\n                            set -e\r\n                            youtube-dl --write-description --skip-download -o \"%(title)s.%(ext)s\" ${YOUTUBE_VIDEO_URL}\r\n                            title=$(youtube-dl --get-title --skip-download -o \"%(title)s.%(ext)s\" ${YOUTUBE_VIDEO_URL})\r\n                            echo \"${title}\" > ${title}.title\r\n                            youtube-dl --write-info-json --skip-download -o \"%(title)s.%(ext)s\" ${YOUTUBE_VIDEO_URL}\r\n                            python3 get_tags.py \"${YOUTUBE_VIDEO_URL}\"\r\n                        '''\r\n                    }  \r\n                }\r\n            }\r\n        }\r\n```\r\n![Screenshot at Dec 10 07-57-29](https://github.com/user-attachments/assets/a0017981-727c-402d-b0c5-ce6266f33403)\r\n\r\nError info:\r\n\r\n```bash\r\n19:34:12  [youtube] BlHcXfFINcM: Downloading web creator player API JSON\r\n19:34:12  [youtube] BlHcXfFINcM: Downloading m3u8 information\r\n19:34:13  [info] BlHcXfFINcM: Downloading 1 format(s): 337+251\r\n19:34:13  [info] Writing video metadata as JSON to: Prosty przepis na sa\u0142atk\u0119 z broku\u0142a i jajek.info.json\r\n19:34:13  + python3 get_tags.py https://youtu.be/BlHcXfFINcM\r\n19:34:[17](https://jenkins.koska.in/job/WORK/job/KULINARNEPRZYGODY/job/YOUTUBE_DOWNLOAD/job/DownloadVideo/149/pipeline-console/?start-byte=0&selected-node=67#log-17)  WARNING: [youtube] Falling back to generic n function search\r\n19:34:17  ERROR: Unable to extract nsig function code (caused by RegexNotFoundError('Unable to extract Initial JS player n function name; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; see  https://github.com/ytdl-org/youtube-dl/#user-content-installation  on how to update. Be sure to call youtube-dl with the --verbose option and include the complete output.')); please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; see  https://github.com/ytdl-org/youtube-dl/#user-content-installation  on how to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n19:34:18      return self._search_regex(\r\n19:34:18    File \"/usr/local/lib/python3.10/dist-packages/youtube_dl/extractor/common.py\", line 1101, in _search_regex\r\n19:34:18      raise RegexNotFoundError('Unable to extract %s' % _name)\r\n19:34:18  youtube_dl.utils.RegexNotFoundError: Unable to extract Initial JS player n function name; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; see  https://github.com/ytdl-org/youtube-dl/#user-content-installation  on how to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n19:34:18  \r\n19:[34](https://jenkins.koska.in/job/WORK/job/KULINARNEPRZYGODY/job/YOUTUBE_DOWNLOAD/job/DownloadVideo/149/pipeline-console/?start-byte=0&selected-node=67#log-34):18  During handling of the above exception, another exception occurred:\r\n19:34:18  \r\n19:34:18  Traceback (most recent call last):\r\n19:34:18    File \"/usr/local/lib/python3.10/dist-packages/youtube_dl/YoutubeDL.py\", line 875, in wrapper\r\n19:34:18      return func(self, *args, **kwargs)\r\n19:34:18    File \"/usr/local/lib/python3.10/dist-packages/youtube_dl/YoutubeDL.py\", line 971, in __extract_info\r\n19:34:18      ie_result = ie.extract(url)\r\n19:34:18    File \"/usr/local/lib/python3.10/dist-packages/youtube_dl/extractor/common.py\", line 571, in extract\r\n19:34:18      ie_result = self._real_extract(url)\r\n```\r\n\r\n\r\n\nIf yt-dlp [fixed](https://github.com/yt-dlp/yt-dlp/issues/11744#issuecomment-2521680838) this in `_extract_n_function_name()`, and ytdl has function with basically the same name and same function/purpose, then is it possible to adapt it to 'our' case?\r\n\n> To jest ten sam problem, co w przypadku [yt-dlp/yt-dlp#11744](https://github.com/yt-dlp/yt-dlp/issues/11744) . Mam poprawion\u0105 wersj\u0119 do PR _,_ kt\u00f3ra zostanie opublikowana po kontroli jako\u015bci.\r\n\r\nThis erroe still is in version https://github.com/yt-dlp/yt-dlp/releases/tag/2024.12.06\r\n\r\nI used this version, I only changed the name to match my jenkins pipeline (executable file)\r\n\r\nhttps://github.com/ytdl-org/youtube-dl/issues/32986#issuecomment-2530602819\r\n\r\n\n@TheRealMamuth If you have an issue with yt-dlp, please [open an issue there](https://github.com/yt-dlp/yt-dlp/issues/new/choose). youtube-dl and yt-dlp are different projects. You are the first person that reported the fix in yt-dlp 2024.12.06 not working.\n> @TheRealMamuth If you have an issue with yt-dlp, please [open an issue there](https://github.com/yt-dlp/yt-dlp/issues/new/choose). youtube-dl and yt-dlp are different projects. You are the first person that reported the fix in yt-dlp 2024.12.06 not working.\r\n\r\nthx - open issue yt-dlp https://github.com/yt-dlp/yt-dlp/issues/11781\nIt seems to be fixed in #32987. Tried that and it worked. One can test it [here](https://ufile.io/w28bg4to) (md5: e2e8b4a7cb7a40221b3b72003a43e5df), before it is released.\r\n\nAs [@seproDev kindly and accurately commented](https://github.com/yt-dlp/yt-dlp/issues/11744#issuecomment-2525082461), PR #32987 is somewhat misguided. Some fool maintainer relied on the implementation of JS string comparisons that some fool maintainer may have introduced in passing without matching tests, leading to a solution that apparently solved the issue but should not have. However, it is not unrecoverable.\r\n\r\nObviously anyone who wants to use the current PR code may do so but it could fail at any time; also, it will probably be force-pushed out by a better working solution.\r\n\r\nIn this new player, the challenge JS is testing the type of a variable that is set in a declaration outside the challenge JS, but that is still in scope. The (intended, I guess) effect is that the challenge JS returns the original nsig value if it doesn't know about the variable binding, and so 403 (nowadays) on download.\r\n\r\nThe _yt-dlp_ solution was to hard-code (a pattern matching) the guilty test and remove it from any challenge JS. This is effective and could be generalised to some extent, but seems unsatisfactory.\r\n\r\nAs we aren't going to be processing the whole player JS, some better hack is needed. Maybe there could be some way in which `typeof varName` in the challenge JS could search for `var varName = ...` in the whole player JS, but again there are endlessly many other ways in which the binding could have been created.\r\n\r\nA direct and also effective tactic can be to hook the evaluation of `return returnValue;` such that if `returnValue` is the original nsig value the statement behaves like `void returnValue;` instead, and the challenge keeps on running. Our interpreter doesn't know anything about nsig values, but the YT extractor can bind a magically named variable when calling the challenge code; then the interpreter can secretly look at that variable and not `return returnValue;` when `returnValue` matches the value of the magic variable. This is fine until the challenge starts raising an Exception (same technique can be applied) or mixing the value of the alien variable into the challenge calculation.\r\n\r\n\r\n\r\n \r\n\r\n ", "created_at": "2024-12-07T10:37:05Z"}
{"repo": "ytdl-org/youtube-dl", "pull_number": 32950, "instance_id": "ytdl-org__youtube-dl-32950", "issue_numbers": ["32735", "32896", "26218", "31509", "32629", "29343"], "base_commit": "f3cf09258447ee40dbab8f17dbfa0145f98b220b", "patch": "diff --git a/.github/ISSUE_TEMPLATE/config.yml b/.github/ISSUE_TEMPLATE/config.yml\nnew file mode 100644\nindex 00000000000..3ba13e0cec6\n--- /dev/null\n+++ b/.github/ISSUE_TEMPLATE/config.yml\n@@ -0,0 +1,1 @@\n+blank_issues_enabled: false\ndiff --git a/.github/workflows/ci.yml b/.github/workflows/ci.yml\nindex 90bd63c3262..d3b9ae01621 100644\n--- a/.github/workflows/ci.yml\n+++ b/.github/workflows/ci.yml\n@@ -1,81 +1,479 @@\n name: CI\n-on: [push, pull_request]\n+\n+env:\n+  all-cpython-versions: 2.6, 2.7, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 3.10, 3.11, 3.12\n+  main-cpython-versions: 2.7, 3.2, 3.5, 3.9, 3.11\n+  pypy-versions: pypy-2.7, pypy-3.6, pypy-3.7\n+  cpython-versions: main\n+  test-set: core\n+  # Python beta version to be built using pyenv before setup-python support\n+  # Must also be included in all-cpython-versions \n+  next: 3.13\n+\n+on:\n+  push:\n+    # push inputs aren't known to GitHub\n+    inputs:\n+      cpython-versions:\n+        type: string\n+        default: all\n+      test-set:\n+        type: string\n+        default: core\n+  pull_request:\n+    # pull_request inputs aren't known to GitHub\n+    inputs:\n+      cpython-versions:\n+        type: string\n+        default: main\n+      test-set:\n+        type: string\n+        default: both\n+  workflow_dispatch:\n+    inputs:\n+      cpython-versions:\n+        type: choice\n+        description: CPython versions (main = 2.7, 3.2, 3.5, 3.9, 3.11)\n+        options:\n+          - all\n+          - main\n+        required: true\n+        default: main\n+      test-set:\n+        type: choice\n+        description: core, download\n+        options:\n+          - both\n+          - core\n+          - download\n+        required: true\n+        default: both\n+\n+permissions:\n+  contents: read\n+\n jobs:\n+  select:\n+    name: Select tests from inputs\n+    runs-on: ubuntu-latest\n+    outputs:\n+      cpython-versions: ${{ steps.run.outputs.cpython-versions }}\n+      test-set: ${{ steps.run.outputs.test-set }}\n+      own-pip-versions: ${{ steps.run.outputs.own-pip-versions }}\n+    steps:\n+    # push and pull_request inputs aren't known to GitHub (pt3)\n+    - name: Set push defaults\n+      if: ${{ github.event_name == 'push' }}\n+      env:\n+        cpython-versions: all\n+        test-set: core\n+      run: |\n+        echo \"cpython-versions=${{env.cpython-versions}}\" >> \"$GITHUB_ENV\"\n+        echo \"test_set=${{env.test_set}}\" >> \"$GITHUB_ENV\"\n+    - name: Get pull_request inputs\n+      if: ${{ github.event_name == 'pull_request' }}\n+      env:\n+        cpython-versions: main\n+        test-set: both\n+      run: |\n+        echo \"cpython-versions=${{env.cpython-versions}}\" >> \"$GITHUB_ENV\"\n+        echo \"test_set=${{env.test_set}}\" >> \"$GITHUB_ENV\"\n+    - name: Make version array\n+      id: run\n+      run: |\n+        # Make a JSON Array from comma/space-separated string (no extra escaping)\n+        json_list() { \\\n+          ret=\"\"; IFS=\"${IFS},\"; set -- $*; \\\n+          for a in \"$@\"; do \\\n+            ret=$(printf '%s\"%s\"' \"${ret}${ret:+, }\" \"$a\"); \\\n+          done; \\\n+          printf '[%s]' \"$ret\"; }\n+        tests=\"${{ inputs.test-set || env.test-set }}\"\n+        [ $tests = both ] && tests=\"core download\"\n+        printf 'test-set=%s\\n' \"$(json_list $tests)\" >> \"$GITHUB_OUTPUT\"\n+        versions=\"${{ inputs.cpython-versions || env.cpython-versions }}\"\n+        if [ \"$versions\" = all ]; then \\\n+          versions=\"${{ env.all-cpython-versions }}\"; else \\\n+          versions=\"${{ env.main-cpython-versions }}\"; \\\n+        fi\n+        printf 'cpython-versions=%s\\n' \\\n+          \"$(json_list ${versions}${versions:+, }${{ env.pypy-versions }})\" >> \"$GITHUB_OUTPUT\"\n+        # versions with a special get-pip.py in a per-version subdirectory\n+        printf 'own-pip-versions=%s\\n' \\\n+          \"$(json_list 2.6, 2.7, 3.2, 3.3, 3.4, 3.5, 3.6)\" >> \"$GITHUB_OUTPUT\"\n+\n   tests:\n-    name: Tests\n+    name: Run tests\n+    needs: select\n+    permissions:\n+      contents: read\n+      packages: write\n     runs-on: ${{ matrix.os }}\n+    env:\n+      PIP: python -m pip\n+      PIP_DISABLE_PIP_VERSION_CHECK: true\n+      PIP_NO_PYTHON_VERSION_WARNING: true\n     strategy:\n       fail-fast: true\n       matrix:\n-        os: [ubuntu-18.04]\n-        # TODO: python 2.6\n-        python-version: [2.7, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, pypy-2.7, pypy-3.6, pypy-3.7]\n+        os: [ubuntu-20.04]\n+        python-version: ${{ fromJSON(needs.select.outputs.cpython-versions) }}\n         python-impl: [cpython]\n-        ytdl-test-set: [core, download]\n+        ytdl-test-set: ${{ fromJSON(needs.select.outputs.test-set) }}\n         run-tests-ext: [sh]\n         include:\n-        # python 3.2 is only available on windows via setup-python\n-        - os: windows-latest\n-          python-version: 3.2\n+        - os: windows-2019\n+          python-version: 3.4\n           python-impl: cpython\n-          ytdl-test-set: core\n+          ytdl-test-set: ${{ contains(needs.select.outputs.test-set, 'core') && 'core' || 'nocore' }}\n           run-tests-ext: bat\n-        - os: windows-latest\n-          python-version: 3.2\n+        - os: windows-2019\n+          python-version: 3.4\n           python-impl: cpython\n-          ytdl-test-set: download\n+          ytdl-test-set: ${{ contains(needs.select.outputs.test-set, 'download') && 'download'  || 'nodownload' }}\n           run-tests-ext: bat\n         # jython\n-        - os: ubuntu-18.04\n+        - os: ubuntu-20.04\n+          python-version: 2.7\n           python-impl: jython\n-          ytdl-test-set: core\n+          ytdl-test-set: ${{ contains(needs.select.outputs.test-set, 'core') && 'core' || 'nocore' }}\n           run-tests-ext: sh\n-        - os: ubuntu-18.04\n+        - os: ubuntu-20.04\n+          python-version: 2.7\n           python-impl: jython\n-          ytdl-test-set: download\n+          ytdl-test-set: ${{ contains(needs.select.outputs.test-set, 'download') && 'download'  || 'nodownload' }}\n           run-tests-ext: sh\n     steps:\n-    - uses: actions/checkout@v2\n-    - name: Set up Python ${{ matrix.python-version }}\n-      uses: actions/setup-python@v2\n-      if: ${{ matrix.python-impl == 'cpython' }}\n+    - name: Prepare Linux\n+      if: ${{ startswith(matrix.os, 'ubuntu') }}\n+      shell: bash\n+      run: |\n+        # apt in runner, if needed, may not be up-to-date\n+        sudo apt-get update\n+    - name: Checkout\n+      uses: actions/checkout@v3\n+    #-------- Python 3 -----\n+    - name: Set up supported Python ${{ matrix.python-version }}\n+      id: setup-python\n+      if: ${{ matrix.python-impl == 'cpython' && matrix.python-version != '2.6' && matrix.python-version != '2.7' && matrix.python-version != env.next }}\n+      # wrap broken actions/setup-python@v4\n+      # NB may run apt-get install in Linux\n+      uses: ytdl-org/setup-python@v1\n+      env:\n+        # Temporary workaround for Python 3.5 failures - May 2024\n+        PIP_TRUSTED_HOST: \"pypi.python.org pypi.org files.pythonhosted.org\"\n       with:\n         python-version: ${{ matrix.python-version }}\n+        cache-build: true\n+        allow-build: info\n+    - name: Locate supported Python ${{ matrix.python-version }}\n+      if: ${{ env.pythonLocation }}\n+      shell: bash\n+      run: |\n+        echo \"PYTHONHOME=${pythonLocation}\" >> \"$GITHUB_ENV\"\n+        export expected=\"${{ steps.setup-python.outputs.python-path }}\"\n+        dirname() { printf '%s\\n' \\\n+            'import os, sys' \\\n+            'print(os.path.dirname(sys.argv[1]))' \\\n+            | ${expected} - \"$1\"; }\n+        expd=\"$(dirname \"$expected\")\"\n+        export python=\"$(command -v python)\"\n+        [ \"$expd\" = \"$(dirname \"$python\")\" ] || echo \"PATH=$expd:${PATH}\" >> \"$GITHUB_ENV\"\n+        [ -x \"$python\" ] || printf '%s\\n' \\\n+            'import os' \\\n+            'exp = os.environ[\"expected\"]' \\\n+            'python = os.environ[\"python\"]' \\\n+            'exps = os.path.split(exp)' \\\n+            'if python and (os.path.dirname(python) == exp[0]):' \\\n+            '    exit(0)' \\\n+            'exps[1] = \"python\" + os.path.splitext(exps[1])[1]' \\\n+            'python = os.path.join(*exps)' \\\n+            'try:' \\\n+            '    os.symlink(exp, python)' \\\n+            'except AttributeError:' \\\n+            '    os.rename(exp, python)' \\\n+            | ${expected} -\n+        printf '%s\\n' \\\n+            'import sys' \\\n+            'print(sys.path)' \\\n+            | ${expected} -\n+    #-------- Python next (was 3.12) -\n+    - name: Set up CPython 3.next environment\n+      if: ${{ matrix.python-impl == 'cpython' && matrix.python-version == env.next }}\n+      shell: bash\n+      run: |\n+        PYENV_ROOT=$HOME/.local/share/pyenv\n+        echo \"PYENV_ROOT=${PYENV_ROOT}\" >> \"$GITHUB_ENV\"\n+    - name: Cache Python 3.next \n+      id: cachenext\n+      if: ${{ matrix.python-impl == 'cpython' && matrix.python-version == env.next }}\n+      uses: actions/cache@v3\n+      with:\n+        key: python-${{ env.next }}\n+        path: |\n+          ${{ env.PYENV_ROOT }}\n+    - name: Build and set up Python 3.next\n+      if: ${{ matrix.python-impl == 'cpython' && matrix.python-version == env.next && ! steps.cachenext.outputs.cache-hit }}\n+      # dl and build locally\n+      shell: bash\n+      run: |\n+        # Install build environment\n+        sudo apt-get install -y build-essential llvm libssl-dev tk-dev  \\\n+                      libncursesw5-dev libreadline-dev libsqlite3-dev   \\\n+                      libffi-dev xz-utils zlib1g-dev libbz2-dev liblzma-dev\n+        # Download PyEnv from its GitHub repository.\n+        export PYENV_ROOT=${{ env.PYENV_ROOT }}\n+        export PATH=$PYENV_ROOT/bin:$PATH\n+        git clone \"https://github.com/pyenv/pyenv.git\" \"$PYENV_ROOT\"\n+        pyenv install ${{ env.next }}\n+    - name: Locate Python 3.next\n+      if: ${{ matrix.python-impl == 'cpython' && matrix.python-version == env.next }}\n+      shell: bash\n+      run: |\n+        PYTHONHOME=\"$(echo \"${{ env.PYENV_ROOT }}/versions/${{ env.next }}.\"*)\"\n+        test -n \"$PYTHONHOME\"\n+        echo \"PYTHONHOME=$PYTHONHOME\" >> \"$GITHUB_ENV\"\n+        echo \"PATH=${PYTHONHOME}/bin:$PATH\" >> \"$GITHUB_ENV\"\n+    #-------- Python 2.7 --\n+    - name: Set up Python 2.7\n+      if: ${{ matrix.python-impl == 'cpython' && matrix.python-version == '2.7' }}\n+      # install 2.7\n+      shell: bash\n+      run: |\n+        sudo apt-get install -y python2 python-is-python2\n+        echo \"PYTHONHOME=/usr\" >> \"$GITHUB_ENV\"\n+    #-------- Python 2.6 --\n+    - name: Set up Python 2.6 environment\n+      if: ${{ matrix.python-impl == 'cpython' && matrix.python-version == '2.6' }}\n+      shell: bash\n+      run: |\n+        openssl_name=openssl-1.0.2u\n+        echo \"openssl_name=${openssl_name}\" >> \"$GITHUB_ENV\"\n+        openssl_dir=$HOME/.local/opt/$openssl_name\n+        echo \"openssl_dir=${openssl_dir}\" >> \"$GITHUB_ENV\"\n+        PYENV_ROOT=$HOME/.local/share/pyenv\n+        echo \"PYENV_ROOT=${PYENV_ROOT}\" >> \"$GITHUB_ENV\"\n+        sudo apt-get install -y openssl ca-certificates\n+    - name: Cache Python 2.6\n+      id: cache26\n+      if: ${{ matrix.python-version == '2.6' }}\n+      uses: actions/cache@v3\n+      with:\n+        key: python-2.6.9\n+        path: |\n+          ${{ env.openssl_dir }}\n+          ${{ env.PYENV_ROOT }}\n+    - name: Build and set up Python 2.6\n+      if: ${{ matrix.python-impl == 'cpython' && matrix.python-version == '2.6' && ! steps.cache26.outputs.cache-hit }}\n+      # dl and build locally\n+      shell: bash\n+      run: |\n+        # Install build environment\n+        sudo apt-get install -y build-essential llvm libssl-dev tk-dev  \\\n+                      libncursesw5-dev libreadline-dev libsqlite3-dev   \\\n+                      libffi-dev xz-utils zlib1g-dev libbz2-dev liblzma-dev\n+        # Download and install OpenSSL 1.0.2, back in time\n+        openssl_name=${{ env.openssl_name }}\n+        openssl_targz=${openssl_name}.tar.gz\n+        openssl_dir=${{ env.openssl_dir }}\n+        openssl_inc=$openssl_dir/include\n+        openssl_lib=$openssl_dir/lib\n+        openssl_ssl=$openssl_dir/ssl\n+        curl -L \"https://www.openssl.org/source/$openssl_targz\" -o $openssl_targz\n+        tar -xf $openssl_targz\n+        ( cd $openssl_name; \\\n+          ./config --prefix=$openssl_dir --openssldir=${openssl_dir}/ssl \\\n+            --libdir=lib -Wl,-rpath=${openssl_dir}/lib shared zlib-dynamic && \\\n+          make && \\\n+          make install )\n+        rm -rf $openssl_name\n+        rmdir $openssl_ssl/certs && ln -s /etc/ssl/certs $openssl_ssl/certs\n+        # Download PyEnv from its GitHub repository.\n+        export PYENV_ROOT=${{ env.PYENV_ROOT }}\n+        export PATH=$PYENV_ROOT/bin:$PATH\n+        git clone \"https://github.com/pyenv/pyenv.git\" \"$PYENV_ROOT\"\n+        # Prevent pyenv build trying (and failing) to update pip\n+        export GET_PIP=get-pip-2.6.py\n+        echo 'import sys; sys.exit(0)' > ${GET_PIP}\n+        GET_PIP=$(realpath $GET_PIP)\n+        # Build and install Python\n+        export CFLAGS=\"-I$openssl_inc\"\n+        export LDFLAGS=\"-L$openssl_lib\"\n+        export LD_LIBRARY_PATH=\"$openssl_lib\"\n+        pyenv install 2.6.9\n+    - name: Locate Python 2.6\n+      if: ${{ matrix.python-impl == 'cpython' && matrix.python-version == '2.6' }}\n+      shell: bash\n+      run: |\n+        PYTHONHOME=\"${{ env.PYENV_ROOT }}/versions/2.6.9\"\n+        echo \"PYTHONHOME=$PYTHONHOME\" >> \"$GITHUB_ENV\"\n+        echo \"PATH=${PYTHONHOME}/bin:$PATH\" >> \"$GITHUB_ENV\"\n+        echo \"LD_LIBRARY_PATH=${{ env.openssl_dir }}/lib${LD_LIBRARY_PATH:+:}${LD_LIBRARY_PATH}\" >> \"$GITHUB_ENV\"\n+    #-------- Jython ------\n     - name: Set up Java 8\n       if: ${{ matrix.python-impl == 'jython' }}\n-      uses: actions/setup-java@v1\n+      uses: actions/setup-java@v3\n       with:\n         java-version: 8\n-    - name: Install Jython\n+        distribution: 'zulu'\n+    - name: Setup Jython environment\n       if: ${{ matrix.python-impl == 'jython' }}\n+      shell: bash\n       run: |\n-        wget https://repo1.maven.org/maven2/org/python/jython-installer/2.7.1/jython-installer-2.7.1.jar -O jython-installer.jar\n-        java -jar jython-installer.jar -s -d \"$HOME/jython\"\n-        echo \"$HOME/jython/bin\" >> $GITHUB_PATH\n-    - name: Install nose\n-      if: ${{ matrix.python-impl != 'jython' }}\n-      run: pip install nose\n-    - name: Install nose (Jython)\n-      if: ${{ matrix.python-impl == 'jython' }}\n-      # Working around deprecation of support for non-SNI clients at PyPI CDN (see https://status.python.org/incidents/hzmjhqsdjqgb)\n+        echo \"JYTHON_ROOT=${HOME}/jython\" >> \"$GITHUB_ENV\"\n+        echo \"PIP=pip\" >> \"$GITHUB_ENV\"\n+    - name: Cache Jython\n+      id: cachejy\n+      if: ${{ matrix.python-impl == 'jython' && matrix.python-version == '2.7' }}\n+      uses: actions/cache@v3\n+      with:\n+        # 2.7.3 now available, may solve SNI issue\n+        key: jython-2.7.1\n+        path: |\n+          ${{ env.JYTHON_ROOT }}\n+    - name: Install Jython\n+      if: ${{ matrix.python-impl == 'jython' && matrix.python-version == '2.7' && ! steps.cachejy.outputs.cache-hit }}\n+      shell: bash\n+      run: |\n+        JYTHON_ROOT=\"${{ env.JYTHON_ROOT }}\"\n+        curl -L \"https://repo1.maven.org/maven2/org/python/jython-installer/2.7.1/jython-installer-2.7.1.jar\" -o jython-installer.jar\n+        java -jar jython-installer.jar -s -d \"${JYTHON_ROOT}\"\n+        echo \"${JYTHON_ROOT}/bin\" >> \"$GITHUB_PATH\"\n+    - name: Set up cached Jython\n+      if: ${{ steps.cachejy.outputs.cache-hit }}\n+      shell: bash\n+      run: |\n+        JYTHON_ROOT=\"${{ env.JYTHON_ROOT }}\"\n+        echo \"${JYTHON_ROOT}/bin\" >> $GITHUB_PATH\n+    - name: Install supporting Python 2.7 if possible\n+      if: ${{ steps.cachejy.outputs.cache-hit }}\n+      shell: bash\n+      run: |\n+        sudo apt-get install -y python2.7 || true\n+    #-------- pip ---------\n+    - name: Set up supported Python ${{ matrix.python-version }} pip\n+      if: ${{ (matrix.python-version != '3.2' && steps.setup-python.outputs.python-path) || matrix.python-version == '2.7' }}\n+      # This step may run in either Linux or Windows\n+      shell: bash\n       run: |\n-        wget https://files.pythonhosted.org/packages/99/4f/13fb671119e65c4dce97c60e67d3fd9e6f7f809f2b307e2611f4701205cb/nose-1.3.7-py2-none-any.whl\n-        pip install nose-1.3.7-py2-none-any.whl\n+        echo \"$PATH\"\n+        echo \"$PYTHONHOME\"\n+        # curl is available on both Windows and Linux, -L follows redirects, -O gets name\n+        python -m ensurepip || python -m pip --version || { \\\n+          get_pip=\"${{ contains(needs.select.outputs.own-pip-versions, matrix.python-version) && format('{0}/', matrix.python-version) || '' }}\"; \\\n+          curl -L -O \"https://bootstrap.pypa.io/pip/${get_pip}get-pip.py\"; \\\n+          python get-pip.py; }\n+    - name: Set up Python 2.6 pip\n+      if: ${{ matrix.python-version == '2.6' }}\n+      shell: bash\n+      run: |\n+        python -m pip --version || { \\\n+          curl -L -O \"https://bootstrap.pypa.io/pip/2.6/get-pip.py\"; \\\n+          curl -L -O \"https://files.pythonhosted.org/packages/ac/95/a05b56bb975efa78d3557efa36acaf9cf5d2fd0ee0062060493687432e03/pip-9.0.3-py2.py3-none-any.whl\"; \\\n+          python get-pip.py --no-setuptools --no-wheel pip-9.0.3-py2.py3-none-any.whl; }\n+        # work-around to invoke pip module on 2.6: https://bugs.python.org/issue2751\n+        echo \"PIP=python -m pip.__main__\" >> \"$GITHUB_ENV\"\n+    - name: Set up other Python ${{ matrix.python-version }} pip\n+      if: ${{ matrix.python-version == '3.2' && steps.setup-python.outputs.python-path }}\n+      shell: bash\n+      run: |\n+        python -m pip --version || { \\\n+          curl -L -O \"https://bootstrap.pypa.io/pip/3.2/get-pip.py\"; \\\n+          curl -L -O \"https://files.pythonhosted.org/packages/b2/d0/cd115fe345dd6f07ec1c780020a7dfe74966fceeb171e0f20d1d4905b0b7/pip-7.1.2-py2.py3-none-any.whl\"; \\\n+          python get-pip.py --no-setuptools --no-wheel pip-7.1.2-py2.py3-none-any.whl; }\n+    #-------- unittest ----\n+    - name: Upgrade Unittest for Python 2.6\n+      if: ${{ matrix.python-version == '2.6' }}\n+      shell: bash\n+      run: |\n+        # Work around deprecation of support for non-SNI clients at PyPI CDN (see https://status.python.org/incidents/hzmjhqsdjqgb)\n+        $PIP -qq show unittest2 || { \\\n+          for u in \"65/26/32b8464df2a97e6dd1b656ed26b2c194606c16fe163c695a992b36c11cdf/six-1.13.0-py2.py3-none-any.whl\" \\\n+              \"f2/94/3af39d34be01a24a6e65433d19e107099374224905f1e0cc6bbe1fd22a2f/argparse-1.4.0-py2.py3-none-any.whl\" \\\n+              \"c7/a3/c5da2a44c85bfbb6eebcfc1dde24933f8704441b98fdde6528f4831757a6/linecache2-1.0.0-py2.py3-none-any.whl\" \\\n+              \"17/0a/6ac05a3723017a967193456a2efa0aa9ac4b51456891af1e2353bb9de21e/traceback2-1.4.0-py2.py3-none-any.whl\" \\\n+              \"72/20/7f0f433060a962200b7272b8c12ba90ef5b903e218174301d0abfd523813/unittest2-1.1.0-py2.py3-none-any.whl\"; do \\\n+            curl -L -O \"https://files.pythonhosted.org/packages/${u}\"; \\\n+            $PIP install ${u##*/}; \\\n+          done; }\n+        # make tests use unittest2\n+        for test in ./test/test_*.py ./test/helper.py; do\n+          sed -r -i -e '/^import unittest$/s/test/test2 as unittest/' \"$test\"\n+        done\n+    #-------- nose --------\n+    - name: Install nose for Python ${{ matrix.python-version }}\n+      if: ${{ (matrix.python-version != '3.2' && steps.setup-python.outputs.python-path) || (matrix.python-impl == 'cpython' && (matrix.python-version == '2.7' || matrix.python-version == env.next)) }}\n+      shell: bash\n+      run: |\n+        echo \"$PATH\"\n+        echo \"$PYTHONHOME\"\n+        # Use PyNose for recent Pythons instead of Nose\n+        py3ver=\"${{ matrix.python-version }}\"\n+        py3ver=${py3ver#3.}\n+        [ \"$py3ver\" != \"${{ matrix.python-version }}\" ] && py3ver=${py3ver%.*} || py3ver=0\n+        [ \"$py3ver\" -ge 9 ] && nose=pynose || nose=nose\n+        $PIP -qq show $nose || $PIP install $nose\n+    - name: Install nose for other Python 2\n+      if: ${{ matrix.python-impl == 'jython' || (matrix.python-impl == 'cpython' && matrix.python-version == '2.6') }}\n+      shell: bash\n+      run: |\n+        # Work around deprecation of support for non-SNI clients at PyPI CDN (see https://status.python.org/incidents/hzmjhqsdjqgb)\n+        $PIP -qq show nose || { \\\n+          curl -L -O \"https://files.pythonhosted.org/packages/99/4f/13fb671119e65c4dce97c60e67d3fd9e6f7f809f2b307e2611f4701205cb/nose-1.3.7-py2-none-any.whl\"; \\\n+          $PIP install nose-1.3.7-py2-none-any.whl; }\n+    - name: Install nose for other Python 3\n+      if: ${{ matrix.python-version == '3.2' && steps.setup-python.outputs.python-path }}\n+      shell: bash\n+      run: |\n+        $PIP -qq show nose || { \\\n+          curl -L -O \"https://files.pythonhosted.org/packages/15/d8/dd071918c040f50fa1cf80da16423af51ff8ce4a0f2399b7bf8de45ac3d9/nose-1.3.7-py3-none-any.whl\"; \\\n+          $PIP install nose-1.3.7-py3-none-any.whl; }\n+    - name: Set up nosetest test\n+      if: ${{ contains(needs.select.outputs.test-set, matrix.ytdl-test-set ) }}\n+      shell: bash\n+      run: |\n+        # set PYTHON_VER\n+        PYTHON_VER=${{ matrix.python-version }}\n+        [ \"${PYTHON_VER#*-}\" != \"$PYTHON_VER\" ] || PYTHON_VER=\"${{ matrix.python-impl }}-${PYTHON_VER}\"\n+        echo \"PYTHON_VER=$PYTHON_VER\" >> \"$GITHUB_ENV\"\n+        echo \"PYTHON_IMPL=${{ matrix.python-impl }}\" >> \"$GITHUB_ENV\"\n+        # define a test to validate the Python version used by nosetests\n+        printf '%s\\n' \\\n+          'from __future__ import unicode_literals' \\\n+          'import sys, os, platform' \\\n+          'try:' \\\n+          '    import unittest2 as unittest' \\\n+          'except ImportError:' \\\n+          '    import unittest' \\\n+          'class TestPython(unittest.TestCase):' \\\n+          '    def setUp(self):' \\\n+          '        self.ver = os.environ[\"PYTHON_VER\"].split(\"-\")' \\\n+          '    def test_python_ver(self):' \\\n+          '        self.assertEqual([\"%d\" % v for v in sys.version_info[:2]], self.ver[-1].split(\".\")[:2])' \\\n+          '        self.assertTrue(sys.version.startswith(self.ver[-1]))' \\\n+          '        self.assertIn(self.ver[0], \",\".join((sys.version, platform.python_implementation())).lower())' \\\n+          '    def test_python_impl(self):' \\\n+          '        self.assertIn(platform.python_implementation().lower(), (os.environ[\"PYTHON_IMPL\"], self.ver[0]))' \\\n+          > test/test_python.py\n+    #-------- TESTS -------\n     - name: Run tests\n+      if: ${{ contains(needs.select.outputs.test-set, matrix.ytdl-test-set ) }}\n       continue-on-error: ${{ matrix.ytdl-test-set == 'download' || matrix.python-impl == 'jython' }}\n       env:\n         YTDL_TEST_SET: ${{ matrix.ytdl-test-set }}\n-      run: ./devscripts/run_tests.${{ matrix.run-tests-ext }}\n+      run: |\n+        ./devscripts/run_tests.${{ matrix.run-tests-ext }}\n   flake8:\n     name: Linter\n     runs-on: ubuntu-latest\n     steps:\n-    - uses: actions/checkout@v2\n+    - uses: actions/checkout@v3\n     - name: Set up Python\n-      uses: actions/setup-python@v2\n+      uses: actions/setup-python@v4\n       with:\n         python-version: 3.9\n     - name: Install flake8\n       run: pip install flake8\n     - name: Run flake8\n       run: flake8 .\n+\ndiff --git a/CONTRIBUTING.md b/CONTRIBUTING.md\nindex 58ab3a4b894..ff40cef784f 100644\n--- a/CONTRIBUTING.md\n+++ b/CONTRIBUTING.md\n@@ -150,7 +150,7 @@ After you have ensured this site is distributing its content legally, you can fo\n                 # TODO more properties (see youtube_dl/extractor/common.py)\n             }\n     ```\n-5. Add an import in [`youtube_dl/extractor/extractors.py`](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/extractors.py).\n+5. Add an import in [`youtube_dl/extractor/extractors.py`](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/extractors.py). This makes the extractor available for use, as long as the class ends with `IE`.\n 6. Run `python test/test_download.py TestDownload.test_YourExtractor`. This *should fail* at first, but you can continually re-run it until you're done. If you decide to add more than one test, then rename ``_TEST`` to ``_TESTS`` and make it into a list of dictionaries. The tests will then be named `TestDownload.test_YourExtractor`, `TestDownload.test_YourExtractor_1`, `TestDownload.test_YourExtractor_2`, etc. Note that tests with `only_matching` key in test's dict are not counted in.\n 7. Have a look at [`youtube_dl/extractor/common.py`](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/common.py) for possible helper methods and a [detailed description of what your extractor should and may return](https://github.com/ytdl-org/youtube-dl/blob/7f41a598b3fba1bcab2817de64a08941200aa3c8/youtube_dl/extractor/common.py#L94-L303). Add tests and code for as many as you want.\n 8. Make sure your code follows [youtube-dl coding conventions](#youtube-dl-coding-conventions) and check the code with [flake8](https://flake8.pycqa.org/en/latest/index.html#quickstart):\ndiff --git a/README.md b/README.md\nindex 2841ed68ff3..47e686f845c 100644\n--- a/README.md\n+++ b/README.md\n@@ -33,7 +33,7 @@ Windows users can [download an .exe file](https://yt-dl.org/latest/youtube-dl.ex\n You can also use pip:\n \n     sudo -H pip install --upgrade youtube-dl\n-    \n+\n This command will update youtube-dl if you have already installed it. See the [pypi page](https://pypi.python.org/pypi/youtube_dl) for more information.\n \n macOS users can install youtube-dl with [Homebrew](https://brew.sh/):\n@@ -563,7 +563,7 @@ The basic usage is not to set any template arguments when downloading a single f\n  - `is_live` (boolean): Whether this video is a live stream or a fixed-length video\n  - `start_time` (numeric): Time in seconds where the reproduction should start, as specified in the URL\n  - `end_time` (numeric): Time in seconds where the reproduction should end, as specified in the URL\n- - `format` (string): A human-readable description of the format \n+ - `format` (string): A human-readable description of the format\n  - `format_id` (string): Format code specified by `--format`\n  - `format_note` (string): Additional info about the format\n  - `width` (numeric): Width of the video\n@@ -632,7 +632,7 @@ To use percent literals in an output template use `%%`. To output to stdout use\n \n The current default template is `%(title)s-%(id)s.%(ext)s`.\n \n-In some cases, you don't want special characters such as \u4e2d, spaces, or &, such as when transferring the downloaded filename to a Windows system or the filename through an 8bit-unsafe channel. In these cases, add the `--restrict-filenames` flag to get a shorter title:\n+In some cases, you don't want special characters such as \u4e2d, spaces, or &, such as when transferring the downloaded filename to a Windows system or the filename through an 8bit-unsafe channel. In these cases, add the `--restrict-filenames` flag to get a shorter title.\n \n #### Output template and Windows batch files\n \n@@ -675,7 +675,7 @@ The general syntax for format selection is `--format FORMAT` or shorter `-f FORM\n \n **tl;dr:** [navigate me to examples](#format-selection-examples).\n \n-The simplest case is requesting a specific format, for example with `-f 22` you can download the format with format code equal to 22. You can get the list of available format codes for particular video using `--list-formats` or `-F`. Note that these format codes are extractor specific. \n+The simplest case is requesting a specific format, for example with `-f 22` you can download the format with format code equal to 22. You can get the list of available format codes for particular video using `--list-formats` or `-F`. Note that these format codes are extractor specific.\n \n You can also use a file extension (currently `3gp`, `aac`, `flv`, `m4a`, `mp3`, `mp4`, `ogg`, `wav`, `webm` are supported) to download the best quality format of a particular file extension served as a single file, e.g. `-f webm` will download the best quality format with the `webm` extension served as a single file.\n \n@@ -760,7 +760,7 @@ Videos can be filtered by their upload date using the options `--date`, `--dateb\n \n  - Absolute dates: Dates in the format `YYYYMMDD`.\n  - Relative dates: Dates in the format `(now|today)[+-][0-9](day|week|month|year)(s)?`\n- \n+\n Examples:\n \n ```bash\n@@ -918,7 +918,7 @@ Either prepend `https://www.youtube.com/watch?v=` or separate the ID from the op\n \n Use the `--cookies` option, for example `--cookies /path/to/cookies/file.txt`.\n \n-In order to extract cookies from browser use any conforming browser extension for exporting cookies. For example, [Get cookies.txt](https://chrome.google.com/webstore/detail/get-cookiestxt/bgaddhkoddajcdgocldbbfleckgcbcid/) (for Chrome) or [cookies.txt](https://addons.mozilla.org/en-US/firefox/addon/cookies-txt/) (for Firefox).\n+In order to extract cookies from browser use any conforming browser extension for exporting cookies. For example, [Get cookies.txt LOCALLY](https://chrome.google.com/webstore/detail/get-cookiestxt-locally/cclelndahbckbenkjhflpdbgdldlbecc) (for Chrome) or [cookies.txt](https://addons.mozilla.org/en-US/firefox/addon/cookies-txt/) (for Firefox).\n \n Note that the cookies file must be in Mozilla/Netscape format and the first line of the cookies file must be either `# HTTP Cookie File` or `# Netscape HTTP Cookie File`. Make sure you have correct [newline format](https://en.wikipedia.org/wiki/Newline) in the cookies file and convert newlines if necessary to correspond with your OS, namely `CRLF` (`\\r\\n`) for Windows and `LF` (`\\n`) for Unix and Unix-like systems (Linux, macOS, etc.). `HTTP Error 400: Bad Request` when using `--cookies` is a good sign of invalid newline format.\n \n@@ -1000,6 +1000,8 @@ To run the test, simply invoke your favorite test runner, or execute a test file\n     python test/test_download.py\n     nosetests\n \n+For Python versions 3.6 and later, you can use [pynose](https://pypi.org/project/pynose/) to implement `nosetests`. The original [nose](https://pypi.org/project/nose/) has not been upgraded for 3.10 and later.\n+\n See item 6 of [new extractor tutorial](#adding-support-for-a-new-site) for how to run extractor specific test cases.\n \n If you want to create a build of youtube-dl yourself, you'll need\n@@ -1069,9 +1071,11 @@ After you have ensured this site is distributing its content legally, you can fo\n             }\n     ```\n 5. Add an import in [`youtube_dl/extractor/extractors.py`](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/extractors.py).\n-6. Run `python test/test_download.py TestDownload.test_YourExtractor`. This *should fail* at first, but you can continually re-run it until you're done. If you decide to add more than one test, then rename ``_TEST`` to ``_TESTS`` and make it into a list of dictionaries. The tests will then be named `TestDownload.test_YourExtractor`, `TestDownload.test_YourExtractor_1`, `TestDownload.test_YourExtractor_2`, etc. Note that tests with `only_matching` key in test's dict are not counted in.\n-7. Have a look at [`youtube_dl/extractor/common.py`](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/common.py) for possible helper methods and a [detailed description of what your extractor should and may return](https://github.com/ytdl-org/youtube-dl/blob/7f41a598b3fba1bcab2817de64a08941200aa3c8/youtube_dl/extractor/common.py#L94-L303). Add tests and code for as many as you want.\n-8. Make sure your code follows [youtube-dl coding conventions](#youtube-dl-coding-conventions) and check the code with [flake8](https://flake8.pycqa.org/en/latest/index.html#quickstart):\n+6. Run `python test/test_download.py TestDownload.test_YourExtractor`. This *should fail* at first, but you can continually re-run it until you're done. If you decide to add more than one test (actually, test case) then rename ``_TEST`` to ``_TESTS`` and make it into a list of dictionaries. The tests will then be named `TestDownload.test_YourExtractor`, `TestDownload.test_YourExtractor_1`, `TestDownload.test_YourExtractor_2`, etc. Note:\n+    * the test names use the extractor class name **without the trailing `IE`**\n+    * tests with `only_matching` key in test's dict are not counted.\n+8. Have a look at [`youtube_dl/extractor/common.py`](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/common.py) for possible helper methods and a [detailed description of what your extractor should and may return](https://github.com/ytdl-org/youtube-dl/blob/7f41a598b3fba1bcab2817de64a08941200aa3c8/youtube_dl/extractor/common.py#L94-L303). Add tests and code for as many as you want.\n+9. Make sure your code follows [youtube-dl coding conventions](#youtube-dl-coding-conventions) and check the code with [flake8](https://flake8.pycqa.org/en/latest/index.html#quickstart):\n \n         $ flake8 youtube_dl/extractor/yourextractor.py\n \n@@ -1089,7 +1093,7 @@ In any case, thank you very much for your contributions!\n \n ## youtube-dl coding conventions\n \n-This section introduces a guide lines for writing idiomatic, robust and future-proof extractor code.\n+This section introduces guidelines for writing idiomatic, robust and future-proof extractor code.\n \n Extractors are very fragile by nature since they depend on the layout of the source data provided by 3rd party media hosters out of your control and this layout tends to change. As an extractor implementer your task is not only to write code that will extract media links and metadata correctly but also to minimize dependency on the source's layout and even to make the code foresee potential future changes and be ready for that. This is important because it will allow the extractor not to break on minor layout changes thus keeping old youtube-dl versions working. Even though this breakage issue is easily fixed by emitting a new version of youtube-dl with a fix incorporated, all the previous versions become broken in all repositories and distros' packages that may not be so prompt in fetching the update from us. Needless to say, some non rolling release distros may never receive an update at all.\n \n@@ -1112,7 +1116,7 @@ Say you have some source dictionary `meta` that you've fetched as JSON with HTTP\n ```python\n meta = self._download_json(url, video_id)\n ```\n-    \n+\n Assume at this point `meta`'s layout is:\n \n ```python\n@@ -1156,7 +1160,7 @@ description = self._search_regex(\n ```\n \n On failure this code will silently continue the extraction with `description` set to `None`. That is useful for metafields that may or may not be present.\n- \n+\n ### Provide fallbacks\n \n When extracting metadata try to do so from multiple sources. For example if `title` is present in several places, try extracting from at least some of them. This makes it more future-proof in case some of the sources become unavailable.\n@@ -1204,7 +1208,7 @@ r'(id|ID)=(?P<id>\\d+)'\n #### Make regular expressions relaxed and flexible\n \n When using regular expressions try to write them fuzzy, relaxed and flexible, skipping insignificant parts that are more likely to change, allowing both single and double quotes for quoted values and so on.\n- \n+\n ##### Example\n \n Say you need to extract `title` from the following HTML code:\n@@ -1228,7 +1232,7 @@ title = self._search_regex(\n     webpage, 'title', group='title')\n ```\n \n-Note how you tolerate potential changes in the `style` attribute's value or switch from using double quotes to single for `class` attribute: \n+Note how you tolerate potential changes in the `style` attribute's value or switch from using double quotes to single for `class` attribute:\n \n The code definitely should not look like:\n \n@@ -1329,27 +1333,114 @@ Wrap all extracted numeric data into safe functions from [`youtube_dl/utils.py`]\n \n Use `url_or_none` for safe URL processing.\n \n-Use `try_get` for safe metadata extraction from parsed JSON.\n+Use `traverse_obj` for safe metadata extraction from parsed JSON.\n \n-Use `unified_strdate` for uniform `upload_date` or any `YYYYMMDD` meta field extraction, `unified_timestamp` for uniform `timestamp` extraction, `parse_filesize` for `filesize` extraction, `parse_count` for count meta fields extraction, `parse_resolution`, `parse_duration` for `duration` extraction, `parse_age_limit` for `age_limit` extraction. \n+Use `unified_strdate` for uniform `upload_date` or any `YYYYMMDD` meta field extraction, `unified_timestamp` for uniform `timestamp` extraction, `parse_filesize` for `filesize` extraction, `parse_count` for count meta fields extraction, `parse_resolution`, `parse_duration` for `duration` extraction, `parse_age_limit` for `age_limit` extraction.\n \n Explore [`youtube_dl/utils.py`](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/utils.py) for more useful convenience functions.\n \n #### More examples\n \n ##### Safely extract optional description from parsed JSON\n+\n+When processing complex JSON, as often returned by site API requests or stashed in web pages for \"hydration\", you can use the `traverse_obj()` utility function to handle multiple fallback values and to ensure the expected type of metadata items. The function's docstring defines how the function works: also review usage in the codebase for more examples.\n+\n+In this example, a text `description`, or `None`, is pulled from the `.result.video[0].summary` member of the parsed JSON `response`, if available.\n+\n+```python\n+description = traverse_obj(response, ('result', 'video', 0, 'summary', T(compat_str)))\n+```\n+`T(...)` is a shorthand for a set literal; if you hate people who still run Python 2.6, `T(type_or_transformation)` could be written as a set literal `{type_or_transformation}`.\n+\n+Some extractors use the older and less capable `try_get()` function in the same way.\n+\n ```python\n description = try_get(response, lambda x: x['result']['video'][0]['summary'], compat_str)\n ```\n \n ##### Safely extract more optional metadata\n+\n+In this example, various optional metadata values are extracted from the `.result.video[0]` member of the parsed JSON `response`, which is expected to be a JS object, parsed into a `dict`, with no crash if that isn't so, or if any of the target values are missing or invalid.\n+\n ```python\n-video = try_get(response, lambda x: x['result']['video'][0], dict) or {}\n+video = traverse_obj(response, ('result', 'video', 0, T(dict))) or {}\n+# formerly:\n+# video = try_get(response, lambda x: x['result']['video'][0], dict) or {}\n description = video.get('summary')\n duration = float_or_none(video.get('durationMs'), scale=1000)\n view_count = int_or_none(video.get('views'))\n ```\n \n+#### Safely extract nested lists\n+\n+Suppose you've extracted JSON like this into a Python data structure named `media_json` using, say, the `_download_json()` or `_parse_json()` methods of `InfoExtractor`:\n+```json\n+{\n+    \"title\": \"Example video\",\n+    \"comment\": \"try extracting this\",\n+    \"media\": [{\n+        \"type\": \"bad\",\n+        \"size\": 320,\n+        \"url\": \"https://some.cdn.site/bad.mp4\"\n+    }, {\n+        \"type\": \"streaming\",\n+        \"url\": \"https://some.cdn.site/hls.m3u8\"\n+    }, {\n+        \"type\": \"super\",\n+        \"size\": 1280,\n+        \"url\": \"https://some.cdn.site/good.webm\"\n+    }],\n+    \"moreStuff\": \"more values\",\n+    ...\n+}\n+```\n+\n+Then extractor code like this can collect the various fields of the JSON:\n+```python\n+...\n+from ..utils import (\n+    determine_ext,\n+    int_or_none,\n+    T,\n+    traverse_obj,\n+    txt_or_none,\n+    url_or_none,\n+)\n+...\n+        ...\n+        info_dict = {}\n+        # extract title and description if valid and not empty\n+        info_dict.update(traverse_obj(media_json, {\n+            'title': ('title', T(txt_or_none)),\n+            'description': ('comment', T(txt_or_none)),\n+        }))\n+\n+        # extract any recognisable media formats\n+        fmts = []\n+        # traverse into \"media\" list, extract `dict`s with desired keys\n+        for fmt in traverse_obj(media_json, ('media', Ellipsis, {\n+                'format_id': ('type', T(txt_or_none)),\n+                'url': ('url', T(url_or_none)),\n+                'width': ('size', T(int_or_none)), })):\n+            # bad `fmt` values were `None` and removed\n+            if 'url' not in fmt:\n+                continue\n+            fmt_url = fmt['url']  # known to be valid URL\n+            ext = determine_ext(fmt_url)\n+            if ext == 'm3u8':\n+                fmts.extend(self._extract_m3u8_formats(fmt_url, video_id, 'mp4', fatal=False))\n+            else:\n+                fmt['ext'] = ext\n+                fmts.append(fmt)\n+\n+        # sort, raise if no formats\n+        self._sort_formats(fmts)\n+\n+        info_dict['formats'] = fmts\n+        ...\n+```\n+The extractor raises an exception rather than random crashes if the JSON structure changes so that no formats are found.\n+\n # EMBEDDING YOUTUBE-DL\n \n youtube-dl makes the best effort to be a good command-line program, and thus should be callable from any programming language. If you encounter any problems parsing its output, feel free to [create a report](https://github.com/ytdl-org/youtube-dl/issues/new).\n@@ -1406,7 +1497,11 @@ with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n \n # BUGS\n \n-Bugs and suggestions should be reported at: <https://github.com/ytdl-org/youtube-dl/issues>. Unless you were prompted to or there is another pertinent reason (e.g. GitHub fails to accept the bug report), please do not send bug reports via personal email. For discussions, join us in the IRC channel [#youtube-dl](irc://chat.freenode.net/#youtube-dl) on freenode ([webchat](https://webchat.freenode.net/?randomnick=1&channels=youtube-dl)).\n+Bugs and suggestions should be reported in the issue tracker: <https://github.com/ytdl-org/youtube-dl/issues> (<https://yt-dl.org/bug> is an alias for this). Unless you were prompted to or there is another pertinent reason (e.g. GitHub fails to accept the bug report), please do not send bug reports via personal email. For discussions, join us in the IRC channel [#youtube-dl](irc://chat.freenode.net/#youtube-dl) on freenode ([webchat](https://webchat.freenode.net/?randomnick=1&channels=youtube-dl)).\n+\n+## Opening a bug report or suggestion\n+\n+Be sure to follow instructions provided **below** and **in the issue tracker**. Complete the appropriate issue template fully. Consider whether your problem is covered by an existing issue: if so, follow the discussion there. Avoid commenting on existing duplicate issues as such comments do not add to the discussion of the issue and are liable to be treated as spam.\n \n **Please include the full output of youtube-dl when run with `-v`**, i.e. **add** `-v` flag to **your command line**, copy the **whole** output and post it in the issue body wrapped in \\`\\`\\` for better formatting. It should look similar to this:\n ```\n@@ -1426,17 +1521,17 @@ $ youtube-dl -v <your command line>\n \n The output (including the first lines) contains important debugging information. Issues without the full output are often not reproducible and therefore do not get solved in short order, if ever.\n \n-Please re-read your issue once again to avoid a couple of common mistakes (you can and should use this as a checklist):\n+Finally please review your issue to avoid various common mistakes (you can and should use this as a checklist) listed below.\n \n ### Is the description of the issue itself sufficient?\n \n-We often get issue reports that we cannot really decipher. While in most cases we eventually get the required information after asking back multiple times, this poses an unnecessary drain on our resources. Many contributors, including myself, are also not native speakers, so we may misread some parts.\n+We often get issue reports that are hard to understand. To avoid subsequent clarifications, and to assist participants who are not native English speakers, please elaborate on what feature you are requesting, or what bug you want to be fixed.\n \n-So please elaborate on what feature you are requesting, or what bug you want to be fixed. Make sure that it's obvious\n+Make sure that it's obvious\n \n - What the problem is\n - How it could be fixed\n-- How your proposed solution would look like\n+- How your proposed solution would look\n \n If your report is shorter than two lines, it is almost certainly missing some of these, which makes it hard for us to respond to it. We're often too polite to close the issue outright, but the missing info makes misinterpretation likely. As a committer myself, I often get frustrated by these issues, since the only possible way for me to move forward on them is to ask for clarification over and over.\n \n@@ -1446,13 +1541,13 @@ If your server has multiple IPs or you suspect censorship, adding `--call-home`\n \n **Site support requests must contain an example URL**. An example URL is a URL you might want to download, like `https://www.youtube.com/watch?v=BaW_jenozKc`. There should be an obvious video present. Except under very special circumstances, the main page of a video service (e.g. `https://www.youtube.com/`) is *not* an example URL.\n \n-###  Are you using the latest version?\n+###  Is the issue already documented?\n \n-Before reporting any issue, type `youtube-dl -U`. This should report that you're up-to-date. About 20% of the reports we receive are already fixed, but people are using outdated versions. This goes for feature requests as well.\n+Make sure that someone has not already opened the issue you're trying to open. Search at the top of the window or browse the [GitHub Issues](https://github.com/ytdl-org/youtube-dl/search?type=Issues) of this repository. Initially, at least, use the search term `-label:duplicate` to focus on active issues. If there is an issue, feel free to write something along the lines of \"This affects me as well, with version 2015.01.01. Here is some more information on the issue: ...\". While some issues may be old, a new post into them often spurs rapid activity.\n \n-###  Is the issue already documented?\n+###  Are you using the latest version?\n \n-Make sure that someone has not already opened the issue you're trying to open. Search at the top of the window or browse the [GitHub Issues](https://github.com/ytdl-org/youtube-dl/search?type=Issues) of this repository. If there is an issue, feel free to write something along the lines of \"This affects me as well, with version 2015.01.01. Here is some more information on the issue: ...\". While some issues may be old, a new post into them often spurs rapid activity.\n+Before reporting any issue, type `youtube-dl -U`. This should report that you're up-to-date. About 20% of the reports we receive are already fixed, but people are using outdated versions. This goes for feature requests as well.\n \n ###  Why are existing options not enough?\n \ndiff --git a/devscripts/__init__.py b/devscripts/__init__.py\nnew file mode 100644\nindex 00000000000..750dbdca786\n--- /dev/null\n+++ b/devscripts/__init__.py\n@@ -0,0 +1,1 @@\n+# Empty file needed to make devscripts.utils properly importable from outside\ndiff --git a/devscripts/bash-completion.py b/devscripts/bash-completion.py\nindex 3d1391334bd..7db396a77fb 100755\n--- a/devscripts/bash-completion.py\n+++ b/devscripts/bash-completion.py\n@@ -5,8 +5,12 @@\n from os.path import dirname as dirn\n import sys\n \n-sys.path.insert(0, dirn(dirn((os.path.abspath(__file__)))))\n+sys.path.insert(0, dirn(dirn(os.path.abspath(__file__))))\n+\n import youtube_dl\n+from youtube_dl.compat import compat_open as open\n+\n+from utils import read_file\n \n BASH_COMPLETION_FILE = \"youtube-dl.bash-completion\"\n BASH_COMPLETION_TEMPLATE = \"devscripts/bash-completion.in\"\n@@ -18,9 +22,8 @@ def build_completion(opt_parser):\n         for option in group.option_list:\n             # for every long flag\n             opts_flag.append(option.get_opt_string())\n-    with open(BASH_COMPLETION_TEMPLATE) as f:\n-        template = f.read()\n-    with open(BASH_COMPLETION_FILE, \"w\") as f:\n+    template = read_file(BASH_COMPLETION_TEMPLATE)\n+    with open(BASH_COMPLETION_FILE, \"w\", encoding='utf-8') as f:\n         # just using the special char\n         filled_template = template.replace(\"{{flags}}\", \" \".join(opts_flag))\n         f.write(filled_template)\ndiff --git a/devscripts/cli_to_api.py b/devscripts/cli_to_api.py\nnew file mode 100755\nindex 00000000000..9fb1d2ba843\n--- /dev/null\n+++ b/devscripts/cli_to_api.py\n@@ -0,0 +1,83 @@\n+#!/usr/bin/env python\n+# coding: utf-8\n+\n+from __future__ import unicode_literals\n+\n+\"\"\"\n+This script displays the API parameters corresponding to a yt-dl command line\n+\n+Example:\n+$ ./cli_to_api.py -f best\n+{u'format': 'best'}\n+$\n+\"\"\"\n+\n+# Allow direct execution\n+import os\n+import sys\n+sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n+\n+import youtube_dl\n+from types import MethodType\n+\n+\n+def cli_to_api(*opts):\n+    YDL = youtube_dl.YoutubeDL\n+\n+    # to extract the parsed options, break out of YoutubeDL instantiation\n+\n+    # return options via this Exception\n+    class ParseYTDLResult(Exception):\n+        def __init__(self, result):\n+            super(ParseYTDLResult, self).__init__('result')\n+            self.opts = result\n+\n+    # replacement constructor that raises ParseYTDLResult\n+    def ytdl_init(ydl, ydl_opts):\n+        super(YDL, ydl).__init__(ydl_opts)\n+        raise ParseYTDLResult(ydl_opts)\n+\n+    # patch in the constructor\n+    YDL.__init__ = MethodType(ytdl_init, YDL)\n+\n+    # core parser\n+    def parsed_options(argv):\n+        try:\n+            youtube_dl._real_main(list(argv))\n+        except ParseYTDLResult as result:\n+            return result.opts\n+\n+    # from https://github.com/yt-dlp/yt-dlp/issues/5859#issuecomment-1363938900\n+    default = parsed_options([])\n+\n+    def neq_opt(a, b):\n+        if a == b:\n+            return False\n+        if a is None and repr(type(object)).endswith(\".utils.DateRange'>\"):\n+            return '0001-01-01 - 9999-12-31' != '{0}'.format(b)\n+        return a != b\n+\n+    diff = dict((k, v) for k, v in parsed_options(opts).items() if neq_opt(default[k], v))\n+    if 'postprocessors' in diff:\n+        diff['postprocessors'] = [pp for pp in diff['postprocessors'] if pp not in default['postprocessors']]\n+    return diff\n+\n+\n+def main():\n+    from pprint import PrettyPrinter\n+\n+    pprint = PrettyPrinter()\n+    super_format = pprint.format\n+\n+    def format(object, context, maxlevels, level):\n+        if repr(type(object)).endswith(\".utils.DateRange'>\"):\n+            return '{0}: {1}>'.format(repr(object)[:-2], object), True, False\n+        return super_format(object, context, maxlevels, level)\n+\n+    pprint.format = format\n+\n+    pprint.pprint(cli_to_api(*sys.argv))\n+\n+\n+if __name__ == '__main__':\n+    main()\ndiff --git a/devscripts/create-github-release.py b/devscripts/create-github-release.py\nindex 2ddfa109698..320bcfc2792 100644\n--- a/devscripts/create-github-release.py\n+++ b/devscripts/create-github-release.py\n@@ -1,7 +1,6 @@\n #!/usr/bin/env python\n from __future__ import unicode_literals\n \n-import io\n import json\n import mimetypes\n import netrc\n@@ -10,7 +9,9 @@\n import re\n import sys\n \n-sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n+dirn = os.path.dirname\n+\n+sys.path.insert(0, dirn(dirn(os.path.abspath(__file__))))\n \n from youtube_dl.compat import (\n     compat_basestring,\n@@ -22,6 +23,7 @@\n     make_HTTPS_handler,\n     sanitized_Request,\n )\n+from utils import read_file\n \n \n class GitHubReleaser(object):\n@@ -89,8 +91,7 @@ def main():\n \n     changelog_file, version, build_path = args\n \n-    with io.open(changelog_file, encoding='utf-8') as inf:\n-        changelog = inf.read()\n+    changelog = read_file(changelog_file)\n \n     mobj = re.search(r'(?s)version %s\\n{2}(.+?)\\n{3}' % version, changelog)\n     body = mobj.group(1) if mobj else ''\ndiff --git a/devscripts/fish-completion.py b/devscripts/fish-completion.py\nindex 51d19dd33d3..ef8a39e0bcd 100755\n--- a/devscripts/fish-completion.py\n+++ b/devscripts/fish-completion.py\n@@ -6,10 +6,13 @@\n from os.path import dirname as dirn\n import sys\n \n-sys.path.insert(0, dirn(dirn((os.path.abspath(__file__)))))\n+sys.path.insert(0, dirn(dirn(os.path.abspath(__file__))))\n+\n import youtube_dl\n from youtube_dl.utils import shell_quote\n \n+from utils import read_file, write_file\n+\n FISH_COMPLETION_FILE = 'youtube-dl.fish'\n FISH_COMPLETION_TEMPLATE = 'devscripts/fish-completion.in'\n \n@@ -38,11 +41,9 @@ def build_completion(opt_parser):\n             complete_cmd.extend(EXTRA_ARGS.get(long_option, []))\n             commands.append(shell_quote(complete_cmd))\n \n-    with open(FISH_COMPLETION_TEMPLATE) as f:\n-        template = f.read()\n+    template = read_file(FISH_COMPLETION_TEMPLATE)\n     filled_template = template.replace('{{commands}}', '\\n'.join(commands))\n-    with open(FISH_COMPLETION_FILE, 'w') as f:\n-        f.write(filled_template)\n+    write_file(FISH_COMPLETION_FILE, filled_template)\n \n \n parser = youtube_dl.parseOpts()[0]\ndiff --git a/devscripts/gh-pages/add-version.py b/devscripts/gh-pages/add-version.py\nindex 867ea0048fb..b84908f8573 100755\n--- a/devscripts/gh-pages/add-version.py\n+++ b/devscripts/gh-pages/add-version.py\n@@ -6,16 +6,21 @@\n import hashlib\n import os.path\n \n+dirn = os.path.dirname\n+\n+sys.path.insert(0, dirn(dirn(dirn(os.path.abspath(__file__)))))\n+\n+from devscripts.utils import read_file, write_file\n+from youtube_dl.compat import compat_open as open\n \n if len(sys.argv) <= 1:\n     print('Specify the version number as parameter')\n     sys.exit()\n version = sys.argv[1]\n \n-with open('update/LATEST_VERSION', 'w') as f:\n-    f.write(version)\n+write_file('update/LATEST_VERSION', version)\n \n-versions_info = json.load(open('update/versions.json'))\n+versions_info = json.loads(read_file('update/versions.json'))\n if 'signature' in versions_info:\n     del versions_info['signature']\n \n@@ -39,5 +44,5 @@\n versions_info['versions'][version] = new_version\n versions_info['latest'] = version\n \n-with open('update/versions.json', 'w') as jsonf:\n-    json.dump(versions_info, jsonf, indent=4, sort_keys=True)\n+with open('update/versions.json', 'w', encoding='utf-8') as jsonf:\n+    json.dumps(versions_info, jsonf, indent=4, sort_keys=True)\ndiff --git a/devscripts/gh-pages/generate-download.py b/devscripts/gh-pages/generate-download.py\nindex a873d32ee43..3e38e929951 100755\n--- a/devscripts/gh-pages/generate-download.py\n+++ b/devscripts/gh-pages/generate-download.py\n@@ -2,14 +2,21 @@\n from __future__ import unicode_literals\n \n import json\n+import os.path\n+import sys\n \n-versions_info = json.load(open('update/versions.json'))\n+dirn = os.path.dirname\n+\n+sys.path.insert(0, dirn(dirn((os.path.abspath(__file__)))))\n+\n+from utils import read_file, write_file\n+\n+versions_info = json.loads(read_file('update/versions.json'))\n version = versions_info['latest']\n version_dict = versions_info['versions'][version]\n \n # Read template page\n-with open('download.html.in', 'r', encoding='utf-8') as tmplf:\n-    template = tmplf.read()\n+template = read_file('download.html.in')\n \n template = template.replace('@PROGRAM_VERSION@', version)\n template = template.replace('@PROGRAM_URL@', version_dict['bin'][0])\n@@ -18,5 +25,5 @@\n template = template.replace('@EXE_SHA256SUM@', version_dict['exe'][1])\n template = template.replace('@TAR_URL@', version_dict['tar'][0])\n template = template.replace('@TAR_SHA256SUM@', version_dict['tar'][1])\n-with open('download.html', 'w', encoding='utf-8') as dlf:\n-    dlf.write(template)\n+\n+write_file('download.html', template)\ndiff --git a/devscripts/gh-pages/update-copyright.py b/devscripts/gh-pages/update-copyright.py\nindex 61487f92588..444595c48d9 100755\n--- a/devscripts/gh-pages/update-copyright.py\n+++ b/devscripts/gh-pages/update-copyright.py\n@@ -5,17 +5,22 @@\n \n import datetime\n import glob\n-import io  # For Python 2 compatibility\n import os\n import re\n+import sys\n \n-year = str(datetime.datetime.now().year)\n+dirn = os.path.dirname\n+\n+sys.path.insert(0, dirn(dirn(dirn(os.path.abspath(__file__)))))\n+\n+from devscripts.utils import read_file, write_file\n+from youtube_dl import compat_str\n+\n+year = compat_str(datetime.datetime.now().year)\n for fn in glob.glob('*.html*'):\n-    with io.open(fn, encoding='utf-8') as f:\n-        content = f.read()\n+    content = read_file(fn)\n     newc = re.sub(r'(?P<copyright>Copyright \u00a9 2011-)(?P<year>[0-9]{4})', 'Copyright \u00a9 2011-' + year, content)\n     if content != newc:\n         tmpFn = fn + '.part'\n-        with io.open(tmpFn, 'wt', encoding='utf-8') as outf:\n-            outf.write(newc)\n+        write_file(tmpFn, newc)\n         os.rename(tmpFn, fn)\ndiff --git a/devscripts/gh-pages/update-feed.py b/devscripts/gh-pages/update-feed.py\nindex 506a623772e..13a367d3433 100755\n--- a/devscripts/gh-pages/update-feed.py\n+++ b/devscripts/gh-pages/update-feed.py\n@@ -2,10 +2,16 @@\n from __future__ import unicode_literals\n \n import datetime\n-import io\n import json\n+import os.path\n import textwrap\n+import sys\n \n+dirn = os.path.dirname\n+\n+sys.path.insert(0, dirn(dirn(os.path.abspath(__file__))))\n+\n+from utils import write_file\n \n atom_template = textwrap.dedent(\"\"\"\\\n     <?xml version=\"1.0\" encoding=\"utf-8\"?>\n@@ -72,5 +78,4 @@\n entries_str = textwrap.indent(''.join(entries), '\\t')\n atom_template = atom_template.replace('@ENTRIES@', entries_str)\n \n-with io.open('update/releases.atom', 'w', encoding='utf-8') as atom_file:\n-    atom_file.write(atom_template)\n+write_file('update/releases.atom', atom_template)\ndiff --git a/devscripts/gh-pages/update-sites.py b/devscripts/gh-pages/update-sites.py\nindex 531c93c7089..06a8a474cb1 100755\n--- a/devscripts/gh-pages/update-sites.py\n+++ b/devscripts/gh-pages/update-sites.py\n@@ -5,15 +5,17 @@\n import os\n import textwrap\n \n+dirn = os.path.dirname\n+\n # We must be able to import youtube_dl\n-sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n+sys.path.insert(0, dirn(dirn(dirn(os.path.abspath(__file__)))))\n \n import youtube_dl\n+from devscripts.utils import read_file, write_file\n \n \n def main():\n-    with open('supportedsites.html.in', 'r', encoding='utf-8') as tmplf:\n-        template = tmplf.read()\n+    template = read_file('supportedsites.html.in')\n \n     ie_htmls = []\n     for ie in youtube_dl.list_extractors(age_limit=None):\n@@ -29,8 +31,7 @@ def main():\n \n     template = template.replace('@SITES@', textwrap.indent('\\n'.join(ie_htmls), '\\t'))\n \n-    with open('supportedsites.html', 'w', encoding='utf-8') as sitesf:\n-        sitesf.write(template)\n+    write_file('supportedsites.html', template)\n \n \n if __name__ == '__main__':\ndiff --git a/devscripts/make_contributing.py b/devscripts/make_contributing.py\nindex 226d1a5d664..5a9eb194f87 100755\n--- a/devscripts/make_contributing.py\n+++ b/devscripts/make_contributing.py\n@@ -1,10 +1,11 @@\n #!/usr/bin/env python\n from __future__ import unicode_literals\n \n-import io\n import optparse\n import re\n \n+from utils import read_file, write_file\n+\n \n def main():\n     parser = optparse.OptionParser(usage='%prog INFILE OUTFILE')\n@@ -14,8 +15,7 @@ def main():\n \n     infile, outfile = args\n \n-    with io.open(infile, encoding='utf-8') as inf:\n-        readme = inf.read()\n+    readme = read_file(infile)\n \n     bug_text = re.search(\n         r'(?s)#\\s*BUGS\\s*[^\\n]*\\s*(.*?)#\\s*COPYRIGHT', readme).group(1)\n@@ -25,8 +25,7 @@ def main():\n \n     out = bug_text + dev_text\n \n-    with io.open(outfile, 'w', encoding='utf-8') as outf:\n-        outf.write(out)\n+    write_file(outfile, out)\n \n \n if __name__ == '__main__':\ndiff --git a/devscripts/make_issue_template.py b/devscripts/make_issue_template.py\nindex b7ad23d8363..65fa8169f16 100644\n--- a/devscripts/make_issue_template.py\n+++ b/devscripts/make_issue_template.py\n@@ -1,8 +1,11 @@\n #!/usr/bin/env python\n from __future__ import unicode_literals\n \n-import io\n import optparse\n+import os.path\n+import sys\n+\n+from utils import read_file, read_version, write_file\n \n \n def main():\n@@ -13,17 +16,11 @@ def main():\n \n     infile, outfile = args\n \n-    with io.open(infile, encoding='utf-8') as inf:\n-        issue_template_tmpl = inf.read()\n-\n-    # Get the version from youtube_dl/version.py without importing the package\n-    exec(compile(open('youtube_dl/version.py').read(),\n-                 'youtube_dl/version.py', 'exec'))\n+    issue_template_tmpl = read_file(infile)\n \n-    out = issue_template_tmpl % {'version': locals()['__version__']}\n+    out = issue_template_tmpl % {'version': read_version()}\n \n-    with io.open(outfile, 'w', encoding='utf-8') as outf:\n-        outf.write(out)\n+    write_file(outfile, out)\n \n if __name__ == '__main__':\n     main()\ndiff --git a/devscripts/make_lazy_extractors.py b/devscripts/make_lazy_extractors.py\nindex 878ae72b132..5b8b123a42f 100644\n--- a/devscripts/make_lazy_extractors.py\n+++ b/devscripts/make_lazy_extractors.py\n@@ -1,28 +1,49 @@\n from __future__ import unicode_literals, print_function\n \n from inspect import getsource\n-import io\n import os\n from os.path import dirname as dirn\n+import re\n import sys\n \n print('WARNING: Lazy loading extractors is an experimental feature that may not always work', file=sys.stderr)\n \n-sys.path.insert(0, dirn(dirn((os.path.abspath(__file__)))))\n+sys.path.insert(0, dirn(dirn(os.path.abspath(__file__))))\n \n lazy_extractors_filename = sys.argv[1]\n if os.path.exists(lazy_extractors_filename):\n     os.remove(lazy_extractors_filename)\n+# Py2: may be confused by leftover lazy_extractors.pyc\n+if sys.version_info[0] < 3:\n+    for c in ('c', 'o'):\n+        try:\n+            os.remove(lazy_extractors_filename + 'c')\n+        except OSError:\n+            pass\n+\n+from devscripts.utils import read_file, write_file\n+from youtube_dl.compat import compat_register_utf8\n+\n+compat_register_utf8()\n \n from youtube_dl.extractor import _ALL_CLASSES\n from youtube_dl.extractor.common import InfoExtractor, SearchInfoExtractor\n \n-with open('devscripts/lazy_load_template.py', 'rt') as f:\n-    module_template = f.read()\n+module_template = read_file('devscripts/lazy_load_template.py')\n+\n+\n+def get_source(m):\n+    return re.sub(r'(?m)^\\s*#.*\\n', '', getsource(m))\n+\n \n module_contents = [\n-    module_template + '\\n' + getsource(InfoExtractor.suitable) + '\\n',\n-    'class LazyLoadSearchExtractor(LazyLoadExtractor):\\n    pass\\n']\n+    module_template,\n+    get_source(InfoExtractor.suitable),\n+    get_source(InfoExtractor._match_valid_url) + '\\n',\n+    'class LazyLoadSearchExtractor(LazyLoadExtractor):\\n    pass\\n',\n+    # needed for suitable() methods of Youtube extractor (see #28780)\n+    'from youtube_dl.utils import parse_qs, variadic\\n',\n+]\n \n ie_template = '''\n class {name}({bases}):\n@@ -54,7 +75,7 @@ def build_lazy_ie(ie, name):\n         valid_url=valid_url,\n         module=ie.__module__)\n     if ie.suitable.__func__ is not InfoExtractor.suitable.__func__:\n-        s += '\\n' + getsource(ie.suitable)\n+        s += '\\n' + get_source(ie.suitable)\n     if hasattr(ie, '_make_valid_url'):\n         # search extractors\n         s += make_valid_template.format(valid_url=ie._make_valid_url())\n@@ -94,7 +115,17 @@ def build_lazy_ie(ie, name):\n module_contents.append(\n     '_ALL_CLASSES = [{0}]'.format(', '.join(names)))\n \n-module_src = '\\n'.join(module_contents) + '\\n'\n+module_src = '\\n'.join(module_contents)\n+\n+write_file(lazy_extractors_filename, module_src + '\\n')\n \n-with io.open(lazy_extractors_filename, 'wt', encoding='utf-8') as f:\n-    f.write(module_src)\n+# work around JVM byte code module limit in Jython\n+if sys.platform.startswith('java') and sys.version_info[:2] == (2, 7):\n+    import subprocess\n+    from youtube_dl.compat import compat_subprocess_get_DEVNULL\n+    # if Python 2.7 is available, use it to compile the module for Jython\n+    try:\n+        # if Python 2.7 is available, use it to compile the module for Jython\n+        subprocess.check_call(['python2.7', '-m', 'py_compile', lazy_extractors_filename], stdout=compat_subprocess_get_DEVNULL())\n+    except Exception:\n+        pass\ndiff --git a/devscripts/make_readme.py b/devscripts/make_readme.py\nindex 8fbce07967c..7a5b04dcc26 100755\n--- a/devscripts/make_readme.py\n+++ b/devscripts/make_readme.py\n@@ -1,8 +1,14 @@\n from __future__ import unicode_literals\n \n-import io\n-import sys\n+import os.path\n import re\n+import sys\n+dirn = os.path.dirname\n+\n+sys.path.insert(0, dirn(dirn(os.path.abspath(__file__))))\n+\n+from utils import read_file\n+from youtube_dl.compat import compat_open as open\n \n README_FILE = 'README.md'\n helptext = sys.stdin.read()\n@@ -10,8 +16,7 @@\n if isinstance(helptext, bytes):\n     helptext = helptext.decode('utf-8')\n \n-with io.open(README_FILE, encoding='utf-8') as f:\n-    oldreadme = f.read()\n+oldreadme = read_file(README_FILE)\n \n header = oldreadme[:oldreadme.index('# OPTIONS')]\n footer = oldreadme[oldreadme.index('# CONFIGURATION'):]\n@@ -20,7 +25,7 @@\n options = re.sub(r'(?m)^  (\\w.+)$', r'## \\1', options)\n options = '# OPTIONS\\n' + options + '\\n'\n \n-with io.open(README_FILE, 'w', encoding='utf-8') as f:\n+with open(README_FILE, 'w', encoding='utf-8') as f:\n     f.write(header)\n     f.write(options)\n     f.write(footer)\ndiff --git a/devscripts/make_supportedsites.py b/devscripts/make_supportedsites.py\nindex 764795bc5b1..c424d18d7de 100644\n--- a/devscripts/make_supportedsites.py\n+++ b/devscripts/make_supportedsites.py\n@@ -1,17 +1,19 @@\n #!/usr/bin/env python\n from __future__ import unicode_literals\n \n-import io\n import optparse\n-import os\n+import os.path\n import sys\n \n-\n # Import youtube_dl\n-ROOT_DIR = os.path.join(os.path.dirname(__file__), '..')\n-sys.path.insert(0, ROOT_DIR)\n+dirn = os.path.dirname\n+\n+sys.path.insert(0, dirn(dirn(os.path.abspath(__file__))))\n+\n import youtube_dl\n \n+from utils import write_file\n+\n \n def main():\n     parser = optparse.OptionParser(usage='%prog OUTFILE.md')\n@@ -38,8 +40,7 @@ def gen_ies_md(ies):\n         ' - ' + md + '\\n'\n         for md in gen_ies_md(ies))\n \n-    with io.open(outfile, 'w', encoding='utf-8') as outf:\n-        outf.write(out)\n+    write_file(outfile, out)\n \n \n if __name__ == '__main__':\ndiff --git a/devscripts/prepare_manpage.py b/devscripts/prepare_manpage.py\nindex 76bf873e1bd..0090ada3e3d 100644\n--- a/devscripts/prepare_manpage.py\n+++ b/devscripts/prepare_manpage.py\n@@ -1,13 +1,13 @@\n from __future__ import unicode_literals\n \n-import io\n import optparse\n import os.path\n import re\n \n+from utils import read_file, write_file\n+\n ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n README_FILE = os.path.join(ROOT_DIR, 'README.md')\n-\n PREFIX = r'''%YOUTUBE-DL(1)\n \n # NAME\n@@ -29,8 +29,7 @@ def main():\n \n     outfile, = args\n \n-    with io.open(README_FILE, encoding='utf-8') as f:\n-        readme = f.read()\n+    readme = read_file(README_FILE)\n \n     readme = re.sub(r'(?s)^.*?(?=# DESCRIPTION)', '', readme)\n     readme = re.sub(r'\\s+youtube-dl \\[OPTIONS\\] URL \\[URL\\.\\.\\.\\]', '', readme)\n@@ -38,8 +37,7 @@ def main():\n \n     readme = filter_options(readme)\n \n-    with io.open(outfile, 'w', encoding='utf-8') as outf:\n-        outf.write(readme)\n+    write_file(outfile, readme)\n \n \n def filter_options(readme):\ndiff --git a/devscripts/utils.py b/devscripts/utils.py\nnew file mode 100644\nindex 00000000000..2d072d2e0bf\n--- /dev/null\n+++ b/devscripts/utils.py\n@@ -0,0 +1,62 @@\n+# coding: utf-8\n+from __future__ import unicode_literals\n+\n+import argparse\n+import functools\n+import os.path\n+import subprocess\n+import sys\n+\n+dirn = os.path.dirname\n+\n+sys.path.insert(0, dirn(dirn(os.path.abspath(__file__))))\n+\n+from youtube_dl.compat import (\n+    compat_kwargs,\n+    compat_open as open,\n+)\n+\n+\n+def read_file(fname):\n+    with open(fname, encoding='utf-8') as f:\n+        return f.read()\n+\n+\n+def write_file(fname, content, mode='w'):\n+    with open(fname, mode, encoding='utf-8') as f:\n+        return f.write(content)\n+\n+\n+def read_version(fname='youtube_dl/version.py'):\n+    \"\"\"Get the version without importing the package\"\"\"\n+    exec(compile(read_file(fname), fname, 'exec'))\n+    return locals()['__version__']\n+\n+\n+def get_filename_args(has_infile=False, default_outfile=None):\n+    parser = argparse.ArgumentParser()\n+    if has_infile:\n+        parser.add_argument('infile', help='Input file')\n+    kwargs = {'nargs': '?', 'default': default_outfile} if default_outfile else {}\n+    kwargs['help'] = 'Output file'\n+    parser.add_argument('outfile', **compat_kwargs(kwargs))\n+\n+    opts = parser.parse_args()\n+    if has_infile:\n+        return opts.infile, opts.outfile\n+    return opts.outfile\n+\n+\n+def compose_functions(*functions):\n+    return lambda x: functools.reduce(lambda y, f: f(y), functions, x)\n+\n+\n+def run_process(*args, **kwargs):\n+    kwargs.setdefault('text', True)\n+    kwargs.setdefault('check', True)\n+    kwargs.setdefault('capture_output', True)\n+    if kwargs['text']:\n+        kwargs.setdefault('encoding', 'utf-8')\n+        kwargs.setdefault('errors', 'replace')\n+        kwargs = compat_kwargs(kwargs)\n+    return subprocess.run(args, **kwargs)\ndiff --git a/devscripts/zsh-completion.py b/devscripts/zsh-completion.py\nindex 60aaf76cc32..ebd552fcba1 100755\n--- a/devscripts/zsh-completion.py\n+++ b/devscripts/zsh-completion.py\n@@ -7,6 +7,8 @@\n \n sys.path.insert(0, dirn(dirn((os.path.abspath(__file__)))))\n import youtube_dl\n+from utils import read_file, write_file\n+\n \n ZSH_COMPLETION_FILE = \"youtube-dl.zsh\"\n ZSH_COMPLETION_TEMPLATE = \"devscripts/zsh-completion.in\"\n@@ -34,15 +36,13 @@ def build_completion(opt_parser):\n \n     flags = [opt.get_opt_string() for opt in opts]\n \n-    with open(ZSH_COMPLETION_TEMPLATE) as f:\n-        template = f.read()\n+    template = read_file(ZSH_COMPLETION_TEMPLATE)\n \n     template = template.replace(\"{{fileopts}}\", \"|\".join(fileopts))\n     template = template.replace(\"{{diropts}}\", \"|\".join(diropts))\n     template = template.replace(\"{{flags}}\", \" \".join(flags))\n \n-    with open(ZSH_COMPLETION_FILE, \"w\") as f:\n-        f.write(template)\n+    write_file(ZSH_COMPLETION_FILE, template)\n \n \n parser = youtube_dl.parseOpts()[0]\ndiff --git a/youtube_dl/YoutubeDL.py b/youtube_dl/YoutubeDL.py\nindex fe30758ef9c..9e5620eefa1 100755\n--- a/youtube_dl/YoutubeDL.py\n+++ b/youtube_dl/YoutubeDL.py\n@@ -4,11 +4,10 @@\n from __future__ import absolute_import, unicode_literals\n \n import collections\n-import contextlib\n import copy\n import datetime\n import errno\n-import fileinput\n+import functools\n import io\n import itertools\n import json\n@@ -26,25 +25,39 @@\n import traceback\n import random\n \n+try:\n+    from ssl import OPENSSL_VERSION\n+except ImportError:\n+    # Must be Python 2.6, should be built against 1.0.2\n+    OPENSSL_VERSION = 'OpenSSL 1.0.2(?)'\n from string import ascii_letters\n \n from .compat import (\n     compat_basestring,\n-    compat_cookiejar,\n+    compat_collections_chain_map as ChainMap,\n+    compat_filter as filter,\n     compat_get_terminal_size,\n     compat_http_client,\n+    compat_http_cookiejar_Cookie,\n+    compat_http_cookies_SimpleCookie,\n+    compat_integer_types,\n     compat_kwargs,\n+    compat_map as map,\n     compat_numeric_types,\n+    compat_open as open,\n     compat_os_name,\n     compat_str,\n     compat_tokenize_tokenize,\n     compat_urllib_error,\n+    compat_urllib_parse,\n     compat_urllib_request,\n     compat_urllib_request_DataHandler,\n )\n from .utils import (\n+    _UnsafeExtensionError,\n     age_restricted,\n     args_to_str,\n+    bug_reports_message,\n     ContentTooShortError,\n     date_from_str,\n     DateRange,\n@@ -62,7 +75,9 @@\n     GeoRestrictedError,\n     int_or_none,\n     ISO3166Utils,\n+    join_nonempty,\n     locked_file,\n+    LazyList,\n     make_HTTPS_handler,\n     MaxDownloadsReached,\n     orderedSet,\n@@ -73,6 +88,7 @@\n     PostProcessingError,\n     preferredencoding,\n     prepend_extension,\n+    process_communicate_or_kill,\n     register_socks_protocols,\n     render_table,\n     replace_extension,\n@@ -84,6 +100,7 @@\n     std_headers,\n     str_or_none,\n     subtitles_filename,\n+    traverse_obj,\n     UnavailableVideoError,\n     url_basename,\n     version_tuple,\n@@ -93,6 +110,7 @@\n     YoutubeDLCookieProcessor,\n     YoutubeDLHandler,\n     YoutubeDLRedirectHandler,\n+    ytdl_is_updateable,\n )\n from .cache import Cache\n from .extractor import get_info_extractor, gen_extractor_classes, _LAZY_LOADER\n@@ -113,6 +131,20 @@\n     import ctypes\n \n \n+def _catch_unsafe_file_extension(func):\n+    @functools.wraps(func)\n+    def wrapper(self, *args, **kwargs):\n+        try:\n+            return func(self, *args, **kwargs)\n+        except _UnsafeExtensionError as error:\n+            self.report_error(\n+                '{0} found; to avoid damaging your system, this value is disallowed.'\n+                ' If you believe this is an error{1}'.format(\n+                    error_to_compat_str(error), bug_reports_message(',')))\n+\n+    return wrapper\n+\n+\n class YoutubeDL(object):\n     \"\"\"YoutubeDL class.\n \n@@ -362,6 +394,9 @@ def __init__(self, params=None, auto_init=True):\n         self.params.update(params)\n         self.cache = Cache(self)\n \n+        self._header_cookies = []\n+        self._load_cookies_from_headers(self.params.get('http_headers'))\n+\n         def check_deprecated(param, option, suggestion):\n             if self.params.get(param) is not None:\n                 self.report_warning(\n@@ -568,7 +603,7 @@ def __exit__(self, *args):\n         if self.params.get('cookiefile') is not None:\n             self.cookiejar.save(ignore_discard=True, ignore_expires=True)\n \n-    def trouble(self, message=None, tb=None):\n+    def trouble(self, *args, **kwargs):\n         \"\"\"Determine action to take when a download problem appears.\n \n         Depending on if the downloader has been configured to ignore\n@@ -577,6 +612,11 @@ def trouble(self, message=None, tb=None):\n \n         tb, if given, is additional traceback information.\n         \"\"\"\n+        # message=None, tb=None, is_error=True\n+        message = args[0] if len(args) > 0 else kwargs.get('message', None)\n+        tb = args[1] if len(args) > 1 else kwargs.get('tb', None)\n+        is_error = args[2] if len(args) > 2 else kwargs.get('is_error', True)\n+\n         if message is not None:\n             self.to_stderr(message)\n         if self.params.get('verbose'):\n@@ -589,7 +629,10 @@ def trouble(self, message=None, tb=None):\n                 else:\n                     tb_data = traceback.format_list(traceback.extract_stack())\n                     tb = ''.join(tb_data)\n-            self.to_stderr(tb)\n+            if tb:\n+                self.to_stderr(tb)\n+        if not is_error:\n+            return\n         if not self.params.get('ignoreerrors', False):\n             if sys.exc_info()[0] and hasattr(sys.exc_info()[1], 'exc_info') and sys.exc_info()[1].exc_info[0]:\n                 exc_info = sys.exc_info()[1].exc_info\n@@ -598,11 +641,18 @@ def trouble(self, message=None, tb=None):\n             raise DownloadError(message, exc_info)\n         self._download_retcode = 1\n \n-    def report_warning(self, message):\n+    def report_warning(self, message, only_once=False, _cache={}):\n         '''\n         Print the message to stderr, it will be prefixed with 'WARNING:'\n         If stderr is a tty file the 'WARNING:' will be colored\n         '''\n+        if only_once:\n+            m_hash = hash((self, message))\n+            m_cnt = _cache.setdefault(m_hash, 0)\n+            _cache[m_hash] = m_cnt + 1\n+            if m_cnt > 0:\n+                return\n+\n         if self.params.get('logger') is not None:\n             self.params['logger'].warning(message)\n         else:\n@@ -615,7 +665,7 @@ def report_warning(self, message):\n             warning_message = '%s %s' % (_msg_header, message)\n             self.to_stderr(warning_message)\n \n-    def report_error(self, message, tb=None):\n+    def report_error(self, message, *args, **kwargs):\n         '''\n         Do the same as trouble, but prefixes the message with 'ERROR:', colored\n         in red if stderr is a tty file.\n@@ -624,8 +674,18 @@ def report_error(self, message, tb=None):\n             _msg_header = '\\033[0;31mERROR:\\033[0m'\n         else:\n             _msg_header = 'ERROR:'\n-        error_message = '%s %s' % (_msg_header, message)\n-        self.trouble(error_message, tb)\n+        kwargs['message'] = '%s %s' % (_msg_header, message)\n+        self.trouble(*args, **kwargs)\n+\n+    def report_unscoped_cookies(self, *args, **kwargs):\n+        # message=None, tb=False, is_error=False\n+        if len(args) <= 2:\n+            kwargs.setdefault('is_error', False)\n+            if len(args) <= 0:\n+                kwargs.setdefault(\n+                    'message',\n+                    'Unscoped cookies are not allowed: please specify some sort of scoping')\n+        self.report_error(*args, **kwargs)\n \n     def report_file_already_downloaded(self, file_name):\n         \"\"\"Report file has already been fully downloaded.\"\"\"\n@@ -720,7 +780,7 @@ def prepare_filename(self, info_dict):\n                 filename = encodeFilename(filename, True).decode(preferredencoding())\n             return sanitize_path(filename)\n         except ValueError as err:\n-            self.report_error('Error in output template: ' + str(err) + ' (encoding: ' + repr(preferredencoding()) + ')')\n+            self.report_error('Error in output template: ' + error_to_compat_str(err) + ' (encoding: ' + repr(preferredencoding()) + ')')\n             return None\n \n     def _match_entry(self, info_dict, incomplete):\n@@ -821,7 +881,7 @@ def wrapper(self, *args, **kwargs):\n                 msg += '\\nYou might want to use a VPN or a proxy server (with --proxy) to workaround.'\n                 self.report_error(msg)\n             except ExtractorError as e:  # An error we somewhat expected\n-                self.report_error(compat_str(e), e.format_traceback())\n+                self.report_error(compat_str(e), tb=e.format_traceback())\n             except MaxDownloadsReached:\n                 raise\n             except Exception as e:\n@@ -831,8 +891,83 @@ def wrapper(self, *args, **kwargs):\n                     raise\n         return wrapper\n \n+    def _remove_cookie_header(self, http_headers):\n+        \"\"\"Filters out `Cookie` header from an `http_headers` dict\n+        The `Cookie` header is removed to prevent leaks as a result of unscoped cookies.\n+        See: https://github.com/yt-dlp/yt-dlp/security/advisories/GHSA-v8mc-9377-rwjj\n+\n+        @param http_headers     An `http_headers` dict from which any `Cookie` header\n+                                should be removed, or None\n+        \"\"\"\n+        return dict(filter(lambda pair: pair[0].lower() != 'cookie', (http_headers or {}).items()))\n+\n+    def _load_cookies(self, data, **kwargs):\n+        \"\"\"Loads cookies from a `Cookie` header\n+\n+        This tries to work around the security vulnerability of passing cookies to every domain.\n+\n+        @param data         The Cookie header as a string to load the cookies from\n+        @param autoscope    If `False`, scope cookies using Set-Cookie syntax and error for cookie without domains\n+                            If `True`, save cookies for later to be stored in the jar with a limited scope\n+                            If a URL, save cookies in the jar with the domain of the URL\n+        \"\"\"\n+        # autoscope=True (kw-only)\n+        autoscope = kwargs.get('autoscope', True)\n+\n+        for cookie in compat_http_cookies_SimpleCookie(data).values() if data else []:\n+            if autoscope and any(cookie.values()):\n+                raise ValueError('Invalid syntax in Cookie Header')\n+\n+            domain = cookie.get('domain') or ''\n+            expiry = cookie.get('expires')\n+            if expiry == '':  # 0 is valid so we check for `''` explicitly\n+                expiry = None\n+            prepared_cookie = compat_http_cookiejar_Cookie(\n+                cookie.get('version') or 0, cookie.key, cookie.value, None, False,\n+                domain, True, True, cookie.get('path') or '', bool(cookie.get('path')),\n+                bool(cookie.get('secure')), expiry, False, None, None, {})\n+\n+            if domain:\n+                self.cookiejar.set_cookie(prepared_cookie)\n+            elif autoscope is True:\n+                self.report_warning(\n+                    'Passing cookies as a header is a potential security risk; '\n+                    'they will be scoped to the domain of the downloaded urls. '\n+                    'Please consider loading cookies from a file or browser instead.',\n+                    only_once=True)\n+                self._header_cookies.append(prepared_cookie)\n+            elif autoscope:\n+                self.report_warning(\n+                    'The extractor result contains an unscoped cookie as an HTTP header. '\n+                    'If you are specifying an input URL, ' + bug_reports_message(),\n+                    only_once=True)\n+                self._apply_header_cookies(autoscope, [prepared_cookie])\n+            else:\n+                self.report_unscoped_cookies()\n+\n+    def _load_cookies_from_headers(self, headers):\n+        self._load_cookies(traverse_obj(headers, 'cookie', casesense=False))\n+\n+    def _apply_header_cookies(self, url, cookies=None):\n+        \"\"\"This method applies stray header cookies to the provided url\n+\n+        This loads header cookies and scopes them to the domain provided in `url`.\n+        While this is not ideal, it helps reduce the risk of them being sent to\n+        an unintended destination.\n+        \"\"\"\n+        parsed = compat_urllib_parse.urlparse(url)\n+        if not parsed.hostname:\n+            return\n+\n+        for cookie in map(copy.copy, cookies or self._header_cookies):\n+            cookie.domain = '.' + parsed.hostname\n+            self.cookiejar.set_cookie(cookie)\n+\n     @__handle_extraction_exceptions\n     def __extract_info(self, url, ie, download, extra_info, process):\n+        # Compat with passing cookies in http headers\n+        self._apply_header_cookies(url)\n+\n         ie_result = ie.extract(url)\n         if ie_result is None:  # Finished already (backwards compatibility; listformats and friends should be moved here)\n             return\n@@ -858,7 +993,7 @@ def add_default_extra_info(self, ie_result, ie, url):\n \n     def process_ie_result(self, ie_result, download=True, extra_info={}):\n         \"\"\"\n-        Take the result of the ie(may be modified) and resolve all unresolved\n+        Take the result of the ie (may be modified) and resolve all unresolved\n         references (URLs, playlist items).\n \n         It will also download the videos if 'download'.\n@@ -920,8 +1055,8 @@ def process_ie_result(self, ie_result, download=True, extra_info={}):\n         elif result_type in ('playlist', 'multi_video'):\n             # Protect from infinite recursion due to recursively nested playlists\n             # (see https://github.com/ytdl-org/youtube-dl/issues/27833)\n-            webpage_url = ie_result['webpage_url']\n-            if webpage_url in self._playlist_urls:\n+            webpage_url = ie_result.get('webpage_url')  # not all pl/mv have this\n+            if webpage_url and webpage_url in self._playlist_urls:\n                 self.to_screen(\n                     '[download] Skipping already downloaded playlist: %s'\n                     % ie_result.get('title') or ie_result.get('id'))\n@@ -929,6 +1064,10 @@ def process_ie_result(self, ie_result, download=True, extra_info={}):\n \n             self._playlist_level += 1\n             self._playlist_urls.add(webpage_url)\n+            new_result = dict((k, v) for k, v in extra_info.items() if k not in ie_result)\n+            if new_result:\n+                new_result.update(ie_result)\n+                ie_result = new_result\n             try:\n                 return self.__process_playlist(ie_result, download)\n             finally:\n@@ -1385,17 +1524,16 @@ def _merge(formats_info):\n                         'abr': formats_info[1].get('abr'),\n                         'ext': output_ext,\n                     }\n-                video_selector, audio_selector = map(_build_selector_function, selector.selector)\n \n                 def selector_function(ctx):\n-                    for pair in itertools.product(\n-                            video_selector(copy.deepcopy(ctx)), audio_selector(copy.deepcopy(ctx))):\n+                    selector_fn = lambda x: _build_selector_function(x)(ctx)\n+                    for pair in itertools.product(*map(selector_fn, selector.selector)):\n                         yield _merge(pair)\n \n             filters = [self._build_format_filter(f) for f in selector.filters]\n \n             def final_selector(ctx):\n-                ctx_copy = copy.deepcopy(ctx)\n+                ctx_copy = dict(ctx)\n                 for _filter in filters:\n                     ctx_copy['formats'] = list(filter(_filter, ctx_copy['formats']))\n                 return selector_function(ctx_copy)\n@@ -1430,29 +1568,73 @@ def restore_last_token(self):\n         parsed_selector = _parse_format_selection(iter(TokenIterator(tokens)))\n         return _build_selector_function(parsed_selector)\n \n-    def _calc_headers(self, info_dict):\n-        res = std_headers.copy()\n-\n-        add_headers = info_dict.get('http_headers')\n-        if add_headers:\n-            res.update(add_headers)\n+    def _calc_headers(self, info_dict, load_cookies=False):\n+        if load_cookies:  # For --load-info-json\n+            # load cookies from http_headers in legacy info.json\n+            self._load_cookies(traverse_obj(info_dict, ('http_headers', 'Cookie'), casesense=False),\n+                               autoscope=info_dict['url'])\n+            # load scoped cookies from info.json\n+            self._load_cookies(info_dict.get('cookies'), autoscope=False)\n \n-        cookies = self._calc_cookies(info_dict)\n+        cookies = self.cookiejar.get_cookies_for_url(info_dict['url'])\n         if cookies:\n-            res['Cookie'] = cookies\n+            # Make a string like name1=val1; attr1=a_val1; ...name2=val2; ...\n+            # By convention a cookie name can't be a well-known attribute name\n+            # so this syntax is unambiguous and can be parsed by (eg) SimpleCookie\n+            encoder = compat_http_cookies_SimpleCookie()\n+            values = []\n+            attributes = (('Domain', '='), ('Path', '='), ('Secure',), ('Expires', '='), ('Version', '='))\n+            attributes = tuple([x[0].lower()] + list(x) for x in attributes)\n+            for cookie in cookies:\n+                _, value = encoder.value_encode(cookie.value)\n+                # Py 2 '' --> '', Py 3 '' --> '\"\"'\n+                if value == '':\n+                    value = '\"\"'\n+                values.append('='.join((cookie.name, value)))\n+                for attr in attributes:\n+                    value = getattr(cookie, attr[0], None)\n+                    if value:\n+                        values.append('%s%s' % (''.join(attr[1:]), value if len(attr) == 3 else ''))\n+            info_dict['cookies'] = '; '.join(values)\n+\n+        res = std_headers.copy()\n+        res.update(info_dict.get('http_headers') or {})\n+        res = self._remove_cookie_header(res)\n \n         if 'X-Forwarded-For' not in res:\n             x_forwarded_for_ip = info_dict.get('__x_forwarded_for_ip')\n             if x_forwarded_for_ip:\n                 res['X-Forwarded-For'] = x_forwarded_for_ip\n \n-        return res\n+        return res or None\n \n     def _calc_cookies(self, info_dict):\n         pr = sanitized_Request(info_dict['url'])\n         self.cookiejar.add_cookie_header(pr)\n         return pr.get_header('Cookie')\n \n+    def _fill_common_fields(self, info_dict, final=True):\n+\n+        for ts_key, date_key in (\n+                ('timestamp', 'upload_date'),\n+                ('release_timestamp', 'release_date'),\n+        ):\n+            if info_dict.get(date_key) is None and info_dict.get(ts_key) is not None:\n+                # Working around out-of-range timestamp values (e.g. negative ones on Windows,\n+                # see http://bugs.python.org/issue1646728)\n+                try:\n+                    upload_date = datetime.datetime.utcfromtimestamp(info_dict[ts_key])\n+                    info_dict[date_key] = compat_str(upload_date.strftime('%Y%m%d'))\n+                except (ValueError, OverflowError, OSError):\n+                    pass\n+\n+        # Auto generate title fields corresponding to the *_number fields when missing\n+        # in order to always have clean titles. This is very common for TV series.\n+        if final:\n+            for field in ('chapter', 'season', 'episode'):\n+                if info_dict.get('%s_number' % field) is not None and not info_dict.get(field):\n+                    info_dict[field] = '%s %d' % (field.capitalize(), info_dict['%s_number' % field])\n+\n     def process_video_result(self, info_dict, download=True):\n         assert info_dict.get('_type', 'video') == 'video'\n \n@@ -1520,24 +1702,7 @@ def sanitize_numeric_fields(info):\n         if 'display_id' not in info_dict and 'id' in info_dict:\n             info_dict['display_id'] = info_dict['id']\n \n-        for ts_key, date_key in (\n-                ('timestamp', 'upload_date'),\n-                ('release_timestamp', 'release_date'),\n-        ):\n-            if info_dict.get(date_key) is None and info_dict.get(ts_key) is not None:\n-                # Working around out-of-range timestamp values (e.g. negative ones on Windows,\n-                # see http://bugs.python.org/issue1646728)\n-                try:\n-                    upload_date = datetime.datetime.utcfromtimestamp(info_dict[ts_key])\n-                    info_dict[date_key] = upload_date.strftime('%Y%m%d')\n-                except (ValueError, OverflowError, OSError):\n-                    pass\n-\n-        # Auto generate title fields corresponding to the *_number fields when missing\n-        # in order to always have clean titles. This is very common for TV series.\n-        for field in ('chapter', 'season', 'episode'):\n-            if info_dict.get('%s_number' % field) is not None and not info_dict.get(field):\n-                info_dict[field] = '%s %d' % (field.capitalize(), info_dict['%s_number' % field])\n+        self._fill_common_fields(info_dict)\n \n         for cc_kind in ('subtitles', 'automatic_captions'):\n             cc = info_dict.get(cc_kind)\n@@ -1569,9 +1734,6 @@ def sanitize_numeric_fields(info):\n         else:\n             formats = info_dict['formats']\n \n-        if not formats:\n-            raise ExtractorError('No video formats found!')\n-\n         def is_wellformed(f):\n             url = f.get('url')\n             if not url:\n@@ -1584,7 +1746,10 @@ def is_wellformed(f):\n             return True\n \n         # Filter out malformed formats for better extraction robustness\n-        formats = list(filter(is_wellformed, formats))\n+        formats = list(filter(is_wellformed, formats or []))\n+\n+        if not formats:\n+            raise ExtractorError('No video formats found!')\n \n         formats_dict = {}\n \n@@ -1625,10 +1790,13 @@ def is_wellformed(f):\n                 format['protocol'] = determine_protocol(format)\n             # Add HTTP headers, so that external programs can use them from the\n             # json output\n-            full_format_info = info_dict.copy()\n-            full_format_info.update(format)\n-            format['http_headers'] = self._calc_headers(full_format_info)\n-        # Remove private housekeeping stuff\n+            format['http_headers'] = self._calc_headers(ChainMap(format, info_dict), load_cookies=True)\n+\n+        # Safeguard against old/insecure infojson when using --load-info-json\n+        info_dict['http_headers'] = self._remove_cookie_header(\n+            info_dict.get('http_headers') or {}) or None\n+\n+        # Remove private housekeeping stuff (copied to http_headers in _calc_headers())\n         if '__x_forwarded_for_ip' in info_dict:\n             del info_dict['__x_forwarded_for_ip']\n \n@@ -1771,17 +1939,17 @@ def print_optional(field):\n             self.to_stdout(formatSeconds(info_dict['duration']))\n         print_mandatory('format')\n         if self.params.get('forcejson', False):\n-            self.to_stdout(json.dumps(info_dict))\n+            self.to_stdout(json.dumps(self.sanitize_info(info_dict)))\n \n+    @_catch_unsafe_file_extension\n     def process_info(self, info_dict):\n         \"\"\"Process a single resolved IE result.\"\"\"\n \n         assert info_dict.get('_type', 'video') == 'video'\n \n-        max_downloads = self.params.get('max_downloads')\n-        if max_downloads is not None:\n-            if self._num_downloads >= int(max_downloads):\n-                raise MaxDownloadsReached()\n+        max_downloads = int_or_none(self.params.get('max_downloads')) or float('inf')\n+        if self._num_downloads >= max_downloads:\n+            raise MaxDownloadsReached()\n \n         # TODO: backward compatibility, to be removed\n         info_dict['fulltitle'] = info_dict['title']\n@@ -1832,7 +2000,7 @@ def ensure_dir_exists(path):\n             else:\n                 try:\n                     self.to_screen('[info] Writing video description to: ' + descfn)\n-                    with io.open(encodeFilename(descfn), 'w', encoding='utf-8') as descfile:\n+                    with open(encodeFilename(descfn), 'w', encoding='utf-8') as descfile:\n                         descfile.write(info_dict['description'])\n                 except (OSError, IOError):\n                     self.report_error('Cannot write description file ' + descfn)\n@@ -1847,7 +2015,7 @@ def ensure_dir_exists(path):\n             else:\n                 try:\n                     self.to_screen('[info] Writing video annotations to: ' + annofn)\n-                    with io.open(encodeFilename(annofn), 'w', encoding='utf-8') as annofile:\n+                    with open(encodeFilename(annofn), 'w', encoding='utf-8') as annofile:\n                         annofile.write(info_dict['annotations'])\n                 except (KeyError, TypeError):\n                     self.report_warning('There are no annotations to write.')\n@@ -1874,7 +2042,7 @@ def ensure_dir_exists(path):\n                         try:\n                             # Use newline='' to prevent conversion of newline characters\n                             # See https://github.com/ytdl-org/youtube-dl/issues/10268\n-                            with io.open(encodeFilename(sub_filename), 'w', encoding='utf-8', newline='') as subfile:\n+                            with open(encodeFilename(sub_filename), 'w', encoding='utf-8', newline='') as subfile:\n                                 subfile.write(sub_info['data'])\n                         except (OSError, IOError):\n                             self.report_error('Cannot write subtitles file ' + sub_filename)\n@@ -1883,36 +2051,41 @@ def ensure_dir_exists(path):\n                         try:\n                             sub_data = ie._request_webpage(\n                                 sub_info['url'], info_dict['id'], note=False).read()\n-                            with io.open(encodeFilename(sub_filename), 'wb') as subfile:\n+                            with open(encodeFilename(sub_filename), 'wb') as subfile:\n                                 subfile.write(sub_data)\n                         except (ExtractorError, IOError, OSError, ValueError) as err:\n                             self.report_warning('Unable to download subtitle for \"%s\": %s' %\n                                                 (sub_lang, error_to_compat_str(err)))\n                             continue\n \n-        if self.params.get('writeinfojson', False):\n-            infofn = replace_extension(filename, 'info.json', info_dict.get('ext'))\n-            if self.params.get('nooverwrites', False) and os.path.exists(encodeFilename(infofn)):\n-                self.to_screen('[info] Video description metadata is already present')\n-            else:\n-                self.to_screen('[info] Writing video description metadata as JSON to: ' + infofn)\n-                try:\n-                    write_json_file(self.filter_requested_info(info_dict), infofn)\n-                except (OSError, IOError):\n-                    self.report_error('Cannot write metadata to JSON file ' + infofn)\n-                    return\n+        self._write_info_json(\n+            'video description', info_dict,\n+            replace_extension(filename, 'info.json', info_dict.get('ext')))\n \n         self._write_thumbnails(info_dict, filename)\n \n         if not self.params.get('skip_download', False):\n             try:\n+                def checked_get_suitable_downloader(info_dict, params):\n+                    ed_args = params.get('external_downloader_args')\n+                    dler = get_suitable_downloader(info_dict, params)\n+                    if ed_args and not params.get('external_downloader_args'):\n+                        # external_downloader_args was cleared because external_downloader was rejected\n+                        self.report_warning('Requested external downloader cannot be used: '\n+                                            'ignoring --external-downloader-args.')\n+                    return dler\n+\n                 def dl(name, info):\n-                    fd = get_suitable_downloader(info, self.params)(self, self.params)\n+                    fd = checked_get_suitable_downloader(info, self.params)(self, self.params)\n                     for ph in self._progress_hooks:\n                         fd.add_progress_hook(ph)\n                     if self.params.get('verbose'):\n                         self.to_screen('[debug] Invoking downloader on %r' % info.get('url'))\n-                    return fd.download(name, info)\n+\n+                    new_info = dict((k, v) for k, v in info.items() if not k.startswith('__p'))\n+                    new_info['http_headers'] = self._calc_headers(new_info)\n+\n+                    return fd.download(name, new_info)\n \n                 if info_dict.get('requested_formats') is not None:\n                     downloaded = []\n@@ -1941,18 +2114,26 @@ def compatible_formats(formats):\n                         # TODO: Check acodec/vcodec\n                         return False\n \n-                    filename_real_ext = os.path.splitext(filename)[1][1:]\n-                    filename_wo_ext = (\n-                        os.path.splitext(filename)[0]\n-                        if filename_real_ext == info_dict['ext']\n-                        else filename)\n+                    exts = [info_dict['ext']]\n                     requested_formats = info_dict['requested_formats']\n                     if self.params.get('merge_output_format') is None and not compatible_formats(requested_formats):\n                         info_dict['ext'] = 'mkv'\n                         self.report_warning(\n                             'Requested formats are incompatible for merge and will be merged into mkv.')\n+                    exts.append(info_dict['ext'])\n+\n                     # Ensure filename always has a correct extension for successful merge\n-                    filename = '%s.%s' % (filename_wo_ext, info_dict['ext'])\n+                    def correct_ext(filename, ext=exts[1]):\n+                        if filename == '-':\n+                            return filename\n+                        f_name, f_real_ext = os.path.splitext(filename)\n+                        f_real_ext = f_real_ext[1:]\n+                        filename_wo_ext = f_name if f_real_ext in exts else filename\n+                        if ext is None:\n+                            ext = f_real_ext or None\n+                        return join_nonempty(filename_wo_ext, ext, delim='.')\n+\n+                    filename = correct_ext(filename)\n                     if os.path.exists(encodeFilename(filename)):\n                         self.to_screen(\n                             '[download] %s has already been downloaded and '\n@@ -1962,8 +2143,9 @@ def compatible_formats(formats):\n                             new_info = dict(info_dict)\n                             new_info.update(f)\n                             fname = prepend_extension(\n-                                self.prepare_filename(new_info),\n-                                'f%s' % f['format_id'], new_info['ext'])\n+                                correct_ext(\n+                                    self.prepare_filename(new_info), new_info['ext']),\n+                                'f%s' % (f['format_id'],), new_info['ext'])\n                             if not ensure_dir_exists(fname):\n                                 return\n                             downloaded.append(fname)\n@@ -2049,9 +2231,12 @@ def compatible_formats(formats):\n                 try:\n                     self.post_process(filename, info_dict)\n                 except (PostProcessingError) as err:\n-                    self.report_error('postprocessing: %s' % str(err))\n+                    self.report_error('postprocessing: %s' % error_to_compat_str(err))\n                     return\n                 self.record_download_archive(info_dict)\n+                # avoid possible nugatory search for further items (PR #26638)\n+                if self._num_downloads >= max_downloads:\n+                    raise MaxDownloadsReached()\n \n     def download(self, url_list):\n         \"\"\"Download a given list of URLs.\"\"\"\n@@ -2074,16 +2259,13 @@ def download(self, url_list):\n                 raise\n             else:\n                 if self.params.get('dump_single_json', False):\n-                    self.to_stdout(json.dumps(res))\n+                    self.to_stdout(json.dumps(self.sanitize_info(res)))\n \n         return self._download_retcode\n \n     def download_with_info_file(self, info_filename):\n-        with contextlib.closing(fileinput.FileInput(\n-                [info_filename], mode='r',\n-                openhook=fileinput.hook_encoded('utf-8'))) as f:\n-            # FileInput doesn't have a read method, we can't call json.load\n-            info = self.filter_requested_info(json.loads('\\n'.join(f)))\n+        with open(info_filename, encoding='utf-8') as f:\n+            info = self.filter_requested_info(json.load(f))\n         try:\n             self.process_ie_result(info, download=True)\n         except DownloadError:\n@@ -2096,10 +2278,36 @@ def download_with_info_file(self, info_filename):\n         return self._download_retcode\n \n     @staticmethod\n-    def filter_requested_info(info_dict):\n-        return dict(\n-            (k, v) for k, v in info_dict.items()\n-            if k not in ['requested_formats', 'requested_subtitles'])\n+    def sanitize_info(info_dict, remove_private_keys=False):\n+        ''' Sanitize the infodict for converting to json '''\n+        if info_dict is None:\n+            return info_dict\n+\n+        if remove_private_keys:\n+            reject = lambda k, v: (v is None\n+                                   or k.startswith('__')\n+                                   or k in ('requested_formats',\n+                                            'requested_subtitles'))\n+        else:\n+            reject = lambda k, v: False\n+\n+        def filter_fn(obj):\n+            if isinstance(obj, dict):\n+                return dict((k, filter_fn(v)) for k, v in obj.items() if not reject(k, v))\n+            elif isinstance(obj, (list, tuple, set, LazyList)):\n+                return list(map(filter_fn, obj))\n+            elif obj is None or any(isinstance(obj, c)\n+                                    for c in (compat_integer_types,\n+                                              (compat_str, float, bool))):\n+                return obj\n+            else:\n+                return repr(obj)\n+\n+        return filter_fn(info_dict)\n+\n+    @classmethod\n+    def filter_requested_info(cls, info_dict):\n+        return cls.sanitize_info(info_dict, True)\n \n     def post_process(self, filename, ie_info):\n         \"\"\"Run all the postprocessors on the given file.\"\"\"\n@@ -2306,18 +2514,21 @@ def print_debug_header(self):\n                 self.get_encoding()))\n         write_string(encoding_str, encoding=None)\n \n-        self._write_string('[debug] youtube-dl version ' + __version__ + '\\n')\n+        writeln_debug = lambda *s: self._write_string('[debug] %s\\n' % (''.join(s), ))\n+        writeln_debug('youtube-dl version ', __version__)\n         if _LAZY_LOADER:\n-            self._write_string('[debug] Lazy loading extractors enabled' + '\\n')\n+            writeln_debug('Lazy loading extractors enabled')\n+        if ytdl_is_updateable():\n+            writeln_debug('Single file build')\n         try:\n             sp = subprocess.Popen(\n                 ['git', 'rev-parse', '--short', 'HEAD'],\n                 stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n                 cwd=os.path.dirname(os.path.abspath(__file__)))\n-            out, err = sp.communicate()\n+            out, err = process_communicate_or_kill(sp)\n             out = out.decode().strip()\n             if re.match('[0-9a-f]+', out):\n-                self._write_string('[debug] Git HEAD: ' + out + '\\n')\n+                writeln_debug('Git HEAD: ', out)\n         except Exception:\n             try:\n                 sys.exc_clear()\n@@ -2330,9 +2541,22 @@ def python_implementation():\n                 return impl_name + ' version %d.%d.%d' % sys.pypy_version_info[:3]\n             return impl_name\n \n-        self._write_string('[debug] Python version %s (%s) - %s\\n' % (\n-            platform.python_version(), python_implementation(),\n-            platform_name()))\n+        def libc_ver():\n+            try:\n+                return platform.libc_ver()\n+            except OSError:  # We may not have access to the executable\n+                return []\n+\n+        libc = join_nonempty(*libc_ver(), delim=' ')\n+        writeln_debug('Python %s (%s %s %s) - %s - %s%s' % (\n+            platform.python_version(),\n+            python_implementation(),\n+            platform.machine(),\n+            platform.architecture()[0],\n+            platform_name(),\n+            OPENSSL_VERSION,\n+            (' - %s' % (libc, )) if libc else ''\n+        ))\n \n         exe_versions = FFmpegPostProcessor.get_versions(self)\n         exe_versions['rtmpdump'] = rtmpdump_version()\n@@ -2344,17 +2568,17 @@ def python_implementation():\n         )\n         if not exe_str:\n             exe_str = 'none'\n-        self._write_string('[debug] exe versions: %s\\n' % exe_str)\n+        writeln_debug('exe versions: %s' % (exe_str, ))\n \n         proxy_map = {}\n         for handler in self._opener.handlers:\n             if hasattr(handler, 'proxies'):\n                 proxy_map.update(handler.proxies)\n-        self._write_string('[debug] Proxy map: ' + compat_str(proxy_map) + '\\n')\n+        writeln_debug('Proxy map: ', compat_str(proxy_map))\n \n         if self.params.get('call_home', False):\n             ipaddr = self.urlopen('https://yt-dl.org/ip').read().decode('utf-8')\n-            self._write_string('[debug] Public IP address: %s\\n' % ipaddr)\n+            writeln_debug('Public IP address: %s' % (ipaddr, ))\n             latest_version = self.urlopen(\n                 'https://yt-dl.org/latest/version').read().decode('utf-8')\n             if version_tuple(latest_version) > version_tuple(__version__):\n@@ -2371,7 +2595,7 @@ def _setup_opener(self):\n         opts_proxy = self.params.get('proxy')\n \n         if opts_cookiefile is None:\n-            self.cookiejar = compat_cookiejar.CookieJar()\n+            self.cookiejar = YoutubeDLCookieJar()\n         else:\n             opts_cookiefile = expand_path(opts_cookiefile)\n             self.cookiejar = YoutubeDLCookieJar(opts_cookiefile)\n@@ -2432,6 +2656,28 @@ def get_encoding(self):\n             encoding = preferredencoding()\n         return encoding\n \n+    def _write_info_json(self, label, info_dict, infofn, overwrite=None):\n+        if not self.params.get('writeinfojson', False):\n+            return False\n+\n+        def msg(fmt, lbl):\n+            return fmt % (lbl + ' metadata',)\n+\n+        if overwrite is None:\n+            overwrite = not self.params.get('nooverwrites', False)\n+\n+        if not overwrite and os.path.exists(encodeFilename(infofn)):\n+            self.to_screen(msg('[info] %s is already present', label.title()))\n+            return 'exists'\n+        else:\n+            self.to_screen(msg('[info] Writing %s as JSON to: ', label) + infofn)\n+            try:\n+                write_json_file(self.filter_requested_info(info_dict), infofn)\n+                return True\n+            except (OSError, IOError):\n+                self.report_error(msg('Cannot write %s to JSON file ', label) + infofn)\n+                return\n+\n     def _write_thumbnails(self, info_dict, filename):\n         if self.params.get('writethumbnail', False):\n             thumbnails = info_dict.get('thumbnails')\ndiff --git a/youtube_dl/__init__.py b/youtube_dl/__init__.py\nindex e1bd67919cf..06bdfb68903 100644\n--- a/youtube_dl/__init__.py\n+++ b/youtube_dl/__init__.py\n@@ -5,7 +5,6 @@\n \n __license__ = 'Public Domain'\n \n-import codecs\n import io\n import os\n import random\n@@ -17,10 +16,12 @@\n )\n from .compat import (\n     compat_getpass,\n+    compat_register_utf8,\n     compat_shlex_split,\n     workaround_optparse_bug9161,\n )\n from .utils import (\n+    _UnsafeExtensionError,\n     DateRange,\n     decodeOption,\n     DEFAULT_OUTTMPL,\n@@ -46,10 +47,8 @@\n \n \n def _real_main(argv=None):\n-    # Compatibility fixes for Windows\n-    if sys.platform == 'win32':\n-        # https://github.com/ytdl-org/youtube-dl/issues/820\n-        codecs.register(lambda name: codecs.lookup('utf-8') if name == 'cp65001' else None)\n+    # Compatibility fix for Windows\n+    compat_register_utf8()\n \n     workaround_optparse_bug9161()\n \n@@ -175,6 +174,9 @@ def _real_main(argv=None):\n     if opts.ap_mso and opts.ap_mso not in MSO_INFO:\n         parser.error('Unsupported TV Provider, use --ap-list-mso to get a list of supported TV Providers')\n \n+    if opts.no_check_extensions:\n+        _UnsafeExtensionError.lenient = True\n+\n     def parse_retries(retries):\n         if retries in ('inf', 'infinite'):\n             parsed_retries = float('inf')\ndiff --git a/youtube_dl/aes.py b/youtube_dl/aes.py\nindex 461bb6d413a..a94a410798b 100644\n--- a/youtube_dl/aes.py\n+++ b/youtube_dl/aes.py\n@@ -8,6 +8,18 @@\n BLOCK_SIZE_BYTES = 16\n \n \n+def pkcs7_padding(data):\n+    \"\"\"\n+    PKCS#7 padding\n+\n+    @param {int[]} data        cleartext\n+    @returns {int[]}           padding data\n+    \"\"\"\n+\n+    remaining_length = BLOCK_SIZE_BYTES - len(data) % BLOCK_SIZE_BYTES\n+    return data + [remaining_length] * remaining_length\n+\n+\n def aes_ctr_decrypt(data, key, counter):\n     \"\"\"\n     Decrypt with aes in counter mode\n@@ -76,8 +88,7 @@ def aes_cbc_encrypt(data, key, iv):\n     previous_cipher_block = iv\n     for i in range(block_count):\n         block = data[i * BLOCK_SIZE_BYTES: (i + 1) * BLOCK_SIZE_BYTES]\n-        remaining_length = BLOCK_SIZE_BYTES - len(block)\n-        block += [remaining_length] * remaining_length\n+        block = pkcs7_padding(block)\n         mixed_block = xor(block, previous_cipher_block)\n \n         encrypted_block = aes_encrypt(mixed_block, expanded_key)\n@@ -88,6 +99,28 @@ def aes_cbc_encrypt(data, key, iv):\n     return encrypted_data\n \n \n+def aes_ecb_encrypt(data, key):\n+    \"\"\"\n+    Encrypt with aes in ECB mode. Using PKCS#7 padding\n+\n+    @param {int[]} data        cleartext\n+    @param {int[]} key         16/24/32-Byte cipher key\n+    @returns {int[]}           encrypted data\n+    \"\"\"\n+    expanded_key = key_expansion(key)\n+    block_count = int(ceil(float(len(data)) / BLOCK_SIZE_BYTES))\n+\n+    encrypted_data = []\n+    for i in range(block_count):\n+        block = data[i * BLOCK_SIZE_BYTES: (i + 1) * BLOCK_SIZE_BYTES]\n+        block = pkcs7_padding(block)\n+\n+        encrypted_block = aes_encrypt(block, expanded_key)\n+        encrypted_data += encrypted_block\n+\n+    return encrypted_data\n+\n+\n def key_expansion(data):\n     \"\"\"\n     Generate key schedule\n@@ -303,7 +336,7 @@ def xor(data1, data2):\n \n \n def rijndael_mul(a, b):\n-    if(a == 0 or b == 0):\n+    if (a == 0 or b == 0):\n         return 0\n     return RIJNDAEL_EXP_TABLE[(RIJNDAEL_LOG_TABLE[a] + RIJNDAEL_LOG_TABLE[b]) % 0xFF]\n \ndiff --git a/youtube_dl/cache.py b/youtube_dl/cache.py\nindex 7bdade1bdb4..54123da0e7e 100644\n--- a/youtube_dl/cache.py\n+++ b/youtube_dl/cache.py\n@@ -1,21 +1,32 @@\n from __future__ import unicode_literals\n \n import errno\n-import io\n import json\n import os\n import re\n import shutil\n import traceback\n \n-from .compat import compat_getenv\n+from .compat import (\n+    compat_getenv,\n+    compat_open as open,\n+)\n from .utils import (\n+    error_to_compat_str,\n     expand_path,\n+    is_outdated_version,\n+    try_get,\n     write_json_file,\n )\n+from .version import __version__\n \n \n class Cache(object):\n+\n+    _YTDL_DIR = 'youtube-dl'\n+    _VERSION_KEY = _YTDL_DIR + '_version'\n+    _DEFAULT_VERSION = '2021.12.17'\n+\n     def __init__(self, ydl):\n         self._ydl = ydl\n \n@@ -23,7 +34,7 @@ def _get_root_dir(self):\n         res = self._ydl.params.get('cachedir')\n         if res is None:\n             cache_root = compat_getenv('XDG_CACHE_HOME', '~/.cache')\n-            res = os.path.join(cache_root, 'youtube-dl')\n+            res = os.path.join(cache_root, self._YTDL_DIR)\n         return expand_path(res)\n \n     def _get_cache_fn(self, section, key, dtype):\n@@ -50,13 +61,22 @@ def store(self, section, key, data, dtype='json'):\n             except OSError as ose:\n                 if ose.errno != errno.EEXIST:\n                     raise\n-            write_json_file(data, fn)\n+            write_json_file({self._VERSION_KEY: __version__, 'data': data}, fn)\n         except Exception:\n             tb = traceback.format_exc()\n             self._ydl.report_warning(\n                 'Writing cache to %r failed: %s' % (fn, tb))\n \n-    def load(self, section, key, dtype='json', default=None):\n+    def _validate(self, data, min_ver):\n+        version = try_get(data, lambda x: x[self._VERSION_KEY])\n+        if not version:  # Backward compatibility\n+            data, version = {'data': data}, self._DEFAULT_VERSION\n+        if not is_outdated_version(version, min_ver or '0', assume_new=False):\n+            return data['data']\n+        self._ydl.to_screen(\n+            'Discarding old cache from version {version} (needs {min_ver})'.format(**locals()))\n+\n+    def load(self, section, key, dtype='json', default=None, min_ver=None):\n         assert dtype in ('json',)\n \n         if not self.enabled:\n@@ -65,13 +85,13 @@ def load(self, section, key, dtype='json', default=None):\n         cache_fn = self._get_cache_fn(section, key, dtype)\n         try:\n             try:\n-                with io.open(cache_fn, 'r', encoding='utf-8') as cachef:\n-                    return json.load(cachef)\n+                with open(cache_fn, 'r', encoding='utf-8') as cachef:\n+                    return self._validate(json.load(cachef), min_ver)\n             except ValueError:\n                 try:\n                     file_size = os.path.getsize(cache_fn)\n                 except (OSError, IOError) as oe:\n-                    file_size = str(oe)\n+                    file_size = error_to_compat_str(oe)\n                 self._ydl.report_warning(\n                     'Cache retrieval from %s failed (%s)' % (cache_fn, file_size))\n         except IOError:\ndiff --git a/youtube_dl/casefold.py b/youtube_dl/casefold.py\nnew file mode 100644\nindex 00000000000..ad9c66f8ede\n--- /dev/null\n+++ b/youtube_dl/casefold.py\n@@ -0,0 +1,1667 @@\n+# coding: utf-8\n+from __future__ import unicode_literals\n+\n+from .compat import (\n+    compat_str,\n+    compat_chr,\n+)\n+\n+# Below is included the text of icu/CaseFolding.txt retrieved from\n+# https://github.com/unicode-org/icu/blob/main/icu4c/source/data/unidata/CaseFolding.txt\n+# In case newly foldable Unicode characters are defined, paste the new version\n+# of the text inside the ''' marks.\n+# The text is expected to have only blank lines andlines with 1st character #,\n+# all ignored, and fold definitions like this:\n+# `from_hex_code; space_separated_to_hex_code_list; comment`\n+\n+_map_str = '''\n+# CaseFolding-15.0.0.txt\n+# Date: 2022-02-02, 23:35:35 GMT\n+# \u00a9 2022 Unicode\u00ae, Inc.\n+# Unicode and the Unicode Logo are registered trademarks of Unicode, Inc. in the U.S. and other countries.\n+# For terms of use, see https://www.unicode.org/terms_of_use.html\n+#\n+# Unicode Character Database\n+#   For documentation, see https://www.unicode.org/reports/tr44/\n+#\n+# Case Folding Properties\n+#\n+# This file is a supplement to the UnicodeData file.\n+# It provides a case folding mapping generated from the Unicode Character Database.\n+# If all characters are mapped according to the full mapping below, then\n+# case differences (according to UnicodeData.txt and SpecialCasing.txt)\n+# are eliminated.\n+#\n+# The data supports both implementations that require simple case foldings\n+# (where string lengths don't change), and implementations that allow full case folding\n+# (where string lengths may grow). Note that where they can be supported, the\n+# full case foldings are superior: for example, they allow \"MASSE\" and \"Ma\u00dfe\" to match.\n+#\n+# All code points not listed in this file map to themselves.\n+#\n+# NOTE: case folding does not preserve normalization formats!\n+#\n+# For information on case folding, including how to have case folding\n+# preserve normalization formats, see Section 3.13 Default Case Algorithms in\n+# The Unicode Standard.\n+#\n+# ================================================================================\n+# Format\n+# ================================================================================\n+# The entries in this file are in the following machine-readable format:\n+#\n+# <code>; <status>; <mapping>; # <name>\n+#\n+# The status field is:\n+# C: common case folding, common mappings shared by both simple and full mappings.\n+# F: full case folding, mappings that cause strings to grow in length. Multiple characters are separated by spaces.\n+# S: simple case folding, mappings to single characters where different from F.\n+# T: special case for uppercase I and dotted uppercase I\n+#    - For non-Turkic languages, this mapping is normally not used.\n+#    - For Turkic languages (tr, az), this mapping can be used instead of the normal mapping for these characters.\n+#      Note that the Turkic mappings do not maintain canonical equivalence without additional processing.\n+#      See the discussions of case mapping in the Unicode Standard for more information.\n+#\n+# Usage:\n+#  A. To do a simple case folding, use the mappings with status C + S.\n+#  B. To do a full case folding, use the mappings with status C + F.\n+#\n+#    The mappings with status T can be used or omitted depending on the desired case-folding\n+#    behavior. (The default option is to exclude them.)\n+#\n+# =================================================================\n+\n+# Property: Case_Folding\n+\n+#  All code points not explicitly listed for Case_Folding\n+#  have the value C for the status field, and the code point itself for the mapping field.\n+\n+# =================================================================\n+0041; C; 0061; # LATIN CAPITAL LETTER A\n+0042; C; 0062; # LATIN CAPITAL LETTER B\n+0043; C; 0063; # LATIN CAPITAL LETTER C\n+0044; C; 0064; # LATIN CAPITAL LETTER D\n+0045; C; 0065; # LATIN CAPITAL LETTER E\n+0046; C; 0066; # LATIN CAPITAL LETTER F\n+0047; C; 0067; # LATIN CAPITAL LETTER G\n+0048; C; 0068; # LATIN CAPITAL LETTER H\n+0049; C; 0069; # LATIN CAPITAL LETTER I\n+0049; T; 0131; # LATIN CAPITAL LETTER I\n+004A; C; 006A; # LATIN CAPITAL LETTER J\n+004B; C; 006B; # LATIN CAPITAL LETTER K\n+004C; C; 006C; # LATIN CAPITAL LETTER L\n+004D; C; 006D; # LATIN CAPITAL LETTER M\n+004E; C; 006E; # LATIN CAPITAL LETTER N\n+004F; C; 006F; # LATIN CAPITAL LETTER O\n+0050; C; 0070; # LATIN CAPITAL LETTER P\n+0051; C; 0071; # LATIN CAPITAL LETTER Q\n+0052; C; 0072; # LATIN CAPITAL LETTER R\n+0053; C; 0073; # LATIN CAPITAL LETTER S\n+0054; C; 0074; # LATIN CAPITAL LETTER T\n+0055; C; 0075; # LATIN CAPITAL LETTER U\n+0056; C; 0076; # LATIN CAPITAL LETTER V\n+0057; C; 0077; # LATIN CAPITAL LETTER W\n+0058; C; 0078; # LATIN CAPITAL LETTER X\n+0059; C; 0079; # LATIN CAPITAL LETTER Y\n+005A; C; 007A; # LATIN CAPITAL LETTER Z\n+00B5; C; 03BC; # MICRO SIGN\n+00C0; C; 00E0; # LATIN CAPITAL LETTER A WITH GRAVE\n+00C1; C; 00E1; # LATIN CAPITAL LETTER A WITH ACUTE\n+00C2; C; 00E2; # LATIN CAPITAL LETTER A WITH CIRCUMFLEX\n+00C3; C; 00E3; # LATIN CAPITAL LETTER A WITH TILDE\n+00C4; C; 00E4; # LATIN CAPITAL LETTER A WITH DIAERESIS\n+00C5; C; 00E5; # LATIN CAPITAL LETTER A WITH RING ABOVE\n+00C6; C; 00E6; # LATIN CAPITAL LETTER AE\n+00C7; C; 00E7; # LATIN CAPITAL LETTER C WITH CEDILLA\n+00C8; C; 00E8; # LATIN CAPITAL LETTER E WITH GRAVE\n+00C9; C; 00E9; # LATIN CAPITAL LETTER E WITH ACUTE\n+00CA; C; 00EA; # LATIN CAPITAL LETTER E WITH CIRCUMFLEX\n+00CB; C; 00EB; # LATIN CAPITAL LETTER E WITH DIAERESIS\n+00CC; C; 00EC; # LATIN CAPITAL LETTER I WITH GRAVE\n+00CD; C; 00ED; # LATIN CAPITAL LETTER I WITH ACUTE\n+00CE; C; 00EE; # LATIN CAPITAL LETTER I WITH CIRCUMFLEX\n+00CF; C; 00EF; # LATIN CAPITAL LETTER I WITH DIAERESIS\n+00D0; C; 00F0; # LATIN CAPITAL LETTER ETH\n+00D1; C; 00F1; # LATIN CAPITAL LETTER N WITH TILDE\n+00D2; C; 00F2; # LATIN CAPITAL LETTER O WITH GRAVE\n+00D3; C; 00F3; # LATIN CAPITAL LETTER O WITH ACUTE\n+00D4; C; 00F4; # LATIN CAPITAL LETTER O WITH CIRCUMFLEX\n+00D5; C; 00F5; # LATIN CAPITAL LETTER O WITH TILDE\n+00D6; C; 00F6; # LATIN CAPITAL LETTER O WITH DIAERESIS\n+00D8; C; 00F8; # LATIN CAPITAL LETTER O WITH STROKE\n+00D9; C; 00F9; # LATIN CAPITAL LETTER U WITH GRAVE\n+00DA; C; 00FA; # LATIN CAPITAL LETTER U WITH ACUTE\n+00DB; C; 00FB; # LATIN CAPITAL LETTER U WITH CIRCUMFLEX\n+00DC; C; 00FC; # LATIN CAPITAL LETTER U WITH DIAERESIS\n+00DD; C; 00FD; # LATIN CAPITAL LETTER Y WITH ACUTE\n+00DE; C; 00FE; # LATIN CAPITAL LETTER THORN\n+00DF; F; 0073 0073; # LATIN SMALL LETTER SHARP S\n+0100; C; 0101; # LATIN CAPITAL LETTER A WITH MACRON\n+0102; C; 0103; # LATIN CAPITAL LETTER A WITH BREVE\n+0104; C; 0105; # LATIN CAPITAL LETTER A WITH OGONEK\n+0106; C; 0107; # LATIN CAPITAL LETTER C WITH ACUTE\n+0108; C; 0109; # LATIN CAPITAL LETTER C WITH CIRCUMFLEX\n+010A; C; 010B; # LATIN CAPITAL LETTER C WITH DOT ABOVE\n+010C; C; 010D; # LATIN CAPITAL LETTER C WITH CARON\n+010E; C; 010F; # LATIN CAPITAL LETTER D WITH CARON\n+0110; C; 0111; # LATIN CAPITAL LETTER D WITH STROKE\n+0112; C; 0113; # LATIN CAPITAL LETTER E WITH MACRON\n+0114; C; 0115; # LATIN CAPITAL LETTER E WITH BREVE\n+0116; C; 0117; # LATIN CAPITAL LETTER E WITH DOT ABOVE\n+0118; C; 0119; # LATIN CAPITAL LETTER E WITH OGONEK\n+011A; C; 011B; # LATIN CAPITAL LETTER E WITH CARON\n+011C; C; 011D; # LATIN CAPITAL LETTER G WITH CIRCUMFLEX\n+011E; C; 011F; # LATIN CAPITAL LETTER G WITH BREVE\n+0120; C; 0121; # LATIN CAPITAL LETTER G WITH DOT ABOVE\n+0122; C; 0123; # LATIN CAPITAL LETTER G WITH CEDILLA\n+0124; C; 0125; # LATIN CAPITAL LETTER H WITH CIRCUMFLEX\n+0126; C; 0127; # LATIN CAPITAL LETTER H WITH STROKE\n+0128; C; 0129; # LATIN CAPITAL LETTER I WITH TILDE\n+012A; C; 012B; # LATIN CAPITAL LETTER I WITH MACRON\n+012C; C; 012D; # LATIN CAPITAL LETTER I WITH BREVE\n+012E; C; 012F; # LATIN CAPITAL LETTER I WITH OGONEK\n+0130; F; 0069 0307; # LATIN CAPITAL LETTER I WITH DOT ABOVE\n+0130; T; 0069; # LATIN CAPITAL LETTER I WITH DOT ABOVE\n+0132; C; 0133; # LATIN CAPITAL LIGATURE IJ\n+0134; C; 0135; # LATIN CAPITAL LETTER J WITH CIRCUMFLEX\n+0136; C; 0137; # LATIN CAPITAL LETTER K WITH CEDILLA\n+0139; C; 013A; # LATIN CAPITAL LETTER L WITH ACUTE\n+013B; C; 013C; # LATIN CAPITAL LETTER L WITH CEDILLA\n+013D; C; 013E; # LATIN CAPITAL LETTER L WITH CARON\n+013F; C; 0140; # LATIN CAPITAL LETTER L WITH MIDDLE DOT\n+0141; C; 0142; # LATIN CAPITAL LETTER L WITH STROKE\n+0143; C; 0144; # LATIN CAPITAL LETTER N WITH ACUTE\n+0145; C; 0146; # LATIN CAPITAL LETTER N WITH CEDILLA\n+0147; C; 0148; # LATIN CAPITAL LETTER N WITH CARON\n+0149; F; 02BC 006E; # LATIN SMALL LETTER N PRECEDED BY APOSTROPHE\n+014A; C; 014B; # LATIN CAPITAL LETTER ENG\n+014C; C; 014D; # LATIN CAPITAL LETTER O WITH MACRON\n+014E; C; 014F; # LATIN CAPITAL LETTER O WITH BREVE\n+0150; C; 0151; # LATIN CAPITAL LETTER O WITH DOUBLE ACUTE\n+0152; C; 0153; # LATIN CAPITAL LIGATURE OE\n+0154; C; 0155; # LATIN CAPITAL LETTER R WITH ACUTE\n+0156; C; 0157; # LATIN CAPITAL LETTER R WITH CEDILLA\n+0158; C; 0159; # LATIN CAPITAL LETTER R WITH CARON\n+015A; C; 015B; # LATIN CAPITAL LETTER S WITH ACUTE\n+015C; C; 015D; # LATIN CAPITAL LETTER S WITH CIRCUMFLEX\n+015E; C; 015F; # LATIN CAPITAL LETTER S WITH CEDILLA\n+0160; C; 0161; # LATIN CAPITAL LETTER S WITH CARON\n+0162; C; 0163; # LATIN CAPITAL LETTER T WITH CEDILLA\n+0164; C; 0165; # LATIN CAPITAL LETTER T WITH CARON\n+0166; C; 0167; # LATIN CAPITAL LETTER T WITH STROKE\n+0168; C; 0169; # LATIN CAPITAL LETTER U WITH TILDE\n+016A; C; 016B; # LATIN CAPITAL LETTER U WITH MACRON\n+016C; C; 016D; # LATIN CAPITAL LETTER U WITH BREVE\n+016E; C; 016F; # LATIN CAPITAL LETTER U WITH RING ABOVE\n+0170; C; 0171; # LATIN CAPITAL LETTER U WITH DOUBLE ACUTE\n+0172; C; 0173; # LATIN CAPITAL LETTER U WITH OGONEK\n+0174; C; 0175; # LATIN CAPITAL LETTER W WITH CIRCUMFLEX\n+0176; C; 0177; # LATIN CAPITAL LETTER Y WITH CIRCUMFLEX\n+0178; C; 00FF; # LATIN CAPITAL LETTER Y WITH DIAERESIS\n+0179; C; 017A; # LATIN CAPITAL LETTER Z WITH ACUTE\n+017B; C; 017C; # LATIN CAPITAL LETTER Z WITH DOT ABOVE\n+017D; C; 017E; # LATIN CAPITAL LETTER Z WITH CARON\n+017F; C; 0073; # LATIN SMALL LETTER LONG S\n+0181; C; 0253; # LATIN CAPITAL LETTER B WITH HOOK\n+0182; C; 0183; # LATIN CAPITAL LETTER B WITH TOPBAR\n+0184; C; 0185; # LATIN CAPITAL LETTER TONE SIX\n+0186; C; 0254; # LATIN CAPITAL LETTER OPEN O\n+0187; C; 0188; # LATIN CAPITAL LETTER C WITH HOOK\n+0189; C; 0256; # LATIN CAPITAL LETTER AFRICAN D\n+018A; C; 0257; # LATIN CAPITAL LETTER D WITH HOOK\n+018B; C; 018C; # LATIN CAPITAL LETTER D WITH TOPBAR\n+018E; C; 01DD; # LATIN CAPITAL LETTER REVERSED E\n+018F; C; 0259; # LATIN CAPITAL LETTER SCHWA\n+0190; C; 025B; # LATIN CAPITAL LETTER OPEN E\n+0191; C; 0192; # LATIN CAPITAL LETTER F WITH HOOK\n+0193; C; 0260; # LATIN CAPITAL LETTER G WITH HOOK\n+0194; C; 0263; # LATIN CAPITAL LETTER GAMMA\n+0196; C; 0269; # LATIN CAPITAL LETTER IOTA\n+0197; C; 0268; # LATIN CAPITAL LETTER I WITH STROKE\n+0198; C; 0199; # LATIN CAPITAL LETTER K WITH HOOK\n+019C; C; 026F; # LATIN CAPITAL LETTER TURNED M\n+019D; C; 0272; # LATIN CAPITAL LETTER N WITH LEFT HOOK\n+019F; C; 0275; # LATIN CAPITAL LETTER O WITH MIDDLE TILDE\n+01A0; C; 01A1; # LATIN CAPITAL LETTER O WITH HORN\n+01A2; C; 01A3; # LATIN CAPITAL LETTER OI\n+01A4; C; 01A5; # LATIN CAPITAL LETTER P WITH HOOK\n+01A6; C; 0280; # LATIN LETTER YR\n+01A7; C; 01A8; # LATIN CAPITAL LETTER TONE TWO\n+01A9; C; 0283; # LATIN CAPITAL LETTER ESH\n+01AC; C; 01AD; # LATIN CAPITAL LETTER T WITH HOOK\n+01AE; C; 0288; # LATIN CAPITAL LETTER T WITH RETROFLEX HOOK\n+01AF; C; 01B0; # LATIN CAPITAL LETTER U WITH HORN\n+01B1; C; 028A; # LATIN CAPITAL LETTER UPSILON\n+01B2; C; 028B; # LATIN CAPITAL LETTER V WITH HOOK\n+01B3; C; 01B4; # LATIN CAPITAL LETTER Y WITH HOOK\n+01B5; C; 01B6; # LATIN CAPITAL LETTER Z WITH STROKE\n+01B7; C; 0292; # LATIN CAPITAL LETTER EZH\n+01B8; C; 01B9; # LATIN CAPITAL LETTER EZH REVERSED\n+01BC; C; 01BD; # LATIN CAPITAL LETTER TONE FIVE\n+01C4; C; 01C6; # LATIN CAPITAL LETTER DZ WITH CARON\n+01C5; C; 01C6; # LATIN CAPITAL LETTER D WITH SMALL LETTER Z WITH CARON\n+01C7; C; 01C9; # LATIN CAPITAL LETTER LJ\n+01C8; C; 01C9; # LATIN CAPITAL LETTER L WITH SMALL LETTER J\n+01CA; C; 01CC; # LATIN CAPITAL LETTER NJ\n+01CB; C; 01CC; # LATIN CAPITAL LETTER N WITH SMALL LETTER J\n+01CD; C; 01CE; # LATIN CAPITAL LETTER A WITH CARON\n+01CF; C; 01D0; # LATIN CAPITAL LETTER I WITH CARON\n+01D1; C; 01D2; # LATIN CAPITAL LETTER O WITH CARON\n+01D3; C; 01D4; # LATIN CAPITAL LETTER U WITH CARON\n+01D5; C; 01D6; # LATIN CAPITAL LETTER U WITH DIAERESIS AND MACRON\n+01D7; C; 01D8; # LATIN CAPITAL LETTER U WITH DIAERESIS AND ACUTE\n+01D9; C; 01DA; # LATIN CAPITAL LETTER U WITH DIAERESIS AND CARON\n+01DB; C; 01DC; # LATIN CAPITAL LETTER U WITH DIAERESIS AND GRAVE\n+01DE; C; 01DF; # LATIN CAPITAL LETTER A WITH DIAERESIS AND MACRON\n+01E0; C; 01E1; # LATIN CAPITAL LETTER A WITH DOT ABOVE AND MACRON\n+01E2; C; 01E3; # LATIN CAPITAL LETTER AE WITH MACRON\n+01E4; C; 01E5; # LATIN CAPITAL LETTER G WITH STROKE\n+01E6; C; 01E7; # LATIN CAPITAL LETTER G WITH CARON\n+01E8; C; 01E9; # LATIN CAPITAL LETTER K WITH CARON\n+01EA; C; 01EB; # LATIN CAPITAL LETTER O WITH OGONEK\n+01EC; C; 01ED; # LATIN CAPITAL LETTER O WITH OGONEK AND MACRON\n+01EE; C; 01EF; # LATIN CAPITAL LETTER EZH WITH CARON\n+01F0; F; 006A 030C; # LATIN SMALL LETTER J WITH CARON\n+01F1; C; 01F3; # LATIN CAPITAL LETTER DZ\n+01F2; C; 01F3; # LATIN CAPITAL LETTER D WITH SMALL LETTER Z\n+01F4; C; 01F5; # LATIN CAPITAL LETTER G WITH ACUTE\n+01F6; C; 0195; # LATIN CAPITAL LETTER HWAIR\n+01F7; C; 01BF; # LATIN CAPITAL LETTER WYNN\n+01F8; C; 01F9; # LATIN CAPITAL LETTER N WITH GRAVE\n+01FA; C; 01FB; # LATIN CAPITAL LETTER A WITH RING ABOVE AND ACUTE\n+01FC; C; 01FD; # LATIN CAPITAL LETTER AE WITH ACUTE\n+01FE; C; 01FF; # LATIN CAPITAL LETTER O WITH STROKE AND ACUTE\n+0200; C; 0201; # LATIN CAPITAL LETTER A WITH DOUBLE GRAVE\n+0202; C; 0203; # LATIN CAPITAL LETTER A WITH INVERTED BREVE\n+0204; C; 0205; # LATIN CAPITAL LETTER E WITH DOUBLE GRAVE\n+0206; C; 0207; # LATIN CAPITAL LETTER E WITH INVERTED BREVE\n+0208; C; 0209; # LATIN CAPITAL LETTER I WITH DOUBLE GRAVE\n+020A; C; 020B; # LATIN CAPITAL LETTER I WITH INVERTED BREVE\n+020C; C; 020D; # LATIN CAPITAL LETTER O WITH DOUBLE GRAVE\n+020E; C; 020F; # LATIN CAPITAL LETTER O WITH INVERTED BREVE\n+0210; C; 0211; # LATIN CAPITAL LETTER R WITH DOUBLE GRAVE\n+0212; C; 0213; # LATIN CAPITAL LETTER R WITH INVERTED BREVE\n+0214; C; 0215; # LATIN CAPITAL LETTER U WITH DOUBLE GRAVE\n+0216; C; 0217; # LATIN CAPITAL LETTER U WITH INVERTED BREVE\n+0218; C; 0219; # LATIN CAPITAL LETTER S WITH COMMA BELOW\n+021A; C; 021B; # LATIN CAPITAL LETTER T WITH COMMA BELOW\n+021C; C; 021D; # LATIN CAPITAL LETTER YOGH\n+021E; C; 021F; # LATIN CAPITAL LETTER H WITH CARON\n+0220; C; 019E; # LATIN CAPITAL LETTER N WITH LONG RIGHT LEG\n+0222; C; 0223; # LATIN CAPITAL LETTER OU\n+0224; C; 0225; # LATIN CAPITAL LETTER Z WITH HOOK\n+0226; C; 0227; # LATIN CAPITAL LETTER A WITH DOT ABOVE\n+0228; C; 0229; # LATIN CAPITAL LETTER E WITH CEDILLA\n+022A; C; 022B; # LATIN CAPITAL LETTER O WITH DIAERESIS AND MACRON\n+022C; C; 022D; # LATIN CAPITAL LETTER O WITH TILDE AND MACRON\n+022E; C; 022F; # LATIN CAPITAL LETTER O WITH DOT ABOVE\n+0230; C; 0231; # LATIN CAPITAL LETTER O WITH DOT ABOVE AND MACRON\n+0232; C; 0233; # LATIN CAPITAL LETTER Y WITH MACRON\n+023A; C; 2C65; # LATIN CAPITAL LETTER A WITH STROKE\n+023B; C; 023C; # LATIN CAPITAL LETTER C WITH STROKE\n+023D; C; 019A; # LATIN CAPITAL LETTER L WITH BAR\n+023E; C; 2C66; # LATIN CAPITAL LETTER T WITH DIAGONAL STROKE\n+0241; C; 0242; # LATIN CAPITAL LETTER GLOTTAL STOP\n+0243; C; 0180; # LATIN CAPITAL LETTER B WITH STROKE\n+0244; C; 0289; # LATIN CAPITAL LETTER U BAR\n+0245; C; 028C; # LATIN CAPITAL LETTER TURNED V\n+0246; C; 0247; # LATIN CAPITAL LETTER E WITH STROKE\n+0248; C; 0249; # LATIN CAPITAL LETTER J WITH STROKE\n+024A; C; 024B; # LATIN CAPITAL LETTER SMALL Q WITH HOOK TAIL\n+024C; C; 024D; # LATIN CAPITAL LETTER R WITH STROKE\n+024E; C; 024F; # LATIN CAPITAL LETTER Y WITH STROKE\n+0345; C; 03B9; # COMBINING GREEK YPOGEGRAMMENI\n+0370; C; 0371; # GREEK CAPITAL LETTER HETA\n+0372; C; 0373; # GREEK CAPITAL LETTER ARCHAIC SAMPI\n+0376; C; 0377; # GREEK CAPITAL LETTER PAMPHYLIAN DIGAMMA\n+037F; C; 03F3; # GREEK CAPITAL LETTER YOT\n+0386; C; 03AC; # GREEK CAPITAL LETTER ALPHA WITH TONOS\n+0388; C; 03AD; # GREEK CAPITAL LETTER EPSILON WITH TONOS\n+0389; C; 03AE; # GREEK CAPITAL LETTER ETA WITH TONOS\n+038A; C; 03AF; # GREEK CAPITAL LETTER IOTA WITH TONOS\n+038C; C; 03CC; # GREEK CAPITAL LETTER OMICRON WITH TONOS\n+038E; C; 03CD; # GREEK CAPITAL LETTER UPSILON WITH TONOS\n+038F; C; 03CE; # GREEK CAPITAL LETTER OMEGA WITH TONOS\n+0390; F; 03B9 0308 0301; # GREEK SMALL LETTER IOTA WITH DIALYTIKA AND TONOS\n+0391; C; 03B1; # GREEK CAPITAL LETTER ALPHA\n+0392; C; 03B2; # GREEK CAPITAL LETTER BETA\n+0393; C; 03B3; # GREEK CAPITAL LETTER GAMMA\n+0394; C; 03B4; # GREEK CAPITAL LETTER DELTA\n+0395; C; 03B5; # GREEK CAPITAL LETTER EPSILON\n+0396; C; 03B6; # GREEK CAPITAL LETTER ZETA\n+0397; C; 03B7; # GREEK CAPITAL LETTER ETA\n+0398; C; 03B8; # GREEK CAPITAL LETTER THETA\n+0399; C; 03B9; # GREEK CAPITAL LETTER IOTA\n+039A; C; 03BA; # GREEK CAPITAL LETTER KAPPA\n+039B; C; 03BB; # GREEK CAPITAL LETTER LAMDA\n+039C; C; 03BC; # GREEK CAPITAL LETTER MU\n+039D; C; 03BD; # GREEK CAPITAL LETTER NU\n+039E; C; 03BE; # GREEK CAPITAL LETTER XI\n+039F; C; 03BF; # GREEK CAPITAL LETTER OMICRON\n+03A0; C; 03C0; # GREEK CAPITAL LETTER PI\n+03A1; C; 03C1; # GREEK CAPITAL LETTER RHO\n+03A3; C; 03C3; # GREEK CAPITAL LETTER SIGMA\n+03A4; C; 03C4; # GREEK CAPITAL LETTER TAU\n+03A5; C; 03C5; # GREEK CAPITAL LETTER UPSILON\n+03A6; C; 03C6; # GREEK CAPITAL LETTER PHI\n+03A7; C; 03C7; # GREEK CAPITAL LETTER CHI\n+03A8; C; 03C8; # GREEK CAPITAL LETTER PSI\n+03A9; C; 03C9; # GREEK CAPITAL LETTER OMEGA\n+03AA; C; 03CA; # GREEK CAPITAL LETTER IOTA WITH DIALYTIKA\n+03AB; C; 03CB; # GREEK CAPITAL LETTER UPSILON WITH DIALYTIKA\n+03B0; F; 03C5 0308 0301; # GREEK SMALL LETTER UPSILON WITH DIALYTIKA AND TONOS\n+03C2; C; 03C3; # GREEK SMALL LETTER FINAL SIGMA\n+03CF; C; 03D7; # GREEK CAPITAL KAI SYMBOL\n+03D0; C; 03B2; # GREEK BETA SYMBOL\n+03D1; C; 03B8; # GREEK THETA SYMBOL\n+03D5; C; 03C6; # GREEK PHI SYMBOL\n+03D6; C; 03C0; # GREEK PI SYMBOL\n+03D8; C; 03D9; # GREEK LETTER ARCHAIC KOPPA\n+03DA; C; 03DB; # GREEK LETTER STIGMA\n+03DC; C; 03DD; # GREEK LETTER DIGAMMA\n+03DE; C; 03DF; # GREEK LETTER KOPPA\n+03E0; C; 03E1; # GREEK LETTER SAMPI\n+03E2; C; 03E3; # COPTIC CAPITAL LETTER SHEI\n+03E4; C; 03E5; # COPTIC CAPITAL LETTER FEI\n+03E6; C; 03E7; # COPTIC CAPITAL LETTER KHEI\n+03E8; C; 03E9; # COPTIC CAPITAL LETTER HORI\n+03EA; C; 03EB; # COPTIC CAPITAL LETTER GANGIA\n+03EC; C; 03ED; # COPTIC CAPITAL LETTER SHIMA\n+03EE; C; 03EF; # COPTIC CAPITAL LETTER DEI\n+03F0; C; 03BA; # GREEK KAPPA SYMBOL\n+03F1; C; 03C1; # GREEK RHO SYMBOL\n+03F4; C; 03B8; # GREEK CAPITAL THETA SYMBOL\n+03F5; C; 03B5; # GREEK LUNATE EPSILON SYMBOL\n+03F7; C; 03F8; # GREEK CAPITAL LETTER SHO\n+03F9; C; 03F2; # GREEK CAPITAL LUNATE SIGMA SYMBOL\n+03FA; C; 03FB; # GREEK CAPITAL LETTER SAN\n+03FD; C; 037B; # GREEK CAPITAL REVERSED LUNATE SIGMA SYMBOL\n+03FE; C; 037C; # GREEK CAPITAL DOTTED LUNATE SIGMA SYMBOL\n+03FF; C; 037D; # GREEK CAPITAL REVERSED DOTTED LUNATE SIGMA SYMBOL\n+0400; C; 0450; # CYRILLIC CAPITAL LETTER IE WITH GRAVE\n+0401; C; 0451; # CYRILLIC CAPITAL LETTER IO\n+0402; C; 0452; # CYRILLIC CAPITAL LETTER DJE\n+0403; C; 0453; # CYRILLIC CAPITAL LETTER GJE\n+0404; C; 0454; # CYRILLIC CAPITAL LETTER UKRAINIAN IE\n+0405; C; 0455; # CYRILLIC CAPITAL LETTER DZE\n+0406; C; 0456; # CYRILLIC CAPITAL LETTER BYELORUSSIAN-UKRAINIAN I\n+0407; C; 0457; # CYRILLIC CAPITAL LETTER YI\n+0408; C; 0458; # CYRILLIC CAPITAL LETTER JE\n+0409; C; 0459; # CYRILLIC CAPITAL LETTER LJE\n+040A; C; 045A; # CYRILLIC CAPITAL LETTER NJE\n+040B; C; 045B; # CYRILLIC CAPITAL LETTER TSHE\n+040C; C; 045C; # CYRILLIC CAPITAL LETTER KJE\n+040D; C; 045D; # CYRILLIC CAPITAL LETTER I WITH GRAVE\n+040E; C; 045E; # CYRILLIC CAPITAL LETTER SHORT U\n+040F; C; 045F; # CYRILLIC CAPITAL LETTER DZHE\n+0410; C; 0430; # CYRILLIC CAPITAL LETTER A\n+0411; C; 0431; # CYRILLIC CAPITAL LETTER BE\n+0412; C; 0432; # CYRILLIC CAPITAL LETTER VE\n+0413; C; 0433; # CYRILLIC CAPITAL LETTER GHE\n+0414; C; 0434; # CYRILLIC CAPITAL LETTER DE\n+0415; C; 0435; # CYRILLIC CAPITAL LETTER IE\n+0416; C; 0436; # CYRILLIC CAPITAL LETTER ZHE\n+0417; C; 0437; # CYRILLIC CAPITAL LETTER ZE\n+0418; C; 0438; # CYRILLIC CAPITAL LETTER I\n+0419; C; 0439; # CYRILLIC CAPITAL LETTER SHORT I\n+041A; C; 043A; # CYRILLIC CAPITAL LETTER KA\n+041B; C; 043B; # CYRILLIC CAPITAL LETTER EL\n+041C; C; 043C; # CYRILLIC CAPITAL LETTER EM\n+041D; C; 043D; # CYRILLIC CAPITAL LETTER EN\n+041E; C; 043E; # CYRILLIC CAPITAL LETTER O\n+041F; C; 043F; # CYRILLIC CAPITAL LETTER PE\n+0420; C; 0440; # CYRILLIC CAPITAL LETTER ER\n+0421; C; 0441; # CYRILLIC CAPITAL LETTER ES\n+0422; C; 0442; # CYRILLIC CAPITAL LETTER TE\n+0423; C; 0443; # CYRILLIC CAPITAL LETTER U\n+0424; C; 0444; # CYRILLIC CAPITAL LETTER EF\n+0425; C; 0445; # CYRILLIC CAPITAL LETTER HA\n+0426; C; 0446; # CYRILLIC CAPITAL LETTER TSE\n+0427; C; 0447; # CYRILLIC CAPITAL LETTER CHE\n+0428; C; 0448; # CYRILLIC CAPITAL LETTER SHA\n+0429; C; 0449; # CYRILLIC CAPITAL LETTER SHCHA\n+042A; C; 044A; # CYRILLIC CAPITAL LETTER HARD SIGN\n+042B; C; 044B; # CYRILLIC CAPITAL LETTER YERU\n+042C; C; 044C; # CYRILLIC CAPITAL LETTER SOFT SIGN\n+042D; C; 044D; # CYRILLIC CAPITAL LETTER E\n+042E; C; 044E; # CYRILLIC CAPITAL LETTER YU\n+042F; C; 044F; # CYRILLIC CAPITAL LETTER YA\n+0460; C; 0461; # CYRILLIC CAPITAL LETTER OMEGA\n+0462; C; 0463; # CYRILLIC CAPITAL LETTER YAT\n+0464; C; 0465; # CYRILLIC CAPITAL LETTER IOTIFIED E\n+0466; C; 0467; # CYRILLIC CAPITAL LETTER LITTLE YUS\n+0468; C; 0469; # CYRILLIC CAPITAL LETTER IOTIFIED LITTLE YUS\n+046A; C; 046B; # CYRILLIC CAPITAL LETTER BIG YUS\n+046C; C; 046D; # CYRILLIC CAPITAL LETTER IOTIFIED BIG YUS\n+046E; C; 046F; # CYRILLIC CAPITAL LETTER KSI\n+0470; C; 0471; # CYRILLIC CAPITAL LETTER PSI\n+0472; C; 0473; # CYRILLIC CAPITAL LETTER FITA\n+0474; C; 0475; # CYRILLIC CAPITAL LETTER IZHITSA\n+0476; C; 0477; # CYRILLIC CAPITAL LETTER IZHITSA WITH DOUBLE GRAVE ACCENT\n+0478; C; 0479; # CYRILLIC CAPITAL LETTER UK\n+047A; C; 047B; # CYRILLIC CAPITAL LETTER ROUND OMEGA\n+047C; C; 047D; # CYRILLIC CAPITAL LETTER OMEGA WITH TITLO\n+047E; C; 047F; # CYRILLIC CAPITAL LETTER OT\n+0480; C; 0481; # CYRILLIC CAPITAL LETTER KOPPA\n+048A; C; 048B; # CYRILLIC CAPITAL LETTER SHORT I WITH TAIL\n+048C; C; 048D; # CYRILLIC CAPITAL LETTER SEMISOFT SIGN\n+048E; C; 048F; # CYRILLIC CAPITAL LETTER ER WITH TICK\n+0490; C; 0491; # CYRILLIC CAPITAL LETTER GHE WITH UPTURN\n+0492; C; 0493; # CYRILLIC CAPITAL LETTER GHE WITH STROKE\n+0494; C; 0495; # CYRILLIC CAPITAL LETTER GHE WITH MIDDLE HOOK\n+0496; C; 0497; # CYRILLIC CAPITAL LETTER ZHE WITH DESCENDER\n+0498; C; 0499; # CYRILLIC CAPITAL LETTER ZE WITH DESCENDER\n+049A; C; 049B; # CYRILLIC CAPITAL LETTER KA WITH DESCENDER\n+049C; C; 049D; # CYRILLIC CAPITAL LETTER KA WITH VERTICAL STROKE\n+049E; C; 049F; # CYRILLIC CAPITAL LETTER KA WITH STROKE\n+04A0; C; 04A1; # CYRILLIC CAPITAL LETTER BASHKIR KA\n+04A2; C; 04A3; # CYRILLIC CAPITAL LETTER EN WITH DESCENDER\n+04A4; C; 04A5; # CYRILLIC CAPITAL LIGATURE EN GHE\n+04A6; C; 04A7; # CYRILLIC CAPITAL LETTER PE WITH MIDDLE HOOK\n+04A8; C; 04A9; # CYRILLIC CAPITAL LETTER ABKHASIAN HA\n+04AA; C; 04AB; # CYRILLIC CAPITAL LETTER ES WITH DESCENDER\n+04AC; C; 04AD; # CYRILLIC CAPITAL LETTER TE WITH DESCENDER\n+04AE; C; 04AF; # CYRILLIC CAPITAL LETTER STRAIGHT U\n+04B0; C; 04B1; # CYRILLIC CAPITAL LETTER STRAIGHT U WITH STROKE\n+04B2; C; 04B3; # CYRILLIC CAPITAL LETTER HA WITH DESCENDER\n+04B4; C; 04B5; # CYRILLIC CAPITAL LIGATURE TE TSE\n+04B6; C; 04B7; # CYRILLIC CAPITAL LETTER CHE WITH DESCENDER\n+04B8; C; 04B9; # CYRILLIC CAPITAL LETTER CHE WITH VERTICAL STROKE\n+04BA; C; 04BB; # CYRILLIC CAPITAL LETTER SHHA\n+04BC; C; 04BD; # CYRILLIC CAPITAL LETTER ABKHASIAN CHE\n+04BE; C; 04BF; # CYRILLIC CAPITAL LETTER ABKHASIAN CHE WITH DESCENDER\n+04C0; C; 04CF; # CYRILLIC LETTER PALOCHKA\n+04C1; C; 04C2; # CYRILLIC CAPITAL LETTER ZHE WITH BREVE\n+04C3; C; 04C4; # CYRILLIC CAPITAL LETTER KA WITH HOOK\n+04C5; C; 04C6; # CYRILLIC CAPITAL LETTER EL WITH TAIL\n+04C7; C; 04C8; # CYRILLIC CAPITAL LETTER EN WITH HOOK\n+04C9; C; 04CA; # CYRILLIC CAPITAL LETTER EN WITH TAIL\n+04CB; C; 04CC; # CYRILLIC CAPITAL LETTER KHAKASSIAN CHE\n+04CD; C; 04CE; # CYRILLIC CAPITAL LETTER EM WITH TAIL\n+04D0; C; 04D1; # CYRILLIC CAPITAL LETTER A WITH BREVE\n+04D2; C; 04D3; # CYRILLIC CAPITAL LETTER A WITH DIAERESIS\n+04D4; C; 04D5; # CYRILLIC CAPITAL LIGATURE A IE\n+04D6; C; 04D7; # CYRILLIC CAPITAL LETTER IE WITH BREVE\n+04D8; C; 04D9; # CYRILLIC CAPITAL LETTER SCHWA\n+04DA; C; 04DB; # CYRILLIC CAPITAL LETTER SCHWA WITH DIAERESIS\n+04DC; C; 04DD; # CYRILLIC CAPITAL LETTER ZHE WITH DIAERESIS\n+04DE; C; 04DF; # CYRILLIC CAPITAL LETTER ZE WITH DIAERESIS\n+04E0; C; 04E1; # CYRILLIC CAPITAL LETTER ABKHASIAN DZE\n+04E2; C; 04E3; # CYRILLIC CAPITAL LETTER I WITH MACRON\n+04E4; C; 04E5; # CYRILLIC CAPITAL LETTER I WITH DIAERESIS\n+04E6; C; 04E7; # CYRILLIC CAPITAL LETTER O WITH DIAERESIS\n+04E8; C; 04E9; # CYRILLIC CAPITAL LETTER BARRED O\n+04EA; C; 04EB; # CYRILLIC CAPITAL LETTER BARRED O WITH DIAERESIS\n+04EC; C; 04ED; # CYRILLIC CAPITAL LETTER E WITH DIAERESIS\n+04EE; C; 04EF; # CYRILLIC CAPITAL LETTER U WITH MACRON\n+04F0; C; 04F1; # CYRILLIC CAPITAL LETTER U WITH DIAERESIS\n+04F2; C; 04F3; # CYRILLIC CAPITAL LETTER U WITH DOUBLE ACUTE\n+04F4; C; 04F5; # CYRILLIC CAPITAL LETTER CHE WITH DIAERESIS\n+04F6; C; 04F7; # CYRILLIC CAPITAL LETTER GHE WITH DESCENDER\n+04F8; C; 04F9; # CYRILLIC CAPITAL LETTER YERU WITH DIAERESIS\n+04FA; C; 04FB; # CYRILLIC CAPITAL LETTER GHE WITH STROKE AND HOOK\n+04FC; C; 04FD; # CYRILLIC CAPITAL LETTER HA WITH HOOK\n+04FE; C; 04FF; # CYRILLIC CAPITAL LETTER HA WITH STROKE\n+0500; C; 0501; # CYRILLIC CAPITAL LETTER KOMI DE\n+0502; C; 0503; # CYRILLIC CAPITAL LETTER KOMI DJE\n+0504; C; 0505; # CYRILLIC CAPITAL LETTER KOMI ZJE\n+0506; C; 0507; # CYRILLIC CAPITAL LETTER KOMI DZJE\n+0508; C; 0509; # CYRILLIC CAPITAL LETTER KOMI LJE\n+050A; C; 050B; # CYRILLIC CAPITAL LETTER KOMI NJE\n+050C; C; 050D; # CYRILLIC CAPITAL LETTER KOMI SJE\n+050E; C; 050F; # CYRILLIC CAPITAL LETTER KOMI TJE\n+0510; C; 0511; # CYRILLIC CAPITAL LETTER REVERSED ZE\n+0512; C; 0513; # CYRILLIC CAPITAL LETTER EL WITH HOOK\n+0514; C; 0515; # CYRILLIC CAPITAL LETTER LHA\n+0516; C; 0517; # CYRILLIC CAPITAL LETTER RHA\n+0518; C; 0519; # CYRILLIC CAPITAL LETTER YAE\n+051A; C; 051B; # CYRILLIC CAPITAL LETTER QA\n+051C; C; 051D; # CYRILLIC CAPITAL LETTER WE\n+051E; C; 051F; # CYRILLIC CAPITAL LETTER ALEUT KA\n+0520; C; 0521; # CYRILLIC CAPITAL LETTER EL WITH MIDDLE HOOK\n+0522; C; 0523; # CYRILLIC CAPITAL LETTER EN WITH MIDDLE HOOK\n+0524; C; 0525; # CYRILLIC CAPITAL LETTER PE WITH DESCENDER\n+0526; C; 0527; # CYRILLIC CAPITAL LETTER SHHA WITH DESCENDER\n+0528; C; 0529; # CYRILLIC CAPITAL LETTER EN WITH LEFT HOOK\n+052A; C; 052B; # CYRILLIC CAPITAL LETTER DZZHE\n+052C; C; 052D; # CYRILLIC CAPITAL LETTER DCHE\n+052E; C; 052F; # CYRILLIC CAPITAL LETTER EL WITH DESCENDER\n+0531; C; 0561; # ARMENIAN CAPITAL LETTER AYB\n+0532; C; 0562; # ARMENIAN CAPITAL LETTER BEN\n+0533; C; 0563; # ARMENIAN CAPITAL LETTER GIM\n+0534; C; 0564; # ARMENIAN CAPITAL LETTER DA\n+0535; C; 0565; # ARMENIAN CAPITAL LETTER ECH\n+0536; C; 0566; # ARMENIAN CAPITAL LETTER ZA\n+0537; C; 0567; # ARMENIAN CAPITAL LETTER EH\n+0538; C; 0568; # ARMENIAN CAPITAL LETTER ET\n+0539; C; 0569; # ARMENIAN CAPITAL LETTER TO\n+053A; C; 056A; # ARMENIAN CAPITAL LETTER ZHE\n+053B; C; 056B; # ARMENIAN CAPITAL LETTER INI\n+053C; C; 056C; # ARMENIAN CAPITAL LETTER LIWN\n+053D; C; 056D; # ARMENIAN CAPITAL LETTER XEH\n+053E; C; 056E; # ARMENIAN CAPITAL LETTER CA\n+053F; C; 056F; # ARMENIAN CAPITAL LETTER KEN\n+0540; C; 0570; # ARMENIAN CAPITAL LETTER HO\n+0541; C; 0571; # ARMENIAN CAPITAL LETTER JA\n+0542; C; 0572; # ARMENIAN CAPITAL LETTER GHAD\n+0543; C; 0573; # ARMENIAN CAPITAL LETTER CHEH\n+0544; C; 0574; # ARMENIAN CAPITAL LETTER MEN\n+0545; C; 0575; # ARMENIAN CAPITAL LETTER YI\n+0546; C; 0576; # ARMENIAN CAPITAL LETTER NOW\n+0547; C; 0577; # ARMENIAN CAPITAL LETTER SHA\n+0548; C; 0578; # ARMENIAN CAPITAL LETTER VO\n+0549; C; 0579; # ARMENIAN CAPITAL LETTER CHA\n+054A; C; 057A; # ARMENIAN CAPITAL LETTER PEH\n+054B; C; 057B; # ARMENIAN CAPITAL LETTER JHEH\n+054C; C; 057C; # ARMENIAN CAPITAL LETTER RA\n+054D; C; 057D; # ARMENIAN CAPITAL LETTER SEH\n+054E; C; 057E; # ARMENIAN CAPITAL LETTER VEW\n+054F; C; 057F; # ARMENIAN CAPITAL LETTER TIWN\n+0550; C; 0580; # ARMENIAN CAPITAL LETTER REH\n+0551; C; 0581; # ARMENIAN CAPITAL LETTER CO\n+0552; C; 0582; # ARMENIAN CAPITAL LETTER YIWN\n+0553; C; 0583; # ARMENIAN CAPITAL LETTER PIWR\n+0554; C; 0584; # ARMENIAN CAPITAL LETTER KEH\n+0555; C; 0585; # ARMENIAN CAPITAL LETTER OH\n+0556; C; 0586; # ARMENIAN CAPITAL LETTER FEH\n+0587; F; 0565 0582; # ARMENIAN SMALL LIGATURE ECH YIWN\n+10A0; C; 2D00; # GEORGIAN CAPITAL LETTER AN\n+10A1; C; 2D01; # GEORGIAN CAPITAL LETTER BAN\n+10A2; C; 2D02; # GEORGIAN CAPITAL LETTER GAN\n+10A3; C; 2D03; # GEORGIAN CAPITAL LETTER DON\n+10A4; C; 2D04; # GEORGIAN CAPITAL LETTER EN\n+10A5; C; 2D05; # GEORGIAN CAPITAL LETTER VIN\n+10A6; C; 2D06; # GEORGIAN CAPITAL LETTER ZEN\n+10A7; C; 2D07; # GEORGIAN CAPITAL LETTER TAN\n+10A8; C; 2D08; # GEORGIAN CAPITAL LETTER IN\n+10A9; C; 2D09; # GEORGIAN CAPITAL LETTER KAN\n+10AA; C; 2D0A; # GEORGIAN CAPITAL LETTER LAS\n+10AB; C; 2D0B; # GEORGIAN CAPITAL LETTER MAN\n+10AC; C; 2D0C; # GEORGIAN CAPITAL LETTER NAR\n+10AD; C; 2D0D; # GEORGIAN CAPITAL LETTER ON\n+10AE; C; 2D0E; # GEORGIAN CAPITAL LETTER PAR\n+10AF; C; 2D0F; # GEORGIAN CAPITAL LETTER ZHAR\n+10B0; C; 2D10; # GEORGIAN CAPITAL LETTER RAE\n+10B1; C; 2D11; # GEORGIAN CAPITAL LETTER SAN\n+10B2; C; 2D12; # GEORGIAN CAPITAL LETTER TAR\n+10B3; C; 2D13; # GEORGIAN CAPITAL LETTER UN\n+10B4; C; 2D14; # GEORGIAN CAPITAL LETTER PHAR\n+10B5; C; 2D15; # GEORGIAN CAPITAL LETTER KHAR\n+10B6; C; 2D16; # GEORGIAN CAPITAL LETTER GHAN\n+10B7; C; 2D17; # GEORGIAN CAPITAL LETTER QAR\n+10B8; C; 2D18; # GEORGIAN CAPITAL LETTER SHIN\n+10B9; C; 2D19; # GEORGIAN CAPITAL LETTER CHIN\n+10BA; C; 2D1A; # GEORGIAN CAPITAL LETTER CAN\n+10BB; C; 2D1B; # GEORGIAN CAPITAL LETTER JIL\n+10BC; C; 2D1C; # GEORGIAN CAPITAL LETTER CIL\n+10BD; C; 2D1D; # GEORGIAN CAPITAL LETTER CHAR\n+10BE; C; 2D1E; # GEORGIAN CAPITAL LETTER XAN\n+10BF; C; 2D1F; # GEORGIAN CAPITAL LETTER JHAN\n+10C0; C; 2D20; # GEORGIAN CAPITAL LETTER HAE\n+10C1; C; 2D21; # GEORGIAN CAPITAL LETTER HE\n+10C2; C; 2D22; # GEORGIAN CAPITAL LETTER HIE\n+10C3; C; 2D23; # GEORGIAN CAPITAL LETTER WE\n+10C4; C; 2D24; # GEORGIAN CAPITAL LETTER HAR\n+10C5; C; 2D25; # GEORGIAN CAPITAL LETTER HOE\n+10C7; C; 2D27; # GEORGIAN CAPITAL LETTER YN\n+10CD; C; 2D2D; # GEORGIAN CAPITAL LETTER AEN\n+13F8; C; 13F0; # CHEROKEE SMALL LETTER YE\n+13F9; C; 13F1; # CHEROKEE SMALL LETTER YI\n+13FA; C; 13F2; # CHEROKEE SMALL LETTER YO\n+13FB; C; 13F3; # CHEROKEE SMALL LETTER YU\n+13FC; C; 13F4; # CHEROKEE SMALL LETTER YV\n+13FD; C; 13F5; # CHEROKEE SMALL LETTER MV\n+1C80; C; 0432; # CYRILLIC SMALL LETTER ROUNDED VE\n+1C81; C; 0434; # CYRILLIC SMALL LETTER LONG-LEGGED DE\n+1C82; C; 043E; # CYRILLIC SMALL LETTER NARROW O\n+1C83; C; 0441; # CYRILLIC SMALL LETTER WIDE ES\n+1C84; C; 0442; # CYRILLIC SMALL LETTER TALL TE\n+1C85; C; 0442; # CYRILLIC SMALL LETTER THREE-LEGGED TE\n+1C86; C; 044A; # CYRILLIC SMALL LETTER TALL HARD SIGN\n+1C87; C; 0463; # CYRILLIC SMALL LETTER TALL YAT\n+1C88; C; A64B; # CYRILLIC SMALL LETTER UNBLENDED UK\n+1C90; C; 10D0; # GEORGIAN MTAVRULI CAPITAL LETTER AN\n+1C91; C; 10D1; # GEORGIAN MTAVRULI CAPITAL LETTER BAN\n+1C92; C; 10D2; # GEORGIAN MTAVRULI CAPITAL LETTER GAN\n+1C93; C; 10D3; # GEORGIAN MTAVRULI CAPITAL LETTER DON\n+1C94; C; 10D4; # GEORGIAN MTAVRULI CAPITAL LETTER EN\n+1C95; C; 10D5; # GEORGIAN MTAVRULI CAPITAL LETTER VIN\n+1C96; C; 10D6; # GEORGIAN MTAVRULI CAPITAL LETTER ZEN\n+1C97; C; 10D7; # GEORGIAN MTAVRULI CAPITAL LETTER TAN\n+1C98; C; 10D8; # GEORGIAN MTAVRULI CAPITAL LETTER IN\n+1C99; C; 10D9; # GEORGIAN MTAVRULI CAPITAL LETTER KAN\n+1C9A; C; 10DA; # GEORGIAN MTAVRULI CAPITAL LETTER LAS\n+1C9B; C; 10DB; # GEORGIAN MTAVRULI CAPITAL LETTER MAN\n+1C9C; C; 10DC; # GEORGIAN MTAVRULI CAPITAL LETTER NAR\n+1C9D; C; 10DD; # GEORGIAN MTAVRULI CAPITAL LETTER ON\n+1C9E; C; 10DE; # GEORGIAN MTAVRULI CAPITAL LETTER PAR\n+1C9F; C; 10DF; # GEORGIAN MTAVRULI CAPITAL LETTER ZHAR\n+1CA0; C; 10E0; # GEORGIAN MTAVRULI CAPITAL LETTER RAE\n+1CA1; C; 10E1; # GEORGIAN MTAVRULI CAPITAL LETTER SAN\n+1CA2; C; 10E2; # GEORGIAN MTAVRULI CAPITAL LETTER TAR\n+1CA3; C; 10E3; # GEORGIAN MTAVRULI CAPITAL LETTER UN\n+1CA4; C; 10E4; # GEORGIAN MTAVRULI CAPITAL LETTER PHAR\n+1CA5; C; 10E5; # GEORGIAN MTAVRULI CAPITAL LETTER KHAR\n+1CA6; C; 10E6; # GEORGIAN MTAVRULI CAPITAL LETTER GHAN\n+1CA7; C; 10E7; # GEORGIAN MTAVRULI CAPITAL LETTER QAR\n+1CA8; C; 10E8; # GEORGIAN MTAVRULI CAPITAL LETTER SHIN\n+1CA9; C; 10E9; # GEORGIAN MTAVRULI CAPITAL LETTER CHIN\n+1CAA; C; 10EA; # GEORGIAN MTAVRULI CAPITAL LETTER CAN\n+1CAB; C; 10EB; # GEORGIAN MTAVRULI CAPITAL LETTER JIL\n+1CAC; C; 10EC; # GEORGIAN MTAVRULI CAPITAL LETTER CIL\n+1CAD; C; 10ED; # GEORGIAN MTAVRULI CAPITAL LETTER CHAR\n+1CAE; C; 10EE; # GEORGIAN MTAVRULI CAPITAL LETTER XAN\n+1CAF; C; 10EF; # GEORGIAN MTAVRULI CAPITAL LETTER JHAN\n+1CB0; C; 10F0; # GEORGIAN MTAVRULI CAPITAL LETTER HAE\n+1CB1; C; 10F1; # GEORGIAN MTAVRULI CAPITAL LETTER HE\n+1CB2; C; 10F2; # GEORGIAN MTAVRULI CAPITAL LETTER HIE\n+1CB3; C; 10F3; # GEORGIAN MTAVRULI CAPITAL LETTER WE\n+1CB4; C; 10F4; # GEORGIAN MTAVRULI CAPITAL LETTER HAR\n+1CB5; C; 10F5; # GEORGIAN MTAVRULI CAPITAL LETTER HOE\n+1CB6; C; 10F6; # GEORGIAN MTAVRULI CAPITAL LETTER FI\n+1CB7; C; 10F7; # GEORGIAN MTAVRULI CAPITAL LETTER YN\n+1CB8; C; 10F8; # GEORGIAN MTAVRULI CAPITAL LETTER ELIFI\n+1CB9; C; 10F9; # GEORGIAN MTAVRULI CAPITAL LETTER TURNED GAN\n+1CBA; C; 10FA; # GEORGIAN MTAVRULI CAPITAL LETTER AIN\n+1CBD; C; 10FD; # GEORGIAN MTAVRULI CAPITAL LETTER AEN\n+1CBE; C; 10FE; # GEORGIAN MTAVRULI CAPITAL LETTER HARD SIGN\n+1CBF; C; 10FF; # GEORGIAN MTAVRULI CAPITAL LETTER LABIAL SIGN\n+1E00; C; 1E01; # LATIN CAPITAL LETTER A WITH RING BELOW\n+1E02; C; 1E03; # LATIN CAPITAL LETTER B WITH DOT ABOVE\n+1E04; C; 1E05; # LATIN CAPITAL LETTER B WITH DOT BELOW\n+1E06; C; 1E07; # LATIN CAPITAL LETTER B WITH LINE BELOW\n+1E08; C; 1E09; # LATIN CAPITAL LETTER C WITH CEDILLA AND ACUTE\n+1E0A; C; 1E0B; # LATIN CAPITAL LETTER D WITH DOT ABOVE\n+1E0C; C; 1E0D; # LATIN CAPITAL LETTER D WITH DOT BELOW\n+1E0E; C; 1E0F; # LATIN CAPITAL LETTER D WITH LINE BELOW\n+1E10; C; 1E11; # LATIN CAPITAL LETTER D WITH CEDILLA\n+1E12; C; 1E13; # LATIN CAPITAL LETTER D WITH CIRCUMFLEX BELOW\n+1E14; C; 1E15; # LATIN CAPITAL LETTER E WITH MACRON AND GRAVE\n+1E16; C; 1E17; # LATIN CAPITAL LETTER E WITH MACRON AND ACUTE\n+1E18; C; 1E19; # LATIN CAPITAL LETTER E WITH CIRCUMFLEX BELOW\n+1E1A; C; 1E1B; # LATIN CAPITAL LETTER E WITH TILDE BELOW\n+1E1C; C; 1E1D; # LATIN CAPITAL LETTER E WITH CEDILLA AND BREVE\n+1E1E; C; 1E1F; # LATIN CAPITAL LETTER F WITH DOT ABOVE\n+1E20; C; 1E21; # LATIN CAPITAL LETTER G WITH MACRON\n+1E22; C; 1E23; # LATIN CAPITAL LETTER H WITH DOT ABOVE\n+1E24; C; 1E25; # LATIN CAPITAL LETTER H WITH DOT BELOW\n+1E26; C; 1E27; # LATIN CAPITAL LETTER H WITH DIAERESIS\n+1E28; C; 1E29; # LATIN CAPITAL LETTER H WITH CEDILLA\n+1E2A; C; 1E2B; # LATIN CAPITAL LETTER H WITH BREVE BELOW\n+1E2C; C; 1E2D; # LATIN CAPITAL LETTER I WITH TILDE BELOW\n+1E2E; C; 1E2F; # LATIN CAPITAL LETTER I WITH DIAERESIS AND ACUTE\n+1E30; C; 1E31; # LATIN CAPITAL LETTER K WITH ACUTE\n+1E32; C; 1E33; # LATIN CAPITAL LETTER K WITH DOT BELOW\n+1E34; C; 1E35; # LATIN CAPITAL LETTER K WITH LINE BELOW\n+1E36; C; 1E37; # LATIN CAPITAL LETTER L WITH DOT BELOW\n+1E38; C; 1E39; # LATIN CAPITAL LETTER L WITH DOT BELOW AND MACRON\n+1E3A; C; 1E3B; # LATIN CAPITAL LETTER L WITH LINE BELOW\n+1E3C; C; 1E3D; # LATIN CAPITAL LETTER L WITH CIRCUMFLEX BELOW\n+1E3E; C; 1E3F; # LATIN CAPITAL LETTER M WITH ACUTE\n+1E40; C; 1E41; # LATIN CAPITAL LETTER M WITH DOT ABOVE\n+1E42; C; 1E43; # LATIN CAPITAL LETTER M WITH DOT BELOW\n+1E44; C; 1E45; # LATIN CAPITAL LETTER N WITH DOT ABOVE\n+1E46; C; 1E47; # LATIN CAPITAL LETTER N WITH DOT BELOW\n+1E48; C; 1E49; # LATIN CAPITAL LETTER N WITH LINE BELOW\n+1E4A; C; 1E4B; # LATIN CAPITAL LETTER N WITH CIRCUMFLEX BELOW\n+1E4C; C; 1E4D; # LATIN CAPITAL LETTER O WITH TILDE AND ACUTE\n+1E4E; C; 1E4F; # LATIN CAPITAL LETTER O WITH TILDE AND DIAERESIS\n+1E50; C; 1E51; # LATIN CAPITAL LETTER O WITH MACRON AND GRAVE\n+1E52; C; 1E53; # LATIN CAPITAL LETTER O WITH MACRON AND ACUTE\n+1E54; C; 1E55; # LATIN CAPITAL LETTER P WITH ACUTE\n+1E56; C; 1E57; # LATIN CAPITAL LETTER P WITH DOT ABOVE\n+1E58; C; 1E59; # LATIN CAPITAL LETTER R WITH DOT ABOVE\n+1E5A; C; 1E5B; # LATIN CAPITAL LETTER R WITH DOT BELOW\n+1E5C; C; 1E5D; # LATIN CAPITAL LETTER R WITH DOT BELOW AND MACRON\n+1E5E; C; 1E5F; # LATIN CAPITAL LETTER R WITH LINE BELOW\n+1E60; C; 1E61; # LATIN CAPITAL LETTER S WITH DOT ABOVE\n+1E62; C; 1E63; # LATIN CAPITAL LETTER S WITH DOT BELOW\n+1E64; C; 1E65; # LATIN CAPITAL LETTER S WITH ACUTE AND DOT ABOVE\n+1E66; C; 1E67; # LATIN CAPITAL LETTER S WITH CARON AND DOT ABOVE\n+1E68; C; 1E69; # LATIN CAPITAL LETTER S WITH DOT BELOW AND DOT ABOVE\n+1E6A; C; 1E6B; # LATIN CAPITAL LETTER T WITH DOT ABOVE\n+1E6C; C; 1E6D; # LATIN CAPITAL LETTER T WITH DOT BELOW\n+1E6E; C; 1E6F; # LATIN CAPITAL LETTER T WITH LINE BELOW\n+1E70; C; 1E71; # LATIN CAPITAL LETTER T WITH CIRCUMFLEX BELOW\n+1E72; C; 1E73; # LATIN CAPITAL LETTER U WITH DIAERESIS BELOW\n+1E74; C; 1E75; # LATIN CAPITAL LETTER U WITH TILDE BELOW\n+1E76; C; 1E77; # LATIN CAPITAL LETTER U WITH CIRCUMFLEX BELOW\n+1E78; C; 1E79; # LATIN CAPITAL LETTER U WITH TILDE AND ACUTE\n+1E7A; C; 1E7B; # LATIN CAPITAL LETTER U WITH MACRON AND DIAERESIS\n+1E7C; C; 1E7D; # LATIN CAPITAL LETTER V WITH TILDE\n+1E7E; C; 1E7F; # LATIN CAPITAL LETTER V WITH DOT BELOW\n+1E80; C; 1E81; # LATIN CAPITAL LETTER W WITH GRAVE\n+1E82; C; 1E83; # LATIN CAPITAL LETTER W WITH ACUTE\n+1E84; C; 1E85; # LATIN CAPITAL LETTER W WITH DIAERESIS\n+1E86; C; 1E87; # LATIN CAPITAL LETTER W WITH DOT ABOVE\n+1E88; C; 1E89; # LATIN CAPITAL LETTER W WITH DOT BELOW\n+1E8A; C; 1E8B; # LATIN CAPITAL LETTER X WITH DOT ABOVE\n+1E8C; C; 1E8D; # LATIN CAPITAL LETTER X WITH DIAERESIS\n+1E8E; C; 1E8F; # LATIN CAPITAL LETTER Y WITH DOT ABOVE\n+1E90; C; 1E91; # LATIN CAPITAL LETTER Z WITH CIRCUMFLEX\n+1E92; C; 1E93; # LATIN CAPITAL LETTER Z WITH DOT BELOW\n+1E94; C; 1E95; # LATIN CAPITAL LETTER Z WITH LINE BELOW\n+1E96; F; 0068 0331; # LATIN SMALL LETTER H WITH LINE BELOW\n+1E97; F; 0074 0308; # LATIN SMALL LETTER T WITH DIAERESIS\n+1E98; F; 0077 030A; # LATIN SMALL LETTER W WITH RING ABOVE\n+1E99; F; 0079 030A; # LATIN SMALL LETTER Y WITH RING ABOVE\n+1E9A; F; 0061 02BE; # LATIN SMALL LETTER A WITH RIGHT HALF RING\n+1E9B; C; 1E61; # LATIN SMALL LETTER LONG S WITH DOT ABOVE\n+1E9E; F; 0073 0073; # LATIN CAPITAL LETTER SHARP S\n+1E9E; S; 00DF; # LATIN CAPITAL LETTER SHARP S\n+1EA0; C; 1EA1; # LATIN CAPITAL LETTER A WITH DOT BELOW\n+1EA2; C; 1EA3; # LATIN CAPITAL LETTER A WITH HOOK ABOVE\n+1EA4; C; 1EA5; # LATIN CAPITAL LETTER A WITH CIRCUMFLEX AND ACUTE\n+1EA6; C; 1EA7; # LATIN CAPITAL LETTER A WITH CIRCUMFLEX AND GRAVE\n+1EA8; C; 1EA9; # LATIN CAPITAL LETTER A WITH CIRCUMFLEX AND HOOK ABOVE\n+1EAA; C; 1EAB; # LATIN CAPITAL LETTER A WITH CIRCUMFLEX AND TILDE\n+1EAC; C; 1EAD; # LATIN CAPITAL LETTER A WITH CIRCUMFLEX AND DOT BELOW\n+1EAE; C; 1EAF; # LATIN CAPITAL LETTER A WITH BREVE AND ACUTE\n+1EB0; C; 1EB1; # LATIN CAPITAL LETTER A WITH BREVE AND GRAVE\n+1EB2; C; 1EB3; # LATIN CAPITAL LETTER A WITH BREVE AND HOOK ABOVE\n+1EB4; C; 1EB5; # LATIN CAPITAL LETTER A WITH BREVE AND TILDE\n+1EB6; C; 1EB7; # LATIN CAPITAL LETTER A WITH BREVE AND DOT BELOW\n+1EB8; C; 1EB9; # LATIN CAPITAL LETTER E WITH DOT BELOW\n+1EBA; C; 1EBB; # LATIN CAPITAL LETTER E WITH HOOK ABOVE\n+1EBC; C; 1EBD; # LATIN CAPITAL LETTER E WITH TILDE\n+1EBE; C; 1EBF; # LATIN CAPITAL LETTER E WITH CIRCUMFLEX AND ACUTE\n+1EC0; C; 1EC1; # LATIN CAPITAL LETTER E WITH CIRCUMFLEX AND GRAVE\n+1EC2; C; 1EC3; # LATIN CAPITAL LETTER E WITH CIRCUMFLEX AND HOOK ABOVE\n+1EC4; C; 1EC5; # LATIN CAPITAL LETTER E WITH CIRCUMFLEX AND TILDE\n+1EC6; C; 1EC7; # LATIN CAPITAL LETTER E WITH CIRCUMFLEX AND DOT BELOW\n+1EC8; C; 1EC9; # LATIN CAPITAL LETTER I WITH HOOK ABOVE\n+1ECA; C; 1ECB; # LATIN CAPITAL LETTER I WITH DOT BELOW\n+1ECC; C; 1ECD; # LATIN CAPITAL LETTER O WITH DOT BELOW\n+1ECE; C; 1ECF; # LATIN CAPITAL LETTER O WITH HOOK ABOVE\n+1ED0; C; 1ED1; # LATIN CAPITAL LETTER O WITH CIRCUMFLEX AND ACUTE\n+1ED2; C; 1ED3; # LATIN CAPITAL LETTER O WITH CIRCUMFLEX AND GRAVE\n+1ED4; C; 1ED5; # LATIN CAPITAL LETTER O WITH CIRCUMFLEX AND HOOK ABOVE\n+1ED6; C; 1ED7; # LATIN CAPITAL LETTER O WITH CIRCUMFLEX AND TILDE\n+1ED8; C; 1ED9; # LATIN CAPITAL LETTER O WITH CIRCUMFLEX AND DOT BELOW\n+1EDA; C; 1EDB; # LATIN CAPITAL LETTER O WITH HORN AND ACUTE\n+1EDC; C; 1EDD; # LATIN CAPITAL LETTER O WITH HORN AND GRAVE\n+1EDE; C; 1EDF; # LATIN CAPITAL LETTER O WITH HORN AND HOOK ABOVE\n+1EE0; C; 1EE1; # LATIN CAPITAL LETTER O WITH HORN AND TILDE\n+1EE2; C; 1EE3; # LATIN CAPITAL LETTER O WITH HORN AND DOT BELOW\n+1EE4; C; 1EE5; # LATIN CAPITAL LETTER U WITH DOT BELOW\n+1EE6; C; 1EE7; # LATIN CAPITAL LETTER U WITH HOOK ABOVE\n+1EE8; C; 1EE9; # LATIN CAPITAL LETTER U WITH HORN AND ACUTE\n+1EEA; C; 1EEB; # LATIN CAPITAL LETTER U WITH HORN AND GRAVE\n+1EEC; C; 1EED; # LATIN CAPITAL LETTER U WITH HORN AND HOOK ABOVE\n+1EEE; C; 1EEF; # LATIN CAPITAL LETTER U WITH HORN AND TILDE\n+1EF0; C; 1EF1; # LATIN CAPITAL LETTER U WITH HORN AND DOT BELOW\n+1EF2; C; 1EF3; # LATIN CAPITAL LETTER Y WITH GRAVE\n+1EF4; C; 1EF5; # LATIN CAPITAL LETTER Y WITH DOT BELOW\n+1EF6; C; 1EF7; # LATIN CAPITAL LETTER Y WITH HOOK ABOVE\n+1EF8; C; 1EF9; # LATIN CAPITAL LETTER Y WITH TILDE\n+1EFA; C; 1EFB; # LATIN CAPITAL LETTER MIDDLE-WELSH LL\n+1EFC; C; 1EFD; # LATIN CAPITAL LETTER MIDDLE-WELSH V\n+1EFE; C; 1EFF; # LATIN CAPITAL LETTER Y WITH LOOP\n+1F08; C; 1F00; # GREEK CAPITAL LETTER ALPHA WITH PSILI\n+1F09; C; 1F01; # GREEK CAPITAL LETTER ALPHA WITH DASIA\n+1F0A; C; 1F02; # GREEK CAPITAL LETTER ALPHA WITH PSILI AND VARIA\n+1F0B; C; 1F03; # GREEK CAPITAL LETTER ALPHA WITH DASIA AND VARIA\n+1F0C; C; 1F04; # GREEK CAPITAL LETTER ALPHA WITH PSILI AND OXIA\n+1F0D; C; 1F05; # GREEK CAPITAL LETTER ALPHA WITH DASIA AND OXIA\n+1F0E; C; 1F06; # GREEK CAPITAL LETTER ALPHA WITH PSILI AND PERISPOMENI\n+1F0F; C; 1F07; # GREEK CAPITAL LETTER ALPHA WITH DASIA AND PERISPOMENI\n+1F18; C; 1F10; # GREEK CAPITAL LETTER EPSILON WITH PSILI\n+1F19; C; 1F11; # GREEK CAPITAL LETTER EPSILON WITH DASIA\n+1F1A; C; 1F12; # GREEK CAPITAL LETTER EPSILON WITH PSILI AND VARIA\n+1F1B; C; 1F13; # GREEK CAPITAL LETTER EPSILON WITH DASIA AND VARIA\n+1F1C; C; 1F14; # GREEK CAPITAL LETTER EPSILON WITH PSILI AND OXIA\n+1F1D; C; 1F15; # GREEK CAPITAL LETTER EPSILON WITH DASIA AND OXIA\n+1F28; C; 1F20; # GREEK CAPITAL LETTER ETA WITH PSILI\n+1F29; C; 1F21; # GREEK CAPITAL LETTER ETA WITH DASIA\n+1F2A; C; 1F22; # GREEK CAPITAL LETTER ETA WITH PSILI AND VARIA\n+1F2B; C; 1F23; # GREEK CAPITAL LETTER ETA WITH DASIA AND VARIA\n+1F2C; C; 1F24; # GREEK CAPITAL LETTER ETA WITH PSILI AND OXIA\n+1F2D; C; 1F25; # GREEK CAPITAL LETTER ETA WITH DASIA AND OXIA\n+1F2E; C; 1F26; # GREEK CAPITAL LETTER ETA WITH PSILI AND PERISPOMENI\n+1F2F; C; 1F27; # GREEK CAPITAL LETTER ETA WITH DASIA AND PERISPOMENI\n+1F38; C; 1F30; # GREEK CAPITAL LETTER IOTA WITH PSILI\n+1F39; C; 1F31; # GREEK CAPITAL LETTER IOTA WITH DASIA\n+1F3A; C; 1F32; # GREEK CAPITAL LETTER IOTA WITH PSILI AND VARIA\n+1F3B; C; 1F33; # GREEK CAPITAL LETTER IOTA WITH DASIA AND VARIA\n+1F3C; C; 1F34; # GREEK CAPITAL LETTER IOTA WITH PSILI AND OXIA\n+1F3D; C; 1F35; # GREEK CAPITAL LETTER IOTA WITH DASIA AND OXIA\n+1F3E; C; 1F36; # GREEK CAPITAL LETTER IOTA WITH PSILI AND PERISPOMENI\n+1F3F; C; 1F37; # GREEK CAPITAL LETTER IOTA WITH DASIA AND PERISPOMENI\n+1F48; C; 1F40; # GREEK CAPITAL LETTER OMICRON WITH PSILI\n+1F49; C; 1F41; # GREEK CAPITAL LETTER OMICRON WITH DASIA\n+1F4A; C; 1F42; # GREEK CAPITAL LETTER OMICRON WITH PSILI AND VARIA\n+1F4B; C; 1F43; # GREEK CAPITAL LETTER OMICRON WITH DASIA AND VARIA\n+1F4C; C; 1F44; # GREEK CAPITAL LETTER OMICRON WITH PSILI AND OXIA\n+1F4D; C; 1F45; # GREEK CAPITAL LETTER OMICRON WITH DASIA AND OXIA\n+1F50; F; 03C5 0313; # GREEK SMALL LETTER UPSILON WITH PSILI\n+1F52; F; 03C5 0313 0300; # GREEK SMALL LETTER UPSILON WITH PSILI AND VARIA\n+1F54; F; 03C5 0313 0301; # GREEK SMALL LETTER UPSILON WITH PSILI AND OXIA\n+1F56; F; 03C5 0313 0342; # GREEK SMALL LETTER UPSILON WITH PSILI AND PERISPOMENI\n+1F59; C; 1F51; # GREEK CAPITAL LETTER UPSILON WITH DASIA\n+1F5B; C; 1F53; # GREEK CAPITAL LETTER UPSILON WITH DASIA AND VARIA\n+1F5D; C; 1F55; # GREEK CAPITAL LETTER UPSILON WITH DASIA AND OXIA\n+1F5F; C; 1F57; # GREEK CAPITAL LETTER UPSILON WITH DASIA AND PERISPOMENI\n+1F68; C; 1F60; # GREEK CAPITAL LETTER OMEGA WITH PSILI\n+1F69; C; 1F61; # GREEK CAPITAL LETTER OMEGA WITH DASIA\n+1F6A; C; 1F62; # GREEK CAPITAL LETTER OMEGA WITH PSILI AND VARIA\n+1F6B; C; 1F63; # GREEK CAPITAL LETTER OMEGA WITH DASIA AND VARIA\n+1F6C; C; 1F64; # GREEK CAPITAL LETTER OMEGA WITH PSILI AND OXIA\n+1F6D; C; 1F65; # GREEK CAPITAL LETTER OMEGA WITH DASIA AND OXIA\n+1F6E; C; 1F66; # GREEK CAPITAL LETTER OMEGA WITH PSILI AND PERISPOMENI\n+1F6F; C; 1F67; # GREEK CAPITAL LETTER OMEGA WITH DASIA AND PERISPOMENI\n+1F80; F; 1F00 03B9; # GREEK SMALL LETTER ALPHA WITH PSILI AND YPOGEGRAMMENI\n+1F81; F; 1F01 03B9; # GREEK SMALL LETTER ALPHA WITH DASIA AND YPOGEGRAMMENI\n+1F82; F; 1F02 03B9; # GREEK SMALL LETTER ALPHA WITH PSILI AND VARIA AND YPOGEGRAMMENI\n+1F83; F; 1F03 03B9; # GREEK SMALL LETTER ALPHA WITH DASIA AND VARIA AND YPOGEGRAMMENI\n+1F84; F; 1F04 03B9; # GREEK SMALL LETTER ALPHA WITH PSILI AND OXIA AND YPOGEGRAMMENI\n+1F85; F; 1F05 03B9; # GREEK SMALL LETTER ALPHA WITH DASIA AND OXIA AND YPOGEGRAMMENI\n+1F86; F; 1F06 03B9; # GREEK SMALL LETTER ALPHA WITH PSILI AND PERISPOMENI AND YPOGEGRAMMENI\n+1F87; F; 1F07 03B9; # GREEK SMALL LETTER ALPHA WITH DASIA AND PERISPOMENI AND YPOGEGRAMMENI\n+1F88; F; 1F00 03B9; # GREEK CAPITAL LETTER ALPHA WITH PSILI AND PROSGEGRAMMENI\n+1F88; S; 1F80; # GREEK CAPITAL LETTER ALPHA WITH PSILI AND PROSGEGRAMMENI\n+1F89; F; 1F01 03B9; # GREEK CAPITAL LETTER ALPHA WITH DASIA AND PROSGEGRAMMENI\n+1F89; S; 1F81; # GREEK CAPITAL LETTER ALPHA WITH DASIA AND PROSGEGRAMMENI\n+1F8A; F; 1F02 03B9; # GREEK CAPITAL LETTER ALPHA WITH PSILI AND VARIA AND PROSGEGRAMMENI\n+1F8A; S; 1F82; # GREEK CAPITAL LETTER ALPHA WITH PSILI AND VARIA AND PROSGEGRAMMENI\n+1F8B; F; 1F03 03B9; # GREEK CAPITAL LETTER ALPHA WITH DASIA AND VARIA AND PROSGEGRAMMENI\n+1F8B; S; 1F83; # GREEK CAPITAL LETTER ALPHA WITH DASIA AND VARIA AND PROSGEGRAMMENI\n+1F8C; F; 1F04 03B9; # GREEK CAPITAL LETTER ALPHA WITH PSILI AND OXIA AND PROSGEGRAMMENI\n+1F8C; S; 1F84; # GREEK CAPITAL LETTER ALPHA WITH PSILI AND OXIA AND PROSGEGRAMMENI\n+1F8D; F; 1F05 03B9; # GREEK CAPITAL LETTER ALPHA WITH DASIA AND OXIA AND PROSGEGRAMMENI\n+1F8D; S; 1F85; # GREEK CAPITAL LETTER ALPHA WITH DASIA AND OXIA AND PROSGEGRAMMENI\n+1F8E; F; 1F06 03B9; # GREEK CAPITAL LETTER ALPHA WITH PSILI AND PERISPOMENI AND PROSGEGRAMMENI\n+1F8E; S; 1F86; # GREEK CAPITAL LETTER ALPHA WITH PSILI AND PERISPOMENI AND PROSGEGRAMMENI\n+1F8F; F; 1F07 03B9; # GREEK CAPITAL LETTER ALPHA WITH DASIA AND PERISPOMENI AND PROSGEGRAMMENI\n+1F8F; S; 1F87; # GREEK CAPITAL LETTER ALPHA WITH DASIA AND PERISPOMENI AND PROSGEGRAMMENI\n+1F90; F; 1F20 03B9; # GREEK SMALL LETTER ETA WITH PSILI AND YPOGEGRAMMENI\n+1F91; F; 1F21 03B9; # GREEK SMALL LETTER ETA WITH DASIA AND YPOGEGRAMMENI\n+1F92; F; 1F22 03B9; # GREEK SMALL LETTER ETA WITH PSILI AND VARIA AND YPOGEGRAMMENI\n+1F93; F; 1F23 03B9; # GREEK SMALL LETTER ETA WITH DASIA AND VARIA AND YPOGEGRAMMENI\n+1F94; F; 1F24 03B9; # GREEK SMALL LETTER ETA WITH PSILI AND OXIA AND YPOGEGRAMMENI\n+1F95; F; 1F25 03B9; # GREEK SMALL LETTER ETA WITH DASIA AND OXIA AND YPOGEGRAMMENI\n+1F96; F; 1F26 03B9; # GREEK SMALL LETTER ETA WITH PSILI AND PERISPOMENI AND YPOGEGRAMMENI\n+1F97; F; 1F27 03B9; # GREEK SMALL LETTER ETA WITH DASIA AND PERISPOMENI AND YPOGEGRAMMENI\n+1F98; F; 1F20 03B9; # GREEK CAPITAL LETTER ETA WITH PSILI AND PROSGEGRAMMENI\n+1F98; S; 1F90; # GREEK CAPITAL LETTER ETA WITH PSILI AND PROSGEGRAMMENI\n+1F99; F; 1F21 03B9; # GREEK CAPITAL LETTER ETA WITH DASIA AND PROSGEGRAMMENI\n+1F99; S; 1F91; # GREEK CAPITAL LETTER ETA WITH DASIA AND PROSGEGRAMMENI\n+1F9A; F; 1F22 03B9; # GREEK CAPITAL LETTER ETA WITH PSILI AND VARIA AND PROSGEGRAMMENI\n+1F9A; S; 1F92; # GREEK CAPITAL LETTER ETA WITH PSILI AND VARIA AND PROSGEGRAMMENI\n+1F9B; F; 1F23 03B9; # GREEK CAPITAL LETTER ETA WITH DASIA AND VARIA AND PROSGEGRAMMENI\n+1F9B; S; 1F93; # GREEK CAPITAL LETTER ETA WITH DASIA AND VARIA AND PROSGEGRAMMENI\n+1F9C; F; 1F24 03B9; # GREEK CAPITAL LETTER ETA WITH PSILI AND OXIA AND PROSGEGRAMMENI\n+1F9C; S; 1F94; # GREEK CAPITAL LETTER ETA WITH PSILI AND OXIA AND PROSGEGRAMMENI\n+1F9D; F; 1F25 03B9; # GREEK CAPITAL LETTER ETA WITH DASIA AND OXIA AND PROSGEGRAMMENI\n+1F9D; S; 1F95; # GREEK CAPITAL LETTER ETA WITH DASIA AND OXIA AND PROSGEGRAMMENI\n+1F9E; F; 1F26 03B9; # GREEK CAPITAL LETTER ETA WITH PSILI AND PERISPOMENI AND PROSGEGRAMMENI\n+1F9E; S; 1F96; # GREEK CAPITAL LETTER ETA WITH PSILI AND PERISPOMENI AND PROSGEGRAMMENI\n+1F9F; F; 1F27 03B9; # GREEK CAPITAL LETTER ETA WITH DASIA AND PERISPOMENI AND PROSGEGRAMMENI\n+1F9F; S; 1F97; # GREEK CAPITAL LETTER ETA WITH DASIA AND PERISPOMENI AND PROSGEGRAMMENI\n+1FA0; F; 1F60 03B9; # GREEK SMALL LETTER OMEGA WITH PSILI AND YPOGEGRAMMENI\n+1FA1; F; 1F61 03B9; # GREEK SMALL LETTER OMEGA WITH DASIA AND YPOGEGRAMMENI\n+1FA2; F; 1F62 03B9; # GREEK SMALL LETTER OMEGA WITH PSILI AND VARIA AND YPOGEGRAMMENI\n+1FA3; F; 1F63 03B9; # GREEK SMALL LETTER OMEGA WITH DASIA AND VARIA AND YPOGEGRAMMENI\n+1FA4; F; 1F64 03B9; # GREEK SMALL LETTER OMEGA WITH PSILI AND OXIA AND YPOGEGRAMMENI\n+1FA5; F; 1F65 03B9; # GREEK SMALL LETTER OMEGA WITH DASIA AND OXIA AND YPOGEGRAMMENI\n+1FA6; F; 1F66 03B9; # GREEK SMALL LETTER OMEGA WITH PSILI AND PERISPOMENI AND YPOGEGRAMMENI\n+1FA7; F; 1F67 03B9; # GREEK SMALL LETTER OMEGA WITH DASIA AND PERISPOMENI AND YPOGEGRAMMENI\n+1FA8; F; 1F60 03B9; # GREEK CAPITAL LETTER OMEGA WITH PSILI AND PROSGEGRAMMENI\n+1FA8; S; 1FA0; # GREEK CAPITAL LETTER OMEGA WITH PSILI AND PROSGEGRAMMENI\n+1FA9; F; 1F61 03B9; # GREEK CAPITAL LETTER OMEGA WITH DASIA AND PROSGEGRAMMENI\n+1FA9; S; 1FA1; # GREEK CAPITAL LETTER OMEGA WITH DASIA AND PROSGEGRAMMENI\n+1FAA; F; 1F62 03B9; # GREEK CAPITAL LETTER OMEGA WITH PSILI AND VARIA AND PROSGEGRAMMENI\n+1FAA; S; 1FA2; # GREEK CAPITAL LETTER OMEGA WITH PSILI AND VARIA AND PROSGEGRAMMENI\n+1FAB; F; 1F63 03B9; # GREEK CAPITAL LETTER OMEGA WITH DASIA AND VARIA AND PROSGEGRAMMENI\n+1FAB; S; 1FA3; # GREEK CAPITAL LETTER OMEGA WITH DASIA AND VARIA AND PROSGEGRAMMENI\n+1FAC; F; 1F64 03B9; # GREEK CAPITAL LETTER OMEGA WITH PSILI AND OXIA AND PROSGEGRAMMENI\n+1FAC; S; 1FA4; # GREEK CAPITAL LETTER OMEGA WITH PSILI AND OXIA AND PROSGEGRAMMENI\n+1FAD; F; 1F65 03B9; # GREEK CAPITAL LETTER OMEGA WITH DASIA AND OXIA AND PROSGEGRAMMENI\n+1FAD; S; 1FA5; # GREEK CAPITAL LETTER OMEGA WITH DASIA AND OXIA AND PROSGEGRAMMENI\n+1FAE; F; 1F66 03B9; # GREEK CAPITAL LETTER OMEGA WITH PSILI AND PERISPOMENI AND PROSGEGRAMMENI\n+1FAE; S; 1FA6; # GREEK CAPITAL LETTER OMEGA WITH PSILI AND PERISPOMENI AND PROSGEGRAMMENI\n+1FAF; F; 1F67 03B9; # GREEK CAPITAL LETTER OMEGA WITH DASIA AND PERISPOMENI AND PROSGEGRAMMENI\n+1FAF; S; 1FA7; # GREEK CAPITAL LETTER OMEGA WITH DASIA AND PERISPOMENI AND PROSGEGRAMMENI\n+1FB2; F; 1F70 03B9; # GREEK SMALL LETTER ALPHA WITH VARIA AND YPOGEGRAMMENI\n+1FB3; F; 03B1 03B9; # GREEK SMALL LETTER ALPHA WITH YPOGEGRAMMENI\n+1FB4; F; 03AC 03B9; # GREEK SMALL LETTER ALPHA WITH OXIA AND YPOGEGRAMMENI\n+1FB6; F; 03B1 0342; # GREEK SMALL LETTER ALPHA WITH PERISPOMENI\n+1FB7; F; 03B1 0342 03B9; # GREEK SMALL LETTER ALPHA WITH PERISPOMENI AND YPOGEGRAMMENI\n+1FB8; C; 1FB0; # GREEK CAPITAL LETTER ALPHA WITH VRACHY\n+1FB9; C; 1FB1; # GREEK CAPITAL LETTER ALPHA WITH MACRON\n+1FBA; C; 1F70; # GREEK CAPITAL LETTER ALPHA WITH VARIA\n+1FBB; C; 1F71; # GREEK CAPITAL LETTER ALPHA WITH OXIA\n+1FBC; F; 03B1 03B9; # GREEK CAPITAL LETTER ALPHA WITH PROSGEGRAMMENI\n+1FBC; S; 1FB3; # GREEK CAPITAL LETTER ALPHA WITH PROSGEGRAMMENI\n+1FBE; C; 03B9; # GREEK PROSGEGRAMMENI\n+1FC2; F; 1F74 03B9; # GREEK SMALL LETTER ETA WITH VARIA AND YPOGEGRAMMENI\n+1FC3; F; 03B7 03B9; # GREEK SMALL LETTER ETA WITH YPOGEGRAMMENI\n+1FC4; F; 03AE 03B9; # GREEK SMALL LETTER ETA WITH OXIA AND YPOGEGRAMMENI\n+1FC6; F; 03B7 0342; # GREEK SMALL LETTER ETA WITH PERISPOMENI\n+1FC7; F; 03B7 0342 03B9; # GREEK SMALL LETTER ETA WITH PERISPOMENI AND YPOGEGRAMMENI\n+1FC8; C; 1F72; # GREEK CAPITAL LETTER EPSILON WITH VARIA\n+1FC9; C; 1F73; # GREEK CAPITAL LETTER EPSILON WITH OXIA\n+1FCA; C; 1F74; # GREEK CAPITAL LETTER ETA WITH VARIA\n+1FCB; C; 1F75; # GREEK CAPITAL LETTER ETA WITH OXIA\n+1FCC; F; 03B7 03B9; # GREEK CAPITAL LETTER ETA WITH PROSGEGRAMMENI\n+1FCC; S; 1FC3; # GREEK CAPITAL LETTER ETA WITH PROSGEGRAMMENI\n+1FD2; F; 03B9 0308 0300; # GREEK SMALL LETTER IOTA WITH DIALYTIKA AND VARIA\n+1FD3; F; 03B9 0308 0301; # GREEK SMALL LETTER IOTA WITH DIALYTIKA AND OXIA\n+1FD6; F; 03B9 0342; # GREEK SMALL LETTER IOTA WITH PERISPOMENI\n+1FD7; F; 03B9 0308 0342; # GREEK SMALL LETTER IOTA WITH DIALYTIKA AND PERISPOMENI\n+1FD8; C; 1FD0; # GREEK CAPITAL LETTER IOTA WITH VRACHY\n+1FD9; C; 1FD1; # GREEK CAPITAL LETTER IOTA WITH MACRON\n+1FDA; C; 1F76; # GREEK CAPITAL LETTER IOTA WITH VARIA\n+1FDB; C; 1F77; # GREEK CAPITAL LETTER IOTA WITH OXIA\n+1FE2; F; 03C5 0308 0300; # GREEK SMALL LETTER UPSILON WITH DIALYTIKA AND VARIA\n+1FE3; F; 03C5 0308 0301; # GREEK SMALL LETTER UPSILON WITH DIALYTIKA AND OXIA\n+1FE4; F; 03C1 0313; # GREEK SMALL LETTER RHO WITH PSILI\n+1FE6; F; 03C5 0342; # GREEK SMALL LETTER UPSILON WITH PERISPOMENI\n+1FE7; F; 03C5 0308 0342; # GREEK SMALL LETTER UPSILON WITH DIALYTIKA AND PERISPOMENI\n+1FE8; C; 1FE0; # GREEK CAPITAL LETTER UPSILON WITH VRACHY\n+1FE9; C; 1FE1; # GREEK CAPITAL LETTER UPSILON WITH MACRON\n+1FEA; C; 1F7A; # GREEK CAPITAL LETTER UPSILON WITH VARIA\n+1FEB; C; 1F7B; # GREEK CAPITAL LETTER UPSILON WITH OXIA\n+1FEC; C; 1FE5; # GREEK CAPITAL LETTER RHO WITH DASIA\n+1FF2; F; 1F7C 03B9; # GREEK SMALL LETTER OMEGA WITH VARIA AND YPOGEGRAMMENI\n+1FF3; F; 03C9 03B9; # GREEK SMALL LETTER OMEGA WITH YPOGEGRAMMENI\n+1FF4; F; 03CE 03B9; # GREEK SMALL LETTER OMEGA WITH OXIA AND YPOGEGRAMMENI\n+1FF6; F; 03C9 0342; # GREEK SMALL LETTER OMEGA WITH PERISPOMENI\n+1FF7; F; 03C9 0342 03B9; # GREEK SMALL LETTER OMEGA WITH PERISPOMENI AND YPOGEGRAMMENI\n+1FF8; C; 1F78; # GREEK CAPITAL LETTER OMICRON WITH VARIA\n+1FF9; C; 1F79; # GREEK CAPITAL LETTER OMICRON WITH OXIA\n+1FFA; C; 1F7C; # GREEK CAPITAL LETTER OMEGA WITH VARIA\n+1FFB; C; 1F7D; # GREEK CAPITAL LETTER OMEGA WITH OXIA\n+1FFC; F; 03C9 03B9; # GREEK CAPITAL LETTER OMEGA WITH PROSGEGRAMMENI\n+1FFC; S; 1FF3; # GREEK CAPITAL LETTER OMEGA WITH PROSGEGRAMMENI\n+2126; C; 03C9; # OHM SIGN\n+212A; C; 006B; # KELVIN SIGN\n+212B; C; 00E5; # ANGSTROM SIGN\n+2132; C; 214E; # TURNED CAPITAL F\n+2160; C; 2170; # ROMAN NUMERAL ONE\n+2161; C; 2171; # ROMAN NUMERAL TWO\n+2162; C; 2172; # ROMAN NUMERAL THREE\n+2163; C; 2173; # ROMAN NUMERAL FOUR\n+2164; C; 2174; # ROMAN NUMERAL FIVE\n+2165; C; 2175; # ROMAN NUMERAL SIX\n+2166; C; 2176; # ROMAN NUMERAL SEVEN\n+2167; C; 2177; # ROMAN NUMERAL EIGHT\n+2168; C; 2178; # ROMAN NUMERAL NINE\n+2169; C; 2179; # ROMAN NUMERAL TEN\n+216A; C; 217A; # ROMAN NUMERAL ELEVEN\n+216B; C; 217B; # ROMAN NUMERAL TWELVE\n+216C; C; 217C; # ROMAN NUMERAL FIFTY\n+216D; C; 217D; # ROMAN NUMERAL ONE HUNDRED\n+216E; C; 217E; # ROMAN NUMERAL FIVE HUNDRED\n+216F; C; 217F; # ROMAN NUMERAL ONE THOUSAND\n+2183; C; 2184; # ROMAN NUMERAL REVERSED ONE HUNDRED\n+24B6; C; 24D0; # CIRCLED LATIN CAPITAL LETTER A\n+24B7; C; 24D1; # CIRCLED LATIN CAPITAL LETTER B\n+24B8; C; 24D2; # CIRCLED LATIN CAPITAL LETTER C\n+24B9; C; 24D3; # CIRCLED LATIN CAPITAL LETTER D\n+24BA; C; 24D4; # CIRCLED LATIN CAPITAL LETTER E\n+24BB; C; 24D5; # CIRCLED LATIN CAPITAL LETTER F\n+24BC; C; 24D6; # CIRCLED LATIN CAPITAL LETTER G\n+24BD; C; 24D7; # CIRCLED LATIN CAPITAL LETTER H\n+24BE; C; 24D8; # CIRCLED LATIN CAPITAL LETTER I\n+24BF; C; 24D9; # CIRCLED LATIN CAPITAL LETTER J\n+24C0; C; 24DA; # CIRCLED LATIN CAPITAL LETTER K\n+24C1; C; 24DB; # CIRCLED LATIN CAPITAL LETTER L\n+24C2; C; 24DC; # CIRCLED LATIN CAPITAL LETTER M\n+24C3; C; 24DD; # CIRCLED LATIN CAPITAL LETTER N\n+24C4; C; 24DE; # CIRCLED LATIN CAPITAL LETTER O\n+24C5; C; 24DF; # CIRCLED LATIN CAPITAL LETTER P\n+24C6; C; 24E0; # CIRCLED LATIN CAPITAL LETTER Q\n+24C7; C; 24E1; # CIRCLED LATIN CAPITAL LETTER R\n+24C8; C; 24E2; # CIRCLED LATIN CAPITAL LETTER S\n+24C9; C; 24E3; # CIRCLED LATIN CAPITAL LETTER T\n+24CA; C; 24E4; # CIRCLED LATIN CAPITAL LETTER U\n+24CB; C; 24E5; # CIRCLED LATIN CAPITAL LETTER V\n+24CC; C; 24E6; # CIRCLED LATIN CAPITAL LETTER W\n+24CD; C; 24E7; # CIRCLED LATIN CAPITAL LETTER X\n+24CE; C; 24E8; # CIRCLED LATIN CAPITAL LETTER Y\n+24CF; C; 24E9; # CIRCLED LATIN CAPITAL LETTER Z\n+2C00; C; 2C30; # GLAGOLITIC CAPITAL LETTER AZU\n+2C01; C; 2C31; # GLAGOLITIC CAPITAL LETTER BUKY\n+2C02; C; 2C32; # GLAGOLITIC CAPITAL LETTER VEDE\n+2C03; C; 2C33; # GLAGOLITIC CAPITAL LETTER GLAGOLI\n+2C04; C; 2C34; # GLAGOLITIC CAPITAL LETTER DOBRO\n+2C05; C; 2C35; # GLAGOLITIC CAPITAL LETTER YESTU\n+2C06; C; 2C36; # GLAGOLITIC CAPITAL LETTER ZHIVETE\n+2C07; C; 2C37; # GLAGOLITIC CAPITAL LETTER DZELO\n+2C08; C; 2C38; # GLAGOLITIC CAPITAL LETTER ZEMLJA\n+2C09; C; 2C39; # GLAGOLITIC CAPITAL LETTER IZHE\n+2C0A; C; 2C3A; # GLAGOLITIC CAPITAL LETTER INITIAL IZHE\n+2C0B; C; 2C3B; # GLAGOLITIC CAPITAL LETTER I\n+2C0C; C; 2C3C; # GLAGOLITIC CAPITAL LETTER DJERVI\n+2C0D; C; 2C3D; # GLAGOLITIC CAPITAL LETTER KAKO\n+2C0E; C; 2C3E; # GLAGOLITIC CAPITAL LETTER LJUDIJE\n+2C0F; C; 2C3F; # GLAGOLITIC CAPITAL LETTER MYSLITE\n+2C10; C; 2C40; # GLAGOLITIC CAPITAL LETTER NASHI\n+2C11; C; 2C41; # GLAGOLITIC CAPITAL LETTER ONU\n+2C12; C; 2C42; # GLAGOLITIC CAPITAL LETTER POKOJI\n+2C13; C; 2C43; # GLAGOLITIC CAPITAL LETTER RITSI\n+2C14; C; 2C44; # GLAGOLITIC CAPITAL LETTER SLOVO\n+2C15; C; 2C45; # GLAGOLITIC CAPITAL LETTER TVRIDO\n+2C16; C; 2C46; # GLAGOLITIC CAPITAL LETTER UKU\n+2C17; C; 2C47; # GLAGOLITIC CAPITAL LETTER FRITU\n+2C18; C; 2C48; # GLAGOLITIC CAPITAL LETTER HERU\n+2C19; C; 2C49; # GLAGOLITIC CAPITAL LETTER OTU\n+2C1A; C; 2C4A; # GLAGOLITIC CAPITAL LETTER PE\n+2C1B; C; 2C4B; # GLAGOLITIC CAPITAL LETTER SHTA\n+2C1C; C; 2C4C; # GLAGOLITIC CAPITAL LETTER TSI\n+2C1D; C; 2C4D; # GLAGOLITIC CAPITAL LETTER CHRIVI\n+2C1E; C; 2C4E; # GLAGOLITIC CAPITAL LETTER SHA\n+2C1F; C; 2C4F; # GLAGOLITIC CAPITAL LETTER YERU\n+2C20; C; 2C50; # GLAGOLITIC CAPITAL LETTER YERI\n+2C21; C; 2C51; # GLAGOLITIC CAPITAL LETTER YATI\n+2C22; C; 2C52; # GLAGOLITIC CAPITAL LETTER SPIDERY HA\n+2C23; C; 2C53; # GLAGOLITIC CAPITAL LETTER YU\n+2C24; C; 2C54; # GLAGOLITIC CAPITAL LETTER SMALL YUS\n+2C25; C; 2C55; # GLAGOLITIC CAPITAL LETTER SMALL YUS WITH TAIL\n+2C26; C; 2C56; # GLAGOLITIC CAPITAL LETTER YO\n+2C27; C; 2C57; # GLAGOLITIC CAPITAL LETTER IOTATED SMALL YUS\n+2C28; C; 2C58; # GLAGOLITIC CAPITAL LETTER BIG YUS\n+2C29; C; 2C59; # GLAGOLITIC CAPITAL LETTER IOTATED BIG YUS\n+2C2A; C; 2C5A; # GLAGOLITIC CAPITAL LETTER FITA\n+2C2B; C; 2C5B; # GLAGOLITIC CAPITAL LETTER IZHITSA\n+2C2C; C; 2C5C; # GLAGOLITIC CAPITAL LETTER SHTAPIC\n+2C2D; C; 2C5D; # GLAGOLITIC CAPITAL LETTER TROKUTASTI A\n+2C2E; C; 2C5E; # GLAGOLITIC CAPITAL LETTER LATINATE MYSLITE\n+2C2F; C; 2C5F; # GLAGOLITIC CAPITAL LETTER CAUDATE CHRIVI\n+2C60; C; 2C61; # LATIN CAPITAL LETTER L WITH DOUBLE BAR\n+2C62; C; 026B; # LATIN CAPITAL LETTER L WITH MIDDLE TILDE\n+2C63; C; 1D7D; # LATIN CAPITAL LETTER P WITH STROKE\n+2C64; C; 027D; # LATIN CAPITAL LETTER R WITH TAIL\n+2C67; C; 2C68; # LATIN CAPITAL LETTER H WITH DESCENDER\n+2C69; C; 2C6A; # LATIN CAPITAL LETTER K WITH DESCENDER\n+2C6B; C; 2C6C; # LATIN CAPITAL LETTER Z WITH DESCENDER\n+2C6D; C; 0251; # LATIN CAPITAL LETTER ALPHA\n+2C6E; C; 0271; # LATIN CAPITAL LETTER M WITH HOOK\n+2C6F; C; 0250; # LATIN CAPITAL LETTER TURNED A\n+2C70; C; 0252; # LATIN CAPITAL LETTER TURNED ALPHA\n+2C72; C; 2C73; # LATIN CAPITAL LETTER W WITH HOOK\n+2C75; C; 2C76; # LATIN CAPITAL LETTER HALF H\n+2C7E; C; 023F; # LATIN CAPITAL LETTER S WITH SWASH TAIL\n+2C7F; C; 0240; # LATIN CAPITAL LETTER Z WITH SWASH TAIL\n+2C80; C; 2C81; # COPTIC CAPITAL LETTER ALFA\n+2C82; C; 2C83; # COPTIC CAPITAL LETTER VIDA\n+2C84; C; 2C85; # COPTIC CAPITAL LETTER GAMMA\n+2C86; C; 2C87; # COPTIC CAPITAL LETTER DALDA\n+2C88; C; 2C89; # COPTIC CAPITAL LETTER EIE\n+2C8A; C; 2C8B; # COPTIC CAPITAL LETTER SOU\n+2C8C; C; 2C8D; # COPTIC CAPITAL LETTER ZATA\n+2C8E; C; 2C8F; # COPTIC CAPITAL LETTER HATE\n+2C90; C; 2C91; # COPTIC CAPITAL LETTER THETHE\n+2C92; C; 2C93; # COPTIC CAPITAL LETTER IAUDA\n+2C94; C; 2C95; # COPTIC CAPITAL LETTER KAPA\n+2C96; C; 2C97; # COPTIC CAPITAL LETTER LAULA\n+2C98; C; 2C99; # COPTIC CAPITAL LETTER MI\n+2C9A; C; 2C9B; # COPTIC CAPITAL LETTER NI\n+2C9C; C; 2C9D; # COPTIC CAPITAL LETTER KSI\n+2C9E; C; 2C9F; # COPTIC CAPITAL LETTER O\n+2CA0; C; 2CA1; # COPTIC CAPITAL LETTER PI\n+2CA2; C; 2CA3; # COPTIC CAPITAL LETTER RO\n+2CA4; C; 2CA5; # COPTIC CAPITAL LETTER SIMA\n+2CA6; C; 2CA7; # COPTIC CAPITAL LETTER TAU\n+2CA8; C; 2CA9; # COPTIC CAPITAL LETTER UA\n+2CAA; C; 2CAB; # COPTIC CAPITAL LETTER FI\n+2CAC; C; 2CAD; # COPTIC CAPITAL LETTER KHI\n+2CAE; C; 2CAF; # COPTIC CAPITAL LETTER PSI\n+2CB0; C; 2CB1; # COPTIC CAPITAL LETTER OOU\n+2CB2; C; 2CB3; # COPTIC CAPITAL LETTER DIALECT-P ALEF\n+2CB4; C; 2CB5; # COPTIC CAPITAL LETTER OLD COPTIC AIN\n+2CB6; C; 2CB7; # COPTIC CAPITAL LETTER CRYPTOGRAMMIC EIE\n+2CB8; C; 2CB9; # COPTIC CAPITAL LETTER DIALECT-P KAPA\n+2CBA; C; 2CBB; # COPTIC CAPITAL LETTER DIALECT-P NI\n+2CBC; C; 2CBD; # COPTIC CAPITAL LETTER CRYPTOGRAMMIC NI\n+2CBE; C; 2CBF; # COPTIC CAPITAL LETTER OLD COPTIC OOU\n+2CC0; C; 2CC1; # COPTIC CAPITAL LETTER SAMPI\n+2CC2; C; 2CC3; # COPTIC CAPITAL LETTER CROSSED SHEI\n+2CC4; C; 2CC5; # COPTIC CAPITAL LETTER OLD COPTIC SHEI\n+2CC6; C; 2CC7; # COPTIC CAPITAL LETTER OLD COPTIC ESH\n+2CC8; C; 2CC9; # COPTIC CAPITAL LETTER AKHMIMIC KHEI\n+2CCA; C; 2CCB; # COPTIC CAPITAL LETTER DIALECT-P HORI\n+2CCC; C; 2CCD; # COPTIC CAPITAL LETTER OLD COPTIC HORI\n+2CCE; C; 2CCF; # COPTIC CAPITAL LETTER OLD COPTIC HA\n+2CD0; C; 2CD1; # COPTIC CAPITAL LETTER L-SHAPED HA\n+2CD2; C; 2CD3; # COPTIC CAPITAL LETTER OLD COPTIC HEI\n+2CD4; C; 2CD5; # COPTIC CAPITAL LETTER OLD COPTIC HAT\n+2CD6; C; 2CD7; # COPTIC CAPITAL LETTER OLD COPTIC GANGIA\n+2CD8; C; 2CD9; # COPTIC CAPITAL LETTER OLD COPTIC DJA\n+2CDA; C; 2CDB; # COPTIC CAPITAL LETTER OLD COPTIC SHIMA\n+2CDC; C; 2CDD; # COPTIC CAPITAL LETTER OLD NUBIAN SHIMA\n+2CDE; C; 2CDF; # COPTIC CAPITAL LETTER OLD NUBIAN NGI\n+2CE0; C; 2CE1; # COPTIC CAPITAL LETTER OLD NUBIAN NYI\n+2CE2; C; 2CE3; # COPTIC CAPITAL LETTER OLD NUBIAN WAU\n+2CEB; C; 2CEC; # COPTIC CAPITAL LETTER CRYPTOGRAMMIC SHEI\n+2CED; C; 2CEE; # COPTIC CAPITAL LETTER CRYPTOGRAMMIC GANGIA\n+2CF2; C; 2CF3; # COPTIC CAPITAL LETTER BOHAIRIC KHEI\n+A640; C; A641; # CYRILLIC CAPITAL LETTER ZEMLYA\n+A642; C; A643; # CYRILLIC CAPITAL LETTER DZELO\n+A644; C; A645; # CYRILLIC CAPITAL LETTER REVERSED DZE\n+A646; C; A647; # CYRILLIC CAPITAL LETTER IOTA\n+A648; C; A649; # CYRILLIC CAPITAL LETTER DJERV\n+A64A; C; A64B; # CYRILLIC CAPITAL LETTER MONOGRAPH UK\n+A64C; C; A64D; # CYRILLIC CAPITAL LETTER BROAD OMEGA\n+A64E; C; A64F; # CYRILLIC CAPITAL LETTER NEUTRAL YER\n+A650; C; A651; # CYRILLIC CAPITAL LETTER YERU WITH BACK YER\n+A652; C; A653; # CYRILLIC CAPITAL LETTER IOTIFIED YAT\n+A654; C; A655; # CYRILLIC CAPITAL LETTER REVERSED YU\n+A656; C; A657; # CYRILLIC CAPITAL LETTER IOTIFIED A\n+A658; C; A659; # CYRILLIC CAPITAL LETTER CLOSED LITTLE YUS\n+A65A; C; A65B; # CYRILLIC CAPITAL LETTER BLENDED YUS\n+A65C; C; A65D; # CYRILLIC CAPITAL LETTER IOTIFIED CLOSED LITTLE YUS\n+A65E; C; A65F; # CYRILLIC CAPITAL LETTER YN\n+A660; C; A661; # CYRILLIC CAPITAL LETTER REVERSED TSE\n+A662; C; A663; # CYRILLIC CAPITAL LETTER SOFT DE\n+A664; C; A665; # CYRILLIC CAPITAL LETTER SOFT EL\n+A666; C; A667; # CYRILLIC CAPITAL LETTER SOFT EM\n+A668; C; A669; # CYRILLIC CAPITAL LETTER MONOCULAR O\n+A66A; C; A66B; # CYRILLIC CAPITAL LETTER BINOCULAR O\n+A66C; C; A66D; # CYRILLIC CAPITAL LETTER DOUBLE MONOCULAR O\n+A680; C; A681; # CYRILLIC CAPITAL LETTER DWE\n+A682; C; A683; # CYRILLIC CAPITAL LETTER DZWE\n+A684; C; A685; # CYRILLIC CAPITAL LETTER ZHWE\n+A686; C; A687; # CYRILLIC CAPITAL LETTER CCHE\n+A688; C; A689; # CYRILLIC CAPITAL LETTER DZZE\n+A68A; C; A68B; # CYRILLIC CAPITAL LETTER TE WITH MIDDLE HOOK\n+A68C; C; A68D; # CYRILLIC CAPITAL LETTER TWE\n+A68E; C; A68F; # CYRILLIC CAPITAL LETTER TSWE\n+A690; C; A691; # CYRILLIC CAPITAL LETTER TSSE\n+A692; C; A693; # CYRILLIC CAPITAL LETTER TCHE\n+A694; C; A695; # CYRILLIC CAPITAL LETTER HWE\n+A696; C; A697; # CYRILLIC CAPITAL LETTER SHWE\n+A698; C; A699; # CYRILLIC CAPITAL LETTER DOUBLE O\n+A69A; C; A69B; # CYRILLIC CAPITAL LETTER CROSSED O\n+A722; C; A723; # LATIN CAPITAL LETTER EGYPTOLOGICAL ALEF\n+A724; C; A725; # LATIN CAPITAL LETTER EGYPTOLOGICAL AIN\n+A726; C; A727; # LATIN CAPITAL LETTER HENG\n+A728; C; A729; # LATIN CAPITAL LETTER TZ\n+A72A; C; A72B; # LATIN CAPITAL LETTER TRESILLO\n+A72C; C; A72D; # LATIN CAPITAL LETTER CUATRILLO\n+A72E; C; A72F; # LATIN CAPITAL LETTER CUATRILLO WITH COMMA\n+A732; C; A733; # LATIN CAPITAL LETTER AA\n+A734; C; A735; # LATIN CAPITAL LETTER AO\n+A736; C; A737; # LATIN CAPITAL LETTER AU\n+A738; C; A739; # LATIN CAPITAL LETTER AV\n+A73A; C; A73B; # LATIN CAPITAL LETTER AV WITH HORIZONTAL BAR\n+A73C; C; A73D; # LATIN CAPITAL LETTER AY\n+A73E; C; A73F; # LATIN CAPITAL LETTER REVERSED C WITH DOT\n+A740; C; A741; # LATIN CAPITAL LETTER K WITH STROKE\n+A742; C; A743; # LATIN CAPITAL LETTER K WITH DIAGONAL STROKE\n+A744; C; A745; # LATIN CAPITAL LETTER K WITH STROKE AND DIAGONAL STROKE\n+A746; C; A747; # LATIN CAPITAL LETTER BROKEN L\n+A748; C; A749; # LATIN CAPITAL LETTER L WITH HIGH STROKE\n+A74A; C; A74B; # LATIN CAPITAL LETTER O WITH LONG STROKE OVERLAY\n+A74C; C; A74D; # LATIN CAPITAL LETTER O WITH LOOP\n+A74E; C; A74F; # LATIN CAPITAL LETTER OO\n+A750; C; A751; # LATIN CAPITAL LETTER P WITH STROKE THROUGH DESCENDER\n+A752; C; A753; # LATIN CAPITAL LETTER P WITH FLOURISH\n+A754; C; A755; # LATIN CAPITAL LETTER P WITH SQUIRREL TAIL\n+A756; C; A757; # LATIN CAPITAL LETTER Q WITH STROKE THROUGH DESCENDER\n+A758; C; A759; # LATIN CAPITAL LETTER Q WITH DIAGONAL STROKE\n+A75A; C; A75B; # LATIN CAPITAL LETTER R ROTUNDA\n+A75C; C; A75D; # LATIN CAPITAL LETTER RUM ROTUNDA\n+A75E; C; A75F; # LATIN CAPITAL LETTER V WITH DIAGONAL STROKE\n+A760; C; A761; # LATIN CAPITAL LETTER VY\n+A762; C; A763; # LATIN CAPITAL LETTER VISIGOTHIC Z\n+A764; C; A765; # LATIN CAPITAL LETTER THORN WITH STROKE\n+A766; C; A767; # LATIN CAPITAL LETTER THORN WITH STROKE THROUGH DESCENDER\n+A768; C; A769; # LATIN CAPITAL LETTER VEND\n+A76A; C; A76B; # LATIN CAPITAL LETTER ET\n+A76C; C; A76D; # LATIN CAPITAL LETTER IS\n+A76E; C; A76F; # LATIN CAPITAL LETTER CON\n+A779; C; A77A; # LATIN CAPITAL LETTER INSULAR D\n+A77B; C; A77C; # LATIN CAPITAL LETTER INSULAR F\n+A77D; C; 1D79; # LATIN CAPITAL LETTER INSULAR G\n+A77E; C; A77F; # LATIN CAPITAL LETTER TURNED INSULAR G\n+A780; C; A781; # LATIN CAPITAL LETTER TURNED L\n+A782; C; A783; # LATIN CAPITAL LETTER INSULAR R\n+A784; C; A785; # LATIN CAPITAL LETTER INSULAR S\n+A786; C; A787; # LATIN CAPITAL LETTER INSULAR T\n+A78B; C; A78C; # LATIN CAPITAL LETTER SALTILLO\n+A78D; C; 0265; # LATIN CAPITAL LETTER TURNED H\n+A790; C; A791; # LATIN CAPITAL LETTER N WITH DESCENDER\n+A792; C; A793; # LATIN CAPITAL LETTER C WITH BAR\n+A796; C; A797; # LATIN CAPITAL LETTER B WITH FLOURISH\n+A798; C; A799; # LATIN CAPITAL LETTER F WITH STROKE\n+A79A; C; A79B; # LATIN CAPITAL LETTER VOLAPUK AE\n+A79C; C; A79D; # LATIN CAPITAL LETTER VOLAPUK OE\n+A79E; C; A79F; # LATIN CAPITAL LETTER VOLAPUK UE\n+A7A0; C; A7A1; # LATIN CAPITAL LETTER G WITH OBLIQUE STROKE\n+A7A2; C; A7A3; # LATIN CAPITAL LETTER K WITH OBLIQUE STROKE\n+A7A4; C; A7A5; # LATIN CAPITAL LETTER N WITH OBLIQUE STROKE\n+A7A6; C; A7A7; # LATIN CAPITAL LETTER R WITH OBLIQUE STROKE\n+A7A8; C; A7A9; # LATIN CAPITAL LETTER S WITH OBLIQUE STROKE\n+A7AA; C; 0266; # LATIN CAPITAL LETTER H WITH HOOK\n+A7AB; C; 025C; # LATIN CAPITAL LETTER REVERSED OPEN E\n+A7AC; C; 0261; # LATIN CAPITAL LETTER SCRIPT G\n+A7AD; C; 026C; # LATIN CAPITAL LETTER L WITH BELT\n+A7AE; C; 026A; # LATIN CAPITAL LETTER SMALL CAPITAL I\n+A7B0; C; 029E; # LATIN CAPITAL LETTER TURNED K\n+A7B1; C; 0287; # LATIN CAPITAL LETTER TURNED T\n+A7B2; C; 029D; # LATIN CAPITAL LETTER J WITH CROSSED-TAIL\n+A7B3; C; AB53; # LATIN CAPITAL LETTER CHI\n+A7B4; C; A7B5; # LATIN CAPITAL LETTER BETA\n+A7B6; C; A7B7; # LATIN CAPITAL LETTER OMEGA\n+A7B8; C; A7B9; # LATIN CAPITAL LETTER U WITH STROKE\n+A7BA; C; A7BB; # LATIN CAPITAL LETTER GLOTTAL A\n+A7BC; C; A7BD; # LATIN CAPITAL LETTER GLOTTAL I\n+A7BE; C; A7BF; # LATIN CAPITAL LETTER GLOTTAL U\n+A7C0; C; A7C1; # LATIN CAPITAL LETTER OLD POLISH O\n+A7C2; C; A7C3; # LATIN CAPITAL LETTER ANGLICANA W\n+A7C4; C; A794; # LATIN CAPITAL LETTER C WITH PALATAL HOOK\n+A7C5; C; 0282; # LATIN CAPITAL LETTER S WITH HOOK\n+A7C6; C; 1D8E; # LATIN CAPITAL LETTER Z WITH PALATAL HOOK\n+A7C7; C; A7C8; # LATIN CAPITAL LETTER D WITH SHORT STROKE OVERLAY\n+A7C9; C; A7CA; # LATIN CAPITAL LETTER S WITH SHORT STROKE OVERLAY\n+A7D0; C; A7D1; # LATIN CAPITAL LETTER CLOSED INSULAR G\n+A7D6; C; A7D7; # LATIN CAPITAL LETTER MIDDLE SCOTS S\n+A7D8; C; A7D9; # LATIN CAPITAL LETTER SIGMOID S\n+A7F5; C; A7F6; # LATIN CAPITAL LETTER REVERSED HALF H\n+AB70; C; 13A0; # CHEROKEE SMALL LETTER A\n+AB71; C; 13A1; # CHEROKEE SMALL LETTER E\n+AB72; C; 13A2; # CHEROKEE SMALL LETTER I\n+AB73; C; 13A3; # CHEROKEE SMALL LETTER O\n+AB74; C; 13A4; # CHEROKEE SMALL LETTER U\n+AB75; C; 13A5; # CHEROKEE SMALL LETTER V\n+AB76; C; 13A6; # CHEROKEE SMALL LETTER GA\n+AB77; C; 13A7; # CHEROKEE SMALL LETTER KA\n+AB78; C; 13A8; # CHEROKEE SMALL LETTER GE\n+AB79; C; 13A9; # CHEROKEE SMALL LETTER GI\n+AB7A; C; 13AA; # CHEROKEE SMALL LETTER GO\n+AB7B; C; 13AB; # CHEROKEE SMALL LETTER GU\n+AB7C; C; 13AC; # CHEROKEE SMALL LETTER GV\n+AB7D; C; 13AD; # CHEROKEE SMALL LETTER HA\n+AB7E; C; 13AE; # CHEROKEE SMALL LETTER HE\n+AB7F; C; 13AF; # CHEROKEE SMALL LETTER HI\n+AB80; C; 13B0; # CHEROKEE SMALL LETTER HO\n+AB81; C; 13B1; # CHEROKEE SMALL LETTER HU\n+AB82; C; 13B2; # CHEROKEE SMALL LETTER HV\n+AB83; C; 13B3; # CHEROKEE SMALL LETTER LA\n+AB84; C; 13B4; # CHEROKEE SMALL LETTER LE\n+AB85; C; 13B5; # CHEROKEE SMALL LETTER LI\n+AB86; C; 13B6; # CHEROKEE SMALL LETTER LO\n+AB87; C; 13B7; # CHEROKEE SMALL LETTER LU\n+AB88; C; 13B8; # CHEROKEE SMALL LETTER LV\n+AB89; C; 13B9; # CHEROKEE SMALL LETTER MA\n+AB8A; C; 13BA; # CHEROKEE SMALL LETTER ME\n+AB8B; C; 13BB; # CHEROKEE SMALL LETTER MI\n+AB8C; C; 13BC; # CHEROKEE SMALL LETTER MO\n+AB8D; C; 13BD; # CHEROKEE SMALL LETTER MU\n+AB8E; C; 13BE; # CHEROKEE SMALL LETTER NA\n+AB8F; C; 13BF; # CHEROKEE SMALL LETTER HNA\n+AB90; C; 13C0; # CHEROKEE SMALL LETTER NAH\n+AB91; C; 13C1; # CHEROKEE SMALL LETTER NE\n+AB92; C; 13C2; # CHEROKEE SMALL LETTER NI\n+AB93; C; 13C3; # CHEROKEE SMALL LETTER NO\n+AB94; C; 13C4; # CHEROKEE SMALL LETTER NU\n+AB95; C; 13C5; # CHEROKEE SMALL LETTER NV\n+AB96; C; 13C6; # CHEROKEE SMALL LETTER QUA\n+AB97; C; 13C7; # CHEROKEE SMALL LETTER QUE\n+AB98; C; 13C8; # CHEROKEE SMALL LETTER QUI\n+AB99; C; 13C9; # CHEROKEE SMALL LETTER QUO\n+AB9A; C; 13CA; # CHEROKEE SMALL LETTER QUU\n+AB9B; C; 13CB; # CHEROKEE SMALL LETTER QUV\n+AB9C; C; 13CC; # CHEROKEE SMALL LETTER SA\n+AB9D; C; 13CD; # CHEROKEE SMALL LETTER S\n+AB9E; C; 13CE; # CHEROKEE SMALL LETTER SE\n+AB9F; C; 13CF; # CHEROKEE SMALL LETTER SI\n+ABA0; C; 13D0; # CHEROKEE SMALL LETTER SO\n+ABA1; C; 13D1; # CHEROKEE SMALL LETTER SU\n+ABA2; C; 13D2; # CHEROKEE SMALL LETTER SV\n+ABA3; C; 13D3; # CHEROKEE SMALL LETTER DA\n+ABA4; C; 13D4; # CHEROKEE SMALL LETTER TA\n+ABA5; C; 13D5; # CHEROKEE SMALL LETTER DE\n+ABA6; C; 13D6; # CHEROKEE SMALL LETTER TE\n+ABA7; C; 13D7; # CHEROKEE SMALL LETTER DI\n+ABA8; C; 13D8; # CHEROKEE SMALL LETTER TI\n+ABA9; C; 13D9; # CHEROKEE SMALL LETTER DO\n+ABAA; C; 13DA; # CHEROKEE SMALL LETTER DU\n+ABAB; C; 13DB; # CHEROKEE SMALL LETTER DV\n+ABAC; C; 13DC; # CHEROKEE SMALL LETTER DLA\n+ABAD; C; 13DD; # CHEROKEE SMALL LETTER TLA\n+ABAE; C; 13DE; # CHEROKEE SMALL LETTER TLE\n+ABAF; C; 13DF; # CHEROKEE SMALL LETTER TLI\n+ABB0; C; 13E0; # CHEROKEE SMALL LETTER TLO\n+ABB1; C; 13E1; # CHEROKEE SMALL LETTER TLU\n+ABB2; C; 13E2; # CHEROKEE SMALL LETTER TLV\n+ABB3; C; 13E3; # CHEROKEE SMALL LETTER TSA\n+ABB4; C; 13E4; # CHEROKEE SMALL LETTER TSE\n+ABB5; C; 13E5; # CHEROKEE SMALL LETTER TSI\n+ABB6; C; 13E6; # CHEROKEE SMALL LETTER TSO\n+ABB7; C; 13E7; # CHEROKEE SMALL LETTER TSU\n+ABB8; C; 13E8; # CHEROKEE SMALL LETTER TSV\n+ABB9; C; 13E9; # CHEROKEE SMALL LETTER WA\n+ABBA; C; 13EA; # CHEROKEE SMALL LETTER WE\n+ABBB; C; 13EB; # CHEROKEE SMALL LETTER WI\n+ABBC; C; 13EC; # CHEROKEE SMALL LETTER WO\n+ABBD; C; 13ED; # CHEROKEE SMALL LETTER WU\n+ABBE; C; 13EE; # CHEROKEE SMALL LETTER WV\n+ABBF; C; 13EF; # CHEROKEE SMALL LETTER YA\n+FB00; F; 0066 0066; # LATIN SMALL LIGATURE FF\n+FB01; F; 0066 0069; # LATIN SMALL LIGATURE FI\n+FB02; F; 0066 006C; # LATIN SMALL LIGATURE FL\n+FB03; F; 0066 0066 0069; # LATIN SMALL LIGATURE FFI\n+FB04; F; 0066 0066 006C; # LATIN SMALL LIGATURE FFL\n+FB05; F; 0073 0074; # LATIN SMALL LIGATURE LONG S T\n+FB06; F; 0073 0074; # LATIN SMALL LIGATURE ST\n+FB13; F; 0574 0576; # ARMENIAN SMALL LIGATURE MEN NOW\n+FB14; F; 0574 0565; # ARMENIAN SMALL LIGATURE MEN ECH\n+FB15; F; 0574 056B; # ARMENIAN SMALL LIGATURE MEN INI\n+FB16; F; 057E 0576; # ARMENIAN SMALL LIGATURE VEW NOW\n+FB17; F; 0574 056D; # ARMENIAN SMALL LIGATURE MEN XEH\n+FF21; C; FF41; # FULLWIDTH LATIN CAPITAL LETTER A\n+FF22; C; FF42; # FULLWIDTH LATIN CAPITAL LETTER B\n+FF23; C; FF43; # FULLWIDTH LATIN CAPITAL LETTER C\n+FF24; C; FF44; # FULLWIDTH LATIN CAPITAL LETTER D\n+FF25; C; FF45; # FULLWIDTH LATIN CAPITAL LETTER E\n+FF26; C; FF46; # FULLWIDTH LATIN CAPITAL LETTER F\n+FF27; C; FF47; # FULLWIDTH LATIN CAPITAL LETTER G\n+FF28; C; FF48; # FULLWIDTH LATIN CAPITAL LETTER H\n+FF29; C; FF49; # FULLWIDTH LATIN CAPITAL LETTER I\n+FF2A; C; FF4A; # FULLWIDTH LATIN CAPITAL LETTER J\n+FF2B; C; FF4B; # FULLWIDTH LATIN CAPITAL LETTER K\n+FF2C; C; FF4C; # FULLWIDTH LATIN CAPITAL LETTER L\n+FF2D; C; FF4D; # FULLWIDTH LATIN CAPITAL LETTER M\n+FF2E; C; FF4E; # FULLWIDTH LATIN CAPITAL LETTER N\n+FF2F; C; FF4F; # FULLWIDTH LATIN CAPITAL LETTER O\n+FF30; C; FF50; # FULLWIDTH LATIN CAPITAL LETTER P\n+FF31; C; FF51; # FULLWIDTH LATIN CAPITAL LETTER Q\n+FF32; C; FF52; # FULLWIDTH LATIN CAPITAL LETTER R\n+FF33; C; FF53; # FULLWIDTH LATIN CAPITAL LETTER S\n+FF34; C; FF54; # FULLWIDTH LATIN CAPITAL LETTER T\n+FF35; C; FF55; # FULLWIDTH LATIN CAPITAL LETTER U\n+FF36; C; FF56; # FULLWIDTH LATIN CAPITAL LETTER V\n+FF37; C; FF57; # FULLWIDTH LATIN CAPITAL LETTER W\n+FF38; C; FF58; # FULLWIDTH LATIN CAPITAL LETTER X\n+FF39; C; FF59; # FULLWIDTH LATIN CAPITAL LETTER Y\n+FF3A; C; FF5A; # FULLWIDTH LATIN CAPITAL LETTER Z\n+10400; C; 10428; # DESERET CAPITAL LETTER LONG I\n+10401; C; 10429; # DESERET CAPITAL LETTER LONG E\n+10402; C; 1042A; # DESERET CAPITAL LETTER LONG A\n+10403; C; 1042B; # DESERET CAPITAL LETTER LONG AH\n+10404; C; 1042C; # DESERET CAPITAL LETTER LONG O\n+10405; C; 1042D; # DESERET CAPITAL LETTER LONG OO\n+10406; C; 1042E; # DESERET CAPITAL LETTER SHORT I\n+10407; C; 1042F; # DESERET CAPITAL LETTER SHORT E\n+10408; C; 10430; # DESERET CAPITAL LETTER SHORT A\n+10409; C; 10431; # DESERET CAPITAL LETTER SHORT AH\n+1040A; C; 10432; # DESERET CAPITAL LETTER SHORT O\n+1040B; C; 10433; # DESERET CAPITAL LETTER SHORT OO\n+1040C; C; 10434; # DESERET CAPITAL LETTER AY\n+1040D; C; 10435; # DESERET CAPITAL LETTER OW\n+1040E; C; 10436; # DESERET CAPITAL LETTER WU\n+1040F; C; 10437; # DESERET CAPITAL LETTER YEE\n+10410; C; 10438; # DESERET CAPITAL LETTER H\n+10411; C; 10439; # DESERET CAPITAL LETTER PEE\n+10412; C; 1043A; # DESERET CAPITAL LETTER BEE\n+10413; C; 1043B; # DESERET CAPITAL LETTER TEE\n+10414; C; 1043C; # DESERET CAPITAL LETTER DEE\n+10415; C; 1043D; # DESERET CAPITAL LETTER CHEE\n+10416; C; 1043E; # DESERET CAPITAL LETTER JEE\n+10417; C; 1043F; # DESERET CAPITAL LETTER KAY\n+10418; C; 10440; # DESERET CAPITAL LETTER GAY\n+10419; C; 10441; # DESERET CAPITAL LETTER EF\n+1041A; C; 10442; # DESERET CAPITAL LETTER VEE\n+1041B; C; 10443; # DESERET CAPITAL LETTER ETH\n+1041C; C; 10444; # DESERET CAPITAL LETTER THEE\n+1041D; C; 10445; # DESERET CAPITAL LETTER ES\n+1041E; C; 10446; # DESERET CAPITAL LETTER ZEE\n+1041F; C; 10447; # DESERET CAPITAL LETTER ESH\n+10420; C; 10448; # DESERET CAPITAL LETTER ZHEE\n+10421; C; 10449; # DESERET CAPITAL LETTER ER\n+10422; C; 1044A; # DESERET CAPITAL LETTER EL\n+10423; C; 1044B; # DESERET CAPITAL LETTER EM\n+10424; C; 1044C; # DESERET CAPITAL LETTER EN\n+10425; C; 1044D; # DESERET CAPITAL LETTER ENG\n+10426; C; 1044E; # DESERET CAPITAL LETTER OI\n+10427; C; 1044F; # DESERET CAPITAL LETTER EW\n+104B0; C; 104D8; # OSAGE CAPITAL LETTER A\n+104B1; C; 104D9; # OSAGE CAPITAL LETTER AI\n+104B2; C; 104DA; # OSAGE CAPITAL LETTER AIN\n+104B3; C; 104DB; # OSAGE CAPITAL LETTER AH\n+104B4; C; 104DC; # OSAGE CAPITAL LETTER BRA\n+104B5; C; 104DD; # OSAGE CAPITAL LETTER CHA\n+104B6; C; 104DE; # OSAGE CAPITAL LETTER EHCHA\n+104B7; C; 104DF; # OSAGE CAPITAL LETTER E\n+104B8; C; 104E0; # OSAGE CAPITAL LETTER EIN\n+104B9; C; 104E1; # OSAGE CAPITAL LETTER HA\n+104BA; C; 104E2; # OSAGE CAPITAL LETTER HYA\n+104BB; C; 104E3; # OSAGE CAPITAL LETTER I\n+104BC; C; 104E4; # OSAGE CAPITAL LETTER KA\n+104BD; C; 104E5; # OSAGE CAPITAL LETTER EHKA\n+104BE; C; 104E6; # OSAGE CAPITAL LETTER KYA\n+104BF; C; 104E7; # OSAGE CAPITAL LETTER LA\n+104C0; C; 104E8; # OSAGE CAPITAL LETTER MA\n+104C1; C; 104E9; # OSAGE CAPITAL LETTER NA\n+104C2; C; 104EA; # OSAGE CAPITAL LETTER O\n+104C3; C; 104EB; # OSAGE CAPITAL LETTER OIN\n+104C4; C; 104EC; # OSAGE CAPITAL LETTER PA\n+104C5; C; 104ED; # OSAGE CAPITAL LETTER EHPA\n+104C6; C; 104EE; # OSAGE CAPITAL LETTER SA\n+104C7; C; 104EF; # OSAGE CAPITAL LETTER SHA\n+104C8; C; 104F0; # OSAGE CAPITAL LETTER TA\n+104C9; C; 104F1; # OSAGE CAPITAL LETTER EHTA\n+104CA; C; 104F2; # OSAGE CAPITAL LETTER TSA\n+104CB; C; 104F3; # OSAGE CAPITAL LETTER EHTSA\n+104CC; C; 104F4; # OSAGE CAPITAL LETTER TSHA\n+104CD; C; 104F5; # OSAGE CAPITAL LETTER DHA\n+104CE; C; 104F6; # OSAGE CAPITAL LETTER U\n+104CF; C; 104F7; # OSAGE CAPITAL LETTER WA\n+104D0; C; 104F8; # OSAGE CAPITAL LETTER KHA\n+104D1; C; 104F9; # OSAGE CAPITAL LETTER GHA\n+104D2; C; 104FA; # OSAGE CAPITAL LETTER ZA\n+104D3; C; 104FB; # OSAGE CAPITAL LETTER ZHA\n+10570; C; 10597; # VITHKUQI CAPITAL LETTER A\n+10571; C; 10598; # VITHKUQI CAPITAL LETTER BBE\n+10572; C; 10599; # VITHKUQI CAPITAL LETTER BE\n+10573; C; 1059A; # VITHKUQI CAPITAL LETTER CE\n+10574; C; 1059B; # VITHKUQI CAPITAL LETTER CHE\n+10575; C; 1059C; # VITHKUQI CAPITAL LETTER DE\n+10576; C; 1059D; # VITHKUQI CAPITAL LETTER DHE\n+10577; C; 1059E; # VITHKUQI CAPITAL LETTER EI\n+10578; C; 1059F; # VITHKUQI CAPITAL LETTER E\n+10579; C; 105A0; # VITHKUQI CAPITAL LETTER FE\n+1057A; C; 105A1; # VITHKUQI CAPITAL LETTER GA\n+1057C; C; 105A3; # VITHKUQI CAPITAL LETTER HA\n+1057D; C; 105A4; # VITHKUQI CAPITAL LETTER HHA\n+1057E; C; 105A5; # VITHKUQI CAPITAL LETTER I\n+1057F; C; 105A6; # VITHKUQI CAPITAL LETTER IJE\n+10580; C; 105A7; # VITHKUQI CAPITAL LETTER JE\n+10581; C; 105A8; # VITHKUQI CAPITAL LETTER KA\n+10582; C; 105A9; # VITHKUQI CAPITAL LETTER LA\n+10583; C; 105AA; # VITHKUQI CAPITAL LETTER LLA\n+10584; C; 105AB; # VITHKUQI CAPITAL LETTER ME\n+10585; C; 105AC; # VITHKUQI CAPITAL LETTER NE\n+10586; C; 105AD; # VITHKUQI CAPITAL LETTER NJE\n+10587; C; 105AE; # VITHKUQI CAPITAL LETTER O\n+10588; C; 105AF; # VITHKUQI CAPITAL LETTER PE\n+10589; C; 105B0; # VITHKUQI CAPITAL LETTER QA\n+1058A; C; 105B1; # VITHKUQI CAPITAL LETTER RE\n+1058C; C; 105B3; # VITHKUQI CAPITAL LETTER SE\n+1058D; C; 105B4; # VITHKUQI CAPITAL LETTER SHE\n+1058E; C; 105B5; # VITHKUQI CAPITAL LETTER TE\n+1058F; C; 105B6; # VITHKUQI CAPITAL LETTER THE\n+10590; C; 105B7; # VITHKUQI CAPITAL LETTER U\n+10591; C; 105B8; # VITHKUQI CAPITAL LETTER VE\n+10592; C; 105B9; # VITHKUQI CAPITAL LETTER XE\n+10594; C; 105BB; # VITHKUQI CAPITAL LETTER Y\n+10595; C; 105BC; # VITHKUQI CAPITAL LETTER ZE\n+10C80; C; 10CC0; # OLD HUNGARIAN CAPITAL LETTER A\n+10C81; C; 10CC1; # OLD HUNGARIAN CAPITAL LETTER AA\n+10C82; C; 10CC2; # OLD HUNGARIAN CAPITAL LETTER EB\n+10C83; C; 10CC3; # OLD HUNGARIAN CAPITAL LETTER AMB\n+10C84; C; 10CC4; # OLD HUNGARIAN CAPITAL LETTER EC\n+10C85; C; 10CC5; # OLD HUNGARIAN CAPITAL LETTER ENC\n+10C86; C; 10CC6; # OLD HUNGARIAN CAPITAL LETTER ECS\n+10C87; C; 10CC7; # OLD HUNGARIAN CAPITAL LETTER ED\n+10C88; C; 10CC8; # OLD HUNGARIAN CAPITAL LETTER AND\n+10C89; C; 10CC9; # OLD HUNGARIAN CAPITAL LETTER E\n+10C8A; C; 10CCA; # OLD HUNGARIAN CAPITAL LETTER CLOSE E\n+10C8B; C; 10CCB; # OLD HUNGARIAN CAPITAL LETTER EE\n+10C8C; C; 10CCC; # OLD HUNGARIAN CAPITAL LETTER EF\n+10C8D; C; 10CCD; # OLD HUNGARIAN CAPITAL LETTER EG\n+10C8E; C; 10CCE; # OLD HUNGARIAN CAPITAL LETTER EGY\n+10C8F; C; 10CCF; # OLD HUNGARIAN CAPITAL LETTER EH\n+10C90; C; 10CD0; # OLD HUNGARIAN CAPITAL LETTER I\n+10C91; C; 10CD1; # OLD HUNGARIAN CAPITAL LETTER II\n+10C92; C; 10CD2; # OLD HUNGARIAN CAPITAL LETTER EJ\n+10C93; C; 10CD3; # OLD HUNGARIAN CAPITAL LETTER EK\n+10C94; C; 10CD4; # OLD HUNGARIAN CAPITAL LETTER AK\n+10C95; C; 10CD5; # OLD HUNGARIAN CAPITAL LETTER UNK\n+10C96; C; 10CD6; # OLD HUNGARIAN CAPITAL LETTER EL\n+10C97; C; 10CD7; # OLD HUNGARIAN CAPITAL LETTER ELY\n+10C98; C; 10CD8; # OLD HUNGARIAN CAPITAL LETTER EM\n+10C99; C; 10CD9; # OLD HUNGARIAN CAPITAL LETTER EN\n+10C9A; C; 10CDA; # OLD HUNGARIAN CAPITAL LETTER ENY\n+10C9B; C; 10CDB; # OLD HUNGARIAN CAPITAL LETTER O\n+10C9C; C; 10CDC; # OLD HUNGARIAN CAPITAL LETTER OO\n+10C9D; C; 10CDD; # OLD HUNGARIAN CAPITAL LETTER NIKOLSBURG OE\n+10C9E; C; 10CDE; # OLD HUNGARIAN CAPITAL LETTER RUDIMENTA OE\n+10C9F; C; 10CDF; # OLD HUNGARIAN CAPITAL LETTER OEE\n+10CA0; C; 10CE0; # OLD HUNGARIAN CAPITAL LETTER EP\n+10CA1; C; 10CE1; # OLD HUNGARIAN CAPITAL LETTER EMP\n+10CA2; C; 10CE2; # OLD HUNGARIAN CAPITAL LETTER ER\n+10CA3; C; 10CE3; # OLD HUNGARIAN CAPITAL LETTER SHORT ER\n+10CA4; C; 10CE4; # OLD HUNGARIAN CAPITAL LETTER ES\n+10CA5; C; 10CE5; # OLD HUNGARIAN CAPITAL LETTER ESZ\n+10CA6; C; 10CE6; # OLD HUNGARIAN CAPITAL LETTER ET\n+10CA7; C; 10CE7; # OLD HUNGARIAN CAPITAL LETTER ENT\n+10CA8; C; 10CE8; # OLD HUNGARIAN CAPITAL LETTER ETY\n+10CA9; C; 10CE9; # OLD HUNGARIAN CAPITAL LETTER ECH\n+10CAA; C; 10CEA; # OLD HUNGARIAN CAPITAL LETTER U\n+10CAB; C; 10CEB; # OLD HUNGARIAN CAPITAL LETTER UU\n+10CAC; C; 10CEC; # OLD HUNGARIAN CAPITAL LETTER NIKOLSBURG UE\n+10CAD; C; 10CED; # OLD HUNGARIAN CAPITAL LETTER RUDIMENTA UE\n+10CAE; C; 10CEE; # OLD HUNGARIAN CAPITAL LETTER EV\n+10CAF; C; 10CEF; # OLD HUNGARIAN CAPITAL LETTER EZ\n+10CB0; C; 10CF0; # OLD HUNGARIAN CAPITAL LETTER EZS\n+10CB1; C; 10CF1; # OLD HUNGARIAN CAPITAL LETTER ENT-SHAPED SIGN\n+10CB2; C; 10CF2; # OLD HUNGARIAN CAPITAL LETTER US\n+118A0; C; 118C0; # WARANG CITI CAPITAL LETTER NGAA\n+118A1; C; 118C1; # WARANG CITI CAPITAL LETTER A\n+118A2; C; 118C2; # WARANG CITI CAPITAL LETTER WI\n+118A3; C; 118C3; # WARANG CITI CAPITAL LETTER YU\n+118A4; C; 118C4; # WARANG CITI CAPITAL LETTER YA\n+118A5; C; 118C5; # WARANG CITI CAPITAL LETTER YO\n+118A6; C; 118C6; # WARANG CITI CAPITAL LETTER II\n+118A7; C; 118C7; # WARANG CITI CAPITAL LETTER UU\n+118A8; C; 118C8; # WARANG CITI CAPITAL LETTER E\n+118A9; C; 118C9; # WARANG CITI CAPITAL LETTER O\n+118AA; C; 118CA; # WARANG CITI CAPITAL LETTER ANG\n+118AB; C; 118CB; # WARANG CITI CAPITAL LETTER GA\n+118AC; C; 118CC; # WARANG CITI CAPITAL LETTER KO\n+118AD; C; 118CD; # WARANG CITI CAPITAL LETTER ENY\n+118AE; C; 118CE; # WARANG CITI CAPITAL LETTER YUJ\n+118AF; C; 118CF; # WARANG CITI CAPITAL LETTER UC\n+118B0; C; 118D0; # WARANG CITI CAPITAL LETTER ENN\n+118B1; C; 118D1; # WARANG CITI CAPITAL LETTER ODD\n+118B2; C; 118D2; # WARANG CITI CAPITAL LETTER TTE\n+118B3; C; 118D3; # WARANG CITI CAPITAL LETTER NUNG\n+118B4; C; 118D4; # WARANG CITI CAPITAL LETTER DA\n+118B5; C; 118D5; # WARANG CITI CAPITAL LETTER AT\n+118B6; C; 118D6; # WARANG CITI CAPITAL LETTER AM\n+118B7; C; 118D7; # WARANG CITI CAPITAL LETTER BU\n+118B8; C; 118D8; # WARANG CITI CAPITAL LETTER PU\n+118B9; C; 118D9; # WARANG CITI CAPITAL LETTER HIYO\n+118BA; C; 118DA; # WARANG CITI CAPITAL LETTER HOLO\n+118BB; C; 118DB; # WARANG CITI CAPITAL LETTER HORR\n+118BC; C; 118DC; # WARANG CITI CAPITAL LETTER HAR\n+118BD; C; 118DD; # WARANG CITI CAPITAL LETTER SSUU\n+118BE; C; 118DE; # WARANG CITI CAPITAL LETTER SII\n+118BF; C; 118DF; # WARANG CITI CAPITAL LETTER VIYO\n+16E40; C; 16E60; # MEDEFAIDRIN CAPITAL LETTER M\n+16E41; C; 16E61; # MEDEFAIDRIN CAPITAL LETTER S\n+16E42; C; 16E62; # MEDEFAIDRIN CAPITAL LETTER V\n+16E43; C; 16E63; # MEDEFAIDRIN CAPITAL LETTER W\n+16E44; C; 16E64; # MEDEFAIDRIN CAPITAL LETTER ATIU\n+16E45; C; 16E65; # MEDEFAIDRIN CAPITAL LETTER Z\n+16E46; C; 16E66; # MEDEFAIDRIN CAPITAL LETTER KP\n+16E47; C; 16E67; # MEDEFAIDRIN CAPITAL LETTER P\n+16E48; C; 16E68; # MEDEFAIDRIN CAPITAL LETTER T\n+16E49; C; 16E69; # MEDEFAIDRIN CAPITAL LETTER G\n+16E4A; C; 16E6A; # MEDEFAIDRIN CAPITAL LETTER F\n+16E4B; C; 16E6B; # MEDEFAIDRIN CAPITAL LETTER I\n+16E4C; C; 16E6C; # MEDEFAIDRIN CAPITAL LETTER K\n+16E4D; C; 16E6D; # MEDEFAIDRIN CAPITAL LETTER A\n+16E4E; C; 16E6E; # MEDEFAIDRIN CAPITAL LETTER J\n+16E4F; C; 16E6F; # MEDEFAIDRIN CAPITAL LETTER E\n+16E50; C; 16E70; # MEDEFAIDRIN CAPITAL LETTER B\n+16E51; C; 16E71; # MEDEFAIDRIN CAPITAL LETTER C\n+16E52; C; 16E72; # MEDEFAIDRIN CAPITAL LETTER U\n+16E53; C; 16E73; # MEDEFAIDRIN CAPITAL LETTER YU\n+16E54; C; 16E74; # MEDEFAIDRIN CAPITAL LETTER L\n+16E55; C; 16E75; # MEDEFAIDRIN CAPITAL LETTER Q\n+16E56; C; 16E76; # MEDEFAIDRIN CAPITAL LETTER HP\n+16E57; C; 16E77; # MEDEFAIDRIN CAPITAL LETTER NY\n+16E58; C; 16E78; # MEDEFAIDRIN CAPITAL LETTER X\n+16E59; C; 16E79; # MEDEFAIDRIN CAPITAL LETTER D\n+16E5A; C; 16E7A; # MEDEFAIDRIN CAPITAL LETTER OE\n+16E5B; C; 16E7B; # MEDEFAIDRIN CAPITAL LETTER N\n+16E5C; C; 16E7C; # MEDEFAIDRIN CAPITAL LETTER R\n+16E5D; C; 16E7D; # MEDEFAIDRIN CAPITAL LETTER O\n+16E5E; C; 16E7E; # MEDEFAIDRIN CAPITAL LETTER AI\n+16E5F; C; 16E7F; # MEDEFAIDRIN CAPITAL LETTER Y\n+1E900; C; 1E922; # ADLAM CAPITAL LETTER ALIF\n+1E901; C; 1E923; # ADLAM CAPITAL LETTER DAALI\n+1E902; C; 1E924; # ADLAM CAPITAL LETTER LAAM\n+1E903; C; 1E925; # ADLAM CAPITAL LETTER MIIM\n+1E904; C; 1E926; # ADLAM CAPITAL LETTER BA\n+1E905; C; 1E927; # ADLAM CAPITAL LETTER SINNYIIYHE\n+1E906; C; 1E928; # ADLAM CAPITAL LETTER PE\n+1E907; C; 1E929; # ADLAM CAPITAL LETTER BHE\n+1E908; C; 1E92A; # ADLAM CAPITAL LETTER RA\n+1E909; C; 1E92B; # ADLAM CAPITAL LETTER E\n+1E90A; C; 1E92C; # ADLAM CAPITAL LETTER FA\n+1E90B; C; 1E92D; # ADLAM CAPITAL LETTER I\n+1E90C; C; 1E92E; # ADLAM CAPITAL LETTER O\n+1E90D; C; 1E92F; # ADLAM CAPITAL LETTER DHA\n+1E90E; C; 1E930; # ADLAM CAPITAL LETTER YHE\n+1E90F; C; 1E931; # ADLAM CAPITAL LETTER WAW\n+1E910; C; 1E932; # ADLAM CAPITAL LETTER NUN\n+1E911; C; 1E933; # ADLAM CAPITAL LETTER KAF\n+1E912; C; 1E934; # ADLAM CAPITAL LETTER YA\n+1E913; C; 1E935; # ADLAM CAPITAL LETTER U\n+1E914; C; 1E936; # ADLAM CAPITAL LETTER JIIM\n+1E915; C; 1E937; # ADLAM CAPITAL LETTER CHI\n+1E916; C; 1E938; # ADLAM CAPITAL LETTER HA\n+1E917; C; 1E939; # ADLAM CAPITAL LETTER QAAF\n+1E918; C; 1E93A; # ADLAM CAPITAL LETTER GA\n+1E919; C; 1E93B; # ADLAM CAPITAL LETTER NYA\n+1E91A; C; 1E93C; # ADLAM CAPITAL LETTER TU\n+1E91B; C; 1E93D; # ADLAM CAPITAL LETTER NHA\n+1E91C; C; 1E93E; # ADLAM CAPITAL LETTER VA\n+1E91D; C; 1E93F; # ADLAM CAPITAL LETTER KHA\n+1E91E; C; 1E940; # ADLAM CAPITAL LETTER GBE\n+1E91F; C; 1E941; # ADLAM CAPITAL LETTER ZAL\n+1E920; C; 1E942; # ADLAM CAPITAL LETTER KPO\n+1E921; C; 1E943; # ADLAM CAPITAL LETTER SHA\n+'''\n+\n+\n+def _parse_unichr(s):\n+    s = int(s, 16)\n+    try:\n+        return compat_chr(s)\n+    except ValueError:\n+        # work around \"unichr() arg not in range(0x10000) (narrow Python build)\"\n+        return ('\\\\U%08x' % s).decode('unicode-escape')\n+\n+\n+_map = dict(\n+    (_parse_unichr(from_), ''.join(map(_parse_unichr, to_.split(' '))))\n+    for from_, type_, to_, _ in (\n+        l.split('; ', 3) for l in _map_str.splitlines() if l and not l[0] == '#')\n+    if type_ in ('C', 'F'))\n+del _map_str\n+\n+\n+def casefold(s):\n+    assert isinstance(s, compat_str)\n+    return ''.join((_map.get(c, c) for c in s))\n+\n+\n+__all__ = [\n+    'casefold',\n+]\ndiff --git a/youtube_dl/compat.py b/youtube_dl/compat.py\nindex 2004a405a8e..ed1a33cf2fb 100644\n--- a/youtube_dl/compat.py\n+++ b/youtube_dl/compat.py\n@@ -1,10 +1,12 @@\n # coding: utf-8\n from __future__ import unicode_literals\n+from __future__ import division\n \n import base64\n import binascii\n import collections\n import ctypes\n+import datetime\n import email\n import getpass\n import io\n@@ -19,8 +21,31 @@\n import struct\n import subprocess\n import sys\n+import types\n import xml.etree.ElementTree\n \n+# naming convention\n+# 'compat_' + Python3_name.replace('.', '_')\n+# other aliases exist for convenience and/or legacy\n+\n+# deal with critical unicode/str things first\n+try:\n+    # Python 2\n+    compat_str, compat_basestring, compat_chr = (\n+        unicode, basestring, unichr\n+    )\n+except NameError:\n+    compat_str, compat_basestring, compat_chr = (\n+        str, (str, bytes), chr\n+    )\n+\n+# casefold\n+try:\n+    compat_str.casefold\n+    compat_casefold = lambda s: s.casefold()\n+except AttributeError:\n+    from .casefold import casefold as compat_casefold\n+\n try:\n     import collections.abc as compat_collections_abc\n except ImportError:\n@@ -31,6 +56,29 @@\n except ImportError:  # Python 2\n     import urllib2 as compat_urllib_request\n \n+# Also fix up lack of method arg in old Pythons\n+try:\n+    type(compat_urllib_request.Request('http://127.0.0.1', method='GET'))\n+except TypeError:\n+    def _add_init_method_arg(cls):\n+\n+        init = cls.__init__\n+\n+        def wrapped_init(self, *args, **kwargs):\n+            method = kwargs.pop('method', 'GET')\n+            init(self, *args, **kwargs)\n+            if any(callable(x.__dict__.get('get_method')) for x in (self.__class__, self) if x != cls):\n+                # allow instance or its subclass to override get_method()\n+                return\n+            if self.has_data() and method == 'GET':\n+                method = 'POST'\n+            self.get_method = types.MethodType(lambda _: method, self)\n+\n+        cls.__init__ = wrapped_init\n+\n+    _add_init_method_arg(compat_urllib_request.Request)\n+    del _add_init_method_arg\n+\n try:\n     import urllib.error as compat_urllib_error\n except ImportError:  # Python 2\n@@ -40,26 +88,32 @@\n     import urllib.parse as compat_urllib_parse\n except ImportError:  # Python 2\n     import urllib as compat_urllib_parse\n+    import urlparse as _urlparse\n+    for a in dir(_urlparse):\n+        if not hasattr(compat_urllib_parse, a):\n+            setattr(compat_urllib_parse, a, getattr(_urlparse, a))\n+    del _urlparse\n \n-try:\n-    from urllib.parse import urlparse as compat_urllib_parse_urlparse\n-except ImportError:  # Python 2\n-    from urlparse import urlparse as compat_urllib_parse_urlparse\n-\n-try:\n-    import urllib.parse as compat_urlparse\n-except ImportError:  # Python 2\n-    import urlparse as compat_urlparse\n+# unfavoured aliases\n+compat_urlparse = compat_urllib_parse\n+compat_urllib_parse_urlparse = compat_urllib_parse.urlparse\n \n try:\n     import urllib.response as compat_urllib_response\n except ImportError:  # Python 2\n     import urllib as compat_urllib_response\n \n+try:\n+    compat_urllib_response.addinfourl.status\n+except AttributeError:\n+    # .getcode() is deprecated in Py 3.\n+    compat_urllib_response.addinfourl.status = property(lambda self: self.getcode())\n+\n try:\n     import http.cookiejar as compat_cookiejar\n except ImportError:  # Python 2\n     import cookielib as compat_cookiejar\n+compat_http_cookiejar = compat_cookiejar\n \n if sys.version_info[0] == 2:\n     class compat_cookiejar_Cookie(compat_cookiejar.Cookie):\n@@ -71,20 +125,35 @@ def __init__(self, version, name, value, *args, **kwargs):\n             compat_cookiejar.Cookie.__init__(self, version, name, value, *args, **kwargs)\n else:\n     compat_cookiejar_Cookie = compat_cookiejar.Cookie\n+compat_http_cookiejar_Cookie = compat_cookiejar_Cookie\n \n try:\n     import http.cookies as compat_cookies\n except ImportError:  # Python 2\n     import Cookie as compat_cookies\n+compat_http_cookies = compat_cookies\n \n-if sys.version_info[0] == 2:\n+if sys.version_info[0] == 2 or sys.version_info < (3, 3):\n     class compat_cookies_SimpleCookie(compat_cookies.SimpleCookie):\n         def load(self, rawdata):\n-            if isinstance(rawdata, compat_str):\n-                rawdata = str(rawdata)\n-            return super(compat_cookies_SimpleCookie, self).load(rawdata)\n+            must_have_value = 0\n+            if not isinstance(rawdata, dict):\n+                if sys.version_info[:2] != (2, 7) or sys.platform.startswith('java'):\n+                    # attribute must have value for parsing\n+                    rawdata, must_have_value = re.subn(\n+                        r'(?i)(;\\s*)(secure|httponly)(\\s*(?:;|$))', r'\\1\\2=\\2\\3', rawdata)\n+                if sys.version_info[0] == 2:\n+                    if isinstance(rawdata, compat_str):\n+                        rawdata = str(rawdata)\n+            super(compat_cookies_SimpleCookie, self).load(rawdata)\n+            if must_have_value > 0:\n+                for morsel in self.values():\n+                    for attr in ('secure', 'httponly'):\n+                        if morsel.get(attr):\n+                            morsel[attr] = True\n else:\n     compat_cookies_SimpleCookie = compat_cookies.SimpleCookie\n+compat_http_cookies_SimpleCookie = compat_cookies_SimpleCookie\n \n try:\n     import html.entities as compat_html_entities\n@@ -2333,39 +2402,45 @@ def load(self, rawdata):\n     import http.client as compat_http_client\n except ImportError:  # Python 2\n     import httplib as compat_http_client\n+try:\n+    compat_http_client.HTTPResponse.getcode\n+except AttributeError:\n+    # Py < 3.1\n+    compat_http_client.HTTPResponse.getcode = lambda self: self.status\n \n try:\n     from urllib.error import HTTPError as compat_HTTPError\n except ImportError:  # Python 2\n     from urllib2 import HTTPError as compat_HTTPError\n+compat_urllib_HTTPError = compat_HTTPError\n \n try:\n     from urllib.request import urlretrieve as compat_urlretrieve\n except ImportError:  # Python 2\n     from urllib import urlretrieve as compat_urlretrieve\n+compat_urllib_request_urlretrieve = compat_urlretrieve\n \n try:\n+    from HTMLParser import (\n+        HTMLParser as compat_HTMLParser,\n+        HTMLParseError as compat_HTMLParseError)\n+except ImportError:  # Python 3\n     from html.parser import HTMLParser as compat_HTMLParser\n-except ImportError:  # Python 2\n-    from HTMLParser import HTMLParser as compat_HTMLParser\n-\n-try:  # Python 2\n-    from HTMLParser import HTMLParseError as compat_HTMLParseError\n-except ImportError:  # Python <3.4\n     try:\n         from html.parser import HTMLParseError as compat_HTMLParseError\n     except ImportError:  # Python >3.4\n-\n-        # HTMLParseError has been deprecated in Python 3.3 and removed in\n+        # HTMLParseError was deprecated in Python 3.3 and removed in\n         # Python 3.5. Introducing dummy exception for Python >3.5 for compatible\n         # and uniform cross-version exception handling\n         class compat_HTMLParseError(Exception):\n             pass\n+compat_html_parser_HTMLParser = compat_HTMLParser\n+compat_html_parser_HTMLParseError = compat_HTMLParseError\n \n try:\n-    from subprocess import DEVNULL\n-    compat_subprocess_get_DEVNULL = lambda: DEVNULL\n-except ImportError:\n+    _DEVNULL = subprocess.DEVNULL\n+    compat_subprocess_get_DEVNULL = lambda: _DEVNULL\n+except AttributeError:\n     compat_subprocess_get_DEVNULL = lambda: open(os.path.devnull, 'w')\n \n try:\n@@ -2373,15 +2448,12 @@ class compat_HTMLParseError(Exception):\n except ImportError:\n     import BaseHTTPServer as compat_http_server\n \n-try:\n-    compat_str = unicode  # Python 2\n-except NameError:\n-    compat_str = str\n-\n try:\n     from urllib.parse import unquote_to_bytes as compat_urllib_parse_unquote_to_bytes\n     from urllib.parse import unquote as compat_urllib_parse_unquote\n     from urllib.parse import unquote_plus as compat_urllib_parse_unquote_plus\n+    from urllib.parse import urlencode as compat_urllib_parse_urlencode\n+    from urllib.parse import parse_qs as compat_parse_qs\n except ImportError:  # Python 2\n     _asciire = (compat_urllib_parse._asciire if hasattr(compat_urllib_parse, '_asciire')\n                 else re.compile(r'([\\x00-\\x7f]+)'))\n@@ -2448,9 +2520,6 @@ def compat_urllib_parse_unquote_plus(string, encoding='utf-8', errors='replace')\n         string = string.replace('+', ' ')\n         return compat_urllib_parse_unquote(string, encoding, errors)\n \n-try:\n-    from urllib.parse import urlencode as compat_urllib_parse_urlencode\n-except ImportError:  # Python 2\n     # Python 2 will choke in urlencode on mixture of byte and unicode strings.\n     # Possible solutions are to either port it from python 3 with all\n     # the friends or manually ensure input query contains only byte strings.\n@@ -2472,7 +2541,62 @@ def encode_dict(d):\n         def encode_list(l):\n             return [encode_elem(e) for e in l]\n \n-        return compat_urllib_parse.urlencode(encode_elem(query), doseq=doseq)\n+        return compat_urllib_parse._urlencode(encode_elem(query), doseq=doseq)\n+\n+    # HACK: The following is the correct parse_qs implementation from cpython 3's stdlib.\n+    # Python 2's version is apparently totally broken\n+    def _parse_qsl(qs, keep_blank_values=False, strict_parsing=False,\n+                   encoding='utf-8', errors='replace'):\n+        qs, _coerce_result = qs, compat_str\n+        pairs = [s2 for s1 in qs.split('&') for s2 in s1.split(';')]\n+        r = []\n+        for name_value in pairs:\n+            if not name_value and not strict_parsing:\n+                continue\n+            nv = name_value.split('=', 1)\n+            if len(nv) != 2:\n+                if strict_parsing:\n+                    raise ValueError('bad query field: %r' % (name_value,))\n+                # Handle case of a control-name with no equal sign\n+                if keep_blank_values:\n+                    nv.append('')\n+                else:\n+                    continue\n+            if len(nv[1]) or keep_blank_values:\n+                name = nv[0].replace('+', ' ')\n+                name = compat_urllib_parse_unquote(\n+                    name, encoding=encoding, errors=errors)\n+                name = _coerce_result(name)\n+                value = nv[1].replace('+', ' ')\n+                value = compat_urllib_parse_unquote(\n+                    value, encoding=encoding, errors=errors)\n+                value = _coerce_result(value)\n+                r.append((name, value))\n+        return r\n+\n+    def compat_parse_qs(qs, keep_blank_values=False, strict_parsing=False,\n+                        encoding='utf-8', errors='replace'):\n+        parsed_result = {}\n+        pairs = _parse_qsl(qs, keep_blank_values, strict_parsing,\n+                           encoding=encoding, errors=errors)\n+        for name, value in pairs:\n+            if name in parsed_result:\n+                parsed_result[name].append(value)\n+            else:\n+                parsed_result[name] = [value]\n+        return parsed_result\n+\n+    setattr(compat_urllib_parse, '_urlencode',\n+            getattr(compat_urllib_parse, 'urlencode'))\n+    for name, fix in (\n+            ('unquote_to_bytes', compat_urllib_parse_unquote_to_bytes),\n+            ('parse_unquote', compat_urllib_parse_unquote),\n+            ('unquote_plus', compat_urllib_parse_unquote_plus),\n+            ('urlencode', compat_urllib_parse_urlencode),\n+            ('parse_qs', compat_parse_qs)):\n+        setattr(compat_urllib_parse, name, fix)\n+\n+compat_urllib_parse_parse_qs = compat_parse_qs\n \n try:\n     from urllib.request import DataHandler as compat_urllib_request_DataHandler\n@@ -2508,21 +2632,11 @@ def data_open(self, req):\n \n             return compat_urllib_response.addinfourl(io.BytesIO(data), headers, url)\n \n-try:\n-    compat_basestring = basestring  # Python 2\n-except NameError:\n-    compat_basestring = str\n-\n-try:\n-    compat_chr = unichr  # Python 2\n-except NameError:\n-    compat_chr = chr\n-\n try:\n     from xml.etree.ElementTree import ParseError as compat_xml_parse_error\n except ImportError:  # Python 2.6\n     from xml.parsers.expat import ExpatError as compat_xml_parse_error\n-\n+compat_xml_etree_ElementTree_ParseError = compat_xml_parse_error\n \n etree = xml.etree.ElementTree\n \n@@ -2536,10 +2650,11 @@ def doctype(self, name, pubid, system):\n     # xml.etree.ElementTree.Element is a method in Python <=2.6 and\n     # the following will crash with:\n     #  TypeError: isinstance() arg 2 must be a class, type, or tuple of classes and types\n-    isinstance(None, xml.etree.ElementTree.Element)\n+    isinstance(None, etree.Element)\n     from xml.etree.ElementTree import Element as compat_etree_Element\n except TypeError:  # Python <=2.6\n     from xml.etree.ElementTree import _ElementInterface as compat_etree_Element\n+compat_xml_etree_ElementTree_Element = compat_etree_Element\n \n if sys.version_info[0] >= 3:\n     def compat_etree_fromstring(text):\n@@ -2595,6 +2710,7 @@ def compat_etree_register_namespace(prefix, uri):\n             if k == uri or v == prefix:\n                 del etree._namespace_map[k]\n         etree._namespace_map[uri] = prefix\n+compat_xml_etree_register_namespace = compat_etree_register_namespace\n \n if sys.version_info < (2, 7):\n     # Here comes the crazy part: In 2.6, if the xpath is a unicode,\n@@ -2603,55 +2719,222 @@ def compat_xpath(xpath):\n         if isinstance(xpath, compat_str):\n             xpath = xpath.encode('ascii')\n         return xpath\n-else:\n-    compat_xpath = lambda xpath: xpath\n \n-try:\n-    from urllib.parse import parse_qs as compat_parse_qs\n-except ImportError:  # Python 2\n-    # HACK: The following is the correct parse_qs implementation from cpython 3's stdlib.\n-    # Python 2's version is apparently totally broken\n-\n-    def _parse_qsl(qs, keep_blank_values=False, strict_parsing=False,\n-                   encoding='utf-8', errors='replace'):\n-        qs, _coerce_result = qs, compat_str\n-        pairs = [s2 for s1 in qs.split('&') for s2 in s1.split(';')]\n-        r = []\n-        for name_value in pairs:\n-            if not name_value and not strict_parsing:\n-                continue\n-            nv = name_value.split('=', 1)\n-            if len(nv) != 2:\n-                if strict_parsing:\n-                    raise ValueError('bad query field: %r' % (name_value,))\n-                # Handle case of a control-name with no equal sign\n-                if keep_blank_values:\n-                    nv.append('')\n+    # further code below based on CPython 2.7 source\n+    import functools\n+\n+    _xpath_tokenizer_re = re.compile(r'''(?x)\n+        (                                   # (1)\n+            '[^']*'|\"[^\"]*\"|                # quoted strings, or\n+            ::|//?|\\.\\.|\\(\\)|[/.*:[\\]()@=]  # navigation specials\n+        )|                                  # or (2)\n+        ((?:\\{[^}]+\\})?[^/[\\]()@=\\s]+)|     # token: optional {ns}, no specials\n+        \\s+                                 # or white space\n+    ''')\n+\n+    def _xpath_tokenizer(pattern, namespaces=None):\n+        for token in _xpath_tokenizer_re.findall(pattern):\n+            tag = token[1]\n+            if tag and tag[0] != \"{\" and \":\" in tag:\n+                try:\n+                    if not namespaces:\n+                        raise KeyError\n+                    prefix, uri = tag.split(\":\", 1)\n+                    yield token[0], \"{%s}%s\" % (namespaces[prefix], uri)\n+                except KeyError:\n+                    raise SyntaxError(\"prefix %r not found in prefix map\" % prefix)\n+            else:\n+                yield token\n+\n+    def _get_parent_map(context):\n+        parent_map = context.parent_map\n+        if parent_map is None:\n+            context.parent_map = parent_map = {}\n+            for p in context.root.getiterator():\n+                for e in p:\n+                    parent_map[e] = p\n+        return parent_map\n+\n+    def _select(context, result, filter_fn=lambda *_: True):\n+        for elem in result:\n+            for e in elem:\n+                if filter_fn(e, elem):\n+                    yield e\n+\n+    def _prepare_child(next_, token):\n+        tag = token[1]\n+        return functools.partial(_select, filter_fn=lambda e, _: e.tag == tag)\n+\n+    def _prepare_star(next_, token):\n+        return _select\n+\n+    def _prepare_self(next_, token):\n+        return lambda _, result: (e for e in result)\n+\n+    def _prepare_descendant(next_, token):\n+        token = next(next_)\n+        if token[0] == \"*\":\n+            tag = \"*\"\n+        elif not token[0]:\n+            tag = token[1]\n+        else:\n+            raise SyntaxError(\"invalid descendant\")\n+\n+        def select(context, result):\n+            for elem in result:\n+                for e in elem.getiterator(tag):\n+                    if e is not elem:\n+                        yield e\n+        return select\n+\n+    def _prepare_parent(next_, token):\n+        def select(context, result):\n+            # FIXME: raise error if .. is applied at toplevel?\n+            parent_map = _get_parent_map(context)\n+            result_map = {}\n+            for elem in result:\n+                if elem in parent_map:\n+                    parent = parent_map[elem]\n+                    if parent not in result_map:\n+                        result_map[parent] = None\n+                        yield parent\n+        return select\n+\n+    def _prepare_predicate(next_, token):\n+        signature = []\n+        predicate = []\n+        for token in next_:\n+            if token[0] == \"]\":\n+                break\n+            if token[0] and token[0][:1] in \"'\\\"\":\n+                token = \"'\", token[0][1:-1]\n+            signature.append(token[0] or \"-\")\n+            predicate.append(token[1])\n+\n+        def select(context, result, filter_fn=lambda _: True):\n+            for elem in result:\n+                if filter_fn(elem):\n+                    yield elem\n+\n+        signature = \"\".join(signature)\n+        # use signature to determine predicate type\n+        if signature == \"@-\":\n+            # [@attribute] predicate\n+            key = predicate[1]\n+            return functools.partial(\n+                select, filter_fn=lambda el: el.get(key) is not None)\n+        if signature == \"@-='\":\n+            # [@attribute='value']\n+            key = predicate[1]\n+            value = predicate[-1]\n+            return functools.partial(\n+                select, filter_fn=lambda el: el.get(key) == value)\n+        if signature == \"-\" and not re.match(r\"\\d+$\", predicate[0]):\n+            # [tag]\n+            tag = predicate[0]\n+            return functools.partial(\n+                select, filter_fn=lambda el: el.find(tag) is not None)\n+        if signature == \"-='\" and not re.match(r\"\\d+$\", predicate[0]):\n+            # [tag='value']\n+            tag = predicate[0]\n+            value = predicate[-1]\n+\n+            def itertext(el):\n+                for e in el.getiterator():\n+                    e = e.text\n+                    if e:\n+                        yield e\n+\n+            def select(context, result):\n+                for elem in result:\n+                    for e in elem.findall(tag):\n+                        if \"\".join(itertext(e)) == value:\n+                            yield elem\n+                            break\n+            return select\n+        if signature == \"-\" or signature == \"-()\" or signature == \"-()-\":\n+            # [index] or [last()] or [last()-index]\n+            if signature == \"-\":\n+                index = int(predicate[0]) - 1\n+            else:\n+                if predicate[0] != \"last\":\n+                    raise SyntaxError(\"unsupported function\")\n+                if signature == \"-()-\":\n+                    try:\n+                        index = int(predicate[2]) - 1\n+                    except ValueError:\n+                        raise SyntaxError(\"unsupported expression\")\n                 else:\n+                    index = -1\n+\n+            def select(context, result):\n+                parent_map = _get_parent_map(context)\n+                for elem in result:\n+                    try:\n+                        parent = parent_map[elem]\n+                        # FIXME: what if the selector is \"*\" ?\n+                        elems = list(parent.findall(elem.tag))\n+                        if elems[index] is elem:\n+                            yield elem\n+                    except (IndexError, KeyError):\n+                        pass\n+            return select\n+        raise SyntaxError(\"invalid predicate\")\n+\n+    ops = {\n+        \"\": _prepare_child,\n+        \"*\": _prepare_star,\n+        \".\": _prepare_self,\n+        \"..\": _prepare_parent,\n+        \"//\": _prepare_descendant,\n+        \"[\": _prepare_predicate,\n+    }\n+\n+    _cache = {}\n+\n+    class _SelectorContext:\n+        parent_map = None\n+\n+        def __init__(self, root):\n+            self.root = root\n+\n+    ##\n+    # Generate all matching objects.\n+\n+    def compat_etree_iterfind(elem, path, namespaces=None):\n+        # compile selector pattern\n+        if path[-1:] == \"/\":\n+            path = path + \"*\"  # implicit all (FIXME: keep this?)\n+        try:\n+            selector = _cache[path]\n+        except KeyError:\n+            if len(_cache) > 100:\n+                _cache.clear()\n+            if path[:1] == \"/\":\n+                raise SyntaxError(\"cannot use absolute path on element\")\n+            tokens = _xpath_tokenizer(path, namespaces)\n+            selector = []\n+            for token in tokens:\n+                if token[0] == \"/\":\n                     continue\n-            if len(nv[1]) or keep_blank_values:\n-                name = nv[0].replace('+', ' ')\n-                name = compat_urllib_parse_unquote(\n-                    name, encoding=encoding, errors=errors)\n-                name = _coerce_result(name)\n-                value = nv[1].replace('+', ' ')\n-                value = compat_urllib_parse_unquote(\n-                    value, encoding=encoding, errors=errors)\n-                value = _coerce_result(value)\n-                r.append((name, value))\n-        return r\n+                try:\n+                    selector.append(ops[token[0]](tokens, token))\n+                except StopIteration:\n+                    raise SyntaxError(\"invalid path\")\n+            _cache[path] = selector\n+        # execute selector pattern\n+        result = [elem]\n+        context = _SelectorContext(elem)\n+        for select in selector:\n+            result = select(context, result)\n+        return result\n+\n+    # end of code based on CPython 2.7 source\n \n-    def compat_parse_qs(qs, keep_blank_values=False, strict_parsing=False,\n-                        encoding='utf-8', errors='replace'):\n-        parsed_result = {}\n-        pairs = _parse_qsl(qs, keep_blank_values, strict_parsing,\n-                           encoding=encoding, errors=errors)\n-        for name, value in pairs:\n-            if name in parsed_result:\n-                parsed_result[name].append(value)\n-            else:\n-                parsed_result[name] = [value]\n-        return parsed_result\n+\n+else:\n+    compat_xpath = lambda xpath: xpath\n+    compat_etree_iterfind = lambda element, match: element.iterfind(match)\n \n \n compat_os_name = os._name if os.name == 'java' else os.name\n@@ -2687,7 +2970,7 @@ def compat_shlex_split(s, comments=False, posix=True):\n \n \n def compat_ord(c):\n-    if type(c) is int:\n+    if isinstance(c, int):\n         return c\n     else:\n         return ord(c)\n@@ -2777,6 +3060,8 @@ def compat_expanduser(path):\n     else:\n         compat_expanduser = os.path.expanduser\n \n+compat_os_path_expanduser = compat_expanduser\n+\n \n if compat_os_name == 'nt' and sys.version_info < (3, 8):\n     # os.path.realpath on Windows does not follow symbolic links\n@@ -2788,6 +3073,8 @@ def compat_realpath(path):\n else:\n     compat_realpath = os.path.realpath\n \n+compat_os_path_realpath = compat_realpath\n+\n \n if sys.version_info < (3, 0):\n     def compat_print(s):\n@@ -2808,11 +3095,15 @@ def compat_getpass(prompt, *args, **kwargs):\n else:\n     compat_getpass = getpass.getpass\n \n+compat_getpass_getpass = compat_getpass\n+\n+\n try:\n     compat_input = raw_input\n except NameError:  # Python 3\n     compat_input = input\n \n+\n # Python < 2.6.5 require kwargs to be bytes\n try:\n     def _testfunc(x):\n@@ -2863,6 +3154,51 @@ def compat_socket_create_connection(address, timeout, source_address=None):\n     compat_socket_create_connection = socket.create_connection\n \n \n+try:\n+    from contextlib import suppress as compat_contextlib_suppress\n+except ImportError:\n+    class compat_contextlib_suppress(object):\n+        _exceptions = None\n+\n+        def __init__(self, *exceptions):\n+            super(compat_contextlib_suppress, self).__init__()\n+            # TODO: [Base]ExceptionGroup (3.12+)\n+            self._exceptions = exceptions\n+\n+        def __enter__(self):\n+            return self\n+\n+        def __exit__(self, exc_type, exc_val, exc_tb):\n+            return exc_type is not None and issubclass(exc_type, self._exceptions or tuple())\n+\n+\n+# subprocess.Popen context manager\n+# avoids leaking handles if .communicate() is not called\n+try:\n+    _Popen = subprocess.Popen\n+    # check for required context manager attributes\n+    _Popen.__enter__ and _Popen.__exit__\n+    compat_subprocess_Popen = _Popen\n+except AttributeError:\n+    # not a context manager - make one\n+    from contextlib import contextmanager\n+\n+    @contextmanager\n+    def compat_subprocess_Popen(*args, **kwargs):\n+        popen = None\n+        try:\n+            popen = _Popen(*args, **kwargs)\n+            yield popen\n+        finally:\n+            if popen:\n+                for f in (popen.stdin, popen.stdout, popen.stderr):\n+                    if f:\n+                        # repeated .close() is OK, but just in case\n+                        with compat_contextlib_suppress(EnvironmentError):\n+                            f.close()\n+                popen.wait()\n+\n+\n # Fix https://github.com/ytdl-org/youtube-dl/issues/4223\n # See http://bugs.python.org/issue9161 for what is broken\n def workaround_optparse_bug9161():\n@@ -2890,6 +3226,7 @@ def _compat_add_option(self, *args, **kwargs):\n     _terminal_size = collections.namedtuple('terminal_size', ['columns', 'lines'])\n \n     def compat_get_terminal_size(fallback=(80, 24)):\n+        from .utils import process_communicate_or_kill\n         columns = compat_getenv('COLUMNS')\n         if columns:\n             columns = int(columns)\n@@ -2906,7 +3243,7 @@ def compat_get_terminal_size(fallback=(80, 24)):\n                 sp = subprocess.Popen(\n                     ['stty', 'size'],\n                     stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n-                out, err = sp.communicate()\n+                out, err = process_communicate_or_kill(sp)\n                 _lines, _columns = map(int, out.split())\n             except Exception:\n                 _columns, _lines = _terminal_size(*fallback)\n@@ -2917,15 +3254,16 @@ def compat_get_terminal_size(fallback=(80, 24)):\n                 lines = _lines\n         return _terminal_size(columns, lines)\n \n+\n try:\n     itertools.count(start=0, step=1)\n     compat_itertools_count = itertools.count\n except TypeError:  # Python 2.6\n     def compat_itertools_count(start=0, step=1):\n-        n = start\n         while True:\n-            yield n\n-            n += step\n+            yield start\n+            start += step\n+\n \n if sys.version_info >= (3, 0):\n     from tokenize import tokenize as compat_tokenize_tokenize\n@@ -2984,7 +3322,6 @@ def unpack(self, string):\n     except ImportError:\n         compat_filter = filter\n \n-\n try:\n     from future_builtins import zip as compat_zip\n except ImportError:  # not 2.6+ or is 3.x\n@@ -2994,6 +3331,82 @@ def unpack(self, string):\n         compat_zip = zip\n \n \n+# method renamed between Py2/3\n+try:\n+    from itertools import zip_longest as compat_itertools_zip_longest\n+except ImportError:\n+    from itertools import izip_longest as compat_itertools_zip_longest\n+\n+\n+# new class in collections\n+try:\n+    from collections import ChainMap as compat_collections_chain_map\n+    # Py3.3's ChainMap is deficient\n+    if sys.version_info < (3, 4):\n+        raise ImportError\n+except ImportError:\n+    # Py <= 3.3\n+    class compat_collections_chain_map(compat_collections_abc.MutableMapping):\n+\n+        maps = [{}]\n+\n+        def __init__(self, *maps):\n+            self.maps = list(maps) or [{}]\n+\n+        def __getitem__(self, k):\n+            for m in self.maps:\n+                if k in m:\n+                    return m[k]\n+            raise KeyError(k)\n+\n+        def __setitem__(self, k, v):\n+            self.maps[0].__setitem__(k, v)\n+            return\n+\n+        def __contains__(self, k):\n+            return any((k in m) for m in self.maps)\n+\n+        def __delitem(self, k):\n+            if k in self.maps[0]:\n+                del self.maps[0][k]\n+                return\n+            raise KeyError(k)\n+\n+        def __delitem__(self, k):\n+            self.__delitem(k)\n+\n+        def __iter__(self):\n+            return itertools.chain(*reversed(self.maps))\n+\n+        def __len__(self):\n+            return len(iter(self))\n+\n+        # to match Py3, don't del directly\n+        def pop(self, k, *args):\n+            if self.__contains__(k):\n+                off = self.__getitem__(k)\n+                self.__delitem(k)\n+                return off\n+            elif len(args) > 0:\n+                return args[0]\n+            raise KeyError(k)\n+\n+        def new_child(self, m=None, **kwargs):\n+            m = m or {}\n+            m.update(kwargs)\n+            return compat_collections_chain_map(m, *self.maps)\n+\n+        @property\n+        def parents(self):\n+            return compat_collections_chain_map(*(self.maps[1:]))\n+\n+\n+# Pythons disagree on the type of a pattern (RegexObject, _sre.SRE_Pattern, Pattern, ...?)\n+compat_re_Pattern = type(re.compile(''))\n+# and on the type of a match\n+compat_re_Match = type(re.match('a', 'a'))\n+\n+\n if sys.version_info < (3, 3):\n     def compat_b64decode(s, *args, **kwargs):\n         if isinstance(s, compat_str):\n@@ -3002,6 +3415,8 @@ def compat_b64decode(s, *args, **kwargs):\n else:\n     compat_b64decode = base64.b64decode\n \n+compat_base64_b64decode = compat_b64decode\n+\n \n if platform.python_implementation() == 'PyPy' and sys.pypy_version_info < (5, 4, 0):\n     # PyPy2 prior to version 5.4.0 expects byte strings as Windows function\n@@ -3021,28 +3436,97 @@ def compat_ctypes_WINFUNCTYPE(*args, **kwargs):\n         return ctypes.WINFUNCTYPE(*args, **kwargs)\n \n \n-__all__ = [\n+if sys.version_info < (3, 0):\n+    # open(file, mode='r', buffering=- 1, encoding=None, errors=None, newline=None, closefd=True) not: opener=None\n+    def compat_open(file_, *args, **kwargs):\n+        if len(args) > 6 or 'opener' in kwargs:\n+            raise ValueError('open: unsupported argument \"opener\"')\n+        return io.open(file_, *args, **kwargs)\n+else:\n+    compat_open = open\n+\n+\n+# compat_register_utf8\n+def compat_register_utf8():\n+    if sys.platform == 'win32':\n+        # https://github.com/ytdl-org/youtube-dl/issues/820\n+        from codecs import register, lookup\n+        register(\n+            lambda name: lookup('utf-8') if name == 'cp65001' else None)\n+\n+\n+# compat_datetime_timedelta_total_seconds\n+try:\n+    compat_datetime_timedelta_total_seconds = datetime.timedelta.total_seconds\n+except AttributeError:\n+    # Py 2.6\n+    def compat_datetime_timedelta_total_seconds(td):\n+        return (td.microseconds + (td.seconds + td.days * 24 * 3600) * 10**6) / 10**6\n+\n+# optional decompression packages\n+# PyPi brotli package implements 'br' Content-Encoding\n+try:\n+    import brotli as compat_brotli\n+except ImportError:\n+    compat_brotli = None\n+# PyPi ncompress package implements 'compress' Content-Encoding\n+try:\n+    import ncompress as compat_ncompress\n+except ImportError:\n+    compat_ncompress = None\n+\n+\n+legacy = [\n     'compat_HTMLParseError',\n     'compat_HTMLParser',\n     'compat_HTTPError',\n-    'compat_Struct',\n     'compat_b64decode',\n-    'compat_basestring',\n-    'compat_chr',\n-    'compat_collections_abc',\n     'compat_cookiejar',\n     'compat_cookiejar_Cookie',\n     'compat_cookies',\n     'compat_cookies_SimpleCookie',\n-    'compat_ctypes_WINFUNCTYPE',\n     'compat_etree_Element',\n-    'compat_etree_fromstring',\n     'compat_etree_register_namespace',\n     'compat_expanduser',\n+    'compat_getpass',\n+    'compat_parse_qs',\n+    'compat_realpath',\n+    'compat_urllib_parse_parse_qs',\n+    'compat_urllib_parse_unquote',\n+    'compat_urllib_parse_unquote_plus',\n+    'compat_urllib_parse_unquote_to_bytes',\n+    'compat_urllib_parse_urlencode',\n+    'compat_urllib_parse_urlparse',\n+    'compat_urlparse',\n+    'compat_urlretrieve',\n+    'compat_xml_parse_error',\n+]\n+\n+\n+__all__ = [\n+    'compat_html_parser_HTMLParseError',\n+    'compat_html_parser_HTMLParser',\n+    'compat_Struct',\n+    'compat_base64_b64decode',\n+    'compat_basestring',\n+    'compat_brotli',\n+    'compat_casefold',\n+    'compat_chr',\n+    'compat_collections_abc',\n+    'compat_collections_chain_map',\n+    'compat_datetime_timedelta_total_seconds',\n+    'compat_http_cookiejar',\n+    'compat_http_cookiejar_Cookie',\n+    'compat_http_cookies',\n+    'compat_http_cookies_SimpleCookie',\n+    'compat_contextlib_suppress',\n+    'compat_ctypes_WINFUNCTYPE',\n+    'compat_etree_fromstring',\n+    'compat_etree_iterfind',\n     'compat_filter',\n     'compat_get_terminal_size',\n     'compat_getenv',\n-    'compat_getpass',\n+    'compat_getpass_getpass',\n     'compat_html_entities',\n     'compat_html_entities_html5',\n     'compat_http_client',\n@@ -3050,14 +3534,20 @@ def compat_ctypes_WINFUNCTYPE(*args, **kwargs):\n     'compat_input',\n     'compat_integer_types',\n     'compat_itertools_count',\n+    'compat_itertools_zip_longest',\n     'compat_kwargs',\n     'compat_map',\n+    'compat_ncompress',\n     'compat_numeric_types',\n+    'compat_open',\n     'compat_ord',\n     'compat_os_name',\n-    'compat_parse_qs',\n+    'compat_os_path_expanduser',\n+    'compat_os_path_realpath',\n     'compat_print',\n-    'compat_realpath',\n+    'compat_re_Match',\n+    'compat_re_Pattern',\n+    'compat_register_utf8',\n     'compat_setenv',\n     'compat_shlex_quote',\n     'compat_shlex_split',\n@@ -3066,20 +3556,18 @@ def compat_ctypes_WINFUNCTYPE(*args, **kwargs):\n     'compat_struct_pack',\n     'compat_struct_unpack',\n     'compat_subprocess_get_DEVNULL',\n+    'compat_subprocess_Popen',\n     'compat_tokenize_tokenize',\n     'compat_urllib_error',\n     'compat_urllib_parse',\n-    'compat_urllib_parse_unquote',\n-    'compat_urllib_parse_unquote_plus',\n-    'compat_urllib_parse_unquote_to_bytes',\n-    'compat_urllib_parse_urlencode',\n-    'compat_urllib_parse_urlparse',\n     'compat_urllib_request',\n     'compat_urllib_request_DataHandler',\n     'compat_urllib_response',\n-    'compat_urlparse',\n-    'compat_urlretrieve',\n-    'compat_xml_parse_error',\n+    'compat_urllib_request_urlretrieve',\n+    'compat_urllib_HTTPError',\n+    'compat_xml_etree_ElementTree_Element',\n+    'compat_xml_etree_ElementTree_ParseError',\n+    'compat_xml_etree_register_namespace',\n     'compat_xpath',\n     'compat_zip',\n     'workaround_optparse_bug9161',\ndiff --git a/youtube_dl/downloader/__init__.py b/youtube_dl/downloader/__init__.py\nindex d8f2fa34226..d701d629221 100644\n--- a/youtube_dl/downloader/__init__.py\n+++ b/youtube_dl/downloader/__init__.py\n@@ -50,6 +50,9 @@ def _get_suitable_downloader(info_dict, params={}):\n         ed = get_external_downloader(external_downloader)\n         if ed.can_download(info_dict):\n             return ed\n+        # Avoid using unwanted args since external_downloader was rejected\n+        if params.get('external_downloader_args'):\n+            params['external_downloader_args'] = None\n \n     protocol = info_dict['protocol']\n     if protocol.startswith('m3u8') and info_dict.get('is_live'):\ndiff --git a/youtube_dl/downloader/common.py b/youtube_dl/downloader/common.py\nindex 1cdba89cd9b..91e691776b1 100644\n--- a/youtube_dl/downloader/common.py\n+++ b/youtube_dl/downloader/common.py\n@@ -88,17 +88,21 @@ def format_percent(percent):\n             return '---.-%'\n         return '%6s' % ('%3.1f%%' % percent)\n \n-    @staticmethod\n-    def calc_eta(start, now, total, current):\n+    @classmethod\n+    def calc_eta(cls, start_or_rate, now_or_remaining, *args):\n+        if len(args) < 2:\n+            rate, remaining = (start_or_rate, now_or_remaining)\n+            if None in (rate, remaining):\n+                return None\n+            return int(float(remaining) / rate)\n+        start, now = (start_or_rate, now_or_remaining)\n+        total, current = args[:2]\n         if total is None:\n             return None\n         if now is None:\n             now = time.time()\n-        dif = now - start\n-        if current == 0 or dif < 0.001:  # One millisecond\n-            return None\n-        rate = float(current) / dif\n-        return int((float(total) - float(current)) / rate)\n+        rate = cls.calc_speed(start, now, current)\n+        return rate and int((float(total) - float(current)) / rate)\n \n     @staticmethod\n     def format_eta(eta):\n@@ -123,6 +127,12 @@ def format_speed(speed):\n     def format_retries(retries):\n         return 'inf' if retries == float('inf') else '%.0f' % retries\n \n+    @staticmethod\n+    def filesize_or_none(unencoded_filename):\n+        fn = encodeFilename(unencoded_filename)\n+        if os.path.isfile(fn):\n+            return os.path.getsize(fn)\n+\n     @staticmethod\n     def best_block_size(elapsed_time, bytes):\n         new_min = max(bytes / 2.0, 1.0)\n@@ -329,6 +339,10 @@ def report_unable_to_resume(self):\n     def download(self, filename, info_dict):\n         \"\"\"Download to a filename using the info from info_dict\n         Return True on success and False otherwise\n+\n+        This method filters the `Cookie` header from the info_dict to prevent leaks.\n+        Downloaders have their own way of handling cookies.\n+        See: https://github.com/yt-dlp/yt-dlp/security/advisories/GHSA-v8mc-9377-rwjj\n         \"\"\"\n \n         nooverwrites_and_exists = (\ndiff --git a/youtube_dl/downloader/dash.py b/youtube_dl/downloader/dash.py\nindex c6d674bc6c0..f3c058879fb 100644\n--- a/youtube_dl/downloader/dash.py\n+++ b/youtube_dl/downloader/dash.py\n@@ -1,5 +1,7 @@\n from __future__ import unicode_literals\n \n+import itertools\n+\n from .fragment import FragmentFD\n from ..compat import compat_urllib_error\n from ..utils import (\n@@ -30,26 +32,28 @@ def real_download(self, filename, info_dict):\n         fragment_retries = self.params.get('fragment_retries', 0)\n         skip_unavailable_fragments = self.params.get('skip_unavailable_fragments', True)\n \n-        frag_index = 0\n-        for i, fragment in enumerate(fragments):\n-            frag_index += 1\n+        for frag_index, fragment in enumerate(fragments, 1):\n             if frag_index <= ctx['fragment_index']:\n                 continue\n+            success = False\n             # In DASH, the first segment contains necessary headers to\n             # generate a valid MP4 file, so always abort for the first segment\n-            fatal = i == 0 or not skip_unavailable_fragments\n-            count = 0\n-            while count <= fragment_retries:\n+            fatal = frag_index == 1 or not skip_unavailable_fragments\n+            fragment_url = fragment.get('url')\n+            if not fragment_url:\n+                assert fragment_base_url\n+                fragment_url = urljoin(fragment_base_url, fragment['path'])\n+            headers = info_dict.get('http_headers')\n+            fragment_range = fragment.get('range')\n+            if fragment_range:\n+                headers = headers.copy() if headers else {}\n+                headers['Range'] = 'bytes=%s' % (fragment_range,)\n+            for count in itertools.count():\n                 try:\n-                    fragment_url = fragment.get('url')\n-                    if not fragment_url:\n-                        assert fragment_base_url\n-                        fragment_url = urljoin(fragment_base_url, fragment['path'])\n-                    success, frag_content = self._download_fragment(ctx, fragment_url, info_dict)\n+                    success, frag_content = self._download_fragment(ctx, fragment_url, info_dict, headers)\n                     if not success:\n                         return False\n                     self._append_fragment(ctx, frag_content)\n-                    break\n                 except compat_urllib_error.HTTPError as err:\n                     # YouTube may often return 404 HTTP error for a fragment causing the\n                     # whole download to fail. However if the same fragment is immediately\n@@ -57,22 +61,21 @@ def real_download(self, filename, info_dict):\n                     # is usually enough) thus allowing to download the whole file successfully.\n                     # To be future-proof we will retry all fragments that fail with any\n                     # HTTP error.\n-                    count += 1\n-                    if count <= fragment_retries:\n-                        self.report_retry_fragment(err, frag_index, count, fragment_retries)\n+                    if count < fragment_retries:\n+                        self.report_retry_fragment(err, frag_index, count + 1, fragment_retries)\n+                        continue\n                 except DownloadError:\n                     # Don't retry fragment if error occurred during HTTP downloading\n-                    # itself since it has own retry settings\n-                    if not fatal:\n-                        self.report_skip_fragment(frag_index)\n-                        break\n-                    raise\n+                    # itself since it has its own retry settings\n+                    if fatal:\n+                        raise\n+                break\n \n-            if count > fragment_retries:\n+            if not success:\n                 if not fatal:\n                     self.report_skip_fragment(frag_index)\n                     continue\n-                self.report_error('giving up after %s fragment retries' % fragment_retries)\n+                self.report_error('giving up after %s fragment retries' % count)\n                 return False\n \n         self._finish_frag_download(ctx)\ndiff --git a/youtube_dl/downloader/external.py b/youtube_dl/downloader/external.py\nindex c31f8910ad8..4fbc0f520e0 100644\n--- a/youtube_dl/downloader/external.py\n+++ b/youtube_dl/downloader/external.py\n@@ -1,17 +1,24 @@\n from __future__ import unicode_literals\n \n-import os.path\n+import os\n import re\n import subprocess\n import sys\n+import tempfile\n import time\n \n from .common import FileDownloader\n from ..compat import (\n     compat_setenv,\n     compat_str,\n+    compat_subprocess_Popen,\n )\n-from ..postprocessor.ffmpeg import FFmpegPostProcessor, EXT_TO_OUT_FORMATS\n+\n+try:\n+    from ..postprocessor.ffmpeg import FFmpegPostProcessor, EXT_TO_OUT_FORMATS\n+except ImportError:\n+    FFmpegPostProcessor = None\n+\n from ..utils import (\n     cli_option,\n     cli_valueless_option,\n@@ -22,6 +29,9 @@\n     handle_youtubedl_headers,\n     check_executable,\n     is_outdated_version,\n+    process_communicate_or_kill,\n+    T,\n+    traverse_obj,\n )\n \n \n@@ -29,6 +39,7 @@ class ExternalFD(FileDownloader):\n     def real_download(self, filename, info_dict):\n         self.report_destination(filename)\n         tmpfilename = self.temp_name(filename)\n+        self._cookies_tempfile = None\n \n         try:\n             started = time.time()\n@@ -41,6 +52,13 @@ def real_download(self, filename, info_dict):\n             # should take place\n             retval = 0\n             self.to_screen('[%s] Interrupted by user' % self.get_basename())\n+        finally:\n+            if self._cookies_tempfile and os.path.isfile(self._cookies_tempfile):\n+                try:\n+                    os.remove(self._cookies_tempfile)\n+                except OSError:\n+                    self.report_warning(\n+                        'Unable to delete temporary cookies file \"{0}\"'.format(self._cookies_tempfile))\n \n         if retval == 0:\n             status = {\n@@ -96,6 +114,16 @@ def _valueless_option(self, command_option, param, expected_value=True):\n     def _configuration_args(self, default=[]):\n         return cli_configuration_args(self.params, 'external_downloader_args', default)\n \n+    def _write_cookies(self):\n+        if not self.ydl.cookiejar.filename:\n+            tmp_cookies = tempfile.NamedTemporaryFile(suffix='.cookies', delete=False)\n+            tmp_cookies.close()\n+            self._cookies_tempfile = tmp_cookies.name\n+            self.to_screen('[download] Writing temporary cookies file to \"{0}\"'.format(self._cookies_tempfile))\n+        # real_download resets _cookies_tempfile; if it's None, save() will write to cookiejar.filename\n+        self.ydl.cookiejar.save(self._cookies_tempfile, ignore_discard=True, ignore_expires=True)\n+        return self.ydl.cookiejar.filename or self._cookies_tempfile\n+\n     def _call_downloader(self, tmpfilename, info_dict):\n         \"\"\" Either overwrite this or implement _make_cmd \"\"\"\n         cmd = [encodeArgument(a) for a in self._make_cmd(tmpfilename, info_dict)]\n@@ -104,18 +132,26 @@ def _call_downloader(self, tmpfilename, info_dict):\n \n         p = subprocess.Popen(\n             cmd, stderr=subprocess.PIPE)\n-        _, stderr = p.communicate()\n+        _, stderr = process_communicate_or_kill(p)\n         if p.returncode != 0:\n             self.to_stderr(stderr.decode('utf-8', 'replace'))\n         return p.returncode\n \n+    @staticmethod\n+    def _header_items(info_dict):\n+        return traverse_obj(\n+            info_dict, ('http_headers', T(dict.items), Ellipsis))\n+\n \n class CurlFD(ExternalFD):\n     AVAILABLE_OPT = '-V'\n \n     def _make_cmd(self, tmpfilename, info_dict):\n-        cmd = [self.exe, '--location', '-o', tmpfilename]\n-        for key, val in info_dict['http_headers'].items():\n+        cmd = [self.exe, '--location', '-o', tmpfilename, '--compressed']\n+        cookie_header = self.ydl.cookiejar.get_cookie_header(info_dict['url'])\n+        if cookie_header:\n+            cmd += ['--cookie', cookie_header]\n+        for key, val in self._header_items(info_dict):\n             cmd += ['--header', '%s: %s' % (key, val)]\n         cmd += self._bool_option('--continue-at', 'continuedl', '-', '0')\n         cmd += self._valueless_option('--silent', 'noprogress')\n@@ -141,7 +177,7 @@ def _call_downloader(self, tmpfilename, info_dict):\n \n         # curl writes the progress to stderr so don't capture it.\n         p = subprocess.Popen(cmd)\n-        p.communicate()\n+        process_communicate_or_kill(p)\n         return p.returncode\n \n \n@@ -150,8 +186,11 @@ class AxelFD(ExternalFD):\n \n     def _make_cmd(self, tmpfilename, info_dict):\n         cmd = [self.exe, '-o', tmpfilename]\n-        for key, val in info_dict['http_headers'].items():\n+        for key, val in self._header_items(info_dict):\n             cmd += ['-H', '%s: %s' % (key, val)]\n+        cookie_header = self.ydl.cookiejar.get_cookie_header(info_dict['url'])\n+        if cookie_header:\n+            cmd += ['-H', 'Cookie: {0}'.format(cookie_header), '--max-redirect=0']\n         cmd += self._configuration_args()\n         cmd += ['--', info_dict['url']]\n         return cmd\n@@ -161,8 +200,10 @@ class WgetFD(ExternalFD):\n     AVAILABLE_OPT = '--version'\n \n     def _make_cmd(self, tmpfilename, info_dict):\n-        cmd = [self.exe, '-O', tmpfilename, '-nv', '--no-cookies']\n-        for key, val in info_dict['http_headers'].items():\n+        cmd = [self.exe, '-O', tmpfilename, '-nv', '--compression=auto']\n+        if self.ydl.cookiejar.get_cookie_header(info_dict['url']):\n+            cmd += ['--load-cookies', self._write_cookies()]\n+        for key, val in self._header_items(info_dict):\n             cmd += ['--header', '%s: %s' % (key, val)]\n         cmd += self._option('--limit-rate', 'ratelimit')\n         retry = self._option('--tries', 'retries')\n@@ -171,7 +212,10 @@ def _make_cmd(self, tmpfilename, info_dict):\n                 retry[1] = '0'\n             cmd += retry\n         cmd += self._option('--bind-address', 'source_address')\n-        cmd += self._option('--proxy', 'proxy')\n+        proxy = self.params.get('proxy')\n+        if proxy:\n+            for var in ('http_proxy', 'https_proxy'):\n+                cmd += ['--execute', '%s=%s' % (var, proxy)]\n         cmd += self._valueless_option('--no-check-certificate', 'nocheckcertificate')\n         cmd += self._configuration_args()\n         cmd += ['--', info_dict['url']]\n@@ -181,24 +225,121 @@ def _make_cmd(self, tmpfilename, info_dict):\n class Aria2cFD(ExternalFD):\n     AVAILABLE_OPT = '-v'\n \n+    @staticmethod\n+    def _aria2c_filename(fn):\n+        return fn if os.path.isabs(fn) else os.path.join('.', fn)\n+\n     def _make_cmd(self, tmpfilename, info_dict):\n-        cmd = [self.exe, '-c']\n-        cmd += self._configuration_args([\n-            '--min-split-size', '1M', '--max-connection-per-server', '4'])\n-        dn = os.path.dirname(tmpfilename)\n-        if dn:\n-            cmd += ['--dir', dn]\n-        cmd += ['--out', os.path.basename(tmpfilename)]\n-        for key, val in info_dict['http_headers'].items():\n+        cmd = [self.exe, '-c',\n+               '--console-log-level=warn', '--summary-interval=0', '--download-result=hide',\n+               '--http-accept-gzip=true', '--file-allocation=none', '-x16', '-j16', '-s16']\n+        if 'fragments' in info_dict:\n+            cmd += ['--allow-overwrite=true', '--allow-piece-length-change=true']\n+        else:\n+            cmd += ['--min-split-size', '1M']\n+\n+        if self.ydl.cookiejar.get_cookie_header(info_dict['url']):\n+            cmd += ['--load-cookies={0}'.format(self._write_cookies())]\n+        for key, val in self._header_items(info_dict):\n             cmd += ['--header', '%s: %s' % (key, val)]\n+        cmd += self._configuration_args(['--max-connection-per-server', '4'])\n+        cmd += ['--out', os.path.basename(tmpfilename)]\n+        cmd += self._option('--max-overall-download-limit', 'ratelimit')\n         cmd += self._option('--interface', 'source_address')\n         cmd += self._option('--all-proxy', 'proxy')\n         cmd += self._bool_option('--check-certificate', 'nocheckcertificate', 'false', 'true', '=')\n         cmd += self._bool_option('--remote-time', 'updatetime', 'true', 'false', '=')\n-        cmd += ['--', info_dict['url']]\n+        cmd += self._bool_option('--show-console-readout', 'noprogress', 'false', 'true', '=')\n+        cmd += self._configuration_args()\n+\n+        # aria2c strips out spaces from the beginning/end of filenames and paths.\n+        # We work around this issue by adding a \"./\" to the beginning of the\n+        # filename and relative path, and adding a \"/\" at the end of the path.\n+        # See: https://github.com/yt-dlp/yt-dlp/issues/276\n+        # https://github.com/ytdl-org/youtube-dl/issues/20312\n+        # https://github.com/aria2/aria2/issues/1373\n+        dn = os.path.dirname(tmpfilename)\n+        if dn:\n+            cmd += ['--dir', self._aria2c_filename(dn) + os.path.sep]\n+        if 'fragments' not in info_dict:\n+            cmd += ['--out', self._aria2c_filename(os.path.basename(tmpfilename))]\n+        cmd += ['--auto-file-renaming=false']\n+        if 'fragments' in info_dict:\n+            cmd += ['--file-allocation=none', '--uri-selector=inorder']\n+            url_list_file = '%s.frag.urls' % (tmpfilename, )\n+            url_list = []\n+            for frag_index, fragment in enumerate(info_dict['fragments']):\n+                fragment_filename = '%s-Frag%d' % (os.path.basename(tmpfilename), frag_index)\n+                url_list.append('%s\\n\\tout=%s' % (fragment['url'], self._aria2c_filename(fragment_filename)))\n+            stream, _ = self.sanitize_open(url_list_file, 'wb')\n+            stream.write('\\n'.join(url_list).encode())\n+            stream.close()\n+            cmd += ['-i', self._aria2c_filename(url_list_file)]\n+        else:\n+            cmd += ['--', info_dict['url']]\n         return cmd\n \n \n+class Aria2pFD(ExternalFD):\n+    ''' Aria2pFD class\n+    This class support to use aria2p as downloader.\n+    (Aria2p, a command-line tool and Python library to interact with an aria2c daemon process\n+    through JSON-RPC.)\n+    It can help you to get download progress more easily.\n+    To use aria2p as downloader, you need to install aria2c and aria2p, aria2p can download with pip.\n+    Then run aria2c in the background and enable with the --enable-rpc option.\n+    '''\n+    try:\n+        import aria2p\n+        __avail = True\n+    except ImportError:\n+        __avail = False\n+\n+    @classmethod\n+    def available(cls):\n+        return cls.__avail\n+\n+    def _call_downloader(self, tmpfilename, info_dict):\n+        aria2 = self.aria2p.API(\n+            self.aria2p.Client(\n+                host='http://localhost',\n+                port=6800,\n+                secret=''\n+            )\n+        )\n+\n+        options = {\n+            'min-split-size': '1M',\n+            'max-connection-per-server': 4,\n+            'auto-file-renaming': 'false',\n+        }\n+        options['dir'] = os.path.dirname(tmpfilename) or os.path.abspath('.')\n+        options['out'] = os.path.basename(tmpfilename)\n+        if self.ydl.cookiejar.get_cookie_header(info_dict['url']):\n+            options['load-cookies'] = self._write_cookies()\n+        options['header'] = []\n+        for key, val in self._header_items(info_dict):\n+            options['header'].append('{0}: {1}'.format(key, val))\n+        download = aria2.add_uris([info_dict['url']], options)\n+        status = {\n+            'status': 'downloading',\n+            'tmpfilename': tmpfilename,\n+        }\n+        started = time.time()\n+        while download.status in ['active', 'waiting']:\n+            download = aria2.get_download(download.gid)\n+            status.update({\n+                'downloaded_bytes': download.completed_length,\n+                'total_bytes': download.total_length,\n+                'elapsed': time.time() - started,\n+                'eta': download.eta.total_seconds(),\n+                'speed': download.download_speed,\n+            })\n+            self._hook_progress(status)\n+            time.sleep(.5)\n+        return download.status != 'complete'\n+\n+\n class HttpieFD(ExternalFD):\n     @classmethod\n     def available(cls):\n@@ -206,25 +347,34 @@ def available(cls):\n \n     def _make_cmd(self, tmpfilename, info_dict):\n         cmd = ['http', '--download', '--output', tmpfilename, info_dict['url']]\n-        for key, val in info_dict['http_headers'].items():\n+        for key, val in self._header_items(info_dict):\n             cmd += ['%s:%s' % (key, val)]\n+\n+        # httpie 3.1.0+ removes the Cookie header on redirect, so this should be safe for now. [1]\n+        # If we ever need cookie handling for redirects, we can export the cookiejar into a session. [2]\n+        # 1: https://github.com/httpie/httpie/security/advisories/GHSA-9w4w-cpc8-h2fq\n+        # 2: https://httpie.io/docs/cli/sessions\n+        cookie_header = self.ydl.cookiejar.get_cookie_header(info_dict['url'])\n+        if cookie_header:\n+            cmd += ['Cookie:%s' % cookie_header]\n         return cmd\n \n \n class FFmpegFD(ExternalFD):\n     @classmethod\n     def supports(cls, info_dict):\n-        return info_dict['protocol'] in ('http', 'https', 'ftp', 'ftps', 'm3u8', 'rtsp', 'rtmp', 'mms')\n+        return info_dict['protocol'] in ('http', 'https', 'ftp', 'ftps', 'm3u8', 'rtsp', 'rtmp', 'mms', 'http_dash_segments')\n \n     @classmethod\n     def available(cls):\n-        return FFmpegPostProcessor().available\n+        # actual availability can only be confirmed for an instance\n+        return bool(FFmpegPostProcessor)\n \n     def _call_downloader(self, tmpfilename, info_dict):\n-        url = info_dict['url']\n-        ffpp = FFmpegPostProcessor(downloader=self)\n+        # `downloader` means the parent `YoutubeDL`\n+        ffpp = FFmpegPostProcessor(downloader=self.ydl)\n         if not ffpp.available:\n-            self.report_error('m3u8 download detected but ffmpeg or avconv could not be found. Please install one.')\n+            self.report_error('ffmpeg required for download but no ffmpeg (nor avconv) executable could be found. Please install one.')\n             return False\n         ffpp.check_version()\n \n@@ -253,7 +403,15 @@ def _call_downloader(self, tmpfilename, info_dict):\n         # if end_time:\n         #     args += ['-t', compat_str(end_time - start_time)]\n \n-        if info_dict['http_headers'] and re.match(r'^https?://', url):\n+        url = info_dict['url']\n+        cookies = self.ydl.cookiejar.get_cookies_for_url(url)\n+        if cookies:\n+            args.extend(['-cookies', ''.join(\n+                '{0}={1}; path={2}; domain={3};\\r\\n'.format(\n+                    cookie.name, cookie.value, cookie.path, cookie.domain)\n+                for cookie in cookies)])\n+\n+        if info_dict.get('http_headers') and re.match(r'^https?://', url):\n             # Trailing \\r\\n after each HTTP header is important to prevent warning from ffmpeg/avconv:\n             # [http @ 00000000003d2fa0] No trailing CRLF found in HTTP header.\n             headers = handle_youtubedl_headers(info_dict['http_headers'])\n@@ -333,18 +491,25 @@ def _call_downloader(self, tmpfilename, info_dict):\n \n         self._debug_cmd(args)\n \n-        proc = subprocess.Popen(args, stdin=subprocess.PIPE, env=env)\n-        try:\n-            retval = proc.wait()\n-        except KeyboardInterrupt:\n-            # subprocces.run would send the SIGKILL signal to ffmpeg and the\n-            # mp4 file couldn't be played, but if we ask ffmpeg to quit it\n-            # produces a file that is playable (this is mostly useful for live\n-            # streams). Note that Windows is not affected and produces playable\n-            # files (see https://github.com/ytdl-org/youtube-dl/issues/8300).\n-            if sys.platform != 'win32':\n-                proc.communicate(b'q')\n-            raise\n+        # From [1], a PIPE opened in Popen() should be closed, unless\n+        # .communicate() is called. Avoid leaking any PIPEs by using Popen\n+        # as a context manager (newer Python 3.x and compat)\n+        # Fixes \"Resource Warning\" in test/test_downloader_external.py\n+        # [1] https://devpress.csdn.net/python/62fde12d7e66823466192e48.html\n+        with compat_subprocess_Popen(args, stdin=subprocess.PIPE, env=env) as proc:\n+            try:\n+                retval = proc.wait()\n+            except BaseException as e:\n+                # subprocess.run would send the SIGKILL signal to ffmpeg and the\n+                # mp4 file couldn't be played, but if we ask ffmpeg to quit it\n+                # produces a file that is playable (this is mostly useful for live\n+                # streams). Note that Windows is not affected and produces playable\n+                # files (see https://github.com/ytdl-org/youtube-dl/issues/8300).\n+                if isinstance(e, KeyboardInterrupt) and sys.platform != 'win32':\n+                    process_communicate_or_kill(proc, b'q')\n+                else:\n+                    proc.kill()\n+                raise\n         return retval\n \n \ndiff --git a/youtube_dl/downloader/fragment.py b/youtube_dl/downloader/fragment.py\nindex 35c76feba00..913e91b64d3 100644\n--- a/youtube_dl/downloader/fragment.py\n+++ b/youtube_dl/downloader/fragment.py\n@@ -71,7 +71,7 @@ def _prepare_and_start_frag_download(self, ctx):\n \n     @staticmethod\n     def __do_ytdl_file(ctx):\n-        return not ctx['live'] and not ctx['tmpfilename'] == '-'\n+        return ctx['live'] is not True and ctx['tmpfilename'] != '-'\n \n     def _read_ytdl_file(self, ctx):\n         assert 'ytdl_corrupt' not in ctx\n@@ -101,6 +101,13 @@ def _download_fragment(self, ctx, frag_url, info_dict, headers=None):\n             'url': frag_url,\n             'http_headers': headers or info_dict.get('http_headers'),\n         }\n+        frag_resume_len = 0\n+        if ctx['dl'].params.get('continuedl', True):\n+            frag_resume_len = self.filesize_or_none(\n+                self.temp_name(fragment_filename))\n+        fragment_info_dict['frag_resume_len'] = frag_resume_len\n+        ctx['frag_resume_len'] = frag_resume_len or 0\n+\n         success = ctx['dl'].download(fragment_filename, fragment_info_dict)\n         if not success:\n             return False, None\n@@ -124,9 +131,7 @@ def _append_fragment(self, ctx, frag_content):\n             del ctx['fragment_filename_sanitized']\n \n     def _prepare_frag_download(self, ctx):\n-        if 'live' not in ctx:\n-            ctx['live'] = False\n-        if not ctx['live']:\n+        if not ctx.setdefault('live', False):\n             total_frags_str = '%d' % ctx['total_frags']\n             ad_frags = ctx.get('ad_frags', 0)\n             if ad_frags:\n@@ -136,10 +141,11 @@ def _prepare_frag_download(self, ctx):\n         self.to_screen(\n             '[%s] Total fragments: %s' % (self.FD_NAME, total_frags_str))\n         self.report_destination(ctx['filename'])\n+        continuedl = self.params.get('continuedl', True)\n         dl = HttpQuietDownloader(\n             self.ydl,\n             {\n-                'continuedl': True,\n+                'continuedl': continuedl,\n                 'quiet': True,\n                 'noprogress': True,\n                 'ratelimit': self.params.get('ratelimit'),\n@@ -150,12 +156,11 @@ def _prepare_frag_download(self, ctx):\n         )\n         tmpfilename = self.temp_name(ctx['filename'])\n         open_mode = 'wb'\n-        resume_len = 0\n \n         # Establish possible resume length\n-        if os.path.isfile(encodeFilename(tmpfilename)):\n+        resume_len = self.filesize_or_none(tmpfilename) or 0\n+        if resume_len > 0:\n             open_mode = 'ab'\n-            resume_len = os.path.getsize(encodeFilename(tmpfilename))\n \n         # Should be initialized before ytdl file check\n         ctx.update({\n@@ -164,7 +169,8 @@ def _prepare_frag_download(self, ctx):\n         })\n \n         if self.__do_ytdl_file(ctx):\n-            if os.path.isfile(encodeFilename(self.ytdl_filename(ctx['filename']))):\n+            ytdl_file_exists = os.path.isfile(encodeFilename(self.ytdl_filename(ctx['filename'])))\n+            if continuedl and ytdl_file_exists:\n                 self._read_ytdl_file(ctx)\n                 is_corrupt = ctx.get('ytdl_corrupt') is True\n                 is_inconsistent = ctx['fragment_index'] > 0 and resume_len == 0\n@@ -178,7 +184,12 @@ def _prepare_frag_download(self, ctx):\n                     if 'ytdl_corrupt' in ctx:\n                         del ctx['ytdl_corrupt']\n                     self._write_ytdl_file(ctx)\n+\n             else:\n+                if not continuedl:\n+                    if ytdl_file_exists:\n+                        self._read_ytdl_file(ctx)\n+                    ctx['fragment_index'] = resume_len = 0\n                 self._write_ytdl_file(ctx)\n                 assert ctx['fragment_index'] == 0\n \n@@ -209,6 +220,7 @@ def _start_frag_download(self, ctx):\n         start = time.time()\n         ctx.update({\n             'started': start,\n+            'fragment_started': start,\n             # Amount of fragment's bytes downloaded by the time of the previous\n             # frag progress hook invocation\n             'prev_frag_downloaded_bytes': 0,\n@@ -218,6 +230,9 @@ def frag_progress_hook(s):\n             if s['status'] not in ('downloading', 'finished'):\n                 return\n \n+            if not total_frags and ctx.get('fragment_count'):\n+                state['fragment_count'] = ctx['fragment_count']\n+\n             time_now = time.time()\n             state['elapsed'] = time_now - start\n             frag_total_bytes = s.get('total_bytes') or 0\n@@ -232,16 +247,17 @@ def frag_progress_hook(s):\n                 ctx['fragment_index'] = state['fragment_index']\n                 state['downloaded_bytes'] += frag_total_bytes - ctx['prev_frag_downloaded_bytes']\n                 ctx['complete_frags_downloaded_bytes'] = state['downloaded_bytes']\n+                ctx['speed'] = state['speed'] = self.calc_speed(\n+                    ctx['fragment_started'], time_now, frag_total_bytes)\n+                ctx['fragment_started'] = time.time()\n                 ctx['prev_frag_downloaded_bytes'] = 0\n             else:\n                 frag_downloaded_bytes = s['downloaded_bytes']\n                 state['downloaded_bytes'] += frag_downloaded_bytes - ctx['prev_frag_downloaded_bytes']\n+                ctx['speed'] = state['speed'] = self.calc_speed(\n+                    ctx['fragment_started'], time_now, frag_downloaded_bytes - ctx['frag_resume_len'])\n                 if not ctx['live']:\n-                    state['eta'] = self.calc_eta(\n-                        start, time_now, estimated_size - resume_len,\n-                        state['downloaded_bytes'] - resume_len)\n-                state['speed'] = s.get('speed') or ctx.get('speed')\n-                ctx['speed'] = state['speed']\n+                    state['eta'] = self.calc_eta(state['speed'], estimated_size - state['downloaded_bytes'])\n                 ctx['prev_frag_downloaded_bytes'] = frag_downloaded_bytes\n             self._hook_progress(state)\n \n@@ -268,7 +284,7 @@ def _finish_frag_download(self, ctx):\n                         os.utime(ctx['filename'], (time.time(), filetime))\n                     except Exception:\n                         pass\n-            downloaded_bytes = os.path.getsize(encodeFilename(ctx['filename']))\n+            downloaded_bytes = self.filesize_or_none(ctx['filename']) or 0\n \n         self._hook_progress({\n             'downloaded_bytes': downloaded_bytes,\ndiff --git a/youtube_dl/downloader/http.py b/youtube_dl/downloader/http.py\nindex d8ac41dcc13..3cad8742091 100644\n--- a/youtube_dl/downloader/http.py\n+++ b/youtube_dl/downloader/http.py\n@@ -58,9 +58,9 @@ class DownloadContext(dict):\n \n         if self.params.get('continuedl', True):\n             # Establish possible resume length\n-            if os.path.isfile(encodeFilename(ctx.tmpfilename)):\n-                ctx.resume_len = os.path.getsize(\n-                    encodeFilename(ctx.tmpfilename))\n+            ctx.resume_len = info_dict.get('frag_resume_len')\n+            if ctx.resume_len is None:\n+                ctx.resume_len = self.filesize_or_none(ctx.tmpfilename) or 0\n \n         ctx.is_resume = ctx.resume_len > 0\n \n@@ -115,9 +115,9 @@ def establish_connection():\n                         raise RetryDownload(err)\n                     raise err\n                 # When trying to resume, Content-Range HTTP header of response has to be checked\n-                # to match the value of requested Range HTTP header. This is due to a webservers\n+                # to match the value of requested Range HTTP header. This is due to webservers\n                 # that don't support resuming and serve a whole file with no Content-Range\n-                # set in response despite of requested Range (see\n+                # set in response despite requested Range (see\n                 # https://github.com/ytdl-org/youtube-dl/issues/6057#issuecomment-126129799)\n                 if has_range:\n                     content_range = ctx.data.headers.get('Content-Range')\n@@ -141,7 +141,8 @@ def establish_connection():\n                     # Content-Range is either not present or invalid. Assuming remote webserver is\n                     # trying to send the whole file, resume is not possible, so wiping the local file\n                     # and performing entire redownload\n-                    self.report_unable_to_resume()\n+                    if range_start > 0:\n+                        self.report_unable_to_resume()\n                     ctx.resume_len = 0\n                     ctx.open_mode = 'wb'\n                 ctx.data_len = int_or_none(ctx.data.info().get('Content-length', None))\n@@ -293,10 +294,7 @@ def retry(e):\n \n                 # Progress message\n                 speed = self.calc_speed(start, now, byte_counter - ctx.resume_len)\n-                if ctx.data_len is None:\n-                    eta = None\n-                else:\n-                    eta = self.calc_eta(start, time.time(), ctx.data_len - ctx.resume_len, byte_counter - ctx.resume_len)\n+                eta = self.calc_eta(speed, ctx.data_len and (ctx.data_len - byte_counter))\n \n                 self._hook_progress({\n                     'status': 'downloading',\ndiff --git a/youtube_dl/downloader/rtmp.py b/youtube_dl/downloader/rtmp.py\nindex fbb7f51b018..8a25dbc8d5a 100644\n--- a/youtube_dl/downloader/rtmp.py\n+++ b/youtube_dl/downloader/rtmp.py\n@@ -89,11 +89,13 @@ def run_rtmpdump(args):\n                                 self.to_screen('')\n                             cursor_in_new_line = True\n                             self.to_screen('[rtmpdump] ' + line)\n-            finally:\n+                if not cursor_in_new_line:\n+                    self.to_screen('')\n+                return proc.wait()\n+            except BaseException:  # Including KeyboardInterrupt\n+                proc.kill()\n                 proc.wait()\n-            if not cursor_in_new_line:\n-                self.to_screen('')\n-            return proc.returncode\n+                raise\n \n         url = info_dict['url']\n         player_url = info_dict.get('player_url')\ndiff --git a/youtube_dl/extractor/adn.py b/youtube_dl/extractor/adn.py\nindex a55ebbcbd68..5ff419f19ef 100644\n--- a/youtube_dl/extractor/adn.py\n+++ b/youtube_dl/extractor/adn.py\n@@ -31,30 +31,34 @@\n \n \n class ADNIE(InfoExtractor):\n-    IE_DESC = 'Anime Digital Network'\n-    _VALID_URL = r'https?://(?:www\\.)?animedigitalnetwork\\.fr/video/[^/]+/(?P<id>\\d+)'\n-    _TEST = {\n-        'url': 'http://animedigitalnetwork.fr/video/blue-exorcist-kyoto-saga/7778-episode-1-debut-des-hostilites',\n-        'md5': '0319c99885ff5547565cacb4f3f9348d',\n+    IE_DESC = 'Animation Digital Network'\n+    _VALID_URL = r'https?://(?:www\\.)?(?:animation|anime)digitalnetwork\\.fr/video/[^/]+/(?P<id>\\d+)'\n+    _TESTS = [{\n+        'url': 'https://animationdigitalnetwork.fr/video/fruits-basket/9841-episode-1-a-ce-soir',\n+        'md5': '1c9ef066ceb302c86f80c2b371615261',\n         'info_dict': {\n-            'id': '7778',\n+            'id': '9841',\n             'ext': 'mp4',\n-            'title': 'Blue Exorcist - Ky\u00f4to Saga - Episode 1',\n-            'description': 'md5:2f7b5aa76edbc1a7a92cedcda8a528d5',\n-            'series': 'Blue Exorcist - Ky\u00f4to Saga',\n-            'duration': 1467,\n-            'release_date': '20170106',\n+            'title': 'Fruits Basket - Episode 1',\n+            'description': 'md5:14be2f72c3c96809b0ca424b0097d336',\n+            'series': 'Fruits Basket',\n+            'duration': 1437,\n+            'release_date': '20190405',\n             'comment_count': int,\n             'average_rating': float,\n-            'season_number': 2,\n-            'episode': 'D\u00e9but des hostilit\u00e9s',\n+            'season_number': 1,\n+            'episode': '\u00c0 ce soir !',\n             'episode_number': 1,\n-        }\n-    }\n+        },\n+        'skip': 'Only available in region (FR, ...)',\n+    }, {\n+        'url': 'http://animedigitalnetwork.fr/video/blue-exorcist-kyoto-saga/7778-episode-1-debut-des-hostilites',\n+        'only_matching': True,\n+    }]\n \n-    _NETRC_MACHINE = 'animedigitalnetwork'\n-    _BASE_URL = 'http://animedigitalnetwork.fr'\n-    _API_BASE_URL = 'https://gw.api.animedigitalnetwork.fr/'\n+    _NETRC_MACHINE = 'animationdigitalnetwork'\n+    _BASE = 'animationdigitalnetwork.fr'\n+    _API_BASE_URL = 'https://gw.api.' + _BASE + '/'\n     _PLAYER_BASE_URL = _API_BASE_URL + 'player/'\n     _HEADERS = {}\n     _LOGIN_ERR_MESSAGE = 'Unable to log in'\n@@ -82,14 +86,14 @@ def _get_subtitles(self, sub_url, video_id):\n         if subtitle_location:\n             enc_subtitles = self._download_webpage(\n                 subtitle_location, video_id, 'Downloading subtitles data',\n-                fatal=False, headers={'Origin': 'https://animedigitalnetwork.fr'})\n+                fatal=False, headers={'Origin': 'https://' + self._BASE})\n         if not enc_subtitles:\n             return None\n \n-        # http://animedigitalnetwork.fr/components/com_vodvideo/videojs/adn-vjs.min.js\n+        # http://animationdigitalnetwork.fr/components/com_vodvideo/videojs/adn-vjs.min.js\n         dec_subtitles = intlist_to_bytes(aes_cbc_decrypt(\n             bytes_to_intlist(compat_b64decode(enc_subtitles[24:])),\n-            bytes_to_intlist(binascii.unhexlify(self._K + 'ab9f52f5baae7c72')),\n+            bytes_to_intlist(binascii.unhexlify(self._K + '7fac1178830cfe0c')),\n             bytes_to_intlist(compat_b64decode(enc_subtitles[:24]))\n         ))\n         subtitles_json = self._parse_json(\n@@ -138,9 +142,9 @@ def _real_initialize(self):\n         if not username:\n             return\n         try:\n+            url = self._API_BASE_URL + 'authentication/login'\n             access_token = (self._download_json(\n-                self._API_BASE_URL + 'authentication/login', None,\n-                'Logging in', self._LOGIN_ERR_MESSAGE, fatal=False,\n+                url, None, 'Logging in', self._LOGIN_ERR_MESSAGE, fatal=False,\n                 data=urlencode_postdata({\n                     'password': password,\n                     'rememberMe': False,\n@@ -153,7 +157,8 @@ def _real_initialize(self):\n             message = None\n             if isinstance(e.cause, compat_HTTPError) and e.cause.code == 401:\n                 resp = self._parse_json(\n-                    e.cause.read().decode(), None, fatal=False) or {}\n+                    self._webpage_read_content(e.cause, url, username),\n+                    username, fatal=False) or {}\n                 message = resp.get('message') or resp.get('code')\n             self.report_warning(message or self._LOGIN_ERR_MESSAGE)\n \n@@ -211,7 +216,9 @@ def _real_extract(self, url):\n                     # This usually goes away with a different random pkcs1pad, so retry\n                     continue\n \n-                error = self._parse_json(e.cause.read(), video_id)\n+                error = self._parse_json(\n+                    self._webpage_read_content(e.cause, links_url, video_id),\n+                    video_id, fatal=False) or {}\n                 message = error.get('message')\n                 if e.cause.code == 403 and error.get('code') == 'player-bad-geolocation-country':\n                     self.raise_geo_restricted(msg=message)\ndiff --git a/youtube_dl/extractor/aenetworks.py b/youtube_dl/extractor/aenetworks.py\nindex e55c03fd701..59fbe048a76 100644\n--- a/youtube_dl/extractor/aenetworks.py\n+++ b/youtube_dl/extractor/aenetworks.py\n@@ -8,6 +8,8 @@\n     ExtractorError,\n     GeoRestrictedError,\n     int_or_none,\n+    remove_start,\n+    traverse_obj,\n     update_url_query,\n     urlencode_postdata,\n )\n@@ -20,8 +22,8 @@ class AENetworksBaseIE(ThePlatformIE):\n             (?:history(?:vault)?|aetv|mylifetime|lifetimemovieclub)\\.com|\n             fyi\\.tv\n         )/'''\n-    _THEPLATFORM_KEY = 'crazyjava'\n-    _THEPLATFORM_SECRET = 's3cr3t'\n+    _THEPLATFORM_KEY = '43jXaGRQud'\n+    _THEPLATFORM_SECRET = 'S10BPXHMlb'\n     _DOMAIN_MAP = {\n         'history.com': ('HISTORY', 'history'),\n         'aetv.com': ('AETV', 'aetv'),\n@@ -33,14 +35,17 @@ class AENetworksBaseIE(ThePlatformIE):\n     }\n \n     def _extract_aen_smil(self, smil_url, video_id, auth=None):\n-        query = {'mbr': 'true'}\n+        query = {\n+            'mbr': 'true',\n+            'formats': 'M3U+none,MPEG-DASH+none,MPEG4,MP3',\n+        }\n         if auth:\n             query['auth'] = auth\n         TP_SMIL_QUERY = [{\n             'assetTypes': 'high_video_ak',\n-            'switch': 'hls_high_ak'\n+            'switch': 'hls_high_ak',\n         }, {\n-            'assetTypes': 'high_video_s3'\n+            'assetTypes': 'high_video_s3',\n         }, {\n             'assetTypes': 'high_video_s3',\n             'switch': 'hls_high_fastly',\n@@ -75,7 +80,14 @@ def _extract_aetn_info(self, domain, filter_key, filter_value, url):\n         requestor_id, brand = self._DOMAIN_MAP[domain]\n         result = self._download_json(\n             'https://feeds.video.aetnd.com/api/v2/%s/videos' % brand,\n-            filter_value, query={'filter[%s]' % filter_key: filter_value})['results'][0]\n+            filter_value, query={'filter[%s]' % filter_key: filter_value})\n+        result = traverse_obj(\n+            result, ('results',\n+                     lambda k, v: k == 0 and v[filter_key] == filter_value),\n+            get_all=False)\n+        if not result:\n+            raise ExtractorError('Show not found in A&E feed (too new?)', expected=True,\n+                                 video_id=remove_start(filter_value, '/'))\n         title = result['title']\n         video_id = result['id']\n         media_url = result['publicUrl']\n@@ -126,7 +138,7 @@ class AENetworksIE(AENetworksBaseIE):\n             'skip_download': True,\n         },\n         'add_ie': ['ThePlatform'],\n-        'skip': 'This video is only available for users of participating TV providers.',\n+        'skip': 'Geo-restricted - This content is not available in your location.'\n     }, {\n         'url': 'http://www.aetv.com/shows/duck-dynasty/season-9/episode-1',\n         'info_dict': {\n@@ -143,6 +155,7 @@ class AENetworksIE(AENetworksBaseIE):\n             'skip_download': True,\n         },\n         'add_ie': ['ThePlatform'],\n+        'skip': 'This video is only available for users of participating TV providers.',\n     }, {\n         'url': 'http://www.fyi.tv/shows/tiny-house-nation/season-1/episode-8',\n         'only_matching': True\ndiff --git a/youtube_dl/extractor/aliexpress.py b/youtube_dl/extractor/aliexpress.py\nindex 6f241e68376..9722fe9ac3b 100644\n--- a/youtube_dl/extractor/aliexpress.py\n+++ b/youtube_dl/extractor/aliexpress.py\n@@ -18,7 +18,7 @@ class AliExpressLiveIE(InfoExtractor):\n             'id': '2800002704436634',\n             'ext': 'mp4',\n             'title': 'CASIMA7.22',\n-            'thumbnail': r're:http://.*\\.jpg',\n+            'thumbnail': r're:https?://.*\\.jpg',\n             'uploader': 'CASIMA Official Store',\n             'timestamp': 1500717600,\n             'upload_date': '20170722',\ndiff --git a/youtube_dl/extractor/alsace20tv.py b/youtube_dl/extractor/alsace20tv.py\nnew file mode 100644\nindex 00000000000..228cec3ec46\n--- /dev/null\n+++ b/youtube_dl/extractor/alsace20tv.py\n@@ -0,0 +1,89 @@\n+# coding: utf-8\n+from __future__ import unicode_literals\n+\n+from .common import InfoExtractor\n+from ..utils import (\n+    clean_html,\n+    dict_get,\n+    get_element_by_class,\n+    int_or_none,\n+    unified_strdate,\n+    url_or_none,\n+)\n+\n+\n+class Alsace20TVIE(InfoExtractor):\n+    _VALID_URL = r'https?://(?:www\\.)?alsace20\\.tv/(?:[\\w-]+/)+[\\w-]+-(?P<id>[\\w]+)'\n+    _TESTS = [{\n+        'url': 'https://www.alsace20.tv/VOD/Actu/JT/Votre-JT-jeudi-3-fevrier-lyNHCXpYJh.html',\n+        # 'md5': 'd91851bf9af73c0ad9b2cdf76c127fbb',\n+        'info_dict': {\n+            'id': 'lyNHCXpYJh',\n+            'ext': 'mp4',\n+            'description': 'md5:fc0bc4a0692d3d2dba4524053de4c7b7',\n+            'title': 'Votre JT du jeudi 3 f\u00e9vrier',\n+            'upload_date': '20220203',\n+            'thumbnail': r're:https?://.+\\.jpg',\n+            'duration': 1073,\n+            'view_count': int,\n+        },\n+        'params': {\n+            'format': 'bestvideo',\n+        },\n+    }]\n+\n+    def _extract_video(self, video_id, url=None):\n+        info = self._download_json(\n+            'https://www.alsace20.tv/visionneuse/visio_v9_js.php?key=%s&habillage=0&mode=html' % (video_id, ),\n+            video_id) or {}\n+        title = info['titre']\n+\n+        formats = []\n+        for res, fmt_url in (info.get('files') or {}).items():\n+            formats.extend(\n+                self._extract_smil_formats(fmt_url, video_id, fatal=False)\n+                if '/smil:_' in fmt_url\n+                else self._extract_mpd_formats(fmt_url, video_id, mpd_id=res, fatal=False))\n+        self._sort_formats(formats)\n+\n+        webpage = (url and self._download_webpage(url, video_id, fatal=False)) or ''\n+        thumbnail = url_or_none(dict_get(info, ('image', 'preview', )) or self._og_search_thumbnail(webpage))\n+        upload_date = self._search_regex(r'/(\\d{6})_', thumbnail, 'upload_date', default=None)\n+        upload_date = unified_strdate('20%s-%s-%s' % (upload_date[:2], upload_date[2:4], upload_date[4:])) if upload_date else None\n+        return {\n+            'id': video_id,\n+            'title': title,\n+            'formats': formats,\n+            'description': clean_html(get_element_by_class('wysiwyg', webpage)),\n+            'upload_date': upload_date,\n+            'thumbnail': thumbnail,\n+            'duration': int_or_none(self._og_search_property('video:duration', webpage) if webpage else None),\n+            'view_count': int_or_none(info.get('nb_vues')),\n+        }\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+        return self._extract_video(video_id, url)\n+\n+\n+class Alsace20TVEmbedIE(Alsace20TVIE):\n+    _VALID_URL = r'https?://(?:www\\.)?alsace20\\.tv/emb/(?P<id>[\\w]+)'\n+    _TESTS = [{\n+        'url': 'https://www.alsace20.tv/emb/lyNHCXpYJh',\n+        # 'md5': 'd91851bf9af73c0ad9b2cdf76c127fbb',\n+        'info_dict': {\n+            'id': 'lyNHCXpYJh',\n+            'ext': 'mp4',\n+            'title': 'Votre JT du jeudi 3 f\u00e9vrier',\n+            'upload_date': '20220203',\n+            'thumbnail': r're:https?://.+\\.jpg',\n+            'view_count': int,\n+        },\n+        'params': {\n+            'format': 'bestvideo',\n+        },\n+    }]\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+        return self._extract_video(video_id)\ndiff --git a/youtube_dl/extractor/bigo.py b/youtube_dl/extractor/bigo.py\nnew file mode 100644\nindex 00000000000..ddf76ac55c8\n--- /dev/null\n+++ b/youtube_dl/extractor/bigo.py\n@@ -0,0 +1,59 @@\n+# coding: utf-8\n+from __future__ import unicode_literals\n+\n+from .common import InfoExtractor\n+from ..utils import ExtractorError, urlencode_postdata\n+\n+\n+class BigoIE(InfoExtractor):\n+    _VALID_URL = r'https?://(?:www\\.)?bigo\\.tv/(?:[a-z]{2,}/)?(?P<id>[^/]+)'\n+\n+    _TESTS = [{\n+        'url': 'https://www.bigo.tv/ja/221338632',\n+        'info_dict': {\n+            'id': '6576287577575737440',\n+            'title': '\u571f\u3088\u301c\ud83d\udc81\u200d\u2642\ufe0f \u4f11\u61a9\u5ba4/REST room',\n+            'thumbnail': r're:https?://.+',\n+            'uploader': '\u2728Shin\ud83d\udcab',\n+            'uploader_id': '221338632',\n+            'is_live': True,\n+        },\n+        'skip': 'livestream',\n+    }, {\n+        'url': 'https://www.bigo.tv/th/Tarlerm1304',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://bigo.tv/115976881',\n+        'only_matching': True,\n+    }]\n+\n+    def _real_extract(self, url):\n+        user_id = self._match_id(url)\n+\n+        info_raw = self._download_json(\n+            'https://bigo.tv/studio/getInternalStudioInfo',\n+            user_id, data=urlencode_postdata({'siteId': user_id}))\n+\n+        if not isinstance(info_raw, dict):\n+            raise ExtractorError('Received invalid JSON data')\n+        if info_raw.get('code'):\n+            raise ExtractorError(\n+                'Bigo says: %s (code %s)' % (info_raw.get('msg'), info_raw.get('code')), expected=True)\n+        info = info_raw.get('data') or {}\n+\n+        if not info.get('alive'):\n+            raise ExtractorError('This user is offline.', expected=True)\n+\n+        return {\n+            'id': info.get('roomId') or user_id,\n+            'title': info.get('roomTopic') or info.get('nick_name') or user_id,\n+            'formats': [{\n+                'url': info.get('hls_src'),\n+                'ext': 'mp4',\n+                'protocol': 'm3u8',\n+            }],\n+            'thumbnail': info.get('snapshot'),\n+            'uploader': info.get('nick_name'),\n+            'uploader_id': user_id,\n+            'is_live': True,\n+        }\ndiff --git a/youtube_dl/extractor/bilibili.py b/youtube_dl/extractor/bilibili.py\nindex bff6ea194d7..d42f0e98a4b 100644\n--- a/youtube_dl/extractor/bilibili.py\n+++ b/youtube_dl/extractor/bilibili.py\n@@ -369,6 +369,11 @@ def _real_extract(self, url):\n             'filesize': int_or_none(play_data.get('size')),\n         }]\n \n+        for a_format in formats:\n+            a_format.setdefault('http_headers', {}).update({\n+                'Referer': url,\n+            })\n+\n         song = self._call_api('song/info', au_id)\n         title = song['title']\n         statistic = song.get('statistic') or {}\ndiff --git a/youtube_dl/extractor/blerp.py b/youtube_dl/extractor/blerp.py\nnew file mode 100644\nindex 00000000000..355daef6e31\n--- /dev/null\n+++ b/youtube_dl/extractor/blerp.py\n@@ -0,0 +1,173 @@\n+# coding: utf-8\n+from __future__ import unicode_literals\n+\n+import json\n+\n+from ..utils import (\n+    strip_or_none,\n+    traverse_obj,\n+)\n+from .common import InfoExtractor\n+\n+\n+class BlerpIE(InfoExtractor):\n+    IE_NAME = 'blerp'\n+    _VALID_URL = r'https?://(?:www\\.)?blerp\\.com/soundbites/(?P<id>[0-9a-zA-Z]+)'\n+    _TESTS = [{\n+        'url': 'https://blerp.com/soundbites/6320fe8745636cb4dd677a5a',\n+        'info_dict': {\n+            'id': '6320fe8745636cb4dd677a5a',\n+            'title': 'Samsung Galaxy S8 Over the Horizon Ringtone 2016',\n+            'uploader': 'luminousaj',\n+            'uploader_id': '5fb81e51aa66ae000c395478',\n+            'ext': 'mp3',\n+            'tags': ['samsung', 'galaxy', 's8', 'over the horizon', '2016', 'ringtone'],\n+        }\n+    }, {\n+        'url': 'https://blerp.com/soundbites/5bc94ef4796001000498429f',\n+        'info_dict': {\n+            'id': '5bc94ef4796001000498429f',\n+            'title': 'Yee',\n+            'uploader': '179617322678353920',\n+            'uploader_id': '5ba99cf71386730004552c42',\n+            'ext': 'mp3',\n+            'tags': ['YEE', 'YEET', 'wo ha haah catchy tune yee', 'yee']\n+        }\n+    }]\n+\n+    _GRAPHQL_OPERATIONNAME = \"webBitePageGetBite\"\n+    _GRAPHQL_QUERY = (\n+        '''query webBitePageGetBite($_id: MongoID!) {\n+            web {\n+                biteById(_id: $_id) {\n+                    ...bitePageFrag\n+                    __typename\n+                }\n+                __typename\n+            }\n+        }\n+\n+        fragment bitePageFrag on Bite {\n+            _id\n+            title\n+            userKeywords\n+            keywords\n+            color\n+            visibility\n+            isPremium\n+            owned\n+            price\n+            extraReview\n+            isAudioExists\n+            image {\n+                filename\n+                original {\n+                    url\n+                    __typename\n+                }\n+                __typename\n+            }\n+            userReactions {\n+                _id\n+                reactions\n+                createdAt\n+                __typename\n+            }\n+            topReactions\n+            totalSaveCount\n+            saved\n+            blerpLibraryType\n+            license\n+            licenseMetaData\n+            playCount\n+            totalShareCount\n+            totalFavoriteCount\n+            totalAddedToBoardCount\n+            userCategory\n+            userAudioQuality\n+            audioCreationState\n+            transcription\n+            userTranscription\n+            description\n+            createdAt\n+            updatedAt\n+            author\n+            listingType\n+            ownerObject {\n+                _id\n+                username\n+                profileImage {\n+                    filename\n+                    original {\n+                        url\n+                        __typename\n+                    }\n+                    __typename\n+                }\n+                __typename\n+            }\n+            transcription\n+            favorited\n+            visibility\n+            isCurated\n+            sourceUrl\n+            audienceRating\n+            strictAudienceRating\n+            ownerId\n+            reportObject {\n+                reportedContentStatus\n+                __typename\n+            }\n+            giphy {\n+                mp4\n+                gif\n+                __typename\n+            }\n+            audio {\n+                filename\n+                original {\n+                    url\n+                    __typename\n+                }\n+                mp3 {\n+                    url\n+                    __typename\n+                }\n+                __typename\n+            }\n+            __typename\n+        }\n+\n+        ''')\n+\n+    def _real_extract(self, url):\n+        audio_id = self._match_id(url)\n+\n+        data = {\n+            'operationName': self._GRAPHQL_OPERATIONNAME,\n+            'query': self._GRAPHQL_QUERY,\n+            'variables': {\n+                '_id': audio_id\n+            }\n+        }\n+\n+        headers = {\n+            'Content-Type': 'application/json'\n+        }\n+\n+        json_result = self._download_json('https://api.blerp.com/graphql',\n+                                          audio_id, data=json.dumps(data).encode('utf-8'), headers=headers)\n+\n+        bite_json = json_result['data']['web']['biteById']\n+\n+        info_dict = {\n+            'id': bite_json['_id'],\n+            'url': bite_json['audio']['mp3']['url'],\n+            'title': bite_json['title'],\n+            'uploader': traverse_obj(bite_json, ('ownerObject', 'username'), expected_type=strip_or_none),\n+            'uploader_id': traverse_obj(bite_json, ('ownerObject', '_id'), expected_type=strip_or_none),\n+            'ext': 'mp3',\n+            'tags': list(filter(None, map(strip_or_none, (traverse_obj(bite_json, 'userKeywords', expected_type=list) or []))) or None)\n+        }\n+\n+        return info_dict\ndiff --git a/youtube_dl/extractor/bongacams.py b/youtube_dl/extractor/bongacams.py\nindex 180542fbc83..016999d55c9 100644\n--- a/youtube_dl/extractor/bongacams.py\n+++ b/youtube_dl/extractor/bongacams.py\n@@ -1,3 +1,4 @@\n+# coding: utf-8\n from __future__ import unicode_literals\n \n import re\n@@ -12,13 +13,28 @@\n \n \n class BongaCamsIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?P<host>(?:[^/]+\\.)?bongacams\\d*\\.com)/(?P<id>[^/?&#]+)'\n+    _VALID_URL = r'https?://(?P<host>(?:[^/]+\\.)?bongacams\\d*\\.(?:com|net))/(?P<id>[^/?&#]+)'\n     _TESTS = [{\n         'url': 'https://de.bongacams.com/azumi-8',\n         'only_matching': True,\n     }, {\n         'url': 'https://cn.bongacams.com/azumi-8',\n         'only_matching': True,\n+    }, {\n+        'url': 'https://de.bongacams.net/claireashton',\n+        'info_dict': {\n+            'id': 'claireashton',\n+            'ext': 'mp4',\n+            'title': r're:ClaireAshton \\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}',\n+            'age_limit': 18,\n+            'uploader_id': 'ClaireAshton',\n+            'uploader': 'ClaireAshton',\n+            'like_count': int,\n+            'is_live': True,\n+        },\n+        'params': {\n+            'skip_download': True,\n+        },\n     }]\n \n     def _real_extract(self, url):\ndiff --git a/youtube_dl/extractor/caffeine.py b/youtube_dl/extractor/caffeine.py\nnew file mode 100644\nindex 00000000000..bffedb9a736\n--- /dev/null\n+++ b/youtube_dl/extractor/caffeine.py\n@@ -0,0 +1,79 @@\n+# coding: utf-8\n+from __future__ import unicode_literals\n+\n+from .common import InfoExtractor\n+from ..utils import (\n+    determine_ext,\n+    int_or_none,\n+    merge_dicts,\n+    parse_iso8601,\n+    T,\n+    traverse_obj,\n+    txt_or_none,\n+    urljoin,\n+)\n+\n+\n+class CaffeineTVIE(InfoExtractor):\n+    _VALID_URL = r'https?://(?:www\\.)?caffeine\\.tv/[^/]+/video/(?P<id>[0-9a-f-]+)'\n+    _TESTS = [{\n+        'url': 'https://www.caffeine.tv/TsuSurf/video/cffc0a00-e73f-11ec-8080-80017d29f26e',\n+        'info_dict': {\n+            'id': 'cffc0a00-e73f-11ec-8080-80017d29f26e',\n+            'ext': 'mp4',\n+            'title': 'GOOOOD MORNINNNNN #highlights',\n+            'timestamp': 1654702180,\n+            'upload_date': '20220608',\n+            'uploader': 'TsuSurf',\n+            'duration': 3145,\n+            'age_limit': 17,\n+        },\n+        'params': {\n+            'format': 'bestvideo',\n+        },\n+    }]\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+        json_data = self._download_json(\n+            'https://api.caffeine.tv/social/public/activity/' + video_id,\n+            video_id)\n+        broadcast_info = traverse_obj(json_data, ('broadcast_info', T(dict))) or {}\n+        title = broadcast_info['broadcast_title']\n+        video_url = broadcast_info['video_url']\n+\n+        ext = determine_ext(video_url)\n+        if ext == 'm3u8':\n+            formats = self._extract_m3u8_formats(\n+                video_url, video_id, 'mp4', entry_protocol='m3u8',\n+                fatal=False)\n+        else:\n+            formats = [{'url': video_url}]\n+        self._sort_formats(formats)\n+\n+        return merge_dicts({\n+            'id': video_id,\n+            'title': title,\n+            'formats': formats,\n+        }, traverse_obj(json_data, {\n+            'uploader': ((None, 'user'), 'username'),\n+        }, get_all=False), traverse_obj(json_data, {\n+            'like_count': ('like_count', T(int_or_none)),\n+            'view_count': ('view_count', T(int_or_none)),\n+            'comment_count': ('comment_count', T(int_or_none)),\n+            'tags': ('tags', Ellipsis, T(txt_or_none)),\n+            'is_live': 'is_live',\n+            'uploader': ('user', 'name'),\n+        }), traverse_obj(broadcast_info, {\n+            'duration': ('content_duration', T(int_or_none)),\n+            'timestamp': ('broadcast_start_time', T(parse_iso8601)),\n+            'thumbnail': ('preview_image_path', T(lambda u: urljoin(url, u))),\n+            'age_limit': ('content_rating', T(lambda r: r and {\n+                # assume Apple Store ratings [1]\n+                # 1. https://en.wikipedia.org/wiki/Mobile_software_content_rating_system\n+                'FOUR_PLUS': 0,\n+                'NINE_PLUS': 9,\n+                'TWELVE_PLUS': 12,\n+                'SEVENTEEN_PLUS': 17,\n+            }.get(r, 17))),\n+        }))\ndiff --git a/youtube_dl/extractor/callin.py b/youtube_dl/extractor/callin.py\nnew file mode 100644\nindex 00000000000..341be479f1d\n--- /dev/null\n+++ b/youtube_dl/extractor/callin.py\n@@ -0,0 +1,74 @@\n+# coding: utf-8\n+from __future__ import unicode_literals\n+\n+from .common import InfoExtractor\n+from ..compat import compat_str\n+from ..utils import (\n+    ExtractorError,\n+    traverse_obj,\n+    try_get,\n+)\n+\n+\n+class CallinIE(InfoExtractor):\n+    _VALID_URL = r'https?://(?:www\\.)?callin\\.com/episode/(?:[^/#?-]+-)*(?P<id>[^/#?-]+)'\n+    _TESTS = [{\n+        'url': 'https://www.callin.com/episode/fcc-commissioner-brendan-carr-on-elons-PrumRdSQJW',\n+        'md5': '14ede27ee2c957b7e4db93140fc0745c',\n+        'info_dict': {\n+            'id': 'PrumRdSQJW',\n+            'ext': 'mp4',\n+            'title': 'FCC Commissioner Brendan Carr on Elon\u2019s Starlink',\n+            'description': 'Or, why the government doesn\u2019t like SpaceX',\n+            'channel': 'The Pull Request',\n+            'channel_url': 'https://callin.com/show/the-pull-request-ucnDJmEKAa',\n+        }\n+    }, {\n+        'url': 'https://www.callin.com/episode/episode-81-elites-melt-down-over-student-debt-lzxMidUnjA',\n+        'md5': '16f704ddbf82a27e3930533b12062f07',\n+        'info_dict': {\n+            'id': 'lzxMidUnjA',\n+            'ext': 'mp4',\n+            'title': 'Episode 81- Elites MELT DOWN over Student Debt Victory? Rumble in NYC?',\n+            'description': 'Let\u2019s talk todays episode about the primary election shake up in NYC and the elites melting down over student debt cancelation.',\n+            'channel': 'The DEBRIEF With Briahna Joy Gray',\n+            'channel_url': 'https://callin.com/show/the-debrief-with-briahna-joy-gray-siiFDzGegm',\n+        }\n+    }]\n+\n+    def _search_nextjs_data(self, webpage, video_id, transform_source=None, fatal=True, **kw):\n+        return self._parse_json(\n+            self._search_regex(\n+                r'(?s)<script[^>]+id=[\\'\"]__NEXT_DATA__[\\'\"][^>]*>([^<]+)</script>',\n+                webpage, 'next.js data', fatal=fatal, **kw),\n+            video_id, transform_source=transform_source, fatal=fatal)\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+        webpage = self._download_webpage(url, video_id)\n+\n+        next_data = self._search_nextjs_data(webpage, video_id)\n+        episode = traverse_obj(next_data, ('props', 'pageProps', 'episode'), expected_type=dict)\n+        if not episode:\n+            raise ExtractorError('Failed to find episode data')\n+\n+        title = episode.get('title') or self._og_search_title(webpage)\n+        description = episode.get('description') or self._og_search_description(webpage)\n+\n+        formats = []\n+        formats.extend(self._extract_m3u8_formats(\n+            episode.get('m3u8'), video_id, 'mp4',\n+            entry_protocol='m3u8_native', fatal=False))\n+        self._sort_formats(formats)\n+\n+        channel = try_get(episode, lambda x: x['show']['title'], compat_str)\n+        channel_url = try_get(episode, lambda x: x['show']['linkObj']['resourceUrl'], compat_str)\n+\n+        return {\n+            'id': video_id,\n+            'title': title,\n+            'description': description,\n+            'formats': formats,\n+            'channel': channel,\n+            'channel_url': channel_url,\n+        }\ndiff --git a/youtube_dl/extractor/cammodels.py b/youtube_dl/extractor/cammodels.py\nindex 1eb81b75e95..d2e860b2424 100644\n--- a/youtube_dl/extractor/cammodels.py\n+++ b/youtube_dl/extractor/cammodels.py\n@@ -3,7 +3,6 @@\n \n from .common import InfoExtractor\n from ..utils import (\n-    ExtractorError,\n     int_or_none,\n     url_or_none,\n )\n@@ -20,32 +19,11 @@ class CamModelsIE(InfoExtractor):\n     def _real_extract(self, url):\n         user_id = self._match_id(url)\n \n-        webpage = self._download_webpage(\n-            url, user_id, headers=self.geo_verification_headers())\n-\n-        manifest_root = self._html_search_regex(\n-            r'manifestUrlRoot=([^&\\']+)', webpage, 'manifest', default=None)\n-\n-        if not manifest_root:\n-            ERRORS = (\n-                (\"I'm offline, but let's stay connected\", 'This user is currently offline'),\n-                ('in a private show', 'This user is in a private show'),\n-                ('is currently performing LIVE', 'This model is currently performing live'),\n-            )\n-            for pattern, message in ERRORS:\n-                if pattern in webpage:\n-                    error = message\n-                    expected = True\n-                    break\n-            else:\n-                error = 'Unable to find manifest URL root'\n-                expected = False\n-            raise ExtractorError(error, expected=expected)\n-\n         manifest = self._download_json(\n-            '%s%s.json' % (manifest_root, user_id), user_id)\n+            'https://manifest-server.naiadsystems.com/live/s:%s.json' % user_id, user_id)\n \n         formats = []\n+        thumbnails = []\n         for format_id, format_dict in manifest['formats'].items():\n             if not isinstance(format_dict, dict):\n                 continue\n@@ -85,6 +63,13 @@ def _real_extract(self, url):\n                         'preference': -1,\n                     })\n                 else:\n+                    if format_id == 'jpeg':\n+                        thumbnails.append({\n+                            'url': f['url'],\n+                            'width': f['width'],\n+                            'height': f['height'],\n+                            'format_id': f['format_id'],\n+                        })\n                     continue\n                 formats.append(f)\n         self._sort_formats(formats)\n@@ -92,6 +77,7 @@ def _real_extract(self, url):\n         return {\n             'id': user_id,\n             'title': self._live_title(user_id),\n+            'thumbnails': thumbnails,\n             'is_live': True,\n             'formats': formats,\n             'age_limit': 18\ndiff --git a/youtube_dl/extractor/ceskatelevize.py b/youtube_dl/extractor/ceskatelevize.py\nindex 7cb4efb746b..fe677d8e83f 100644\n--- a/youtube_dl/extractor/ceskatelevize.py\n+++ b/youtube_dl/extractor/ceskatelevize.py\n@@ -12,70 +12,136 @@\n     ExtractorError,\n     float_or_none,\n     sanitized_Request,\n-    unescapeHTML,\n-    update_url_query,\n+    str_or_none,\n+    traverse_obj,\n     urlencode_postdata,\n     USER_AGENTS,\n )\n \n \n class CeskaTelevizeIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?ceskatelevize\\.cz/ivysilani/(?:[^/?#&]+/)*(?P<id>[^/#?]+)'\n+    _VALID_URL = r'https?://(?:www\\.)?ceskatelevize\\.cz/(?:ivysilani|porady|zive)/(?:[^/?#&]+/)*(?P<id>[^/#?]+)'\n     _TESTS = [{\n-        'url': 'http://www.ceskatelevize.cz/ivysilani/ivysilani/10441294653-hyde-park-civilizace/214411058091220',\n+        'url': 'http://www.ceskatelevize.cz/ivysilani/10441294653-hyde-park-civilizace/215411058090502/bonus/20641-bonus-01-en',\n         'info_dict': {\n-            'id': '61924494877246241',\n+            'id': '61924494877028507',\n             'ext': 'mp4',\n-            'title': 'Hyde Park Civilizace: \u017divot v Gr\u00f3nsku',\n-            'description': 'md5:3fec8f6bb497be5cdb0c9e8781076626',\n+            'title': 'Bonus 01 - En - Hyde Park Civilizace',\n+            'description': 'English Subtittles',\n             'thumbnail': r're:^https?://.*\\.jpg',\n-            'duration': 3350,\n+            'duration': 81.3,\n         },\n         'params': {\n             # m3u8 download\n             'skip_download': True,\n         },\n     }, {\n-        'url': 'http://www.ceskatelevize.cz/ivysilani/10441294653-hyde-park-civilizace/215411058090502/bonus/20641-bonus-01-en',\n+        # live stream\n+        'url': 'http://www.ceskatelevize.cz/zive/ct1/',\n         'info_dict': {\n-            'id': '61924494877028507',\n+            'id': '102',\n             'ext': 'mp4',\n-            'title': 'Hyde Park Civilizace: Bonus 01 - En',\n-            'description': 'English Subtittles',\n-            'thumbnail': r're:^https?://.*\\.jpg',\n-            'duration': 81.3,\n+            'title': r'\u010cT1 - \u017eiv\u00e9 vys\u00edl\u00e1n\u00ed online',\n+            'description': 'Sledujte \u017eiv\u00e9 vys\u00edl\u00e1n\u00ed kan\u00e1lu \u010cT1 online. Vyb\u00edrat si m\u016f\u017eete i z dal\u0161\u00edch kan\u00e1l\u016f \u010cesk\u00e9 televize na kter\u00e9mkoli z va\u0161ich za\u0159\u00edzen\u00ed.',\n+            'is_live': True,\n         },\n         'params': {\n             # m3u8 download\n             'skip_download': True,\n         },\n     }, {\n-        # live stream\n+        # another\n         'url': 'http://www.ceskatelevize.cz/ivysilani/zive/ct4/',\n+        'only_matching': True,\n         'info_dict': {\n             'id': 402,\n             'ext': 'mp4',\n             'title': r're:^\u010cT Sport \\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}$',\n             'is_live': True,\n         },\n+        # 'skip': 'Georestricted to Czech Republic',\n+    }, {\n+        'url': 'http://www.ceskatelevize.cz/ivysilani/embed/iFramePlayer.php?hash=d6a3e1370d2e4fa76296b90bad4dfc19673b641e&IDEC=217 562 22150/0004&channelID=1&width=100%25',\n+        'only_matching': True,\n+    }, {\n+        # video with 18+ caution trailer\n+        'url': 'http://www.ceskatelevize.cz/porady/10520528904-queer/215562210900007-bogotart/',\n+        'info_dict': {\n+            'id': '215562210900007-bogotart',\n+            'title': 'Bogotart - Queer',\n+            'description': 'Hlavn\u00ed m\u011bsto Kolumbie v doprovodu queer um\u011blc\u016f. Vrouc\u00ed sv\u011bt pln\u00fd v\u00e1\u0161n\u011b, sebev\u011bdom\u00ed, ale i n\u00e1sil\u00ed a bolesti',\n+        },\n+        'playlist': [{\n+            'info_dict': {\n+                'id': '61924494877311053',\n+                'ext': 'mp4',\n+                'title': 'Bogotart - Queer (Varov\u00e1n\u00ed 18+)',\n+                'duration': 11.9,\n+            },\n+        }, {\n+            'info_dict': {\n+                'id': '61924494877068022',\n+                'ext': 'mp4',\n+                'title': 'Bogotart - Queer (Queer)',\n+                'thumbnail': r're:^https?://.*\\.jpg',\n+                'duration': 1558.3,\n+            },\n+        }],\n         'params': {\n             # m3u8 download\n             'skip_download': True,\n         },\n-        'skip': 'Georestricted to Czech Republic',\n     }, {\n-        'url': 'http://www.ceskatelevize.cz/ivysilani/embed/iFramePlayer.php?hash=d6a3e1370d2e4fa76296b90bad4dfc19673b641e&IDEC=217 562 22150/0004&channelID=1&width=100%25',\n+        # iframe embed\n+        'url': 'http://www.ceskatelevize.cz/porady/10614999031-neviditelni/21251212048/',\n         'only_matching': True,\n     }]\n \n+    def _search_nextjs_data(self, webpage, video_id, **kw):\n+        return self._parse_json(\n+            self._search_regex(\n+                r'(?s)<script[^>]+id=[\\'\"]__NEXT_DATA__[\\'\"][^>]*>([^<]+)</script>',\n+                webpage, 'next.js data', **kw),\n+            video_id, **kw)\n+\n     def _real_extract(self, url):\n         playlist_id = self._match_id(url)\n-\n-        webpage = self._download_webpage(url, playlist_id)\n+        webpage, urlh = self._download_webpage_handle(url, playlist_id)\n+        parsed_url = compat_urllib_parse_urlparse(urlh.geturl())\n+        site_name = self._og_search_property('site_name', webpage, fatal=False, default='\u010cesk\u00e1 televize')\n+        playlist_title = self._og_search_title(webpage, default=None)\n+        if site_name and playlist_title:\n+            playlist_title = re.split(r'\\s*[\u2014|]\\s*%s' % (site_name, ), playlist_title, 1)[0]\n+        playlist_description = self._og_search_description(webpage, default=None)\n+        if playlist_description:\n+            playlist_description = playlist_description.replace('\\xa0', ' ')\n+\n+        type_ = 'IDEC'\n+        if re.search(r'(^/porady|/zive)/', parsed_url.path):\n+            next_data = self._search_nextjs_data(webpage, playlist_id)\n+            if '/zive/' in parsed_url.path:\n+                idec = traverse_obj(next_data, ('props', 'pageProps', 'data', 'liveBroadcast', 'current', 'idec'), get_all=False)\n+            else:\n+                idec = traverse_obj(next_data, ('props', 'pageProps', 'data', ('show', 'mediaMeta'), 'idec'), get_all=False)\n+                if not idec:\n+                    idec = traverse_obj(next_data, ('props', 'pageProps', 'data', 'videobonusDetail', 'bonusId'), get_all=False)\n+                    if idec:\n+                        type_ = 'bonus'\n+            if not idec:\n+                raise ExtractorError('Failed to find IDEC id')\n+            iframe_hash = self._download_webpage(\n+                'https://www.ceskatelevize.cz/v-api/iframe-hash/',\n+                playlist_id, note='Getting IFRAME hash')\n+            query = {'hash': iframe_hash, 'origin': 'iVysilani', 'autoStart': 'true', type_: idec, }\n+            webpage = self._download_webpage(\n+                'https://www.ceskatelevize.cz/ivysilani/embed/iFramePlayer.php',\n+                playlist_id, note='Downloading player', query=query)\n \n         NOT_AVAILABLE_STRING = 'This content is not available at your territory due to limited copyright.'\n         if '%s</p>' % NOT_AVAILABLE_STRING in webpage:\n-            raise ExtractorError(NOT_AVAILABLE_STRING, expected=True)\n+            self.raise_geo_restricted(NOT_AVAILABLE_STRING)\n+        if any(not_found in webpage for not_found in ('Neplatn\u00fd parametr pro videop\u0159ehr\u00e1va\u010d', 'IDEC nebyl nalezen', )):\n+            raise ExtractorError('no video with IDEC available', video_id=idec, expected=True)\n \n         type_ = None\n         episode_id = None\n@@ -100,7 +166,7 @@ def _real_extract(self, url):\n         data = {\n             'playlist[0][type]': type_,\n             'playlist[0][id]': episode_id,\n-            'requestUrl': compat_urllib_parse_urlparse(url).path,\n+            'requestUrl': parsed_url.path,\n             'requestSource': 'iVysilani',\n         }\n \n@@ -108,7 +174,7 @@ def _real_extract(self, url):\n \n         for user_agent in (None, USER_AGENTS['Safari']):\n             req = sanitized_Request(\n-                'https://www.ceskatelevize.cz/ivysilani/ajax/get-client-playlist',\n+                'https://www.ceskatelevize.cz/ivysilani/ajax/get-client-playlist/',\n                 data=urlencode_postdata(data))\n \n             req.add_header('Content-type', 'application/x-www-form-urlencoded')\n@@ -130,9 +196,6 @@ def _real_extract(self, url):\n             req = sanitized_Request(compat_urllib_parse_unquote(playlist_url))\n             req.add_header('Referer', url)\n \n-            playlist_title = self._og_search_title(webpage, default=None)\n-            playlist_description = self._og_search_description(webpage, default=None)\n-\n             playlist = self._download_json(req, playlist_id, fatal=False)\n             if not playlist:\n                 continue\n@@ -167,7 +230,7 @@ def _real_extract(self, url):\n                     entries[num]['formats'].extend(formats)\n                     continue\n \n-                item_id = item.get('id') or item['assetId']\n+                item_id = str_or_none(item.get('id') or item['assetId'])\n                 title = item['title']\n \n                 duration = float_or_none(item.get('duration'))\n@@ -181,8 +244,6 @@ def _real_extract(self, url):\n \n                 if playlist_len == 1:\n                     final_title = playlist_title or title\n-                    if is_live:\n-                        final_title = self._live_title(final_title)\n                 else:\n                     final_title = '%s (%s)' % (playlist_title, title)\n \n@@ -200,6 +261,8 @@ def _real_extract(self, url):\n         for e in entries:\n             self._sort_formats(e['formats'])\n \n+        if len(entries) == 1:\n+            return entries[0]\n         return self.playlist_result(entries, playlist_id, playlist_title, playlist_description)\n \n     def _get_subtitles(self, episode_id, subs):\n@@ -236,54 +299,3 @@ def _fix_subtitle(subtitle):\n                     yield line\n \n         return '\\r\\n'.join(_fix_subtitle(subtitles))\n-\n-\n-class CeskaTelevizePoradyIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?ceskatelevize\\.cz/porady/(?:[^/?#&]+/)*(?P<id>[^/#?]+)'\n-    _TESTS = [{\n-        # video with 18+ caution trailer\n-        'url': 'http://www.ceskatelevize.cz/porady/10520528904-queer/215562210900007-bogotart/',\n-        'info_dict': {\n-            'id': '215562210900007-bogotart',\n-            'title': 'Queer: Bogotart',\n-            'description': 'Alternativn\u00ed pr\u016fvodce sou\u010dasn\u00fdm queer sv\u011btem',\n-        },\n-        'playlist': [{\n-            'info_dict': {\n-                'id': '61924494876844842',\n-                'ext': 'mp4',\n-                'title': 'Queer: Bogotart (Varov\u00e1n\u00ed 18+)',\n-                'duration': 10.2,\n-            },\n-        }, {\n-            'info_dict': {\n-                'id': '61924494877068022',\n-                'ext': 'mp4',\n-                'title': 'Queer: Bogotart (Queer)',\n-                'thumbnail': r're:^https?://.*\\.jpg',\n-                'duration': 1558.3,\n-            },\n-        }],\n-        'params': {\n-            # m3u8 download\n-            'skip_download': True,\n-        },\n-    }, {\n-        # iframe embed\n-        'url': 'http://www.ceskatelevize.cz/porady/10614999031-neviditelni/21251212048/',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        webpage = self._download_webpage(url, video_id)\n-\n-        data_url = update_url_query(unescapeHTML(self._search_regex(\n-            (r'<span[^>]*\\bdata-url=([\"\\'])(?P<url>(?:(?!\\1).)+)\\1',\n-             r'<iframe[^>]+\\bsrc=([\"\\'])(?P<url>(?:https?:)?//(?:www\\.)?ceskatelevize\\.cz/ivysilani/embed/iFramePlayer\\.php.*?)\\1'),\n-            webpage, 'iframe player url', group='url')), query={\n-                'autoStart': 'true',\n-        })\n-\n-        return self.url_result(data_url, ie=CeskaTelevizeIE.ie_key())\ndiff --git a/youtube_dl/extractor/clipchamp.py b/youtube_dl/extractor/clipchamp.py\nnew file mode 100644\nindex 00000000000..3b485eaab0f\n--- /dev/null\n+++ b/youtube_dl/extractor/clipchamp.py\n@@ -0,0 +1,69 @@\n+# coding: utf-8\n+from __future__ import unicode_literals\n+\n+from .common import InfoExtractor\n+from ..compat import compat_str\n+from ..utils import (\n+    ExtractorError,\n+    merge_dicts,\n+    T,\n+    traverse_obj,\n+    unified_timestamp,\n+    url_or_none,\n+)\n+\n+\n+class ClipchampIE(InfoExtractor):\n+    _VALID_URL = r'https?://(?:www\\.)?clipchamp\\.com/watch/(?P<id>[\\w-]+)'\n+    _TESTS = [{\n+        'url': 'https://clipchamp.com/watch/gRXZ4ZhdDaU',\n+        'info_dict': {\n+            'id': 'gRXZ4ZhdDaU',\n+            'ext': 'mp4',\n+            'title': 'Untitled video',\n+            'uploader': 'Alexander Schwartz',\n+            'timestamp': 1680805580,\n+            'upload_date': '20230406',\n+            'thumbnail': r're:^https?://.+\\.jpg',\n+        },\n+        'params': {\n+            'skip_download': 'm3u8',\n+            'format': 'bestvideo',\n+        },\n+    }]\n+\n+    _STREAM_URL_TMPL = 'https://%s.cloudflarestream.com/%s/manifest/video.%s'\n+    _STREAM_URL_QUERY = {'parentOrigin': 'https://clipchamp.com'}\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+        webpage = self._download_webpage(url, video_id)\n+        data = self._search_nextjs_data(webpage, video_id)['props']['pageProps']['video']\n+\n+        storage_location = data.get('storage_location')\n+        if storage_location != 'cf_stream':\n+            raise ExtractorError('Unsupported clip storage location \"%s\"' % (storage_location,))\n+\n+        path = data['download_url']\n+        iframe = self._download_webpage(\n+            'https://iframe.cloudflarestream.com/' + path, video_id, 'Downloading player iframe')\n+        subdomain = self._search_regex(\n+            r'''\\bcustomer-domain-prefix\\s*=\\s*(\"|')(?P<sd>[\\w-]+)\\1''', iframe,\n+            'subdomain', group='sd', fatal=False) or 'customer-2ut9yn3y6fta1yxe'\n+\n+        formats = self._extract_mpd_formats(\n+            self._STREAM_URL_TMPL % (subdomain, path, 'mpd'), video_id,\n+            query=self._STREAM_URL_QUERY, fatal=False, mpd_id='dash')\n+        formats.extend(self._extract_m3u8_formats(\n+            self._STREAM_URL_TMPL % (subdomain, path, 'm3u8'), video_id, 'mp4',\n+            query=self._STREAM_URL_QUERY, fatal=False, m3u8_id='hls'))\n+\n+        return merge_dicts({\n+            'id': video_id,\n+            'formats': formats,\n+            'uploader': ' '.join(traverse_obj(data, ('creator', ('first_name', 'last_name'), T(compat_str)))) or None,\n+        }, traverse_obj(data, {\n+            'title': ('project', 'project_name', T(compat_str)),\n+            'timestamp': ('created_at', T(unified_timestamp)),\n+            'thumbnail': ('thumbnail_url', T(url_or_none)),\n+        }), rev=True)\ndiff --git a/youtube_dl/extractor/common.py b/youtube_dl/extractor/common.py\nindex 797c35fd52d..9b0016d07ec 100644\n--- a/youtube_dl/extractor/common.py\n+++ b/youtube_dl/extractor/common.py\n@@ -2,7 +2,9 @@\n from __future__ import unicode_literals\n \n import base64\n+import collections\n import datetime\n+import functools\n import hashlib\n import json\n import netrc\n@@ -23,6 +25,9 @@\n     compat_getpass,\n     compat_integer_types,\n     compat_http_client,\n+    compat_kwargs,\n+    compat_map as map,\n+    compat_open as open,\n     compat_os_name,\n     compat_str,\n     compat_urllib_error,\n@@ -31,6 +36,7 @@\n     compat_urllib_request,\n     compat_urlparse,\n     compat_xml_parse_error,\n+    compat_zip as zip,\n )\n from ..downloader.f4m import (\n     get_base_url,\n@@ -54,6 +60,7 @@\n     GeoRestrictedError,\n     GeoUtils,\n     int_or_none,\n+    join_nonempty,\n     js_to_json,\n     JSON_LD_RE,\n     mimetype2ext,\n@@ -70,6 +77,9 @@\n     str_or_none,\n     str_to_int,\n     strip_or_none,\n+    T,\n+    traverse_obj,\n+    try_get,\n     unescapeHTML,\n     unified_strdate,\n     unified_timestamp,\n@@ -78,6 +88,7 @@\n     urljoin,\n     url_basename,\n     url_or_none,\n+    variadic,\n     xpath_element,\n     xpath_text,\n     xpath_with_ns,\n@@ -173,6 +184,8 @@ class InfoExtractor(object):\n                                             fragment_base_url\n                                  * \"duration\" (optional, int or float)\n                                  * \"filesize\" (optional, int)\n+                                 * \"range\" (optional, str of the form \"start-end\"\n+                                            to use in HTTP Range header)\n                     * preference Order number of this format. If this field is\n                                  present and not None, the formats get sorted\n                                  by this field, regardless of all other values.\n@@ -366,9 +379,22 @@ class InfoExtractor(object):\n     title, description etc.\n \n \n-    Subclasses of this one should re-define the _real_initialize() and\n-    _real_extract() methods and define a _VALID_URL regexp.\n-    Probably, they should also be added to the list of extractors.\n+    A subclass of InfoExtractor must be defined to handle each specific site (or\n+    several sites). Such a concrete subclass should be added to the list of\n+    extractors. It should also:\n+    * define its _VALID_URL attribute as a regexp, or a Sequence of alternative\n+      regexps (but see below)\n+    * re-define the _real_extract() method\n+    * optionally re-define the _real_initialize() method.\n+\n+    An extractor subclass may also override suitable() if necessary, but the\n+    function signature must be preserved and the function must import everything\n+    it needs (except other extractors), so that lazy_extractors works correctly.\n+    If the subclass's suitable() and _real_extract() functions avoid using\n+    _VALID_URL, the subclass need not set that class attribute.\n+\n+    An abstract subclass of InfoExtractor may be used to simplify implementation\n+    within an extractor module; it should not be added to the list of extractors.\n \n     _GEO_BYPASS attribute may be set to False in order to disable\n     geo restriction bypass mechanisms for a particular extractor.\n@@ -404,21 +430,32 @@ def __init__(self, downloader=None):\n         self.set_downloader(downloader)\n \n     @classmethod\n-    def suitable(cls, url):\n-        \"\"\"Receives a URL and returns True if suitable for this IE.\"\"\"\n-\n+    def __match_valid_url(cls, url):\n         # This does not use has/getattr intentionally - we want to know whether\n-        # we have cached the regexp for *this* class, whereas getattr would also\n-        # match the superclass\n+        # we have cached the regexp for cls, whereas getattr would also\n+        # match its superclass\n         if '_VALID_URL_RE' not in cls.__dict__:\n-            cls._VALID_URL_RE = re.compile(cls._VALID_URL)\n-        return cls._VALID_URL_RE.match(url) is not None\n+            # _VALID_URL can now be a list/tuple of patterns\n+            cls._VALID_URL_RE = tuple(map(re.compile, variadic(cls._VALID_URL)))\n+        # 20% faster than next(filter(None, (p.match(url) for p in cls._VALID_URL_RE)), None) in 2.7\n+        for p in cls._VALID_URL_RE:\n+            p = p.match(url)\n+            if p:\n+                return p\n+\n+    # The public alias can safely be overridden, as in some back-ports\n+    _match_valid_url = __match_valid_url\n+\n+    @classmethod\n+    def suitable(cls, url):\n+        \"\"\"Receives a URL and returns True if suitable for this IE.\"\"\"\n+        # This function must import everything it needs (except other extractors),\n+        # so that lazy_extractors works correctly\n+        return cls.__match_valid_url(url) is not None\n \n     @classmethod\n     def _match_id(cls, url):\n-        if '_VALID_URL_RE' not in cls.__dict__:\n-            cls._VALID_URL_RE = re.compile(cls._VALID_URL)\n-        m = cls._VALID_URL_RE.match(url)\n+        m = cls.__match_valid_url(url)\n         assert m\n         return compat_str(m.group('id'))\n \n@@ -565,6 +602,14 @@ def set_downloader(self, downloader):\n         \"\"\"Sets the downloader for this IE.\"\"\"\n         self._downloader = downloader\n \n+    @property\n+    def cache(self):\n+        return self._downloader.cache\n+\n+    @property\n+    def cookiejar(self):\n+        return self._downloader.cookiejar\n+\n     def _real_initialize(self):\n         \"\"\"Real initialization process. Redefine in subclasses.\"\"\"\n         pass\n@@ -911,14 +956,47 @@ def _parse_json(self, json_string, video_id, transform_source=None, fatal=True):\n             else:\n                 self.report_warning(errmsg + str(ve))\n \n-    def report_warning(self, msg, video_id=None):\n+    def __ie_msg(self, *msg):\n+        return '[{0}] {1}'.format(self.IE_NAME, ''.join(msg))\n+\n+    # msg, video_id=None, *args, only_once=False, **kwargs\n+    def report_warning(self, msg, *args, **kwargs):\n+        if len(args) > 0:\n+            video_id = args[0]\n+            args = args[1:]\n+        else:\n+            video_id = kwargs.pop('video_id', None)\n         idstr = '' if video_id is None else '%s: ' % video_id\n         self._downloader.report_warning(\n-            '[%s] %s%s' % (self.IE_NAME, idstr, msg))\n+            self.__ie_msg(idstr, msg), *args, **kwargs)\n \n     def to_screen(self, msg):\n         \"\"\"Print msg to screen, prefixing it with '[ie_name]'\"\"\"\n-        self._downloader.to_screen('[%s] %s' % (self.IE_NAME, msg))\n+        self._downloader.to_screen(self.__ie_msg(msg))\n+\n+    def write_debug(self, msg, only_once=False, _cache=[]):\n+        '''Log debug message or Print message to stderr'''\n+        if not self.get_param('verbose', False):\n+            return\n+        message = '[debug] ' + self.__ie_msg(msg)\n+        logger = self.get_param('logger')\n+        if logger:\n+            logger.debug(message)\n+        else:\n+            if only_once and hash(message) in _cache:\n+                return\n+            self._downloader.to_stderr(message)\n+            _cache.append(hash(message))\n+\n+    # name, default=None, *args, **kwargs\n+    def get_param(self, name, *args, **kwargs):\n+        default, args = (args[0], args[1:]) if len(args) > 0 else (kwargs.pop('default', None), args)\n+        if self._downloader:\n+            return self._downloader.params.get(name, default, *args, **kwargs)\n+        return default\n+\n+    def report_drm(self, video_id):\n+        self.raise_no_formats('This video is DRM protected', expected=True, video_id=video_id)\n \n     def report_extraction(self, id_or_name):\n         \"\"\"Report information extraction.\"\"\"\n@@ -946,6 +1024,15 @@ def raise_login_required(msg='This video is only available for registered users'\n     def raise_geo_restricted(msg='This video is not available from your location due to geo restriction', countries=None):\n         raise GeoRestrictedError(msg, countries=countries)\n \n+    def raise_no_formats(self, msg, expected=False, video_id=None):\n+        if expected and (\n+                self.get_param('ignore_no_formats_error') or self.get_param('wait_for_video')):\n+            self.report_warning(msg, video_id)\n+        elif isinstance(msg, ExtractorError):\n+            raise msg\n+        else:\n+            raise ExtractorError(msg, expected=expected, video_id=video_id)\n+\n     # Methods for following #608\n     @staticmethod\n     def url_result(url, ie=None, video_id=None, video_title=None):\n@@ -1004,6 +1091,8 @@ def _search_regex(self, pattern, string, name, default=NO_DEFAULT, fatal=True, f\n             if group is None:\n                 # return the first matching group\n                 return next(g for g in mobj.groups() if g is not None)\n+            elif isinstance(group, (list, tuple)):\n+                return tuple(mobj.group(g) for g in group)\n             else:\n                 return mobj.group(group)\n         elif default is not NO_DEFAULT:\n@@ -1014,23 +1103,76 @@ def _search_regex(self, pattern, string, name, default=NO_DEFAULT, fatal=True, f\n             self._downloader.report_warning('unable to extract %s' % _name + bug_reports_message())\n             return None\n \n+    def _search_json(self, start_pattern, string, name, video_id, **kwargs):\n+        \"\"\"Searches string for the JSON object specified by start_pattern\"\"\"\n+\n+        # self, start_pattern, string, name, video_id, *, end_pattern='',\n+        # contains_pattern=r'{(?s:.+)}', fatal=True, default=NO_DEFAULT\n+        # NB: end_pattern is only used to reduce the size of the initial match\n+        end_pattern = kwargs.pop('end_pattern', '')\n+        # (?:[\\s\\S]) simulates (?(s):.) (eg)\n+        contains_pattern = kwargs.pop('contains_pattern', r'{[\\s\\S]+}')\n+        fatal = kwargs.pop('fatal', True)\n+        default = kwargs.pop('default', NO_DEFAULT)\n+\n+        if default is NO_DEFAULT:\n+            default, has_default = {}, False\n+        else:\n+            fatal, has_default = False, True\n+\n+        json_string = self._search_regex(\n+            r'(?:{0})\\s*(?P<json>{1})\\s*(?:{2})'.format(\n+                start_pattern, contains_pattern, end_pattern),\n+            string, name, group='json', fatal=fatal, default=None if has_default else NO_DEFAULT)\n+        if not json_string:\n+            return default\n+\n+        # yt-dlp has a special JSON parser that allows trailing text.\n+        # Until that arrives here, the diagnostic from the exception\n+        # raised by json.loads() is used to extract the wanted text.\n+        # Either way, it's a problem if a transform_source() can't\n+        # handle the trailing text.\n+\n+        # force an exception\n+        kwargs['fatal'] = True\n+\n+        # self._downloader._format_err(name, self._downloader.Styles.EMPHASIS)\n+        for _ in range(2):\n+            try:\n+                # return self._parse_json(json_string, video_id, ignore_extra=True, **kwargs)\n+                transform_source = kwargs.pop('transform_source', None)\n+                if transform_source:\n+                    json_string = transform_source(json_string)\n+                return self._parse_json(json_string, video_id, **compat_kwargs(kwargs))\n+            except ExtractorError as e:\n+                end = int_or_none(self._search_regex(r'\\(char\\s+(\\d+)', error_to_compat_str(e), 'end', default=None))\n+                if end is not None:\n+                    json_string = json_string[:end]\n+                    continue\n+                msg = 'Unable to extract {0} - Failed to parse JSON'.format(name)\n+                if fatal:\n+                    raise ExtractorError(msg, cause=e.cause, video_id=video_id)\n+                elif not has_default:\n+                    self.report_warning(\n+                        '{0}: {1}'.format(msg, error_to_compat_str(e)), video_id=video_id)\n+            return default\n+\n     def _html_search_regex(self, pattern, string, name, default=NO_DEFAULT, fatal=True, flags=0, group=None):\n         \"\"\"\n         Like _search_regex, but strips HTML tags and unescapes entities.\n         \"\"\"\n         res = self._search_regex(pattern, string, name, default, fatal, flags, group)\n-        if res:\n-            return clean_html(res).strip()\n-        else:\n-            return res\n+        if isinstance(res, tuple):\n+            return tuple(map(clean_html, res))\n+        return clean_html(res)\n \n     def _get_netrc_login_info(self, netrc_machine=None):\n         username = None\n         password = None\n-        netrc_machine = netrc_machine or self._NETRC_MACHINE\n \n         if self._downloader.params.get('usenetrc', False):\n             try:\n+                netrc_machine = netrc_machine or self._NETRC_MACHINE\n                 info = netrc.netrc().authenticators(netrc_machine)\n                 if info is not None:\n                     username = info[0]\n@@ -1038,7 +1180,7 @@ def _get_netrc_login_info(self, netrc_machine=None):\n                 else:\n                     raise netrc.NetrcParseError(\n                         'No authenticators for %s' % netrc_machine)\n-            except (IOError, netrc.NetrcParseError) as err:\n+            except (AttributeError, IOError, netrc.NetrcParseError) as err:\n                 self._downloader.report_warning(\n                     'parsing .netrc: %s' % error_to_compat_str(err))\n \n@@ -1086,7 +1228,7 @@ def _get_tfa_info(self, note='two-factor verification code'):\n     # Helper functions for extracting OpenGraph info\n     @staticmethod\n     def _og_regexes(prop):\n-        content_re = r'content=(?:\"([^\"]+?)\"|\\'([^\\']+?)\\'|\\s*([^\\s\"\\'=<>`]+?))'\n+        content_re = r'content=(?:\"([^\"]+?)\"|\\'([^\\']+?)\\'|\\s*([^\\s\"\\'=<>`]+?)(?=\\s|/?>))'\n         property_re = (r'(?:name|property)=(?:\\'og[:-]%(prop)s\\'|\"og[:-]%(prop)s\"|\\s*og[:-]%(prop)s\\b)'\n                        % {'prop': re.escape(prop)})\n         template = r'<meta[^>]+?%s[^>]+?%s'\n@@ -1347,6 +1489,48 @@ def extract_video_object(e):\n                     break\n         return dict((k, v) for k, v in info.items() if v is not None)\n \n+    def _search_nextjs_data(self, webpage, video_id, **kw):\n+        # ..., *, transform_source=None, fatal=True, default=NO_DEFAULT\n+\n+        # TODO: remove this backward compat\n+        default = kw.get('default', NO_DEFAULT)\n+        if default == '{}':\n+            kw['default'] = {}\n+            kw = compat_kwargs(kw)\n+\n+        return self._search_json(\n+            r'''<script\\s[^>]*?\\bid\\s*=\\s*('|\")__NEXT_DATA__\\1[^>]*>''',\n+            webpage, 'next.js data', video_id, end_pattern='</script>',\n+            **kw)\n+\n+    def _search_nuxt_data(self, webpage, video_id, *args, **kwargs):\n+        \"\"\"Parses Nuxt.js metadata. This works as long as the function __NUXT__ invokes is a pure function\"\"\"\n+\n+        # self, webpage, video_id, context_name='__NUXT__', *, fatal=True, traverse=('data', 0)\n+        context_name = args[0] if len(args) > 0 else kwargs.get('context_name', '__NUXT__')\n+        fatal = kwargs.get('fatal', True)\n+        traverse = kwargs.get('traverse', ('data', 0))\n+\n+        re_ctx = re.escape(context_name)\n+\n+        FUNCTION_RE = (r'\\(\\s*function\\s*\\((?P<arg_keys>[\\s\\S]*?)\\)\\s*\\{\\s*'\n+                       r'return\\s+(?P<js>\\{[\\s\\S]*?})\\s*;?\\s*}\\s*\\((?P<arg_vals>[\\s\\S]*?)\\)')\n+\n+        js, arg_keys, arg_vals = self._search_regex(\n+            (p.format(re_ctx, FUNCTION_RE) for p in\n+             (r'<script>\\s*window\\s*\\.\\s*{0}\\s*=\\s*{1}\\s*\\)\\s*;?\\s*</script>',\n+              r'{0}\\s*\\([\\s\\S]*?{1}')),\n+            webpage, context_name, group=('js', 'arg_keys', 'arg_vals'),\n+            default=NO_DEFAULT if fatal else (None, None, None))\n+        if js is None:\n+            return {}\n+\n+        args = dict(zip(arg_keys.split(','), map(json.dumps, self._parse_json(\n+            '[{0}]'.format(arg_vals), video_id, transform_source=js_to_json, fatal=fatal) or ())))\n+\n+        ret = self._parse_json(js, video_id, transform_source=functools.partial(js_to_json, vars=args), fatal=fatal)\n+        return traverse_obj(ret, traverse) or {}\n+\n     @staticmethod\n     def _hidden_inputs(html):\n         html = re.sub(r'<!--(?:(?!<!--).)*-->', '', html)\n@@ -1631,6 +1815,12 @@ def _m3u8_meta_format(self, m3u8_url, ext=None, preference=None, m3u8_id=None):\n             'format_note': 'Quality selection URL',\n         }\n \n+    def _report_ignoring_subs(self, name):\n+        self.report_warning(bug_reports_message(\n+            'Ignoring subtitle tracks found in the {0} manifest; '\n+            'if any subtitle tracks are missing,'.format(name)\n+        ), only_once=True)\n+\n     def _extract_m3u8_formats(self, m3u8_url, video_id, ext=None,\n                               entry_protocol='m3u8', preference=None,\n                               m3u8_id=None, note=None, errnote=None,\n@@ -2071,23 +2261,46 @@ def _parse_xspf(self, xspf_doc, playlist_id, xspf_url=None, xspf_base_url=None):\n             })\n         return entries\n \n-    def _extract_mpd_formats(self, mpd_url, video_id, mpd_id=None, note=None, errnote=None, fatal=True, data=None, headers={}, query={}):\n+    def _extract_mpd_formats(self, *args, **kwargs):\n+        fmts, subs = self._extract_mpd_formats_and_subtitles(*args, **kwargs)\n+        if subs:\n+            self._report_ignoring_subs('DASH')\n+        return fmts\n+\n+    def _extract_mpd_formats_and_subtitles(\n+            self, mpd_url, video_id, mpd_id=None, note=None, errnote=None,\n+            fatal=True, data=None, headers=None, query=None):\n+\n+        # TODO: or not? param not yet implemented\n+        if self.get_param('ignore_no_formats_error'):\n+            fatal = False\n+\n         res = self._download_xml_handle(\n             mpd_url, video_id,\n-            note=note or 'Downloading MPD manifest',\n-            errnote=errnote or 'Failed to download MPD manifest',\n-            fatal=fatal, data=data, headers=headers, query=query)\n+            note='Downloading MPD manifest' if note is None else note,\n+            errnote='Failed to download MPD manifest' if errnote is None else errnote,\n+            fatal=fatal, data=data, headers=headers or {}, query=query or {})\n         if res is False:\n-            return []\n+            return [], {}\n         mpd_doc, urlh = res\n         if mpd_doc is None:\n-            return []\n-        mpd_base_url = base_url(urlh.geturl())\n+            return [], {}\n+\n+        # We could have been redirected to a new url when we retrieved our mpd file.\n+        mpd_url = urlh.geturl()\n+        mpd_base_url = base_url(mpd_url)\n \n-        return self._parse_mpd_formats(\n+        return self._parse_mpd_formats_and_subtitles(\n             mpd_doc, mpd_id, mpd_base_url, mpd_url)\n \n-    def _parse_mpd_formats(self, mpd_doc, mpd_id=None, mpd_base_url='', mpd_url=None):\n+    def _parse_mpd_formats(self, *args, **kwargs):\n+        fmts, subs = self._parse_mpd_formats_and_subtitles(*args, **kwargs)\n+        if subs:\n+            self._report_ignoring_subs('DASH')\n+        return fmts\n+\n+    def _parse_mpd_formats_and_subtitles(\n+            self, mpd_doc, mpd_id=None, mpd_base_url='', mpd_url=None):\n         \"\"\"\n         Parse formats from MPD manifest.\n         References:\n@@ -2095,8 +2308,10 @@ def _parse_mpd_formats(self, mpd_doc, mpd_id=None, mpd_base_url='', mpd_url=None\n             http://standards.iso.org/ittf/PubliclyAvailableStandards/c065274_ISO_IEC_23009-1_2014.zip\n          2. https://en.wikipedia.org/wiki/Dynamic_Adaptive_Streaming_over_HTTP\n         \"\"\"\n-        if mpd_doc.get('type') == 'dynamic':\n-            return []\n+        # TODO: param not yet implemented: default like previous yt-dl logic\n+        if not self.get_param('dynamic_mpd', False):\n+            if mpd_doc.get('type') == 'dynamic':\n+                return [], {}\n \n         namespace = self._search_regex(r'(?i)^{([^}]+)?}MPD$', mpd_doc.tag, 'namespace', default=None)\n \n@@ -2106,8 +2321,24 @@ def _add_ns(path):\n         def is_drm_protected(element):\n             return element.find(_add_ns('ContentProtection')) is not None\n \n+        from ..utils import YoutubeDLHandler\n+        fix_path = YoutubeDLHandler._fix_path\n+\n+        def resolve_base_url(element, parent_base_url=None):\n+            # TODO: use native XML traversal when ready\n+            b_url = traverse_obj(element, (\n+                T(lambda e: e.find(_add_ns('BaseURL')).text)))\n+            if parent_base_url and b_url:\n+                if not parent_base_url[-1] in ('/', ':'):\n+                    parent_base_url += '/'\n+                b_url = compat_urlparse.urljoin(parent_base_url, b_url)\n+            if b_url:\n+                b_url = fix_path(b_url)\n+            return b_url or parent_base_url\n+\n         def extract_multisegment_info(element, ms_parent_info):\n             ms_info = ms_parent_info.copy()\n+            base_url = ms_info['base_url'] = resolve_base_url(element, ms_info.get('base_url'))\n \n             # As per [1, 5.3.9.2.2] SegmentList and SegmentTemplate share some\n             # common attributes and elements.  We will only extract relevant\n@@ -2141,15 +2372,27 @@ def extract_common(source):\n             def extract_Initialization(source):\n                 initialization = source.find(_add_ns('Initialization'))\n                 if initialization is not None:\n-                    ms_info['initialization_url'] = initialization.attrib['sourceURL']\n+                    ms_info['initialization_url'] = initialization.get('sourceURL') or base_url\n+                    initialization_url_range = initialization.get('range')\n+                    if initialization_url_range:\n+                        ms_info['initialization_url_range'] = initialization_url_range\n \n             segment_list = element.find(_add_ns('SegmentList'))\n             if segment_list is not None:\n                 extract_common(segment_list)\n                 extract_Initialization(segment_list)\n                 segment_urls_e = segment_list.findall(_add_ns('SegmentURL'))\n-                if segment_urls_e:\n-                    ms_info['segment_urls'] = [segment.attrib['media'] for segment in segment_urls_e]\n+                segment_urls = traverse_obj(segment_urls_e, (\n+                    Ellipsis, T(lambda e: e.attrib), 'media'))\n+                if segment_urls:\n+                    ms_info['segment_urls'] = segment_urls\n+                segment_urls_range = traverse_obj(segment_urls_e, (\n+                    Ellipsis, T(lambda e: e.attrib), 'mediaRange',\n+                    T(lambda r: re.findall(r'^\\d+-\\d+$', r)), 0))\n+                if segment_urls_range:\n+                    ms_info['segment_urls_range'] = segment_urls_range\n+                    if not segment_urls:\n+                        ms_info['segment_urls'] = [base_url for _ in segment_urls_range]\n             else:\n                 segment_template = element.find(_add_ns('SegmentTemplate'))\n                 if segment_template is not None:\n@@ -2165,17 +2408,20 @@ def extract_Initialization(source):\n             return ms_info\n \n         mpd_duration = parse_duration(mpd_doc.get('mediaPresentationDuration'))\n-        formats = []\n+        formats, subtitles = [], {}\n+        stream_numbers = collections.defaultdict(int)\n+        mpd_base_url = resolve_base_url(mpd_doc, mpd_base_url or mpd_url)\n         for period in mpd_doc.findall(_add_ns('Period')):\n             period_duration = parse_duration(period.get('duration')) or mpd_duration\n             period_ms_info = extract_multisegment_info(period, {\n                 'start_number': 1,\n                 'timescale': 1,\n+                'base_url': mpd_base_url,\n             })\n             for adaptation_set in period.findall(_add_ns('AdaptationSet')):\n                 if is_drm_protected(adaptation_set):\n                     continue\n-                adaption_set_ms_info = extract_multisegment_info(adaptation_set, period_ms_info)\n+                adaptation_set_ms_info = extract_multisegment_info(adaptation_set, period_ms_info)\n                 for representation in adaptation_set.findall(_add_ns('Representation')):\n                     if is_drm_protected(representation):\n                         continue\n@@ -2183,27 +2429,35 @@ def extract_Initialization(source):\n                     representation_attrib.update(representation.attrib)\n                     # According to [1, 5.3.7.2, Table 9, page 41], @mimeType is mandatory\n                     mime_type = representation_attrib['mimeType']\n-                    content_type = mime_type.split('/')[0]\n-                    if content_type == 'text':\n-                        # TODO implement WebVTT downloading\n-                        pass\n-                    elif content_type in ('video', 'audio'):\n-                        base_url = ''\n-                        for element in (representation, adaptation_set, period, mpd_doc):\n-                            base_url_e = element.find(_add_ns('BaseURL'))\n-                            if base_url_e is not None:\n-                                base_url = base_url_e.text + base_url\n-                                if re.match(r'^https?://', base_url):\n-                                    break\n-                        if mpd_base_url and not re.match(r'^https?://', base_url):\n-                            if not mpd_base_url.endswith('/') and not base_url.startswith('/'):\n-                                mpd_base_url += '/'\n-                            base_url = mpd_base_url + base_url\n-                        representation_id = representation_attrib.get('id')\n-                        lang = representation_attrib.get('lang')\n-                        url_el = representation.find(_add_ns('BaseURL'))\n-                        filesize = int_or_none(url_el.attrib.get('{http://youtube.com/yt/2012/10/10}contentLength') if url_el is not None else None)\n-                        bandwidth = int_or_none(representation_attrib.get('bandwidth'))\n+                    content_type = representation_attrib.get('contentType') or mime_type.split('/')[0]\n+                    codec_str = representation_attrib.get('codecs', '')\n+                    # Some kind of binary subtitle found in some youtube livestreams\n+                    if mime_type == 'application/x-rawcc':\n+                        codecs = {'scodec': codec_str}\n+                    else:\n+                        codecs = parse_codecs(codec_str)\n+                    if content_type not in ('video', 'audio', 'text'):\n+                        if mime_type == 'image/jpeg':\n+                            content_type = mime_type\n+                        elif codecs.get('vcodec', 'none') != 'none':\n+                            content_type = 'video'\n+                        elif codecs.get('acodec', 'none') != 'none':\n+                            content_type = 'audio'\n+                        elif codecs.get('scodec', 'none') != 'none':\n+                            content_type = 'text'\n+                        elif mimetype2ext(mime_type) in ('tt', 'dfxp', 'ttml', 'xml', 'json'):\n+                            content_type = 'text'\n+                        else:\n+                            self.report_warning('Unknown MIME type %s in DASH manifest' % mime_type)\n+                            continue\n+\n+                    representation_id = representation_attrib.get('id')\n+                    lang = representation_attrib.get('lang')\n+                    url_el = representation.find(_add_ns('BaseURL'))\n+                    filesize = int_or_none(url_el.get('{http://youtube.com/yt/2012/10/10}contentLength') if url_el is not None else None)\n+                    bandwidth = int_or_none(representation_attrib.get('bandwidth'))\n+                    format_id = join_nonempty(representation_id or content_type, mpd_id)\n+                    if content_type in ('video', 'audio'):\n                         f = {\n                             'format_id': '%s-%s' % (mpd_id, representation_id) if mpd_id else representation_id,\n                             'manifest_url': mpd_url,\n@@ -2218,104 +2472,130 @@ def extract_Initialization(source):\n                             'filesize': filesize,\n                             'container': mimetype2ext(mime_type) + '_dash',\n                         }\n-                        f.update(parse_codecs(representation_attrib.get('codecs')))\n-                        representation_ms_info = extract_multisegment_info(representation, adaption_set_ms_info)\n-\n-                        def prepare_template(template_name, identifiers):\n-                            tmpl = representation_ms_info[template_name]\n-                            # First of, % characters outside $...$ templates\n-                            # must be escaped by doubling for proper processing\n-                            # by % operator string formatting used further (see\n-                            # https://github.com/ytdl-org/youtube-dl/issues/16867).\n-                            t = ''\n-                            in_template = False\n-                            for c in tmpl:\n+                        f.update(codecs)\n+                    elif content_type == 'text':\n+                        f = {\n+                            'ext': mimetype2ext(mime_type),\n+                            'manifest_url': mpd_url,\n+                            'filesize': filesize,\n+                        }\n+                    elif content_type == 'image/jpeg':\n+                        # See test case in VikiIE\n+                        # https://www.viki.com/videos/1175236v-choosing-spouse-by-lottery-episode-1\n+                        f = {\n+                            'format_id': format_id,\n+                            'ext': 'mhtml',\n+                            'manifest_url': mpd_url,\n+                            'format_note': 'DASH storyboards (jpeg)',\n+                            'acodec': 'none',\n+                            'vcodec': 'none',\n+                        }\n+                    if is_drm_protected(adaptation_set) or is_drm_protected(representation):\n+                        f['has_drm'] = True\n+                    representation_ms_info = extract_multisegment_info(representation, adaptation_set_ms_info)\n+\n+                    def prepare_template(template_name, identifiers):\n+                        tmpl = representation_ms_info[template_name]\n+                        # First of, % characters outside $...$ templates\n+                        # must be escaped by doubling for proper processing\n+                        # by % operator string formatting used further (see\n+                        # https://github.com/ytdl-org/youtube-dl/issues/16867).\n+                        t = ''\n+                        in_template = False\n+                        for c in tmpl:\n+                            t += c\n+                            if c == '$':\n+                                in_template = not in_template\n+                            elif c == '%' and not in_template:\n                                 t += c\n-                                if c == '$':\n-                                    in_template = not in_template\n-                                elif c == '%' and not in_template:\n-                                    t += c\n-                            # Next, $...$ templates are translated to their\n-                            # %(...) counterparts to be used with % operator\n-                            t = t.replace('$RepresentationID$', representation_id)\n-                            t = re.sub(r'\\$(%s)\\$' % '|'.join(identifiers), r'%(\\1)d', t)\n-                            t = re.sub(r'\\$(%s)%%([^$]+)\\$' % '|'.join(identifiers), r'%(\\1)\\2', t)\n-                            t.replace('$$', '$')\n-                            return t\n-\n-                        # @initialization is a regular template like @media one\n-                        # so it should be handled just the same way (see\n-                        # https://github.com/ytdl-org/youtube-dl/issues/11605)\n-                        if 'initialization' in representation_ms_info:\n-                            initialization_template = prepare_template(\n-                                'initialization',\n-                                # As per [1, 5.3.9.4.2, Table 15, page 54] $Number$ and\n-                                # $Time$ shall not be included for @initialization thus\n-                                # only $Bandwidth$ remains\n-                                ('Bandwidth', ))\n-                            representation_ms_info['initialization_url'] = initialization_template % {\n-                                'Bandwidth': bandwidth,\n-                            }\n-\n-                        def location_key(location):\n-                            return 'url' if re.match(r'^https?://', location) else 'path'\n-\n-                        if 'segment_urls' not in representation_ms_info and 'media' in representation_ms_info:\n-\n-                            media_template = prepare_template('media', ('Number', 'Bandwidth', 'Time'))\n-                            media_location_key = location_key(media_template)\n-\n-                            # As per [1, 5.3.9.4.4, Table 16, page 55] $Number$ and $Time$\n-                            # can't be used at the same time\n-                            if '%(Number' in media_template and 's' not in representation_ms_info:\n-                                segment_duration = None\n-                                if 'total_number' not in representation_ms_info and 'segment_duration' in representation_ms_info:\n-                                    segment_duration = float_or_none(representation_ms_info['segment_duration'], representation_ms_info['timescale'])\n-                                    representation_ms_info['total_number'] = int(math.ceil(float(period_duration) / segment_duration))\n-                                representation_ms_info['fragments'] = [{\n-                                    media_location_key: media_template % {\n-                                        'Number': segment_number,\n-                                        'Bandwidth': bandwidth,\n-                                    },\n-                                    'duration': segment_duration,\n-                                } for segment_number in range(\n-                                    representation_ms_info['start_number'],\n-                                    representation_ms_info['total_number'] + representation_ms_info['start_number'])]\n-                            else:\n-                                # $Number*$ or $Time$ in media template with S list available\n-                                # Example $Number*$: http://www.svtplay.se/klipp/9023742/stopptid-om-bjorn-borg\n-                                # Example $Time$: https://play.arkena.com/embed/avp/v2/player/media/b41dda37-d8e7-4d3f-b1b5-9a9db578bdfe/1/129411\n-                                representation_ms_info['fragments'] = []\n-                                segment_time = 0\n-                                segment_d = None\n-                                segment_number = representation_ms_info['start_number']\n-\n-                                def add_segment_url():\n-                                    segment_url = media_template % {\n-                                        'Time': segment_time,\n-                                        'Bandwidth': bandwidth,\n-                                        'Number': segment_number,\n-                                    }\n-                                    representation_ms_info['fragments'].append({\n-                                        media_location_key: segment_url,\n-                                        'duration': float_or_none(segment_d, representation_ms_info['timescale']),\n-                                    })\n+                        # Next, $...$ templates are translated to their\n+                        # %(...) counterparts to be used with % operator\n+                        t = t.replace('$RepresentationID$', representation_id)\n+                        t = re.sub(r'\\$(%s)\\$' % '|'.join(identifiers), r'%(\\1)d', t)\n+                        t = re.sub(r'\\$(%s)%%([^$]+)\\$' % '|'.join(identifiers), r'%(\\1)\\2', t)\n+                        t.replace('$$', '$')\n+                        return t\n+\n+                    # @initialization is a regular template like @media one\n+                    # so it should be handled just the same way (see\n+                    # https://github.com/ytdl-org/youtube-dl/issues/11605)\n+                    if 'initialization' in representation_ms_info:\n+                        initialization_template = prepare_template(\n+                            'initialization',\n+                            # As per [1, 5.3.9.4.2, Table 15, page 54] $Number$ and\n+                            # $Time$ shall not be included for @initialization thus\n+                            # only $Bandwidth$ remains\n+                            ('Bandwidth', ))\n+                        representation_ms_info['initialization_url'] = initialization_template % {\n+                            'Bandwidth': bandwidth,\n+                        }\n \n-                                for num, s in enumerate(representation_ms_info['s']):\n-                                    segment_time = s.get('t') or segment_time\n-                                    segment_d = s['d']\n+                    def location_key(location):\n+                        return 'url' if re.match(r'^https?://', location) else 'path'\n+\n+                    def calc_segment_duration():\n+                        return float_or_none(\n+                            representation_ms_info['segment_duration'],\n+                            representation_ms_info['timescale']) if 'segment_duration' in representation_ms_info else None\n+\n+                    if 'segment_urls' not in representation_ms_info and 'media' in representation_ms_info:\n+\n+                        media_template = prepare_template('media', ('Number', 'Bandwidth', 'Time'))\n+                        media_location_key = location_key(media_template)\n+\n+                        # As per [1, 5.3.9.4.4, Table 16, page 55] $Number$ and $Time$\n+                        # can't be used at the same time\n+                        if '%(Number' in media_template and 's' not in representation_ms_info:\n+                            segment_duration = None\n+                            if 'total_number' not in representation_ms_info and 'segment_duration' in representation_ms_info:\n+                                segment_duration = float_or_none(representation_ms_info['segment_duration'], representation_ms_info['timescale'])\n+                                representation_ms_info['total_number'] = int(math.ceil(\n+                                    float_or_none(period_duration, segment_duration, default=0)))\n+                            representation_ms_info['fragments'] = [{\n+                                media_location_key: media_template % {\n+                                    'Number': segment_number,\n+                                    'Bandwidth': bandwidth,\n+                                },\n+                                'duration': segment_duration,\n+                            } for segment_number in range(\n+                                representation_ms_info['start_number'],\n+                                representation_ms_info['total_number'] + representation_ms_info['start_number'])]\n+                        else:\n+                            # $Number*$ or $Time$ in media template with S list available\n+                            # Example $Number*$: http://www.svtplay.se/klipp/9023742/stopptid-om-bjorn-borg\n+                            # Example $Time$: https://play.arkena.com/embed/avp/v2/player/media/b41dda37-d8e7-4d3f-b1b5-9a9db578bdfe/1/129411\n+                            representation_ms_info['fragments'] = []\n+                            segment_time = 0\n+                            segment_d = None\n+                            segment_number = representation_ms_info['start_number']\n+\n+                            def add_segment_url():\n+                                segment_url = media_template % {\n+                                    'Time': segment_time,\n+                                    'Bandwidth': bandwidth,\n+                                    'Number': segment_number,\n+                                }\n+                                representation_ms_info['fragments'].append({\n+                                    media_location_key: segment_url,\n+                                    'duration': float_or_none(segment_d, representation_ms_info['timescale']),\n+                                })\n+\n+                            for num, s in enumerate(representation_ms_info['s']):\n+                                segment_time = s.get('t') or segment_time\n+                                segment_d = s['d']\n+                                add_segment_url()\n+                                segment_number += 1\n+                                for r in range(s.get('r', 0)):\n+                                    segment_time += segment_d\n                                     add_segment_url()\n                                     segment_number += 1\n-                                    for r in range(s.get('r', 0)):\n-                                        segment_time += segment_d\n-                                        add_segment_url()\n-                                        segment_number += 1\n-                                    segment_time += segment_d\n-                        elif 'segment_urls' in representation_ms_info and 's' in representation_ms_info:\n+                                segment_time += segment_d\n+                    elif 'segment_urls' in representation_ms_info:\n+                        fragments = []\n+                        if 's' in representation_ms_info:\n                             # No media template\n                             # Example: https://www.youtube.com/watch?v=iXZV5uAYMJI\n                             # or any YouTube dashsegments video\n-                            fragments = []\n                             segment_index = 0\n                             timescale = representation_ms_info['timescale']\n                             for s in representation_ms_info['s']:\n@@ -2327,48 +2607,78 @@ def add_segment_url():\n                                         'duration': duration,\n                                     })\n                                     segment_index += 1\n-                            representation_ms_info['fragments'] = fragments\n-                        elif 'segment_urls' in representation_ms_info:\n+                        elif 'segment_urls_range' in representation_ms_info:\n+                            # Segment URLs with mediaRange\n+                            # Example: https://kinescope.io/200615537/master.mpd\n+                            # https://github.com/ytdl-org/youtube-dl/issues/30235\n+                            # or any mpd generated with Bento4 `mp4dash --no-split --use-segment-list`\n+                            segment_duration = calc_segment_duration()\n+                            for segment_url, segment_url_range in zip(\n+                                    representation_ms_info['segment_urls'], representation_ms_info['segment_urls_range']):\n+                                fragments.append({\n+                                    location_key(segment_url): segment_url,\n+                                    'range': segment_url_range,\n+                                    'duration': segment_duration,\n+                                })\n+                        else:\n                             # Segment URLs with no SegmentTimeline\n                             # Example: https://www.seznam.cz/zpravy/clanek/cesko-zasahne-vitr-o-sile-vichrice-muze-byt-i-zivotu-nebezpecny-39091\n                             # https://github.com/ytdl-org/youtube-dl/pull/14844\n-                            fragments = []\n-                            segment_duration = float_or_none(\n-                                representation_ms_info['segment_duration'],\n-                                representation_ms_info['timescale']) if 'segment_duration' in representation_ms_info else None\n+                            segment_duration = calc_segment_duration()\n                             for segment_url in representation_ms_info['segment_urls']:\n-                                fragment = {\n+                                fragments.append({\n                                     location_key(segment_url): segment_url,\n-                                }\n-                                if segment_duration:\n-                                    fragment['duration'] = segment_duration\n-                                fragments.append(fragment)\n-                            representation_ms_info['fragments'] = fragments\n-                        # If there is a fragments key available then we correctly recognized fragmented media.\n-                        # Otherwise we will assume unfragmented media with direct access. Technically, such\n-                        # assumption is not necessarily correct since we may simply have no support for\n-                        # some forms of fragmented media renditions yet, but for now we'll use this fallback.\n-                        if 'fragments' in representation_ms_info:\n-                            f.update({\n-                                # NB: mpd_url may be empty when MPD manifest is parsed from a string\n-                                'url': mpd_url or base_url,\n-                                'fragment_base_url': base_url,\n-                                'fragments': [],\n-                                'protocol': 'http_dash_segments',\n+                                    'duration': segment_duration,\n+                                })\n+                        representation_ms_info['fragments'] = fragments\n+\n+                    # If there is a fragments key available then we correctly recognized fragmented media.\n+                    # Otherwise we will assume unfragmented media with direct access. Technically, such\n+                    # assumption is not necessarily correct since we may simply have no support for\n+                    # some forms of fragmented media renditions yet, but for now we'll use this fallback.\n+                    if 'fragments' in representation_ms_info:\n+                        base_url = representation_ms_info['base_url']\n+                        f.update({\n+                            # NB: mpd_url may be empty when MPD manifest is parsed from a string\n+                            'url': mpd_url or base_url,\n+                            'fragment_base_url': base_url,\n+                            'fragments': [],\n+                            'protocol': 'http_dash_segments',\n+                        })\n+                        if 'initialization_url' in representation_ms_info and 'initialization_url_range' in representation_ms_info:\n+                            # Initialization URL with range (accompanied by Segment URLs with mediaRange above)\n+                            # https://github.com/ytdl-org/youtube-dl/issues/30235\n+                            initialization_url = representation_ms_info['initialization_url']\n+                            f['fragments'].append({\n+                                location_key(initialization_url): initialization_url,\n+                                'range': representation_ms_info['initialization_url_range'],\n                             })\n-                            if 'initialization_url' in representation_ms_info:\n-                                initialization_url = representation_ms_info['initialization_url']\n-                                if not f.get('url'):\n-                                    f['url'] = initialization_url\n-                                f['fragments'].append({location_key(initialization_url): initialization_url})\n-                            f['fragments'].extend(representation_ms_info['fragments'])\n-                        else:\n-                            # Assuming direct URL to unfragmented media.\n-                            f['url'] = base_url\n-                        formats.append(f)\n+                        elif 'initialization_url' in representation_ms_info:\n+                            initialization_url = representation_ms_info['initialization_url']\n+                            if not f.get('url'):\n+                                f['url'] = initialization_url\n+                            f['fragments'].append({location_key(initialization_url): initialization_url})\n+                        elif 'initialization_url_range' in representation_ms_info:\n+                            # no Initialization URL but range (accompanied by no Segment URLs but mediaRange above)\n+                            # https://github.com/ytdl-org/youtube-dl/issues/27575\n+                            f['fragments'].append({\n+                                location_key(base_url): base_url,\n+                                'range': representation_ms_info['initialization_url_range'],\n+                            })\n+                        f['fragments'].extend(representation_ms_info['fragments'])\n+                        if not period_duration:\n+                            period_duration = sum(traverse_obj(representation_ms_info, (\n+                                'fragments', Ellipsis, 'duration', T(float_or_none))))\n                     else:\n-                        self.report_warning('Unknown MIME type %s in DASH manifest' % mime_type)\n-        return formats\n+                        # Assuming direct URL to unfragmented media.\n+                        f['url'] = representation_ms_info['base_url']\n+                    if content_type in ('video', 'audio', 'image/jpeg'):\n+                        f['manifest_stream_number'] = stream_numbers[f['url']]\n+                        stream_numbers[f['url']] += 1\n+                        formats.append(f)\n+                    elif content_type == 'text':\n+                        subtitles.setdefault(lang or 'und', []).append(f)\n+        return formats, subtitles\n \n     def _extract_ism_formats(self, ism_url, video_id, ism_id=None, note=None, errnote=None, fatal=True, data=None, headers={}, query={}):\n         res = self._download_xml_handle(\n@@ -2494,7 +2804,8 @@ def parse_content_type(content_type):\n                 return f\n             return {}\n \n-        def _media_formats(src, cur_media_type, type_info={}):\n+        def _media_formats(src, cur_media_type, type_info=None):\n+            type_info = type_info or {}\n             full_url = absolute_url(src)\n             ext = type_info.get('ext') or determine_ext(full_url)\n             if ext == 'm3u8':\n@@ -2512,6 +2823,7 @@ def _media_formats(src, cur_media_type, type_info={}):\n                 formats = [{\n                     'url': full_url,\n                     'vcodec': 'none' if cur_media_type == 'audio' else None,\n+                    'ext': ext,\n                 }]\n             return is_plain_url, formats\n \n@@ -2520,7 +2832,7 @@ def _media_formats(src, cur_media_type, type_info={}):\n         # so we wll include them right here (see\n         # https://www.ampproject.org/docs/reference/components/amp-video)\n         # For dl8-* tags see https://delight-vr.com/documentation/dl8-video/\n-        _MEDIA_TAG_NAME_RE = r'(?:(?:amp|dl8(?:-live)?)-)?(video|audio)'\n+        _MEDIA_TAG_NAME_RE = r'(?:(?:amp|dl8(?:-live)?)-)?(video(?:-js)?|audio)'\n         media_tags = [(media_tag, media_tag_name, media_type, '')\n                       for media_tag, media_tag_name, media_type\n                       in re.findall(r'(?s)(<(%s)[^>]*/>)' % _MEDIA_TAG_NAME_RE, webpage)]\n@@ -2538,7 +2850,8 @@ def _media_formats(src, cur_media_type, type_info={}):\n             media_attributes = extract_attributes(media_tag)\n             src = strip_or_none(media_attributes.get('src'))\n             if src:\n-                _, formats = _media_formats(src, media_type)\n+                f = parse_content_type(media_attributes.get('type'))\n+                _, formats = _media_formats(src, media_type, f)\n                 media_info['formats'].extend(formats)\n             media_info['thumbnail'] = absolute_url(media_attributes.get('poster'))\n             if media_content:\n@@ -2712,31 +3025,32 @@ def manifest_url(manifest):\n         return formats\n \n     def _find_jwplayer_data(self, webpage, video_id=None, transform_source=js_to_json):\n-        mobj = re.search(\n-            r'(?s)jwplayer\\((?P<quote>[\\'\"])[^\\'\" ]+(?P=quote)\\)(?!</script>).*?\\.setup\\s*\\((?P<options>[^)]+)\\)',\n-            webpage)\n-        if mobj:\n-            try:\n-                jwplayer_data = self._parse_json(mobj.group('options'),\n-                                                 video_id=video_id,\n-                                                 transform_source=transform_source)\n-            except ExtractorError:\n-                pass\n-            else:\n-                if isinstance(jwplayer_data, dict):\n-                    return jwplayer_data\n+        return self._search_json(\n+            r'''(?<!-)\\bjwplayer\\s*\\(\\s*(?P<q>'|\")(?!(?P=q)).+(?P=q)\\s*\\)(?:(?!</script>).)*?\\.\\s*(?:setup\\s*\\(|(?P<load>load)\\s*\\(\\s*\\[)''',\n+            webpage, 'JWPlayer data', video_id,\n+            # must be a {...} or sequence, ending\n+            contains_pattern=r'\\{[\\s\\S]*}(?(load)(?:\\s*,\\s*\\{[\\s\\S]*})*)', end_pattern=r'(?(load)\\]|\\))',\n+            transform_source=transform_source, default=None)\n \n     def _extract_jwplayer_data(self, webpage, video_id, *args, **kwargs):\n-        jwplayer_data = self._find_jwplayer_data(\n-            webpage, video_id, transform_source=js_to_json)\n-        return self._parse_jwplayer_data(\n-            jwplayer_data, video_id, *args, **kwargs)\n+        # allow passing `transform_source` through to _find_jwplayer_data()\n+        transform_source = kwargs.pop('transform_source', None)\n+        kwfind = compat_kwargs({'transform_source': transform_source}) if transform_source else {}\n+\n+        jwplayer_data = self._find_jwplayer_data(webpage, video_id, **kwfind)\n+\n+        return self._parse_jwplayer_data(jwplayer_data, video_id, *args, **kwargs)\n \n     def _parse_jwplayer_data(self, jwplayer_data, video_id=None, require_title=True,\n                              m3u8_id=None, mpd_id=None, rtmp_params=None, base_url=None):\n+        flat_pl = try_get(jwplayer_data, lambda x: x.get('playlist') or True)\n+        if flat_pl is None:\n+            # not even a dict\n+            return []\n+\n         # JWPlayer backward compatibility: flattened playlists\n         # https://github.com/jwplayer/jwplayer/blob/v7.4.3/src/js/api/config.js#L81-L96\n-        if 'playlist' not in jwplayer_data:\n+        if flat_pl is True:\n             jwplayer_data = {'playlist': [jwplayer_data]}\n \n         entries = []\n@@ -2759,22 +3073,14 @@ def _parse_jwplayer_data(self, jwplayer_data, video_id=None, require_title=True,\n                 mpd_id=mpd_id, rtmp_params=rtmp_params, base_url=base_url)\n \n             subtitles = {}\n-            tracks = video_data.get('tracks')\n-            if tracks and isinstance(tracks, list):\n-                for track in tracks:\n-                    if not isinstance(track, dict):\n-                        continue\n-                    track_kind = track.get('kind')\n-                    if not track_kind or not isinstance(track_kind, compat_str):\n-                        continue\n-                    if track_kind.lower() not in ('captions', 'subtitles'):\n-                        continue\n-                    track_url = urljoin(base_url, track.get('file'))\n-                    if not track_url:\n-                        continue\n-                    subtitles.setdefault(track.get('label') or 'en', []).append({\n-                        'url': self._proto_relative_url(track_url)\n-                    })\n+            for track in traverse_obj(video_data, (\n+                    'tracks', lambda _, t: t.get('kind').lower() in ('captions', 'subtitles'))):\n+                track_url = urljoin(base_url, track.get('file'))\n+                if not track_url:\n+                    continue\n+                subtitles.setdefault(track.get('label') or 'en', []).append({\n+                    'url': self._proto_relative_url(track_url)\n+                })\n \n             entry = {\n                 'id': this_video_id,\n@@ -2784,6 +3090,13 @@ def _parse_jwplayer_data(self, jwplayer_data, video_id=None, require_title=True,\n                 'timestamp': int_or_none(video_data.get('pubdate')),\n                 'duration': float_or_none(jwplayer_data.get('duration') or video_data.get('duration')),\n                 'subtitles': subtitles,\n+                'alt_title': clean_html(video_data.get('subtitle')),  # attributes used e.g. by Tele5 ...\n+                'genre': clean_html(video_data.get('genre')),\n+                'channel': clean_html(dict_get(video_data, ('category', 'channel'))),\n+                'season_number': int_or_none(video_data.get('season')),\n+                'episode_number': int_or_none(video_data.get('episode')),\n+                'release_year': int_or_none(video_data.get('releasedate')),\n+                'age_limit': int_or_none(video_data.get('age_restriction')),\n             }\n             # https://github.com/jwplayer/jwplayer/blob/master/src/js/utils/validator.js#L32\n             if len(formats) == 1 and re.search(r'^(?:http|//).*(?:youtube\\.com|youtu\\.be)/.+', formats[0]['url']):\n@@ -2792,7 +3105,9 @@ def _parse_jwplayer_data(self, jwplayer_data, video_id=None, require_title=True,\n                     'url': formats[0]['url'],\n                 })\n             else:\n-                self._sort_formats(formats)\n+                # avoid exception in case of only sttls\n+                if formats:\n+                    self._sort_formats(formats)\n                 entry['formats'] = formats\n             entries.append(entry)\n         if len(entries) == 1:\n@@ -2802,7 +3117,7 @@ def _parse_jwplayer_data(self, jwplayer_data, video_id=None, require_title=True,\n \n     def _parse_jwplayer_formats(self, jwplayer_sources_data, video_id=None,\n                                 m3u8_id=None, mpd_id=None, rtmp_params=None, base_url=None):\n-        urls = []\n+        urls = set()\n         formats = []\n         for source in jwplayer_sources_data:\n             if not isinstance(source, dict):\n@@ -2811,14 +3126,14 @@ def _parse_jwplayer_formats(self, jwplayer_sources_data, video_id=None,\n                 base_url, self._proto_relative_url(source.get('file')))\n             if not source_url or source_url in urls:\n                 continue\n-            urls.append(source_url)\n+            urls.add(source_url)\n             source_type = source.get('type') or ''\n             ext = mimetype2ext(source_type) or determine_ext(source_url)\n-            if source_type == 'hls' or ext == 'm3u8':\n+            if source_type == 'hls' or ext == 'm3u8' or 'format=m3u8-aapl' in source_url:\n                 formats.extend(self._extract_m3u8_formats(\n                     source_url, video_id, 'mp4', entry_protocol='m3u8_native',\n                     m3u8_id=m3u8_id, fatal=False))\n-            elif source_type == 'dash' or ext == 'mpd':\n+            elif source_type == 'dash' or ext == 'mpd' or 'format=mpd-time-csf' in source_url:\n                 formats.extend(self._extract_mpd_formats(\n                     source_url, video_id, mpd_id=mpd_id, fatal=False))\n             elif ext == 'smil':\n@@ -2833,20 +3148,23 @@ def _parse_jwplayer_formats(self, jwplayer_sources_data, video_id=None,\n                     'ext': ext,\n                 })\n             else:\n+                format_id = str_or_none(source.get('label'))\n                 height = int_or_none(source.get('height'))\n-                if height is None:\n+                if height is None and format_id:\n                     # Often no height is provided but there is a label in\n                     # format like \"1080p\", \"720p SD\", or 1080.\n-                    height = int_or_none(self._search_regex(\n-                        r'^(\\d{3,4})[pP]?(?:\\b|$)', compat_str(source.get('label') or ''),\n-                        'height', default=None))\n+                    height = parse_resolution(format_id).get('height')\n                 a_format = {\n                     'url': source_url,\n                     'width': int_or_none(source.get('width')),\n                     'height': height,\n-                    'tbr': int_or_none(source.get('bitrate')),\n+                    'tbr': int_or_none(source.get('bitrate'), scale=1000),\n+                    'filesize': int_or_none(source.get('filesize')),\n                     'ext': ext,\n                 }\n+                if format_id:\n+                    a_format['format_id'] = format_id\n+\n                 if source_url.startswith('rtmp'):\n                     a_format['ext'] = 'flv'\n                     # See com/longtailvideo/jwplayer/media/RTMPMediaProvider.as\n@@ -2981,12 +3299,16 @@ def _merge_subtitle_items(subtitle_list1, subtitle_list2):\n         return ret\n \n     @classmethod\n-    def _merge_subtitles(cls, subtitle_dict1, subtitle_dict2):\n-        \"\"\" Merge two subtitle dictionaries, language by language. \"\"\"\n-        ret = dict(subtitle_dict1)\n-        for lang in subtitle_dict2:\n-            ret[lang] = cls._merge_subtitle_items(subtitle_dict1.get(lang, []), subtitle_dict2[lang])\n-        return ret\n+    def _merge_subtitles(cls, subtitle_dict1, *subtitle_dicts, **kwargs):\n+        \"\"\" Merge subtitle dictionaries, language by language. \"\"\"\n+\n+        # ..., * , target=None\n+        target = kwargs.get('target') or dict(subtitle_dict1)\n+\n+        for subtitle_dict in subtitle_dicts:\n+            for lang in subtitle_dict:\n+                target[lang] = cls._merge_subtitle_items(target.get(lang, []), subtitle_dict[lang])\n+        return target\n \n     def extract_automatic_captions(self, *args, **kwargs):\n         if (self._downloader.params.get('writeautomaticsub', False)\n@@ -3019,6 +3341,29 @@ def _generic_id(self, url):\n     def _generic_title(self, url):\n         return compat_urllib_parse_unquote(os.path.splitext(url_basename(url))[0])\n \n+    def _yes_playlist(self, playlist_id, video_id, *args, **kwargs):\n+        # smuggled_data=None, *, playlist_label='playlist', video_label='video'\n+        smuggled_data = args[0] if len(args) == 1 else kwargs.get('smuggled_data')\n+        playlist_label = kwargs.get('playlist_label', 'playlist')\n+        video_label = kwargs.get('video_label', 'video')\n+\n+        if not playlist_id or not video_id:\n+            return not video_id\n+\n+        no_playlist = (smuggled_data or {}).get('force_noplaylist')\n+        if no_playlist is not None:\n+            return not no_playlist\n+\n+        video_id = '' if video_id is True else ' ' + video_id\n+        noplaylist = self.get_param('noplaylist')\n+        self.to_screen(\n+            'Downloading just the {0}{1} because of --no-playlist'.format(video_label, video_id)\n+            if noplaylist else\n+            'Downloading {0}{1} - add --no-playlist to download just the {2}{3}'.format(\n+                playlist_label, '' if playlist_id is True else ' ' + playlist_id,\n+                video_label, video_id))\n+        return not noplaylist\n+\n \n class SearchInfoExtractor(InfoExtractor):\n     \"\"\"\ndiff --git a/youtube_dl/extractor/cpac.py b/youtube_dl/extractor/cpac.py\nnew file mode 100644\nindex 00000000000..22741152c64\n--- /dev/null\n+++ b/youtube_dl/extractor/cpac.py\n@@ -0,0 +1,148 @@\n+# coding: utf-8\n+from __future__ import unicode_literals\n+\n+from .common import InfoExtractor\n+from ..compat import compat_str\n+from ..utils import (\n+    int_or_none,\n+    str_or_none,\n+    try_get,\n+    unified_timestamp,\n+    update_url_query,\n+    urljoin,\n+)\n+\n+# compat_range\n+try:\n+    if callable(xrange):\n+        range = xrange\n+except (NameError, TypeError):\n+    pass\n+\n+\n+class CPACIE(InfoExtractor):\n+    IE_NAME = 'cpac'\n+    _VALID_URL = r'https?://(?:www\\.)?cpac\\.ca/(?P<fr>l-)?episode\\?id=(?P<id>[\\da-f]{8}(?:-[\\da-f]{4}){3}-[\\da-f]{12})'\n+    _TEST = {\n+        # 'url': 'http://www.cpac.ca/en/programs/primetime-politics/episodes/65490909',\n+        'url': 'https://www.cpac.ca/episode?id=fc7edcae-4660-47e1-ba61-5b7f29a9db0f',\n+        'md5': 'e46ad699caafd7aa6024279f2614e8fa',\n+        'info_dict': {\n+            'id': 'fc7edcae-4660-47e1-ba61-5b7f29a9db0f',\n+            'ext': 'mp4',\n+            'upload_date': '20220215',\n+            'title': 'News Conference to Celebrate National Kindness Week \u2013 February 15, 2022',\n+            'description': 'md5:466a206abd21f3a6f776cdef290c23fb',\n+            'timestamp': 1644901200,\n+        },\n+        'params': {\n+            'format': 'bestvideo',\n+            'hls_prefer_native': True,\n+        },\n+    }\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+        url_lang = 'fr' if '/l-episode?' in url else 'en'\n+\n+        content = self._download_json(\n+            'https://www.cpac.ca/api/1/services/contentModel.json?url=/site/website/episode/index.xml&crafterSite=cpacca&id=' + video_id,\n+            video_id)\n+        video_url = try_get(content, lambda x: x['page']['details']['videoUrl'], compat_str)\n+        formats = []\n+        if video_url:\n+            content = content['page']\n+            title = str_or_none(content['details']['title_%s_t' % (url_lang, )])\n+            formats = self._extract_m3u8_formats(video_url, video_id, m3u8_id='hls', ext='mp4')\n+            for fmt in formats:\n+                # prefer language to match URL\n+                fmt_lang = fmt.get('language')\n+                if fmt_lang == url_lang:\n+                    fmt['language_preference'] = 10\n+                elif not fmt_lang:\n+                    fmt['language_preference'] = -1\n+                else:\n+                    fmt['language_preference'] = -10\n+\n+        self._sort_formats(formats)\n+\n+        category = str_or_none(content['details']['category_%s_t' % (url_lang, )])\n+\n+        def is_live(v_type):\n+            return (v_type == 'live') if v_type is not None else None\n+\n+        return {\n+            'id': video_id,\n+            'formats': formats,\n+            'title': title,\n+            'description': str_or_none(content['details'].get('description_%s_t' % (url_lang, ))),\n+            'timestamp': unified_timestamp(content['details'].get('liveDateTime')),\n+            'category': [category] if category else None,\n+            'thumbnail': urljoin(url, str_or_none(content['details'].get('image_%s_s' % (url_lang, )))),\n+            'is_live': is_live(content['details'].get('type')),\n+        }\n+\n+\n+class CPACPlaylistIE(InfoExtractor):\n+    IE_NAME = 'cpac:playlist'\n+    _VALID_URL = r'(?i)https?://(?:www\\.)?cpac\\.ca/(?:program|search|(?P<fr>emission|rechercher))\\?(?:[^&]+&)*?(?P<id>(?:id=\\d+|programId=\\d+|key=[^&]+))'\n+\n+    _TESTS = [{\n+        'url': 'https://www.cpac.ca/program?id=6',\n+        'info_dict': {\n+            'id': 'id=6',\n+            'title': 'Headline Politics',\n+            'description': 'Watch CPAC\u2019s signature long-form coverage of the day\u2019s pressing political events as they unfold.',\n+        },\n+        'playlist_count': 10,\n+    }, {\n+        'url': 'https://www.cpac.ca/search?key=hudson&type=all&order=desc',\n+        'info_dict': {\n+            'id': 'key=hudson',\n+            'title': 'hudson',\n+        },\n+        'playlist_count': 22,\n+    }, {\n+        'url': 'https://www.cpac.ca/search?programId=50',\n+        'info_dict': {\n+            'id': 'programId=50',\n+            'title': '50',\n+        },\n+        'playlist_count': 9,\n+    }, {\n+        'url': 'https://www.cpac.ca/emission?id=6',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.cpac.ca/rechercher?key=hudson&type=all&order=desc',\n+        'only_matching': True,\n+    }]\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+        url_lang = 'fr' if any(x in url for x in ('/emission?', '/rechercher?')) else 'en'\n+        pl_type, list_type = ('program', 'itemList') if any(x in url for x in ('/program?', '/emission?')) else ('search', 'searchResult')\n+        api_url = (\n+            'https://www.cpac.ca/api/1/services/contentModel.json?url=/site/website/%s/index.xml&crafterSite=cpacca&%s'\n+            % (pl_type, video_id, ))\n+        content = self._download_json(api_url, video_id)\n+        entries = []\n+        total_pages = int_or_none(try_get(content, lambda x: x['page'][list_type]['totalPages']), default=1)\n+        for page in range(1, total_pages + 1):\n+            if page > 1:\n+                api_url = update_url_query(api_url, {'page': '%d' % (page, ), })\n+                content = self._download_json(\n+                    api_url, video_id,\n+                    note='Downloading continuation - %d' % (page, ),\n+                    fatal=False)\n+\n+            for item in try_get(content, lambda x: x['page'][list_type]['item'], list) or []:\n+                episode_url = urljoin(url, try_get(item, lambda x: x['url_%s_s' % (url_lang, )]))\n+                if episode_url:\n+                    entries.append(episode_url)\n+\n+        return self.playlist_result(\n+            (self.url_result(entry) for entry in entries),\n+            playlist_id=video_id,\n+            playlist_title=try_get(content, lambda x: x['page']['program']['title_%s_t' % (url_lang, )]) or video_id.split('=')[-1],\n+            playlist_description=try_get(content, lambda x: x['page']['program']['description_%s_t' % (url_lang, )]),\n+        )\ndiff --git a/youtube_dl/extractor/dlf.py b/youtube_dl/extractor/dlf.py\nnew file mode 100644\nindex 00000000000..cc3de45826d\n--- /dev/null\n+++ b/youtube_dl/extractor/dlf.py\n@@ -0,0 +1,204 @@\n+# coding: utf-8\n+from __future__ import unicode_literals\n+\n+import re\n+\n+from .common import InfoExtractor\n+from ..compat import (\n+    compat_str,\n+)\n+from ..utils import (\n+    determine_ext,\n+    extract_attributes,\n+    int_or_none,\n+    merge_dicts,\n+    traverse_obj,\n+    url_or_none,\n+    variadic,\n+)\n+\n+\n+class DLFBaseIE(InfoExtractor):\n+    _VALID_URL_BASE = r'https?://(?:www\\.)?deutschlandfunk\\.de/'\n+    _BUTTON_REGEX = r'(<button[^>]+alt=\"Anh\u00f6ren\"[^>]+data-audio-diraid[^>]*>)'\n+\n+    def _parse_button_attrs(self, button, audio_id=None):\n+        attrs = extract_attributes(button)\n+        audio_id = audio_id or attrs['data-audio-diraid']\n+\n+        url = traverse_obj(\n+            attrs, 'data-audio-download-src', 'data-audio', 'data-audioreference',\n+            'data-audio-src', expected_type=url_or_none)\n+        ext = determine_ext(url)\n+        formats = (self._extract_m3u8_formats(url, audio_id, fatal=False)\n+                   if ext == 'm3u8' else [{'url': url, 'ext': ext, 'vcodec': 'none'}])\n+        self._sort_formats(formats)\n+\n+        def traverse_attrs(path):\n+            path = list(variadic(path))\n+            t = path.pop() if callable(path[-1]) else None\n+            return traverse_obj(attrs, path, expected_type=t, get_all=False)\n+\n+        def txt_or_none(v, default=None):\n+            return default if v is None else (compat_str(v).strip() or default)\n+\n+        return merge_dicts(*reversed([{\n+            'id': audio_id,\n+            # 'extractor_key': DLFIE.ie_key(),\n+            # 'extractor': DLFIE.IE_NAME,\n+            'formats': formats,\n+        }, dict((k, traverse_attrs(v)) for k, v in {\n+            'title': (('data-audiotitle', 'data-audio-title', 'data-audio-download-tracking-title'), txt_or_none),\n+            'duration': (('data-audioduration', 'data-audio-duration'), int_or_none),\n+            'thumbnail': ('data-audioimage', url_or_none),\n+            'uploader': 'data-audio-producer',\n+            'series': 'data-audio-series',\n+            'channel': 'data-audio-origin-site-name',\n+            'webpage_url': ('data-audio-download-tracking-path', url_or_none),\n+        }.items())]))\n+\n+\n+class DLFIE(DLFBaseIE):\n+    IE_NAME = 'dlf'\n+    _VALID_URL = DLFBaseIE._VALID_URL_BASE + r'[\\w-]+-dlf-(?P<id>[\\da-f]{8})-100\\.html'\n+    _TESTS = [\n+        # Audio as an HLS stream\n+        {\n+            'url': 'https://www.deutschlandfunk.de/tanz-der-saiteninstrumente-das-wild-strings-trio-aus-slowenien-dlf-03a3eb19-100.html',\n+            'info_dict': {\n+                'id': '03a3eb19',\n+                'title': r're:Tanz der Saiteninstrumente [-/] Das Wild Strings Trio aus Slowenien',\n+                'ext': 'm4a',\n+                'duration': 3298,\n+                'thumbnail': 'https://assets.deutschlandfunk.de/FALLBACK-IMAGE-AUDIO/512x512.png?t=1603714364673',\n+                'uploader': 'Deutschlandfunk',\n+                'series': 'On Stage',\n+                'channel': 'deutschlandfunk'\n+            },\n+            'params': {\n+                'skip_download': 'm3u8'\n+            },\n+            'skip': 'This webpage no longer exists'\n+        }, {\n+            'url': 'https://www.deutschlandfunk.de/russische-athleten-kehren-zurueck-auf-die-sportbuehne-ein-gefaehrlicher-tueroeffner-dlf-d9cc1856-100.html',\n+            'info_dict': {\n+                'id': 'd9cc1856',\n+                'title': 'Russische Athleten kehren zur\u00fcck auf die Sportb\u00fchne: Ein gef\u00e4hrlicher T\u00fcr\u00f6ffner',\n+                'ext': 'mp3',\n+                'duration': 291,\n+                'thumbnail': 'https://assets.deutschlandfunk.de/FALLBACK-IMAGE-AUDIO/512x512.png?t=1603714364673',\n+                'uploader': 'Deutschlandfunk',\n+                'series': 'Kommentare und Themen der Woche',\n+                'channel': 'deutschlandfunk'\n+            }\n+        },\n+    ]\n+\n+    def _real_extract(self, url):\n+        audio_id = self._match_id(url)\n+        webpage = self._download_webpage(url, audio_id)\n+\n+        return self._parse_button_attrs(\n+            self._search_regex(self._BUTTON_REGEX, webpage, 'button'), audio_id)\n+\n+\n+class DLFCorpusIE(DLFBaseIE):\n+    IE_NAME = 'dlf:corpus'\n+    IE_DESC = 'DLF Multi-feed Archives'\n+    _VALID_URL = DLFBaseIE._VALID_URL_BASE + r'(?P<id>(?![\\w-]+-dlf-[\\da-f]{8})[\\w-]+-\\d+)\\.html'\n+    _TESTS = [\n+        # Recorded news broadcast with referrals to related broadcasts\n+        {\n+            'url': 'https://www.deutschlandfunk.de/fechten-russland-belarus-ukraine-protest-100.html',\n+            'info_dict': {\n+                'id': 'fechten-russland-belarus-ukraine-protest-100',\n+                'title': r're:Wiederzulassung als neutrale Athleten [-/] Was die R\u00fcckkehr russischer und belarussischer Sportler beim Fechten bedeutet',\n+                'description': 'md5:91340aab29c71aa7518ad5be13d1e8ad'\n+            },\n+            'playlist_mincount': 5,\n+            'playlist': [{\n+                'info_dict': {\n+                    'id': '1fc5d64a',\n+                    'title': r're:Wiederzulassung als neutrale Athleten [-/] Was die R\u00fcckkehr russischer und belarussischer Sportler beim Fechten bedeutet',\n+                    'ext': 'mp3',\n+                    'duration': 252,\n+                    'thumbnail': 'https://assets.deutschlandfunk.de/aad16241-6b76-4a09-958b-96d0ee1d6f57/512x512.jpg?t=1679480020313',\n+                    'uploader': 'Deutschlandfunk',\n+                    'series': 'Sport',\n+                    'channel': 'deutschlandfunk'\n+                }\n+            }, {\n+                'info_dict': {\n+                    'id': '2ada145f',\n+                    'title': r're:(?:Sportpolitik / )?Fechtverband votiert f\u00fcr R\u00fcckkehr russischer Athleten',\n+                    'ext': 'mp3',\n+                    'duration': 336,\n+                    'thumbnail': 'https://assets.deutschlandfunk.de/FILE_93982766f7317df30409b8a184ac044a/512x512.jpg?t=1678547581005',\n+                    'uploader': 'Deutschlandfunk',\n+                    'series': 'Deutschlandfunk Nova',\n+                    'channel': 'deutschlandfunk-nova'\n+                }\n+            }, {\n+                'info_dict': {\n+                    'id': '5e55e8c9',\n+                    'title': r're:Wiederzulassung von Russland und Belarus [-/] \"Herumlavieren\" des Fechter-Bundes sorgt f\u00fcr Unverst\u00e4ndnis',\n+                    'ext': 'mp3',\n+                    'duration': 187,\n+                    'thumbnail': 'https://assets.deutschlandfunk.de/a595989d-1ed1-4a2e-8370-b64d7f11d757/512x512.jpg?t=1679173825412',\n+                    'uploader': 'Deutschlandfunk',\n+                    'series': 'Sport am Samstag',\n+                    'channel': 'deutschlandfunk'\n+                }\n+            }, {\n+                'info_dict': {\n+                    'id': '47e1a096',\n+                    'title': r're:R\u00fcckkehr Russlands im Fechten [-/] \"Fassungslos, dass es einfach so passiert ist\"',\n+                    'ext': 'mp3',\n+                    'duration': 602,\n+                    'thumbnail': 'https://assets.deutschlandfunk.de/da4c494a-21cc-48b4-9cc7-40e09fd442c2/512x512.jpg?t=1678562155770',\n+                    'uploader': 'Deutschlandfunk',\n+                    'series': 'Sport am Samstag',\n+                    'channel': 'deutschlandfunk'\n+                }\n+            }, {\n+                'info_dict': {\n+                    'id': '5e55e8c9',\n+                    'title': r're:Wiederzulassung von Russland und Belarus [-/] \"Herumlavieren\" des Fechter-Bundes sorgt f\u00fcr Unverst\u00e4ndnis',\n+                    'ext': 'mp3',\n+                    'duration': 187,\n+                    'thumbnail': 'https://assets.deutschlandfunk.de/a595989d-1ed1-4a2e-8370-b64d7f11d757/512x512.jpg?t=1679173825412',\n+                    'uploader': 'Deutschlandfunk',\n+                    'series': 'Sport am Samstag',\n+                    'channel': 'deutschlandfunk'\n+                }\n+            }]\n+        },\n+        # Podcast feed with tag buttons, playlist count fluctuates\n+        {\n+            'url': 'https://www.deutschlandfunk.de/kommentare-und-themen-der-woche-100.html',\n+            'info_dict': {\n+                'id': 'kommentare-und-themen-der-woche-100',\n+                'title': 'Meinung - Kommentare und Themen der Woche',\n+                'description': 'md5:2901bbd65cd2d45e116d399a099ce5d5',\n+            },\n+            'playlist_mincount': 10,\n+        },\n+        # Podcast feed with no description\n+        {\n+            'url': 'https://www.deutschlandfunk.de/podcast-tolle-idee-100.html',\n+            'info_dict': {\n+                'id': 'podcast-tolle-idee-100',\n+                'title': 'Wissenschaftspodcast - Tolle Idee! - Was wurde daraus?',\n+            },\n+            'playlist_mincount': 11,\n+        },\n+    ]\n+\n+    def _real_extract(self, url):\n+        playlist_id = self._match_id(url)\n+        webpage = self._download_webpage(url, playlist_id)\n+\n+        return self.playlist_result(\n+            map(self._parse_button_attrs, re.findall(self._BUTTON_REGEX, webpage)),\n+            playlist_id, self._html_search_meta(['og:title', 'twitter:title'], webpage, default=None),\n+            self._html_search_meta(['description', 'og:description', 'twitter:description'], webpage, default=None))\ndiff --git a/youtube_dl/extractor/epidemicsound.py b/youtube_dl/extractor/epidemicsound.py\nnew file mode 100644\nindex 00000000000..1a52738aa6e\n--- /dev/null\n+++ b/youtube_dl/extractor/epidemicsound.py\n@@ -0,0 +1,101 @@\n+# coding: utf-8\n+from __future__ import unicode_literals\n+\n+from .common import InfoExtractor\n+from ..utils import (\n+    float_or_none,\n+    T,\n+    traverse_obj,\n+    txt_or_none,\n+    unified_timestamp,\n+    url_or_none,\n+)\n+\n+\n+class EpidemicSoundIE(InfoExtractor):\n+    _VALID_URL = r'https?://(?:www\\.)?epidemicsound\\.com/track/(?P<id>[0-9a-zA-Z]+)'\n+    _TESTS = [{\n+        'url': 'https://www.epidemicsound.com/track/yFfQVRpSPz/',\n+        'md5': 'd98ff2ddb49e8acab9716541cbc9dfac',\n+        'info_dict': {\n+            'id': '45014',\n+            'display_id': 'yFfQVRpSPz',\n+            'ext': 'mp3',\n+            'tags': ['foley', 'door', 'knock', 'glass', 'window', 'glass door knock'],\n+            'title': 'Door Knock Door 1',\n+            'duration': 1,\n+            'thumbnail': 'https://cdn.epidemicsound.com/curation-assets/commercial-release-cover-images/default-sfx/3000x3000.jpg',\n+            'timestamp': 1415320353,\n+            'upload_date': '20141107',\n+            'age_limit': None,\n+            # check that the \"best\" format was found, since test file MD5 doesn't\n+            # distinguish the formats\n+            'format': 'full',\n+        },\n+    }, {\n+        'url': 'https://www.epidemicsound.com/track/mj8GTTwsZd/',\n+        'md5': 'c82b745890f9baf18dc2f8d568ee3830',\n+        'info_dict': {\n+            'id': '148700',\n+            'display_id': 'mj8GTTwsZd',\n+            'ext': 'mp3',\n+            'tags': ['liquid drum n bass', 'energetic'],\n+            'title': 'Noplace',\n+            'duration': 237,\n+            'thumbnail': 'https://cdn.epidemicsound.com/curation-assets/commercial-release-cover-images/11138/3000x3000.jpg',\n+            'timestamp': 1694426482,\n+            'release_timestamp': 1700535606,\n+            'upload_date': '20230911',\n+            'age_limit': None,\n+            'format': 'full',\n+        },\n+    }]\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+        json_data = self._download_json('https://www.epidemicsound.com/json/track/' + video_id, video_id)\n+\n+        def fmt_or_none(f):\n+            if not f.get('format'):\n+                f['format'] = f.get('format_id')\n+            elif not f.get('format_id'):\n+                f['format_id'] = f['format']\n+            if not (f['url'] and f['format']):\n+                return\n+            if f.get('format_note'):\n+                f['format_note'] = 'track ID ' + f['format_note']\n+            f['preference'] = -1 if f['format'] == 'full' else -2\n+            return f\n+\n+        formats = traverse_obj(json_data, (\n+            'stems', T(dict.items), Ellipsis, {\n+                'format': (0, T(txt_or_none)),\n+                'format_note': (1, 's3TrackId', T(txt_or_none)),\n+                'format_id': (1, 'stemType', T(txt_or_none)),\n+                'url': (1, 'lqMp3Url', T(url_or_none)),\n+            }, T(fmt_or_none)))\n+\n+        self._sort_formats(formats)\n+\n+        info = traverse_obj(json_data, {\n+            'id': ('id', T(txt_or_none)),\n+            'tags': ('metadataTags', Ellipsis, T(txt_or_none)),\n+            'title': ('title', T(txt_or_none)),\n+            'duration': ('length', T(float_or_none)),\n+            'timestamp': ('added', T(unified_timestamp)),\n+            'thumbnail': (('imageUrl', 'cover'), T(url_or_none)),\n+            'age_limit': ('isExplicit', T(lambda b: 18 if b else None)),\n+            'release_timestamp': ('releaseDate', T(unified_timestamp)),\n+        }, get_all=False)\n+\n+        info.update(traverse_obj(json_data, {\n+            'categories': ('genres', Ellipsis, 'tag', T(txt_or_none)),\n+            'tags': ('metadataTags', Ellipsis, T(txt_or_none)),\n+        }))\n+\n+        info.update({\n+            'display_id': video_id,\n+            'formats': formats,\n+        })\n+\n+        return info\ndiff --git a/youtube_dl/extractor/extractors.py b/youtube_dl/extractor/extractors.py\nindex 50b7cb4a04f..3da5f802093 100644\n--- a/youtube_dl/extractor/extractors.py\n+++ b/youtube_dl/extractor/extractors.py\n@@ -51,6 +51,10 @@\n from .aol import AolIE\n from .allocine import AllocineIE\n from .aliexpress import AliExpressLiveIE\n+from .alsace20tv import (\n+    Alsace20TVIE,\n+    Alsace20TVEmbedIE,\n+)\n from .apa import APAIE\n from .aparat import AparatIE\n from .appleconnect import AppleConnectIE\n@@ -115,6 +119,7 @@\n )\n from .bibeltv import BibelTVIE\n from .bigflix import BigflixIE\n+from .bigo import BigoIE\n from .bild import BildIE\n from .bilibili import (\n     BiliBiliIE,\n@@ -133,6 +138,7 @@\n     BleacherReportIE,\n     BleacherReportCMSIE,\n )\n+from .blerp import BlerpIE\n from .bloomberg import BloombergIE\n from .bokecc import BokeCCIE\n from .bongacams import BongaCamsIE\n@@ -153,6 +159,8 @@\n from .buzzfeed import BuzzFeedIE\n from .byutv import BYUtvIE\n from .c56 import C56IE\n+from .caffeine import CaffeineTVIE\n+from .callin import CallinIE\n from .camdemy import (\n     CamdemyIE,\n     CamdemyFolderIE\n@@ -203,10 +211,7 @@\n from .ccma import CCMAIE\n from .cctv import CCTVIE\n from .cda import CDAIE\n-from .ceskatelevize import (\n-    CeskaTelevizeIE,\n-    CeskaTelevizePoradyIE,\n-)\n+from .ceskatelevize import CeskaTelevizeIE\n from .channel9 import Channel9IE\n from .charlierose import CharlieRoseIE\n from .chaturbate import ChaturbateIE\n@@ -222,6 +227,7 @@\n     CiscoLiveSearchIE,\n )\n from .cjsw import CJSWIE\n+from .clipchamp import ClipchampIE\n from .cliphunter import CliphunterIE\n from .clippit import ClippitIE\n from .cliprs import ClipRsIE\n@@ -254,6 +260,10 @@\n from .condenast import CondeNastIE\n from .contv import CONtvIE\n from .corus import CorusIE\n+from .cpac import (\n+    CPACIE,\n+    CPACPlaylistIE,\n+)\n from .cracked import CrackedIE\n from .crackle import CrackleIE\n from .crooksandliars import CrooksAndLiarsIE\n@@ -287,6 +297,10 @@\n from .dctp import DctpTvIE\n from .deezer import DeezerPlaylistIE\n from .democracynow import DemocracynowIE\n+from .dlf import (\n+    DLFCorpusIE,\n+    DLFIE,\n+)\n from .dfb import DFBIE\n from .dhm import DHMIE\n from .digg import DiggIE\n@@ -344,6 +358,7 @@\n from .elpais import ElPaisIE\n from .embedly import EmbedlyIE\n from .engadget import EngadgetIE\n+from .epidemicsound import EpidemicSoundIE\n from .eporner import EpornerIE\n from .eroprofile import EroProfileIE\n from .escapist import EscapistIE\n@@ -368,6 +383,7 @@\n     FC2EmbedIE,\n )\n from .fczenit import FczenitIE\n+from .fifa import FifaIE\n from .filmon import (\n     FilmOnIE,\n     FilmOnChannelIE,\n@@ -427,6 +443,7 @@\n from .gamestar import GameStarIE\n from .gaskrank import GaskrankIE\n from .gazeta import GazetaIE\n+from .gbnews import GBNewsIE\n from .gdcvault import GDCVaultIE\n from .gedidigital import GediDigitalIE\n from .generic import GenericIE\n@@ -434,6 +451,13 @@\n from .giantbomb import GiantBombIE\n from .giga import GigaIE\n from .glide import GlideIE\n+from .globalplayer import (\n+    GlobalPlayerLiveIE,\n+    GlobalPlayerLivePlaylistIE,\n+    GlobalPlayerAudioIE,\n+    GlobalPlayerAudioEpisodeIE,\n+    GlobalPlayerVideoIE\n+)\n from .globo import (\n     GloboIE,\n     GloboArticleIE,\n@@ -470,6 +494,7 @@\n )\n from .howcast import HowcastIE\n from .howstuffworks import HowStuffWorksIE\n+from .hrfernsehen import HRFernsehenIE\n from .hrti import (\n     HRTiIE,\n     HRTiPlaylistIE,\n@@ -546,8 +571,10 @@\n from .kickstarter import KickStarterIE\n from .kinja import KinjaEmbedIE\n from .kinopoisk import KinoPoiskIE\n+from .kommunetv import KommunetvIE\n from .konserthusetplay import KonserthusetPlayIE\n from .krasview import KrasViewIE\n+from .kth import KTHIE\n from .ku6 import Ku6IE\n from .kusi import KUSIIE\n from .kuwo import (\n@@ -717,6 +744,7 @@\n     MyviIE,\n     MyviEmbedIE,\n )\n+from .myvideoge import MyVideoGeIE\n from .myvidster import MyVidsterIE\n from .nationalgeographic import (\n     NationalGeographicVideoIE,\n@@ -870,21 +898,13 @@\n )\n from .ora import OraTVIE\n from .orf import (\n-    ORFTVthekIE,\n-    ORFFM4IE,\n+    ORFONIE,\n+    ORFONLiveIE,\n     ORFFM4StoryIE,\n-    ORFOE1IE,\n-    ORFOE3IE,\n-    ORFNOEIE,\n-    ORFWIEIE,\n-    ORFBGLIE,\n-    ORFOOEIE,\n-    ORFSTMIE,\n-    ORFKTNIE,\n-    ORFSBGIE,\n-    ORFTIRIE,\n-    ORFVBGIE,\n     ORFIPTVIE,\n+    ORFPodcastIE,\n+    ORFRadioIE,\n+    ORFRadioCollectionIE,\n )\n from .outsidetv import OutsideTVIE\n from .packtpub import (\n@@ -901,6 +921,10 @@\n from .patreon import PatreonIE\n from .pbs import PBSIE\n from .pearvideo import PearVideoIE\n+from .peekvids import (\n+    PeekVidsIE,\n+    PlayVidsIE,\n+)\n from .peertube import PeerTubeIE\n from .people import PeopleIE\n from .performgroup import PerformGroupIE\n@@ -957,6 +981,10 @@\n from .pornotube import PornotubeIE\n from .pornovoisines import PornoVoisinesIE\n from .pornoxo import PornoXOIE\n+from .pr0gramm import (\n+    Pr0grammIE,\n+    Pr0grammStaticIE,\n+)\n from .puhutv import (\n     PuhuTVIE,\n     PuhuTVSerieIE,\n@@ -994,6 +1022,10 @@\n     RayWenderlichIE,\n     RayWenderlichCourseIE,\n )\n+from .rbgtum import (\n+    RbgTumIE,\n+    RbgTumCourseIE,\n+)\n from .rbmaradio import RBMARadioIE\n from .rds import RDSIE\n from .redbulltv import (\n@@ -1049,6 +1081,10 @@\n from .rutv import RUTVIE\n from .ruutu import RuutuIE\n from .ruv import RuvIE\n+from .s4c import (\n+    S4CIE,\n+    S4CSeriesIE,\n+)\n from .safari import (\n     SafariIE,\n     SafariApiIE,\n@@ -1184,6 +1220,7 @@\n from .streamable import StreamableIE\n from .streamcloud import StreamcloudIE\n from .streamcz import StreamCZIE\n+from .streamsb import StreamsbIE\n from .streetvoice import StreetVoiceIE\n from .stretchinternet import StretchInternetIE\n from .stv import STVPlayerIE\n@@ -1253,6 +1290,11 @@\n from .thisamericanlife import ThisAmericanLifeIE\n from .thisav import ThisAVIE\n from .thisoldhouse import ThisOldHouseIE\n+from .thisvid import (\n+    ThisVidIE,\n+    ThisVidMemberIE,\n+    ThisVidPlaylistIE,\n+)\n from .threeqsdn import ThreeQSDNIE\n from .tiktok import (\n     TikTokIE,\n@@ -1537,6 +1579,7 @@\n     WeiboMobileIE\n )\n from .weiqitv import WeiqiTVIE\n+from .whyp import WhypIE\n from .wistia import (\n     WistiaIE,\n     WistiaPlaylistIE,\n@@ -1602,7 +1645,15 @@\n     YouNowChannelIE,\n     YouNowMomentIE,\n )\n-from .youporn import YouPornIE\n+from .youporn import (\n+    YouPornIE,\n+    YouPornCategoryIE,\n+    YouPornChannelIE,\n+    YouPornCollectionIE,\n+    YouPornStarIE,\n+    YouPornTagIE,\n+    YouPornVideosIE,\n+)\n from .yourporn import YourPornIE\n from .yourupload import YourUploadIE\n from .youtube import (\ndiff --git a/youtube_dl/extractor/fifa.py b/youtube_dl/extractor/fifa.py\nnew file mode 100644\nindex 00000000000..15157774ee4\n--- /dev/null\n+++ b/youtube_dl/extractor/fifa.py\n@@ -0,0 +1,101 @@\n+# coding: utf-8\n+from __future__ import unicode_literals\n+\n+from .common import InfoExtractor\n+\n+from ..utils import (\n+    int_or_none,\n+    traverse_obj,\n+    unified_timestamp,\n+)\n+\n+if not callable(getattr(InfoExtractor, '_match_valid_url', None)):\n+\n+    BaseInfoExtractor = InfoExtractor\n+\n+    import re\n+\n+    class InfoExtractor(BaseInfoExtractor):\n+\n+        @classmethod\n+        def _match_valid_url(cls, url):\n+            return re.match(cls._VALID_URL, url)\n+\n+\n+class FifaIE(InfoExtractor):\n+    _VALID_URL = r'https?://www.fifa.com/fifaplus/(?P<locale>\\w{2})/watch/([^#?]+/)?(?P<id>\\w+)'\n+    _TESTS = [{\n+        'url': 'https://www.fifa.com/fifaplus/en/watch/7on10qPcnyLajDDU3ntg6y',\n+        'info_dict': {\n+            'id': '7on10qPcnyLajDDU3ntg6y',\n+            'title': 'Italy v France | Final | 2006 FIFA World Cup Germany\u2122 | Full Match Replay',\n+            'description': 'md5:f4520d0ee80529c8ba4134a7d692ff8b',\n+            'ext': 'mp4',\n+            'categories': ['FIFA Tournaments'],\n+            'thumbnail': 'https://digitalhub.fifa.com/transform/135e2656-3a51-407b-8810-6c34bec5b59b/FMR_2006_Italy_France_Final_Hero',\n+            'duration': 8165,\n+        },\n+        'params': {'skip_download': 'm3u8'},\n+    }, {\n+        'url': 'https://www.fifa.com/fifaplus/pt/watch/1cg5r5Qt6Qt12ilkDgb1sV',\n+        'info_dict': {\n+            'id': '1cg5r5Qt6Qt12ilkDgb1sV',\n+            'title': 'Brazil v Germany | Semi-finals | 2014 FIFA World Cup Brazil\u2122 | Extended Highlights',\n+            'description': 'md5:d908c74ee66322b804ae2e521b02a855',\n+            'ext': 'mp4',\n+            'categories': ['FIFA Tournaments', 'Highlights'],\n+            'thumbnail': 'https://digitalhub.fifa.com/transform/d8fe6f61-276d-4a73-a7fe-6878a35fd082/FIFAPLS_100EXTHL_2014BRAvGER_TMB',\n+            'duration': 902,\n+            'release_timestamp': 1404777600,\n+            'release_date': '20140708',\n+        },\n+        'params': {'skip_download': 'm3u8'},\n+    }, {\n+        'url': 'https://www.fifa.com/fifaplus/fr/watch/3C6gQH9C2DLwzNx7BMRQdp',\n+        'info_dict': {\n+            'id': '3C6gQH9C2DLwzNx7BMRQdp',\n+            'title': 'Josimar goal against Northern Ireland | Classic Goals',\n+            'description': 'md5:cbe7e7bb52f603c9f1fe9a4780fe983b',\n+            'ext': 'mp4',\n+            'categories': ['FIFA Tournaments', 'Goal'],\n+            'duration': 28,\n+            'thumbnail': 'https://digitalhub.fifa.com/transform/f9301391-f8d9-48b5-823e-c093ac5e3e11/CG_MEN_1986_JOSIMAR',\n+        },\n+        'params': {'skip_download': 'm3u8'},\n+    }]\n+\n+    def _real_extract(self, url):\n+        video_id, locale = self._match_valid_url(url).group('id', 'locale')\n+        webpage = self._download_webpage(url, video_id)\n+\n+        preconnect_link = self._search_regex(\n+            r'<link\\b[^>]+\\brel\\s*=\\s*\"preconnect\"[^>]+href\\s*=\\s*\"([^\"]+)\"', webpage, 'Preconnect Link')\n+\n+        video_details = self._download_json(\n+            '{preconnect_link}/sections/videoDetails/{video_id}'.format(**locals()), video_id, 'Downloading Video Details', fatal=False)\n+\n+        preplay_parameters = self._download_json(\n+            '{preconnect_link}/videoPlayerData/{video_id}'.format(**locals()), video_id, 'Downloading Preplay Parameters')['preplayParameters']\n+\n+        content_data = self._download_json(\n+            # 1. query string is expected to be sent as-is\n+            # 2. `sig` must be appended\n+            # 3. if absent, the call appears to work but the manifest is bad (404)\n+            'https://content.uplynk.com/preplay/{contentId}/multiple.json?{queryStr}&sig={signature}'.format(**preplay_parameters),\n+            video_id, 'Downloading Content Data')\n+\n+        # formats, subtitles = self._extract_m3u8_formats_and_subtitles(content_data['playURL'], video_id)\n+        formats, subtitles = self._extract_m3u8_formats(content_data['playURL'], video_id, ext='mp4', entry_protocol='m3u8_native'), None\n+        self._sort_formats(formats)\n+\n+        return {\n+            'id': video_id,\n+            'title': video_details['title'],\n+            'description': video_details.get('description'),\n+            'duration': int_or_none(video_details.get('duration')),\n+            'release_timestamp': unified_timestamp(video_details.get('dateOfRelease')),\n+            'categories': traverse_obj(video_details, (('videoCategory', 'videoSubcategory'),)),\n+            'thumbnail': traverse_obj(video_details, ('backgroundImage', 'src')),\n+            'formats': formats,\n+            'subtitles': subtitles,\n+        }\ndiff --git a/youtube_dl/extractor/gbnews.py b/youtube_dl/extractor/gbnews.py\nnew file mode 100644\nindex 00000000000..f04f30e5afb\n--- /dev/null\n+++ b/youtube_dl/extractor/gbnews.py\n@@ -0,0 +1,139 @@\n+# coding: utf-8\n+from __future__ import unicode_literals\n+\n+from .common import InfoExtractor\n+from ..utils import (\n+    extract_attributes,\n+    ExtractorError,\n+    T,\n+    traverse_obj,\n+    txt_or_none,\n+    url_or_none,\n+)\n+\n+\n+class GBNewsIE(InfoExtractor):\n+    IE_DESC = 'GB News clips, features and live stream'\n+\n+    # \\w+ is normally shows or news, but apparently any word redirects to the correct URL\n+    _VALID_URL = r'https?://(?:www\\.)?gbnews\\.(?:uk|com)/(?:\\w+/)?(?P<id>[^#?]+)'\n+\n+    _PLATFORM = 'safari'\n+    _SSMP_URL = 'https://mm-v2.simplestream.com/ssmp/api.php'\n+    _TESTS = [{\n+        'url': 'https://www.gbnews.uk/shows/andrew-neils-message-to-companies-choosing-to-boycott-gb-news/106889',\n+        'info_dict': {\n+            'id': '106889',\n+            'ext': 'mp4',\n+            'title': \"Andrew Neil's message to companies choosing to boycott GB News\",\n+            'description': 'md5:b281f5d22fd6d5eda64a4e3ba771b351',\n+        },\n+        'skip': '404 not found',\n+    }, {\n+        'url': 'https://www.gbnews.com/news/bbc-claudine-gay-harvard-university-antisemitism-row',\n+        'info_dict': {\n+            'id': '52264136',\n+            'display_id': 'bbc-claudine-gay-harvard-university-antisemitism-row',\n+            'ext': 'mp4',\n+            'title': 'BBC deletes post after furious backlash over headline downplaying antisemitism',\n+            'description': 'The post was criticised by former employers of the broadcaster',\n+        },\n+    }, {\n+        'url': 'https://www.gbnews.uk/watchlive',\n+        'info_dict': {\n+            'id': '1069',\n+            'display_id': 'watchlive',\n+            'ext': 'mp4',\n+            'title': 'GB News Live',\n+            'is_live': True,\n+        },\n+        'params': {\n+            'skip_download': 'm3u8',\n+        },\n+    }]\n+\n+    def _real_extract(self, url):\n+        display_id = self._match_id(url).split('/')[-1]\n+\n+        webpage = self._download_webpage(url, display_id)\n+        # extraction based on https://github.com/ytdl-org/youtube-dl/issues/29341\n+        '''\n+        <div id=\"video-106908\"\n+            class=\"simplestream\"\n+            data-id=\"GB001\"\n+            data-type=\"vod\"\n+            data-key=\"3Li3Nt2Qs8Ct3Xq9Fi5Uy0Mb2Bj0Qs\"\n+            data-token=\"f9c317c727dc07f515b20036c8ef14a6\"\n+            data-expiry=\"1624300052\"\n+            data-uvid=\"37900558\"\n+            data-poster=\"https://thumbnails.simplestreamcdn.com/gbnews/ondemand/37900558.jpg?width=700&\"\n+            data-npaw=\"false\"\n+            data-env=\"production\">\n+        '''\n+        # exception if no match\n+        video_data = self._search_regex(\n+            r'(<div\\s[^>]*\\bclass\\s*=\\s*(\\'|\")(?!.*sidebar\\b)simplestream(?:\\s[\\s\\w$-]*)?\\2[^>]*>)',\n+            webpage, 'video data')\n+\n+        video_data = extract_attributes(video_data)\n+        ss_id = video_data.get('data-id')\n+        if not ss_id:\n+            raise ExtractorError('Simplestream ID not found')\n+\n+        json_data = self._download_json(\n+            self._SSMP_URL, display_id,\n+            note='Downloading Simplestream JSON metadata',\n+            errnote='Unable to download Simplestream JSON metadata',\n+            query={\n+                'id': ss_id,\n+                'env': video_data.get('data-env', 'production'),\n+            }, fatal=False)\n+\n+        meta_url = traverse_obj(json_data, ('response', 'api_hostname'))\n+        if not meta_url:\n+            raise ExtractorError('No API host found')\n+\n+        uvid = video_data['data-uvid']\n+        dtype = video_data.get('data-type')\n+        stream_data = self._download_json(\n+            '%s/api/%s/stream/%s' % (meta_url, 'show' if dtype == 'vod' else dtype, uvid),\n+            uvid,\n+            query={\n+                'key': video_data.get('data-key'),\n+                'platform': self._PLATFORM,\n+            },\n+            headers={\n+                'Token': video_data.get('data-token'),\n+                'Token-Expiry': video_data.get('data-expiry'),\n+                'Uvid': uvid,\n+            }, fatal=False)\n+\n+        stream_url = traverse_obj(stream_data, (\n+            'response', 'stream', T(url_or_none)))\n+        if not stream_url:\n+            raise ExtractorError('No stream data/URL')\n+\n+        # now known to be a dict\n+        stream_data = stream_data['response']\n+        drm = stream_data.get('drm')\n+        if drm:\n+            self.report_drm(uvid)\n+\n+        formats = self._extract_m3u8_formats(\n+            stream_url, uvid, ext='mp4', entry_protocol='m3u8_native',\n+            fatal=False)\n+        # exception if no formats\n+        self._sort_formats(formats)\n+\n+        return {\n+            'id': uvid,\n+            'display_id': display_id,\n+            'title': (traverse_obj(stream_data, ('title', T(txt_or_none)))\n+                      or self._og_search_title(webpage, default=None)\n+                      or display_id.replace('-', ' ').capitalize()),\n+            'description': self._og_search_description(webpage, default=None),\n+            'thumbnail': (traverse_obj(video_data, ('data-poster', T(url_or_none)))\n+                          or self._og_search_thumbnail(webpage)),\n+            'formats': formats,\n+            'is_live': (dtype == 'live') or None,\n+        }\ndiff --git a/youtube_dl/extractor/generic.py b/youtube_dl/extractor/generic.py\nindex a9c064105ff..b01900afaad 100644\n--- a/youtube_dl/extractor/generic.py\n+++ b/youtube_dl/extractor/generic.py\n@@ -28,6 +28,7 @@\n     mimetype2ext,\n     orderedSet,\n     parse_duration,\n+    parse_resolution,\n     sanitized_Request,\n     smuggle_url,\n     unescapeHTML,\n@@ -35,6 +36,7 @@\n     unsmuggle_url,\n     UnsupportedError,\n     url_or_none,\n+    urljoin,\n     xpath_attr,\n     xpath_text,\n     xpath_with_ns,\n@@ -2227,6 +2229,116 @@ class GenericIE(InfoExtractor):\n             # Sibnet embed (https://help.sibnet.ru/?sibnet_video_embed)\n             'url': 'https://phpbb3.x-tk.ru/bbcode-video-sibnet-t24.html',\n             'only_matching': True,\n+        }, {\n+            # KVS Player\n+            'url': 'https://www.kvs-demo.com/videos/105/kelis-4th-of-july/',\n+            'info_dict': {\n+                'id': '105',\n+                'display_id': 'kelis-4th-of-july',\n+                'ext': 'mp4',\n+                'title': 'Kelis - 4th Of July',\n+                'thumbnail': r're:https://(?:www\\.)?kvs-demo.com/contents/videos_screenshots/0/105/preview.jpg',\n+            },\n+        }, {\n+            # KVS Player\n+            'url': 'https://www.kvs-demo.com/embed/105/',\n+            'info_dict': {\n+                'id': '105',\n+                'display_id': 'kelis-4th-of-july',\n+                'ext': 'mp4',\n+                'title': 'Kelis - 4th Of July / Embed Player',\n+                'thumbnail': r're:https://(?:www\\.)?kvs-demo.com/contents/videos_screenshots/0/105/preview.jpg',\n+            },\n+            'params': {\n+                'skip_download': True,\n+            },\n+        }, {\n+            # KVS Player (tested also in thisvid.py)\n+            'url': 'https://youix.com/video/leningrad-zoj/',\n+            'md5': '94f96ba95706dc3880812b27b7d8a2b8',\n+            'info_dict': {\n+                'id': '18485',\n+                'display_id': 'leningrad-zoj',\n+                'ext': 'mp4',\n+                'title': '\u041a\u043b\u0438\u043f: \u041b\u0435\u043d\u0438\u043d\u0433\u0440\u0430\u0434 - \u0417\u041e\u0416 \u0441\u043a\u0430\u0447\u0430\u0442\u044c, \u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u043e\u043d\u043b\u0430\u0439\u043d | Youix.com',\n+                'thumbnail': r're:https://youix.com/contents/videos_screenshots/18000/18485/preview(?:_480x320_youix_com.mp4)?\\.jpg',\n+            },\n+        }, {\n+            # KVS Player\n+            'url': 'https://youix.com/embed/18485',\n+            'md5': '94f96ba95706dc3880812b27b7d8a2b8',\n+            'info_dict': {\n+                'id': '18485',\n+                'display_id': 'leningrad-zoj',\n+                'ext': 'mp4',\n+                'title': '\u041b\u0435\u043d\u0438\u043d\u0433\u0440\u0430\u0434 - \u0417\u041e\u0416',\n+                'thumbnail': r're:https://youix.com/contents/videos_screenshots/18000/18485/preview(?:_480x320_youix_com.mp4)?\\.jpg',\n+            },\n+        }, {\n+            # KVS Player\n+            'url': 'https://bogmedia.org/videos/21217/40-nochey-40-nights-2016/',\n+            'md5': '94166bdb26b4cb1fb9214319a629fc51',\n+            'info_dict': {\n+                'id': '21217',\n+                'display_id': '40-nochey-2016',\n+                'ext': 'mp4',\n+                'title': '40 \u043d\u043e\u0447\u0435\u0439 (2016) - BogMedia.org',\n+                'description': 'md5:4e6d7d622636eb7948275432eb256dc3',\n+                'thumbnail': 'https://bogmedia.org/contents/videos_screenshots/21000/21217/preview_480p.mp4.jpg',\n+            },\n+        }, {\n+            # KVS Player (for sites that serve kt_player.js via non-https urls)\n+            'url': 'http://www.camhub.world/embed/389508',\n+            'md5': 'fbe89af4cfb59c8fd9f34a202bb03e32',\n+            'info_dict': {\n+                'id': '389508',\n+                'display_id': 'syren-de-mer-onlyfans-05-07-2020have-a-happy-safe-holiday5f014e68a220979bdb8cd-source',\n+                'ext': 'mp4',\n+                'title': 'Syren De Mer  onlyfans_05-07-2020Have_a_happy_safe_holiday5f014e68a220979bdb8cd_source / Embed \u043f\u043b\u0435\u0435\u0440',\n+                'thumbnail': r're:https?://www\\.camhub\\.world/contents/videos_screenshots/389000/389508/preview\\.mp4\\.jpg',\n+            },\n+        }, {\n+            'url': 'https://mrdeepfakes.com/video/5/selena-gomez-pov-deep-fakes',\n+            'md5': 'fec4ad5ec150f655e0c74c696a4a2ff4',\n+            'info_dict': {\n+                'id': '5',\n+                'display_id': 'selena-gomez-pov-deep-fakes',\n+                'ext': 'mp4',\n+                'title': 'Selena Gomez POV (Deep Fakes) DeepFake Porn - MrDeepFakes',\n+                'description': 'md5:17d1f84b578c9c26875ac5ef9a932354',\n+                'height': 720,\n+                'age_limit': 18,\n+            },\n+        }, {\n+            'url': 'https://shooshtime.com/videos/284002/just-out-of-the-shower-joi/',\n+            'md5': 'e2f0a4c329f7986280b7328e24036d60',\n+            'info_dict': {\n+                'id': '284002',\n+                'display_id': 'just-out-of-the-shower-joi',\n+                'ext': 'mp4',\n+                'title': 'Just Out Of The Shower JOI - Shooshtime',\n+                'height': 720,\n+                'age_limit': 18,\n+            },\n+        }, {\n+            # would like to use the yt-dl test video but searching for\n+            # '\"\\'/\\\\\u00e4\u21ad\ud835\udd50' fails, so using an old vid from YouTube Korea\n+            'note': 'Test default search',\n+            'url': 'Shorts\ub85c \ud5c8\ub77d \ud544\uc694\uc5c6\uc774 \ub180\uc790! (BTS\ud3b8)',\n+            'info_dict': {\n+                'id': 'usDGO4Zb-dc',\n+                'ext': 'mp4',\n+                'title': 'YouTube Shorts\ub85c \ud5c8\ub77d \ud544\uc694\uc5c6\uc774 \ub180\uc790! (BTS\ud3b8)',\n+                'description': 'md5:96e31607eba81ab441567b5e289f4716',\n+                'upload_date': '20211107',\n+                'uploader': 'YouTube Korea',\n+                'location': '\ub300\ud55c\ubbfc\uad6d',\n+            },\n+            'params': {\n+                'default_search': 'ytsearch',\n+                'skip_download': True,\n+            },\n+            'expected_warnings': ['uploader id'],\n         },\n     ]\n \n@@ -2332,6 +2444,88 @@ def _extract_camtasia(self, url, video_id, webpage):\n             'title': title,\n         }\n \n+    def _extract_kvs(self, url, webpage, video_id):\n+\n+        def getlicensetoken(license):\n+            modlicense = license.replace('$', '').replace('0', '1')\n+            center = int(len(modlicense) / 2)\n+            fronthalf = int(modlicense[:center + 1])\n+            backhalf = int(modlicense[center:])\n+\n+            modlicense = compat_str(4 * abs(fronthalf - backhalf))\n+\n+            def parts():\n+                for o in range(0, center + 1):\n+                    for i in range(1, 5):\n+                        yield compat_str((int(license[o + i]) + int(modlicense[o])) % 10)\n+\n+            return ''.join(parts())\n+\n+        def getrealurl(video_url, license_code):\n+            if not video_url.startswith('function/0/'):\n+                return video_url  # not obfuscated\n+\n+            url_path, _, url_query = video_url.partition('?')\n+            urlparts = url_path.split('/')[2:]\n+            license = getlicensetoken(license_code)\n+            newmagic = urlparts[5][:32]\n+\n+            def spells(x, o):\n+                l = (o + sum(int(n) for n in license[o:])) % 32\n+                for i in range(0, len(x)):\n+                    yield {l: x[o], o: x[l]}.get(i, x[i])\n+\n+            for o in range(len(newmagic) - 1, -1, -1):\n+                newmagic = ''.join(spells(newmagic, o))\n+\n+            urlparts[5] = newmagic + urlparts[5][32:]\n+            return '/'.join(urlparts) + '?' + url_query\n+\n+        flashvars = self._search_regex(\n+            r'(?s)<script\\b[^>]*>.*?var\\s+flashvars\\s*=\\s*(\\{.+?\\});.*?</script>',\n+            webpage, 'flashvars')\n+        flashvars = self._parse_json(flashvars, video_id, transform_source=js_to_json)\n+\n+        # extract the part after the last / as the display_id from the\n+        # canonical URL.\n+        display_id = self._search_regex(\n+            r'(?:<link href=\"https?://[^\"]+/(.+?)/?\" rel=\"canonical\"\\s*/?>'\n+            r'|<link rel=\"canonical\" href=\"https?://[^\"]+/(.+?)/?\"\\s*/?>)',\n+            webpage, 'display_id', fatal=False\n+        )\n+        title = self._html_search_regex(r'<(?:h1|title)>(?:Video: )?(.+?)</(?:h1|title)>', webpage, 'title')\n+\n+        thumbnail = flashvars['preview_url']\n+        if thumbnail.startswith('//'):\n+            protocol, _, _ = url.partition('/')\n+            thumbnail = protocol + thumbnail\n+\n+        url_keys = list(filter(re.compile(r'^video_(?:url|alt_url\\d*)$').match, flashvars.keys()))\n+        formats = []\n+        for key in url_keys:\n+            if '/get_file/' not in flashvars[key]:\n+                continue\n+            format_id = flashvars.get(key + '_text', key)\n+            formats.append(merge_dicts(\n+                parse_resolution(format_id) or parse_resolution(flashvars[key]), {\n+                    'url': urljoin(url, getrealurl(flashvars[key], flashvars['license_code'])),\n+                    'format_id': format_id,\n+                    'ext': 'mp4',\n+                    'http_headers': {'Referer': url},\n+                }))\n+            if not formats[-1].get('height'):\n+                formats[-1]['quality'] = 1\n+\n+        self._sort_formats(formats)\n+\n+        return {\n+            'id': flashvars['video_id'],\n+            'display_id': display_id,\n+            'title': title,\n+            'thumbnail': thumbnail,\n+            'formats': formats,\n+        }\n+\n     def _real_extract(self, url):\n         if url.startswith('//'):\n             return self.url_result(self.http_scheme() + url)\n@@ -2540,9 +2734,16 @@ def _real_extract(self, url):\n         # but actually don't.\n         AGE_LIMIT_MARKERS = [\n             r'Proudly Labeled <a href=\"http://www\\.rtalabel\\.org/\" title=\"Restricted to Adults\">RTA</a>',\n+            r'>[^<]*you acknowledge you are at least (\\d+) years old',\n+            r'>\\s*(?:18\\s+U(?:\\.S\\.C\\.|SC)\\s+)?(?:\u00a7+\\s*)?2257\\b',\n         ]\n-        if any(re.search(marker, webpage) for marker in AGE_LIMIT_MARKERS):\n-            age_limit = 18\n+        for marker in AGE_LIMIT_MARKERS:\n+            m = re.search(marker, webpage)\n+            if not m:\n+                continue\n+            age_limit = max(\n+                age_limit or 0,\n+                int_or_none(m.groups() and m.group(1), default=18))\n \n         # video uploader is domain name\n         video_uploader = self._search_regex(\n@@ -3389,6 +3590,20 @@ def _real_extract(self, url):\n                 info_dict['formats'] = formats\n                 return info_dict\n \n+        # Look for generic KVS player (before ld+json for tests)\n+        found = self._search_regex(\n+            (r'<script\\b[^>]+?\\bsrc\\s*=\\s*([\"\\'])https?://(?:\\S+?/)+kt_player\\.js\\?v=(?P<ver>\\d+(?:\\.\\d+)+)\\1[^>]*>',\n+             # kt_player('kt_player', 'https://i.shoosh.co/player/kt_player.swf?v=5.5.1', ...\n+             r'kt_player\\s*\\(\\s*([\"\\'])(?:(?!\\1)[\\w\\W])+\\1\\s*,\\s*([\"\\'])https?://(?:\\S+?/)+kt_player\\.swf\\?v=(?P<ver>\\d+(?:\\.\\d+)+)\\2\\s*,',\n+             ), webpage, 'KVS player', group='ver', default=False)\n+        if found:\n+            self.report_extraction('%s: KVS Player' % (video_id, ))\n+            if found.split('.')[0] not in ('4', '5', '6'):\n+                self.report_warning('Untested major version (%s) in player engine - download may fail.' % (found, ))\n+            return merge_dicts(\n+                self._extract_kvs(url, webpage, video_id),\n+                info_dict)\n+\n         # Looking for http://schema.org/VideoObject\n         json_ld = self._search_json_ld(\n             webpage, video_id, default={}, expected_type='VideoObject')\ndiff --git a/youtube_dl/extractor/globalplayer.py b/youtube_dl/extractor/globalplayer.py\nnew file mode 100644\nindex 00000000000..ae75dcabf72\n--- /dev/null\n+++ b/youtube_dl/extractor/globalplayer.py\n@@ -0,0 +1,273 @@\n+# coding: utf-8\n+from __future__ import unicode_literals\n+\n+from .common import InfoExtractor\n+from ..utils import (\n+    clean_html,\n+    join_nonempty,\n+    merge_dicts,\n+    parse_duration,\n+    str_or_none,\n+    T,\n+    traverse_obj,\n+    unified_strdate,\n+    unified_timestamp,\n+    urlhandle_detect_ext,\n+)\n+\n+\n+class GlobalPlayerBaseIE(InfoExtractor):\n+\n+    def _get_page_props(self, url, video_id):\n+        webpage = self._download_webpage(url, video_id)\n+        return self._search_nextjs_data(webpage, video_id)['props']['pageProps']\n+\n+    def _request_ext(self, url, video_id):\n+        return urlhandle_detect_ext(self._request_webpage(  # Server rejects HEAD requests\n+            url, video_id, note='Determining source extension'))\n+\n+    @staticmethod\n+    def _clean_desc(x):\n+        x = clean_html(x)\n+        if x:\n+            x = x.replace('\\xa0', ' ')\n+        return x\n+\n+    def _extract_audio(self, episode, series):\n+\n+        return merge_dicts({\n+            'vcodec': 'none',\n+        }, traverse_obj(series, {\n+            'series': 'title',\n+            'series_id': 'id',\n+            'thumbnail': 'imageUrl',\n+            'uploader': 'itunesAuthor',  # podcasts only\n+        }), traverse_obj(episode, {\n+            'id': 'id',\n+            'description': ('description', T(self._clean_desc)),\n+            'duration': ('duration', T(parse_duration)),\n+            'thumbnail': 'imageUrl',\n+            'url': 'streamUrl',\n+            'timestamp': (('pubDate', 'startDate'), T(unified_timestamp)),\n+            'title': 'title',\n+        }, get_all=False), rev=True)\n+\n+\n+class GlobalPlayerLiveIE(GlobalPlayerBaseIE):\n+    _VALID_URL = r'https?://www\\.globalplayer\\.com/live/(?P<id>\\w+)/\\w+'\n+    _TESTS = [{\n+        'url': 'https://www.globalplayer.com/live/smoothchill/uk/',\n+        'info_dict': {\n+            'id': '2mx1E',\n+            'ext': 'aac',\n+            'display_id': 'smoothchill-uk',\n+            'title': 're:^Smooth Chill.+$',\n+            'thumbnail': 'https://herald.musicradio.com/media/f296ade8-50c9-4f60-911f-924e96873620.png',\n+            'description': 'Music To Chill To',\n+            # 'live_status': 'is_live',\n+            'is_live': True,\n+        },\n+    }, {\n+        # national station\n+        'url': 'https://www.globalplayer.com/live/heart/uk/',\n+        'info_dict': {\n+            'id': '2mwx4',\n+            'ext': 'aac',\n+            'description': 'turn up the feel good!',\n+            'thumbnail': 'https://herald.musicradio.com/media/49b9e8cb-15bf-4bf2-8c28-a4850cc6b0f3.png',\n+            # 'live_status': 'is_live',\n+            'is_live': True,\n+            'title': 're:^Heart UK.+$',\n+            'display_id': 'heart-uk',\n+        },\n+    }, {\n+        # regional variation\n+        'url': 'https://www.globalplayer.com/live/heart/london/',\n+        'info_dict': {\n+            'id': 'AMqg',\n+            'ext': 'aac',\n+            'thumbnail': 'https://herald.musicradio.com/media/49b9e8cb-15bf-4bf2-8c28-a4850cc6b0f3.png',\n+            'title': 're:^Heart London.+$',\n+            # 'live_status': 'is_live',\n+            'is_live': True,\n+            'display_id': 'heart-london',\n+            'description': 'turn up the feel good!',\n+        },\n+    }]\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+        station = self._get_page_props(url, video_id)['station']\n+        stream_url = station['streamUrl']\n+\n+        return merge_dicts({\n+            'id': station['id'],\n+            'display_id': (\n+                join_nonempty('brandSlug', 'slug', from_dict=station)\n+                or station.get('legacyStationPrefix')),\n+            'url': stream_url,\n+            'ext': self._request_ext(stream_url, video_id),\n+            'vcodec': 'none',\n+            'is_live': True,\n+        }, {\n+            'title': self._live_title(traverse_obj(\n+                station, (('name', 'brandName'), T(str_or_none)),\n+                get_all=False)),\n+        }, traverse_obj(station, {\n+            'description': 'tagline',\n+            'thumbnail': 'brandLogo',\n+        }), rev=True)\n+\n+\n+class GlobalPlayerLivePlaylistIE(GlobalPlayerBaseIE):\n+    _VALID_URL = r'https?://www\\.globalplayer\\.com/playlists/(?P<id>\\w+)'\n+    _TESTS = [{\n+        # \"live playlist\"\n+        'url': 'https://www.globalplayer.com/playlists/8bLk/',\n+        'info_dict': {\n+            'id': '8bLk',\n+            'ext': 'aac',\n+            # 'live_status': 'is_live',\n+            'is_live': True,\n+            'description': r're:(?s).+\\bclassical\\b.+\\bClassic FM Hall [oO]f Fame\\b',\n+            'thumbnail': 'https://images.globalplayer.com/images/551379?width=450&signature=oMLPZIoi5_dBSHnTMREW0Xg76mA=',\n+            'title': 're:Classic FM Hall of Fame.+$'\n+        },\n+    }]\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+        station = self._get_page_props(url, video_id)['playlistData']\n+        stream_url = station['streamUrl']\n+\n+        return merge_dicts({\n+            'id': video_id,\n+            'url': stream_url,\n+            'ext': self._request_ext(stream_url, video_id),\n+            'vcodec': 'none',\n+            'is_live': True,\n+        }, traverse_obj(station, {\n+            'title': 'title',\n+            'description': ('description', T(self._clean_desc)),\n+            'thumbnail': 'image',\n+        }), rev=True)\n+\n+\n+class GlobalPlayerAudioIE(GlobalPlayerBaseIE):\n+    _VALID_URL = r'https?://www\\.globalplayer\\.com/(?:(?P<podcast>podcasts)/|catchup/\\w+/\\w+/)(?P<id>\\w+)/?(?:$|[?#])'\n+    _TESTS = [{\n+        # podcast\n+        'url': 'https://www.globalplayer.com/podcasts/42KuaM/',\n+        'playlist_mincount': 5,\n+        'info_dict': {\n+            'id': '42KuaM',\n+            'title': 'Filthy Ritual',\n+            'thumbnail': 'md5:60286e7d12d795bd1bbc9efc6cee643e',\n+            'categories': ['Society & Culture', 'True Crime'],\n+            'uploader': 'Global',\n+            'description': r're:(?s).+\\bscam\\b.+?\\bseries available now\\b',\n+        },\n+    }, {\n+        # radio catchup\n+        'url': 'https://www.globalplayer.com/catchup/lbc/uk/46vyD7z/',\n+        'playlist_mincount': 2,\n+        'info_dict': {\n+            'id': '46vyD7z',\n+            'description': 'Nick Ferrari At Breakfast is Leading Britain\\'s Conversation.',\n+            'title': 'Nick Ferrari',\n+            'thumbnail': 'md5:4df24d8a226f5b2508efbcc6ae874ebf',\n+        },\n+    }]\n+\n+    def _real_extract(self, url):\n+        video_id, podcast = self._match_valid_url(url).group('id', 'podcast')\n+        props = self._get_page_props(url, video_id)\n+        series = props['podcastInfo'] if podcast else props['catchupInfo']\n+\n+        return merge_dicts({\n+            '_type': 'playlist',\n+            'id': video_id,\n+            'entries': [self._extract_audio(ep, series) for ep in traverse_obj(\n+                        series, ('episodes', lambda _, v: v['id'] and v['streamUrl']))],\n+            'categories': traverse_obj(series, ('categories', Ellipsis, 'name')) or None,\n+        }, traverse_obj(series, {\n+            'description': ('description', T(self._clean_desc)),\n+            'thumbnail': 'imageUrl',\n+            'title': 'title',\n+            'uploader': 'itunesAuthor',  # podcasts only\n+        }), rev=True)\n+\n+\n+class GlobalPlayerAudioEpisodeIE(GlobalPlayerBaseIE):\n+    _VALID_URL = r'https?://www\\.globalplayer\\.com/(?:(?P<podcast>podcasts)|catchup/\\w+/\\w+)/episodes/(?P<id>\\w+)/?(?:$|[?#])'\n+    _TESTS = [{\n+        # podcast\n+        'url': 'https://www.globalplayer.com/podcasts/episodes/7DrfNnE/',\n+        'info_dict': {\n+            'id': '7DrfNnE',\n+            'ext': 'mp3',\n+            'title': 'Filthy Ritual - Trailer',\n+            'description': 'md5:1f1562fd0f01b4773b590984f94223e0',\n+            'thumbnail': 'md5:60286e7d12d795bd1bbc9efc6cee643e',\n+            'duration': 225.0,\n+            'timestamp': 1681254900,\n+            'series': 'Filthy Ritual',\n+            'series_id': '42KuaM',\n+            'upload_date': '20230411',\n+            'uploader': 'Global',\n+        },\n+    }, {\n+        # radio catchup\n+        'url': 'https://www.globalplayer.com/catchup/lbc/uk/episodes/2zGq26Vcv1fCWhddC4JAwETXWe/',\n+        'only_matching': True,\n+        # expired: refresh the details with a current show for a full test\n+        'info_dict': {\n+            'id': '2zGq26Vcv1fCWhddC4JAwETXWe',\n+            'ext': 'm4a',\n+            'timestamp': 1682056800,\n+            'series': 'Nick Ferrari',\n+            'thumbnail': 'md5:4df24d8a226f5b2508efbcc6ae874ebf',\n+            'upload_date': '20230421',\n+            'series_id': '46vyD7z',\n+            'description': 'Nick Ferrari At Breakfast is Leading Britain\\'s Conversation.',\n+            'title': 'Nick Ferrari',\n+            'duration': 10800.0,\n+        },\n+    }]\n+\n+    def _real_extract(self, url):\n+        video_id, podcast = self._match_valid_url(url).group('id', 'podcast')\n+        props = self._get_page_props(url, video_id)\n+        episode = props['podcastEpisode'] if podcast else props['catchupEpisode']\n+\n+        return self._extract_audio(\n+            episode, traverse_obj(episode, 'podcast', 'show', expected_type=dict) or {})\n+\n+\n+class GlobalPlayerVideoIE(GlobalPlayerBaseIE):\n+    _VALID_URL = r'https?://www\\.globalplayer\\.com/videos/(?P<id>\\w+)'\n+    _TESTS = [{\n+        'url': 'https://www.globalplayer.com/videos/2JsSZ7Gm2uP/',\n+        'info_dict': {\n+            'id': '2JsSZ7Gm2uP',\n+            'ext': 'mp4',\n+            'description': 'md5:6a9f063c67c42f218e42eee7d0298bfd',\n+            'thumbnail': 'md5:d4498af48e15aae4839ce77b97d39550',\n+            'upload_date': '20230420',\n+            'title': 'Treble Malakai Bayoh sings a sublime Handel aria at Classic FM Live',\n+        },\n+    }]\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+        meta = self._get_page_props(url, video_id)['videoData']\n+\n+        return merge_dicts({\n+            'id': video_id,\n+        }, traverse_obj(meta, {\n+            'url': 'url',\n+            'thumbnail': ('image', 'url'),\n+            'title': 'title',\n+            'upload_date': ('publish_date', T(unified_strdate)),\n+            'description': 'description',\n+        }), rev=True)\ndiff --git a/youtube_dl/extractor/hrfernsehen.py b/youtube_dl/extractor/hrfernsehen.py\nnew file mode 100644\nindex 00000000000..11b879dbdd4\n--- /dev/null\n+++ b/youtube_dl/extractor/hrfernsehen.py\n@@ -0,0 +1,101 @@\n+# coding: utf-8\n+\n+from __future__ import unicode_literals\n+\n+import json\n+import re\n+\n+from ..utils import (\n+    int_or_none,\n+    unified_timestamp,\n+    unescapeHTML\n+)\n+from .common import InfoExtractor\n+\n+\n+class HRFernsehenIE(InfoExtractor):\n+    IE_NAME = 'hrfernsehen'\n+    _VALID_URL = r'^https?://www\\.(?:hr-fernsehen|hessenschau)\\.de/.*,video-(?P<id>[0-9]{6})\\.html'\n+\n+    _TESTS = [{\n+        'url': 'https://www.hessenschau.de/tv-sendung/hessenschau-vom-26082020,video-130546.html',\n+        'md5': '5c4e0ba94677c516a2f65a84110fc536',\n+        'info_dict': {\n+            'id': '130546',\n+            'ext': 'mp4',\n+            'description': 'Sturmtief Kirsten fegt \u00fcber Hessen / Die Corona-Pandemie \u2013 eine Chronologie / '\n+                           'Sterbehilfe: Die Lage in Hessen / Miss Hessen leitet zwei eigene Unternehmen / '\n+                           'Pop-Up Museum zeigt Schwarze Unterhaltung und Black Music',\n+            'subtitles': {'de': [{\n+                'url': 'https://hr-a.akamaihd.net/video/as/hessenschau/2020_08/hrLogo_200826200407_L385592_512x288-25p-500kbit.vtt'\n+            }]},\n+            'timestamp': 1598470200,\n+            'upload_date': '20200826',\n+            'thumbnail': 'https://www.hessenschau.de/tv-sendung/hs_ganz-1554~_t-1598465545029_v-16to9__medium.jpg',\n+            'title': 'hessenschau vom 26.08.2020'\n+        }\n+    }, {\n+        'url': 'https://www.hr-fernsehen.de/sendungen-a-z/mex/sendungen/fair-und-gut---was-hinter-aldis-eigenem-guetesiegel-steckt,video-130544.html',\n+        'only_matching': True\n+    }]\n+\n+    _GEO_COUNTRIES = ['DE']\n+\n+    def extract_airdate(self, loader_data):\n+        airdate_str = loader_data.get('mediaMetadata', {}).get('agf', {}).get('airdate')\n+\n+        if airdate_str is None:\n+            return None\n+\n+        return unified_timestamp(airdate_str)\n+\n+    def extract_formats(self, loader_data):\n+        stream_formats = []\n+        for stream_obj in loader_data[\"videoResolutionLevels\"]:\n+            stream_format = {\n+                'format_id': str(stream_obj['verticalResolution']) + \"p\",\n+                'height': stream_obj['verticalResolution'],\n+                'url': stream_obj['url'],\n+            }\n+\n+            quality_information = re.search(r'([0-9]{3,4})x([0-9]{3,4})-([0-9]{2})p-([0-9]{3,4})kbit',\n+                                            stream_obj['url'])\n+            if quality_information:\n+                stream_format['width'] = int_or_none(quality_information.group(1))\n+                stream_format['height'] = int_or_none(quality_information.group(2))\n+                stream_format['fps'] = int_or_none(quality_information.group(3))\n+                stream_format['tbr'] = int_or_none(quality_information.group(4))\n+\n+            stream_formats.append(stream_format)\n+\n+        self._sort_formats(stream_formats)\n+        return stream_formats\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+        webpage = self._download_webpage(url, video_id)\n+\n+        title = self._html_search_meta(\n+            ['og:title', 'twitter:title', 'name'], webpage)\n+        description = self._html_search_meta(\n+            ['description'], webpage)\n+\n+        loader_str = unescapeHTML(self._search_regex(r\"data-new-hr-mediaplayer-loader='([^']*)'\", webpage, \"ardloader\"))\n+        loader_data = json.loads(loader_str)\n+\n+        info = {\n+            'id': video_id,\n+            'title': title,\n+            'description': description,\n+            'formats': self.extract_formats(loader_data),\n+            'timestamp': self.extract_airdate(loader_data)\n+        }\n+\n+        if \"subtitle\" in loader_data:\n+            info[\"subtitles\"] = {\"de\": [{\"url\": loader_data[\"subtitle\"]}]}\n+\n+        thumbnails = list(set([t for t in loader_data.get(\"previewImageUrl\", {}).values()]))\n+        if len(thumbnails) > 0:\n+            info[\"thumbnails\"] = [{\"url\": t} for t in thumbnails]\n+\n+        return info\ndiff --git a/youtube_dl/extractor/ign.py b/youtube_dl/extractor/ign.py\nindex 0d9f50ed2a9..c7daa30e575 100644\n--- a/youtube_dl/extractor/ign.py\n+++ b/youtube_dl/extractor/ign.py\n@@ -1,19 +1,29 @@\n+# coding: utf-8\n+\n from __future__ import unicode_literals\n \n import re\n \n from .common import InfoExtractor\n from ..compat import (\n+    compat_filter as filter,\n+    compat_HTTPError,\n     compat_parse_qs,\n-    compat_urllib_parse_urlparse,\n+    compat_urlparse,\n )\n from ..utils import (\n-    HEADRequest,\n     determine_ext,\n+    error_to_compat_str,\n+    extract_attributes,\n+    ExtractorError,\n     int_or_none,\n+    merge_dicts,\n+    orderedSet,\n     parse_iso8601,\n     strip_or_none,\n-    try_get,\n+    traverse_obj,\n+    url_or_none,\n+    urljoin,\n )\n \n \n@@ -22,69 +32,37 @@ def _call_api(self, slug):\n         return self._download_json(\n             'http://apis.ign.com/{0}/v3/{0}s/slug/{1}'.format(self._PAGE_TYPE, slug), slug)\n \n+    def _checked_call_api(self, slug):\n+        try:\n+            return self._call_api(slug)\n+        except ExtractorError as e:\n+            if isinstance(e.cause, compat_HTTPError) and e.cause.code == 404:\n+                e.cause.args = e.cause.args or [\n+                    e.cause.geturl(), e.cause.getcode(), e.cause.reason]\n+                raise ExtractorError(\n+                    'Content not found: expired?', cause=e.cause,\n+                    expected=True)\n+            raise\n \n-class IGNIE(IGNBaseIE):\n-    \"\"\"\n-    Extractor for some of the IGN sites, like www.ign.com, es.ign.com de.ign.com.\n-    Some videos of it.ign.com are also supported\n-    \"\"\"\n-\n-    _VALID_URL = r'https?://(?:.+?\\.ign|www\\.pcmag)\\.com/videos/(?:\\d{4}/\\d{2}/\\d{2}/)?(?P<id>[^/?&#]+)'\n-    IE_NAME = 'ign.com'\n-    _PAGE_TYPE = 'video'\n-\n-    _TESTS = [{\n-        'url': 'http://www.ign.com/videos/2013/06/05/the-last-of-us-review',\n-        'md5': 'd2e1586d9987d40fad7867bf96a018ea',\n-        'info_dict': {\n-            'id': '8f862beef863986b2785559b9e1aa599',\n-            'ext': 'mp4',\n-            'title': 'The Last of Us Review',\n-            'description': 'md5:c8946d4260a4d43a00d5ae8ed998870c',\n-            'timestamp': 1370440800,\n-            'upload_date': '20130605',\n-            'tags': 'count:9',\n-        }\n-    }, {\n-        'url': 'http://www.pcmag.com/videos/2015/01/06/010615-whats-new-now-is-gogo-snooping-on-your-data',\n-        'md5': 'f1581a6fe8c5121be5b807684aeac3f6',\n-        'info_dict': {\n-            'id': 'ee10d774b508c9b8ec07e763b9125b91',\n-            'ext': 'mp4',\n-            'title': 'What\\'s New Now: Is GoGo Snooping on Your Data?',\n-            'description': 'md5:817a20299de610bd56f13175386da6fa',\n-            'timestamp': 1420571160,\n-            'upload_date': '20150106',\n-            'tags': 'count:4',\n-        }\n-    }, {\n-        'url': 'https://www.ign.com/videos/is-a-resident-evil-4-remake-on-the-way-ign-daily-fix',\n-        'only_matching': True,\n-    }]\n-\n-    def _real_extract(self, url):\n-        display_id = self._match_id(url)\n-        video = self._call_api(display_id)\n+    def _extract_video_info(self, video, fatal=True):\n         video_id = video['videoId']\n-        metadata = video['metadata']\n-        title = metadata.get('longTitle') or metadata.get('title') or metadata['name']\n \n         formats = []\n-        refs = video.get('refs') or {}\n+        refs = traverse_obj(video, 'refs', expected_type=dict) or {}\n \n-        m3u8_url = refs.get('m3uUrl')\n+        m3u8_url = url_or_none(refs.get('m3uUrl'))\n         if m3u8_url:\n             formats.extend(self._extract_m3u8_formats(\n                 m3u8_url, video_id, 'mp4', 'm3u8_native',\n                 m3u8_id='hls', fatal=False))\n \n-        f4m_url = refs.get('f4mUrl')\n+        f4m_url = url_or_none(refs.get('f4mUrl'))\n         if f4m_url:\n             formats.extend(self._extract_f4m_formats(\n                 f4m_url, video_id, f4m_id='hds', fatal=False))\n \n         for asset in (video.get('assets') or []):\n-            asset_url = asset.get('url')\n+            asset_url = url_or_none(asset.get('url'))\n             if not asset_url:\n                 continue\n             formats.append({\n@@ -95,7 +73,8 @@ def _real_extract(self, url):\n                 'width': int_or_none(asset.get('width')),\n             })\n \n-        mezzanine_url = try_get(video, lambda x: x['system']['mezzanineUrl'])\n+        mezzanine_url = traverse_obj(\n+            video, ('system', 'mezzanineUrl'), expected_type=url_or_none)\n         if mezzanine_url:\n             formats.append({\n                 'ext': determine_ext(mezzanine_url, 'mp4'),\n@@ -104,23 +83,21 @@ def _real_extract(self, url):\n                 'url': mezzanine_url,\n             })\n \n-        self._sort_formats(formats)\n+        if formats or fatal:\n+            self._sort_formats(formats)\n+        else:\n+            return\n \n-        thumbnails = []\n-        for thumbnail in (video.get('thumbnails') or []):\n-            thumbnail_url = thumbnail.get('url')\n-            if not thumbnail_url:\n-                continue\n-            thumbnails.append({\n-                'url': thumbnail_url,\n-            })\n+        thumbnails = traverse_obj(\n+            video, ('thumbnails', Ellipsis, {'url': 'url'}), expected_type=url_or_none)\n+        tags = traverse_obj(\n+            video, ('tags', Ellipsis, 'displayName'),\n+            expected_type=lambda x: x.strip() or None)\n \n-        tags = []\n-        for tag in (video.get('tags') or []):\n-            display_name = tag.get('displayName')\n-            if not display_name:\n-                continue\n-            tags.append(display_name)\n+        metadata = traverse_obj(video, 'metadata', expected_type=dict) or {}\n+        title = traverse_obj(\n+            metadata, 'longTitle', 'title', 'name',\n+            expected_type=lambda x: x.strip() or None)\n \n         return {\n             'id': video_id,\n@@ -128,14 +105,103 @@ def _real_extract(self, url):\n             'description': strip_or_none(metadata.get('description')),\n             'timestamp': parse_iso8601(metadata.get('publishDate')),\n             'duration': int_or_none(metadata.get('duration')),\n-            'display_id': display_id,\n             'thumbnails': thumbnails,\n             'formats': formats,\n             'tags': tags,\n         }\n \n+    # yt-dlp shim\n+    @classmethod\n+    def _extract_from_webpage(cls, url, webpage):\n+        for embed_url in orderedSet(\n+                cls._extract_embed_urls(url, webpage) or [], lazy=True):\n+            yield cls.url_result(embed_url, None if cls._VALID_URL is False else cls)\n+\n \n-class IGNVideoIE(InfoExtractor):\n+class IGNIE(IGNBaseIE):\n+    \"\"\"\n+    Extractor for some of the IGN sites, like www.ign.com, es.ign.com de.ign.com.\n+    Some videos of it.ign.com are also supported\n+    \"\"\"\n+    _VIDEO_PATH_RE = r'/(?:\\d{4}/\\d{2}/\\d{2}/)?(?P<id>.+?)'\n+    _PLAYLIST_PATH_RE = r'(?:/?\\?(?P<filt>[^&#]+))?'\n+    _VALID_URL = (\n+        r'https?://(?:.+?\\.ign|www\\.pcmag)\\.com/videos(?:%s)'\n+        % '|'.join((_VIDEO_PATH_RE + r'(?:[/?&#]|$)', _PLAYLIST_PATH_RE)))\n+    IE_NAME = 'ign.com'\n+    _PAGE_TYPE = 'video'\n+\n+    _TESTS = [{\n+        'url': 'http://www.ign.com/videos/2013/06/05/the-last-of-us-review',\n+        'md5': 'd2e1586d9987d40fad7867bf96a018ea',\n+        'info_dict': {\n+            'id': '8f862beef863986b2785559b9e1aa599',\n+            'ext': 'mp4',\n+            'title': 'The Last of Us Review',\n+            'description': 'md5:c8946d4260a4d43a00d5ae8ed998870c',\n+            'timestamp': 1370440800,\n+            'upload_date': '20130605',\n+            'tags': 'count:9',\n+        },\n+        'params': {\n+            'nocheckcertificate': True,\n+        },\n+    }, {\n+        'url': 'http://www.pcmag.com/videos/2015/01/06/010615-whats-new-now-is-gogo-snooping-on-your-data',\n+        'md5': 'f1581a6fe8c5121be5b807684aeac3f6',\n+        'info_dict': {\n+            'id': 'ee10d774b508c9b8ec07e763b9125b91',\n+            'ext': 'mp4',\n+            'title': 'What\\'s New Now: Is GoGo Snooping on Your Data?',\n+            'description': 'md5:817a20299de610bd56f13175386da6fa',\n+            'timestamp': 1420571160,\n+            'upload_date': '20150106',\n+            'tags': 'count:4',\n+        },\n+        'skip': '404 Not Found',\n+    }, {\n+        'url': 'https://www.ign.com/videos/is-a-resident-evil-4-remake-on-the-way-ign-daily-fix',\n+        'only_matching': True,\n+    }]\n+\n+    @classmethod\n+    def _extract_embed_urls(cls, url, webpage):\n+        grids = re.findall(\n+            r'''(?s)<section\\b[^>]+\\bclass\\s*=\\s*['\"](?:[\\w-]+\\s+)*?content-feed-grid(?!\\B|-)[^>]+>(.+?)</section[^>]*>''',\n+            webpage)\n+        return filter(None,\n+                      (urljoin(url, m.group('path')) for m in re.finditer(\n+                          r'''<a\\b[^>]+\\bhref\\s*=\\s*('|\")(?P<path>/videos%s)\\1'''\n+                          % cls._VIDEO_PATH_RE, grids[0] if grids else '')))\n+\n+    def _real_extract(self, url):\n+        m = re.match(self._VALID_URL, url)\n+        display_id = m.group('id')\n+        if display_id:\n+            return self._extract_video(url, display_id)\n+        display_id = m.group('filt') or 'all'\n+        return self._extract_playlist(url, display_id)\n+\n+    def _extract_playlist(self, url, display_id):\n+        webpage = self._download_webpage(url, display_id)\n+\n+        return self.playlist_result(\n+            (self.url_result(u, ie=self.ie_key())\n+             for u in self._extract_embed_urls(url, webpage)),\n+            playlist_id=display_id)\n+\n+    def _extract_video(self, url, display_id):\n+        display_id = self._match_id(url)\n+        video = self._checked_call_api(display_id)\n+\n+        info = self._extract_video_info(video)\n+\n+        return merge_dicts({\n+            'display_id': display_id,\n+        }, info)\n+\n+\n+class IGNVideoIE(IGNBaseIE):\n     _VALID_URL = r'https?://.+?\\.ign\\.com/(?:[a-z]{2}/)?[^/]+/(?P<id>\\d+)/(?:video|trailer)/'\n     _TESTS = [{\n         'url': 'http://me.ign.com/en/videos/112203/video/how-hitman-aims-to-be-different-than-every-other-s',\n@@ -147,7 +213,8 @@ class IGNVideoIE(InfoExtractor):\n             'description': 'Taking out assassination targets in Hitman has never been more stylish.',\n             'timestamp': 1444665600,\n             'upload_date': '20151012',\n-        }\n+        },\n+        'expected_warnings': ['HTTP Error 400: Bad Request'],\n     }, {\n         'url': 'http://me.ign.com/ar/angry-birds-2/106533/video/lrd-ldyy-lwl-lfylm-angry-birds',\n         'only_matching': True,\n@@ -167,22 +234,38 @@ class IGNVideoIE(InfoExtractor):\n \n     def _real_extract(self, url):\n         video_id = self._match_id(url)\n-        req = HEADRequest(url.rsplit('/', 1)[0] + '/embed')\n-        url = self._request_webpage(req, video_id).geturl()\n+        parsed_url = compat_urlparse.urlparse(url)\n+        embed_url = compat_urlparse.urlunparse(\n+            parsed_url._replace(path=parsed_url.path.rsplit('/', 1)[0] + '/embed'))\n+\n+        webpage, urlh = self._download_webpage_handle(embed_url, video_id)\n+        new_url = urlh.geturl()\n         ign_url = compat_parse_qs(\n-            compat_urllib_parse_urlparse(url).query).get('url', [None])[0]\n+            compat_urlparse.urlparse(new_url).query).get('url', [None])[-1]\n         if ign_url:\n             return self.url_result(ign_url, IGNIE.ie_key())\n-        return self.url_result(url)\n+        video = self._search_regex(r'(<div\\b[^>]+\\bdata-video-id\\s*=\\s*[^>]+>)', webpage, 'video element', fatal=False)\n+        if not video:\n+            if new_url == url:\n+                raise ExtractorError('Redirect loop: ' + url)\n+            return self.url_result(new_url)\n+        video = extract_attributes(video)\n+        video_data = video.get('data-settings') or '{}'\n+        video_data = self._parse_json(video_data, video_id)['video']\n+        info = self._extract_video_info(video_data)\n+\n+        return merge_dicts({\n+            'display_id': video_id,\n+        }, info)\n \n \n class IGNArticleIE(IGNBaseIE):\n-    _VALID_URL = r'https?://.+?\\.ign\\.com/(?:articles(?:/\\d{4}/\\d{2}/\\d{2})?|(?:[a-z]{2}/)?feature/\\d+)/(?P<id>[^/?&#]+)'\n+    _VALID_URL = r'https?://.+?\\.ign\\.com/(?:articles(?:/\\d{4}/\\d{2}/\\d{2})?|(?:[a-z]{2}/)?(?:[\\w-]+/)*?feature/\\d+)/(?P<id>[^/?&#]+)'\n     _PAGE_TYPE = 'article'\n     _TESTS = [{\n         'url': 'http://me.ign.com/en/feature/15775/100-little-things-in-gta-5-that-will-blow-your-mind',\n         'info_dict': {\n-            'id': '524497489e4e8ff5848ece34',\n+            'id': '72113',\n             'title': '100 Little Things in GTA 5 That Will Blow Your Mind',\n         },\n         'playlist': [\n@@ -190,7 +273,7 @@ class IGNArticleIE(IGNBaseIE):\n                 'info_dict': {\n                     'id': '5ebbd138523268b93c9141af17bec937',\n                     'ext': 'mp4',\n-                    'title': 'GTA 5 Video Review',\n+                    'title': 'Grand Theft Auto V Video Review',\n                     'description': 'Rockstar drops the mic on this generation of games. Watch our review of the masterly Grand Theft Auto V.',\n                     'timestamp': 1379339880,\n                     'upload_date': '20130916',\n@@ -200,7 +283,7 @@ class IGNArticleIE(IGNBaseIE):\n                 'info_dict': {\n                     'id': '638672ee848ae4ff108df2a296418ee2',\n                     'ext': 'mp4',\n-                    'title': '26 Twisted Moments from GTA 5 in Slow Motion',\n+                    'title': 'GTA 5 In Slow Motion',\n                     'description': 'The twisted beauty of GTA 5 in stunning slow motion.',\n                     'timestamp': 1386878820,\n                     'upload_date': '20131212',\n@@ -208,16 +291,17 @@ class IGNArticleIE(IGNBaseIE):\n             },\n         ],\n         'params': {\n-            'playlist_items': '2-3',\n             'skip_download': True,\n         },\n+        'expected_warnings': ['Backend fetch failed'],\n     }, {\n         'url': 'http://www.ign.com/articles/2014/08/15/rewind-theater-wild-trailer-gamescom-2014?watch',\n         'info_dict': {\n             'id': '53ee806780a81ec46e0790f8',\n             'title': 'Rewind Theater - Wild Trailer Gamescom 2014',\n         },\n-        'playlist_count': 2,\n+        'playlist_count': 1,\n+        'expected_warnings': ['Backend fetch failed'],\n     }, {\n         # videoId pattern\n         'url': 'http://www.ign.com/articles/2017/06/08/new-ducktales-short-donalds-birthday-doesnt-go-as-planned',\n@@ -240,18 +324,91 @@ class IGNArticleIE(IGNBaseIE):\n         'only_matching': True,\n     }]\n \n+    def _checked_call_api(self, slug):\n+        try:\n+            return self._call_api(slug)\n+        except ExtractorError as e:\n+            if isinstance(e.cause, compat_HTTPError):\n+                e.cause.args = e.cause.args or [\n+                    e.cause.geturl(), e.cause.getcode(), e.cause.reason]\n+                if e.cause.code == 404:\n+                    raise ExtractorError(\n+                        'Content not found: expired?', cause=e.cause,\n+                        expected=True)\n+                elif e.cause.code == 503:\n+                    self.report_warning(error_to_compat_str(e.cause))\n+                    return\n+            raise\n+\n+    def _search_nextjs_data(self, webpage, video_id, **kw):\n+        return self._parse_json(\n+            self._search_regex(\n+                r'(?s)<script[^>]+id=[\\'\"]__NEXT_DATA__[\\'\"][^>]*>([^<]+)</script>',\n+                webpage, 'next.js data', **kw),\n+            video_id, **kw)\n+\n     def _real_extract(self, url):\n         display_id = self._match_id(url)\n-        article = self._call_api(display_id)\n+        article = self._checked_call_api(display_id)\n+\n+        if article:\n+            # obsolete ?\n+            def entries():\n+                media_url = traverse_obj(\n+                    article, ('mediaRelations', 0, 'media', 'metadata', 'url'),\n+                    expected_type=url_or_none)\n+                if media_url:\n+                    yield self.url_result(media_url, IGNIE.ie_key())\n+                for content in (article.get('content') or []):\n+                    for video_url in re.findall(r'(?:\\[(?:ignvideo\\s+url|youtube\\s+clip_id)|<iframe[^>]+src)=\"([^\"]+)\"', content):\n+                        if url_or_none(video_url):\n+                            yield self.url_result(video_url)\n+\n+            return self.playlist_result(\n+                entries(), article.get('articleId'),\n+                traverse_obj(\n+                    article, ('metadata', 'headline'),\n+                    expected_type=lambda x: x.strip() or None))\n+\n+        webpage = self._download_webpage(url, display_id)\n+\n+        playlist_id = self._html_search_meta('dable:item_id', webpage, default=None)\n+        if playlist_id:\n+\n+            def entries():\n+                for m in re.finditer(\n+                        r'''(?s)<object\\b[^>]+\\bclass\\s*=\\s*(\"|')ign-videoplayer\\1[^>]*>(?P<params>.+?)</object''',\n+                        webpage):\n+                    flashvars = self._search_regex(\n+                        r'''(<param\\b[^>]+\\bname\\s*=\\s*(\"|')flashvars\\2[^>]*>)''',\n+                        m.group('params'), 'flashvars', default='')\n+                    flashvars = compat_parse_qs(extract_attributes(flashvars).get('value') or '')\n+                    v_url = url_or_none((flashvars.get('url') or [None])[-1])\n+                    if v_url:\n+                        yield self.url_result(v_url)\n+        else:\n+            playlist_id = self._search_regex(\n+                r'''\\bdata-post-id\\s*=\\s*(\"|')(?P<id>[\\da-f]+)\\1''',\n+                webpage, 'id', group='id', default=None)\n+\n+            nextjs_data = self._search_nextjs_data(webpage, display_id)\n \n-        def entries():\n-            media_url = try_get(article, lambda x: x['mediaRelations'][0]['media']['metadata']['url'])\n-            if media_url:\n-                yield self.url_result(media_url, IGNIE.ie_key())\n-            for content in (article.get('content') or []):\n-                for video_url in re.findall(r'(?:\\[(?:ignvideo\\s+url|youtube\\s+clip_id)|<iframe[^>]+src)=\"([^\"]+)\"', content):\n-                    yield self.url_result(video_url)\n+            def entries():\n+                for player in traverse_obj(\n+                        nextjs_data,\n+                        ('props', 'apolloState', 'ROOT_QUERY', lambda k, _: k.startswith('videoPlayerProps('), '__ref')):\n+                    # skip promo links (which may not always be served, eg GH CI servers)\n+                    if traverse_obj(nextjs_data,\n+                                    ('props', 'apolloState', player.replace('PlayerProps', 'ModernContent')),\n+                                    expected_type=dict):\n+                        continue\n+                    video = traverse_obj(nextjs_data, ('props', 'apolloState', player), expected_type=dict) or {}\n+                    info = self._extract_video_info(video, fatal=False)\n+                    if info:\n+                        yield merge_dicts({\n+                            'display_id': display_id,\n+                        }, info)\n \n         return self.playlist_result(\n-            entries(), article.get('articleId'),\n-            strip_or_none(try_get(article, lambda x: x['metadata']['headline'])))\n+            entries(), playlist_id or display_id,\n+            re.sub(r'\\s+-\\s+IGN\\s*$', '', self._og_search_title(webpage, default='')) or None)\ndiff --git a/youtube_dl/extractor/imgur.py b/youtube_dl/extractor/imgur.py\nindex a5ba03efae5..59f129d6abc 100644\n--- a/youtube_dl/extractor/imgur.py\n+++ b/youtube_dl/extractor/imgur.py\n@@ -1,101 +1,267 @@\n+# coding: utf-8\n from __future__ import unicode_literals\n \n import re\n \n from .common import InfoExtractor\n from ..utils import (\n+    determine_ext,\n+    ExtractorError,\n+    float_or_none,\n     int_or_none,\n     js_to_json,\n+    merge_dicts,\n     mimetype2ext,\n-    ExtractorError,\n+    parse_iso8601,\n+    T,\n+    traverse_obj,\n+    txt_or_none,\n+    url_or_none,\n )\n \n \n-class ImgurIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:i\\.)?imgur\\.com/(?!(?:a|gallery|(?:t(?:opic)?|r)/[^/]+)/)(?P<id>[a-zA-Z0-9]+)'\n+class ImgurBaseIE(InfoExtractor):\n+    # hard-coded value, as also used by ArchiveTeam\n+    _CLIENT_ID = '546c25a59c58ad7'\n+\n+    @classmethod\n+    def _imgur_result(cls, item_id):\n+        return cls.url_result('imgur:%s' % item_id, ImgurIE.ie_key(), item_id)\n+\n+    def _call_api(self, endpoint, video_id, **kwargs):\n+        return self._download_json(\n+            'https://api.imgur.com/post/v1/%s/%s?client_id=%s&include=media,account' % (endpoint, video_id, self._CLIENT_ID),\n+            video_id, **kwargs)\n+\n+    @staticmethod\n+    def get_description(s):\n+        if 'Discover the magic of the internet at Imgur' in s:\n+            return None\n+        return txt_or_none(s)\n+\n+\n+class ImgurIE(ImgurBaseIE):\n+    _VALID_URL = r'''(?x)\n+        (?:\n+            https?://(?:i\\.)?imgur\\.com/(?!(?:a|gallery|t|topic|r)/)|\n+            imgur:\n+        )(?P<id>[a-zA-Z0-9]+)\n+    '''\n \n     _TESTS = [{\n-        'url': 'https://i.imgur.com/A61SaA1.gifv',\n+        'url': 'https://imgur.com/A61SaA1',\n         'info_dict': {\n             'id': 'A61SaA1',\n             'ext': 'mp4',\n             'title': 're:Imgur GIF$|MRW gifv is up and running without any bugs$',\n+            'timestamp': 1416446068,\n+            'upload_date': '20141120',\n         },\n     }, {\n-        'url': 'https://imgur.com/A61SaA1',\n+        'url': 'https://i.imgur.com/A61SaA1.gifv',\n         'only_matching': True,\n     }, {\n         'url': 'https://i.imgur.com/crGpqCV.mp4',\n         'only_matching': True,\n     }, {\n-        # no title\n+        # previously, no title\n         'url': 'https://i.imgur.com/jxBXAMC.gifv',\n-        'only_matching': True,\n+        'info_dict': {\n+            'id': 'jxBXAMC',\n+            'ext': 'mp4',\n+            'title': 'Fahaka puffer feeding',\n+            'timestamp': 1533835503,\n+            'upload_date': '20180809',\n+        },\n     }]\n \n+    def _extract_twitter_formats(self, html, tw_id='twitter', **kwargs):\n+        fatal = kwargs.pop('fatal', False)\n+        tw_stream = self._html_search_meta('twitter:player:stream', html, fatal=fatal, **kwargs)\n+        if not tw_stream:\n+            return []\n+        ext = mimetype2ext(self._html_search_meta(\n+            'twitter:player:stream:content_type', html, default=None))\n+        width, height = (int_or_none(self._html_search_meta('twitter:player:' + v, html, default=None))\n+                         for v in ('width', 'height'))\n+        return [{\n+            'format_id': tw_id,\n+            'url': tw_stream,\n+            'ext': ext or determine_ext(tw_stream),\n+            'width': width,\n+            'height': height,\n+        }]\n+\n     def _real_extract(self, url):\n         video_id = self._match_id(url)\n+        data = self._call_api('media', video_id, fatal=False, expected_status=404)\n         webpage = self._download_webpage(\n-            'https://i.imgur.com/{id}.gifv'.format(id=video_id), video_id)\n+            'https://i.imgur.com/{id}.gifv'.format(id=video_id), video_id, fatal=not data) or ''\n+\n+        if not traverse_obj(data, ('media', 0, (\n+                ('type', T(lambda t: t == 'video' or None)),\n+                ('metadata', 'is_animated'))), get_all=False):\n+            raise ExtractorError(\n+                '%s is not a video or animated image' % video_id,\n+                expected=True)\n+\n+        media_fmt = traverse_obj(data, ('media', 0, {\n+            'url': ('url', T(url_or_none)),\n+            'ext': 'ext',\n+            'width': ('width', T(int_or_none)),\n+            'height': ('height', T(int_or_none)),\n+            'filesize': ('size', T(int_or_none)),\n+            'acodec': ('metadata', 'has_sound', T(lambda b: None if b else 'none')),\n+        }))\n \n-        width = int_or_none(self._og_search_property(\n-            'video:width', webpage, default=None))\n-        height = int_or_none(self._og_search_property(\n-            'video:height', webpage, default=None))\n+        media_url = traverse_obj(media_fmt, 'url')\n+        if media_url:\n+            if not media_fmt.get('ext'):\n+                media_fmt['ext'] = mimetype2ext(traverse_obj(\n+                    data, ('media', 0, 'mime_type'))) or determine_ext(media_url)\n+            if traverse_obj(data, ('media', 0, 'type')) == 'image':\n+                media_fmt['acodec'] = 'none'\n+                media_fmt.setdefault('preference', -10)\n+\n+        tw_formats = self._extract_twitter_formats(webpage)\n+        if traverse_obj(tw_formats, (0, 'url')) == media_url:\n+            tw_formats = []\n+        else:\n+            # maybe this isn't an animated image/video?\n+            self._check_formats(tw_formats, video_id)\n \n         video_elements = self._search_regex(\n             r'(?s)<div class=\"video-elements\">(.*?)</div>',\n             webpage, 'video elements', default=None)\n-        if not video_elements:\n+        if not (video_elements or tw_formats or media_url):\n             raise ExtractorError(\n-                'No sources found for video %s. Maybe an image?' % video_id,\n+                'No sources found for video %s. Maybe a plain image?' % video_id,\n                 expected=True)\n \n-        formats = []\n-        for m in re.finditer(r'<source\\s+src=\"(?P<src>[^\"]+)\"\\s+type=\"(?P<type>[^\"]+)\"', video_elements):\n-            formats.append({\n-                'format_id': m.group('type').partition('/')[2],\n-                'url': self._proto_relative_url(m.group('src')),\n-                'ext': mimetype2ext(m.group('type')),\n-                'width': width,\n-                'height': height,\n+        def mung_format(fmt, *extra):\n+            fmt.update({\n                 'http_headers': {\n                     'User-Agent': 'youtube-dl (like wget)',\n                 },\n             })\n+            for d in extra:\n+                fmt.update(d)\n+            return fmt\n \n-        gif_json = self._search_regex(\n-            r'(?s)var\\s+videoItem\\s*=\\s*(\\{.*?\\})',\n-            webpage, 'GIF code', fatal=False)\n-        if gif_json:\n-            gifd = self._parse_json(\n-                gif_json, video_id, transform_source=js_to_json)\n-            formats.append({\n-                'format_id': 'gif',\n-                'preference': -10,\n-                'width': width,\n-                'height': height,\n-                'ext': 'gif',\n-                'acodec': 'none',\n-                'vcodec': 'gif',\n-                'container': 'gif',\n-                'url': self._proto_relative_url(gifd['gifUrl']),\n-                'filesize': gifd.get('size'),\n-                'http_headers': {\n-                    'User-Agent': 'youtube-dl (like wget)',\n-                },\n-            })\n+        if video_elements:\n+            def og_get_size(media_type):\n+                return dict((p, int_or_none(self._og_search_property(\n+                    ':'.join((media_type, p)), webpage, default=None)))\n+                    for p in ('width', 'height'))\n+\n+            size = og_get_size('video')\n+            if all(v is None for v in size.values()):\n+                size = og_get_size('image')\n+\n+            formats = traverse_obj(\n+                re.finditer(r'<source\\s+src=\"(?P<src>[^\"]+)\"\\s+type=\"(?P<type>[^\"]+)\"', video_elements),\n+                (Ellipsis, {\n+                    'format_id': ('type', T(lambda s: s.partition('/')[2])),\n+                    'url': ('src', T(self._proto_relative_url)),\n+                    'ext': ('type', T(mimetype2ext)),\n+                }, T(lambda f: mung_format(f, size))))\n+\n+            gif_json = self._search_regex(\n+                r'(?s)var\\s+videoItem\\s*=\\s*(\\{.*?\\})',\n+                webpage, 'GIF code', fatal=False)\n+            MUST_BRANCH = (None, T(lambda _: None))\n+            formats.extend(traverse_obj(gif_json, (\n+                T(lambda j: self._parse_json(\n+                    j, video_id, transform_source=js_to_json, fatal=False)), {\n+                        'url': ('gifUrl', T(self._proto_relative_url)),\n+                        'filesize': ('size', T(int_or_none)),\n+                }, T(lambda f: mung_format(f, size, {\n+                    'format_id': 'gif',\n+                    'preference': -10,  # gifs are worse than videos\n+                    'ext': 'gif',\n+                    'acodec': 'none',\n+                    'vcodec': 'gif',\n+                    'container': 'gif',\n+                })), MUST_BRANCH)))\n+        else:\n+            formats = []\n+\n+        # maybe add formats from JSON or page Twitter metadata\n+        if not any((u == media_url) for u in traverse_obj(formats, (Ellipsis, 'url'))):\n+            formats.append(mung_format(media_fmt))\n+        tw_url = traverse_obj(tw_formats, (0, 'url'))\n+        if not any((u == tw_url) for u in traverse_obj(formats, (Ellipsis, 'url'))):\n+            formats.extend(mung_format(f) for f in tw_formats)\n \n         self._sort_formats(formats)\n \n-        return {\n+        return merge_dicts(traverse_obj(data, {\n+            'uploader_id': ('account_id', T(txt_or_none),\n+                            T(lambda a: a if int_or_none(a) != 0 else None)),\n+            'uploader': ('account', 'username', T(txt_or_none)),\n+            'uploader_url': ('account', 'avatar_url', T(url_or_none)),\n+            'like_count': ('upvote_count', T(int_or_none)),\n+            'dislike_count': ('downvote_count', T(int_or_none)),\n+            'comment_count': ('comment_count', T(int_or_none)),\n+            'age_limit': ('is_mature', T(lambda x: 18 if x else None)),\n+            'timestamp': (('updated_at', 'created_at'), T(parse_iso8601)),\n+            'release_timestamp': ('created_at', T(parse_iso8601)),\n+        }, get_all=False), traverse_obj(data, ('media', 0, 'metadata', {\n+            'title': ('title', T(txt_or_none)),\n+            'description': ('description', T(self.get_description)),\n+            'duration': ('duration', T(float_or_none)),\n+            'timestamp': (('updated_at', 'created_at'), T(parse_iso8601)),\n+            'release_timestamp': ('created_at', T(parse_iso8601)),\n+        })), {\n             'id': video_id,\n             'formats': formats,\n-            'title': self._og_search_title(webpage, default=video_id),\n-        }\n+            'title': self._og_search_title(webpage, default='Imgur video ' + video_id),\n+            'description': self.get_description(self._og_search_description(webpage)),\n+            'thumbnail': url_or_none(self._html_search_meta('thumbnailUrl', webpage, default=None)),\n+        })\n+\n+\n+class ImgurGalleryBaseIE(ImgurBaseIE):\n+    _GALLERY = True\n+\n+    def _real_extract(self, url):\n+        gallery_id = self._match_id(url)\n \n+        data = self._call_api('albums', gallery_id, fatal=False, expected_status=404)\n \n-class ImgurGalleryIE(InfoExtractor):\n+        info = traverse_obj(data, {\n+            'title': ('title', T(txt_or_none)),\n+            'description': ('description', T(self.get_description)),\n+        })\n+\n+        if traverse_obj(data, 'is_album'):\n+\n+            def yield_media_ids():\n+                for m_id in traverse_obj(data, (\n+                        'media', lambda _, v: v.get('type') == 'video' or v['metadata']['is_animated'],\n+                        'id', T(txt_or_none))):\n+                    yield m_id\n+\n+            # if a gallery with exactly one video, apply album metadata to video\n+            media_id = (\n+                self._GALLERY\n+                and traverse_obj(data, ('image_count', T(lambda c: c == 1)))\n+                and next(yield_media_ids(), None))\n+\n+            if not media_id:\n+                result = self.playlist_result(\n+                    map(self._imgur_result, yield_media_ids()), gallery_id)\n+                result.update(info)\n+                return result\n+            gallery_id = media_id\n+\n+        result = self._imgur_result(gallery_id)\n+        info['_type'] = 'url_transparent'\n+        result.update(info)\n+        return result\n+\n+\n+class ImgurGalleryIE(ImgurGalleryBaseIE):\n     IE_NAME = 'imgur:gallery'\n     _VALID_URL = r'https?://(?:i\\.)?imgur\\.com/(?:gallery|(?:t(?:opic)?|r)/[^/]+)/(?P<id>[a-zA-Z0-9]+)'\n \n@@ -106,49 +272,93 @@ class ImgurGalleryIE(InfoExtractor):\n             'title': 'Adding faces make every GIF better',\n         },\n         'playlist_count': 25,\n+        'skip': 'Zoinks! You\\'ve taken a wrong turn.',\n     }, {\n+        # TODO: static images - replace with animated/video gallery\n         'url': 'http://imgur.com/topic/Aww/ll5Vk',\n         'only_matching': True,\n     }, {\n         'url': 'https://imgur.com/gallery/YcAQlkx',\n+        'add_ies': ['Imgur'],\n         'info_dict': {\n             'id': 'YcAQlkx',\n             'ext': 'mp4',\n             'title': 'Classic Steve Carell gif...cracks me up everytime....damn the repost downvotes....',\n-        }\n+            'timestamp': 1358554297,\n+            'upload_date': '20130119',\n+            'uploader_id': '1648642',\n+            'uploader': 'wittyusernamehere',\n+        },\n     }, {\n+        # TODO: static image - replace with animated/video gallery\n         'url': 'http://imgur.com/topic/Funny/N8rOudd',\n         'only_matching': True,\n     }, {\n         'url': 'http://imgur.com/r/aww/VQcQPhM',\n-        'only_matching': True,\n+        'add_ies': ['Imgur'],\n+        'info_dict': {\n+            'id': 'VQcQPhM',\n+            'ext': 'mp4',\n+            'title': 'The boss is here',\n+            'timestamp': 1476494751,\n+            'upload_date': '20161015',\n+            'uploader_id': '19138530',\n+            'uploader': 'thematrixcam',\n+        },\n+    },\n+        # from PR #16674\n+        {\n+        'url': 'https://imgur.com/t/unmuted/6lAn9VQ',\n+        'info_dict': {\n+            'id': '6lAn9VQ',\n+            'title': 'Penguins !',\n+        },\n+        'playlist_count': 3,\n+    }, {\n+        'url': 'https://imgur.com/t/unmuted/kx2uD3C',\n+        'add_ies': ['Imgur'],\n+        'info_dict': {\n+            'id': 'ZVMv45i',\n+            'ext': 'mp4',\n+            'title': 'Intruder',\n+            'timestamp': 1528129683,\n+            'upload_date': '20180604',\n+        },\n+    }, {\n+        'url': 'https://imgur.com/t/unmuted/wXSK0YH',\n+        'add_ies': ['Imgur'],\n+        'info_dict': {\n+            'id': 'JCAP4io',\n+            'ext': 'mp4',\n+            'title': 're:I got the blues$',\n+            'description': 'Luka\u2019s vocal stylings.\\n\\nFP edit: don\u2019t encourage me. I\u2019ll never stop posting Luka and friends.',\n+            'timestamp': 1527809525,\n+            'upload_date': '20180531',\n+        },\n     }]\n \n-    def _real_extract(self, url):\n-        gallery_id = self._match_id(url)\n-\n-        data = self._download_json(\n-            'https://imgur.com/gallery/%s.json' % gallery_id,\n-            gallery_id)['data']['image']\n \n-        if data.get('is_album'):\n-            entries = [\n-                self.url_result('http://imgur.com/%s' % image['hash'], ImgurIE.ie_key(), image['hash'])\n-                for image in data['album_images']['images'] if image.get('hash')]\n-            return self.playlist_result(entries, gallery_id, data.get('title'), data.get('description'))\n-\n-        return self.url_result('http://imgur.com/%s' % gallery_id, ImgurIE.ie_key(), gallery_id)\n-\n-\n-class ImgurAlbumIE(ImgurGalleryIE):\n+class ImgurAlbumIE(ImgurGalleryBaseIE):\n     IE_NAME = 'imgur:album'\n     _VALID_URL = r'https?://(?:i\\.)?imgur\\.com/a/(?P<id>[a-zA-Z0-9]+)'\n-\n+    _GALLERY = False\n     _TESTS = [{\n+        # TODO: only static images - replace with animated/video gallery\n         'url': 'http://imgur.com/a/j6Orj',\n+        'only_matching': True,\n+    },\n+        # from PR #21693\n+        {\n+        'url': 'https://imgur.com/a/iX265HX',\n+        'info_dict': {\n+            'id': 'iX265HX',\n+            'title': 'enen-no-shouboutai'\n+        },\n+        'playlist_count': 2,\n+    }, {\n+        'url': 'https://imgur.com/a/8pih2Ed',\n         'info_dict': {\n-            'id': 'j6Orj',\n-            'title': 'A Literary Analysis of \"Star Wars: The Force Awakens\"',\n+            'id': '8pih2Ed'\n         },\n-        'playlist_count': 12,\n+        'playlist_mincount': 1,\n     }]\ndiff --git a/youtube_dl/extractor/infoq.py b/youtube_dl/extractor/infoq.py\nindex 0a70a1fb442..60b02b69958 100644\n--- a/youtube_dl/extractor/infoq.py\n+++ b/youtube_dl/extractor/infoq.py\n@@ -1,6 +1,9 @@\n # coding: utf-8\n \n from __future__ import unicode_literals\n+from ..utils import (\n+    ExtractorError,\n+)\n \n from ..compat import (\n     compat_b64decode,\n@@ -90,7 +93,11 @@ def _extract_http_video(self, webpage):\n         }]\n \n     def _extract_http_audio(self, webpage, video_id):\n-        fields = self._form_hidden_inputs('mp3Form', webpage)\n+        try:\n+            fields = self._form_hidden_inputs('mp3Form', webpage)\n+        except ExtractorError:\n+            fields = {}\n+\n         http_audio_url = fields.get('filename')\n         if not http_audio_url:\n             return []\ndiff --git a/youtube_dl/extractor/itv.py b/youtube_dl/extractor/itv.py\nindex e86c40b42e5..c64af3be61f 100644\n--- a/youtube_dl/extractor/itv.py\n+++ b/youtube_dl/extractor/itv.py\n@@ -3,123 +3,266 @@\n \n import json\n import re\n+import sys\n \n from .common import InfoExtractor\n from .brightcove import BrightcoveNewIE\n+from ..compat import (\n+    compat_HTTPError,\n+    compat_integer_types,\n+    compat_kwargs,\n+    compat_urlparse,\n+)\n from ..utils import (\n     clean_html,\n     determine_ext,\n+    error_to_compat_str,\n     extract_attributes,\n-    get_element_by_class,\n-    JSON_LD_RE,\n+    ExtractorError,\n+    get_element_by_attribute,\n+    int_or_none,\n     merge_dicts,\n     parse_duration,\n+    parse_iso8601,\n+    remove_start,\n     smuggle_url,\n+    strip_or_none,\n+    traverse_obj,\n     url_or_none,\n+    urljoin,\n )\n \n \n-class ITVIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?itv\\.com/hub/[^/]+/(?P<id>[0-9a-zA-Z]+)'\n-    _GEO_COUNTRIES = ['GB']\n+class ITVBaseIE(InfoExtractor):\n+\n+    def _search_nextjs_data(self, webpage, video_id, **kw):\n+        transform_source = kw.pop('transform_source', None)\n+        fatal = kw.pop('fatal', True)\n+        return self._parse_json(\n+            self._search_regex(\n+                r'''<script\\b[^>]+\\bid=('|\")__NEXT_DATA__\\1[^>]*>(?P<js>[^<]+)</script>''',\n+                webpage, 'next.js data', group='js', fatal=fatal, **kw),\n+            video_id, transform_source=transform_source, fatal=fatal)\n+\n+    def __handle_request_webpage_error(self, err, video_id=None, errnote=None, fatal=True):\n+        if errnote is False:\n+            return False\n+        if errnote is None:\n+            errnote = 'Unable to download webpage'\n+\n+        errmsg = '%s: %s' % (errnote, error_to_compat_str(err))\n+        if fatal:\n+            raise ExtractorError(errmsg, sys.exc_info()[2], cause=err, video_id=video_id)\n+        else:\n+            self._downloader.report_warning(errmsg)\n+            return False\n+\n+    @staticmethod\n+    def _vanilla_ua_header():\n+        return {'User-Agent': 'Mozilla/5.0'}\n+\n+    def _download_webpage_handle(self, url, video_id, *args, **kwargs):\n+        # specialised to (a) use vanilla UA (b) detect geo-block\n+        params = self._downloader.params\n+        nkwargs = {}\n+        if (\n+                'user_agent' not in params\n+                and not any(re.match(r'(?i)user-agent\\s*:', h)\n+                            for h in (params.get('headers') or []))\n+                and 'User-Agent' not in (kwargs.get('headers') or {})):\n+\n+            kwargs.setdefault('headers', {})\n+            kwargs['headers'] = self._vanilla_ua_header()\n+            nkwargs = kwargs\n+        if kwargs.get('expected_status') is not None:\n+            exp = kwargs['expected_status']\n+            if isinstance(exp, compat_integer_types):\n+                exp = [exp]\n+            if isinstance(exp, (list, tuple)) and 403 not in exp:\n+                kwargs['expected_status'] = [403]\n+                kwargs['expected_status'].extend(exp)\n+                nkwargs = kwargs\n+        else:\n+            kwargs['expected_status'] = 403\n+            nkwargs = kwargs\n+\n+        if nkwargs:\n+            kwargs = compat_kwargs(kwargs)\n+\n+        ret = super(ITVBaseIE, self)._download_webpage_handle(url, video_id, *args, **kwargs)\n+        if ret is False:\n+            return ret\n+        webpage, urlh = ret\n+\n+        if urlh.getcode() == 403:\n+            # geo-block error is like this, with an unnecessary 'Of':\n+            # '{\\n  \"Message\" : \"Request Originated Outside Of Allowed Geographic Region\",\\\n+            # \\n  \"TransactionId\" : \"oas-magni-475082-xbYF0W\"\\n}'\n+            if '\"Request Originated Outside Of Allowed Geographic Region\"' in webpage:\n+                self.raise_geo_restricted(countries=['GB'])\n+            ret = self.__handle_request_webpage_error(\n+                compat_HTTPError(urlh.geturl(), 403, 'HTTP Error 403: Forbidden', urlh.headers, urlh),\n+                fatal=kwargs.get('fatal'))\n+\n+        return ret\n+\n+\n+class ITVIE(ITVBaseIE):\n+    _VALID_URL = r'https?://(?:www\\.)?itv\\.com/(?:(?P<w>watch)|hub)/[^/]+/(?(w)[\\w-]+/)(?P<id>\\w+)'\n+    _IE_DESC = 'ITVX'\n     _TESTS = [{\n+        'note': 'Hub URLs redirect to ITVX',\n         'url': 'https://www.itv.com/hub/liar/2a4547a0012',\n-        'info_dict': {\n-            'id': '2a4547a0012',\n-            'ext': 'mp4',\n-            'title': 'Liar - Series 2 - Episode 6',\n-            'description': 'md5:d0f91536569dec79ea184f0a44cca089',\n-            'series': 'Liar',\n-            'season_number': 2,\n-            'episode_number': 6,\n-        },\n-        'params': {\n-            # m3u8 download\n-            'skip_download': True,\n-        },\n+        'only_matching': True,\n     }, {\n-        # unavailable via data-playlist-url\n+        'note': 'Hub page unavailable via data-playlist-url (404 now)',\n         'url': 'https://www.itv.com/hub/through-the-keyhole/2a2271a0033',\n         'only_matching': True,\n     }, {\n-        # InvalidVodcrid\n+        'note': 'Hub page with InvalidVodcrid (404 now)',\n         'url': 'https://www.itv.com/hub/james-martins-saturday-morning/2a5159a0034',\n         'only_matching': True,\n     }, {\n-        # ContentUnavailable\n+        'note': 'Hub page with ContentUnavailable (404 now)',\n         'url': 'https://www.itv.com/hub/whos-doing-the-dishes/2a2898a0024',\n         'only_matching': True,\n-    }]\n+    }, {\n+        'note': 'ITVX, or itvX, show',\n+        'url': 'https://www.itv.com/watch/vera/1a7314/1a7314a0014',\n+        'md5': 'bd0ad666b2c058fffe7d036785880064',\n+        'info_dict': {\n+            'id': '1a7314a0014',\n+            'ext': 'mp4',\n+            'title': 'Vera - Series 3 - Episode 4 - Prodigal Son',\n+            'description': 'Vera and her team investigate the fatal stabbing of an ex-Met police officer outside a busy Newcastle nightclub - but there aren\\'t many clues.',\n+            'timestamp': 1653591600,\n+            'upload_date': '20220526',\n+            'uploader': 'ITVX',\n+            'thumbnail': r're:https://\\w+\\.itv\\.com/images/(?:\\w+/)+\\d+x\\d+\\?',\n+            'duration': 5340.8,\n+            'age_limit': 16,\n+            'series': 'Vera',\n+            'series_number': 3,\n+            'episode': 'Prodigal Son',\n+            'episode_number': 4,\n+            'channel': 'ITV3',\n+            'categories': list,\n+        },\n+        'params': {\n+            # m3u8 download\n+            # 'skip_download': True,\n+        },\n+        'skip': 'only available in UK',\n+    }, {\n+        'note': 'Latest ITV news bulletin: details change daily',\n+        'url': 'https://www.itv.com/watch/news/varies-but-is-not-checked/6js5d0f',\n+        'info_dict': {\n+            'id': '6js5d0f',\n+            'ext': 'mp4',\n+            'title': r're:The latest ITV News headlines - \\S.+',\n+            'description': r'''re:.* today's top stories from the ITV News team.$''',\n+            'timestamp': int,\n+            'upload_date': r're:2\\d\\d\\d(?:0[1-9]|1[0-2])(?:[012][1-9]|3[01])',\n+            'uploader': 'ITVX',\n+            'thumbnail': r're:https://images\\.ctfassets\\.net/(?:\\w+/)+[\\w.]+\\.(?:jpg|png)',\n+            'duration': float,\n+            'age_limit': None,\n+        },\n+        'params': {\n+            # variable download\n+            # 'skip_download': True,\n+        },\n+        'skip': 'only available in UK',\n+    }\n+    ]\n+\n+    def _og_extract(self, webpage, require_title=False):\n+        return {\n+            'title': self._og_search_title(webpage, fatal=require_title),\n+            'description': self._og_search_description(webpage, default=None),\n+            'thumbnail': self._og_search_thumbnail(webpage, default=None),\n+            'uploader': self._og_search_property('site_name', webpage, default=None),\n+        }\n \n     def _real_extract(self, url):\n         video_id = self._match_id(url)\n+\n         webpage = self._download_webpage(url, video_id)\n+\n+        # now quite different params!\n         params = extract_attributes(self._search_regex(\n-            r'(?s)(<[^>]+id=\"video\"[^>]*>)', webpage, 'params'))\n+            r'''(<[^>]+\\b(?:class|data-testid)\\s*=\\s*(\"|')genie-container\\2[^>]*>)''',\n+            webpage, 'params'))\n+\n+        ios_playlist_url = traverse_obj(\n+            params, 'data-video-id', 'data-video-playlist',\n+            get_all=False, expected_type=url_or_none)\n \n-        ios_playlist_url = params.get('data-video-playlist') or params['data-video-id']\n-        hmac = params['data-video-hmac']\n         headers = self.geo_verification_headers()\n         headers.update({\n             'Accept': 'application/vnd.itv.vod.playlist.v2+json',\n             'Content-Type': 'application/json',\n-            'hmac': hmac.upper(),\n         })\n         ios_playlist = self._download_json(\n             ios_playlist_url, video_id, data=json.dumps({\n                 'user': {\n-                    'itvUserId': '',\n                     'entitlements': [],\n-                    'token': ''\n                 },\n                 'device': {\n-                    'manufacturer': 'Safari',\n-                    'model': '5',\n+                    'manufacturer': 'Mobile Safari',\n+                    'model': '5.1',\n                     'os': {\n-                        'name': 'Windows NT',\n-                        'version': '6.1',\n-                        'type': 'desktop'\n+                        'name': 'iOS',\n+                        'version': '5.0',\n+                        'type': ' mobile'\n                     }\n                 },\n                 'client': {\n                     'version': '4.1',\n-                    'id': 'browser'\n+                    'id': 'browser',\n+                    'supportsAdPods': True,\n+                    'service': 'itv.x',\n+                    'appversion': '2.43.28',\n                 },\n                 'variantAvailability': {\n+                    'player': 'hls',\n                     'featureset': {\n                         'min': ['hls', 'aes', 'outband-webvtt'],\n                         'max': ['hls', 'aes', 'outband-webvtt']\n                     },\n-                    'platformTag': 'dotcom'\n+                    'platformTag': 'mobile'\n                 }\n             }).encode(), headers=headers)\n         video_data = ios_playlist['Playlist']['Video']\n-        ios_base_url = video_data.get('Base')\n+        ios_base_url = traverse_obj(video_data, 'Base', expected_type=url_or_none)\n+\n+        media_url = (\n+            (lambda u: url_or_none(urljoin(ios_base_url, u)))\n+            if ios_base_url else url_or_none)\n \n         formats = []\n-        for media_file in (video_data.get('MediaFiles') or []):\n-            href = media_file.get('Href')\n+        for media_file in traverse_obj(video_data, 'MediaFiles', expected_type=list) or []:\n+            href = traverse_obj(media_file, 'Href', expected_type=media_url)\n             if not href:\n                 continue\n-            if ios_base_url:\n-                href = ios_base_url + href\n             ext = determine_ext(href)\n             if ext == 'm3u8':\n                 formats.extend(self._extract_m3u8_formats(\n-                    href, video_id, 'mp4', entry_protocol='m3u8_native',\n+                    href, video_id, 'mp4', entry_protocol='m3u8',\n                     m3u8_id='hls', fatal=False))\n+\n             else:\n                 formats.append({\n                     'url': href,\n                 })\n         self._sort_formats(formats)\n+        for f in formats:\n+            f.setdefault('http_headers', {})\n+            f['http_headers'].update(self._vanilla_ua_header())\n \n         subtitles = {}\n-        subs = video_data.get('Subtitles') or []\n-        for sub in subs:\n-            if not isinstance(sub, dict):\n-                continue\n-            href = url_or_none(sub.get('Href'))\n+        for sub in traverse_obj(video_data, 'Subtitles', expected_type=list) or []:\n+            href = traverse_obj(sub, 'Href', expected_type=url_or_none)\n             if not href:\n                 continue\n             subtitles.setdefault('en', []).append({\n@@ -127,59 +270,132 @@ def _real_extract(self, url):\n                 'ext': determine_ext(href, 'vtt'),\n             })\n \n-        info = self._search_json_ld(webpage, video_id, default={})\n-        if not info:\n-            json_ld = self._parse_json(self._search_regex(\n-                JSON_LD_RE, webpage, 'JSON-LD', '{}',\n-                group='json_ld'), video_id, fatal=False)\n-            if json_ld and json_ld.get('@type') == 'BreadcrumbList':\n-                for ile in (json_ld.get('itemListElement:') or []):\n-                    item = ile.get('item:') or {}\n-                    if item.get('@type') == 'TVEpisode':\n-                        item['@context'] = 'http://schema.org'\n-                        info = self._json_ld(item, video_id, fatal=False) or {}\n-                        break\n+        next_data = self._search_nextjs_data(webpage, video_id, fatal=False, default='{}')\n+        video_data.update(traverse_obj(next_data, ('props', 'pageProps', ('title', 'episode')), expected_type=dict)[0] or {})\n+        title = traverse_obj(video_data, 'headerTitle', 'episodeTitle')\n+        info = self._og_extract(webpage, require_title=not title)\n+        tn = info.pop('thumbnail', None)\n+        if tn:\n+            info['thumbnails'] = [{'url': tn}]\n+\n+        # num. episode title\n+        num_ep_title = video_data.get('numberedEpisodeTitle')\n+        if not num_ep_title:\n+            num_ep_title = clean_html(get_element_by_attribute('data-testid', 'episode-hero-description-strong', webpage))\n+            num_ep_title = num_ep_title and num_ep_title.rstrip(' -')\n+        ep_title = strip_or_none(\n+            video_data.get('episodeTitle')\n+            or (num_ep_title.split('.', 1)[-1] if num_ep_title else None))\n+        title = title or re.sub(r'\\s+-\\s+ITVX$', '', info['title'])\n+        if ep_title and ep_title != title:\n+            title = title + ' - ' + ep_title\n+\n+        def get_thumbnails():\n+            tns = []\n+            for w, x in (traverse_obj(video_data, ('imagePresets'), expected_type=dict) or {}).items():\n+                if isinstance(x, dict):\n+                    for y, z in x.items():\n+                        tns.append({'id': w + '_' + y, 'url': z})\n+            return tns or None\n+\n+        video_str = lambda *x: traverse_obj(\n+            video_data, *x, get_all=False, expected_type=strip_or_none)\n \n         return merge_dicts({\n             'id': video_id,\n-            'title': self._html_search_meta(['og:title', 'twitter:title'], webpage),\n+            'title': title,\n             'formats': formats,\n             'subtitles': subtitles,\n-            'duration': parse_duration(video_data.get('Duration')),\n-            'description': clean_html(get_element_by_class('episode-info__synopsis', webpage)),\n+            # parsing hh:mm:ss:nnn not yet patched\n+            'duration': parse_duration(re.sub(r'(\\d{2})(:)(\\d{3}$)', r'\\1.\\3', video_data.get('Duration') or '')),\n+            'description': video_str('synopsis'),\n+            'timestamp': traverse_obj(video_data, 'broadcastDateTime', 'dateTime', expected_type=parse_iso8601),\n+            'thumbnails': get_thumbnails(),\n+            'series': video_str('showTitle', 'programmeTitle'),\n+            'series_number': int_or_none(video_data.get('seriesNumber')),\n+            'episode': ep_title,\n+            'episode_number': int_or_none((num_ep_title or '').split('.')[0]),\n+            'channel': video_str('channel'),\n+            'categories': traverse_obj(video_data, ('categories', 'formatted'), expected_type=list),\n+            'age_limit': {False: 16, True: 0}.get(video_data.get('isChildrenCategory')),\n         }, info)\n \n \n-class ITVBTCCIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?itv\\.com/btcc/(?:[^/]+/)*(?P<id>[^/?#&]+)'\n-    _TEST = {\n-        'url': 'http://www.itv.com/btcc/races/btcc-2018-all-the-action-from-brands-hatch',\n+class ITVBTCCIE(ITVBaseIE):\n+    _VALID_URL = r'https?://(?:www\\.)?itv\\.com/(?!(?:watch|hub)/)(?:[^/]+/)+(?P<id>[^/?#&]+)'\n+    _IE_DESC = 'ITV articles: News, British Touring Car Championship'\n+    _TESTS = [{\n+        'note': 'British Touring Car Championship',\n+        'url': 'https://www.itv.com/btcc/articles/btcc-2018-all-the-action-from-brands-hatch',\n         'info_dict': {\n             'id': 'btcc-2018-all-the-action-from-brands-hatch',\n             'title': 'BTCC 2018: All the action from Brands Hatch',\n         },\n         'playlist_mincount': 9,\n-    }\n-    BRIGHTCOVE_URL_TEMPLATE = 'http://players.brightcove.net/1582188683001/HkiHLnNRx_default/index.html?videoId=%s'\n+    }, {\n+        'note': 'redirects to /btcc/articles/...',\n+        'url': 'http://www.itv.com/btcc/races/btcc-2018-all-the-action-from-brands-hatch',\n+        'only_matching': True,\n+    }, {\n+        'note': 'news article',\n+        'url': 'https://www.itv.com/news/wales/2020-07-23/sean-fletcher-shows-off-wales-coastline-in-new-itv-series-as-british-tourists-opt-for-staycations',\n+        'info_dict': {\n+            'id': 'sean-fletcher-shows-off-wales-coastline-in-new-itv-series-as-british-tourists-opt-for-staycations',\n+            'title': '''Sean Fletcher on why Wales' coastline should be your 'staycation' destination | ITV News''',\n+        },\n+        'playlist_mincount': 1,\n+    }]\n+\n+    # should really be a class var of the BC IE\n+    BRIGHTCOVE_URL_TEMPLATE = 'http://players.brightcove.net/%s/%s_default/index.html?videoId=%s'\n+    BRIGHTCOVE_ACCOUNT = '1582188683001'\n+    BRIGHTCOVE_PLAYER = 'HkiHLnNRx'\n \n     def _real_extract(self, url):\n         playlist_id = self._match_id(url)\n \n-        webpage = self._download_webpage(url, playlist_id)\n-\n-        entries = [\n-            self.url_result(\n-                smuggle_url(self.BRIGHTCOVE_URL_TEMPLATE % video_id, {\n-                    # ITV does not like some GB IP ranges, so here are some\n-                    # IP blocks it accepts\n-                    'geo_ip_blocks': [\n-                        '193.113.0.0/16', '54.36.162.0/23', '159.65.16.0/21'\n-                    ],\n-                    'referrer': url,\n-                }),\n-                ie=BrightcoveNewIE.ie_key(), video_id=video_id)\n-            for video_id in re.findall(r'data-video-id=[\"\\'](\\d+)', webpage)]\n+        webpage, urlh = self._download_webpage_handle(url, playlist_id)\n+        link = compat_urlparse.urlparse(urlh.geturl()).path.strip('/')\n+\n+        next_data = self._search_nextjs_data(webpage, playlist_id, fatal=False, default='{}')\n+        path_prefix = compat_urlparse.urlparse(next_data.get('assetPrefix') or '').path.strip('/')\n+        link = remove_start(link, path_prefix).strip('/')\n+\n+        content = traverse_obj(\n+            next_data, ('props', 'pageProps', Ellipsis),\n+            expected_type=lambda x: x if x['link'] == link else None,\n+            get_all=False, default={})\n+        content = traverse_obj(\n+            content, ('body', 'content', Ellipsis, 'data'),\n+            expected_type=lambda x: x if x.get('name') == 'Brightcove' or x.get('type') == 'Brightcove' else None)\n+\n+        contraband = {\n+            # ITV does not like some GB IP ranges, so here are some\n+            # IP blocks it accepts\n+            'geo_ip_blocks': [\n+                '193.113.0.0/16', '54.36.162.0/23', '159.65.16.0/21'\n+            ],\n+            'referrer': urlh.geturl(),\n+        }\n+\n+        def entries():\n+\n+            for data in content or []:\n+                video_id = data.get('id')\n+                if not video_id:\n+                    continue\n+                account = data.get('accountId') or self.BRIGHTCOVE_ACCOUNT\n+                player = data.get('playerId') or self.BRIGHTCOVE_PLAYER\n+                yield self.url_result(\n+                    smuggle_url(self.BRIGHTCOVE_URL_TEMPLATE % (account, player, video_id), contraband),\n+                    ie=BrightcoveNewIE.ie_key(), video_id=video_id)\n+\n+            # obsolete ?\n+            for video_id in re.findall(r'''data-video-id=[\"'](\\d+)''', webpage):\n+                yield self.url_result(\n+                    smuggle_url(self.BRIGHTCOVE_URL_TEMPLATE % (self.BRIGHTCOVE_ACCOUNT, self.BRIGHTCOVE_PLAYER, video_id), contraband),\n+                    ie=BrightcoveNewIE.ie_key(), video_id=video_id)\n \n         title = self._og_search_title(webpage, fatal=False)\n \n-        return self.playlist_result(entries, playlist_id, title)\n+        return self.playlist_result(entries(), playlist_id, title)\ndiff --git a/youtube_dl/extractor/kaltura.py b/youtube_dl/extractor/kaltura.py\nindex c731612c4ef..6d4d9339478 100644\n--- a/youtube_dl/extractor/kaltura.py\n+++ b/youtube_dl/extractor/kaltura.py\n@@ -373,5 +373,5 @@ def sign_url(unsigned_url):\n             'duration': info.get('duration'),\n             'timestamp': info.get('createdAt'),\n             'uploader_id': info.get('userId') if info.get('userId') != 'None' else None,\n-            'view_count': info.get('plays'),\n+            'view_count': int_or_none(info.get('plays')),\n         }\ndiff --git a/youtube_dl/extractor/kommunetv.py b/youtube_dl/extractor/kommunetv.py\nnew file mode 100644\nindex 00000000000..91d06a74f2e\n--- /dev/null\n+++ b/youtube_dl/extractor/kommunetv.py\n@@ -0,0 +1,35 @@\n+# coding: utf-8\n+from __future__ import unicode_literals\n+\n+from .common import InfoExtractor\n+from ..utils import update_url\n+\n+\n+class KommunetvIE(InfoExtractor):\n+    _VALID_URL = r'https://(\\w+).kommunetv.no/archive/(?P<id>\\w+)'\n+    _TEST = {\n+        'url': 'https://oslo.kommunetv.no/archive/921',\n+        'md5': '5f102be308ee759be1e12b63d5da4bbc',\n+        'info_dict': {\n+            'id': '921',\n+            'title': 'Bystyrem\u00f8te',\n+            'ext': 'mp4'\n+        }\n+    }\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+        headers = {\n+            'Accept': 'application/json'\n+        }\n+        data = self._download_json('https://oslo.kommunetv.no/api/streams?streamType=1&id=%s' % video_id, video_id, headers=headers)\n+        title = data['stream']['title']\n+        file = data['playlist'][0]['playlist'][0]['file']\n+        url = update_url(file, query=None, fragment=None)\n+        formats = self._extract_m3u8_formats(url, video_id, ext='mp4', entry_protocol='m3u8_native', m3u8_id='hls', fatal=False)\n+        self._sort_formats(formats)\n+        return {\n+            'id': video_id,\n+            'formats': formats,\n+            'title': title\n+        }\ndiff --git a/youtube_dl/extractor/kth.py b/youtube_dl/extractor/kth.py\nnew file mode 100644\nindex 00000000000..b8db461f55a\n--- /dev/null\n+++ b/youtube_dl/extractor/kth.py\n@@ -0,0 +1,31 @@\n+# coding: utf-8\n+from __future__ import unicode_literals\n+\n+from .common import InfoExtractor\n+from ..utils import smuggle_url\n+\n+\n+class KTHIE(InfoExtractor):\n+    _VALID_URL = r'https?://play\\.kth\\.se/(?:[^/]+/)+(?P<id>[a-z0-9_]+)'\n+    _TEST = {\n+        'url': 'https://play.kth.se/media/Lunch+breakA+De+nya+aff%C3%A4rerna+inom+Fordonsdalen/0_uoop6oz9',\n+        'md5': 'd83ada6d00ca98b73243a88efe19e8a6',\n+        'info_dict': {\n+            'id': '0_uoop6oz9',\n+            'ext': 'mp4',\n+            'title': 'md5:bd1d6931facb6828762a33e6ce865f37',\n+            'thumbnail': 're:https?://.+/thumbnail/.+',\n+            'duration': 3516,\n+            'timestamp': 1647345358,\n+            'upload_date': '20220315',\n+            'uploader_id': 'md5:0ec23e33a89e795a4512930c8102509f',\n+        }\n+    }\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+        result = self.url_result(\n+            smuggle_url('kaltura:308:%s' % video_id, {\n+                'service_url': 'https://api.kaltura.nordu.net'}),\n+            'Kaltura')\n+        return result\ndiff --git a/youtube_dl/extractor/manyvids.py b/youtube_dl/extractor/manyvids.py\nindex e8d7163e4ab..75978cfd697 100644\n--- a/youtube_dl/extractor/manyvids.py\n+++ b/youtube_dl/extractor/manyvids.py\n@@ -1,11 +1,16 @@\n # coding: utf-8\n from __future__ import unicode_literals\n \n+import re\n+\n from .common import InfoExtractor\n+from ..compat import compat_str\n from ..utils import (\n     determine_ext,\n+    extract_attributes,\n     int_or_none,\n     str_to_int,\n+    url_or_none,\n     urlencode_postdata,\n )\n \n@@ -20,17 +25,20 @@ class ManyVidsIE(InfoExtractor):\n             'id': '133957',\n             'ext': 'mp4',\n             'title': 'everthing about me (Preview)',\n+            'uploader': 'ellyxxix',\n             'view_count': int,\n             'like_count': int,\n         },\n     }, {\n         # full video\n         'url': 'https://www.manyvids.com/Video/935718/MY-FACE-REVEAL/',\n-        'md5': 'f3e8f7086409e9b470e2643edb96bdcc',\n+        'md5': 'bb47bab0e0802c2a60c24ef079dfe60f',\n         'info_dict': {\n             'id': '935718',\n             'ext': 'mp4',\n             'title': 'MY FACE REVEAL',\n+            'description': 'md5:ec5901d41808b3746fed90face161612',\n+            'uploader': 'Sarah Calanthe',\n             'view_count': int,\n             'like_count': int,\n         },\n@@ -39,17 +47,50 @@ class ManyVidsIE(InfoExtractor):\n     def _real_extract(self, url):\n         video_id = self._match_id(url)\n \n-        webpage = self._download_webpage(url, video_id)\n+        real_url = 'https://www.manyvids.com/video/%s/gtm.js' % (video_id, )\n+        try:\n+            webpage = self._download_webpage(real_url, video_id)\n+        except Exception:\n+            # probably useless fallback\n+            webpage = self._download_webpage(url, video_id)\n+\n+        info = self._search_regex(\n+            r'''(<div\\b[^>]*\\bid\\s*=\\s*(['\"])pageMetaDetails\\2[^>]*>)''',\n+            webpage, 'meta details', default='')\n+        info = extract_attributes(info)\n+\n+        player = self._search_regex(\n+            r'''(<div\\b[^>]*\\bid\\s*=\\s*(['\"])rmpPlayerStream\\2[^>]*>)''',\n+            webpage, 'player details', default='')\n+        player = extract_attributes(player)\n+\n+        video_urls_and_ids = (\n+            (info.get('data-meta-video'), 'video'),\n+            (player.get('data-video-transcoded'), 'transcoded'),\n+            (player.get('data-video-filepath'), 'filepath'),\n+            (self._og_search_video_url(webpage, secure=False, default=None), 'og_video'),\n+        )\n+\n+        def txt_or_none(s, default=None):\n+            return (s.strip() or default) if isinstance(s, compat_str) else default\n+\n+        uploader = txt_or_none(info.get('data-meta-author'))\n \n-        video_url = self._search_regex(\n-            r'data-(?:video-filepath|meta-video)\\s*=s*([\"\\'])(?P<url>(?:(?!\\1).)+)\\1',\n-            webpage, 'video URL', group='url')\n+        def mung_title(s):\n+            if uploader:\n+                s = re.sub(r'^\\s*%s\\s+[|-]' % (re.escape(uploader), ), '', s)\n+            return txt_or_none(s)\n \n-        title = self._html_search_regex(\n-            (r'<span[^>]+class=[\"\\']item-title[^>]+>([^<]+)',\n-             r'<h2[^>]+class=[\"\\']h2 m-0[\"\\'][^>]*>([^<]+)'),\n-            webpage, 'title', default=None) or self._html_search_meta(\n-            'twitter:title', webpage, 'title', fatal=True)\n+        title = (\n+            mung_title(info.get('data-meta-title'))\n+            or self._html_search_regex(\n+                (r'<span[^>]+class=[\"\\']item-title[^>]+>([^<]+)',\n+                 r'<h2[^>]+class=[\"\\']h2 m-0[\"\\'][^>]*>([^<]+)'),\n+                webpage, 'title', default=None)\n+            or self._html_search_meta(\n+                'twitter:title', webpage, 'title', fatal=True))\n+\n+        title = re.sub(r'\\s*[|-]\\s+ManyVids\\s*$', '', title) or title\n \n         if any(p in webpage for p in ('preview_videos', '_preview.mp4')):\n             title += ' (Preview)'\n@@ -62,7 +103,8 @@ def _real_extract(self, url):\n             # Sets some cookies\n             self._download_webpage(\n                 'https://www.manyvids.com/includes/ajax_repository/you_had_me_at_hello.php',\n-                video_id, fatal=False, data=urlencode_postdata({\n+                video_id, note='Setting format cookies', fatal=False,\n+                data=urlencode_postdata({\n                     'mvtoken': mv_token,\n                     'vid': video_id,\n                 }), headers={\n@@ -70,23 +112,56 @@ def _real_extract(self, url):\n                     'X-Requested-With': 'XMLHttpRequest'\n                 })\n \n-        if determine_ext(video_url) == 'm3u8':\n-            formats = self._extract_m3u8_formats(\n-                video_url, video_id, 'mp4', entry_protocol='m3u8_native',\n-                m3u8_id='hls')\n-        else:\n-            formats = [{'url': video_url}]\n+        formats = []\n+        for v_url, fmt in video_urls_and_ids:\n+            v_url = url_or_none(v_url)\n+            if not v_url:\n+                continue\n+            if determine_ext(v_url) == 'm3u8':\n+                formats.extend(self._extract_m3u8_formats(\n+                    v_url, video_id, 'mp4', entry_protocol='m3u8_native',\n+                    m3u8_id='hls'))\n+            else:\n+                formats.append({\n+                    'url': v_url,\n+                    'format_id': fmt,\n+                })\n+\n+        self._remove_duplicate_formats(formats)\n+\n+        for f in formats:\n+            if f.get('height') is None:\n+                f['height'] = int_or_none(\n+                    self._search_regex(r'_(\\d{2,3}[02468])_', f['url'], 'video height', default=None))\n+            if '/preview/' in f['url']:\n+                f['format_id'] = '_'.join(filter(None, (f.get('format_id'), 'preview')))\n+                f['preference'] = -10\n+            if 'transcoded' in f['format_id']:\n+                f['preference'] = f.get('preference', -1) - 1\n+\n+        self._sort_formats(formats)\n+\n+        def get_likes():\n+            likes = self._search_regex(\n+                r'''(<a\\b[^>]*\\bdata-id\\s*=\\s*(['\"])%s\\2[^>]*>)''' % (video_id, ),\n+                webpage, 'likes', default='')\n+            likes = extract_attributes(likes)\n+            return int_or_none(likes.get('data-likes'))\n \n-        like_count = int_or_none(self._search_regex(\n-            r'data-likes=[\"\\'](\\d+)', webpage, 'like count', default=None))\n-        view_count = str_to_int(self._html_search_regex(\n-            r'(?s)<span[^>]+class=\"views-wrapper\"[^>]*>(.+?)</span', webpage,\n-            'view count', default=None))\n+        def get_views():\n+            return str_to_int(self._html_search_regex(\n+                r'''(?s)<span\\b[^>]*\\bclass\\s*=[\"']views-wrapper\\b[^>]+>.+?<span\\b[^>]+>\\s*(\\d[\\d,.]*)\\s*</span>''',\n+                webpage, 'view count', default=None))\n \n         return {\n             'id': video_id,\n             'title': title,\n-            'view_count': view_count,\n-            'like_count': like_count,\n             'formats': formats,\n+            'description': txt_or_none(info.get('data-meta-description')),\n+            'uploader': txt_or_none(info.get('data-meta-author')),\n+            'thumbnail': (\n+                url_or_none(info.get('data-meta-image'))\n+                or url_or_none(player.get('data-video-screenshot'))),\n+            'view_count': get_views(),\n+            'like_count': get_likes(),\n         }\ndiff --git a/youtube_dl/extractor/mediaset.py b/youtube_dl/extractor/mediaset.py\nindex 2c16fc9e218..20048c6abf9 100644\n--- a/youtube_dl/extractor/mediaset.py\n+++ b/youtube_dl/extractor/mediaset.py\n@@ -24,7 +24,7 @@ class MediasetIE(ThePlatformBaseIE):\n                             (?:(?:www|static3)\\.)?mediasetplay\\.mediaset\\.it/\n                             (?:\n                                 (?:video|on-demand|movie)/(?:[^/]+/)+[^/]+_|\n-                                player/index\\.html\\?.*?\\bprogramGuid=\n+                                player(?:/v\\d+)?/index\\.html\\?.*?\\bprogramGuid=\n                             )\n                     )(?P<id>[0-9A-Z]{16,})\n                     '''\n@@ -73,6 +73,10 @@ class MediasetIE(ThePlatformBaseIE):\n         # iframe twitter (from http://www.wittytv.it/se-prima-mi-fidavo-zero/)\n         'url': 'https://static3.mediasetplay.mediaset.it/player/index.html?appKey=5ad3966b1de1c4000d5cec48&programGuid=FAFU000000665104&id=665104',\n         'only_matching': True,\n+    }, {\n+        # embedUrl (from https://www.wittytv.it/amici/est-ce-que-tu-maimes-gabriele-5-dicembre-copia/)\n+        'url': 'https://static3.mediasetplay.mediaset.it/player/v2/index.html?partnerId=wittytv&configId=&programGuid=FD00000000153323&autoplay=true&purl=http://www.wittytv.it/amici/est-ce-que-tu-maimes-gabriele-5-dicembre-copia/',\n+        'only_matching': True,\n     }, {\n         'url': 'mediaset:FAFU000000665924',\n         'only_matching': True,\ndiff --git a/youtube_dl/extractor/minds.py b/youtube_dl/extractor/minds.py\nindex 8e9f0f8254e..e8fd582aa97 100644\n--- a/youtube_dl/extractor/minds.py\n+++ b/youtube_dl/extractor/minds.py\n@@ -78,7 +78,7 @@ def _real_extract(self, url):\n             else:\n                 return self.url_result(entity['perma_url'])\n         else:\n-            assert(entity['subtype'] == 'video')\n+            assert (entity['subtype'] == 'video')\n             video_id = entity_id\n         # 1080p and webm formats available only on the sources array\n         video = self._call_api(\ndiff --git a/youtube_dl/extractor/mixcloud.py b/youtube_dl/extractor/mixcloud.py\nindex 69319857dfb..2b5e2c15c5b 100644\n--- a/youtube_dl/extractor/mixcloud.py\n+++ b/youtube_dl/extractor/mixcloud.py\n@@ -1,3 +1,4 @@\n+# coding: utf-8\n from __future__ import unicode_literals\n \n import itertools\n@@ -10,7 +11,7 @@\n     compat_ord,\n     compat_str,\n     compat_urllib_parse_unquote,\n-    compat_zip\n+    compat_zip as zip,\n )\n from ..utils import (\n     int_or_none,\n@@ -24,7 +25,7 @@ class MixcloudBaseIE(InfoExtractor):\n     def _call_api(self, object_type, object_fields, display_id, username, slug=None):\n         lookup_key = object_type + 'Lookup'\n         return self._download_json(\n-            'https://www.mixcloud.com/graphql', display_id, query={\n+            'https://app.mixcloud.com/graphql', display_id, query={\n                 'query': '''{\n   %s(lookup: {username: \"%s\"%s}) {\n     %s\n@@ -44,7 +45,7 @@ class MixcloudIE(MixcloudBaseIE):\n             'ext': 'm4a',\n             'title': 'Cryptkeeper',\n             'description': 'After quite a long silence from myself, finally another Drum\\'n\\'Bass mix with my favourite current dance floor bangers.',\n-            'uploader': 'Daniel Holbach',\n+            'uploader': 'dholbach',  # was: 'Daniel Holbach',\n             'uploader_id': 'dholbach',\n             'thumbnail': r're:https?://.*\\.jpg',\n             'view_count': int,\n@@ -57,7 +58,7 @@ class MixcloudIE(MixcloudBaseIE):\n             'id': 'gillespeterson_caribou-7-inch-vinyl-mix-chat',\n             'ext': 'mp3',\n             'title': 'Caribou 7 inch Vinyl Mix & Chat',\n-            'description': 'md5:2b8aec6adce69f9d41724647c65875e8',\n+            'description': r're:Last week Dan Snaith aka Caribou swung by the Brownswood.{136}',\n             'uploader': 'Gilles Peterson Worldwide',\n             'uploader_id': 'gillespeterson',\n             'thumbnail': 're:https?://.*',\n@@ -65,6 +66,23 @@ class MixcloudIE(MixcloudBaseIE):\n             'timestamp': 1422987057,\n             'upload_date': '20150203',\n         },\n+        'params': {\n+            'skip_download': '404 not found',\n+        },\n+    }, {\n+        'url': 'https://www.mixcloud.com/gillespeterson/carnival-m%C3%BAsica-popular-brasileira-mix/',\n+        'info_dict': {\n+            'id': 'gillespeterson_carnival-m\u00fasica-popular-brasileira-mix',\n+            'ext': 'm4a',\n+            'title': 'Carnival M\u00fasica Popular Brasileira Mix',\n+            'description': r're:Gilles was recently in Brazil to play at Boiler Room.{208}',\n+            'timestamp': 1454347174,\n+            'upload_date': '20160201',\n+            'uploader': 'Gilles Peterson Worldwide',\n+            'uploader_id': 'gillespeterson',\n+            'thumbnail': 're:https?://.*',\n+            'view_count': int,\n+        },\n     }, {\n         'url': 'https://beta.mixcloud.com/RedLightRadio/nosedrip-15-red-light-radio-01-18-2016/',\n         'only_matching': True,\n@@ -76,10 +94,10 @@ def _decrypt_xor_cipher(key, ciphertext):\n         \"\"\"Encrypt/Decrypt XOR cipher. Both ways are possible because it's XOR.\"\"\"\n         return ''.join([\n             compat_chr(compat_ord(ch) ^ compat_ord(k))\n-            for ch, k in compat_zip(ciphertext, itertools.cycle(key))])\n+            for ch, k in zip(ciphertext, itertools.cycle(key))])\n \n     def _real_extract(self, url):\n-        username, slug = re.match(self._VALID_URL, url).groups()\n+        username, slug = self._match_valid_url(url).groups()\n         username, slug = compat_urllib_parse_unquote(username), compat_urllib_parse_unquote(slug)\n         track_id = '%s_%s' % (username, slug)\n \ndiff --git a/youtube_dl/extractor/motherless.py b/youtube_dl/extractor/motherless.py\nindex ef1e081f20e..d352cb180d4 100644\n--- a/youtube_dl/extractor/motherless.py\n+++ b/youtube_dl/extractor/motherless.py\n@@ -1,3 +1,4 @@\n+# coding: utf-8\n from __future__ import unicode_literals\n \n import datetime\n@@ -71,7 +72,7 @@ class MotherlessIE(InfoExtractor):\n             'title': 'a/ Hot Teens',\n             'categories': list,\n             'upload_date': '20210104',\n-            'uploader_id': 'yonbiw',\n+            'uploader_id': 'anonymous',\n             'thumbnail': r're:https?://.*\\.jpg',\n             'age_limit': 18,\n         },\n@@ -125,9 +126,10 @@ def _real_extract(self, url):\n                 kwargs = {_AGO_UNITS.get(uploaded_ago[-1]): delta}\n                 upload_date = (datetime.datetime.utcnow() - datetime.timedelta(**kwargs)).strftime('%Y%m%d')\n \n-        comment_count = webpage.count('class=\"media-comment-contents\"')\n+        comment_count = len(re.findall(r'''class\\s*=\\s*['\"]media-comment-contents\\b''', webpage))\n         uploader_id = self._html_search_regex(\n-            r'\"thumb-member-username\">\\s+<a href=\"/m/([^\"]+)\"',\n+            (r'''<span\\b[^>]+\\bclass\\s*=\\s*[\"']username\\b[^>]*>([^<]+)</span>''',\n+             r'''(?s)['\"](?:media-meta-member|thumb-member-username)\\b[^>]+>\\s*<a\\b[^>]+\\bhref\\s*=\\s*['\"]/m/([^\"']+)'''),\n             webpage, 'uploader_id')\n \n         categories = self._html_search_meta('keywords', webpage, default=None)\n@@ -169,7 +171,18 @@ class MotherlessGroupIE(InfoExtractor):\n             'description': 'Sex can be funny. Wide smiles,laugh, games, fun of '\n                            'any kind!'\n         },\n-        'playlist_mincount': 9,\n+        'playlist_mincount': 0,\n+        'expected_warnings': [\n+            'This group has no videos.',\n+        ]\n+    }, {\n+        'url': 'https://motherless.com/g/beautiful_cock',\n+        'info_dict': {\n+            'id': 'beautiful_cock',\n+            'title': 'Beautiful Cock',\n+            'description': 'Group for lovely cocks yours, mine, a friends anything human',\n+        },\n+        'playlist_mincount': 2500,\n     }]\n \n     @classmethod\n@@ -208,16 +221,23 @@ def _real_extract(self, url):\n             r'<title>([\\w\\s]+\\w)\\s+-', webpage, 'title', fatal=False)\n         description = self._html_search_meta(\n             'description', webpage, fatal=False)\n-        page_count = self._int(self._search_regex(\n-            r'(\\d+)</(?:a|span)><(?:a|span)[^>]+>\\s*NEXT',\n-            webpage, 'page_count'), 'page_count')\n+        page_count = str_to_int(self._search_regex(\n+            r'(\\d+)\\s*</(?:a|span)>\\s*<(?:a|span)[^>]+(?:>\\s*NEXT|\\brel\\s*=\\s*[\"\\']?next)\\b',\n+            webpage, 'page_count', default=0))\n+        if not page_count:\n+            message = self._search_regex(\n+                r'''class\\s*=\\s*['\"]error-page\\b[^>]*>\\s*<p[^>]*>\\s*(?P<error_msg>[^<]+)(?<=\\S)\\s*''',\n+                webpage, 'error_msg', default=None) or 'This group has no videos.'\n+            self.report_warning(message, group_id)\n+            page_count = 1\n         PAGE_SIZE = 80\n \n         def _get_page(idx):\n-            webpage = self._download_webpage(\n-                page_url, group_id, query={'page': idx + 1},\n-                note='Downloading page %d/%d' % (idx + 1, page_count)\n-            )\n+            if idx > 0:\n+                webpage = self._download_webpage(\n+                    page_url, group_id, query={'page': idx + 1},\n+                    note='Downloading page %d/%d' % (idx + 1, page_count)\n+                )\n             for entry in self._extract_entries(webpage, url):\n                 yield entry\n \ndiff --git a/youtube_dl/extractor/myspass.py b/youtube_dl/extractor/myspass.py\nindex db7ebc94ca7..f540c52ee68 100644\n--- a/youtube_dl/extractor/myspass.py\n+++ b/youtube_dl/extractor/myspass.py\n@@ -35,7 +35,9 @@ def _real_extract(self, url):\n         title = xpath_text(metadata, 'title', fatal=True)\n         video_url = xpath_text(metadata, 'url_flv', 'download url', True)\n         video_id_int = int(video_id)\n-        for group in re.search(r'/myspass2009/\\d+/(\\d+)/(\\d+)/(\\d+)/', video_url).groups():\n+\n+        grps = re.search(r'/myspass2009/\\d+/(\\d+)/(\\d+)/(\\d+)/', video_url)\n+        for group in grps.groups() if grps else []:\n             group_int = int(group)\n             if group_int > video_id_int:\n                 video_url = video_url.replace(\ndiff --git a/youtube_dl/extractor/myvideoge.py b/youtube_dl/extractor/myvideoge.py\nnew file mode 100644\nindex 00000000000..efbfda7a65e\n--- /dev/null\n+++ b/youtube_dl/extractor/myvideoge.py\n@@ -0,0 +1,87 @@\n+# coding: utf-8\n+from __future__ import unicode_literals\n+\n+import re\n+\n+from .common import InfoExtractor\n+from ..utils import (\n+    clean_html,\n+    get_element_by_id,\n+    get_element_by_class,\n+    int_or_none,\n+    js_to_json,\n+    MONTH_NAMES,\n+    qualities,\n+    unified_strdate,\n+)\n+\n+\n+class MyVideoGeIE(InfoExtractor):\n+    _VALID_URL = r'https?://(?:www\\.)?myvideo\\.ge/v/(?P<id>[0-9]+)'\n+    _TEST = {\n+        'url': 'https://www.myvideo.ge/v/3941048',\n+        'md5': '8c192a7d2b15454ba4f29dc9c9a52ea9',\n+        'info_dict': {\n+            'id': '3941048',\n+            'ext': 'mp4',\n+            'title': 'The best prikol',\n+            'upload_date': '20200611',\n+            'thumbnail': r're:^https?://.*\\.jpg$',\n+            'uploader': 'chixa33',\n+            'description': 'md5:5b067801318e33c2e6eea4ab90b1fdd3',\n+        },\n+        # working from local dev system\n+        'skip': 'site blocks CI servers',\n+    }\n+    _MONTH_NAMES_KA = ['\u10d8\u10d0\u10dc\u10d5\u10d0\u10e0\u10d8', '\u10d7\u10d4\u10d1\u10d4\u10e0\u10d5\u10d0\u10da\u10d8', '\u10db\u10d0\u10e0\u10e2\u10d8', '\u10d0\u10de\u10e0\u10d8\u10da\u10d8', '\u10db\u10d0\u10d8\u10e1\u10d8', '\u10d8\u10d5\u10dc\u10d8\u10e1\u10d8', '\u10d8\u10d5\u10da\u10d8\u10e1\u10d8', '\u10d0\u10d2\u10d5\u10d8\u10e1\u10e2\u10dd', '\u10e1\u10d4\u10e5\u10e2\u10d4\u10db\u10d1\u10d4\u10e0\u10d8', '\u10dd\u10e5\u10e2\u10dd\u10db\u10d1\u10d4\u10e0\u10d8', '\u10dc\u10dd\u10d4\u10db\u10d1\u10d4\u10e0\u10d8', '\u10d3\u10d4\u10d9\u10d4\u10db\u10d1\u10d4\u10e0\u10d8']\n+\n+    _quality = staticmethod(qualities(('SD', 'HD')))\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+        webpage = self._download_webpage(url, video_id)\n+\n+        title = (\n+            self._og_search_title(webpage, default=None)\n+            or clean_html(get_element_by_class('my_video_title', webpage))\n+            or self._html_search_regex(r'<title\\b[^>]*>([^<]+)</title\\b', webpage, 'title'))\n+\n+        jwplayer_sources = self._parse_json(\n+            self._search_regex(\n+                r'''(?s)jwplayer\\s*\\(\\s*['\"]mvplayer['\"]\\s*\\)\\s*\\.\\s*setup\\s*\\(.*?\\bsources\\s*:\\s*(\\[.*?])\\s*[,});]''', webpage, 'jwplayer sources', fatal=False)\n+            or '',\n+            video_id, transform_source=js_to_json, fatal=False)\n+\n+        formats = self._parse_jwplayer_formats(jwplayer_sources or [], video_id)\n+        for f in formats or []:\n+            f['preference'] = self._quality(f['format_id'])\n+        self._sort_formats(formats)\n+\n+        description = (\n+            self._og_search_description(webpage)\n+            or get_element_by_id('long_desc_holder', webpage)\n+            or self._html_search_meta('description', webpage))\n+\n+        uploader = self._search_regex(r'<a[^>]+class=\"mv_user_name\"[^>]*>([^<]+)<', webpage, 'uploader', fatal=False)\n+\n+        upload_date = get_element_by_class('mv_vid_upl_date', webpage)\n+        # as ka locale may not be present roll a local date conversion\n+        upload_date = (unified_strdate(\n+            # translate any ka month to an en one\n+            re.sub('|'.join(self._MONTH_NAMES_KA),\n+                   lambda m: MONTH_NAMES['en'][self._MONTH_NAMES_KA.index(m.group(0))],\n+                   upload_date, re.I))\n+            if upload_date else None)\n+\n+        return {\n+            'id': video_id,\n+            'title': title,\n+            'description': description,\n+            'uploader': uploader,\n+            'formats': formats,\n+            'thumbnail': self._og_search_thumbnail(webpage),\n+            'upload_date': upload_date,\n+            'view_count': int_or_none(get_element_by_class('mv_vid_views', webpage)),\n+            'like_count': int_or_none(get_element_by_id('likes_count', webpage)),\n+            'dislike_count': int_or_none(get_element_by_id('dislikes_count', webpage)),\n+        }\ndiff --git a/youtube_dl/extractor/neteasemusic.py b/youtube_dl/extractor/neteasemusic.py\nindex 978a05841ce..5e5c6271bbe 100644\n--- a/youtube_dl/extractor/neteasemusic.py\n+++ b/youtube_dl/extractor/neteasemusic.py\n@@ -1,20 +1,32 @@\n # coding: utf-8\n from __future__ import unicode_literals\n \n-from hashlib import md5\n from base64 import b64encode\n+from binascii import hexlify\n from datetime import datetime\n+from hashlib import md5\n+from random import randint\n+import json\n import re\n+import time\n \n from .common import InfoExtractor\n+from ..aes import aes_ecb_encrypt, pkcs7_padding\n from ..compat import (\n     compat_urllib_parse_urlencode,\n     compat_str,\n     compat_itertools_count,\n )\n from ..utils import (\n-    sanitized_Request,\n+    ExtractorError,\n+    bytes_to_intlist,\n+    error_to_compat_str,\n     float_or_none,\n+    int_or_none,\n+    intlist_to_bytes,\n+    sanitized_Request,\n+    std_headers,\n+    try_get,\n )\n \n \n@@ -35,32 +47,106 @@ def _encrypt(cls, dfsid):\n         result = b64encode(m.digest()).decode('ascii')\n         return result.replace('/', '_').replace('+', '-')\n \n+    @classmethod\n+    def make_player_api_request_data_and_headers(cls, song_id, bitrate):\n+        KEY = b'e82ckenh8dichen8'\n+        URL = '/api/song/enhance/player/url'\n+        now = int(time.time() * 1000)\n+        rand = randint(0, 1000)\n+        cookie = {\n+            'osver': None,\n+            'deviceId': None,\n+            'appver': '8.0.0',\n+            'versioncode': '140',\n+            'mobilename': None,\n+            'buildver': '1623435496',\n+            'resolution': '1920x1080',\n+            '__csrf': '',\n+            'os': 'pc',\n+            'channel': None,\n+            'requestId': '{0}_{1:04}'.format(now, rand),\n+        }\n+        request_text = json.dumps(\n+            {'ids': '[{0}]'.format(song_id), 'br': bitrate, 'header': cookie},\n+            separators=(',', ':'))\n+        message = 'nobody{0}use{1}md5forencrypt'.format(\n+            URL, request_text).encode('latin1')\n+        msg_digest = md5(message).hexdigest()\n+\n+        data = '{0}-36cd479b6b5-{1}-36cd479b6b5-{2}'.format(\n+            URL, request_text, msg_digest)\n+        data = pkcs7_padding(bytes_to_intlist(data))\n+        encrypted = intlist_to_bytes(aes_ecb_encrypt(data, bytes_to_intlist(KEY)))\n+        encrypted_params = hexlify(encrypted).decode('ascii').upper()\n+\n+        cookie = '; '.join(\n+            ['{0}={1}'.format(k, v if v is not None else 'undefined')\n+             for [k, v] in cookie.items()])\n+\n+        headers = {\n+            'User-Agent': std_headers['User-Agent'],\n+            'Content-Type': 'application/x-www-form-urlencoded',\n+            'Referer': 'https://music.163.com',\n+            'Cookie': cookie,\n+        }\n+        return ('params={0}'.format(encrypted_params), headers)\n+\n+    def _call_player_api(self, song_id, bitrate):\n+        url = 'https://interface3.music.163.com/eapi/song/enhance/player/url'\n+        data, headers = self.make_player_api_request_data_and_headers(song_id, bitrate)\n+        try:\n+            msg = 'empty result'\n+            result = self._download_json(\n+                url, song_id, data=data.encode('ascii'), headers=headers)\n+            if result:\n+                return result\n+        except ExtractorError as e:\n+            if type(e.cause) in (ValueError, TypeError):\n+                # JSON load failure\n+                raise\n+        except Exception as e:\n+            msg = error_to_compat_str(e)\n+            self.report_warning('%s API call (%s) failed: %s' % (\n+                song_id, bitrate, msg))\n+        return {}\n+\n     def extract_formats(self, info):\n+        err = 0\n         formats = []\n+        song_id = info['id']\n         for song_format in self._FORMATS:\n             details = info.get(song_format)\n             if not details:\n                 continue\n-            song_file_path = '/%s/%s.%s' % (\n-                self._encrypt(details['dfsId']), details['dfsId'], details['extension'])\n-\n-            # 203.130.59.9, 124.40.233.182, 115.231.74.139, etc is a reverse proxy-like feature\n-            # from NetEase's CDN provider that can be used if m5.music.126.net does not\n-            # work, especially for users outside of Mainland China\n-            # via: https://github.com/JixunMoe/unblock-163/issues/3#issuecomment-163115880\n-            for host in ('http://m5.music.126.net', 'http://115.231.74.139/m1.music.126.net',\n-                         'http://124.40.233.182/m1.music.126.net', 'http://203.130.59.9/m1.music.126.net'):\n-                song_url = host + song_file_path\n+\n+            bitrate = int_or_none(details.get('bitrate')) or 999000\n+            data = self._call_player_api(song_id, bitrate)\n+            for song in try_get(data, lambda x: x['data'], list) or []:\n+                song_url = try_get(song, lambda x: x['url'])\n+                if not song_url:\n+                    continue\n                 if self._is_valid_url(song_url, info['id'], 'song'):\n                     formats.append({\n                         'url': song_url,\n                         'ext': details.get('extension'),\n-                        'abr': float_or_none(details.get('bitrate'), scale=1000),\n+                        'abr': float_or_none(song.get('br'), scale=1000),\n                         'format_id': song_format,\n-                        'filesize': details.get('size'),\n-                        'asr': details.get('sr')\n+                        'filesize': int_or_none(song.get('size')),\n+                        'asr': int_or_none(details.get('sr')),\n                     })\n-                    break\n+                elif err == 0:\n+                    err = try_get(song, lambda x: x['code'], int)\n+\n+        if not formats:\n+            msg = 'No media links found'\n+            if err != 0 and (err < 200 or err >= 400):\n+                raise ExtractorError(\n+                    '%s (site code %d)' % (msg, err, ), expected=True)\n+            else:\n+                self.raise_geo_restricted(\n+                    msg + ': probably this video is not available from your location due to geo restriction.',\n+                    countries=['CN'])\n+\n         return formats\n \n     @classmethod\n@@ -76,33 +162,19 @@ def query_api(self, endpoint, video_id, note):\n class NetEaseMusicIE(NetEaseMusicBaseIE):\n     IE_NAME = 'netease:song'\n     IE_DESC = '\u7f51\u6613\u4e91\u97f3\u4e50'\n-    _VALID_URL = r'https?://music\\.163\\.com/(#/)?song\\?id=(?P<id>[0-9]+)'\n+    _VALID_URL = r'https?://(y\\.)?music\\.163\\.com/(?:[#m]/)?song\\?.*?\\bid=(?P<id>[0-9]+)'\n     _TESTS = [{\n         'url': 'http://music.163.com/#/song?id=32102397',\n-        'md5': 'f2e97280e6345c74ba9d5677dd5dcb45',\n+        'md5': '3e909614ce09b1ccef4a3eb205441190',\n         'info_dict': {\n             'id': '32102397',\n             'ext': 'mp3',\n-            'title': 'Bad Blood (feat. Kendrick Lamar)',\n+            'title': 'Bad Blood',\n             'creator': 'Taylor Swift / Kendrick Lamar',\n-            'upload_date': '20150517',\n-            'timestamp': 1431878400,\n-            'description': 'md5:a10a54589c2860300d02e1de821eb2ef',\n-        },\n-        'skip': 'Blocked outside Mainland China',\n-    }, {\n-        'note': 'No lyrics translation.',\n-        'url': 'http://music.163.com/#/song?id=29822014',\n-        'info_dict': {\n-            'id': '29822014',\n-            'ext': 'mp3',\n-            'title': '\u542c\u89c1\u4e0b\u96e8\u7684\u58f0\u97f3',\n-            'creator': '\u5468\u6770\u4f26',\n-            'upload_date': '20141225',\n-            'timestamp': 1419523200,\n-            'description': 'md5:a4d8d89f44656af206b7b2555c0bce6c',\n+            'upload_date': '20150516',\n+            'timestamp': 1431792000,\n+            'description': 'md5:25fc5f27e47aad975aa6d36382c7833c',\n         },\n-        'skip': 'Blocked outside Mainland China',\n     }, {\n         'note': 'No lyrics.',\n         'url': 'http://music.163.com/song?id=17241424',\n@@ -112,9 +184,9 @@ class NetEaseMusicIE(NetEaseMusicBaseIE):\n             'title': 'Opus 28',\n             'creator': 'Dustin O\\'Halloran',\n             'upload_date': '20080211',\n+            'description': 'md5:f12945b0f6e0365e3b73c5032e1b0ff4',\n             'timestamp': 1202745600,\n         },\n-        'skip': 'Blocked outside Mainland China',\n     }, {\n         'note': 'Has translated name.',\n         'url': 'http://music.163.com/#/song?id=22735043',\n@@ -128,7 +200,18 @@ class NetEaseMusicIE(NetEaseMusicBaseIE):\n             'timestamp': 1264608000,\n             'alt_title': '\u8bf4\u51fa\u613f\u671b\u5427(Genie)',\n         },\n-        'skip': 'Blocked outside Mainland China',\n+    }, {\n+        'url': 'https://y.music.163.com/m/song?app_version=8.8.45&id=95670&uct2=sKnvS4+0YStsWkqsPhFijw%3D%3D&dlt=0846',\n+        'md5': '95826c73ea50b1c288b22180ec9e754d',\n+        'info_dict': {\n+            'id': '95670',\n+            'ext': 'mp3',\n+            'title': '\u56fd\u9645\u6b4c',\n+            'creator': '\u9a6c\u5907',\n+            'upload_date': '19911130',\n+            'timestamp': 691516800,\n+            'description': 'md5:1ba2f911a2b0aa398479f595224f2141',\n+        },\n     }]\n \n     def _process_lyrics(self, lyrics_info):\ndiff --git a/youtube_dl/extractor/nhk.py b/youtube_dl/extractor/nhk.py\nindex 8a9331a79f2..f43d91cd5fc 100644\n--- a/youtube_dl/extractor/nhk.py\n+++ b/youtube_dl/extractor/nhk.py\n@@ -1,3 +1,4 @@\n+# coding: utf-8\n from __future__ import unicode_literals\n \n import re\n@@ -7,7 +8,7 @@\n \n \n class NhkBaseIE(InfoExtractor):\n-    _API_URL_TEMPLATE = 'https://api.nhk.or.jp/nhkworld/%sod%slist/v7a/%s/%s/%s/all%s.json'\n+    _API_URL_TEMPLATE = 'https://nwapi.nhk.jp/nhkworld/%sod%slist/v7b/%s/%s/%s/all%s.json'\n     _BASE_URL_REGEX = r'https?://www3\\.nhk\\.or\\.jp/nhkworld/(?P<lang>[a-z]{2})/ondemand'\n     _TYPE_REGEX = r'/(?P<type>video|audio)/'\n \n@@ -23,7 +24,7 @@ def _call_api(self, m_id, lang, is_video, is_episode, is_clip):\n     def _extract_episode_info(self, url, episode=None):\n         fetch_episode = episode is None\n         lang, m_type, episode_id = re.match(NhkVodIE._VALID_URL, url).groups()\n-        if episode_id.isdigit():\n+        if len(episode_id) == 7:\n             episode_id = episode_id[:4] + '-' + episode_id[4:]\n \n         is_video = m_type == 'video'\n@@ -84,7 +85,8 @@ def get_clean_field(key):\n \n \n class NhkVodIE(NhkBaseIE):\n-    _VALID_URL = r'%s%s(?P<id>\\d{7}|[^/]+?-\\d{8}-[0-9a-z]+)' % (NhkBaseIE._BASE_URL_REGEX, NhkBaseIE._TYPE_REGEX)\n+    # the 7-character IDs can have alphabetic chars too: assume [a-z] rather than just [a-f], eg\n+    _VALID_URL = r'%s%s(?P<id>[0-9a-z]{7}|[^/]+?-\\d{8}-[0-9a-z]+)' % (NhkBaseIE._BASE_URL_REGEX, NhkBaseIE._TYPE_REGEX)\n     # Content available only for a limited period of time. Visit\n     # https://www3.nhk.or.jp/nhkworld/en/ondemand/ for working samples.\n     _TESTS = [{\n@@ -124,6 +126,19 @@ class NhkVodIE(NhkBaseIE):\n     }, {\n         'url': 'https://www3.nhk.or.jp/nhkworld/en/ondemand/audio/j_art-20150903-1/',\n         'only_matching': True,\n+    }, {\n+        # video, alphabetic character in ID #29670\n+        'url': 'https://www3.nhk.or.jp/nhkworld/en/ondemand/video/9999a34/',\n+        'only_matching': True,\n+        'info_dict': {\n+            'id': 'qfjay6cg',\n+            'ext': 'mp4',\n+            'title': 'DESIGN TALKS plus - Fishermen\u2019s Finery',\n+            'description': 'md5:8a8f958aaafb0d7cb59d38de53f1e448',\n+            'thumbnail': r're:^https?:/(/[a-z0-9.-]+)+\\.jpg\\?w=1920&h=1080$',\n+            'upload_date': '20210615',\n+            'timestamp': 1623722008,\n+        }\n     }]\n \n     def _real_extract(self, url):\ndiff --git a/youtube_dl/extractor/nrk.py b/youtube_dl/extractor/nrk.py\nindex 6d01a25c3e5..5a62b50fcaa 100644\n--- a/youtube_dl/extractor/nrk.py\n+++ b/youtube_dl/extractor/nrk.py\n@@ -60,8 +60,7 @@ def _call_api(self, path, video_id, item=None, note=None, fatal=True, query=None\n         return self._download_json(\n             urljoin('https://psapi.nrk.no/', path),\n             video_id, note or 'Downloading %s JSON' % item,\n-            fatal=fatal, query=query,\n-            headers={'Accept-Encoding': 'gzip, deflate, br'})\n+            fatal=fatal, query=query)\n \n \n class NRKIE(NRKBaseIE):\ndiff --git a/youtube_dl/extractor/openload.py b/youtube_dl/extractor/openload.py\nindex 0c20d0177e0..45b1add73ae 100644\n--- a/youtube_dl/extractor/openload.py\n+++ b/youtube_dl/extractor/openload.py\n@@ -7,6 +7,7 @@\n import tempfile\n \n from ..compat import (\n+    compat_open as open,\n     compat_urlparse,\n     compat_kwargs,\n )\n@@ -16,6 +17,7 @@\n     ExtractorError,\n     get_exe_version,\n     is_outdated_version,\n+    process_communicate_or_kill,\n     std_headers,\n )\n \n@@ -226,7 +228,7 @@ def get(self, url, html=None, video_id=None, note=None, note2='Executing JS on w\n             self.exe, '--ssl-protocol=any',\n             self._TMP_FILES['script'].name\n         ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n-        out, err = p.communicate()\n+        out, err = process_communicate_or_kill(p)\n         if p.returncode != 0:\n             raise ExtractorError(\n                 'Executing JS failed\\n:' + encodeArgument(err))\ndiff --git a/youtube_dl/extractor/orf.py b/youtube_dl/extractor/orf.py\nindex 8d537d7ae71..1ee78edbc17 100644\n--- a/youtube_dl/extractor/orf.py\n+++ b/youtube_dl/extractor/orf.py\n@@ -1,407 +1,394 @@\n # coding: utf-8\n from __future__ import unicode_literals\n \n+import base64\n+import functools\n import re\n \n from .common import InfoExtractor\n-from ..compat import compat_str\n+from .youtube import YoutubeIE\n from ..utils import (\n     clean_html,\n     determine_ext,\n+    ExtractorError,\n     float_or_none,\n-    HEADRequest,\n     int_or_none,\n-    orderedSet,\n-    remove_end,\n-    str_or_none,\n+    merge_dicts,\n+    mimetype2ext,\n+    parse_age_limit,\n+    parse_iso8601,\n     strip_jsonp,\n-    unescapeHTML,\n+    txt_or_none,\n     unified_strdate,\n+    update_url_query,\n     url_or_none,\n )\n-\n-\n-class ORFTVthekIE(InfoExtractor):\n-    IE_NAME = 'orf:tvthek'\n-    IE_DESC = 'ORF TVthek'\n-    _VALID_URL = r'https?://tvthek\\.orf\\.at/(?:[^/]+/)+(?P<id>\\d+)'\n+from ..traversal import T, traverse_obj\n+\n+k_float_or_none = functools.partial(float_or_none, scale=1000)\n+\n+\n+class ORFRadioBase(InfoExtractor):\n+    STATION_INFO = {\n+        'fm4': ('fm4', 'fm4', 'orffm4'),\n+        'noe': ('noe', 'oe2n', 'orfnoe'),\n+        'wien': ('wie', 'oe2w', 'orfwie'),\n+        'burgenland': ('bgl', 'oe2b', 'orfbgl'),\n+        'ooe': ('ooe', 'oe2o', 'orfooe'),\n+        'steiermark': ('stm', 'oe2st', 'orfstm'),\n+        'kaernten': ('ktn', 'oe2k', 'orfktn'),\n+        'salzburg': ('sbg', 'oe2s', 'orfsbg'),\n+        'tirol': ('tir', 'oe2t', 'orftir'),\n+        'vorarlberg': ('vbg', 'oe2v', 'orfvbg'),\n+        'oe3': ('oe3', 'oe3', 'orfoe3'),\n+        'oe1': ('oe1', 'oe1', 'orfoe1'),\n+    }\n+    _ID_NAMES = ('id', 'guid', 'program')\n+\n+    @classmethod\n+    def _get_item_id(cls, data):\n+        return traverse_obj(data, *cls._ID_NAMES, expected_type=txt_or_none)\n+\n+    @classmethod\n+    def _get_api_payload(cls, data, expected_id, in_payload=False):\n+        if expected_id not in traverse_obj(data, ('payload',)[:1 if in_payload else 0] + (cls._ID_NAMES, T(txt_or_none))):\n+            raise ExtractorError('Unexpected API data result', video_id=expected_id)\n+        return data['payload']\n+\n+    @staticmethod\n+    def _extract_podcast_upload(data):\n+        return traverse_obj(data, {\n+            'url': ('enclosures', 0, 'url'),\n+            'ext': ('enclosures', 0, 'type', T(mimetype2ext)),\n+            'filesize': ('enclosures', 0, 'length', T(int_or_none)),\n+            'title': ('title', T(txt_or_none)),\n+            'description': ('description', T(clean_html)),\n+            'timestamp': (('published', 'postDate'), T(parse_iso8601)),\n+            'duration': ('duration', T(k_float_or_none)),\n+            'series': ('podcast', 'title'),\n+            'uploader': ((('podcast', 'author'), 'station'), T(txt_or_none)),\n+            'uploader_id': ('podcast', 'channel', T(txt_or_none)),\n+        }, get_all=False)\n+\n+    @classmethod\n+    def _entries(cls, data, station, item_type=None):\n+        if item_type in ('upload', 'podcast-episode'):\n+            yield merge_dicts({\n+                'id': cls._get_item_id(data),\n+                'ext': 'mp3',\n+                'vcodec': 'none',\n+            }, cls._extract_podcast_upload(data), rev=True)\n+            return\n+\n+        loop_station = cls.STATION_INFO[station][1]\n+        for info in traverse_obj(data, ((('streams', Ellipsis), 'stream'), T(lambda v: v if v['loopStreamId'] else None))):\n+            item_id = info['loopStreamId']\n+            host = info.get('host') or 'loopstream01.apa.at'\n+            yield merge_dicts({\n+                'id': item_id.replace('.mp3', ''),\n+                'ext': 'mp3',\n+                'url': update_url_query('https://{0}/'.format(host), {\n+                    'channel': loop_station,\n+                    'id': item_id,\n+                }),\n+                'vcodec': 'none',\n+                # '_old_archive_ids': [make_archive_id(old_ie, video_id)],\n+            }, traverse_obj(data, {\n+                'title': ('title', T(txt_or_none)),\n+                'description': ('subtitle', T(clean_html)),\n+                'uploader': 'station',\n+                'series': ('programTitle', T(txt_or_none)),\n+            }), traverse_obj(info, {\n+                'duration': (('duration',\n+                              (None, T(lambda x: x['end'] - x['start']))),\n+                             T(k_float_or_none), any),\n+                'timestamp': (('start', 'startISO'), T(parse_iso8601), any),\n+            }))\n+\n+\n+class ORFRadioIE(ORFRadioBase):\n+    IE_NAME = 'orf:sound'\n+    _STATION_RE = '|'.join(map(re.escape, ORFRadioBase.STATION_INFO.keys()))\n+\n+    _VALID_URL = (\n+        r'https?://sound\\.orf\\.at/radio/(?P<station>{0})/sendung/(?P<id>\\d+)(?:/(?P<show>\\w+))?'.format(_STATION_RE),\n+        r'https?://(?P<station>{0})\\.orf\\.at/player/(?P<date>\\d{{8}})/(?P<id>\\d+)'.format(_STATION_RE),\n+    )\n \n     _TESTS = [{\n-        'url': 'http://tvthek.orf.at/program/Aufgetischt/2745173/Aufgetischt-Mit-der-Steirischen-Tafelrunde/8891389',\n+        'url': 'https://sound.orf.at/radio/ooe/sendung/37802/guten-morgen-oberoesterreich-am-feiertag',\n+        'info_dict': {\n+            'id': '37802',\n+            'title': 'Guten Morgen Ober\u00f6sterreich am Feiertag',\n+            'description': 'Ober\u00f6sterreichs meistgeh\u00f6rte regionale Fr\u00fchsendung.\\nRegionale Nachrichten zu jeder halben Stunde.\\nModeration: Wolfgang Lehner\\nNachrichten:  Stephan Schnabl',\n+        },\n         'playlist': [{\n-            'md5': '2942210346ed779588f428a92db88712',\n+            'md5': 'f9ff8517dd681b642a2c900e2c9e6085',\n             'info_dict': {\n-                'id': '8896777',\n-                'ext': 'mp4',\n-                'title': 'Aufgetischt: Mit der Steirischen Tafelrunde',\n-                'description': 'md5:c1272f0245537812d4e36419c207b67d',\n-                'duration': 2668,\n-                'upload_date': '20141208',\n-            },\n+                'id': '2024-05-30_0559_tl_66_7DaysThu1_443862',\n+                'ext': 'mp3',\n+                'title': 'Guten Morgen Ober\u00f6sterreich am Feiertag',\n+                'description': 'Ober\u00f6sterreichs meistgeh\u00f6rte regionale Fr\u00fchsendung.\\nRegionale Nachrichten zu jeder halben Stunde.\\nModeration: Wolfgang Lehner\\nNachrichten:  Stephan Schnabl',\n+                'timestamp': 1717041587,\n+                'upload_date': '20240530',\n+                'uploader': 'ooe',\n+                'duration': 14413.0,\n+            }\n         }],\n-        'skip': 'Blocked outside of Austria / Germany',\n+        'skip': 'Shows from ORF Sound are only available for 30 days.'\n     }, {\n-        'url': 'http://tvthek.orf.at/topic/Im-Wandel-der-Zeit/8002126/Best-of-Ingrid-Thurnher/7982256',\n+        'url': 'https://oe1.orf.at/player/20240531/758136',\n+        'md5': '2397717aaf3ae9c22a4f090ee3b8d374',\n         'info_dict': {\n-            'id': '7982259',\n-            'ext': 'mp4',\n-            'title': 'Best of Ingrid Thurnher',\n-            'upload_date': '20140527',\n-            'description': 'Viele Jahre war Ingrid Thurnher das \"Gesicht\" der ZIB 2. Vor ihrem Wechsel zur ZIB 2 im Jahr 1995 moderierte sie unter anderem \"Land und Leute\", \"\u00d6sterreich-Bild\" und \"Nieder\u00f6sterreich heute\".',\n-        },\n-        'params': {\n-            'skip_download': True,  # rtsp downloads\n+            'id': '2024-05-31_1905_tl_51_7DaysFri35_2413387',\n+            'ext': 'mp3',\n+            'title': '\"Who Cares?\"',\n+            'description': 'Europas gr\u00f6\u00dfte Netzkonferenz re:publica 2024',\n+            'timestamp': 1717175100,\n+            'upload_date': '20240531',\n+            'uploader': 'oe1',\n+            'duration': 1500,\n         },\n-        'skip': 'Blocked outside of Austria / Germany',\n-    }, {\n-        'url': 'http://tvthek.orf.at/topic/Fluechtlingskrise/10463081/Heimat-Fremde-Heimat/13879132/Senioren-betreuen-Migrantenkinder/13879141',\n-        'only_matching': True,\n-    }, {\n-        'url': 'http://tvthek.orf.at/profile/Universum/35429',\n-        'only_matching': True,\n+        'skip': 'Shows from ORF Sound are only available for 30 days.'\n     }]\n \n     def _real_extract(self, url):\n-        playlist_id = self._match_id(url)\n-        webpage = self._download_webpage(url, playlist_id)\n+        m = self._match_valid_url(url)\n+        station, show_id = m.group('station', 'id')\n+        api_station, _, _ = self.STATION_INFO[station]\n+        if 'date' in m.groupdict():\n+            data = self._download_json(\n+                'https://audioapi.orf.at/{0}/json/4.0/broadcast/{1}/{2}?_o={3}.orf.at'.format(\n+                    api_station, show_id, m.group('date'), station), show_id)\n+            show_id = data['id']\n+        else:\n+            data = self._download_json(\n+                'https://audioapi.orf.at/{0}/api/json/5.0/broadcast/{1}?_o=sound.orf.at'.format(\n+                    api_station, show_id), show_id)\n \n-        data_jsb = self._parse_json(\n-            self._search_regex(\n-                r'<div[^>]+class=([\"\\']).*?VideoPlaylist.*?\\1[^>]+data-jsb=([\"\\'])(?P<json>.+?)\\2',\n-                webpage, 'playlist', group='json'),\n-            playlist_id, transform_source=unescapeHTML)['playlist']['videos']\n+            data = self._get_api_payload(data, show_id, in_payload=True)\n \n-        entries = []\n-        for sd in data_jsb:\n-            video_id, title = sd.get('id'), sd.get('title')\n-            if not video_id or not title:\n-                continue\n-            video_id = compat_str(video_id)\n-            formats = []\n-            for fd in sd['sources']:\n-                src = url_or_none(fd.get('src'))\n-                if not src:\n-                    continue\n-                format_id_list = []\n-                for key in ('delivery', 'quality', 'quality_string'):\n-                    value = fd.get(key)\n-                    if value:\n-                        format_id_list.append(value)\n-                format_id = '-'.join(format_id_list)\n-                ext = determine_ext(src)\n-                if ext == 'm3u8':\n-                    m3u8_formats = self._extract_m3u8_formats(\n-                        src, video_id, 'mp4', m3u8_id=format_id, fatal=False)\n-                    if any('/geoprotection' in f['url'] for f in m3u8_formats):\n-                        self.raise_geo_restricted()\n-                    formats.extend(m3u8_formats)\n-                elif ext == 'f4m':\n-                    formats.extend(self._extract_f4m_formats(\n-                        src, video_id, f4m_id=format_id, fatal=False))\n-                elif ext == 'mpd':\n-                    formats.extend(self._extract_mpd_formats(\n-                        src, video_id, mpd_id=format_id, fatal=False))\n-                else:\n-                    formats.append({\n-                        'format_id': format_id,\n-                        'url': src,\n-                        'protocol': fd.get('protocol'),\n-                    })\n-\n-            # Check for geoblocking.\n-            # There is a property is_geoprotection, but that's always false\n-            geo_str = sd.get('geoprotection_string')\n-            if geo_str:\n-                try:\n-                    http_url = next(\n-                        f['url']\n-                        for f in formats\n-                        if re.match(r'^https?://.*\\.mp4$', f['url']))\n-                except StopIteration:\n-                    pass\n-                else:\n-                    req = HEADRequest(http_url)\n-                    self._request_webpage(\n-                        req, video_id,\n-                        note='Testing for geoblocking',\n-                        errnote=((\n-                            'This video seems to be blocked outside of %s. '\n-                            'You may want to try the streaming-* formats.')\n-                            % geo_str),\n-                        fatal=False)\n-\n-            self._check_formats(formats, video_id)\n-            self._sort_formats(formats)\n+        # site sends ISO8601 GMT date-times with separate TZ offset, ignored\n+        # TODO: should `..._date` be calculated relative to TZ?\n \n-            subtitles = {}\n-            for sub in sd.get('subtitles', []):\n-                sub_src = sub.get('src')\n-                if not sub_src:\n-                    continue\n-                subtitles.setdefault(sub.get('lang', 'de-AT'), []).append({\n-                    'url': sub_src,\n-                })\n-\n-            upload_date = unified_strdate(sd.get('created_date'))\n+        return merge_dicts(\n+            {'_type': 'multi_video'},\n+            self.playlist_result(\n+                self._entries(data, station), show_id,\n+                txt_or_none(data.get('title')),\n+                clean_html(data.get('subtitle'))))\n \n-            thumbnails = []\n-            preview = sd.get('preview_image_url')\n-            if preview:\n-                thumbnails.append({\n-                    'id': 'preview',\n-                    'url': preview,\n-                    'preference': 0,\n-                })\n-            image = sd.get('image_full_url')\n-            if not image and len(data_jsb) == 1:\n-                image = self._og_search_thumbnail(webpage)\n-            if image:\n-                thumbnails.append({\n-                    'id': 'full',\n-                    'url': image,\n-                    'preference': 1,\n-                })\n \n-            entries.append({\n-                '_type': 'video',\n-                'id': video_id,\n-                'title': title,\n-                'formats': formats,\n-                'subtitles': subtitles,\n-                'description': sd.get('description'),\n-                'duration': int_or_none(sd.get('duration_in_seconds')),\n-                'upload_date': upload_date,\n-                'thumbnails': thumbnails,\n-            })\n-\n-        return {\n-            '_type': 'playlist',\n-            'entries': entries,\n-            'id': playlist_id,\n-        }\n-\n-\n-class ORFRadioIE(InfoExtractor):\n-    def _real_extract(self, url):\n-        mobj = re.match(self._VALID_URL, url)\n-        show_date = mobj.group('date')\n-        show_id = mobj.group('show')\n+class ORFRadioCollectionIE(ORFRadioBase):\n+    IE_NAME = 'orf:collection'\n+    _VALID_URL = r'https?://sound\\.orf\\.at/collection/(?P<coll_id>\\d+)(?:/(?P<item_id>\\d+))?'\n \n-        data = self._download_json(\n-            'http://audioapi.orf.at/%s/api/json/current/broadcast/%s/%s'\n-            % (self._API_STATION, show_id, show_date), show_id)\n-\n-        entries = []\n-        for info in data['streams']:\n-            loop_stream_id = str_or_none(info.get('loopStreamId'))\n-            if not loop_stream_id:\n-                continue\n-            title = str_or_none(data.get('title'))\n-            if not title:\n-                continue\n-            start = int_or_none(info.get('start'), scale=1000)\n-            end = int_or_none(info.get('end'), scale=1000)\n-            duration = end - start if end and start else None\n-            entries.append({\n-                'id': loop_stream_id.replace('.mp3', ''),\n-                'url': 'https://loopstream01.apa.at/?channel=%s&id=%s' % (self._LOOP_STATION, loop_stream_id),\n-                'title': title,\n-                'description': clean_html(data.get('subtitle')),\n-                'duration': duration,\n-                'timestamp': start,\n+    _TESTS = [{\n+        'url': 'https://sound.orf.at/collection/4/61908/was-das-uberschreiten-des-15-limits-bedeutet',\n+        'info_dict': {\n+            'id': '2577582',\n+        },\n+        'playlist': [{\n+            'md5': '5789cec7d75575ff58d19c0428c80eb3',\n+            'info_dict': {\n+                'id': '2024-06-06_1659_tl_54_7DaysThu6_153926',\n                 'ext': 'mp3',\n-                'series': data.get('programTitle'),\n-            })\n-\n-        return {\n-            '_type': 'playlist',\n-            'id': show_id,\n-            'title': data.get('title'),\n-            'description': clean_html(data.get('subtitle')),\n-            'entries': entries,\n-        }\n-\n-\n-class ORFFM4IE(ORFRadioIE):\n-    IE_NAME = 'orf:fm4'\n-    IE_DESC = 'radio FM4'\n-    _VALID_URL = r'https?://(?P<station>fm4)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>4\\w+)'\n-    _API_STATION = 'fm4'\n-    _LOOP_STATION = 'fm4'\n-\n-    _TEST = {\n-        'url': 'http://fm4.orf.at/player/20170107/4CC',\n-        'md5': '2b0be47375432a7ef104453432a19212',\n+                'title': 'Klimakrise: Was das \u00dcberschreiten des 1,5\u00b0-Limits bedeutet',\n+                'timestamp': 1717686674,\n+                'upload_date': '20240606',\n+                'uploader': 'fm4',\n+            },\n+        }],\n+        'skip': 'Shows from ORF Sound are only available for 30 days.'\n+    }, {\n+        # persistent playlist (FM4 Highlights)\n+        'url': 'https://sound.orf.at/collection/4/',\n         'info_dict': {\n-            'id': '2017-01-07_2100_tl_54_7DaysSat18_31295',\n-            'ext': 'mp3',\n-            'title': 'Solid Steel Radioshow',\n-            'description': 'Die Mixshow von Coldcut und Ninja Tune.',\n-            'duration': 3599,\n-            'timestamp': 1483819257,\n-            'upload_date': '20170107',\n+            'id': '4',\n         },\n-        'skip': 'Shows from ORF radios are only available for 7 days.',\n-        'only_matching': True,\n-    }\n-\n-\n-class ORFNOEIE(ORFRadioIE):\n-    IE_NAME = 'orf:noe'\n-    IE_DESC = 'Radio Nieder\u00f6sterreich'\n-    _VALID_URL = r'https?://(?P<station>noe)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>\\w+)'\n-    _API_STATION = 'noe'\n-    _LOOP_STATION = 'oe2n'\n-\n-    _TEST = {\n-        'url': 'https://noe.orf.at/player/20200423/NGM',\n-        'only_matching': True,\n-    }\n-\n-\n-class ORFWIEIE(ORFRadioIE):\n-    IE_NAME = 'orf:wien'\n-    IE_DESC = 'Radio Wien'\n-    _VALID_URL = r'https?://(?P<station>wien)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>\\w+)'\n-    _API_STATION = 'wie'\n-    _LOOP_STATION = 'oe2w'\n-\n-    _TEST = {\n-        'url': 'https://wien.orf.at/player/20200423/WGUM',\n-        'only_matching': True,\n-    }\n-\n-\n-class ORFBGLIE(ORFRadioIE):\n-    IE_NAME = 'orf:burgenland'\n-    IE_DESC = 'Radio Burgenland'\n-    _VALID_URL = r'https?://(?P<station>burgenland)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>\\w+)'\n-    _API_STATION = 'bgl'\n-    _LOOP_STATION = 'oe2b'\n-\n-    _TEST = {\n-        'url': 'https://burgenland.orf.at/player/20200423/BGM',\n-        'only_matching': True,\n-    }\n-\n-\n-class ORFOOEIE(ORFRadioIE):\n-    IE_NAME = 'orf:oberoesterreich'\n-    IE_DESC = 'Radio Ober\u00f6sterreich'\n-    _VALID_URL = r'https?://(?P<station>ooe)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>\\w+)'\n-    _API_STATION = 'ooe'\n-    _LOOP_STATION = 'oe2o'\n+        'playlist_mincount': 10,\n+        'playlist_maxcount': 13,\n+    }]\n \n-    _TEST = {\n-        'url': 'https://ooe.orf.at/player/20200423/OGMO',\n-        'only_matching': True,\n-    }\n+    def _real_extract(self, url):\n+        coll_id, item_id = self._match_valid_url(url).group('coll_id', 'item_id')\n+        data = self._download_json(\n+            'https://collector.orf.at/api/frontend/collections/{0}?_o=sound.orf.at'.format(\n+                coll_id), coll_id)\n+        data = self._get_api_payload(data, coll_id, in_payload=True)\n+\n+        def yield_items():\n+            for item in traverse_obj(data, (\n+                    'content', 'items', lambda _, v: any(k in v['target']['params'] for k in self._ID_NAMES))):\n+                if item_id is None or item_id == txt_or_none(item.get('id')):\n+                    target = item['target']\n+                    typed_item_id = self._get_item_id(target['params'])\n+                    station = target['params'].get('station')\n+                    item_type = target.get('type')\n+                    if typed_item_id and (station or item_type):\n+                        yield station, typed_item_id, item_type\n+                    if item_id is not None:\n+                        break\n+            else:\n+                if item_id is not None:\n+                    raise ExtractorError('Item not found in collection',\n+                                         video_id=coll_id, expected=True)\n+\n+        def item_playlist(station, typed_item_id, item_type):\n+            if item_type == 'upload':\n+                item_data = self._download_json('https://audioapi.orf.at/radiothek/api/2.0/upload/{0}?_o=sound.orf.at'.format(\n+                    typed_item_id), typed_item_id)\n+            elif item_type == 'podcast-episode':\n+                item_data = self._download_json('https://audioapi.orf.at/radiothek/api/2.0/episode/{0}?_o=sound.orf.at'.format(\n+                    typed_item_id), typed_item_id)\n+            else:\n+                api_station, _, _ = self.STATION_INFO[station]\n+                item_data = self._download_json(\n+                    'https://audioapi.orf.at/{0}/api/json/5.0/{1}/{2}?_o=sound.orf.at'.format(\n+                        api_station, item_type or 'broadcastitem', typed_item_id), typed_item_id)\n \n+            item_data = self._get_api_payload(item_data, typed_item_id, in_payload=True)\n \n-class ORFSTMIE(ORFRadioIE):\n-    IE_NAME = 'orf:steiermark'\n-    IE_DESC = 'Radio Steiermark'\n-    _VALID_URL = r'https?://(?P<station>steiermark)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>\\w+)'\n-    _API_STATION = 'stm'\n-    _LOOP_STATION = 'oe2st'\n+            return merge_dicts(\n+                {'_type': 'multi_video'},\n+                self.playlist_result(\n+                    self._entries(item_data, station, item_type), typed_item_id,\n+                    txt_or_none(data.get('title')),\n+                    clean_html(data.get('subtitle'))))\n \n-    _TEST = {\n-        'url': 'https://steiermark.orf.at/player/20200423/STGMS',\n-        'only_matching': True,\n-    }\n+        def yield_item_entries():\n+            for station, typed_id, item_type in yield_items():\n+                yield item_playlist(station, typed_id, item_type)\n \n+        if item_id is not None:\n+            # coll_id = '/'.join((coll_id, item_id))\n+            return next(yield_item_entries())\n \n-class ORFKTNIE(ORFRadioIE):\n-    IE_NAME = 'orf:kaernten'\n-    IE_DESC = 'Radio K\u00e4rnten'\n-    _VALID_URL = r'https?://(?P<station>kaernten)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>\\w+)'\n-    _API_STATION = 'ktn'\n-    _LOOP_STATION = 'oe2k'\n+        return self.playlist_result(yield_item_entries(), coll_id, data.get('title'))\n \n-    _TEST = {\n-        'url': 'https://kaernten.orf.at/player/20200423/KGUMO',\n-        'only_matching': True,\n-    }\n \n+class ORFPodcastIE(ORFRadioBase):\n+    IE_NAME = 'orf:podcast'\n+    _STATION_RE = '|'.join(map(re.escape, (x[0] for x in ORFRadioBase.STATION_INFO.values()))) + '|tv'\n+    _VALID_URL = r'https?://sound\\.orf\\.at/podcast/(?P<station>{0})/(?P<show>[\\w-]+)/(?P<id>[\\w-]+)'.format(_STATION_RE)\n+    _TESTS = [{\n+        'url': 'https://sound.orf.at/podcast/stm/der-kraeutertipp-von-christine-lackner/rotklee',\n+        'md5': '1f2bab2ba90c2ce0c2754196ea78b35f',\n+        'info_dict': {\n+            'id': 'der-kraeutertipp-von-christine-lackner/rotklee',\n+            'ext': 'mp3',\n+            'title': 'Rotklee',\n+            'description': 'In der Natur weit verbreitet - in der Medizin l\u00e4ngst anerkennt: Rotklee. Dieser Podcast begleitet die Sendung \"Radio Steiermark am Vormittag\", Radio Steiermark, 28. Mai 2024.',\n+            'timestamp': 1716891761,\n+            'upload_date': '20240528',\n+            'uploader_id': 'stm_kraeutertipp',\n+            'uploader': 'ORF Radio Steiermark',\n+            'duration': 101,\n+            'series': 'Der Kr\u00e4utertipp von Christine Lackner',\n+        },\n+        'skip': 'ORF podcasts are only available for a limited time'\n+    }]\n \n-class ORFSBGIE(ORFRadioIE):\n-    IE_NAME = 'orf:salzburg'\n-    IE_DESC = 'Radio Salzburg'\n-    _VALID_URL = r'https?://(?P<station>salzburg)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>\\w+)'\n-    _API_STATION = 'sbg'\n-    _LOOP_STATION = 'oe2s'\n+    _ID_NAMES = ('slug', 'guid')\n \n-    _TEST = {\n-        'url': 'https://salzburg.orf.at/player/20200423/SGUM',\n-        'only_matching': True,\n-    }\n+    def _real_extract(self, url):\n+        station, show, show_id = self._match_valid_url(url).group('station', 'show', 'id')\n+        data = self._download_json(\n+            'https://audioapi.orf.at/radiothek/api/2.0/podcast/{0}/{1}/{2}'.format(\n+                station, show, show_id), show_id)\n+        data = self._get_api_payload(data, show_id, in_payload=True)\n \n+        return merge_dicts({\n+            'id': '/'.join((show, show_id)),\n+            'ext': 'mp3',\n+            'vcodec': 'none',\n+        }, self._extract_podcast_upload(data), rev=True)\n \n-class ORFTIRIE(ORFRadioIE):\n-    IE_NAME = 'orf:tirol'\n-    IE_DESC = 'Radio Tirol'\n-    _VALID_URL = r'https?://(?P<station>tirol)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>\\w+)'\n-    _API_STATION = 'tir'\n-    _LOOP_STATION = 'oe2t'\n \n-    _TEST = {\n-        'url': 'https://tirol.orf.at/player/20200423/TGUMO',\n-        'only_matching': True,\n-    }\n+class ORFIPTVBase(InfoExtractor):\n+    _TITLE_STRIP_RE = ''\n \n+    def _extract_video(self, video_id, webpage, fatal=False):\n \n-class ORFVBGIE(ORFRadioIE):\n-    IE_NAME = 'orf:vorarlberg'\n-    IE_DESC = 'Radio Vorarlberg'\n-    _VALID_URL = r'https?://(?P<station>vorarlberg)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>\\w+)'\n-    _API_STATION = 'vbg'\n-    _LOOP_STATION = 'oe2v'\n+        data = self._download_json(\n+            'http://bits.orf.at/filehandler/static-api/json/current/data.json?file=%s' % video_id,\n+            video_id)[0]\n \n-    _TEST = {\n-        'url': 'https://vorarlberg.orf.at/player/20200423/VGUM',\n-        'only_matching': True,\n-    }\n+        video = traverse_obj(data, (\n+            'sources', ('default', 'q8c'),\n+            T(lambda x: x if x['loadBalancerUrl'] else None),\n+            any))\n \n+        load_balancer_url = video['loadBalancerUrl']\n \n-class ORFOE3IE(ORFRadioIE):\n-    IE_NAME = 'orf:oe3'\n-    IE_DESC = 'Radio \u00d6sterreich 3'\n-    _VALID_URL = r'https?://(?P<station>oe3)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>\\w+)'\n-    _API_STATION = 'oe3'\n-    _LOOP_STATION = 'oe3'\n+        try:\n+            rendition = self._download_json(\n+                load_balancer_url, video_id, transform_source=strip_jsonp)\n+        except ExtractorError:\n+            rendition = None\n+\n+        if not rendition:\n+            rendition = {\n+                'redirect': {\n+                    'smil': re.sub(\n+                        r'(/)jsonp(/.+\\.)mp4$', r'\\1dash\\2smil/manifest.mpd',\n+                        load_balancer_url),\n+                },\n+            }\n \n-    _TEST = {\n-        'url': 'https://oe3.orf.at/player/20200424/3WEK',\n-        'only_matching': True,\n-    }\n+        f = traverse_obj(video, {\n+            'abr': ('audioBitrate', T(int_or_none)),\n+            'vbr': ('bitrate', T(int_or_none)),\n+            'fps': ('videoFps', T(int_or_none)),\n+            'width': ('videoWidth', T(int_or_none)),\n+            'height': ('videoHeight', T(int_or_none)),\n+        })\n \n+        formats = []\n+        for format_id, format_url in traverse_obj(rendition, (\n+                'redirect', T(dict.items), Ellipsis)):\n+            if format_id == 'rtmp':\n+                ff = f.copy()\n+                ff.update({\n+                    'url': format_url,\n+                    'format_id': format_id,\n+                })\n+                formats.append(ff)\n+            elif determine_ext(format_url) == 'f4m':\n+                formats.extend(self._extract_f4m_formats(\n+                    format_url, video_id, f4m_id=format_id))\n+            elif determine_ext(format_url) == 'm3u8':\n+                formats.extend(self._extract_m3u8_formats(\n+                    format_url, video_id, 'mp4', m3u8_id=format_id,\n+                    entry_protocol='m3u8_native'))\n+            elif determine_ext(format_url) == 'mpd':\n+                formats.extend(self._extract_mpd_formats(\n+                    format_url, video_id, mpd_id=format_id))\n \n-class ORFOE1IE(ORFRadioIE):\n-    IE_NAME = 'orf:oe1'\n-    IE_DESC = 'Radio \u00d6sterreich 1'\n-    _VALID_URL = r'https?://(?P<station>oe1)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>\\w+)'\n-    _API_STATION = 'oe1'\n-    _LOOP_STATION = 'oe1'\n+        if formats or fatal:\n+            self._sort_formats(formats)\n+        else:\n+            return\n \n-    _TEST = {\n-        'url': 'http://oe1.orf.at/player/20170108/456544',\n-        'md5': '34d8a6e67ea888293741c86a099b745b',\n-        'info_dict': {\n-            'id': '2017-01-08_0759_tl_51_7DaysSun6_256141',\n-            'ext': 'mp3',\n-            'title': 'Morgenjournal',\n-            'duration': 609,\n-            'timestamp': 1483858796,\n-            'upload_date': '20170108',\n-        },\n-        'skip': 'Shows from ORF radios are only available for 7 days.'\n-    }\n+        return merge_dicts({\n+            'id': video_id,\n+            'title': re.sub(self._TITLE_STRIP_RE, '', self._og_search_title(webpage)),\n+            'description': self._og_search_description(webpage),\n+            'upload_date': unified_strdate(self._html_search_meta(\n+                'dc.date', webpage, 'upload date', fatal=False)),\n+            'formats': formats,\n+        }, traverse_obj(data, {\n+            'duration': ('duration', T(k_float_or_none)),\n+            'thumbnail': ('sources', 'default', 'preview', T(url_or_none)),\n+        }), rev=True)\n \n \n-class ORFIPTVIE(InfoExtractor):\n+class ORFIPTVIE(ORFIPTVBase):\n     IE_NAME = 'orf:iptv'\n     IE_DESC = 'iptv.ORF.at'\n+    _WORKING = False  # URLs redirect to orf.at/\n     _VALID_URL = r'https?://iptv\\.orf\\.at/(?:#/)?stories/(?P<id>\\d+)'\n+    _TITLE_STRIP_RE = r'\\s+-\\s+iptv\\.ORF\\.at\\S*$'\n \n     _TEST = {\n         'url': 'http://iptv.orf.at/stories/2275236/',\n@@ -426,74 +413,32 @@ def _real_extract(self, url):\n         video_id = self._search_regex(\n             r'data-video(?:id)?=\"(\\d+)\"', webpage, 'video id')\n \n-        data = self._download_json(\n-            'http://bits.orf.at/filehandler/static-api/json/current/data.json?file=%s' % video_id,\n-            video_id)[0]\n-\n-        duration = float_or_none(data['duration'], 1000)\n+        return self._extract_video(video_id, webpage)\n \n-        video = data['sources']['default']\n-        load_balancer_url = video['loadBalancerUrl']\n-        abr = int_or_none(video.get('audioBitrate'))\n-        vbr = int_or_none(video.get('bitrate'))\n-        fps = int_or_none(video.get('videoFps'))\n-        width = int_or_none(video.get('videoWidth'))\n-        height = int_or_none(video.get('videoHeight'))\n-        thumbnail = video.get('preview')\n-\n-        rendition = self._download_json(\n-            load_balancer_url, video_id, transform_source=strip_jsonp)\n-\n-        f = {\n-            'abr': abr,\n-            'vbr': vbr,\n-            'fps': fps,\n-            'width': width,\n-            'height': height,\n-        }\n \n-        formats = []\n-        for format_id, format_url in rendition['redirect'].items():\n-            if format_id == 'rtmp':\n-                ff = f.copy()\n-                ff.update({\n-                    'url': format_url,\n-                    'format_id': format_id,\n-                })\n-                formats.append(ff)\n-            elif determine_ext(format_url) == 'f4m':\n-                formats.extend(self._extract_f4m_formats(\n-                    format_url, video_id, f4m_id=format_id))\n-            elif determine_ext(format_url) == 'm3u8':\n-                formats.extend(self._extract_m3u8_formats(\n-                    format_url, video_id, 'mp4', m3u8_id=format_id))\n-            else:\n-                continue\n-        self._sort_formats(formats)\n-\n-        title = remove_end(self._og_search_title(webpage), ' - iptv.ORF.at')\n-        description = self._og_search_description(webpage)\n-        upload_date = unified_strdate(self._html_search_meta(\n-            'dc.date', webpage, 'upload date'))\n-\n-        return {\n-            'id': video_id,\n-            'title': title,\n-            'description': description,\n-            'duration': duration,\n-            'thumbnail': thumbnail,\n-            'upload_date': upload_date,\n-            'formats': formats,\n-        }\n-\n-\n-class ORFFM4StoryIE(InfoExtractor):\n+class ORFFM4StoryIE(ORFIPTVBase):\n     IE_NAME = 'orf:fm4:story'\n     IE_DESC = 'fm4.orf.at stories'\n     _VALID_URL = r'https?://fm4\\.orf\\.at/stories/(?P<id>\\d+)'\n+    _TITLE_STRIP_RE = r'\\s+-\\s+fm4\\.ORF\\.at\\s*$'\n \n-    _TEST = {\n+    _TESTS = [{\n+        'url': 'https://fm4.orf.at/stories/3041554/',\n+        'add_ie': ['Youtube'],\n+        'info_dict': {\n+            'id': '3041554',\n+            'title': 'Is The EU Green Deal In Mortal Danger?',\n+        },\n+        'playlist_count': 4,\n+        'params': {\n+            'format': 'bestvideo',\n+        },\n+    }, {\n         'url': 'http://fm4.orf.at/stories/2865738/',\n+        'info_dict': {\n+            'id': '2865738',\n+            'title': 'Manu Delago und Inner Tongue live',\n+        },\n         'playlist': [{\n             'md5': 'e1c2c706c45c7b34cf478bbf409907ca',\n             'info_dict': {\n@@ -510,83 +455,311 @@ class ORFFM4StoryIE(InfoExtractor):\n             'info_dict': {\n                 'id': '547798',\n                 'ext': 'flv',\n-                'title': 'Manu Delago und Inner Tongue live (2)',\n+                'title': 'Manu Delago und Inner Tongue https://vod-ww.mdn.ors.at/cms-worldwide_episodes_nas/_definst_/nas/cms-worldwide_episodes/online/14228823_0005.smil/chunklist_b992000_vo.m3u8live (2)',\n                 'duration': 1504.08,\n                 'thumbnail': r're:^https?://.*\\.jpg$',\n                 'upload_date': '20170913',\n                 'description': 'Manu Delago und Inner Tongue haben bei der FM4 Soundpark Session live alles gegeben. Hier gibt es Fotos und die gesamte Session als Video.',\n             },\n         }],\n-    }\n+        'skip': 'Videos gone',\n+    }]\n \n     def _real_extract(self, url):\n         story_id = self._match_id(url)\n         webpage = self._download_webpage(url, story_id)\n \n         entries = []\n-        all_ids = orderedSet(re.findall(r'data-video(?:id)?=\"(\\d+)\"', webpage))\n-        for idx, video_id in enumerate(all_ids):\n-            data = self._download_json(\n-                'http://bits.orf.at/filehandler/static-api/json/current/data.json?file=%s' % video_id,\n-                video_id)[0]\n+        seen_ids = set()\n+        for idx, video_id in enumerate(re.findall(r'data-video(?:id)?=\"(\\d+)\"', webpage)):\n+            if video_id in seen_ids:\n+                continue\n+            seen_ids.add(video_id)\n+            entry = self._extract_video(video_id, webpage, fatal=False)\n+            if not entry:\n+                continue\n+\n+            if idx >= 1:\n+                # Titles are duplicates, make them unique\n+                entry['title'] = '%s (%d)' % (entry['title'], idx)\n \n-            duration = float_or_none(data['duration'], 1000)\n+            entries.append(entry)\n \n-            video = data['sources']['q8c']\n-            load_balancer_url = video['loadBalancerUrl']\n-            abr = int_or_none(video.get('audioBitrate'))\n-            vbr = int_or_none(video.get('bitrate'))\n-            fps = int_or_none(video.get('videoFps'))\n-            width = int_or_none(video.get('videoWidth'))\n-            height = int_or_none(video.get('videoHeight'))\n-            thumbnail = video.get('preview')\n+        seen_ids = set()\n+        for yt_id in re.findall(\n+                r'data-id\\s*=\\s*[\"\\']([\\w-]+)[^>]+\\bclass\\s*=\\s*[\"\\']youtube\\b',\n+                webpage):\n+            if yt_id in seen_ids:\n+                continue\n+            seen_ids.add(yt_id)\n+            if YoutubeIE.suitable(yt_id):\n+                entries.append(self.url_result(yt_id, ie='Youtube', video_id=yt_id))\n+\n+        return self.playlist_result(\n+            entries, story_id,\n+            re.sub(self._TITLE_STRIP_RE, '', self._og_search_title(webpage, default='') or None))\n+\n+\n+class ORFONBase(InfoExtractor):\n+    _ENC_PFX = '3dSlfek03nsLKdj4Jsd'\n+    _API_PATH = 'episode'\n+\n+    def _call_api(self, video_id, **kwargs):\n+        encrypted_id = base64.b64encode('{0}{1}'.format(\n+            self._ENC_PFX, video_id).encode('utf-8')).decode('ascii')\n+        return self._download_json(\n+            'https://api-tvthek.orf.at/api/v4.3/public/{0}/encrypted/{1}'.format(\n+                self._API_PATH, encrypted_id),\n+            video_id, **kwargs)\n+\n+    @classmethod\n+    def _parse_metadata(cls, api_json):\n+        return traverse_obj(api_json, {\n+            'id': ('id', T(int), T(txt_or_none)),\n+            'age_limit': ('age_classification', T(parse_age_limit)),\n+            'duration': ((('exact_duration', T(k_float_or_none)),\n+                          ('duration_second', T(float_or_none))),),\n+            'title': (('title', 'headline'), T(txt_or_none)),\n+            'description': (('description', 'teaser_text'), T(txt_or_none)),\n+            # 'media_type': ('video_type', T(txt_or_none)),\n+            'thumbnail': ('_embedded', 'image', 'public_urls', 'highlight_teaser', 'url', T(url_or_none)),\n+            'timestamp': (('date', 'episode_date'), T(parse_iso8601)),\n+            'release_timestamp': ('release_date', T(parse_iso8601)),\n+            # 'modified_timestamp': ('updated_at', T(parse_iso8601)),\n+        }, get_all=False)\n+\n+    def _extract_video(self, video_id, segment_id):\n+        # Not a segmented episode: return single video\n+        # Segmented episode without valid segment id: return entire playlist\n+        # Segmented episode with valid segment id and yes-playlist: return entire playlist\n+        # Segmented episode with valid segment id and no-playlist: return single video corresponding to segment id\n+        # If a multi_video playlist would be returned, but an unsegmented source exists, that source is chosen instead.\n+\n+        api_json = self._call_api(video_id)\n+\n+        if traverse_obj(api_json, 'is_drm_protected'):\n+            self.report_drm(video_id)\n+\n+        # updates formats, subtitles\n+        def extract_sources(src_json, video_id):\n+            for manifest_type in traverse_obj(src_json, ('sources', T(dict.keys), Ellipsis)):\n+                for manifest_url in traverse_obj(src_json, ('sources', manifest_type, Ellipsis, 'src', T(url_or_none))):\n+                    if manifest_type == 'hls':\n+                        fmts, subs = self._extract_m3u8_formats(\n+                            manifest_url, video_id, fatal=False, m3u8_id='hls',\n+                            ext='mp4', entry_protocol='m3u8_native'), {}\n+                        for f in fmts:\n+                            if '_vo.' in f['url']:\n+                                f['acodec'] = 'none'\n+                    elif manifest_type == 'dash':\n+                        fmts, subs = self._extract_mpd_formats_and_subtitles(\n+                            manifest_url, video_id, fatal=False, mpd_id='dash')\n+                    else:\n+                        continue\n+                    formats.extend(fmts)\n+                    self._merge_subtitles(subs, target=subtitles)\n+\n+        formats, subtitles = [], {}\n+        if segment_id is None:\n+            extract_sources(api_json, video_id)\n+        if not formats:\n+            segments = traverse_obj(api_json, (\n+                '_embedded', 'segments', lambda _, v: v['id']))\n+            if len(segments) > 1 and segment_id is not None:\n+                if not self._yes_playlist(video_id, segment_id, playlist_label='collection', video_label='segment'):\n+                    segments = [next(s for s in segments if txt_or_none(s['id']) == segment_id)]\n+\n+            entries = []\n+            for seg in segments:\n+                formats, subtitles = [], {}\n+                extract_sources(seg, segment_id)\n+                self._sort_formats(formats)\n+                entries.append(merge_dicts({\n+                    'formats': formats,\n+                    'subtitles': subtitles,\n+                }, self._parse_metadata(seg), rev=True))\n+            result = merge_dicts(\n+                {'_type': 'multi_video' if len(entries) > 1 else 'playlist'},\n+                self._parse_metadata(api_json),\n+                self.playlist_result(entries, video_id))\n+            # not yet processed in core for playlist/multi\n+            self._downloader._fill_common_fields(result)\n+            return result\n+        else:\n+            self._sort_formats(formats)\n \n-            rendition = self._download_json(\n-                load_balancer_url, video_id, transform_source=strip_jsonp)\n+        for sub_url in traverse_obj(api_json, (\n+                '_embedded', 'subtitle',\n+                ('xml_url', 'sami_url', 'stl_url', 'ttml_url', 'srt_url', 'vtt_url'),\n+                T(url_or_none))):\n+            self._merge_subtitles({'de': [{'url': sub_url}]}, target=subtitles)\n \n-            f = {\n-                'abr': abr,\n-                'vbr': vbr,\n-                'fps': fps,\n-                'width': width,\n-                'height': height,\n-            }\n+        return merge_dicts({\n+            'id': video_id,\n+            'formats': formats,\n+            'subtitles': subtitles,\n+            # '_old_archive_ids': [self._downloader._make_archive_id({'ie_key': 'ORFTVthek', 'id': video_id})],\n+        }, self._parse_metadata(api_json), rev=True)\n \n-            formats = []\n-            for format_id, format_url in rendition['redirect'].items():\n-                if format_id == 'rtmp':\n-                    ff = f.copy()\n-                    ff.update({\n-                        'url': format_url,\n-                        'format_id': format_id,\n-                    })\n-                    formats.append(ff)\n-                elif determine_ext(format_url) == 'f4m':\n-                    formats.extend(self._extract_f4m_formats(\n-                        format_url, video_id, f4m_id=format_id))\n-                elif determine_ext(format_url) == 'm3u8':\n-                    formats.extend(self._extract_m3u8_formats(\n-                        format_url, video_id, 'mp4', m3u8_id=format_id))\n-                else:\n-                    continue\n-            self._sort_formats(formats)\n+    def _real_extract(self, url):\n+        video_id, segment_id = self._match_valid_url(url).group('id', 'segment')\n+        webpage = self._download_webpage(url, video_id)\n \n-            title = remove_end(self._og_search_title(webpage), ' - fm4.ORF.at')\n-            if idx >= 1:\n-                # Titles are duplicates, make them unique\n-                title += ' (' + str(idx + 1) + ')'\n-            description = self._og_search_description(webpage)\n-            upload_date = unified_strdate(self._html_search_meta(\n-                'dc.date', webpage, 'upload date'))\n-\n-            entries.append({\n-                'id': video_id,\n-                'title': title,\n-                'description': description,\n-                'duration': duration,\n-                'thumbnail': thumbnail,\n-                'upload_date': upload_date,\n-                'formats': formats,\n-            })\n-\n-        return self.playlist_result(entries)\n+        # ORF doesn't like 410 or 404\n+        if self._search_regex(r'<div\\b[^>]*>\\s*(Nicht mehr verf\u00fcgbar)\\s*</div>', webpage, 'Availability', default=False):\n+            raise ExtractorError('Content is no longer available', expected=True, video_id=video_id)\n+\n+        return merge_dicts({\n+            'id': video_id,\n+            'title': self._html_search_meta(['og:title', 'twitter:title'], webpage, default=None),\n+            'description': self._html_search_meta(\n+                ['description', 'og:description', 'twitter:description'], webpage, default=None),\n+        }, self._search_json_ld(webpage, video_id, default={}),\n+            self._extract_video(video_id, segment_id),\n+            rev=True)\n+\n+\n+class ORFONIE(ORFONBase):\n+    IE_NAME = 'orf:on'\n+    _VALID_URL = r'https?://on\\.orf\\.at/video/(?P<id>\\d+)(?:/(?P<segment>\\d+))?'\n+    _TESTS = [{\n+        'url': 'https://on.orf.at/video/14210000/school-of-champions-48',\n+        'info_dict': {\n+            'id': '14210000',\n+            'ext': 'mp4',\n+            'duration': 2651.08,\n+            'thumbnail': 'https://api-tvthek.orf.at/assets/segments/0167/98/thumb_16697671_segments_highlight_teaser.jpeg',\n+            'title': 'School of Champions (4/8)',\n+            'description': r're:(?s)Luca hat sein ganzes Leben in den Bergen S\u00fcdtirols verbracht und ist bei seiner Mutter aufgewachsen, .{1029} Leo$',\n+            # 'media_type': 'episode',\n+            'timestamp': 1706558922,\n+            'upload_date': '20240129',\n+            'release_timestamp': 1706472362,\n+            'release_date': '20240128',\n+            # 'modified_timestamp': 1712756663,\n+            # 'modified_date': '20240410',\n+            # '_old_archive_ids': ['orftvthek 14210000'],\n+        },\n+        'params': {\n+            'format': 'bestvideo',\n+        },\n+        'skip': 'Available until 2024-08-12',\n+    }, {\n+        'url': 'https://on.orf.at/video/3220355',\n+        'md5': '925a93b2b9a37da5c9b979d7cf71aa2e',\n+        'info_dict': {\n+            'id': '3220355',\n+            'ext': 'mp4',\n+            'duration': 445.04,\n+            'thumbnail': 'https://api-tvthek.orf.at/assets/segments/0002/60/thumb_159573_segments_highlight_teaser.png',\n+            'title': '50 Jahre Burgenland: Der Festumzug',\n+            'description': r're:(?s)Aus allen Landesteilen zogen festlich geschm\u00fcckte Wagen und Musikkapellen .{270} Jenakowitsch$',\n+            # 'media_type': 'episode',\n+            'timestamp': 52916400,\n+            'upload_date': '19710905',\n+            'release_timestamp': 52916400,\n+            'release_date': '19710905',\n+            # 'modified_timestamp': 1498536049,\n+            # 'modified_date': '20170627',\n+            # '_old_archive_ids': ['orftvthek 3220355'],\n+        },\n+    }, {\n+        # Video with multiple segments selecting the second segment\n+        'url': 'https://on.orf.at/video/14226549/15639808/jugendbande-einbrueche-aus-langeweile',\n+        'md5': 'fc151bba8c05ea77ab5693617e4a33d3',\n+        'info_dict': {\n+            'id': '15639808',\n+            'ext': 'mp4',\n+            'duration': 97.707,\n+            'thumbnail': 'https://api-tvthek.orf.at/assets/segments/0175/43/thumb_17442704_segments_highlight_teaser.jpg',\n+            'title': 'Jugendbande: Einbr\u00fcche aus Langeweile',\n+            'description': r're:Jugendbande: Einbr\u00fcche aus Langeweile \\| Neuer Kinder- und .{259} Wanda$',\n+            # 'media_type': 'segment',\n+            'timestamp': 1715792400,\n+            'upload_date': '20240515',\n+            # 'modified_timestamp': 1715794394,\n+            # 'modified_date': '20240515',\n+            # '_old_archive_ids': ['orftvthek 15639808'],\n+        },\n+        'params': {\n+            'noplaylist': True,\n+            'format': 'bestvideo',\n+        },\n+        'skip': 'Available until 2024-06-14',\n+    }, {\n+        # Video with multiple segments and no combined version\n+        'url': 'https://on.orf.at/video/14227864/formel-1-grosser-preis-von-monaco-2024',\n+        'info_dict': {\n+            '_type': 'multi_video',\n+            'id': '14227864',\n+            'duration': 18410.52,\n+            'thumbnail': 'https://api-tvthek.orf.at/assets/segments/0176/04/thumb_17503881_segments_highlight_teaser.jpg',\n+            'title': 'Formel 1: Gro\u00dfer Preis von Monaco 2024',\n+            'description': 'md5:aeeb010710ccf70ce28ccb4482243d4f',\n+            # 'media_type': 'episode',\n+            'timestamp': 1716721200,\n+            'upload_date': '20240526',\n+            'release_timestamp': 1716721802,\n+            'release_date': '20240526',\n+            # 'modified_timestamp': 1716884702,\n+            # 'modified_date': '20240528',\n+        },\n+        'playlist_count': 42,\n+        'skip': 'Gone: Nicht mehr verf\u00fcgbar',\n+    }, {\n+        # Video with multiple segments, but with combined version\n+        'url': 'https://on.orf.at/video/14228172',\n+        'info_dict': {\n+            'id': '14228172',\n+            'ext': 'mp4',\n+            'duration': 3294.878,\n+            'thumbnail': 'https://api-tvthek.orf.at/assets/segments/0176/29/thumb_17528242_segments_highlight_teaser.jpg',\n+            'title': 'Willkommen \u00d6sterreich mit Stermann & Grissemann',\n+            'description': r're:Zum Saisonfinale freuen sich die urlaubsreifen Gastgeber Stermann und .{1863} Geschichten\\.$',\n+            # 'media_type': 'episode',\n+            'timestamp': 1716926584,\n+            'upload_date': '20240528',\n+            'release_timestamp': 1716919202,\n+            'release_date': '20240528',\n+            # 'modified_timestamp': 1716968045,\n+            # 'modified_date': '20240529',\n+            # '_old_archive_ids': ['orftvthek 14228172'],\n+        },\n+        'params': {\n+            'format': 'bestvideo',\n+        },\n+        'skip': 'Gone: Nicht mehr verf\u00fcgbar',\n+    }]\n+\n+\n+class ORFONLiveIE(ORFONBase):\n+    _ENC_PFX = '8876324jshjd7293ktd'\n+    _API_PATH = 'livestream'\n+    _VALID_URL = r'https?://on\\.orf\\.at/livestream/(?P<id>\\d+)(?:/(?P<segment>\\d+))?'\n+    _TESTS = [{\n+        'url': 'https://on.orf.at/livestream/14320204/pressekonferenz-neos-zu-aktuellen-entwicklungen',\n+        'info_dict': {\n+            'id': '14320204',\n+            'ext': 'mp4',\n+            'title': 'Pressekonferenz: Neos zu aktuellen Entwicklungen',\n+            'description': r're:(?s)Neos-Chefin Beate Meinl-Reisinger informi.{598}ng\\.\"',\n+            'timestamp': 1716886335,\n+            'upload_date': '20240528',\n+            # 'modified_timestamp': 1712756663,\n+            # 'modified_date': '20240410',\n+            # '_old_archive_ids': ['orftvthek 14210000'],\n+        },\n+        'params': {\n+            'format': 'bestvideo',\n+        },\n+    }]\n+\n+    @classmethod\n+    def _parse_metadata(cls, api_json):\n+        return merge_dicts(\n+            super(ORFONLiveIE, cls)._parse_metadata(api_json),\n+            traverse_obj(api_json, {\n+                'timestamp': ('updated_at', T(parse_iso8601)),\n+                'release_timestamp': ('start', T(parse_iso8601)),\n+                'is_live': True,\n+            }))\ndiff --git a/youtube_dl/extractor/palcomp3.py b/youtube_dl/extractor/palcomp3.py\nindex fb29d83f9f2..60f7a4d48db 100644\n--- a/youtube_dl/extractor/palcomp3.py\n+++ b/youtube_dl/extractor/palcomp3.py\n@@ -8,7 +8,7 @@\n from ..utils import (\n     int_or_none,\n     str_or_none,\n-    try_get,\n+    traverse_obj,\n )\n \n \n@@ -109,7 +109,7 @@ class PalcoMP3ArtistIE(PalcoMP3BaseIE):\n     }\n     name'''\n \n-    @ classmethod\n+    @classmethod\n     def suitable(cls, url):\n         return False if re.match(PalcoMP3IE._VALID_URL, url) else super(PalcoMP3ArtistIE, cls).suitable(url)\n \n@@ -118,7 +118,8 @@ def _real_extract(self, url):\n         artist = self._call_api(artist_slug, self._ARTIST_FIELDS_TMPL)['artist']\n \n         def entries():\n-            for music in (try_get(artist, lambda x: x['musics']['nodes'], list) or []):\n+            for music in traverse_obj(artist, (\n+                    'musics', 'nodes', lambda _, m: m['musicID'])):\n                 yield self._parse_music(music)\n \n         return self.playlist_result(\n@@ -137,7 +138,7 @@ class PalcoMP3VideoIE(PalcoMP3BaseIE):\n             'title': 'Maiara e Maraisa - Voc\u00ea Faz Falta Aqui - DVD Ao Vivo Em Campo Grande',\n             'description': 'md5:7043342c09a224598e93546e98e49282',\n             'upload_date': '20161107',\n-            'uploader_id': 'maiaramaraisaoficial',\n+            'uploader_id': '@maiaramaraisaoficial',\n             'uploader': 'Maiara e Maraisa',\n         }\n     }]\ndiff --git a/youtube_dl/extractor/peekvids.py b/youtube_dl/extractor/peekvids.py\nnew file mode 100644\nindex 00000000000..c8aad564b5d\n--- /dev/null\n+++ b/youtube_dl/extractor/peekvids.py\n@@ -0,0 +1,193 @@\n+# coding: utf-8\n+from __future__ import unicode_literals\n+\n+import re\n+\n+from .common import InfoExtractor\n+from ..utils import (\n+    ExtractorError,\n+    get_element_by_class,\n+    int_or_none,\n+    merge_dicts,\n+    url_or_none,\n+)\n+\n+\n+class PeekVidsIE(InfoExtractor):\n+    _VALID_URL = r'''(?x)\n+        https?://(?:www\\.)?peekvids\\.com/\n+        (?:(?:[^/?#]+/){2}|embed/?\\?(?:[^#]*&)?v=)\n+        (?P<id>[^/?&#]*)\n+    '''\n+    _TESTS = [{\n+        'url': 'https://peekvids.com/pc/dane-jones-cute-redhead-with-perfect-tits-with-mini-vamp/BSyLMbN0YCd',\n+        'md5': '2ff6a357a9717dc9dc9894b51307e9a2',\n+        'info_dict': {\n+            'id': '1262717',\n+            'display_id': 'BSyLMbN0YCd',\n+            'title': ' Dane Jones - Cute redhead with perfect tits with Mini Vamp',\n+            'ext': 'mp4',\n+            'thumbnail': r're:^https?://.*\\.jpg$',\n+            'description': 'md5:0a61df3620de26c0af8963b1a730cd69',\n+            'timestamp': 1642579329,\n+            'upload_date': '20220119',\n+            'duration': 416,\n+            'view_count': int,\n+            'age_limit': 18,\n+            'uploader': 'SEXYhub.com',\n+            'categories': list,\n+            'tags': list,\n+        },\n+    }]\n+    _DOMAIN = 'www.peekvids.com'\n+\n+    def _get_detail(self, html):\n+        return get_element_by_class('detail-video-block', html)\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+        webpage = self._download_webpage(url, video_id, expected_status=429)\n+        if '>Rate Limit Exceeded' in webpage:\n+            raise ExtractorError(\n+                '[%s] %s: %s' % (self.IE_NAME, video_id, 'You are suspected as a bot. Wait, or pass the captcha test on the site and provide --cookies.'),\n+                expected=True)\n+\n+        title = self._html_search_regex(r'(?s)<h1\\b[^>]*>(.+?)</h1>', webpage, 'title')\n+\n+        display_id = video_id\n+        video_id = self._search_regex(r'(?s)<video\\b[^>]+\\bdata-id\\s*=\\s*[\"\\']?([\\w-]+)', webpage, 'short video ID')\n+        srcs = self._download_json(\n+            'https://%s/v-alt/%s' % (self._DOMAIN, video_id), video_id,\n+            note='Downloading list of source files')\n+        formats = [{\n+            'url': f_url,\n+            'format_id': f_id,\n+            'height': int_or_none(f_id),\n+        } for f_url, f_id in (\n+            (url_or_none(f_v), f_match.group(1))\n+            for f_v, f_match in (\n+                (v, re.match(r'^data-src(\\d{3,})$', k))\n+                for k, v in srcs.items() if v) if f_match)\n+            if f_url\n+        ]\n+        if not formats:\n+            formats = [{'url': url} for url in srcs.values()]\n+        self._sort_formats(formats)\n+\n+        info = self._search_json_ld(webpage, video_id, expected_type='VideoObject', default={})\n+        info.pop('url', None)\n+        # may not have found the thumbnail if it was in a list in the ld+json\n+        info.setdefault('thumbnail', self._og_search_thumbnail(webpage))\n+        detail = self._get_detail(webpage) or ''\n+        info['description'] = self._html_search_regex(\n+            r'(?s)(.+?)(?:%s\\s*<|<ul\\b)' % (re.escape(info.get('description', '')), ),\n+            detail, 'description', default=None) or None\n+        info['title'] = re.sub(r'\\s*[,-][^,-]+$', '', info.get('title') or title) or self._generic_title(url)\n+\n+        def cat_tags(name, html):\n+            l = self._html_search_regex(\n+                r'(?s)<span\\b[^>]*>\\s*%s\\s*:\\s*</span>(.+?)</li>' % (re.escape(name), ),\n+                html, name, default='')\n+            return [x for x in re.split(r'\\s+', l) if x]\n+\n+        return merge_dicts({\n+            'id': video_id,\n+            'display_id': display_id,\n+            'age_limit': 18,\n+            'formats': formats,\n+            'categories': cat_tags('Categories', detail),\n+            'tags': cat_tags('Tags', detail),\n+            'uploader': self._html_search_regex(r'[Uu]ploaded\\s+by\\s(.+?)\"', webpage, 'uploader', default=None),\n+        }, info)\n+\n+\n+class PlayVidsIE(PeekVidsIE):\n+    _VALID_URL = r'https?://(?:www\\.)?playvids\\.com/(?:embed/|\\w\\w?/)?(?P<id>[^/?#]*)'\n+    _TESTS = [{\n+        'url': 'https://www.playvids.com/U3pBrYhsjXM/pc/dane-jones-cute-redhead-with-perfect-tits-with-mini-vamp',\n+        'md5': '2f12e50213dd65f142175da633c4564c',\n+        'info_dict': {\n+            'id': '1978030',\n+            'display_id': 'U3pBrYhsjXM',\n+            'title': ' Dane Jones - Cute redhead with perfect tits with Mini Vamp',\n+            'ext': 'mp4',\n+            'thumbnail': r're:^https?://.*\\.jpg$',\n+            'description': 'md5:0a61df3620de26c0af8963b1a730cd69',\n+            'timestamp': 1640435839,\n+            'upload_date': '20211225',\n+            'duration': 416,\n+            'view_count': int,\n+            'age_limit': 18,\n+            'uploader': 'SEXYhub.com',\n+            'categories': list,\n+            'tags': list,\n+        },\n+    }, {\n+        'url': 'https://www.playvids.com/es/U3pBrYhsjXM/pc/dane-jones-cute-redhead-with-perfect-tits-with-mini-vamp',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.playvids.com/embed/U3pBrYhsjXM',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.playvids.com/bKmGLe3IwjZ/sv/brazzers-800-phone-sex-madison-ivy-always-on-the-line',\n+        'md5': 'e783986e596cafbf46411a174ab42ba6',\n+        'info_dict': {\n+            'id': '762385',\n+            'display_id': 'bKmGLe3IwjZ',\n+            'ext': 'mp4',\n+            'title': 'Brazzers - 1 800 Phone Sex: Madison Ivy Always On The Line 6',\n+            'description': 'md5:bdcd2db2b8ad85831a491d7c8605dcef',\n+            'timestamp': 1516958544,\n+            'upload_date': '20180126',\n+            'thumbnail': r're:^https?://.*\\.jpg$',\n+            'duration': 480,\n+            'uploader': 'Brazzers',\n+            'age_limit': 18,\n+            'view_count': int,\n+            'age_limit': 18,\n+            'categories': list,\n+            'tags': list,\n+        },\n+    }, {\n+        'url': 'https://www.playvids.com/v/47iUho33toY',\n+        'md5': 'b056b5049d34b648c1e86497cf4febce',\n+        'info_dict': {\n+            'id': '700621',\n+            'display_id': '47iUho33toY',\n+            'ext': 'mp4',\n+            'title': 'KATEE OWEN STRIPTIASE IN SEXY RED LINGERIE',\n+            'description': None,\n+            'timestamp': 1507052209,\n+            'upload_date': '20171003',\n+            'thumbnail': r're:^https?://.*\\.jpg$',\n+            'duration': 332,\n+            'uploader': 'Cacerenele',\n+            'age_limit': 18,\n+            'view_count': int,\n+            'categories': list,\n+            'tags': list,\n+        }\n+    }, {\n+        'url': 'https://www.playvids.com/z3_7iwWCmqt/sexy-teen-filipina-striptease-beautiful-pinay-bargirl-strips-and-dances',\n+        'md5': 'efa09be9f031314b7b7e3bc6510cd0df',\n+        'info_dict': {\n+            'id': '1523518',\n+            'display_id': 'z3_7iwWCmqt',\n+            'ext': 'mp4',\n+            'title': 'SEXY TEEN FILIPINA STRIPTEASE - Beautiful Pinay Bargirl Strips and Dances',\n+            'description': None,\n+            'timestamp': 1607470323,\n+            'upload_date': '20201208',\n+            'thumbnail': r're:^https?://.*\\.jpg$',\n+            'duration': 593,\n+            'uploader': 'yorours',\n+            'age_limit': 18,\n+            'view_count': int,\n+            'categories': list,\n+            'tags': list,\n+        },\n+    }]\n+    _DOMAIN = 'www.playvids.com'\n+\n+    def _get_detail(self, html):\n+        return get_element_by_class('detail-block', html)\ndiff --git a/youtube_dl/extractor/pr0gramm.py b/youtube_dl/extractor/pr0gramm.py\nnew file mode 100644\nindex 00000000000..b68224fd5ed\n--- /dev/null\n+++ b/youtube_dl/extractor/pr0gramm.py\n@@ -0,0 +1,105 @@\n+# coding: utf-8\n+from __future__ import unicode_literals\n+\n+from .common import InfoExtractor\n+\n+import re\n+from ..utils import (\n+    merge_dicts,\n+)\n+\n+\n+class Pr0grammStaticIE(InfoExtractor):\n+    # Possible urls:\n+    # https://pr0gramm.com/static/5466437\n+    _VALID_URL = r'https?://pr0gramm\\.com/static/(?P<id>[0-9]+)'\n+    _TEST = {\n+        'url': 'https://pr0gramm.com/static/5466437',\n+        'md5': '52fa540d70d3edc286846f8ca85938aa',\n+        'info_dict': {\n+            'id': '5466437',\n+            'ext': 'mp4',\n+            'title': 'pr0gramm-5466437 by g11st',\n+            'uploader': 'g11st',\n+            'upload_date': '20221221',\n+        }\n+    }\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+        webpage = self._download_webpage(url, video_id)\n+\n+        # Fetch media sources\n+        entries = self._parse_html5_media_entries(url, webpage, video_id)\n+        media_info = entries[0]\n+\n+        # this raises if there are no formats\n+        self._sort_formats(media_info.get('formats') or [])\n+\n+        # Fetch author\n+        uploader = self._html_search_regex(r'by\\W+([\\w-]+)\\W+', webpage, 'uploader')\n+\n+        # Fetch approx upload timestamp from filename\n+        # Have None-defaults in case the extraction fails\n+        uploadDay = None\n+        uploadMon = None\n+        uploadYear = None\n+        uploadTimestr = None\n+        # (//img.pr0gramm.com/2022/12/21/62ae8aa5e2da0ebf.mp4)\n+        m = re.search(r'//img\\.pr0gramm\\.com/(?P<year>[\\d]+)/(?P<mon>[\\d]+)/(?P<day>[\\d]+)/\\w+\\.\\w{,4}', webpage)\n+\n+        if (m):\n+            # Up to a day of accuracy should suffice...\n+            uploadDay = m.groupdict().get('day')\n+            uploadMon = m.groupdict().get('mon')\n+            uploadYear = m.groupdict().get('year')\n+            uploadTimestr = uploadYear + uploadMon + uploadDay\n+\n+        return merge_dicts({\n+            'id': video_id,\n+            'title': 'pr0gramm-%s%s' % (video_id, (' by ' + uploader) if uploader else ''),\n+            'uploader': uploader,\n+            'upload_date': uploadTimestr\n+        }, media_info)\n+\n+\n+# This extractor is for the primary url (used for sharing, and appears in the\n+# location bar) Since this page loads the DOM via JS, yt-dl can't find any\n+# video information here. So let's redirect to a compatibility version of\n+# the site, which does contain the <video>-element  by itself,  without requiring\n+# js to be ran.\n+class Pr0grammIE(InfoExtractor):\n+    # Possible urls:\n+    # https://pr0gramm.com/new/546637\n+    # https://pr0gramm.com/new/video/546637\n+    # https://pr0gramm.com/top/546637\n+    # https://pr0gramm.com/top/video/546637\n+    # https://pr0gramm.com/user/g11st/uploads/5466437\n+    # https://pr0gramm.com/user/froschler/dafur-ist-man-hier/5091290\n+    # https://pr0gramm.com/user/froschler/reinziehen-1elf/5232030\n+    # https://pr0gramm.com/user/froschler/1elf/5232030\n+    # https://pr0gramm.com/new/5495710:comment62621020 <- this is not the id!\n+    # https://pr0gramm.com/top/fruher war alles damals/5498175\n+\n+    _VALID_URL = r'https?:\\/\\/pr0gramm\\.com\\/(?!static/\\d+).+?\\/(?P<id>[\\d]+)(:|$)'\n+    _TEST = {\n+        'url': 'https://pr0gramm.com/new/video/5466437',\n+        'info_dict': {\n+            'id': '5466437',\n+            'ext': 'mp4',\n+            'title': 'pr0gramm-5466437 by g11st',\n+            'uploader': 'g11st',\n+            'upload_date': '20221221',\n+        }\n+    }\n+\n+    def _generic_title():\n+        return \"oof\"\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+\n+        return self.url_result(\n+            'https://pr0gramm.com/static/' + video_id,\n+            video_id=video_id,\n+            ie=Pr0grammStaticIE.ie_key())\ndiff --git a/youtube_dl/extractor/rai.py b/youtube_dl/extractor/rai.py\nindex 67b86fc72c1..563d3400f8a 100644\n--- a/youtube_dl/extractor/rai.py\n+++ b/youtube_dl/extractor/rai.py\n@@ -5,15 +5,16 @@\n \n from .common import InfoExtractor\n from ..compat import (\n-    compat_urlparse,\n     compat_str,\n+    compat_urlparse,\n )\n from ..utils import (\n-    ExtractorError,\n     determine_ext,\n+    ExtractorError,\n     find_xpath_attr,\n     fix_xml_ampersands,\n     GeoRestrictedError,\n+    HEADRequest,\n     int_or_none,\n     parse_duration,\n     remove_start,\n@@ -96,12 +97,100 @@ def _extract_relinker_info(self, relinker_url, video_id):\n         if not formats and geoprotection is True:\n             self.raise_geo_restricted(countries=self._GEO_COUNTRIES)\n \n+        formats.extend(self._create_http_urls(relinker_url, formats))\n+\n         return dict((k, v) for k, v in {\n             'is_live': is_live,\n             'duration': duration,\n             'formats': formats,\n         }.items() if v is not None)\n \n+    def _create_http_urls(self, relinker_url, fmts):\n+        _RELINKER_REG = r'https?://(?P<host>[^/]+?)/(?:i/)?(?P<extra>[^/]+?)/(?P<path>.+?)/(?P<id>\\w+)(?:_(?P<quality>[\\d\\,]+))?(?:\\.mp4|/playlist\\.m3u8).+?'\n+        _MP4_TMPL = '%s&overrideUserAgentRule=mp4-%s'\n+        _QUALITY = {\n+            # tbr: w, h\n+            '250': [352, 198],\n+            '400': [512, 288],\n+            '700': [512, 288],\n+            '800': [700, 394],\n+            '1200': [736, 414],\n+            '1800': [1024, 576],\n+            '2400': [1280, 720],\n+            '3200': [1440, 810],\n+            '3600': [1440, 810],\n+            '5000': [1920, 1080],\n+            '10000': [1920, 1080],\n+        }\n+\n+        def test_url(url):\n+            resp = self._request_webpage(\n+                HEADRequest(url), None, headers={'User-Agent': 'Rai'},\n+                fatal=False, errnote=False, note=False)\n+\n+            if resp is False:\n+                return False\n+\n+            if resp.code == 200:\n+                return False if resp.url == url else resp.url\n+            return None\n+\n+        def get_format_info(tbr):\n+            import math\n+            br = int_or_none(tbr)\n+            if len(fmts) == 1 and not br:\n+                br = fmts[0].get('tbr')\n+            if br > 300:\n+                tbr = compat_str(math.floor(br / 100) * 100)\n+            else:\n+                tbr = '250'\n+\n+            # try extracting info from available m3u8 formats\n+            format_copy = None\n+            for f in fmts:\n+                if f.get('tbr'):\n+                    br_limit = math.floor(br / 100)\n+                    if br_limit - 1 <= math.floor(f['tbr'] / 100) <= br_limit + 1:\n+                        format_copy = f.copy()\n+            return {\n+                'width': format_copy.get('width'),\n+                'height': format_copy.get('height'),\n+                'tbr': format_copy.get('tbr'),\n+                'vcodec': format_copy.get('vcodec'),\n+                'acodec': format_copy.get('acodec'),\n+                'fps': format_copy.get('fps'),\n+                'format_id': 'https-%s' % tbr,\n+            } if format_copy else {\n+                'width': _QUALITY[tbr][0],\n+                'height': _QUALITY[tbr][1],\n+                'format_id': 'https-%s' % tbr,\n+                'tbr': int(tbr),\n+            }\n+\n+        loc = test_url(_MP4_TMPL % (relinker_url, '*'))\n+        if not isinstance(loc, compat_str):\n+            return []\n+\n+        mobj = re.match(\n+            _RELINKER_REG,\n+            test_url(relinker_url) or '')\n+        if not mobj:\n+            return []\n+\n+        available_qualities = mobj.group('quality').split(',') if mobj.group('quality') else ['*']\n+        available_qualities = [i for i in available_qualities if i]\n+\n+        formats = []\n+        for q in available_qualities:\n+            fmt = {\n+                'url': _MP4_TMPL % (relinker_url, q),\n+                'protocol': 'https',\n+                'ext': 'mp4',\n+            }\n+            fmt.update(get_format_info(q))\n+            formats.append(fmt)\n+        return formats\n+\n     @staticmethod\n     def _extract_subtitles(url, video_data):\n         STL_EXT = 'stl'\n@@ -151,6 +240,22 @@ class RaiPlayIE(RaiBaseIE):\n         'params': {\n             'skip_download': True,\n         },\n+    }, {\n+        # 1080p direct mp4 url\n+        'url': 'https://www.raiplay.it/video/2021/03/Leonardo-S1E1-b5703b02-82ee-475a-85b6-c9e4a8adf642.html',\n+        'md5': '2e501e8651d72f05ffe8f5d286ad560b',\n+        'info_dict': {\n+            'id': 'b5703b02-82ee-475a-85b6-c9e4a8adf642',\n+            'ext': 'mp4',\n+            'title': 'Leonardo - S1E1',\n+            'alt_title': 'St 1 Ep 1 - Episodio 1',\n+            'description': 'md5:f5360cd267d2de146e4e3879a5a47d31',\n+            'thumbnail': r're:^https?://.*\\.jpg$',\n+            'uploader': 'Rai 1',\n+            'duration': 3229,\n+            'series': 'Leonardo',\n+            'season': 'Season 1',\n+        },\n     }, {\n         'url': 'http://www.raiplay.it/video/2016/11/gazebotraindesi-efebe701-969c-4593-92f3-285f0d1ce750.html?',\n         'only_matching': True,\n@@ -158,6 +263,10 @@ class RaiPlayIE(RaiBaseIE):\n         # subtitles at 'subtitlesArray' key (see #27698)\n         'url': 'https://www.raiplay.it/video/2020/12/Report---04-01-2021-2e90f1de-8eee-4de4-ac0e-78d21db5b600.html',\n         'only_matching': True,\n+    }, {\n+        # DRM protected\n+        'url': 'https://www.raiplay.it/video/2020/09/Lo-straordinario-mondo-di-Zoey-S1E1-Lo-straordinario-potere-di-Zoey-ed493918-1d32-44b7-8454-862e473d00ff.html',\n+        'only_matching': True,\n     }]\n \n     def _real_extract(self, url):\n@@ -166,6 +275,13 @@ def _real_extract(self, url):\n         media = self._download_json(\n             base + '.json', video_id, 'Downloading video JSON')\n \n+        if try_get(\n+                media,\n+                (lambda x: x['rights_management']['rights']['drm'],\n+                 lambda x: x['program_info']['rights_management']['rights']['drm']),\n+                dict):\n+            raise ExtractorError('This video is DRM protected.', expected=True)\n+\n         title = media['name']\n \n         video = media['video']\n@@ -307,7 +423,7 @@ class RaiIE(RaiBaseIE):\n     }, {\n         # with ContentItem in og:url\n         'url': 'http://www.rai.it/dl/RaiTV/programmi/media/ContentItem-efb17665-691c-45d5-a60c-5301333cbb0c.html',\n-        'md5': '6865dd00cf0bbf5772fdd89d59bd768a',\n+        'md5': '06345bd97c932f19ffb129973d07a020',\n         'info_dict': {\n             'id': 'efb17665-691c-45d5-a60c-5301333cbb0c',\n             'ext': 'mp4',\ndiff --git a/youtube_dl/extractor/rbgtum.py b/youtube_dl/extractor/rbgtum.py\nnew file mode 100644\nindex 00000000000..da48ebbc494\n--- /dev/null\n+++ b/youtube_dl/extractor/rbgtum.py\n@@ -0,0 +1,97 @@\n+# coding: utf-8\n+from __future__ import unicode_literals\n+\n+import re\n+\n+from .common import InfoExtractor\n+\n+\n+class RbgTumIE(InfoExtractor):\n+    _VALID_URL = r'https://live\\.rbg\\.tum\\.de/w/(?P<id>.+)'\n+    _TESTS = [{\n+        # Combined view\n+        'url': 'https://live.rbg.tum.de/w/cpp/22128',\n+        'md5': '53a5e7b3e07128e33bbf36687fe1c08f',\n+        'info_dict': {\n+            'id': 'cpp/22128',\n+            'ext': 'mp4',\n+            'title': 'Lecture: October 18. 2022',\n+            'series': 'Concepts of C++ programming (IN2377)',\n+        }\n+    }, {\n+        # Presentation only\n+        'url': 'https://live.rbg.tum.de/w/I2DL/12349/PRES',\n+        'md5': '36c584272179f3e56b0db5d880639cba',\n+        'info_dict': {\n+            'id': 'I2DL/12349/PRES',\n+            'ext': 'mp4',\n+            'title': 'Lecture 3: Introduction to Neural Networks',\n+            'series': 'Introduction to Deep Learning (IN2346)',\n+        }\n+    }, {\n+        # Camera only\n+        'url': 'https://live.rbg.tum.de/w/fvv-info/16130/CAM',\n+        'md5': 'e04189d92ff2f56aedf5cede65d37aad',\n+        'info_dict': {\n+            'id': 'fvv-info/16130/CAM',\n+            'ext': 'mp4',\n+            'title': 'Fachschaftsvollversammlung',\n+            'series': 'Fachschaftsvollversammlung Informatik',\n+        }\n+    }, ]\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+        webpage = self._download_webpage(url, video_id)\n+\n+        m3u8 = self._html_search_regex(r'(https://.+?\\.m3u8)', webpage, 'm3u8')\n+        lecture_title = self._html_search_regex(r'(?si)<h1.*?>(.*)</h1>', webpage, 'title')\n+        lecture_series_title = self._html_search_regex(\n+            r'(?s)<title\\b[^>]*>\\s*(?:TUM-Live\\s\\|\\s?)?([^:]+):?.*?</title>', webpage, 'series')\n+\n+        formats = self._extract_m3u8_formats(m3u8, video_id, 'mp4', entry_protocol='m3u8_native', m3u8_id='hls')\n+        self._sort_formats(formats)\n+\n+        return {\n+            'id': video_id,\n+            'title': lecture_title,\n+            'series': lecture_series_title,\n+            'formats': formats,\n+        }\n+\n+\n+class RbgTumCourseIE(InfoExtractor):\n+    _VALID_URL = r'https://live\\.rbg\\.tum\\.de/course/(?P<id>.+)'\n+    _TESTS = [{\n+        'url': 'https://live.rbg.tum.de/course/2022/S/fpv',\n+        'info_dict': {\n+            'title': 'Funktionale Programmierung und Verifikation (IN0003)',\n+            'id': '2022/S/fpv',\n+        },\n+        'params': {\n+            'noplaylist': False,\n+        },\n+        'playlist_count': 13,\n+    }, {\n+        'url': 'https://live.rbg.tum.de/course/2022/W/set',\n+        'info_dict': {\n+            'title': 'SET FSMPIC',\n+            'id': '2022/W/set',\n+        },\n+        'params': {\n+            'noplaylist': False,\n+        },\n+        'playlist_count': 6,\n+    }, ]\n+\n+    def _real_extract(self, url):\n+        course_id = self._match_id(url)\n+        webpage = self._download_webpage(url, course_id)\n+\n+        lecture_series_title = self._html_search_regex(r'(?si)<h1.*?>(.*)</h1>', webpage, 'title')\n+\n+        lecture_urls = []\n+        for lecture_url in re.findall(r'(?i)href=\"/w/(.+)(?<!/cam)(?<!/pres)(?<!/chat)\"', webpage):\n+            lecture_urls.append(self.url_result('https://live.rbg.tum.de/w/' + lecture_url, ie=RbgTumIE.ie_key()))\n+\n+        return self.playlist_result(lecture_urls, course_id, lecture_series_title)\ndiff --git a/youtube_dl/extractor/s4c.py b/youtube_dl/extractor/s4c.py\nnew file mode 100644\nindex 00000000000..b152e668005\n--- /dev/null\n+++ b/youtube_dl/extractor/s4c.py\n@@ -0,0 +1,124 @@\n+# coding: utf-8\n+\n+from __future__ import unicode_literals\n+\n+from functools import partial as partial_f\n+\n+from .common import InfoExtractor\n+from ..utils import (\n+    float_or_none,\n+    merge_dicts,\n+    T,\n+    traverse_obj,\n+    txt_or_none,\n+    url_or_none,\n+)\n+\n+\n+class S4CIE(InfoExtractor):\n+    _VALID_URL = r'https?://(?:www\\.)?s4c\\.cymru/clic/programme/(?P<id>\\d+)'\n+    _TESTS = [{\n+        'url': 'https://www.s4c.cymru/clic/programme/861362209',\n+        'info_dict': {\n+            'id': '861362209',\n+            'ext': 'mp4',\n+            'title': 'Y Swn',\n+            'description': 'md5:f7681a30e4955b250b3224aa9fe70cf0',\n+            'duration': 5340,\n+            'thumbnail': 'https://www.s4c.cymru/amg/1920x1080/Y_Swn_2023S4C_099_ii.jpg',\n+        },\n+    }, {\n+        'url': 'https://www.s4c.cymru/clic/programme/856636948',\n+        'info_dict': {\n+            'id': '856636948',\n+            'ext': 'mp4',\n+            'title': 'Am Dro',\n+            'duration': 2880,\n+            'description': 'md5:100d8686fc9a632a0cb2db52a3433ffe',\n+            'thumbnail': 'https://www.s4c.cymru/amg/1920x1080/Am_Dro_2022-23S4C_P6_4005.jpg',\n+        },\n+    }]\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+        details = self._download_json(\n+            'https://www.s4c.cymru/df/full_prog_details',\n+            video_id, query={\n+                'lang': 'e',\n+                'programme_id': video_id,\n+            }, fatal=False)\n+\n+        player_config = self._download_json(\n+            'https://player-api.s4c-cdn.co.uk/player-configuration/prod', video_id, query={\n+                'programme_id': video_id,\n+                'signed': '0',\n+                'lang': 'en',\n+                'mode': 'od',\n+                'appId': 'clic',\n+                'streamName': '',\n+            }, note='Downloading player config JSON')\n+\n+        m3u8_url = self._download_json(\n+            'https://player-api.s4c-cdn.co.uk/streaming-urls/prod', video_id, query={\n+                'mode': 'od',\n+                'application': 'clic',\n+                'region': 'WW',\n+                'extra': 'false',\n+                'thirdParty': 'false',\n+                'filename': player_config['filename'],\n+            }, note='Downloading streaming urls JSON')['hls']\n+        formats = self._extract_m3u8_formats(m3u8_url, video_id, 'mp4', m3u8_id='hls', entry_protocol='m3u8_native')\n+        self._sort_formats(formats)\n+\n+        subtitles = {}\n+        for sub in traverse_obj(player_config, ('subtitles', lambda _, v: url_or_none(v['0']))):\n+            subtitles.setdefault(sub.get('3', 'en'), []).append({\n+                'url': sub['0'],\n+                'name': sub.get('1'),\n+            })\n+\n+        return merge_dicts({\n+            'id': video_id,\n+            'formats': formats,\n+            'subtitles': subtitles,\n+            'thumbnail': url_or_none(player_config.get('poster')),\n+        }, traverse_obj(details, ('full_prog_details', 0, {\n+            'title': (('programme_title', 'series_title'), T(txt_or_none)),\n+            'description': ('full_billing', T(txt_or_none)),\n+            'duration': ('duration', T(partial_f(float_or_none, invscale=60))),\n+        }), get_all=False),\n+            rev=True)\n+\n+\n+class S4CSeriesIE(InfoExtractor):\n+    _VALID_URL = r'https?://(?:www\\.)?s4c\\.cymru/clic/series/(?P<id>\\d+)'\n+    _TESTS = [{\n+        'url': 'https://www.s4c.cymru/clic/series/864982911',\n+        'playlist_mincount': 6,\n+        'info_dict': {\n+            'id': '864982911',\n+            'title': 'Iaith ar Daith',\n+        },\n+    }, {\n+        'url': 'https://www.s4c.cymru/clic/series/866852587',\n+        'playlist_mincount': 8,\n+        'info_dict': {\n+            'id': '866852587',\n+            'title': 'FFIT Cymru',\n+        },\n+    }]\n+\n+    def _real_extract(self, url):\n+        series_id = self._match_id(url)\n+        series_details = self._download_json(\n+            'https://www.s4c.cymru/df/series_details', series_id, query={\n+                'lang': 'e',\n+                'series_id': series_id,\n+                'show_prog_in_series': 'Y'\n+            }, note='Downloading series details JSON')\n+\n+        return self.playlist_result(\n+            (self.url_result('https://www.s4c.cymru/clic/programme/' + episode_id, S4CIE, episode_id)\n+             for episode_id in traverse_obj(series_details, ('other_progs_in_series', Ellipsis, 'id'))),\n+            playlist_id=series_id, playlist_title=traverse_obj(\n+                series_details, ('full_prog_details', 0, 'series_title', T(txt_or_none))))\ndiff --git a/youtube_dl/extractor/streamcz.py b/youtube_dl/extractor/streamcz.py\nindex 060ba32e040..97b2eb7f8d8 100644\n--- a/youtube_dl/extractor/streamcz.py\n+++ b/youtube_dl/extractor/streamcz.py\n@@ -62,7 +62,7 @@ def _extract_formats(self, spl_url, video):\n                 if not stream.get('url'):\n                     continue\n                 yield merge_dicts({\n-                    'format_id': '{}-{}'.format(format_id, ext),\n+                    'format_id': '-'.join((format_id, ext)),\n                     'ext': ext,\n                     'source_preference': pref,\n                     'url': urljoin(spl_url, stream['url']),\ndiff --git a/youtube_dl/extractor/streamsb.py b/youtube_dl/extractor/streamsb.py\nnew file mode 100644\nindex 00000000000..bffcb3de15a\n--- /dev/null\n+++ b/youtube_dl/extractor/streamsb.py\n@@ -0,0 +1,61 @@\n+# coding: utf-8\n+from __future__ import unicode_literals\n+\n+import binascii\n+import random\n+import re\n+import string\n+\n+from .common import InfoExtractor\n+from ..utils import urljoin, url_basename\n+\n+\n+def to_ascii_hex(str1):\n+    return binascii.hexlify(str1.encode('utf-8')).decode('ascii')\n+\n+\n+def generate_random_string(length):\n+    return ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(length))\n+\n+\n+class StreamsbIE(InfoExtractor):\n+    _DOMAINS = ('viewsb.com', )\n+    _VALID_URL = r'https://(?P<domain>%s)/(?P<id>.+)' % '|'.join(_DOMAINS)\n+    _TEST = {\n+        'url': 'https://viewsb.com/dxfvlu4qanjx',\n+        'md5': '488d111a63415369bf90ea83adc8a325',\n+        'info_dict': {\n+            'id': 'dxfvlu4qanjx',\n+            'ext': 'mp4',\n+            'title': 'Sintel'\n+        }\n+    }\n+\n+    def _real_extract(self, url):\n+        domain, video_id = re.match(self._VALID_URL, url).group('domain', 'id')\n+        webpage = self._download_webpage(url, video_id)\n+\n+        iframe_rel_url = self._search_regex(r'''(?i)<iframe\\b[^>]+\\bsrc\\s*=\\s*('|\")(?P<path>/.*\\.html)\\1''', webpage, 'iframe', group='path')\n+        iframe_url = urljoin('https://' + domain, iframe_rel_url)\n+\n+        iframe_data = self._download_webpage(iframe_url, video_id)\n+        app_version = self._search_regex(r'''<script\\b[^>]+\\bsrc\\s*=\\s*[\"|'].*/app\\.min\\.(\\d+)\\.js''', iframe_data, 'app version', fatal=False) or '50'\n+\n+        video_code = url_basename(iframe_url).rsplit('.')[0]\n+\n+        length = 12\n+        req = '||'.join((generate_random_string(length), video_code, generate_random_string(length), 'streamsb'))\n+        ereq = 'https://{0}/sources{1}/{2}'.format(domain, app_version, to_ascii_hex(req))\n+\n+        video_data = self._download_webpage(ereq, video_id, headers={\n+            'Referer': iframe_url,\n+            'watchsb': 'sbstream',\n+        })\n+        player_data = self._parse_json(video_data, video_id)\n+        title = player_data['stream_data']['title']\n+        formats = self._extract_m3u8_formats(player_data['stream_data']['file'], video_id, ext='mp4', entry_protocol='m3u8_native', m3u8_id='hls', fatal=False)\n+        return {\n+            'id': video_id,\n+            'formats': formats,\n+            'title': title,\n+        }\ndiff --git a/youtube_dl/extractor/telegraaf.py b/youtube_dl/extractor/telegraaf.py\nindex 2dc0205373a..5174898f2ab 100644\n--- a/youtube_dl/extractor/telegraaf.py\n+++ b/youtube_dl/extractor/telegraaf.py\n@@ -34,7 +34,9 @@ def _real_extract(self, url):\n         article_id = self._match_id(url)\n \n         video_id = self._download_json(\n-            'https://www.telegraaf.nl/graphql', article_id, query={\n+            'https://app.telegraaf.nl/graphql', article_id,\n+            headers={'User-Agent': 'De Telegraaf/6.8.11 (Android 11; en_US)'},\n+            query={\n                 'query': '''{\n   article(uid: %s) {\n     videos {\ndiff --git a/youtube_dl/extractor/telewebion.py b/youtube_dl/extractor/telewebion.py\nindex 1207b1a1b8c..30192d74e17 100644\n--- a/youtube_dl/extractor/telewebion.py\n+++ b/youtube_dl/extractor/telewebion.py\n@@ -3,17 +3,23 @@\n \n from .common import InfoExtractor\n \n+from ..utils import (\n+    float_or_none,\n+    int_or_none,\n+    url_or_none,\n+)\n+\n \n class TelewebionIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?telewebion\\.com/#!/episode/(?P<id>\\d+)'\n+    _VALID_URL = r'https?://(?:www\\.)?telewebion\\.com/(episode|clip)/(?P<id>[a-zA-Z0-9]+)'\n \n     _TEST = {\n-        'url': 'http://www.telewebion.com/#!/episode/1263668/',\n+        'url': 'http://www.telewebion.com/episode/0x1b3139c/',\n         'info_dict': {\n-            'id': '1263668',\n+            'id': '0x1b3139c',\n             'ext': 'mp4',\n             'title': '\u0642\u0631\u0639\u0647\\u200c\u06a9\u0634\u06cc \u0644\u06cc\u06af \u0642\u0647\u0631\u0645\u0627\u0646\u0627\u0646 \u0627\u0631\u0648\u067e\u0627',\n-            'thumbnail': r're:^https?://.*\\.jpg',\n+            'thumbnail': r're:^https?://static\\.telewebion\\.com/episodeImages/.*/default',\n             'view_count': int,\n         },\n         'params': {\n@@ -25,31 +31,24 @@ class TelewebionIE(InfoExtractor):\n     def _real_extract(self, url):\n         video_id = self._match_id(url)\n \n-        secure_token = self._download_webpage(\n-            'http://m.s2.telewebion.com/op/op?action=getSecurityToken', video_id)\n-        episode_details = self._download_json(\n-            'http://m.s2.telewebion.com/op/op', video_id,\n-            query={'action': 'getEpisodeDetails', 'episode_id': video_id})\n-\n-        m3u8_url = 'http://m.s1.telewebion.com/smil/%s.m3u8?filepath=%s&m3u8=1&secure_token=%s' % (\n-            video_id, episode_details['file_path'], secure_token)\n-        formats = self._extract_m3u8_formats(\n-            m3u8_url, video_id, ext='mp4', m3u8_id='hls')\n+        episode_details = self._download_json('https://gateway.telewebion.ir/kandoo/episode/getEpisodeDetail/?EpisodeId={0}'.format(video_id), video_id)\n+        episode_details = episode_details['body']['queryEpisode'][0]\n \n-        picture_paths = [\n-            episode_details.get('picture_path'),\n-            episode_details.get('large_picture_path'),\n-        ]\n+        channel_id = episode_details['channel']['descriptor']\n+        episode_image_id = episode_details.get('image')\n+        episode_image = 'https://static.telewebion.com/episodeImages/{0}/default'.format(episode_image_id) if episode_image_id else None\n \n-        thumbnails = [{\n-            'url': picture_path,\n-            'preference': idx,\n-        } for idx, picture_path in enumerate(picture_paths) if picture_path is not None]\n+        m3u8_url = 'https://cdna.telewebion.com/{0}/episode/{1}/playlist.m3u8'.format(channel_id, video_id)\n+        formats = self._extract_m3u8_formats(\n+            m3u8_url, video_id, ext='mp4', m3u8_id='hls',\n+            entry_protocol='m3u8_native')\n+        self._sort_formats(formats)\n \n         return {\n             'id': video_id,\n             'title': episode_details['title'],\n             'formats': formats,\n-            'thumbnails': thumbnails,\n-            'view_count': episode_details.get('view_count'),\n+            'thumbnail': url_or_none(episode_image),\n+            'view_count': int_or_none(episode_details.get('view_count')),\n+            'duration': float_or_none(episode_details.get('duration')),\n         }\ndiff --git a/youtube_dl/extractor/thisvid.py b/youtube_dl/extractor/thisvid.py\nnew file mode 100644\nindex 00000000000..bc4bcb2d1f4\n--- /dev/null\n+++ b/youtube_dl/extractor/thisvid.py\n@@ -0,0 +1,218 @@\n+# coding: utf-8\n+from __future__ import unicode_literals\n+\n+import re\n+import itertools\n+\n+from .common import InfoExtractor\n+from ..compat import (\n+    compat_urlparse,\n+)\n+from ..utils import (\n+    clean_html,\n+    get_element_by_class,\n+    int_or_none,\n+    merge_dicts,\n+    url_or_none,\n+    urljoin,\n+)\n+\n+\n+class ThisVidIE(InfoExtractor):\n+    _VALID_URL = r'https?://(?:www\\.)?thisvid\\.com/(?P<type>videos|embed)/(?P<id>[A-Za-z0-9-]+)'\n+    _TESTS = [{\n+        'url': 'https://thisvid.com/videos/sitting-on-ball-tight-jeans/',\n+        'md5': '839becb572995687e11a69dc4358a386',\n+        'info_dict': {\n+            'id': '3533241',\n+            'ext': 'mp4',\n+            'title': 'Sitting on ball tight jeans',\n+            'description': 'md5:372353bb995883d1b65fddf507489acd',\n+            'thumbnail': r're:https?://\\w+\\.thisvid\\.com/(?:[^/]+/)+3533241/preview\\.jpg',\n+            'uploader_id': '150629',\n+            'uploader': 'jeanslevisjeans',\n+            'age_limit': 18,\n+        }\n+    }, {\n+        'url': 'https://thisvid.com/embed/3533241/',\n+        'md5': '839becb572995687e11a69dc4358a386',\n+        'info_dict': {\n+            'id': '3533241',\n+            'ext': 'mp4',\n+            'title': 'Sitting on ball tight jeans',\n+            'thumbnail': r're:https?://\\w+\\.thisvid\\.com/(?:[^/]+/)+3533241/preview\\.jpg',\n+            'uploader_id': '150629',\n+            'uploader': 'jeanslevisjeans',\n+            'age_limit': 18,\n+        }\n+    }]\n+\n+    def _real_extract(self, url):\n+        main_id, type_ = re.match(self._VALID_URL, url).group('id', 'type')\n+        webpage = self._download_webpage(url, main_id)\n+\n+        title = self._html_search_regex(\n+            r'<title\\b[^>]*?>(?:Video:\\s+)?(.+?)(?:\\s+-\\s+ThisVid(?:\\.com| tube))?</title>',\n+            webpage, 'title')\n+\n+        if type_ == 'embed':\n+            # look for more metadata\n+            video_alt_url = url_or_none(self._search_regex(\n+                r'''video_alt_url\\s*:\\s+'(%s/)',''' % (self._VALID_URL, ),\n+                webpage, 'video_alt_url', default=None))\n+            if video_alt_url and video_alt_url != url:\n+                webpage = self._download_webpage(\n+                    video_alt_url, main_id,\n+                    note='Redirecting embed to main page', fatal=False) or webpage\n+\n+        video_holder = get_element_by_class('video-holder', webpage) or ''\n+        if '>This video is a private video' in video_holder:\n+            self.raise_login_required(\n+                (clean_html(video_holder) or 'Private video').split('\\n', 1)[0])\n+\n+        uploader = self._html_search_regex(\n+            r'''(?s)<span\\b[^>]*>Added by:\\s*</span><a\\b[^>]+\\bclass\\s*=\\s*[\"']author\\b[^>]+\\bhref\\s*=\\s*[\"']https://thisvid\\.com/members/([0-9]+/.{3,}?)\\s*</a>''',\n+            webpage, 'uploader', default='')\n+        uploader = re.split(r'''/[\"'][^>]*>\\s*''', uploader)\n+        if len(uploader) == 2:\n+            # id must be non-empty, uploader could be ''\n+            uploader_id, uploader = uploader\n+            uploader = uploader or None\n+        else:\n+            uploader_id = uploader = None\n+\n+        return merge_dicts({\n+            '_type': 'url_transparent',\n+            'title': title,\n+            'age_limit': 18,\n+            'uploader': uploader,\n+            'uploader_id': uploader_id,\n+        }, self.url_result(url, ie='Generic'))\n+\n+\n+class ThisVidMemberIE(InfoExtractor):\n+    _VALID_URL = r'https?://thisvid\\.com/members/(?P<id>\\d+)'\n+    _TESTS = [{\n+        'url': 'https://thisvid.com/members/2140501/',\n+        'info_dict': {\n+            'id': '2140501',\n+            'title': 'Rafflesia\\'s Profile',\n+        },\n+        'playlist_mincount': 16,\n+    }, {\n+        'url': 'https://thisvid.com/members/2140501/favourite_videos/',\n+        'info_dict': {\n+            'id': '2140501',\n+            'title': 'Rafflesia\\'s Favourite Videos',\n+        },\n+        'playlist_mincount': 15,\n+    }, {\n+        'url': 'https://thisvid.com/members/636468/public_videos/',\n+        'info_dict': {\n+            'id': '636468',\n+            'title': 'Happymouth\\'s Public Videos',\n+        },\n+        'playlist_mincount': 196,\n+    },\n+    ]\n+\n+    def _urls(self, html):\n+        for m in re.finditer(r'''<a\\b[^>]+\\bhref\\s*=\\s*[\"'](?P<url>%s\\b)[^>]+>''' % (ThisVidIE._VALID_URL, ), html):\n+            yield m.group('url')\n+\n+    def _real_extract(self, url):\n+        pl_id = self._match_id(url)\n+        webpage = self._download_webpage(url, pl_id)\n+\n+        title = re.split(\n+            r'(?i)\\s*\\|\\s*ThisVid\\.com\\s*$',\n+            self._og_search_title(webpage, default=None) or self._html_search_regex(r'(?s)<title\\b[^>]*>(.+?)</title', webpage, 'title', fatal=False) or '', 1)[0] or None\n+\n+        def entries(page_url, html=None):\n+            for page in itertools.count(1):\n+                if not html:\n+                    html = self._download_webpage(\n+                        page_url, pl_id, note='Downloading page %d' % (page, ),\n+                        fatal=False) or ''\n+                for u in self._urls(html):\n+                    yield u\n+                next_page = get_element_by_class('pagination-next', html) or ''\n+                if next_page:\n+                    # member list page\n+                    next_page = urljoin(url, self._search_regex(\n+                        r'''<a\\b[^>]+\\bhref\\s*=\\s*(\"|')(?P<url>(?!#)(?:(?!\\1).)+)''',\n+                        next_page, 'next page link', group='url', default=None))\n+                # in case a member page should have pagination-next with empty link, not just `else:`\n+                if next_page is None:\n+                    # playlist page\n+                    parsed_url = compat_urlparse.urlparse(page_url)\n+                    base_path, num = parsed_url.path.rsplit('/', 1)\n+                    num = int_or_none(num)\n+                    if num is None:\n+                        base_path, num = parsed_url.path.rstrip('/'), 1\n+                    parsed_url = parsed_url._replace(path=base_path + ('/%d' % (num + 1, )))\n+                    next_page = compat_urlparse.urlunparse(parsed_url)\n+                    if page_url == next_page:\n+                        next_page = None\n+                if not next_page:\n+                    break\n+                page_url, html = next_page, None\n+\n+        return self.playlist_from_matches(\n+            entries(url, webpage), playlist_id=pl_id, playlist_title=title, ie='ThisVid')\n+\n+\n+class ThisVidPlaylistIE(ThisVidMemberIE):\n+    _VALID_URL = r'https?://thisvid\\.com/playlist/(?P<id>\\d+)/video/(?P<video_id>[A-Za-z0-9-]+)'\n+    _TESTS = [{\n+        'url': 'https://thisvid.com/playlist/6615/video/big-italian-booty-28/',\n+        'info_dict': {\n+            'id': '6615',\n+            'title': 'Underwear Stuff',\n+        },\n+        'playlist_mincount': 200,\n+    }, {\n+        'url': 'https://thisvid.com/playlist/6615/video/big-italian-booty-28/',\n+        'info_dict': {\n+            'id': '1072387',\n+            'ext': 'mp4',\n+            'title': 'Big Italian Booty 28',\n+            'description': 'md5:1bccf7b13765e18fb27bf764dba7ede2',\n+            'uploader_id': '367912',\n+            'uploader': 'Jcmusclefun',\n+            'age_limit': 18,\n+        },\n+        'params': {\n+            'noplaylist': True,\n+        },\n+    }]\n+\n+    def _get_video_url(self, pl_url):\n+        video_id = re.match(self._VALID_URL, pl_url).group('video_id')\n+        return urljoin(pl_url, '/videos/%s/' % (video_id, ))\n+\n+    def _urls(self, html):\n+        for m in re.finditer(r'''<a\\b[^>]+\\bhref\\s*=\\s*[\"'](?P<url>%s\\b)[^>]+>''' % (self._VALID_URL, ), html):\n+            yield self._get_video_url(m.group('url'))\n+\n+    def _real_extract(self, url):\n+        pl_id = self._match_id(url)\n+\n+        if self._downloader.params.get('noplaylist'):\n+            self.to_screen('Downloading just the featured video because of --no-playlist')\n+            return self.url_result(self._get_video_url(url), 'ThisVid')\n+\n+        self.to_screen(\n+            'Downloading playlist %s - add --no-playlist to download just the featured video' % (pl_id, ))\n+        result = super(ThisVidPlaylistIE, self)._real_extract(url)\n+\n+        # rework title returned as `the title - the title`\n+        title = result['title']\n+        t_len = len(title)\n+        if t_len > 5 and t_len % 2 != 0:\n+            t_len = t_len // 2\n+            if title[t_len] == '-':\n+                title = [t.strip() for t in (title[:t_len], title[t_len + 1:])]\n+                if title[0] and title[0] == title[1]:\n+                    result['title'] = title[0]\n+        return result\ndiff --git a/youtube_dl/extractor/uktvplay.py b/youtube_dl/extractor/uktvplay.py\nindex f28fd514db6..9ef9638cde9 100644\n--- a/youtube_dl/extractor/uktvplay.py\n+++ b/youtube_dl/extractor/uktvplay.py\n@@ -5,7 +5,7 @@\n \n \n class UKTVPlayIE(InfoExtractor):\n-    _VALID_URL = r'https?://uktvplay\\.uktv\\.co\\.uk/(?:.+?\\?.*?\\bvideo=|([^/]+/)*watch-online/)(?P<id>\\d+)'\n+    _VALID_URL = r'https?://uktvplay\\.(?:uktv\\.)?co\\.uk/(?:.+?\\?.*?\\bvideo=|([^/]+/)*watch-online/)(?P<id>\\d+)'\n     _TESTS = [{\n         'url': 'https://uktvplay.uktv.co.uk/shows/world-at-war/c/200/watch-online/?video=2117008346001',\n         'info_dict': {\ndiff --git a/youtube_dl/extractor/vbox7.py b/youtube_dl/extractor/vbox7.py\nindex 8152acefd09..1c0b770749b 100644\n--- a/youtube_dl/extractor/vbox7.py\n+++ b/youtube_dl/extractor/vbox7.py\n@@ -2,9 +2,22 @@\n from __future__ import unicode_literals\n \n import re\n+import time\n \n from .common import InfoExtractor\n-from ..utils import ExtractorError\n+from ..compat import compat_kwargs\n+from ..utils import (\n+    base_url,\n+    determine_ext,\n+    ExtractorError,\n+    float_or_none,\n+    merge_dicts,\n+    T,\n+    traverse_obj,\n+    txt_or_none,\n+    url_basename,\n+    url_or_none,\n+)\n \n \n class Vbox7IE(InfoExtractor):\n@@ -20,23 +33,27 @@ class Vbox7IE(InfoExtractor):\n                         )\n                         (?P<id>[\\da-fA-F]+)\n                     '''\n+    _EMBED_REGEX = [r'<iframe[^>]+src=(?P<q>[\"\\'])(?P<url>(?:https?:)?//vbox7\\.com/emb/external\\.php.+?)(?P=q)']\n     _GEO_COUNTRIES = ['BG']\n     _TESTS = [{\n-        'url': 'http://vbox7.com/play:0946fff23c',\n-        'md5': 'a60f9ab3a3a2f013ef9a967d5f7be5bf',\n+        # the http: URL just redirects here\n+        'url': 'https://vbox7.com/play:0946fff23c',\n+        'md5': '50ca1f78345a9c15391af47d8062d074',\n         'info_dict': {\n             'id': '0946fff23c',\n             'ext': 'mp4',\n             'title': '\u0411\u043e\u0440\u0438\u0441\u043e\u0432: \u041f\u0440\u0438\u0442\u0435\u0441\u043d\u0435\u043d \u0441\u044a\u043c \u0437\u0430 \u0431\u044a\u0434\u0435\u0449\u0435\u0442\u043e \u043d\u0430 \u0411\u044a\u043b\u0433\u0430\u0440\u0438\u044f',\n             'description': '\u041f\u043e \u0434\u0443\u043c\u0438\u0442\u0435 \u043c\u0443 \u0435 \u043e\u043f\u0430\u0441\u043d\u043e \u0441\u0442\u0440\u0430\u043d\u0430\u0442\u0430 \u043d\u0438 \u0434\u0430 \u0431\u044a\u0434\u0435 \u043e\u0431\u044f\u0432\u0435\u043d\u0430 \u0437\u0430 \"\u0441\u0438\u0433\u0443\u0440\u043d\u0430\"',\n-            'thumbnail': r're:^https?://.*\\.jpg$',\n             'timestamp': 1470982814,\n             'upload_date': '20160812',\n             'uploader': 'zdraveibulgaria',\n+            'thumbnail': r're:^https?://.*\\.jpg$',\n+            'view_count': int,\n+            'duration': 2640,\n         },\n-        'params': {\n-            'proxy': '127.0.0.1:8118',\n-        },\n+        'expected_warnings': [\n+            'Unable to download webpage',\n+        ],\n     }, {\n         'url': 'http://vbox7.com/play:249bb972c2',\n         'md5': '99f65c0c9ef9b682b97313e052734c3f',\n@@ -44,8 +61,15 @@ class Vbox7IE(InfoExtractor):\n             'id': '249bb972c2',\n             'ext': 'mp4',\n             'title': '\u0421\u043c\u044f\u0445! \u0427\u0443\u0434\u043e - \u0447\u0438\u0441\u0442 \u0437\u0430 \u0441\u0435\u043a\u0443\u043d\u0434\u0438 - \u0421\u043a\u0440\u0438\u0442\u0430 \u043a\u0430\u043c\u0435\u0440\u0430',\n+            'description': '\u0421\u043c\u044f\u0445! \u0427\u0443\u0434\u043e - \u0447\u0438\u0441\u0442 \u0437\u0430 \u0441\u0435\u043a\u0443\u043d\u0434\u0438 - \u0421\u043a\u0440\u0438\u0442\u0430 \u043a\u0430\u043c\u0435\u0440\u0430',\n+            'timestamp': 1360215023,\n+            'upload_date': '20130207',\n+            'uploader': 'svideteliat_ot_varshava',\n+            'thumbnail': 'https://i49.vbox7.com/o/249/249bb972c20.jpg',\n+            'view_count': int,\n+            'duration': 83,\n         },\n-        'skip': 'georestricted',\n+        'expected_warnings': ['Failed to download m3u8 information'],\n     }, {\n         'url': 'http://vbox7.com/emb/external.php?vid=a240d20f9c&autoplay=1',\n         'only_matching': True,\n@@ -54,52 +78,127 @@ class Vbox7IE(InfoExtractor):\n         'only_matching': True,\n     }]\n \n-    @staticmethod\n-    def _extract_url(webpage):\n-        mobj = re.search(\n-            r'<iframe[^>]+src=(?P<q>[\"\\'])(?P<url>(?:https?:)?//vbox7\\.com/emb/external\\.php.+?)(?P=q)',\n-            webpage)\n+    @classmethod\n+    def _extract_url(cls, webpage):\n+        mobj = re.search(cls._EMBED_REGEX[0], webpage)\n         if mobj:\n             return mobj.group('url')\n \n+    # specialisation to transform what looks like ld+json that\n+    # may contain invalid character combinations\n+\n+    # transform_source=None, fatal=True\n+    def _parse_json(self, json_string, video_id, *args, **kwargs):\n+        if '\"@context\"' in json_string[:30]:\n+            # this is ld+json, or that's the way to bet\n+            transform_source = args[0] if len(args) > 0 else kwargs.get('transform_source')\n+            if not transform_source:\n+\n+                def fix_chars(src):\n+                    # fix malformed ld+json: replace raw CRLFs with escaped LFs\n+                    return re.sub(\n+                        r'\"[^\"]+\"', lambda m: re.sub(r'\\r?\\n', r'\\\\n', m.group(0)), src)\n+\n+                if len(args) > 0:\n+                    args = (fix_chars,) + args[1:]\n+                else:\n+                    kwargs['transform_source'] = fix_chars\n+                    kwargs = compat_kwargs(kwargs)\n+\n+        return super(Vbox7IE, self)._parse_json(\n+            json_string, video_id, *args, **kwargs)\n+\n     def _real_extract(self, url):\n         video_id = self._match_id(url)\n+        url = 'https://vbox7.com/play:%s' % (video_id,)\n \n+        now = time.time()\n         response = self._download_json(\n-            'https://www.vbox7.com/ajax/video/nextvideo.php?vid=%s' % video_id,\n-            video_id)\n+            'https://www.vbox7.com/aj/player/item/options', video_id,\n+            query={'vid': video_id}, headers={'Referer': url})\n+        # estimate time to which possible `ago` member is relative\n+        now = now + 0.5 * (time.time() - now)\n \n-        if 'error' in response:\n+        if traverse_obj(response, 'error'):\n             raise ExtractorError(\n                 '%s said: %s' % (self.IE_NAME, response['error']), expected=True)\n \n-        video = response['options']\n-\n-        title = video['title']\n-        video_url = video['src']\n+        src_url = traverse_obj(response, ('options', 'src', T(url_or_none))) or ''\n \n-        if '/na.mp4' in video_url:\n+        fmt_base = url_basename(src_url).rsplit('.', 1)[0].rsplit('_', 1)[0]\n+        if fmt_base in ('na', 'vn'):\n             self.raise_geo_restricted(countries=self._GEO_COUNTRIES)\n \n-        uploader = video.get('uploader')\n-\n-        webpage = self._download_webpage(\n-            'http://vbox7.com/play:%s' % video_id, video_id, fatal=None)\n-\n-        info = {}\n-\n-        if webpage:\n-            info = self._search_json_ld(\n-                webpage.replace('\"/*@context\"', '\"@context\"'), video_id,\n-                fatal=False)\n-\n-        info.update({\n+        ext = determine_ext(src_url)\n+        if ext == 'mpd':\n+            # extract MPD\n+            try:\n+                formats, subtitles = self._extract_mpd_formats_and_subtitles(\n+                    src_url, video_id, 'dash', fatal=False)\n+            except KeyError:  # fatal doesn't catch this\n+                self.report_warning('Failed to parse MPD manifest')\n+                formats, subtitles = [], {}\n+        elif ext != 'm3u8':\n+            formats = [{\n+                'url': src_url,\n+            }] if src_url else []\n+            subtitles = {}\n+\n+        if src_url:\n+            # possibly extract HLS, based on https://github.com/yt-dlp/yt-dlp/pull/9100\n+            fmt_base = base_url(src_url) + fmt_base\n+            # prepare for _extract_m3u8_formats_and_subtitles()\n+            # hls_formats, hls_subs = self._extract_m3u8_formats_and_subtitles(\n+            hls_formats = self._extract_m3u8_formats(\n+                '{0}.m3u8'.format(fmt_base), video_id, m3u8_id='hls', fatal=False)\n+            formats.extend(hls_formats)\n+            # self._merge_subtitles(hls_subs, target=subtitles)\n+\n+            # In case MPD/HLS cannot be parsed, or anyway, get mp4 combined\n+            # formats usually provided to Safari, iOS, and old Windows\n+            video = response['options']\n+            resolutions = (1080, 720, 480, 240, 144)\n+            highest_res = traverse_obj(video, (\n+                'highestRes', T(int))) or resolutions[0]\n+            resolutions = traverse_obj(video, (\n+                'resolutions', lambda _, r: highest_res >= int(r) > 0)) or resolutions\n+            mp4_formats = traverse_obj(resolutions, (\n+                Ellipsis, T(lambda res: {\n+                    'url': '{0}_{1}.mp4'.format(fmt_base, res),\n+                    'format_id': 'http-{0}'.format(res),\n+                    'height': res,\n+                })))\n+            # if above formats are flaky, enable the line below\n+            # self._check_formats(mp4_formats, video_id)\n+            formats.extend(mp4_formats)\n+\n+        self._sort_formats(formats)\n+\n+        webpage = self._download_webpage(url, video_id, fatal=False) or ''\n+\n+        info = self._search_json_ld(\n+            webpage.replace('\"/*@context\"', '\"@context\"'), video_id,\n+            fatal=False) if webpage else {}\n+\n+        if not info.get('title'):\n+            info['title'] = traverse_obj(response, (\n+                'options', 'title', T(txt_or_none))) or self._og_search_title(webpage)\n+\n+        def if_missing(k):\n+            return lambda x: None if k in info else x\n+\n+        info = merge_dicts(info, {\n             'id': video_id,\n-            'title': title,\n-            'url': video_url,\n-            'uploader': uploader,\n-            'thumbnail': self._proto_relative_url(\n+            'formats': formats,\n+            'subtitles': subtitles or None,\n+        }, info, traverse_obj(response, ('options', {\n+            'uploader': ('uploader', T(txt_or_none)),\n+            'timestamp': ('ago', T(if_missing('timestamp')), T(lambda t: int(round((now - t) / 60.0)) * 60)),\n+            'duration': ('duration', T(if_missing('duration')), T(float_or_none)),\n+        })))\n+        if 'thumbnail' not in info:\n+            info['thumbnail'] = self._proto_relative_url(\n                 info.get('thumbnail') or self._og_search_thumbnail(webpage),\n-                'http:'),\n-        })\n+                'https:'),\n+\n         return info\ndiff --git a/youtube_dl/extractor/videa.py b/youtube_dl/extractor/videa.py\nindex 4589e78a1a8..194b4b011e7 100644\n--- a/youtube_dl/extractor/videa.py\n+++ b/youtube_dl/extractor/videa.py\n@@ -6,22 +6,31 @@\n import string\n \n from .common import InfoExtractor\n+from ..compat import (\n+    compat_b64decode,\n+    compat_ord,\n+    compat_struct_pack,\n+)\n from ..utils import (\n     ExtractorError,\n     int_or_none,\n     mimetype2ext,\n     parse_codecs,\n+    parse_qs,\n     update_url_query,\n     urljoin,\n     xpath_element,\n     xpath_text,\n )\n-from ..compat import (\n-    compat_b64decode,\n-    compat_ord,\n-    compat_struct_pack,\n-    compat_urlparse,\n-)\n+\n+\n+def compat_random_choices(population, *args, **kwargs):\n+    # weights=None, *, cum_weights=None, k=1\n+    # limited implementation needed here\n+    weights = args[0] if args else kwargs.get('weights')\n+    assert all(w is None for w in (weights, kwargs.get('cum_weights')))\n+    k = kwargs.get('k', 1)\n+    return ''.join(random.choice(population) for _ in range(k))\n \n \n class VideaIE(InfoExtractor):\n@@ -35,6 +44,7 @@ class VideaIE(InfoExtractor):\n                         )\n                         (?P<id>[^?#&]+)\n                     '''\n+    _EMBED_REGEX = [r'<iframe[^>]+src=([\"\\'])(?P<url>(?:https?:)?//videa\\.hu/player\\?.*?\\bv=.+?)\\1']\n     _TESTS = [{\n         'url': 'http://videa.hu/videok/allatok/az-orult-kigyasz-285-kigyot-kigyo-8YfIAjxwWGwT8HVQ',\n         'md5': '97a7af41faeaffd9f1fc864a7c7e7603',\n@@ -44,6 +54,7 @@ class VideaIE(InfoExtractor):\n             'title': 'Az \u0151r\u00fclt k\u00edgy\u00e1sz 285 k\u00edgy\u00f3t enged szabadon',\n             'thumbnail': r're:^https?://.*',\n             'duration': 21,\n+            'age_limit': 0,\n         },\n     }, {\n         'url': 'http://videa.hu/videok/origo/jarmuvek/supercars-elozes-jAHDWfWSJH5XuFhH',\n@@ -54,6 +65,7 @@ class VideaIE(InfoExtractor):\n             'title': 'Supercars el\u0151z\u00e9s',\n             'thumbnail': r're:^https?://.*',\n             'duration': 64,\n+            'age_limit': 0,\n         },\n     }, {\n         'url': 'http://videa.hu/player?v=8YfIAjxwWGwT8HVQ',\n@@ -64,6 +76,7 @@ class VideaIE(InfoExtractor):\n             'title': 'Az \u0151r\u00fclt k\u00edgy\u00e1sz 285 k\u00edgy\u00f3t enged szabadon',\n             'thumbnail': r're:^https?://.*',\n             'duration': 21,\n+            'age_limit': 0,\n         },\n     }, {\n         'url': 'http://videa.hu/player/v/8YfIAjxwWGwT8HVQ?autoplay=1',\n@@ -80,11 +93,14 @@ class VideaIE(InfoExtractor):\n     }]\n     _STATIC_SECRET = 'xHb0ZvME5q8CBcoQi6AngerDu3FGO9fkUlwPmLVY_RTzj2hJIS4NasXWKy1td7p'\n \n-    @staticmethod\n-    def _extract_urls(webpage):\n-        return [url for _, url in re.findall(\n-            r'<iframe[^>]+src=([\"\\'])(?P<url>(?:https?:)?//videa\\.hu/player\\?.*?\\bv=.+?)\\1',\n-            webpage)]\n+    @classmethod\n+    def _extract_urls(cls, webpage):\n+        def yield_urls():\n+            for pattern in cls._EMBED_REGEX:\n+                for m in re.finditer(pattern, webpage):\n+                    yield m.group('url')\n+\n+        return list(yield_urls())\n \n     @staticmethod\n     def rc4(cipher_text, key):\n@@ -130,13 +146,13 @@ def _real_extract(self, url):\n         for i in range(0, 32):\n             result += s[i - (self._STATIC_SECRET.index(l[i]) - 31)]\n \n-        query = compat_urlparse.parse_qs(compat_urlparse.urlparse(player_url).query)\n-        random_seed = ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(8))\n+        query = parse_qs(player_url)\n+        random_seed = ''.join(compat_random_choices(string.ascii_letters + string.digits, k=8))\n         query['_s'] = random_seed\n         query['_t'] = result[:16]\n \n         b64_info, handle = self._download_webpage_handle(\n-            'http://videa.hu/videaplayer_get_xml.php', video_id, query=query)\n+            'http://videa.hu/player/xml', video_id, query=query)\n         if b64_info.startswith('<?xml'):\n             info = self._parse_xml(b64_info, video_id)\n         else:\ndiff --git a/youtube_dl/extractor/vidlii.py b/youtube_dl/extractor/vidlii.py\nindex f4774256bd5..47f328e8743 100644\n--- a/youtube_dl/extractor/vidlii.py\n+++ b/youtube_dl/extractor/vidlii.py\n@@ -4,6 +4,7 @@\n import re\n \n from .common import InfoExtractor\n+\n from ..utils import (\n     float_or_none,\n     get_element_by_id,\n@@ -11,6 +12,7 @@\n     strip_or_none,\n     unified_strdate,\n     urljoin,\n+    str_to_int,\n )\n \n \n@@ -35,6 +37,26 @@ class VidLiiIE(InfoExtractor):\n             'categories': ['News & Politics'],\n             'tags': ['Vidlii', 'Jan', 'Videogames'],\n         }\n+    }, {\n+        # HD\n+        'url': 'https://www.vidlii.com/watch?v=2Ng8Abj2Fkl',\n+        'md5': '450e7da379c884788c3a4fa02a3ce1a4',\n+        'info_dict': {\n+            'id': '2Ng8Abj2Fkl',\n+            'ext': 'mp4',\n+            'title': 'test',\n+            'description': 'md5:cc55a86032a7b6b3cbfd0f6b155b52e9',\n+            'thumbnail': 'https://www.vidlii.com/usfi/thmp/2Ng8Abj2Fkl.jpg',\n+            'uploader': 'VidLii',\n+            'uploader_url': 'https://www.vidlii.com/user/VidLii',\n+            'upload_date': '20200927',\n+            'duration': 5,\n+            'view_count': int,\n+            'comment_count': int,\n+            'average_rating': float,\n+            'categories': ['Film & Animation'],\n+            'tags': list,\n+        },\n     }, {\n         'url': 'https://www.vidlii.com/embed?v=tJluaH4BJ3v&a=0',\n         'only_matching': True,\n@@ -46,11 +68,32 @@ def _real_extract(self, url):\n         webpage = self._download_webpage(\n             'https://www.vidlii.com/watch?v=%s' % video_id, video_id)\n \n-        video_url = self._search_regex(\n-            r'src\\s*:\\s*([\"\\'])(?P<url>(?:https?://)?(?:(?!\\1).)+)\\1', webpage,\n-            'video url', group='url')\n+        formats = []\n+\n+        def add_format(format_url, height=None):\n+            height = int(self._search_regex(r'(\\d+)\\.mp4',\n+                         format_url, 'height', default=360))\n+\n+            formats.append({\n+                'url': format_url,\n+                'format_id': '%dp' % height if height else None,\n+                'height': height,\n+            })\n+\n+        sources = re.findall(\n+            r'src\\s*:\\s*([\"\\'])(?P<url>(?:https?://)?(?:(?!\\1).)+)\\1',\n+            webpage)\n+\n+        formats = []\n+        if len(sources) > 1:\n+            add_format(sources[1][1])\n+            self._check_formats(formats, video_id)\n+        if len(sources) > 0:\n+            add_format(sources[0][1])\n+\n+        self._sort_formats(formats)\n \n-        title = self._search_regex(\n+        title = self._html_search_regex(\n             (r'<h1>([^<]+)</h1>', r'<title>([^<]+) - VidLii<'), webpage,\n             'title')\n \n@@ -82,9 +125,9 @@ def _real_extract(self, url):\n             default=None) or self._search_regex(\n             r'duration\\s*:\\s*(\\d+)', webpage, 'duration', fatal=False))\n \n-        view_count = int_or_none(self._search_regex(\n-            (r'<strong>(\\d+)</strong> views',\n-             r'Views\\s*:\\s*<strong>(\\d+)</strong>'),\n+        view_count = str_to_int(self._html_search_regex(\n+            (r'<strong>([\\d,.]+)</strong> views',\n+             r'Views\\s*:\\s*<strong>([\\d,.]+)</strong>'),\n             webpage, 'view count', fatal=False))\n \n         comment_count = int_or_none(self._search_regex(\n@@ -109,7 +152,7 @@ def _real_extract(self, url):\n \n         return {\n             'id': video_id,\n-            'url': video_url,\n+            'formats': formats,\n             'title': title,\n             'description': description,\n             'thumbnail': thumbnail,\ndiff --git a/youtube_dl/extractor/vimeo.py b/youtube_dl/extractor/vimeo.py\nindex 0b386f450b7..47ec0a9b449 100644\n--- a/youtube_dl/extractor/vimeo.py\n+++ b/youtube_dl/extractor/vimeo.py\n@@ -261,27 +261,33 @@ class VimeoIE(VimeoBaseInfoExtractor):\n \n     # _VALID_URL matches Vimeo URLs\n     _VALID_URL = r'''(?x)\n-                    https?://\n-                        (?:\n-                            (?:\n-                                www|\n-                                player\n-                            )\n-                            \\.\n-                        )?\n-                        vimeo(?:pro)?\\.com/\n-                        (?!(?:channels|album|showcase)/[^/?#]+/?(?:$|[?#])|[^/]+/review/|ondemand/)\n-                        (?:.*?/)?\n-                        (?:\n-                            (?:\n-                                play_redirect_hls|\n-                                moogaloop\\.swf)\\?clip_id=\n-                            )?\n-                        (?:videos?/)?\n-                        (?P<id>[0-9]+)\n-                        (?:/(?P<unlisted_hash>[\\da-f]{10}))?\n-                        /?(?:[?&].*)?(?:[#].*)?$\n-                    '''\n+                     https?://\n+                         (?:\n+                             (?:\n+                                 www|\n+                                 player\n+                             )\n+                             \\.\n+                         )?\n+                         vimeo(?:pro)?\\.com/\n+                         (?:\n+                             (?P<u>user)|\n+                             (?!(?:channels|album|showcase)/[^/?#]+/?(?:$|[?#])|[^/]+/review/|ondemand/)\n+                             (?:.*?/)??\n+                             (?P<q>\n+                                 (?:\n+                                     play_redirect_hls|\n+                                     moogaloop\\.swf)\\?clip_id=\n+                             )?\n+                             (?:videos?/)?\n+                         )\n+                         (?P<id>[0-9]+)\n+                         (?(u)\n+                             /(?!videos|likes)[^/?#]+/?|\n+                             (?(q)|/(?P<unlisted_hash>[\\da-f]{10}))?\n+                         )\n+                         (?:(?(q)[&]|(?(u)|/?)[?]).+?)?(?:[#].*)?$\n+                 '''\n     IE_NAME = 'vimeo'\n     _TESTS = [\n         {\n@@ -518,14 +524,33 @@ class VimeoIE(VimeoBaseInfoExtractor):\n             'only_matching': True,\n         },\n         {\n-            'url': 'https://vimeo.com/160743502/abd0e13fb4',\n+            # requires passing unlisted_hash(a52724358e) to load_download_config request\n+            'url': 'https://vimeo.com/392479337/a52724358e',\n             'only_matching': True,\n         },\n         {\n-            # requires passing unlisted_hash(a52724358e) to load_download_config request\n-            'url': 'https://vimeo.com/392479337/a52724358e',\n+            # similar, but all numeric: ID must be 581039021, not 9603038895\n+            # issue #29690\n+            'url': 'https://vimeo.com/581039021/9603038895',\n+            'info_dict': {\n+                'id': '581039021',\n+                # these have to be provided but we don't care\n+                'ext': 'mp4',\n+                'timestamp': 1627621014,\n+                'title': 're:.+',\n+                'uploader_id': 're:.+',\n+                'uploader': 're:.+',\n+                'upload_date': r're:\\d+',\n+            },\n+            'params': {\n+                'skip_download': True,\n+            },\n+        },\n+        {\n+            # user playlist alias -> https://vimeo.com/258705797\n+            'url': 'https://vimeo.com/user26785108/newspiritualguide',\n             'only_matching': True,\n-        }\n+        },\n         # https://gettingthingsdone.com/workflowmap/\n         # vimeo embed with check-password page protected by Referer header\n     ]\n@@ -648,8 +673,8 @@ def _real_extract(self, url):\n             raise\n \n         if '//player.vimeo.com/video/' in url:\n-            config = self._parse_json(self._search_regex(\n-                r'\\bconfig\\s*=\\s*({.+?})\\s*;', webpage, 'info section'), video_id)\n+            config = self._search_json(\n+                r'\\b(?:playerC|c)onfig\\s*=', webpage, 'info section', video_id)\n             if config.get('view') == 4:\n                 config = self._verify_player_video_password(\n                     redirect_url, video_id, headers)\ndiff --git a/youtube_dl/extractor/vvvvid.py b/youtube_dl/extractor/vvvvid.py\nindex bc196f8a0ac..6a0d4e8f020 100644\n--- a/youtube_dl/extractor/vvvvid.py\n+++ b/youtube_dl/extractor/vvvvid.py\n@@ -64,6 +64,18 @@ class VVVVIDIE(InfoExtractor):\n         'params': {\n             'skip_download': True,\n         },\n+    }, {\n+        # video_type == 'video/dash'\n+        'url': 'https://www.vvvvid.it/show/683/made-in-abyss/1542/693786/nanachi',\n+        'info_dict': {\n+            'id': '693786',\n+            'ext': 'mp4',\n+            'title': 'Nanachi',\n+        },\n+        'params': {\n+            'skip_download': True,\n+            'format': 'mp4',\n+        },\n     }, {\n         'url': 'https://www.vvvvid.it/show/434/perche-dovrei-guardarlo-di-dario-moccia/437/489048',\n         'only_matching': True\n@@ -205,6 +217,9 @@ def metadata_from_url(r_url):\n                 })\n                 is_youtube = True\n                 break\n+            elif video_type == 'video/dash':\n+                formats.extend(self._extract_m3u8_formats(\n+                    embed_code, video_id, 'mp4', m3u8_id='hls', fatal=False))\n             else:\n                 formats.extend(self._extract_wowza_formats(\n                     'http://sb.top-ix.org/videomg/_definst_/mp4:%s/playlist.m3u8' % embed_code, video_id))\ndiff --git a/youtube_dl/extractor/wat.py b/youtube_dl/extractor/wat.py\nindex f1bccc2d6b2..b15e0376889 100644\n--- a/youtube_dl/extractor/wat.py\n+++ b/youtube_dl/extractor/wat.py\n@@ -57,7 +57,7 @@ def _real_extract(self, url):\n         #     'http://www.wat.tv/interface/contentv4s/' + video_id, video_id)\n         video_data = self._download_json(\n             'https://mediainfo.tf1.fr/mediainfocombo/' + video_id,\n-            video_id, query={'context': 'MYTF1'})\n+            video_id, query={'context': 'MYTF1', 'pver': '4001000'})\n         video_info = video_data['media']\n \n         error_desc = video_info.get('error_desc')\ndiff --git a/youtube_dl/extractor/whyp.py b/youtube_dl/extractor/whyp.py\nnew file mode 100644\nindex 00000000000..644eb4617eb\n--- /dev/null\n+++ b/youtube_dl/extractor/whyp.py\n@@ -0,0 +1,55 @@\n+# coding: utf-8\n+from __future__ import unicode_literals\n+\n+from .common import InfoExtractor\n+from ..utils import (\n+    float_or_none,\n+    merge_dicts,\n+    str_or_none,\n+    T,\n+    traverse_obj,\n+    url_or_none,\n+)\n+\n+\n+class WhypIE(InfoExtractor):\n+    _VALID_URL = r'https?://(?:www\\.)?whyp\\.it/tracks/(?P<id>\\d+)'\n+    _TESTS = [{\n+        'url': 'https://www.whyp.it/tracks/18337/home-page-example-track-b4kq7',\n+        'md5': 'c1187b42ebf8605284e3dc92aeb33d16',\n+        'info_dict': {\n+            'url': 'https://cdn.whyp.it/50eb17cc-e9ff-4e18-b89b-dc9206a95cb1.mp3',\n+            'id': '18337',\n+            'title': 'Home Page Example Track',\n+            'description': r're:(?s).+\\bexample track\\b',\n+            'ext': 'mp3',\n+            'duration': 52.82,\n+            'uploader': 'Brad',\n+            'uploader_id': '1',\n+            'thumbnail': 'https://cdn.whyp.it/a537bb36-3373-4c61-96c8-27fc1b2f427a.jpg',\n+        },\n+    }, {\n+        'url': 'https://www.whyp.it/tracks/18337',\n+        'only_matching': True,\n+    }]\n+\n+    def _real_extract(self, url):\n+        unique_id = self._match_id(url)\n+        webpage = self._download_webpage(url, unique_id)\n+        data = self._search_nuxt_data(webpage, unique_id)['rawTrack']\n+\n+        return merge_dicts({\n+            'url': data['audio_url'],\n+            'id': unique_id,\n+        }, traverse_obj(data, {\n+            'title': 'title',\n+            'description': 'description',\n+            'duration': ('duration', T(float_or_none)),\n+            'uploader': ('user', 'username'),\n+            'uploader_id': ('user', 'id', T(str_or_none)),\n+            'thumbnail': ('artwork_url', T(url_or_none)),\n+        }), {\n+            'ext': 'mp3',\n+            'vcodec': 'none',\n+            'http_headers': {'Referer': 'https://whyp.it/'},\n+        }, rev=True)\ndiff --git a/youtube_dl/extractor/xfileshare.py b/youtube_dl/extractor/xfileshare.py\nindex df9efa9faed..4dc3032e7e0 100644\n--- a/youtube_dl/extractor/xfileshare.py\n+++ b/youtube_dl/extractor/xfileshare.py\n@@ -4,20 +4,28 @@\n import re\n \n from .common import InfoExtractor\n-from ..compat import compat_chr\n+from ..compat import (\n+    compat_chr,\n+    compat_zip as zip,\n+)\n from ..utils import (\n+    clean_html,\n     decode_packed_codes,\n     determine_ext,\n     ExtractorError,\n+    get_element_by_id,\n     int_or_none,\n-    js_to_json,\n+    merge_dicts,\n+    T,\n+    traverse_obj,\n+    url_or_none,\n     urlencode_postdata,\n )\n \n \n # based on openload_decode from 2bfeee69b976fe049761dd3012e30b637ee05a58\n def aa_decode(aa_code):\n-    symbol_table = [\n+    symbol_table = (\n         ('7', '((\uff9f\uff70\uff9f) + (o^_^o))'),\n         ('6', '((o^_^o) +(o^_^o))'),\n         ('5', '((\uff9f\uff70\uff9f) + (\uff9f\u0398\uff9f))'),\n@@ -26,84 +34,180 @@ def aa_decode(aa_code):\n         ('3', '(o^_^o)'),\n         ('1', '(\uff9f\u0398\uff9f)'),\n         ('0', '(c^_^o)'),\n-    ]\n+        ('+', ''),\n+    )\n     delim = '(\uff9f\u0414\uff9f)[\uff9f\u03b5\uff9f]+'\n-    ret = ''\n-    for aa_char in aa_code.split(delim):\n+\n+    def chr_from_code(c):\n         for val, pat in symbol_table:\n-            aa_char = aa_char.replace(pat, val)\n-        aa_char = aa_char.replace('+ ', '')\n-        m = re.match(r'^\\d+', aa_char)\n-        if m:\n-            ret += compat_chr(int(m.group(0), 8))\n+            c = c.replace(pat, val)\n+        if c.startswith(('u', 'U')):\n+            base = 16\n+            c = c[1:]\n         else:\n-            m = re.match(r'^u([\\da-f]+)', aa_char)\n-            if m:\n-                ret += compat_chr(int(m.group(1), 16))\n-    return ret\n+            base = 10\n+        c = int_or_none(c, base=base)\n+        return '' if c is None else compat_chr(c)\n+\n+    return ''.join(\n+        chr_from_code(aa_char)\n+        for aa_char in aa_code.split(delim))\n \n \n class XFileShareIE(InfoExtractor):\n     _SITES = (\n-        (r'aparat\\.cam', 'Aparat'),\n-        (r'clipwatching\\.com', 'ClipWatching'),\n-        (r'gounlimited\\.to', 'GoUnlimited'),\n-        (r'govid\\.me', 'GoVid'),\n-        (r'holavid\\.com', 'HolaVid'),\n-        (r'streamty\\.com', 'Streamty'),\n-        (r'thevideobee\\.to', 'TheVideoBee'),\n-        (r'uqload\\.com', 'Uqload'),\n-        (r'vidbom\\.com', 'VidBom'),\n-        (r'vidlo\\.us', 'vidlo'),\n-        (r'vidlocker\\.xyz', 'VidLocker'),\n-        (r'vidshare\\.tv', 'VidShare'),\n-        (r'vup\\.to', 'VUp'),\n+        # status check 2024-02: site availability, G site: search\n+        (r'aparat\\.cam', 'Aparat'),  # Cloudflare says host error 522, apparently changed to wolfstreeam.tv\n+        (r'filemoon\\.sx/.', 'FileMoon'),\n+        (r'gounlimited\\.to', 'GoUnlimited'),  # no media pages listed\n+        (r'govid\\.me', 'GoVid'),  # no media pages listed\n+        (r'highstream\\.tv', 'HighStream'),  # clipwatching.com redirects here\n+        (r'holavid\\.com', 'HolaVid'),  # Cloudflare says host error 522\n+        # (r'streamty\\.com', 'Streamty'),  # no media pages listed, connection timeout\n+        # (r'thevideobee\\.to', 'TheVideoBee'),  # no pages listed, refuses connection\n+        (r'uqload\\.to', 'Uqload'),  # .com, .co redirect here\n+        (r'(?:vedbam\\.xyz|vadbam.net)', 'V?dB?m'),  # vidbom.com redirects here, but no valid media pages listed\n+        (r'vidlo\\.us', 'vidlo'),  # no valid media pages listed\n+        (r'vidlocker\\.xyz', 'VidLocker'),  # no media pages listed\n+        (r'(?:w\\d\\.)?viidshar\\.com', 'VidShare'),  # vidshare.tv redirects here\n+        # (r'vup\\.to', 'VUp'),  # domain not found\n         (r'wolfstream\\.tv', 'WolfStream'),\n-        (r'xvideosharing\\.com', 'XVideoSharing'),\n+        (r'xvideosharing\\.com', 'XVideoSharing'),  # just started showing 'maintenance mode'\n     )\n \n-    IE_DESC = 'XFileShare based sites: %s' % ', '.join(list(zip(*_SITES))[1])\n+    IE_DESC = 'XFileShare-based sites: %s' % ', '.join(list(zip(*_SITES))[1])\n     _VALID_URL = (r'https?://(?:www\\.)?(?P<host>%s)/(?:embed-)?(?P<id>[0-9a-zA-Z]+)'\n                   % '|'.join(site for site in list(zip(*_SITES))[0]))\n+    _EMBED_REGEX = [r'<iframe\\b[^>]+\\bsrc=([\"\\'])(?P<url>(?:https?:)?//(?:%s)/embed-[0-9a-zA-Z]+.*?)\\1' % '|'.join(site for site in list(zip(*_SITES))[0])]\n \n     _FILE_NOT_FOUND_REGEXES = (\n         r'>(?:404 - )?File Not Found<',\n         r'>The file was removed by administrator<',\n     )\n+    _TITLE_REGEXES = (\n+        r'style=\"z-index: [0-9]+;\">([^<]+)</span>',\n+        r'<td nowrap>([^<]+)</td>',\n+        r'h4-fine[^>]*>([^<]+)<',\n+        r'>Watch (.+)[ <]',\n+        r'<h2 class=\"video-page-head\">([^<]+)</h2>',\n+        r'<h2 style=\"[^\"]*color:#403f3d[^\"]*\"[^>]*>([^<]+)<',  # streamin.to (dead)\n+        r'title\\s*:\\s*\"([^\"]+)\"',  # govid.me\n+    )\n+    _SOURCE_URL_REGEXES = (\n+        r'(?:file|src)\\s*:\\s*([\"\\'])(?P<url>http(?:(?!\\1).)+\\.(?:m3u8|mp4|flv)(?:(?!\\1).)*)\\1',\n+        r'file_link\\s*=\\s*([\"\\'])(?P<url>http(?:(?!\\1).)+)\\1',\n+        r'addVariable\\((\\\\?[\"\\'])file\\1\\s*,\\s*(\\\\?[\"\\'])(?P<url>http(?:(?!\\2).)+)\\2\\)',\n+        r'<embed[^>]+src=([\"\\'])(?P<url>http(?:(?!\\1).)+\\.(?:m3u8|mp4|flv)(?:(?!\\1).)*)\\1',\n+    )\n+    _THUMBNAIL_REGEXES = (\n+        r'<video[^>]+poster=\"([^\"]+)\"',\n+        r'(?:image|poster)\\s*:\\s*[\"\\'](http[^\"\\']+)[\"\\'],',\n+    )\n \n     _TESTS = [{\n-        'url': 'http://xvideosharing.com/fq65f94nd2ve',\n-        'md5': '4181f63957e8fe90ac836fa58dc3c8a6',\n+        'note': 'link in `sources`',\n+        'url': 'https://uqload.to/dcsu06gdb45o',\n+        'md5': '7f8db187b254379440bf4fcad094ae86',\n         'info_dict': {\n-            'id': 'fq65f94nd2ve',\n+            'id': 'dcsu06gdb45o',\n             'ext': 'mp4',\n-            'title': 'sample',\n-            'thumbnail': r're:http://.*\\.jpg',\n+            'title': 'f2e31015957e74c8c8427982e161c3fc mp4',\n+            'thumbnail': r're:https://.*\\.jpg'\n+        },\n+        'params': {\n+            'nocheckcertificate': True,\n+        },\n+        'expected_warnings': ['Unable to extract JWPlayer data'],\n+    }, {\n+        'note': 'link in decoded `sources`',\n+        'url': 'https://xvideosharing.com/1tlg6agrrdgc',\n+        'md5': '2608ce41932c1657ae56258a64e647d9',\n+        'info_dict': {\n+            'id': '1tlg6agrrdgc',\n+            'ext': 'mp4',\n+            'title': '0121',\n+            'thumbnail': r're:https?://.*\\.jpg',\n+        },\n+        'skip': 'This server is in maintenance mode.',\n+    }, {\n+        'note': 'JWPlayer link in un-p,a,c,k,e,d JS',\n+        'url': 'https://filemoon.sx/e/dw40rxrzruqz',\n+        'md5': '5a713742f57ac4aef29b74733e8dda01',\n+        'info_dict': {\n+            'id': 'dw40rxrzruqz',\n+            'title': 'dw40rxrzruqz',\n+            'ext': 'mp4'\n+        },\n+    }, {\n+        'note': 'JWPlayer link in un-p,a,c,k,e,d JS',\n+        'url': 'https://vadbam.net/6lnbkci96wly.html',\n+        'md5': 'a1616800076177e2ac769203957c54bc',\n+        'info_dict': {\n+            'id': '6lnbkci96wly',\n+            'title': 'Heart Crime S01 E03 weciima autos',\n+            'ext': 'mp4'\n+        },\n+    }, {\n+        'note': 'JWPlayer link in clear',\n+        'url': 'https://w1.viidshar.com/nnibe0xf0h79.html',\n+        'md5': 'f0a580ce9df06cc61b4a5c979d672367',\n+        'info_dict': {\n+            'id': 'nnibe0xf0h79',\n+            'title': 'JaGa 68ar',\n+            'ext': 'mp4'\n+        },\n+        'params': {\n+            'skip_download': 'ffmpeg',\n+        },\n+        'expected_warnings': ['hlsnative has detected features it does not support'],\n+    }, {\n+        'note': 'JWPlayer link in clear',\n+        'url': 'https://wolfstream.tv/a3drtehyrg52.html',\n+        'md5': '1901d86a79c5e0c6a51bdc9a4cfd3769',\n+        'info_dict': {\n+            'id': 'a3drtehyrg52',\n+            'title': 'NFL 2023 W04 DET@GB',\n+            'ext': 'mp4'\n         },\n     }, {\n         'url': 'https://aparat.cam/n4d6dh0wvlpr',\n         'only_matching': True,\n     }, {\n-        'url': 'https://wolfstream.tv/nthme29v9u2x',\n+        'url': 'https://uqload.to/ug5somm0ctnk.html',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://highstream.tv/2owiyz3sjoux',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://vedbam.xyz/6lnbkci96wly.html',\n         'only_matching': True,\n     }]\n \n-    @staticmethod\n-    def _extract_urls(webpage):\n-        return [\n-            mobj.group('url')\n-            for mobj in re.finditer(\n-                r'<iframe\\b[^>]+\\bsrc=([\"\\'])(?P<url>(?:https?:)?//(?:%s)/embed-[0-9a-zA-Z]+.*?)\\1'\n-                % '|'.join(site for site in list(zip(*XFileShareIE._SITES))[0]),\n-                webpage)]\n+    @classmethod\n+    def _extract_urls(cls, webpage):\n+\n+        def yield_urls():\n+            for regex in cls._EMBED_REGEX:\n+                for mobj in re.finditer(regex, webpage):\n+                    yield mobj.group('url')\n+\n+        return list(yield_urls())\n \n     def _real_extract(self, url):\n-        host, video_id = re.match(self._VALID_URL, url).groups()\n+        host, video_id = self._match_valid_url(url).group('host', 'id')\n \n-        url = 'https://%s/' % host + ('embed-%s.html' % video_id if host in ('govid.me', 'vidlo.us') else video_id)\n+        url = 'https://%s/%s' % (\n+            host,\n+            'embed-%s.html' % video_id if host in ('govid.me', 'vidlo.us') else video_id)\n         webpage = self._download_webpage(url, video_id)\n-\n-        if any(re.search(p, webpage) for p in self._FILE_NOT_FOUND_REGEXES):\n+        container_div = get_element_by_id('container', webpage) or webpage\n+        if self._search_regex(\n+                r'>This server is in maintenance mode\\.', container_div,\n+                'maint error', group=0, default=None):\n+            raise ExtractorError(clean_html(container_div), expected=True)\n+        if self._search_regex(\n+                self._FILE_NOT_FOUND_REGEXES, container_div,\n+                'missing video error', group=0, default=None):\n             raise ExtractorError('Video %s does not exist' % video_id, expected=True)\n \n         fields = self._hidden_inputs(webpage)\n@@ -122,59 +226,43 @@ def _real_extract(self, url):\n                     'Content-type': 'application/x-www-form-urlencoded',\n                 })\n \n-        title = (self._search_regex(\n-            (r'style=\"z-index: [0-9]+;\">([^<]+)</span>',\n-             r'<td nowrap>([^<]+)</td>',\n-             r'h4-fine[^>]*>([^<]+)<',\n-             r'>Watch (.+)[ <]',\n-             r'<h2 class=\"video-page-head\">([^<]+)</h2>',\n-             r'<h2 style=\"[^\"]*color:#403f3d[^\"]*\"[^>]*>([^<]+)<',  # streamin.to\n-             r'title\\s*:\\s*\"([^\"]+)\"'),  # govid.me\n-            webpage, 'title', default=None) or self._og_search_title(\n-            webpage, default=None) or video_id).strip()\n-\n-        for regex, func in (\n-                (r'(eval\\(function\\(p,a,c,k,e,d\\){.+)', decode_packed_codes),\n-                (r'(\uff9f.+)', aa_decode)):\n-            obf_code = self._search_regex(regex, webpage, 'obfuscated code', default=None)\n-            if obf_code:\n-                webpage = webpage.replace(obf_code, func(obf_code))\n-\n-        formats = []\n-\n-        jwplayer_data = self._search_regex(\n-            [\n-                r'jwplayer\\(\"[^\"]+\"\\)\\.load\\(\\[({.+?})\\]\\);',\n-                r'jwplayer\\(\"[^\"]+\"\\)\\.setup\\(({.+?})\\);',\n-            ], webpage,\n-            'jwplayer data', default=None)\n-        if jwplayer_data:\n-            jwplayer_data = self._parse_json(\n-                jwplayer_data.replace(r\"\\'\", \"'\"), video_id, js_to_json)\n+        title = (\n+            self._search_regex(self._TITLE_REGEXES, webpage, 'title', default=None)\n+            or self._og_search_title(webpage, default=None)\n+            or video_id).strip()\n+\n+        obf_code = True\n+        while obf_code:\n+            for regex, func in (\n+                    (r'(?s)(?<!-)\\b(eval\\(function\\(p,a,c,k,e,d\\)\\{(?:(?!</script>).)+\\)\\))',\n+                     decode_packed_codes),\n+                    (r'(\uff9f.+)', aa_decode)):\n+                obf_code = self._search_regex(regex, webpage, 'obfuscated code', default=None)\n+                if obf_code:\n+                    webpage = webpage.replace(obf_code, func(obf_code))\n+                    break\n+\n+        jwplayer_data = self._find_jwplayer_data(\n+            webpage.replace(r'\\'', '\\''), video_id)\n+        result = self._parse_jwplayer_data(\n+            jwplayer_data, video_id, require_title=False,\n+            m3u8_id='hls', mpd_id='dash')\n+\n+        if not traverse_obj(result, 'formats'):\n             if jwplayer_data:\n-                formats = self._parse_jwplayer_data(\n-                    jwplayer_data, video_id, False,\n-                    m3u8_id='hls', mpd_id='dash')['formats']\n-\n-        if not formats:\n-            urls = []\n-            for regex in (\n-                    r'(?:file|src)\\s*:\\s*([\"\\'])(?P<url>http(?:(?!\\1).)+\\.(?:m3u8|mp4|flv)(?:(?!\\1).)*)\\1',\n-                    r'file_link\\s*=\\s*([\"\\'])(?P<url>http(?:(?!\\1).)+)\\1',\n-                    r'addVariable\\((\\\\?[\"\\'])file\\1\\s*,\\s*(\\\\?[\"\\'])(?P<url>http(?:(?!\\2).)+)\\2\\)',\n-                    r'<embed[^>]+src=([\"\\'])(?P<url>http(?:(?!\\1).)+\\.(?:m3u8|mp4|flv)(?:(?!\\1).)*)\\1'):\n+                self.report_warning(\n+                    'Failed to extract JWPlayer formats', video_id=video_id)\n+            urls = set()\n+            for regex in self._SOURCE_URL_REGEXES:\n                 for mobj in re.finditer(regex, webpage):\n-                    video_url = mobj.group('url')\n-                    if video_url not in urls:\n-                        urls.append(video_url)\n+                    urls.add(mobj.group('url'))\n \n             sources = self._search_regex(\n                 r'sources\\s*:\\s*(\\[(?!{)[^\\]]+\\])', webpage, 'sources', default=None)\n-            if sources:\n-                urls.extend(self._parse_json(sources, video_id))\n+            urls.update(traverse_obj(sources, (T(lambda s: self._parse_json(s, video_id)), Ellipsis)))\n \n             formats = []\n-            for video_url in urls:\n+            for video_url in traverse_obj(urls, (Ellipsis, T(url_or_none))):\n                 if determine_ext(video_url) == 'm3u8':\n                     formats.extend(self._extract_m3u8_formats(\n                         video_url, video_id, 'mp4',\n@@ -185,17 +273,19 @@ def _real_extract(self, url):\n                         'url': video_url,\n                         'format_id': 'sd',\n                     })\n-        self._sort_formats(formats)\n+            result = {'formats': formats}\n+\n+        self._sort_formats(result['formats'])\n \n         thumbnail = self._search_regex(\n-            [\n-                r'<video[^>]+poster=\"([^\"]+)\"',\n-                r'(?:image|poster)\\s*:\\s*[\"\\'](http[^\"\\']+)[\"\\'],',\n-            ], webpage, 'thumbnail', default=None)\n+            self._THUMBNAIL_REGEXES, webpage, 'thumbnail', default=None)\n+\n+        if not (title or result.get('title')):\n+            title = self._generic_title(url) or video_id\n \n-        return {\n+        return merge_dicts(result, {\n             'id': video_id,\n-            'title': title,\n+            'title': title or None,\n             'thumbnail': thumbnail,\n-            'formats': formats,\n-        }\n+            'http_headers': {'Referer': url}\n+        })\ndiff --git a/youtube_dl/extractor/xhamster.py b/youtube_dl/extractor/xhamster.py\nindex f73b9778f6a..e17947fc6a7 100644\n--- a/youtube_dl/extractor/xhamster.py\n+++ b/youtube_dl/extractor/xhamster.py\n@@ -1,3 +1,4 @@\n+# coding: utf-8\n from __future__ import unicode_literals\n \n import itertools\n@@ -23,7 +24,7 @@\n \n \n class XHamsterIE(InfoExtractor):\n-    _DOMAINS = r'(?:xhamster\\.(?:com|one|desi)|xhms\\.pro|xhamster\\d+\\.com)'\n+    _DOMAINS = r'(?:xhamster\\.(?:com|one|desi)|xhms\\.pro|xhamster\\d+\\.com|xhday\\.com|xhvid\\.com)'\n     _VALID_URL = r'''(?x)\n                     https?://\n                         (?:.+?\\.)?%s/\n@@ -34,7 +35,7 @@ class XHamsterIE(InfoExtractor):\n                     ''' % _DOMAINS\n     _TESTS = [{\n         'url': 'https://xhamster.com/videos/femaleagent-shy-beauty-takes-the-bait-1509445',\n-        'md5': '98b4687efb1ffd331c4197854dc09e8f',\n+        'md5': '34e1ab926db5dc2750fed9e1f34304bb',\n         'info_dict': {\n             'id': '1509445',\n             'display_id': 'femaleagent-shy-beauty-takes-the-bait',\n@@ -43,6 +44,7 @@ class XHamsterIE(InfoExtractor):\n             'timestamp': 1350194821,\n             'upload_date': '20121014',\n             'uploader': 'Ruseful2011',\n+            'uploader_id': 'ruseful2011',\n             'duration': 893,\n             'age_limit': 18,\n         },\n@@ -72,6 +74,7 @@ class XHamsterIE(InfoExtractor):\n             'timestamp': 1454948101,\n             'upload_date': '20160208',\n             'uploader': 'parejafree',\n+            'uploader_id': 'parejafree',\n             'duration': 72,\n             'age_limit': 18,\n         },\n@@ -117,6 +120,12 @@ class XHamsterIE(InfoExtractor):\n     }, {\n         'url': 'http://de.xhamster.com/videos/skinny-girl-fucks-herself-hard-in-the-forest-xhnBJZx',\n         'only_matching': True,\n+    }, {\n+        'url': 'https://xhday.com/videos/strapless-threesome-xhh7yVf',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://xhvid.com/videos/lk-mm-xhc6wn6',\n+        'only_matching': True,\n     }]\n \n     def _real_extract(self, url):\n@@ -245,6 +254,7 @@ def get_height(s):\n             else:\n                 categories = None\n \n+            uploader_url = url_or_none(try_get(video, lambda x: x['author']['pageURL']))\n             return {\n                 'id': video_id,\n                 'display_id': display_id,\n@@ -253,6 +263,8 @@ def get_height(s):\n                 'timestamp': int_or_none(video.get('created')),\n                 'uploader': try_get(\n                     video, lambda x: x['author']['name'], compat_str),\n+                'uploader_url': uploader_url,\n+                'uploader_id': uploader_url.split('/')[-1] if uploader_url else None,\n                 'thumbnail': video.get('thumbURL'),\n                 'duration': int_or_none(video.get('duration')),\n                 'view_count': int_or_none(video.get('views')),\n@@ -261,7 +273,7 @@ def get_height(s):\n                 'dislike_count': int_or_none(try_get(\n                     video, lambda x: x['rating']['dislikes'], int)),\n                 'comment_count': int_or_none(video.get('views')),\n-                'age_limit': age_limit,\n+                'age_limit': age_limit if age_limit is not None else 18,\n                 'categories': categories,\n                 'formats': formats,\n             }\n@@ -352,6 +364,7 @@ def get_height(s):\n             'description': description,\n             'upload_date': upload_date,\n             'uploader': uploader,\n+            'uploader_id': uploader.lower() if uploader else None,\n             'thumbnail': thumbnail,\n             'duration': duration,\n             'view_count': view_count,\n@@ -420,6 +433,12 @@ class XHamsterUserIE(InfoExtractor):\n             'id': 'firatkaan',\n         },\n         'playlist_mincount': 1,\n+    }, {\n+        'url': 'https://xhday.com/users/mobhunter',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://xhvid.com/users/pelushe21',\n+        'only_matching': True,\n     }]\n \n     def _entries(self, user_id):\ndiff --git a/youtube_dl/extractor/yandexmusic.py b/youtube_dl/extractor/yandexmusic.py\nindex 84969f8e114..8da5b430f7c 100644\n--- a/youtube_dl/extractor/yandexmusic.py\n+++ b/youtube_dl/extractor/yandexmusic.py\n@@ -106,6 +106,25 @@ class YandexMusicTrackIE(YandexMusicBaseIE):\n     }, {\n         'url': 'http://music.yandex.com/album/540508/track/4878838',\n         'only_matching': True,\n+    }, {\n+        'url': 'https://music.yandex.ru/album/16302456/track/85430762',\n+        'md5': '11b8d50ab03b57738deeaadf661a0a48',\n+        'info_dict': {\n+            'id': '85430762',\n+            'ext': 'mp3',\n+            'abr': 128,\n+            'title': 'Haddadi Von Engst, Phonic Youth, Super Flu - Til The End (Super Flu Remix)',\n+            'filesize': int,\n+            'duration': 431.14,\n+            'track': 'Til The End (Super Flu Remix)',\n+            'album': 'Til The End',\n+            'album_artist': 'Haddadi Von Engst, Phonic Youth',\n+            'artist': 'Haddadi Von Engst, Phonic Youth, Super Flu',\n+            'release_year': 2021,\n+            'genre': 'house',\n+            'disc_number': 1,\n+            'track_number': 2,\n+        }\n     }]\n \n     def _real_extract(self, url):\n@@ -116,10 +135,14 @@ def _real_extract(self, url):\n             'track', tld, url, track_id, 'Downloading track JSON',\n             {'track': '%s:%s' % (track_id, album_id)})['track']\n         track_title = track['title']\n+        track_version = track.get('version')\n+        if track_version:\n+            track_title = '%s (%s)' % (track_title, track_version)\n \n         download_data = self._download_json(\n             'https://music.yandex.ru/api/v2.1/handlers/track/%s:%s/web-album_track-track-track-main/download/m' % (track_id, album_id),\n             track_id, 'Downloading track location url JSON',\n+            query={'hq': 1},\n             headers={'X-Retpath-Y': url})\n \n         fd_data = self._download_json(\ndiff --git a/youtube_dl/extractor/youporn.py b/youtube_dl/extractor/youporn.py\nindex 7084d3d1215..ec6125a79cb 100644\n--- a/youtube_dl/extractor/youporn.py\n+++ b/youtube_dl/extractor/youporn.py\n@@ -1,19 +1,38 @@\n+# coding: utf-8\n from __future__ import unicode_literals\n \n+import itertools\n import re\n+from time import sleep\n \n from .common import InfoExtractor\n from ..utils import (\n+    clean_html,\n     extract_attributes,\n+    ExtractorError,\n+    get_element_by_class,\n+    get_element_by_id,\n     int_or_none,\n-    str_to_int,\n+    merge_dicts,\n+    parse_count,\n+    parse_qs,\n+    T,\n+    traverse_obj,\n     unified_strdate,\n     url_or_none,\n+    urljoin,\n )\n \n \n class YouPornIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?youporn\\.com/(?:watch|embed)/(?P<id>\\d+)(?:/(?P<display_id>[^/?#&]+))?'\n+    _VALID_URL = (\n+        r'youporn:(?P<id>\\d+)',\n+        r'''(?x)\n+            https?://(?:www\\.)?youporn\\.com/(?:watch|embed)/(?P<id>\\d+)\n+            (?:/(?:(?P<display_id>[^/?#&]+)/?)?)?(?:[#?]|$)\n+    '''\n+    )\n+    _EMBED_REGEX = [r'<iframe[^>]+\\bsrc=[\"\\'](?P<url>(?:https?:)?//(?:www\\.)?youporn\\.com/embed/\\d+)']\n     _TESTS = [{\n         'url': 'http://www.youporn.com/watch/505835/sex-ed-is-it-safe-to-masturbate-daily/',\n         'md5': '3744d24c50438cf5b6f6d59feb5055c2',\n@@ -33,7 +52,7 @@ class YouPornIE(InfoExtractor):\n             'tags': list,\n             'age_limit': 18,\n         },\n-        'skip': 'This video has been disabled',\n+        'skip': 'This video has been deactivated',\n     }, {\n         # Unknown uploader\n         'url': 'http://www.youporn.com/watch/561726/big-tits-awesome-brunette-on-amazing-webcam-show/?from=related3&al=2&from_id=561726&pos=4',\n@@ -65,57 +84,104 @@ class YouPornIE(InfoExtractor):\n     }, {\n         'url': 'https://www.youporn.com/watch/13922959/femdom-principal/',\n         'only_matching': True,\n+    }, {\n+        'url': 'https://www.youporn.com/watch/16290308/tinderspecial-trailer1/',\n+        'info_dict': {\n+            'id': '16290308',\n+            'age_limit': 18,\n+            'categories': [],\n+            'description': None,  # SEO spam using title removed\n+            'display_id': 'tinderspecial-trailer1',\n+            'duration': 298.0,\n+            'ext': 'mp4',\n+            'upload_date': '20201123',\n+            'uploader': 'Ersties',\n+            'tags': [],\n+            'thumbnail': 'https://fi1.ypncdn.com/m=eaSaaTbWx/202011/23/16290308/original/3.jpg',\n+            'timestamp': 1606147564,\n+            'title': 'Tinder In Real Life',\n+            'view_count': int,\n+        }\n     }]\n \n-    @staticmethod\n-    def _extract_urls(webpage):\n-        return re.findall(\n-            r'<iframe[^>]+\\bsrc=[\"\\']((?:https?:)?//(?:www\\.)?youporn\\.com/embed/\\d+)',\n-            webpage)\n+    @classmethod\n+    def _extract_urls(cls, webpage):\n+        def yield_urls():\n+            for p in cls._EMBED_REGEX:\n+                for m in re.finditer(p, webpage):\n+                    yield m.group('url')\n+\n+        return list(yield_urls())\n \n     def _real_extract(self, url):\n-        mobj = re.match(self._VALID_URL, url)\n-        video_id = mobj.group('id')\n-        display_id = mobj.group('display_id') or video_id\n+        # A different video ID (data-video-id) is hidden in the page but\n+        # never seems to be used\n+        video_id, display_id = self._match_valid_url(url).group('id', 'display_id')\n+        url = 'http://www.youporn.com/watch/%s' % (video_id,)\n+        webpage = self._download_webpage(\n+            url, video_id, headers={'Cookie': 'age_verified=1'})\n+\n+        watchable = self._search_regex(\n+            r'''(<div\\s[^>]*\\bid\\s*=\\s*('|\")?watch-container(?(2)\\2|(?!-)\\b)[^>]*>)''',\n+            webpage, 'watchability', default=None)\n+        if not watchable:\n+            msg = re.split(r'\\s{4}', clean_html(get_element_by_id(\n+                'mainContent', webpage)) or '')[0]\n+            raise ExtractorError(\n+                ('%s says: %s' % (self.IE_NAME, msg))\n+                if msg else 'Video unavailable: no reason found',\n+                expected=True)\n+        # internal ID ?\n+        # video_id = extract_attributes(watchable).get('data-video-id')\n+\n+        playervars = self._search_json(\n+            r'\\bplayervars\\s*:', webpage, 'playervars', video_id)\n+\n+        def get_fmt(x):\n+            v_url = url_or_none(x.get('videoUrl'))\n+            if v_url:\n+                x['videoUrl'] = v_url\n+                return (x['format'], x)\n \n-        definitions = self._download_json(\n-            'https://www.youporn.com/api/video/media_definitions/%s/' % video_id,\n-            display_id)\n+        defs_by_format = dict(traverse_obj(playervars, (\n+            'mediaDefinitions', lambda _, v: v.get('format'), T(get_fmt))))\n+\n+        def get_format_data(f):\n+            if f not in defs_by_format:\n+                return []\n+            return self._download_json(\n+                defs_by_format[f]['videoUrl'], video_id, '{0}-formats'.format(f))\n \n         formats = []\n-        for definition in definitions:\n-            if not isinstance(definition, dict):\n-                continue\n-            video_url = url_or_none(definition.get('videoUrl'))\n-            if not video_url:\n-                continue\n-            f = {\n-                'url': video_url,\n-                'filesize': int_or_none(definition.get('videoSize')),\n-            }\n-            height = int_or_none(definition.get('quality'))\n+        # Try to extract only the actual master m3u8 first, avoiding the duplicate single resolution \"master\" m3u8s\n+        for hls_url in traverse_obj(\n+                get_format_data('hls'),\n+                (lambda _, v: not isinstance(v['defaultQuality'], bool), 'videoUrl'),\n+                (Ellipsis, 'videoUrl')):\n+            formats.extend(self._extract_m3u8_formats(\n+                hls_url, video_id, 'mp4', fatal=False, m3u8_id='hls',\n+                entry_protocol='m3u8_native'))\n+\n+        for f in traverse_obj(get_format_data('mp4'), (\n+                lambda _, v: v.get('videoUrl'), {\n+                    'url': ('videoUrl', T(url_or_none)),\n+                    'filesize': ('videoSize', T(int_or_none)),\n+                    'height': ('quality', T(int_or_none)),\n+                }, T(lambda x: x.get('videoUrl') and x))):\n             # Video URL's path looks like this:\n             #  /201012/17/505835/720p_1500k_505835/YouPorn%20-%20Sex%20Ed%20Is%20It%20Safe%20To%20Masturbate%20Daily.mp4\n             #  /201012/17/505835/vl_240p_240k_505835/YouPorn%20-%20Sex%20Ed%20Is%20It%20Safe%20To%20Masturbate%20Daily.mp4\n             #  /videos/201703/11/109285532/1080P_4000K_109285532.mp4\n             # We will benefit from it by extracting some metadata\n-            mobj = re.search(r'(?P<height>\\d{3,4})[pP]_(?P<bitrate>\\d+)[kK]_\\d+', video_url)\n+            mobj = re.search(r'(?P<height>\\d{3,4})[pP]_(?P<bitrate>\\d+)[kK]_\\d+', f['videoUrl'])\n             if mobj:\n-                if not height:\n-                    height = int(mobj.group('height'))\n-                bitrate = int(mobj.group('bitrate'))\n-                f.update({\n-                    'format_id': '%dp-%dk' % (height, bitrate),\n-                    'tbr': bitrate,\n-                })\n-            f['height'] = height\n+                if not f.get('height'):\n+                    f['height'] = int(mobj.group('height'))\n+                f['tbr'] = int(mobj.group('bitrate'))\n+                f['format_id'] = '%dp-%dk' % (f['height'], f['tbr'])\n             formats.append(f)\n         self._sort_formats(formats)\n \n-        webpage = self._download_webpage(\n-            'http://www.youporn.com/watch/%s' % video_id, display_id,\n-            headers={'Cookie': 'age_verified=1'})\n-\n         title = self._html_search_regex(\n             r'(?s)<div[^>]+class=[\"\\']watchVideoTitle[^>]+>(.+?)</div>',\n             webpage, 'title', default=None) or self._og_search_title(\n@@ -130,27 +196,30 @@ def _real_extract(self, url):\n         thumbnail = self._search_regex(\n             r'(?:imageurl\\s*=|poster\\s*:)\\s*([\"\\'])(?P<thumbnail>.+?)\\1',\n             webpage, 'thumbnail', fatal=False, group='thumbnail')\n-        duration = int_or_none(self._html_search_meta(\n-            'video:duration', webpage, 'duration', fatal=False))\n+        duration = traverse_obj(playervars, ('duration', T(int_or_none)))\n+        if duration is None:\n+            duration = int_or_none(self._html_search_meta(\n+                'video:duration', webpage, 'duration', fatal=False))\n \n         uploader = self._html_search_regex(\n             r'(?s)<div[^>]+class=[\"\\']submitByLink[\"\\'][^>]*>(.+?)</div>',\n             webpage, 'uploader', fatal=False)\n         upload_date = unified_strdate(self._html_search_regex(\n-            [r'UPLOADED:\\s*<span>([^<]+)',\n+            (r'UPLOADED:\\s*<span>([^<]+)',\n              r'Date\\s+[Aa]dded:\\s*<span>([^<]+)',\n-             r'(?s)<div[^>]+class=[\"\\']videoInfo(?:Date|Time)[\"\\'][^>]*>(.+?)</div>'],\n+             r'''(?s)<div[^>]+class=[\"']videoInfo(?:Date|Time)\\b[^>]*>(.+?)</div>''',\n+             r'(?s)<label\\b[^>]*>Uploaded[^<]*</label>\\s*<span\\b[^>]*>(.+?)</span>'),\n             webpage, 'upload date', fatal=False))\n \n         age_limit = self._rta_search(webpage)\n \n         view_count = None\n         views = self._search_regex(\n-            r'(<div[^>]+\\bclass=[\"\\']js_videoInfoViews[\"\\']>)', webpage,\n-            'views', default=None)\n+            r'(<div\\s[^>]*\\bdata-value\\s*=[^>]+>)\\s*<label>Views:</label>',\n+            webpage, 'views', default=None)\n         if views:\n-            view_count = str_to_int(extract_attributes(views).get('data-value'))\n-        comment_count = str_to_int(self._search_regex(\n+            view_count = parse_count(extract_attributes(views).get('data-value'))\n+        comment_count = parse_count(self._search_regex(\n             r'>All [Cc]omments? \\(([\\d,.]+)\\)',\n             webpage, 'comment count', default=None))\n \n@@ -166,7 +235,10 @@ def extract_tag_box(regex, title):\n             r'(?s)Tags:.*?</div>\\s*<div[^>]+class=[\"\\']tagBoxContent[\"\\'][^>]*>(.+?)</div>',\n             'tags')\n \n-        return {\n+        data = self._search_json_ld(webpage, video_id, expected_type='VideoObject', fatal=False) or {}\n+        data.pop('url', None)\n+\n+        result = merge_dicts(data, {\n             'id': video_id,\n             'display_id': display_id,\n             'title': title,\n@@ -181,4 +253,442 @@ def extract_tag_box(regex, title):\n             'tags': tags,\n             'age_limit': age_limit,\n             'formats': formats,\n-        }\n+        })\n+        # Remove promotional non-description\n+        if result.get('description', '').startswith(\n+                'Watch %s online' % (result['title'],)):\n+            del result['description']\n+        return result\n+\n+\n+class YouPornListBase(InfoExtractor):\n+    # pattern in '.title-text' element of page section containing videos\n+    _PLAYLIST_TITLEBAR_RE = r'\\s+[Vv]ideos\\s*$'\n+    _PAGE_RETRY_COUNT = 0  # ie, no retry\n+    _PAGE_RETRY_DELAY = 2  # seconds\n+\n+    def _get_next_url(self, url, pl_id, html):\n+        return urljoin(url, self._search_regex(\n+            r'''<a\\s[^>]*?\\bhref\\s*=\\s*(\"|')(?P<url>(?:(?!\\1)[^>])+)\\1''',\n+            get_element_by_id('next', html) or '', 'next page',\n+            group='url', default=None))\n+\n+    @classmethod\n+    def _get_title_from_slug(cls, title_slug):\n+        return re.sub(r'[_-]', ' ', title_slug)\n+\n+    def _entries(self, url, pl_id, html=None, page_num=None):\n+\n+        # separates page sections\n+        PLAYLIST_SECTION_RE = (\n+            r'''<div\\s[^>]*\\bclass\\s*=\\s*('|\")(?:[\\w$-]+\\s+|\\s)*?title-bar(?:\\s+[\\w$-]+|\\s)*\\1[^>]*>'''\n+        )\n+        # contains video link\n+        VIDEO_URL_RE = r'''(?x)\n+            <div\\s[^>]*\\bdata-video-id\\s*=\\s*('|\")\\d+\\1[^>]*>\\s*\n+            (?:<div\\b[\\s\\S]+?</div>\\s*)*\n+            <a\\s[^>]*\\bhref\\s*=\\s*('|\")(?P<url>(?:(?!\\2)[^>])+)\\2\n+        '''\n+\n+        def yield_pages(url, html=html, page_num=page_num):\n+            fatal = not html\n+            for pnum in itertools.count(start=page_num or 1):\n+                if not html:\n+                    html = self._download_webpage(\n+                        url, pl_id, note='Downloading page %d' % pnum,\n+                        fatal=fatal)\n+                if not html:\n+                    break\n+                fatal = False\n+                yield (url, html, pnum)\n+                # explicit page: extract just that page\n+                if page_num is not None:\n+                    break\n+                next_url = self._get_next_url(url, pl_id, html)\n+                if not next_url or next_url == url:\n+                    break\n+                url, html = next_url, None\n+\n+        def retry_page(msg, tries_left, page_data):\n+            if tries_left <= 0:\n+                return\n+            self.report_warning(msg, pl_id)\n+            sleep(self._PAGE_RETRY_DELAY)\n+            return next(\n+                yield_pages(page_data[0], page_num=page_data[2]), None)\n+\n+        def yield_entries(html):\n+            for frag in re.split(PLAYLIST_SECTION_RE, html):\n+                if not frag:\n+                    continue\n+                t_text = get_element_by_class('title-text', frag or '')\n+                if not (t_text and re.search(self._PLAYLIST_TITLEBAR_RE, t_text)):\n+                    continue\n+                for m in re.finditer(VIDEO_URL_RE, frag):\n+                    video_url = urljoin(url, m.group('url'))\n+                    if video_url:\n+                        yield self.url_result(video_url)\n+\n+        last_first_url = None\n+        for page_data in yield_pages(url, html=html, page_num=page_num):\n+            # page_data: url, html, page_num\n+            first_url = None\n+            tries_left = self._PAGE_RETRY_COUNT + 1\n+            while tries_left > 0:\n+                tries_left -= 1\n+                for from_ in yield_entries(page_data[1]):\n+                    # may get the same page twice instead of empty page\n+                    # or (site bug) intead of actual next page\n+                    if not first_url:\n+                        first_url = from_['url']\n+                        if first_url == last_first_url:\n+                            # sometimes (/porntags/) the site serves the previous page\n+                            # instead but may provide the correct page after a delay\n+                            page_data = retry_page(\n+                                'Retrying duplicate page...', tries_left, page_data)\n+                            if page_data:\n+                                first_url = None\n+                                break\n+                            continue\n+                    yield from_\n+                else:\n+                    if not first_url and 'no-result-paragarph1' in page_data[1]:\n+                        page_data = retry_page(\n+                            'Retrying empty page...', tries_left, page_data)\n+                        if page_data:\n+                            continue\n+                    else:\n+                        # success/failure\n+                        break\n+            # may get an infinite (?) sequence of empty pages\n+            if not first_url:\n+                break\n+            last_first_url = first_url\n+\n+    def _real_extract(self, url, html=None):\n+        # exceptionally, id may be None\n+        m_dict = self._match_valid_url(url).groupdict()\n+        pl_id, page_type, sort = (m_dict.get(k) for k in ('id', 'type', 'sort'))\n+\n+        qs = parse_qs(url)\n+        for q, v in qs.items():\n+            if v:\n+                qs[q] = v[-1]\n+            else:\n+                del qs[q]\n+\n+        base_id = pl_id or 'YouPorn'\n+        title = self._get_title_from_slug(base_id)\n+        if page_type:\n+            title = '%s %s' % (page_type.capitalize(), title)\n+        base_id = [base_id.lower()]\n+        if sort is None:\n+            title += ' videos'\n+        else:\n+            title = '%s videos by %s' % (title, re.sub(r'[_-]', ' ', sort))\n+            base_id.append(sort)\n+        if qs:\n+            ps = ['%s=%s' % item for item in sorted(qs.items())]\n+            title += ' (%s)' % ','.join(ps)\n+            base_id.extend(ps)\n+        pl_id = '/'.join(base_id)\n+\n+        return self.playlist_result(\n+            self._entries(url, pl_id, html=html,\n+                          page_num=int_or_none(qs.get('page'))),\n+            playlist_id=pl_id, playlist_title=title)\n+\n+\n+class YouPornCategoryIE(YouPornListBase):\n+    IE_DESC = 'YouPorn category, with sorting, filtering and pagination'\n+    _VALID_URL = r'''(?x)\n+        https?://(?:www\\.)?youporn\\.com/\n+        (?P<type>category)/(?P<id>[^/?#&]+)\n+        (?:/(?P<sort>popular|views|rating|time|duration))?/?(?:[#?]|$)\n+    '''\n+    _TESTS = [{\n+        'note': 'Full list with pagination',\n+        'url': 'https://www.youporn.com/category/lingerie/popular/',\n+        'info_dict': {\n+            'id': 'lingerie/popular',\n+            'title': 'Category lingerie videos by popular',\n+        },\n+        'playlist_mincount': 39,\n+    }, {\n+        'note': 'Filtered paginated list with single page result',\n+        'url': 'https://www.youporn.com/category/lingerie/duration/?min_minutes=10',\n+        'info_dict': {\n+            'id': 'lingerie/duration/min_minutes=10',\n+            'title': 'Category lingerie videos by duration (min_minutes=10)',\n+        },\n+        'playlist_maxcount': 30,\n+    }, {\n+        'note': 'Single page of full list',\n+        'url': 'https://www.youporn.com/category/lingerie/popular?page=1',\n+        'info_dict': {\n+            'id': 'lingerie/popular/page=1',\n+            'title': 'Category lingerie videos by popular (page=1)',\n+        },\n+        'playlist_count': 30,\n+    }]\n+\n+\n+class YouPornChannelIE(YouPornListBase):\n+    IE_DESC = 'YouPorn channel, with sorting and pagination'\n+    _VALID_URL = r'''(?x)\n+        https?://(?:www\\.)?youporn\\.com/\n+        (?P<type>channel)/(?P<id>[^/?#&]+)\n+        (?:/(?P<sort>rating|views|duration))?/?(?:[#?]|$)\n+    '''\n+    _TESTS = [{\n+        'note': 'Full list with pagination',\n+        'url': 'https://www.youporn.com/channel/x-feeds/',\n+        'info_dict': {\n+            'id': 'x-feeds',\n+            'title': 'Channel X-Feeds videos',\n+        },\n+        'playlist_mincount': 37,\n+    }, {\n+        'note': 'Single page of full list (no filters here)',\n+        'url': 'https://www.youporn.com/channel/x-feeds/duration?page=1',\n+        'info_dict': {\n+            'id': 'x-feeds/duration/page=1',\n+            'title': 'Channel X-Feeds videos by duration (page=1)',\n+        },\n+        'playlist_count': 24,\n+    }]\n+\n+    @staticmethod\n+    def _get_title_from_slug(title_slug):\n+        return re.sub(r'_', ' ', title_slug).title()\n+\n+\n+class YouPornCollectionIE(YouPornListBase):\n+    IE_DESC = 'YouPorn collection (user playlist), with sorting and pagination'\n+    _VALID_URL = r'''(?x)\n+        https?://(?:www\\.)?youporn\\.com/\n+        (?P<type>collection)s/videos/(?P<id>\\d+)\n+        (?:/(?P<sort>rating|views|time|duration))?/?(?:[#?]|$)\n+    '''\n+    _PLAYLIST_TITLEBAR_RE = r'^\\s*Videos\\s+in\\s'\n+    _TESTS = [{\n+        'note': 'Full list with pagination',\n+        'url': 'https://www.youporn.com/collections/videos/33044251/',\n+        'info_dict': {\n+            'id': '33044251',\n+            'title': 'Collection Sexy Lips videos',\n+            'uploader': 'ph-littlewillyb',\n+        },\n+        'playlist_mincount': 50,\n+    }, {\n+        'note': 'Single page of full list (no filters here)',\n+        'url': 'https://www.youporn.com/collections/videos/33044251/time?page=1',\n+        'info_dict': {\n+            'id': '33044251/time/page=1',\n+            'title': 'Collection Sexy Lips videos by time (page=1)',\n+            'uploader': 'ph-littlewillyb',\n+        },\n+        'playlist_count': 20,\n+    }]\n+\n+    def _real_extract(self, url):\n+        pl_id = self._match_id(url)\n+        html = self._download_webpage(url, pl_id)\n+        playlist = super(YouPornCollectionIE, self)._real_extract(url, html=html)\n+        infos = re.sub(r'\\s+', ' ', clean_html(get_element_by_class(\n+            'collection-infos', html)) or '')\n+        title, uploader = self._search_regex(\n+            r'^\\s*Collection: (?P<title>.+?) \\d+ VIDEOS \\d+ VIEWS \\d+ days LAST UPDATED From: (?P<uploader>[\\w_-]+)',\n+            infos, 'title/uploader', group=('title', 'uploader'), default=(None, None))\n+\n+        return merge_dicts({\n+            'title': playlist['title'].replace(playlist['id'].split('/')[0], title),\n+            'uploader': uploader,\n+        }, playlist) if title else playlist\n+\n+\n+class YouPornTagIE(YouPornListBase):\n+    IE_DESC = 'YouPorn tag (porntags), with sorting, filtering and pagination'\n+    _VALID_URL = r'''(?x)\n+        https?://(?:www\\.)?youporn\\.com/\n+        porn(?P<type>tag)s/(?P<id>[^/?#&]+)\n+        (?:/(?P<sort>views|rating|time|duration))?/?(?:[#?]|$)\n+    '''\n+    _PLAYLIST_TITLEBAR_RE = r'^\\s*Videos\\s+tagged\\s'\n+    _PAGE_RETRY_COUNT = 1\n+    _TESTS = [{\n+        'note': 'Full list with pagination',\n+        'url': 'https://www.youporn.com/porntags/austrian',\n+        'info_dict': {\n+            'id': 'austrian',\n+            'title': 'Tag austrian videos',\n+        },\n+        'playlist_mincount': 35,\n+        'expected_warnings': ['Retrying duplicate page'],\n+    }, {\n+        'note': 'Filtered paginated list with single page result',\n+        'url': 'https://www.youporn.com/porntags/austrian/duration/?min_minutes=10',\n+        'info_dict': {\n+            'id': 'austrian/duration/min_minutes=10',\n+            'title': 'Tag austrian videos by duration (min_minutes=10)',\n+        },\n+        # number of videos per page is (row x col) 2x3 + 6x4 + 2, or + 3,\n+        # or more, varying with number of ads; let's set max as 9x4\n+        # NB col 1 may not be shown in non-JS page with site CSS and zoom 100%\n+        'playlist_maxcount': 32,\n+        'expected_warnings': ['Retrying duplicate page', 'Retrying empty page'],\n+    }, {\n+        'note': 'Single page of full list',\n+        'url': 'https://www.youporn.com/porntags/austrian/?page=1',\n+        'info_dict': {\n+            'id': 'austrian/page=1',\n+            'title': 'Tag austrian videos (page=1)',\n+        },\n+        'playlist_mincount': 32,\n+        'playlist_maxcount': 34,\n+        'expected_warnings': ['Retrying duplicate page', 'Retrying empty page'],\n+    }]\n+\n+    # YP tag navigation is broken, loses sort\n+    def _get_next_url(self, url, pl_id, html):\n+        next_url = super(YouPornTagIE, self)._get_next_url(url, pl_id, html)\n+        if next_url:\n+            n = self._match_valid_url(next_url)\n+            if n:\n+                s = n.groupdict().get('sort')\n+            if s:\n+                u = self._match_valid_url(url)\n+                if u:\n+                    u = u.groupdict().get('sort')\n+                    if s and not u:\n+                        n = n.end('sort')\n+                        next_url = next_url[:n] + '/' + u + next_url[n:]\n+        return next_url\n+\n+\n+class YouPornStarIE(YouPornListBase):\n+    IE_DESC = 'YouPorn Pornstar, with description, sorting and pagination'\n+    _VALID_URL = r'''(?x)\n+        https?://(?:www\\.)?youporn\\.com/\n+        (?P<type>pornstar)/(?P<id>[^/?#&]+)\n+        (?:/(?P<sort>rating|views|duration))?/?(?:[#?]|$)\n+    '''\n+    _PLAYLIST_TITLEBAR_RE = r'^\\s*Videos\\s+[fF]eaturing\\s'\n+    _TESTS = [{\n+        'note': 'Full list with pagination',\n+        'url': 'https://www.youporn.com/pornstar/daynia/',\n+        'info_dict': {\n+            'id': 'daynia',\n+            'title': 'Pornstar Daynia videos',\n+            'description': r're:Daynia Rank \\d+ Videos \\d+ Views [\\d,.]+ .+ Subscribers \\d+',\n+        },\n+        'playlist_mincount': 45,\n+    }, {\n+        'note': 'Single page of full list (no filters here)',\n+        'url': 'https://www.youporn.com/pornstar/daynia/?page=1',\n+        'info_dict': {\n+            'id': 'daynia/page=1',\n+            'title': 'Pornstar Daynia videos (page=1)',\n+            'description': 're:.{180,}',\n+        },\n+        'playlist_count': 26,\n+    }]\n+\n+    @staticmethod\n+    def _get_title_from_slug(title_slug):\n+        return re.sub(r'_', ' ', title_slug).title()\n+\n+    def _real_extract(self, url):\n+        pl_id = self._match_id(url)\n+        html = self._download_webpage(url, pl_id)\n+        playlist = super(YouPornStarIE, self)._real_extract(url, html=html)\n+        INFO_ELEMENT_RE = r'''(?x)\n+            <div\\s[^>]*\\bclass\\s*=\\s*('|\")(?:[\\w$-]+\\s+|\\s)*?pornstar-info-wrapper(?:\\s+[\\w$-]+|\\s)*\\1[^>]*>\n+            (?P<info>[\\s\\S]+?)(?:</div>\\s*){6,}\n+        '''\n+\n+        infos = self._search_regex(INFO_ELEMENT_RE, html, 'infos', group='info', default='')\n+        if infos:\n+            infos = re.sub(\n+                r'(?:\\s*nl=nl)+\\s*', ' ',\n+                re.sub(r'(?u)\\s+', ' ', clean_html(\n+                    re.sub('\\n', 'nl=nl', infos)))).replace('ribe Subsc', '')\n+\n+        return merge_dicts({\n+            'description': infos.strip() or None,\n+        }, playlist)\n+\n+\n+class YouPornVideosIE(YouPornListBase):\n+    IE_DESC = 'YouPorn video (browse) playlists, with sorting, filtering and pagination'\n+    _VALID_URL = r'''(?x)\n+        https?://(?:www\\.)?youporn\\.com/\n+            (?:(?P<id>browse)/)?\n+            (?P<sort>(?(id)\n+                (?:duration|rating|time|views)|\n+                (?:most_(?:favou?rit|view)ed|recommended|top_rated)?))\n+            (?:[/#?]|$)\n+    '''\n+    _PLAYLIST_TITLEBAR_RE = r'\\s+(?:[Vv]ideos|VIDEOS)\\s*$'\n+    _TESTS = [{\n+        'note': 'Full list with pagination (too long for test)',\n+        'url': 'https://www.youporn.com/',\n+        'info_dict': {\n+            'id': 'youporn',\n+            'title': 'YouPorn videos',\n+        },\n+        'only_matching': True,\n+    }, {\n+        'note': 'Full list with pagination (too long for test)',\n+        'url': 'https://www.youporn.com/recommended',\n+        'info_dict': {\n+            'id': 'youporn/recommended',\n+            'title': 'YouPorn videos by recommended',\n+        },\n+        'only_matching': True,\n+    }, {\n+        'note': 'Full list with pagination (too long for test)',\n+        'url': 'https://www.youporn.com/top_rated',\n+        'info_dict': {\n+            'id': 'youporn/top_rated',\n+            'title': 'YouPorn videos by top rated',\n+        },\n+        'only_matching': True,\n+    }, {\n+        'note': 'Full list with pagination (too long for test)',\n+        'url': 'https://www.youporn.com/browse/time',\n+        'info_dict': {\n+            'id': 'browse/time',\n+            'title': 'YouPorn videos by time',\n+        },\n+        'only_matching': True,\n+    }, {\n+        'note': 'Filtered paginated list with single page result',\n+        'url': 'https://www.youporn.com/most_favorited/?res=VR&max_minutes=2',\n+        'info_dict': {\n+            'id': 'youporn/most_favorited/max_minutes=2/res=VR',\n+            'title': 'YouPorn videos by most favorited (max_minutes=2,res=VR)',\n+        },\n+        'playlist_mincount': 10,\n+        'playlist_maxcount': 28,\n+    }, {\n+        'note': 'Filtered paginated list with several pages',\n+        'url': 'https://www.youporn.com/most_favorited/?res=VR&max_minutes=5',\n+        'info_dict': {\n+            'id': 'youporn/most_favorited/max_minutes=5/res=VR',\n+            'title': 'YouPorn videos by most favorited (max_minutes=5,res=VR)',\n+        },\n+        'playlist_mincount': 45,\n+    }, {\n+        'note': 'Single page of full list',\n+        'url': 'https://www.youporn.com/browse/time?page=1',\n+        'info_dict': {\n+            'id': 'browse/time/page=1',\n+            'title': 'YouPorn videos by time (page=1)',\n+        },\n+        'playlist_count': 36,\n+    }]\n+\n+    @staticmethod\n+    def _get_title_from_slug(title_slug):\n+        return 'YouPorn' if title_slug == 'browse' else title_slug\ndiff --git a/youtube_dl/extractor/youtube.py b/youtube_dl/extractor/youtube.py\nindex 41695a561e5..6fe520e9a44 100644\n--- a/youtube_dl/extractor/youtube.py\n+++ b/youtube_dl/extractor/youtube.py\n@@ -2,6 +2,7 @@\n \n from __future__ import unicode_literals\n \n+import collections\n import itertools\n import json\n import os.path\n@@ -14,34 +15,47 @@\n     compat_chr,\n     compat_HTTPError,\n     compat_map as map,\n-    compat_parse_qs,\n     compat_str,\n+    compat_urllib_parse,\n+    compat_urllib_parse_parse_qs as compat_parse_qs,\n     compat_urllib_parse_unquote_plus,\n-    compat_urllib_parse_urlencode,\n     compat_urllib_parse_urlparse,\n-    compat_urlparse,\n+    compat_zip as zip,\n )\n from ..jsinterp import JSInterpreter\n from ..utils import (\n-    ExtractorError,\n     clean_html,\n     dict_get,\n     error_to_compat_str,\n+    ExtractorError,\n     float_or_none,\n+    extract_attributes,\n+    get_element_by_attribute,\n     int_or_none,\n+    join_nonempty,\n     js_to_json,\n+    LazyList,\n+    merge_dicts,\n     mimetype2ext,\n+    NO_DEFAULT,\n     parse_codecs,\n+    parse_count,\n     parse_duration,\n+    parse_qs,\n     qualities,\n     remove_start,\n     smuggle_url,\n     str_or_none,\n     str_to_int,\n+    T,\n+    traverse_obj,\n+    try_call,\n     try_get,\n+    txt_or_none,\n     unescapeHTML,\n     unified_strdate,\n     unsmuggle_url,\n+    update_url,\n     update_url_query,\n     url_or_none,\n     urlencode_postdata,\n@@ -49,10 +63,6 @@\n )\n \n \n-def parse_qs(url):\n-    return compat_urlparse.parse_qs(compat_urlparse.urlparse(url).query)\n-\n-\n class YoutubeBaseInfoExtractor(InfoExtractor):\n     \"\"\"Provide base functions for Youtube extractors\"\"\"\n     _LOGIN_URL = 'https://accounts.google.com/ServiceLogin'\n@@ -255,16 +265,10 @@ def _initialize_consent(self):\n         cookies = self._get_cookies('https://www.youtube.com/')\n         if cookies.get('__Secure-3PSID'):\n             return\n-        consent_id = None\n-        consent = cookies.get('CONSENT')\n-        if consent:\n-            if 'YES' in consent.value:\n-                return\n-            consent_id = self._search_regex(\n-                r'PENDING\\+(\\d+)', consent.value, 'consent', default=None)\n-        if not consent_id:\n-            consent_id = random.randint(100, 999)\n-        self._set_cookie('.youtube.com', 'CONSENT', 'YES+cb.20210328-17-p0.en+FX+%s' % consent_id)\n+        socs = cookies.get('SOCS')\n+        if socs and not socs.value.startswith('CAA'):  # not consented\n+            return\n+        self._set_cookie('.youtube.com', 'SOCS', 'CAI', secure=True)  # accept all (required for mixes)\n \n     def _real_initialize(self):\n         self._initialize_consent()\n@@ -286,15 +290,18 @@ def _real_initialize(self):\n     _YT_INITIAL_PLAYER_RESPONSE_RE = r'ytInitialPlayerResponse\\s*=\\s*({.+?})\\s*;'\n     _YT_INITIAL_BOUNDARY_RE = r'(?:var\\s+meta|</script|\\n)'\n \n-    def _call_api(self, ep, query, video_id, fatal=True):\n+    def _call_api(self, ep, query, video_id, fatal=True, headers=None):\n         data = self._DEFAULT_API_DATA.copy()\n         data.update(query)\n+        real_headers = {'content-type': 'application/json'}\n+        if headers:\n+            real_headers.update(headers)\n \n         return self._download_json(\n             'https://www.youtube.com/youtubei/v1/%s' % ep, video_id=video_id,\n             note='Downloading API JSON', errnote='Unable to download API page',\n             data=json.dumps(data).encode('utf8'), fatal=fatal,\n-            headers={'content-type': 'application/json'},\n+            headers=real_headers,\n             query={'key': 'AIzaSyAO_FJ2SlqU8Q4STEHLGCilw_Y9_11qcW8'})\n \n     def _extract_yt_initial_data(self, video_id, webpage):\n@@ -315,7 +322,8 @@ def _extract_video(self, renderer):\n         title = try_get(\n             renderer,\n             (lambda x: x['title']['runs'][0]['text'],\n-             lambda x: x['title']['simpleText']), compat_str)\n+             lambda x: x['title']['simpleText'],\n+             lambda x: x['headline']['simpleText']), compat_str)\n         description = try_get(\n             renderer, lambda x: x['descriptionSnippet']['runs'][0]['text'],\n             compat_str)\n@@ -396,6 +404,62 @@ def _search_results(self, query, params):\n                 break\n             data['continuation'] = token\n \n+    @staticmethod\n+    def _owner_endpoints_path():\n+        return [\n+            Ellipsis,\n+            lambda k, _: k.endswith('SecondaryInfoRenderer'),\n+            ('owner', 'videoOwner'), 'videoOwnerRenderer', 'title',\n+            'runs', Ellipsis]\n+\n+    def _extract_channel_id(self, webpage, videodetails={}, metadata={}, renderers=[]):\n+        channel_id = None\n+        if any((videodetails, metadata, renderers)):\n+            channel_id = (\n+                traverse_obj(videodetails, 'channelId')\n+                or traverse_obj(metadata, 'externalChannelId', 'externalId')\n+                or traverse_obj(renderers,\n+                                self._owner_endpoints_path() + [\n+                                    'navigationEndpoint', 'browseEndpoint', 'browseId'],\n+                                get_all=False)\n+            )\n+        return channel_id or self._html_search_meta(\n+            'channelId', webpage, 'channel id', default=None)\n+\n+    def _extract_author_var(self, webpage, var_name,\n+                            videodetails={}, metadata={}, renderers=[]):\n+        result = None\n+        paths = {\n+            #       (HTML, videodetails, metadata, renderers)\n+            'name': ('content', 'author', (('ownerChannelName', None), 'title'), ['text']),\n+            'url': ('href', 'ownerProfileUrl', 'vanityChannelUrl',\n+                    ['navigationEndpoint', 'browseEndpoint', 'canonicalBaseUrl'])\n+        }\n+        if any((videodetails, metadata, renderers)):\n+            result = (\n+                traverse_obj(videodetails, paths[var_name][1], get_all=False)\n+                or traverse_obj(metadata, paths[var_name][2], get_all=False)\n+                or traverse_obj(renderers,\n+                                self._owner_endpoints_path() + paths[var_name][3],\n+                                get_all=False)\n+            )\n+        return result or traverse_obj(\n+            extract_attributes(self._search_regex(\n+                r'''(?s)(<link\\b[^>]+\\bitemprop\\s*=\\s*(\"|')%s\\2[^>]*>)'''\n+                % re.escape(var_name),\n+                get_element_by_attribute('itemprop', 'author', webpage or '') or '',\n+                'author link', default='')),\n+            paths[var_name][0])\n+\n+    @staticmethod\n+    def _yt_urljoin(url_or_path):\n+        return urljoin('https://www.youtube.com', url_or_path)\n+\n+    def _extract_uploader_id(self, uploader_url):\n+        return self._search_regex(\n+            r'/(?:(?:channel|user)/|(?=@))([^/?&#]+)', uploader_url or '',\n+            'uploader id', default=None)\n+\n \n class YoutubeIE(YoutubeBaseInfoExtractor):\n     IE_DESC = 'YouTube.com'\n@@ -499,7 +563,7 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n         r'/(?P<id>[a-zA-Z0-9_-]{8,})/player(?:_ias\\.vflset(?:/[a-zA-Z]{2,3}_[a-zA-Z]{2,3})?|-plasma-ias-(?:phone|tablet)-[a-z]{2}_[A-Z]{2}\\.vflset)/base\\.js$',\n         r'\\b(?P<id>vfl[a-zA-Z0-9_-]+)\\b.*?\\.js$',\n     )\n-    _SUBTITLE_FORMATS = ('srv1', 'srv2', 'srv3', 'ttml', 'vtt')\n+    _SUBTITLE_FORMATS = ('json3', 'srv1', 'srv2', 'srv3', 'ttml', 'vtt')\n \n     _GEO_BYPASS = False\n \n@@ -512,8 +576,9 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'ext': 'mp4',\n                 'title': 'youtube-dl test video \"\\'/\\\\\u00e4\u21ad\ud835\udd50',\n                 'uploader': 'Philipp Hagemeister',\n-                'uploader_id': 'phihag',\n-                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/user/phihag',\n+                'uploader_id': '@PhilippHagemeister',\n+                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@PhilippHagemeister',\n+                'channel': 'Philipp Hagemeister',\n                 'channel_id': 'UCLqxVugv74EIW3VWh2NOa3Q',\n                 'channel_url': r're:https?://(?:www\\.)?youtube\\.com/channel/UCLqxVugv74EIW3VWh2NOa3Q',\n                 'upload_date': '20121002',\n@@ -523,10 +588,10 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'duration': 10,\n                 'view_count': int,\n                 'like_count': int,\n-                'dislike_count': int,\n+                'thumbnail': 'https://i.ytimg.com/vi/BaW_jenozKc/maxresdefault.jpg',\n                 'start_time': 1,\n                 'end_time': 9,\n-            }\n+            },\n         },\n         {\n             'url': '//www.YouTube.com/watch?v=yZIXLfi8CZQ',\n@@ -552,8 +617,8 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'ext': 'mp4',\n                 'title': 'youtube-dl test video \"\\'/\\\\\u00e4\u21ad\ud835\udd50',\n                 'uploader': 'Philipp Hagemeister',\n-                'uploader_id': 'phihag',\n-                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/user/phihag',\n+                'uploader_id': '@PhilippHagemeister',\n+                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@PhilippHagemeister',\n                 'upload_date': '20121002',\n                 'description': 'test chars:  \"\\'/\\\\\u00e4\u21ad\ud835\udd50\\ntest URL: https://github.com/rg3/youtube-dl/issues/1892\\n\\nThis is a test video for youtube-dl.\\n\\nFor more information, contact phihag@phihag.de .',\n                 'categories': ['Science & Technology'],\n@@ -561,7 +626,6 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'duration': 10,\n                 'view_count': int,\n                 'like_count': int,\n-                'dislike_count': int,\n             },\n             'params': {\n                 'skip_download': True,\n@@ -584,7 +648,7 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'youtube_include_dash_manifest': True,\n                 'format': '141',\n             },\n-            'skip': 'format 141 not served anymore',\n+            'skip': 'format 141 not served any more',\n         },\n         # DASH manifest with encrypted signature\n         {\n@@ -596,7 +660,7 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'description': 'md5:8f5e2b82460520b619ccac1f509d43bf',\n                 'duration': 244,\n                 'uploader': 'AfrojackVEVO',\n-                'uploader_id': 'AfrojackVEVO',\n+                'uploader_id': '@AfrojackVEVO',\n                 'upload_date': '20131011',\n                 'abr': 129.495,\n             },\n@@ -614,14 +678,15 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'duration': 219,\n                 'upload_date': '20100909',\n                 'uploader': 'Amazing Atheist',\n-                'uploader_id': 'TheAmazingAtheist',\n-                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/user/TheAmazingAtheist',\n+                'uploader_id': '@theamazingatheist',\n+                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@theamazingatheist',\n                 'title': 'Burning Everyone\\'s Koran',\n                 'description': 'SUBSCRIBE: http://www.youtube.com/saturninefilms \\r\\n\\r\\nEven Obama has taken a stand against freedom on this issue: http://www.huffingtonpost.com/2010/09/09/obama-gma-interview-quran_n_710282.html',\n             }\n         },\n-        # Normal age-gate video (No vevo, embed allowed), available via embed page\n+        # Age-gated videos\n         {\n+            'note': 'Age-gated video (No vevo, embed allowed)',\n             'url': 'https://youtube.com/watch?v=HtVdAasjOgU',\n             'info_dict': {\n                 'id': 'HtVdAasjOgU',\n@@ -630,17 +695,101 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'description': r're:(?s).{100,}About the Game\\n.*?The Witcher 3: Wild Hunt.{100,}',\n                 'duration': 142,\n                 'uploader': 'The Witcher',\n-                'uploader_id': 'WitcherGame',\n-                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/user/WitcherGame',\n+                'uploader_id': '@thewitcher',\n+                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@thewitcher',\n                 'upload_date': '20140605',\n+                'thumbnail': 'https://i.ytimg.com/vi/HtVdAasjOgU/maxresdefault.jpg',\n                 'age_limit': 18,\n+                'categories': ['Gaming'],\n+                'tags': 'count:17',\n+                'channel': 'The Witcher',\n+                'channel_url': 'https://www.youtube.com/channel/UCzybXLxv08IApdjdN0mJhEg',\n+                'channel_id': 'UCzybXLxv08IApdjdN0mJhEg',\n+                'view_count': int,\n+                'like_count': int,\n             },\n         },\n         {\n-            # Age-gated video only available with authentication (unavailable\n-            # via embed page workaround)\n+            'note': 'Age-gated video with embed allowed in public site',\n+            'url': 'https://youtube.com/watch?v=HsUATh_Nc2U',\n+            'info_dict': {\n+                'id': 'HsUATh_Nc2U',\n+                'ext': 'mp4',\n+                'title': 'Godzilla 2 (Official Video)',\n+                'description': 'md5:bf77e03fcae5529475e500129b05668a',\n+                'duration': 177,\n+                'uploader': 'FlyingKitty',\n+                'uploader_id': '@FlyingKitty900',\n+                'upload_date': '20200408',\n+                'thumbnail': 'https://i.ytimg.com/vi/HsUATh_Nc2U/maxresdefault.jpg',\n+                'age_limit': 18,\n+                'categories': ['Entertainment'],\n+                'tags': ['Flyingkitty', 'godzilla 2'],\n+                'channel': 'FlyingKitty',\n+                'channel_url': 'https://www.youtube.com/channel/UCYQT13AtrJC0gsM1far_zJg',\n+                'channel_id': 'UCYQT13AtrJC0gsM1far_zJg',\n+                'view_count': int,\n+                'like_count': int,\n+            },\n+        },\n+        {\n+            'note': 'Age-gated video embeddable only with clientScreen=EMBED',\n+            'url': 'https://youtube.com/watch?v=Tq92D6wQ1mg',\n+            'info_dict': {\n+                'id': 'Tq92D6wQ1mg',\n+                'ext': 'mp4',\n+                'title': '[MMD] Adios - EVERGLOW [+Motion DL]',\n+                'description': 'md5:17eccca93a786d51bc67646756894066',\n+                'duration': 106,\n+                'uploader': 'Projekt Melody',\n+                'uploader_id': '@ProjektMelody',\n+                'upload_date': '20191227',\n+                'age_limit': 18,\n+                'thumbnail': 'https://i.ytimg.com/vi/Tq92D6wQ1mg/sddefault.jpg',\n+                'tags': ['mmd', 'dance', 'mikumikudance', 'kpop', 'vtuber'],\n+                'categories': ['Entertainment'],\n+                'channel': 'Projekt Melody',\n+                'channel_url': 'https://www.youtube.com/channel/UC1yoRdFoFJaCY-AGfD9W0wQ',\n+                'channel_id': 'UC1yoRdFoFJaCY-AGfD9W0wQ',\n+                'view_count': int,\n+                'like_count': int,\n+            },\n+        },\n+        {\n+            'note': 'Non-Age-gated non-embeddable video',\n+            'url': 'https://youtube.com/watch?v=MeJVWBSsPAY',\n+            'info_dict': {\n+                'id': 'MeJVWBSsPAY',\n+                'ext': 'mp4',\n+                'title': 'OOMPH! - Such Mich Find Mich (Lyrics)',\n+                'description': 'Fan Video. Music & Lyrics by OOMPH!.',\n+                'duration': 210,\n+                'upload_date': '20130730',\n+                'uploader': 'Herr Lurik',\n+                'uploader_id': '@HerrLurik',\n+                'uploader_url': 'http://www.youtube.com/@HerrLurik',\n+                'age_limit': 0,\n+                'thumbnail': 'https://i.ytimg.com/vi/MeJVWBSsPAY/hqdefault.jpg',\n+                'tags': ['oomph', 'such mich find mich', 'lyrics', 'german industrial', 'musica industrial'],\n+                'categories': ['Music'],\n+                'channel': 'Herr Lurik',\n+                'channel_url': 'https://www.youtube.com/channel/UCdR3RSDPqub28LjZx0v9-aA',\n+                'channel_id': 'UCdR3RSDPqub28LjZx0v9-aA',\n+                'artist': 'OOMPH!',\n+                'view_count': int,\n+                'like_count': int,\n+            },\n+        },\n+        {\n+            'note': 'Non-bypassable age-gated video',\n+            'url': 'https://youtube.com/watch?v=Cr381pDsSsA',\n+            'only_matching': True,\n+        },\n+        {\n+            'note': 'Age-gated video only available with authentication (not via embed workaround)',\n             'url': 'XgnwCQzjau8',\n             'only_matching': True,\n+            'skip': '''This video has been removed for violating YouTube's Community Guidelines''',\n         },\n         # video_info is None (https://github.com/ytdl-org/youtube-dl/issues/4421)\n         # YouTube Red ad is not captured for creator\n@@ -651,8 +800,8 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'ext': 'mp4',\n                 'duration': 266,\n                 'upload_date': '20100430',\n-                'uploader_id': 'deadmau5',\n-                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/user/deadmau5',\n+                'uploader_id': '@deadmau5',\n+                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@deadmau5',\n                 'creator': 'deadmau5',\n                 'description': 'md5:6cbcd3a92ce1bc676fc4d6ab4ace2336',\n                 'uploader': 'deadmau5',\n@@ -669,17 +818,23 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n             'info_dict': {\n                 'id': 'lqQg6PlCWgI',\n                 'ext': 'mp4',\n+                'title': 'Hockey - Women -  GER-AUS - London 2012 Olympic Games',\n+                'description': r're:(?s)(?:.+\\s)?HO09  - Women -  GER-AUS - Hockey - 31 July 2012 - London 2012 Olympic Games\\s*',\n                 'duration': 6085,\n                 'upload_date': '20150827',\n-                'uploader_id': 'olympic',\n-                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/user/olympic',\n-                'description': 'HO09  - Women -  GER-AUS - Hockey - 31 July 2012 - London 2012 Olympic Games',\n-                'uploader': 'Olympic',\n-                'title': 'Hockey - Women -  GER-AUS - London 2012 Olympic Games',\n+                'uploader_id': '@Olympics',\n+                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@Olympics',\n+                'uploader': r're:Olympics?',\n+                'age_limit': 0,\n+                'thumbnail': 'https://i.ytimg.com/vi/lqQg6PlCWgI/maxresdefault.jpg',\n+                'categories': ['Sports'],\n+                'tags': ['Hockey', '2012-07-31', '31 July 2012', 'Riverbank Arena', 'Session', 'Olympics', 'Olympic Games', 'London 2012', '2012 Summer Olympics', 'Summer Games'],\n+                'channel': 'Olympics',\n+                'channel_url': 'https://www.youtube.com/channel/UCTl3QQTvqHFjurroKxexy2Q',\n+                'channel_id': 'UCTl3QQTvqHFjurroKxexy2Q',\n+                'view_count': int,\n+                'like_count': int,\n             },\n-            'params': {\n-                'skip_download': 'requires avconv',\n-            }\n         },\n         # Non-square pixels\n         {\n@@ -690,8 +845,8 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'stretched_ratio': 16 / 9.,\n                 'duration': 85,\n                 'upload_date': '20110310',\n-                'uploader_id': 'AllenMeow',\n-                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/user/AllenMeow',\n+                'uploader_id': '@AllenMeow',\n+                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@AllenMeow',\n                 'description': 'made by Wacom from Korea | \u5b57\u5e55&\u52a0\u6cb9\u6dfb\u918b by TY\\'s Allen | \u611f\u8b1dheylisa00cavey1001\u540c\u5b78\u71b1\u60c5\u63d0\u4f9b\u6897\u53ca\u7ffb\u8b6f',\n                 'uploader': '\u5b6b\u110b\u1105',\n                 'title': '[A-made] \u8b8a\u614b\u598d\u5b57\u5e55\u7248 \u592a\u598d \u6211\u5c31\u662f\u9019\u6a23\u7684\u4eba',\n@@ -729,7 +884,7 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'uploader': 'dorappi2000',\n                 'formats': 'mincount:31',\n             },\n-            'skip': 'not actual anymore',\n+            'skip': 'not actual any more',\n         },\n         # DASH manifest with segment_list\n         {\n@@ -810,6 +965,7 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n             'params': {\n                 'skip_download': True,\n             },\n+            'skip': 'Not multifeed any more',\n         },\n         {\n             # Multifeed video with comma in title (see https://github.com/ytdl-org/youtube-dl/issues/8536)\n@@ -819,7 +975,7 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'title': 'DevConf.cz 2016 Day 2 Workshops 1 14:00 - 15:30',\n             },\n             'playlist_count': 2,\n-            'skip': 'Not multifeed anymore',\n+            'skip': 'Not multifeed any more',\n         },\n         {\n             'url': 'https://vid.plus/FlRa-iH7PGw',\n@@ -839,16 +995,16 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'id': 'lsguqyKfVQg',\n                 'ext': 'mp4',\n                 'title': '{dark walk}; Loki/AC/Dishonored; collab w/Elflover21',\n-                'alt_title': 'Dark Walk - Position Music',\n+                'alt_title': 'Dark Walk',\n                 'description': 'md5:8085699c11dc3f597ce0410b0dcbb34a',\n                 'duration': 133,\n                 'upload_date': '20151119',\n-                'uploader_id': 'IronSoulElf',\n-                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/user/IronSoulElf',\n+                'uploader_id': '@IronSoulElf',\n+                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@IronSoulElf',\n                 'uploader': 'IronSoulElf',\n-                'creator': 'Todd Haberman,  Daniel Law Heath and Aaron Kaplan',\n-                'track': 'Dark Walk - Position Music',\n-                'artist': 'Todd Haberman,  Daniel Law Heath and Aaron Kaplan',\n+                'creator': r're:Todd Haberman[;,]\\s+Daniel Law Heath and Aaron Kaplan',\n+                'track': 'Dark Walk',\n+                'artist': r're:Todd Haberman[;,]\\s+Daniel Law Heath and Aaron Kaplan',\n                 'album': 'Position Music - Production Music Vol. 143 - Dark Walk',\n             },\n             'params': {\n@@ -892,8 +1048,8 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'description': 'md5:a677553cf0840649b731a3024aeff4cc',\n                 'duration': 721,\n                 'upload_date': '20150127',\n-                'uploader_id': 'BerkmanCenter',\n-                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/user/BerkmanCenter',\n+                'uploader_id': '@BKCHarvard',\n+                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@BKCHarvard',\n                 'uploader': 'The Berkman Klein Center for Internet & Society',\n                 'license': 'Creative Commons Attribution license (reuse allowed)',\n             },\n@@ -912,8 +1068,8 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'duration': 4060,\n                 'upload_date': '20151119',\n                 'uploader': 'Bernie Sanders',\n-                'uploader_id': 'UCH1dpzjCEiGAt8CXkryhkZg',\n-                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/channel/UCH1dpzjCEiGAt8CXkryhkZg',\n+                'uploader_id': '@BernieSanders',\n+                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@BernieSanders',\n                 'license': 'Creative Commons Attribution license (reuse allowed)',\n             },\n             'params': {\n@@ -959,8 +1115,8 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'duration': 2085,\n                 'upload_date': '20170118',\n                 'uploader': 'Vsauce',\n-                'uploader_id': 'Vsauce',\n-                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/user/Vsauce',\n+                'uploader_id': '@Vsauce',\n+                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@Vsauce',\n                 'series': 'Mind Field',\n                 'season_number': 1,\n                 'episode_number': 1,\n@@ -1039,7 +1195,7 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'skip_download': True,\n                 'youtube_include_dash_manifest': False,\n             },\n-            'skip': 'not actual anymore',\n+            'skip': 'not actual any more',\n         },\n         {\n             # Youtube Music Auto-generated description\n@@ -1096,8 +1252,8 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'title': 'IMG 3456',\n                 'description': '',\n                 'upload_date': '20170613',\n-                'uploader_id': 'ElevageOrVert',\n-                'uploader': 'ElevageOrVert',\n+                'uploader': \"l'Or Vert asbl\",\n+                'uploader_id': '@ElevageOrVert',\n             },\n             'params': {\n                 'skip_download': True,\n@@ -1115,8 +1271,8 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'title': 'Part 77   Sort a list of simple types in c#',\n                 'description': 'md5:b8746fa52e10cdbf47997903f13b20dc',\n                 'upload_date': '20130831',\n-                'uploader_id': 'kudvenkat',\n                 'uploader': 'kudvenkat',\n+                'uploader_id': '@Csharp-video-tutorialsBlogspot',\n             },\n             'params': {\n                 'skip_download': True,\n@@ -1168,8 +1324,8 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'description': 'md5:ea770e474b7cd6722b4c95b833c03630',\n                 'upload_date': '20201120',\n                 'uploader': 'Walk around Japan',\n-                'uploader_id': 'UC3o_t8PzBmXf5S9b7GLx1Mw',\n-                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/channel/UC3o_t8PzBmXf5S9b7GLx1Mw',\n+                'uploader_id': '@walkaroundjapan7124',\n+                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@walkaroundjapan7124',\n             },\n             'params': {\n                 'skip_download': True,\n@@ -1181,11 +1337,11 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n             'info_dict': {\n                 'id': '4L2J27mJ3Dc',\n                 'ext': 'mp4',\n+                'title': 'Midwest Squid Game #Shorts',\n+                'description': 'md5:976512b8a29269b93bbd8a61edc45a6d',\n                 'upload_date': '20211025',\n                 'uploader': 'Charlie Berens',\n-                'description': 'md5:976512b8a29269b93bbd8a61edc45a6d',\n-                'uploader_id': 'fivedlrmilkshake',\n-                'title': 'Midwest Squid Game #Shorts',\n+                'uploader_id': '@CharlieBerens',\n             },\n             'params': {\n                 'skip_download': True,\n@@ -1300,11 +1456,7 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n \n     @classmethod\n     def suitable(cls, url):\n-        # Hack for lazy extractors until more generic solution is implemented\n-        # (see #28780)\n-        from .youtube import parse_qs\n-        qs = parse_qs(url)\n-        if qs.get('list', [None])[0]:\n+        if parse_qs(url).get('list', [None])[0]:\n             return False\n         return super(YoutubeIE, cls).suitable(url)\n \n@@ -1313,6 +1465,30 @@ def __init__(self, *args, **kwargs):\n         self._code_cache = {}\n         self._player_cache = {}\n \n+    # *ytcfgs, webpage=None\n+    def _extract_player_url(self, *ytcfgs, **kw_webpage):\n+        if ytcfgs and not isinstance(ytcfgs[0], dict):\n+            webpage = kw_webpage.get('webpage') or ytcfgs[0]\n+        if webpage:\n+            player_url = self._search_regex(\n+                r'\"(?:PLAYER_JS_URL|jsUrl)\"\\s*:\\s*\"([^\"]+)\"',\n+                webpage or '', 'player URL', fatal=False)\n+            if player_url:\n+                ytcfgs = ytcfgs + ({'PLAYER_JS_URL': player_url},)\n+        return traverse_obj(\n+            ytcfgs, (Ellipsis, 'PLAYER_JS_URL'), (Ellipsis, 'WEB_PLAYER_CONTEXT_CONFIGS', Ellipsis, 'jsUrl'),\n+            get_all=False, expected_type=lambda u: urljoin('https://www.youtube.com', u))\n+\n+    def _download_player_url(self, video_id, fatal=False):\n+        res = self._download_webpage(\n+            'https://www.youtube.com/iframe_api',\n+            note='Downloading iframe API JS', video_id=video_id, fatal=fatal)\n+        player_version = self._search_regex(\n+            r'player\\\\?/([0-9a-fA-F]{8})\\\\?/', res or '', 'player version', fatal=fatal,\n+            default=NO_DEFAULT if res else None)\n+        if player_version:\n+            return 'https://www.youtube.com/s/player/{0}/player_ias.vflset/en_US/base.js'.format(player_version)\n+\n     def _signature_cache_id(self, example_sig):\n         \"\"\" Return a string representation of a signature \"\"\"\n         return '.'.join(compat_str(len(part)) for part in example_sig.split('.'))\n@@ -1327,46 +1503,49 @@ def _extract_player_info(cls, player_url):\n             raise ExtractorError('Cannot identify player %r' % player_url)\n         return id_m.group('id')\n \n-    def _get_player_code(self, video_id, player_url, player_id=None):\n+    def _load_player(self, video_id, player_url, fatal=True, player_id=None):\n         if not player_id:\n             player_id = self._extract_player_info(player_url)\n-\n         if player_id not in self._code_cache:\n-            self._code_cache[player_id] = self._download_webpage(\n-                player_url, video_id,\n+            code = self._download_webpage(\n+                player_url, video_id, fatal=fatal,\n                 note='Downloading player ' + player_id,\n                 errnote='Download of %s failed' % player_url)\n-        return self._code_cache[player_id]\n+            if code:\n+                self._code_cache[player_id] = code\n+        return self._code_cache[player_id] if fatal else self._code_cache.get(player_id)\n \n     def _extract_signature_function(self, video_id, player_url, example_sig):\n         player_id = self._extract_player_info(player_url)\n \n         # Read from filesystem cache\n-        func_id = 'js_%s_%s' % (\n+        func_id = 'js_{0}_{1}'.format(\n             player_id, self._signature_cache_id(example_sig))\n         assert os.path.basename(func_id) == func_id\n \n-        cache_spec = self._downloader.cache.load('youtube-sigfuncs', func_id)\n-        if cache_spec is not None:\n-            return lambda s: ''.join(s[i] for i in cache_spec)\n-\n-        code = self._get_player_code(video_id, player_url, player_id)\n-        res = self._parse_sig_js(code)\n+        self.write_debug('Extracting signature function {0}'.format(func_id))\n+        cache_spec, code = self.cache.load('youtube-sigfuncs', func_id), None\n \n-        test_string = ''.join(map(compat_chr, range(len(example_sig))))\n-        cache_res = res(test_string)\n-        cache_spec = [ord(c) for c in cache_res]\n+        if not cache_spec:\n+            code = self._load_player(video_id, player_url, player_id)\n+        if code:\n+            res = self._parse_sig_js(code)\n+            test_string = ''.join(map(compat_chr, range(len(example_sig))))\n+            cache_spec = [ord(c) for c in res(test_string)]\n+            self.cache.store('youtube-sigfuncs', func_id, cache_spec)\n \n-        self._downloader.cache.store('youtube-sigfuncs', func_id, cache_spec)\n-        return res\n+        return lambda s: ''.join(s[i] for i in cache_spec)\n \n     def _print_sig_code(self, func, example_sig):\n+        if not self.get_param('youtube_print_sig_code'):\n+            return\n+\n         def gen_sig_code(idxs):\n             def _genslice(start, end, step):\n                 starts = '' if start == 0 else str(start)\n                 ends = (':%d' % (end + step)) if end + step >= 0 else ':'\n                 steps = '' if step == 1 else (':%d' % step)\n-                return 's[%s%s%s]' % (starts, ends, steps)\n+                return 's[{0}{1}{2}]'.format(starts, ends, steps)\n \n             step = None\n             # Quelch pyflakes warnings - start will be set when step is set\n@@ -1405,17 +1584,14 @@ def _parse_sig_js(self, jscode):\n              r'\\b[a-zA-Z0-9]+\\s*&&\\s*[a-zA-Z0-9]+\\.set\\([^,]+\\s*,\\s*encodeURIComponent\\s*\\(\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n              r'\\bm=(?P<sig>[a-zA-Z0-9$]{2,})\\(decodeURIComponent\\(h\\.s\\)\\)',\n              r'\\bc&&\\(c=(?P<sig>[a-zA-Z0-9$]{2,})\\(decodeURIComponent\\(c\\)\\)',\n-             r'(?:\\b|[^a-zA-Z0-9$])(?P<sig>[a-zA-Z0-9$]{2,})\\s*=\\s*function\\(\\s*a\\s*\\)\\s*{\\s*a\\s*=\\s*a\\.split\\(\\s*\"\"\\s*\\);[a-zA-Z0-9$]{2}\\.[a-zA-Z0-9$]{2}\\(a,\\d+\\)',\n-             r'(?:\\b|[^a-zA-Z0-9$])(?P<sig>[a-zA-Z0-9$]{2,})\\s*=\\s*function\\(\\s*a\\s*\\)\\s*{\\s*a\\s*=\\s*a\\.split\\(\\s*\"\"\\s*\\)',\n+             r'(?:\\b|[^a-zA-Z0-9$])(?P<sig>[a-zA-Z0-9$]{2,})\\s*=\\s*function\\(\\s*a\\s*\\)\\s*{\\s*a\\s*=\\s*a\\.split\\(\\s*\"\"\\s*\\)(?:;[a-zA-Z0-9$]{2}\\.[a-zA-Z0-9$]{2}\\(a,\\d+\\))?',\n              r'(?P<sig>[a-zA-Z0-9$]+)\\s*=\\s*function\\(\\s*a\\s*\\)\\s*{\\s*a\\s*=\\s*a\\.split\\(\\s*\"\"\\s*\\)',\n              # Obsolete patterns\n-             r'([\"\\'])signature\\1\\s*,\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n+             r'(\"|\\')signature\\1\\s*,\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n              r'\\.sig\\|\\|(?P<sig>[a-zA-Z0-9$]+)\\(',\n              r'yt\\.akamaized\\.net/\\)\\s*\\|\\|\\s*.*?\\s*[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?:encodeURIComponent\\s*\\()?\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n              r'\\b[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n              r'\\b[a-zA-Z0-9]+\\s*&&\\s*[a-zA-Z0-9]+\\.set\\([^,]+\\s*,\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'\\bc\\s*&&\\s*a\\.set\\([^,]+\\s*,\\s*\\([^)]*\\)\\s*\\(\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n-             r'\\bc\\s*&&\\s*[a-zA-Z0-9]+\\.set\\([^,]+\\s*,\\s*\\([^)]*\\)\\s*\\(\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n              r'\\bc\\s*&&\\s*[a-zA-Z0-9]+\\.set\\([^,]+\\s*,\\s*\\([^)]*\\)\\s*\\(\\s*(?P<sig>[a-zA-Z0-9$]+)\\('),\n             jscode, 'Initial JS player signature function name', group='sig')\n \n@@ -1423,121 +1599,184 @@ def _parse_sig_js(self, jscode):\n         initial_function = jsi.extract_function(funcname)\n         return lambda s: initial_function([s])\n \n-    def _decrypt_signature(self, s, video_id, player_url):\n-        \"\"\"Turn the encrypted s field into a working signature\"\"\"\n+    def _cached(self, func, *cache_id):\n+        def inner(*args, **kwargs):\n+            if cache_id not in self._player_cache:\n+                try:\n+                    self._player_cache[cache_id] = func(*args, **kwargs)\n+                except ExtractorError as e:\n+                    self._player_cache[cache_id] = e\n+                except Exception as e:\n+                    self._player_cache[cache_id] = ExtractorError(traceback.format_exc(), cause=e)\n \n-        if player_url is None:\n-            raise ExtractorError('Cannot decrypt signature without player_url')\n+            ret = self._player_cache[cache_id]\n+            if isinstance(ret, Exception):\n+                raise ret\n+            return ret\n+        return inner\n \n-        try:\n-            player_id = (player_url, self._signature_cache_id(s))\n-            if player_id not in self._player_cache:\n-                func = self._extract_signature_function(\n-                    video_id, player_url, s\n-                )\n-                self._player_cache[player_id] = func\n-            func = self._player_cache[player_id]\n-            if self._downloader.params.get('youtube_print_sig_code'):\n-                self._print_sig_code(func, s)\n-            return func(s)\n-        except Exception as e:\n-            tb = traceback.format_exc()\n-            raise ExtractorError(\n-                'Signature extraction failed: ' + tb, cause=e)\n-\n-    def _extract_player_url(self, webpage):\n-        player_url = self._search_regex(\n-            r'\"(?:PLAYER_JS_URL|jsUrl)\"\\s*:\\s*\"([^\"]+)\"',\n-            webpage or '', 'player URL', fatal=False)\n-        if not player_url:\n-            return\n-        if player_url.startswith('//'):\n-            player_url = 'https:' + player_url\n-        elif not re.match(r'https?://', player_url):\n-            player_url = compat_urlparse.urljoin(\n-                'https://www.youtube.com', player_url)\n-        return player_url\n+    def _decrypt_signature(self, s, video_id, player_url):\n+        \"\"\"Turn the encrypted s field into a working signature\"\"\"\n+        extract_sig = self._cached(\n+            self._extract_signature_function, 'sig', player_url, self._signature_cache_id(s))\n+        func = extract_sig(video_id, player_url, s)\n+        self._print_sig_code(func, s)\n+        return func(s)\n \n     # from yt-dlp\n     # See also:\n     # 1. https://github.com/ytdl-org/youtube-dl/issues/29326#issuecomment-894619419\n     # 2. https://code.videolan.org/videolan/vlc/-/blob/4fb284e5af69aa9ac2100ccbdd3b88debec9987f/share/lua/playlist/youtube.lua#L116\n     # 3. https://github.com/ytdl-org/youtube-dl/issues/30097#issuecomment-950157377\n+    def _decrypt_nsig(self, n, video_id, player_url):\n+        \"\"\"Turn the encrypted n field into a working signature\"\"\"\n+        if player_url is None:\n+            raise ExtractorError('Cannot decrypt nsig without player_url')\n+\n+        try:\n+            jsi, player_id, func_code = self._extract_n_function_code(video_id, player_url)\n+        except ExtractorError as e:\n+            raise ExtractorError('Unable to extract nsig function code', cause=e)\n+        if self.get_param('youtube_print_sig_code'):\n+            self.to_screen('Extracted nsig function from {0}:\\n{1}\\n'.format(\n+                player_id, func_code[1]))\n+\n+        try:\n+            extract_nsig = self._cached(self._extract_n_function_from_code, 'nsig func', player_url)\n+            ret = extract_nsig(jsi, func_code)(n)\n+        except JSInterpreter.Exception as e:\n+            self.report_warning(\n+                '%s (%s %s)' % (\n+                    'Unable to decode n-parameter: expect download to be blocked or throttled',\n+                    error_to_compat_str(e),\n+                    traceback.format_exc()),\n+                video_id=video_id)\n+            return\n+\n+        self.write_debug('Decrypted nsig {0} => {1}'.format(n, ret))\n+        return ret\n+\n     def _extract_n_function_name(self, jscode):\n-        target = r'(?P<nfunc>[a-zA-Z0-9$]{3})(?:\\[(?P<idx>\\d+)\\])?'\n-        nfunc_and_idx = self._search_regex(\n-            r'\\.get\\(\"n\"\\)\\)&&\\(b=(%s)\\([a-zA-Z0-9]\\)' % (target, ),\n-            jscode, 'Initial JS player n function name')\n-        nfunc, idx = re.match(target, nfunc_and_idx).group('nfunc', 'idx')\n+        func_name, idx = self._search_regex(\n+            # new: (b=String.fromCharCode(110),c=a.get(b))&&c=nfunc[idx](c)\n+            # or:  (b=\"nn\"[+a.D],c=a.get(b))&&(c=nfunc[idx](c)\n+            # or:  (PL(a),b=a.j.n||null)&&(b=nfunc[idx](b)\n+            # or:  (b=\"nn\"[+a.D],vL(a),c=a.j[b]||null)&&(c=narray[idx](c),a.set(b,c),narray.length||nfunc(\"\")\n+            # old: (b=a.get(\"n\"))&&(b=nfunc[idx](b)(?P<c>[a-z])\\s*=\\s*[a-z]\\s*\n+            # older: (b=a.get(\"n\"))&&(b=nfunc(b)\n+            r'''(?x)\n+                \\((?:[\\w$()\\s]+,)*?\\s*      # (\n+                (?P<b>[a-z])\\s*=\\s*         # b=\n+                (?:\n+                    (?:                     # expect ,c=a.get(b) (etc)\n+                        String\\s*\\.\\s*fromCharCode\\s*\\(\\s*110\\s*\\)|\n+                        \"n+\"\\[\\s*\\+?s*[\\w$.]+\\s*]\n+                    )\\s*(?:,[\\w$()\\s]+(?=,))*|\n+                       (?P<old>[\\w$]+)      # a (old[er])\n+                   )\\s*\n+                   (?(old)\n+                                            # b.get(\"n\")\n+                       (?:\\.\\s*[\\w$]+\\s*|\\[\\s*[\\w$]+\\s*]\\s*)*?\n+                       (?:\\.\\s*n|\\[\\s*\"n\"\\s*]|\\.\\s*get\\s*\\(\\s*\"n\"\\s*\\))\n+                       |                    # ,c=a.get(b)\n+                       ,\\s*(?P<c>[a-z])\\s*=\\s*[a-z]\\s*\n+                       (?:\\.\\s*[\\w$]+\\s*|\\[\\s*[\\w$]+\\s*]\\s*)*?\n+                       (?:\\[\\s*(?P=b)\\s*]|\\.\\s*get\\s*\\(\\s*(?P=b)\\s*\\))\n+                   )\n+                                            # interstitial junk\n+                   \\s*(?:\\|\\|\\s*null\\s*)?(?:\\)\\s*)?&&\\s*(?:\\(\\s*)?\n+               (?(c)(?P=c)|(?P=b))\\s*=\\s*   # [c|b]=\n+                                            # nfunc|nfunc[idx]\n+                   (?P<nfunc>[a-zA-Z_$][\\w$]*)(?:\\s*\\[(?P<idx>\\d+)\\])?\\s*\\(\\s*[\\w$]+\\s*\\)\n+            ''', jscode, 'Initial JS player n function name', group=('nfunc', 'idx'),\n+            default=(None, None))\n+        # thx bashonly: yt-dlp/yt-dlp/pull/10611\n+        if not func_name:\n+            self.report_warning('Falling back to generic n function search')\n+            return self._search_regex(\n+                r'''(?xs)\n+                    (?:(?<=[^\\w$])|^)       # instead of \\b, which ignores $\n+                    (?P<name>(?!\\d)[a-zA-Z\\d_$]+)\\s*=\\s*function\\((?!\\d)[a-zA-Z\\d_$]+\\)\n+                    \\s*\\{(?:(?!};).)+?[\"']enhanced_except_\n+                ''', jscode, 'Initial JS player n function name', group='name')\n         if not idx:\n-            return nfunc\n+            return func_name\n+\n         return self._parse_json(self._search_regex(\n-            r'var %s\\s*=\\s*(\\[.+?\\]);' % (nfunc, ), jscode,\n-            'Initial JS player n function list ({nfunc}[{idx}])'.format(**locals())), nfunc, transform_source=js_to_json)[int(idx)]\n+            r'var\\s+{0}\\s*=\\s*(\\[.+?\\])\\s*[,;]'.format(re.escape(func_name)), jscode,\n+            'Initial JS player n function list ({0}.{1})'.format(func_name, idx)),\n+            func_name, transform_source=js_to_json)[int(idx)]\n \n-    def _extract_n_function(self, video_id, player_url):\n+    def _extract_n_function_code(self, video_id, player_url):\n         player_id = self._extract_player_info(player_url)\n-        func_code = self._downloader.cache.load('youtube-nsig', player_id)\n+        func_code = self.cache.load('youtube-nsig', player_id)\n+        jscode = func_code or self._load_player(video_id, player_url)\n+        jsi = JSInterpreter(jscode)\n \n         if func_code:\n-            jsi = JSInterpreter(func_code)\n-        else:\n-            player_id = self._extract_player_info(player_url)\n-            jscode = self._get_player_code(video_id, player_url, player_id)\n-            funcname = self._extract_n_function_name(jscode)\n-            jsi = JSInterpreter(jscode)\n-            func_code = jsi.extract_function_code(funcname)\n-            self._downloader.cache.store('youtube-nsig', player_id, func_code)\n+            return jsi, player_id, func_code\n \n-        if self._downloader.params.get('youtube_print_sig_code'):\n-            self.to_screen('Extracted nsig function from {0}:\\n{1}\\n'.format(player_id, func_code[1]))\n+        func_name = self._extract_n_function_name(jscode)\n \n-        return lambda s: jsi.extract_function_from_code(*func_code)([s])\n+        func_code = jsi.extract_function_code(func_name)\n \n-    def _n_descramble(self, n_param, player_url, video_id):\n-        \"\"\"Compute the response to YT's \"n\" parameter challenge\n+        self.cache.store('youtube-nsig', player_id, func_code)\n+        return jsi, player_id, func_code\n \n-        Args:\n-        n_param     -- challenge string that is the value of the\n-                       URL's \"n\" query parameter\n-        player_url  -- URL of YT player JS\n-        video_id\n-        \"\"\"\n+    def _extract_n_function_from_code(self, jsi, func_code):\n+        func = jsi.extract_function_from_code(*func_code)\n \n-        sig_id = ('nsig_value', n_param)\n-        if sig_id in self._player_cache:\n-            return self._player_cache[sig_id]\n+        def extract_nsig(s):\n+            try:\n+                ret = func([s])\n+            except JSInterpreter.Exception:\n+                raise\n+            except Exception as e:\n+                raise JSInterpreter.Exception(traceback.format_exc(), cause=e)\n \n-        try:\n-            player_id = ('nsig', player_url)\n-            if player_id not in self._player_cache:\n-                self._player_cache[player_id] = self._extract_n_function(video_id, player_url)\n-            func = self._player_cache[player_id]\n-            self._player_cache[sig_id] = func(n_param)\n-            if self._downloader.params.get('verbose', False):\n-                self._downloader.to_screen('[debug] [%s] %s' % (self.IE_NAME, 'Decrypted nsig {0} => {1}'.format(n_param, self._player_cache[sig_id])))\n-            return self._player_cache[sig_id]\n-        except Exception as e:\n-            self._downloader.report_warning(\n-                '[%s] %s (%s %s)' % (\n-                    self.IE_NAME,\n-                    'Unable to decode n-parameter: download likely to be throttled',\n-                    error_to_compat_str(e),\n-                    traceback.format_exc()))\n+            if ret.startswith('enhanced_except_'):\n+                raise JSInterpreter.Exception('Signature function returned an exception')\n+            return ret\n+\n+        return extract_nsig\n+\n+    def _unthrottle_format_urls(self, video_id, player_url, *formats):\n+\n+        def decrypt_nsig(n):\n+            return self._cached(self._decrypt_nsig, 'nsig', n, player_url)\n \n-    def _unthrottle_format_urls(self, video_id, player_url, formats):\n         for fmt in formats:\n-            parsed_fmt_url = compat_urlparse.urlparse(fmt['url'])\n-            qs = compat_urlparse.parse_qs(parsed_fmt_url.query)\n-            n_param = qs.get('n')\n+            parsed_fmt_url = compat_urllib_parse.urlparse(fmt['url'])\n+            n_param = compat_parse_qs(parsed_fmt_url.query).get('n')\n             if not n_param:\n                 continue\n             n_param = n_param[-1]\n-            n_response = self._n_descramble(n_param, player_url, video_id)\n-            if n_response:\n-                qs['n'] = [n_response]\n-                fmt['url'] = compat_urlparse.urlunparse(\n-                    parsed_fmt_url._replace(query=compat_urllib_parse_urlencode(qs, True)))\n+            n_response = decrypt_nsig(n_param)(n_param, video_id, player_url)\n+            if n_response is None:\n+                # give up if descrambling failed\n+                break\n+            fmt['url'] = update_url_query(fmt['url'], {'n': n_response})\n+\n+    # from yt-dlp, with tweaks\n+    def _extract_signature_timestamp(self, video_id, player_url, ytcfg=None, fatal=False):\n+        \"\"\"\n+        Extract signatureTimestamp (sts)\n+        Required to tell API what sig/player version is in use.\n+        \"\"\"\n+        sts = traverse_obj(ytcfg, 'STS', expected_type=int)\n+        if not sts:\n+            # Attempt to extract from player\n+            if player_url is None:\n+                error_msg = 'Cannot extract signature timestamp without player_url.'\n+                if fatal:\n+                    raise ExtractorError(error_msg)\n+                self.report_warning(error_msg)\n+                return\n+            code = self._load_player(video_id, player_url, fatal=fatal)\n+            sts = int_or_none(self._search_regex(\n+                r'(?:signatureTimestamp|sts)\\s*:\\s*(?P<sts>[0-9]{5})', code or '',\n+                'JS player signature timestamp', group='sts', fatal=fatal))\n+        return sts\n \n     def _mark_watched(self, video_id, player_response):\n         playback_url = url_or_none(try_get(\n@@ -1545,20 +1784,23 @@ def _mark_watched(self, video_id, player_response):\n             lambda x: x['playbackTracking']['videostatsPlaybackUrl']['baseUrl']))\n         if not playback_url:\n             return\n-        parsed_playback_url = compat_urlparse.urlparse(playback_url)\n-        qs = compat_urlparse.parse_qs(parsed_playback_url.query)\n \n         # cpn generation algorithm is reverse engineered from base.js.\n         # In fact it works even with dummy cpn.\n         CPN_ALPHABET = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-_'\n-        cpn = ''.join((CPN_ALPHABET[random.randint(0, 256) & 63] for _ in range(0, 16)))\n-\n-        qs.update({\n-            'ver': ['2'],\n-            'cpn': [cpn],\n-        })\n-        playback_url = compat_urlparse.urlunparse(\n-            parsed_playback_url._replace(query=compat_urllib_parse_urlencode(qs, True)))\n+        cpn = ''.join(CPN_ALPHABET[random.randint(0, 256) & 63] for _ in range(0, 16))\n+\n+        # more consistent results setting it to right before the end\n+        qs = parse_qs(playback_url)\n+        video_length = '{0}'.format(float((qs.get('len') or ['1.5'])[0]) - 1)\n+\n+        playback_url = update_url_query(\n+            playback_url, {\n+                'ver': '2',\n+                'cpn': cpn,\n+                'cmt': video_length,\n+                'el': 'detailpage',  # otherwise defaults to \"shorts\"\n+            })\n \n         self._download_webpage(\n             playback_url, video_id, 'Marking watched',\n@@ -1663,6 +1905,7 @@ def _real_extract(self, url):\n             webpage_url + '&bpctr=9999999999&has_verified=1', video_id, fatal=False)\n \n         player_response = None\n+        player_url = None\n         if webpage:\n             player_response = self._extract_yt_initial_variable(\n                 webpage, self._YT_INITIAL_PLAYER_RESPONSE_RE,\n@@ -1671,27 +1914,61 @@ def _real_extract(self, url):\n             player_response = self._call_api(\n                 'player', {'videoId': video_id}, video_id)\n \n-        playability_status = player_response.get('playabilityStatus') or {}\n-        if playability_status.get('reason') == 'Sign in to confirm your age':\n-            video_info = self._download_webpage(\n-                base_url + 'get_video_info', video_id,\n-                'Refetching age-gated info webpage',\n-                'unable to download video info webpage', query={\n-                    'video_id': video_id,\n-                    'eurl': 'https://youtube.googleapis.com/v/' + video_id,\n-                    'html5': 1,\n-                    # See https://github.com/ytdl-org/youtube-dl/issues/29333#issuecomment-864049544\n-                    'c': 'TVHTML5',\n-                    'cver': '6.20180913',\n-                }, fatal=False)\n-            if video_info:\n-                pr = self._parse_json(\n-                    try_get(\n-                        compat_parse_qs(video_info),\n-                        lambda x: x['player_response'][0], compat_str) or '{}',\n-                    video_id, fatal=False)\n-                if pr and isinstance(pr, dict):\n-                    player_response = pr\n+        def is_agegated(playability):\n+            if not isinstance(playability, dict):\n+                return\n+\n+            if playability.get('desktopLegacyAgeGateReason'):\n+                return True\n+\n+            reasons = filter(None, (playability.get(r) for r in ('status', 'reason')))\n+            AGE_GATE_REASONS = (\n+                'confirm your age', 'age-restricted', 'inappropriate',  # reason\n+                'age_verification_required', 'age_check_required',  # status\n+            )\n+            return any(expected in reason for expected in AGE_GATE_REASONS for reason in reasons)\n+\n+        def get_playability_status(response):\n+            return try_get(response, lambda x: x['playabilityStatus'], dict) or {}\n+\n+        playability_status = get_playability_status(player_response)\n+        if (is_agegated(playability_status)\n+                and int_or_none(self._downloader.params.get('age_limit'), default=18) >= 18):\n+\n+            self.report_age_confirmation()\n+\n+            # Thanks: https://github.com/yt-dlp/yt-dlp/pull/3233\n+            pb_context = {'html5Preference': 'HTML5_PREF_WANTS'}\n+\n+            # Use signatureTimestamp if available\n+            # Thanks https://github.com/ytdl-org/youtube-dl/issues/31034#issuecomment-1160718026\n+            player_url = self._extract_player_url(webpage)\n+            ytcfg = self._extract_ytcfg(video_id, webpage)\n+            sts = self._extract_signature_timestamp(video_id, player_url, ytcfg)\n+            if sts:\n+                pb_context['signatureTimestamp'] = sts\n+\n+            query = {\n+                'playbackContext': {'contentPlaybackContext': pb_context},\n+                'contentCheckOk': True,\n+                'racyCheckOk': True,\n+                'context': {\n+                    'client': {'clientName': 'TVHTML5_SIMPLY_EMBEDDED_PLAYER', 'clientVersion': '2.0', 'hl': 'en', 'clientScreen': 'EMBED'},\n+                    'thirdParty': {'embedUrl': 'https://google.com'},\n+                },\n+                'videoId': video_id,\n+            }\n+            headers = {\n+                'X-YouTube-Client-Name': '85',\n+                'X-YouTube-Client-Version': '2.0',\n+                'Origin': 'https://www.youtube.com'\n+            }\n+\n+            video_info = self._call_api('player', query, video_id, fatal=False, headers=headers)\n+            age_gate_status = get_playability_status(video_info)\n+            if age_gate_status.get('status') == 'OK':\n+                player_response = video_info\n+                playability_status = age_gate_status\n \n         trailer_video_id = try_get(\n             playability_status,\n@@ -1770,107 +2047,182 @@ def feed_entry(name):\n             else:\n                 self.to_screen('Downloading just video %s because of --no-playlist' % video_id)\n \n+        if not player_url:\n+            player_url = self._extract_player_url(webpage)\n+\n         formats = []\n-        itags = []\n+        itags = collections.defaultdict(set)\n         itag_qualities = {}\n-        player_url = None\n         q = qualities(['tiny', 'small', 'medium', 'large', 'hd720', 'hd1080', 'hd1440', 'hd2160', 'hd2880', 'highres'])\n+        CHUNK_SIZE = 10 << 20\n+\n         streaming_data = player_response.get('streamingData') or {}\n         streaming_formats = streaming_data.get('formats') or []\n         streaming_formats.extend(streaming_data.get('adaptiveFormats') or [])\n+\n+        def build_fragments(f):\n+            return LazyList({\n+                'url': update_url_query(f['url'], {\n+                    'range': '{0}-{1}'.format(range_start, min(range_start + CHUNK_SIZE - 1, f['filesize']))\n+                })\n+            } for range_start in range(0, f['filesize'], CHUNK_SIZE))\n+\n+        lower = lambda s: s.lower()\n+\n         for fmt in streaming_formats:\n-            if fmt.get('targetDurationSec') or fmt.get('drmFamilies'):\n+            if fmt.get('targetDurationSec'):\n                 continue\n \n             itag = str_or_none(fmt.get('itag'))\n-            quality = fmt.get('quality')\n-            if itag and quality:\n+            audio_track = traverse_obj(fmt, ('audioTrack', T(dict))) or {}\n+\n+            quality = traverse_obj(fmt, ((\n+                # The 3gp format (17) in android client has a quality of \"small\",\n+                # but is actually worse than other formats\n+                T(lambda _: 'tiny' if itag == 17 else None),\n+                ('quality', T(lambda q: q if q and q != 'tiny' else None)),\n+                ('audioQuality', T(lower)),\n+                'quality'), T(txt_or_none)), get_all=False)\n+            if quality and itag:\n                 itag_qualities[itag] = quality\n             # FORMAT_STREAM_TYPE_OTF(otf=1) requires downloading the init fragment\n             # (adding `&sq=0` to the URL) and parsing emsg box to determine the\n-            # number of fragment that would subsequently requested with (`&sq=N`)\n+            # number of fragments that would subsequently be requested with (`&sq=N`)\n             if fmt.get('type') == 'FORMAT_STREAM_TYPE_OTF':\n                 continue\n \n             fmt_url = fmt.get('url')\n             if not fmt_url:\n                 sc = compat_parse_qs(fmt.get('signatureCipher'))\n-                fmt_url = url_or_none(try_get(sc, lambda x: x['url'][0]))\n-                encrypted_sig = try_get(sc, lambda x: x['s'][0])\n-                if not (sc and fmt_url and encrypted_sig):\n+                fmt_url = traverse_obj(sc, ('url', -1, T(url_or_none)))\n+                encrypted_sig = traverse_obj(sc, ('s', -1))\n+                if not (fmt_url and encrypted_sig):\n                     continue\n-                if not player_url:\n-                    player_url = self._extract_player_url(webpage)\n+                player_url = player_url or self._extract_player_url(webpage)\n                 if not player_url:\n                     continue\n-                signature = self._decrypt_signature(sc['s'][0], video_id, player_url)\n-                sp = try_get(sc, lambda x: x['sp'][0]) or 'signature'\n-                fmt_url += '&' + sp + '=' + signature\n+                try:\n+                    fmt_url = update_url_query(fmt_url, {\n+                        traverse_obj(sc, ('sp', -1)) or 'signature':\n+                            [self._decrypt_signature(encrypted_sig, video_id, player_url)],\n+                    })\n+                except ExtractorError as e:\n+                    self.report_warning('Signature extraction failed: Some formats may be missing',\n+                                        video_id=video_id, only_once=True)\n+                    self.write_debug(error_to_compat_str(e), only_once=True)\n+                    continue\n \n-            if itag:\n-                itags.append(itag)\n-            tbr = float_or_none(\n-                fmt.get('averageBitrate') or fmt.get('bitrate'), 1000)\n+            language_preference = (\n+                10 if audio_track.get('audioIsDefault')\n+                else -10 if 'descriptive' in (traverse_obj(audio_track, ('displayName', T(lower))) or '')\n+                else -1)\n+            name = (\n+                traverse_obj(fmt, ('qualityLabel', T(txt_or_none)))\n+                or quality.replace('audio_quality_', ''))\n             dct = {\n-                'asr': int_or_none(fmt.get('audioSampleRate')),\n-                'filesize': int_or_none(fmt.get('contentLength')),\n-                'format_id': itag,\n-                'format_note': fmt.get('qualityLabel') or quality,\n-                'fps': int_or_none(fmt.get('fps')),\n-                'height': int_or_none(fmt.get('height')),\n-                'quality': q(quality),\n-                'tbr': tbr,\n+                'format_id': join_nonempty(itag, fmt.get('isDrc') and 'drc'),\n                 'url': fmt_url,\n-                'width': fmt.get('width'),\n+                # Format 22 is likely to be damaged: see https://github.com/yt-dlp/yt-dlp/issues/3372\n+                'source_preference': ((-5 if itag == '22' else -1)\n+                                      + (100 if 'Premium' in name else 0)),\n+                'quality': q(quality),\n+                'language': join_nonempty(audio_track.get('id', '').split('.')[0],\n+                                          'desc' if language_preference < -1 else '') or None,\n+                'language_preference': language_preference,\n+                # Strictly de-prioritize 3gp formats\n+                'preference': -2 if itag == '17' else None,\n             }\n-            mimetype = fmt.get('mimeType')\n-            if mimetype:\n-                mobj = re.match(\n-                    r'((?:[^/]+)/(?:[^;]+))(?:;\\s*codecs=\"([^\"]+)\")?', mimetype)\n-                if mobj:\n-                    dct['ext'] = mimetype2ext(mobj.group(1))\n-                    dct.update(parse_codecs(mobj.group(2)))\n-            no_audio = dct.get('acodec') == 'none'\n-            no_video = dct.get('vcodec') == 'none'\n-            if no_audio:\n-                dct['vbr'] = tbr\n-            if no_video:\n-                dct['abr'] = tbr\n-            if no_audio or no_video:\n-                dct['downloader_options'] = {\n-                    # Youtube throttles chunks >~10M\n-                    'http_chunk_size': 10485760,\n-                }\n-                if dct.get('ext'):\n-                    dct['container'] = dct['ext'] + '_dash'\n+            if itag:\n+                itags[itag].add(('https', dct.get('language')))\n+            self._unthrottle_format_urls(video_id, player_url, dct)\n+            dct.update(traverse_obj(fmt, {\n+                'asr': ('audioSampleRate', T(int_or_none)),\n+                'filesize': ('contentLength', T(int_or_none)),\n+                'format_note': ('qualityLabel', T(lambda x: x or quality)),\n+                # for some formats, fps is wrongly returned as 1\n+                'fps': ('fps', T(int_or_none), T(lambda f: f if f > 1 else None)),\n+                'audio_channels': ('audioChannels', T(int_or_none)),\n+                'height': ('height', T(int_or_none)),\n+                'has_drm': ('drmFamilies', T(bool)),\n+                'tbr': (('averageBitrate', 'bitrate'), T(lambda t: float_or_none(t, 1000))),\n+                'width': ('width', T(int_or_none)),\n+                '_duration_ms': ('approxDurationMs', T(int_or_none)),\n+            }, get_all=False))\n+            mime_mobj = re.match(\n+                r'((?:[^/]+)/(?:[^;]+))(?:;\\s*codecs=\"([^\"]+)\")?', fmt.get('mimeType') or '')\n+            if mime_mobj:\n+                dct['ext'] = mimetype2ext(mime_mobj.group(1))\n+                dct.update(parse_codecs(mime_mobj.group(2)))\n+            single_stream = 'none' in (dct.get(c) for c in ('acodec', 'vcodec'))\n+            if single_stream and dct.get('ext'):\n+                dct['container'] = dct['ext'] + '_dash'\n+            if single_stream or itag == '17':\n+                # avoid Youtube throttling\n+                dct.update({\n+                    'protocol': 'http_dash_segments',\n+                    'fragments': build_fragments(dct),\n+                } if dct['filesize'] else {\n+                    'downloader_options': {'http_chunk_size': CHUNK_SIZE}  # No longer useful?\n+                })\n+\n             formats.append(dct)\n \n+        def process_manifest_format(f, proto, client_name, itag, all_formats=False):\n+            key = (proto, f.get('language'))\n+            if not all_formats and key in itags[itag]:\n+                return False\n+            itags[itag].add(key)\n+\n+            if itag:\n+                f['format_id'] = (\n+                    '{0}-{1}'.format(itag, proto)\n+                    if all_formats or any(p != proto for p, _ in itags[itag])\n+                    else itag)\n+\n+            if f.get('source_preference') is None:\n+                f['source_preference'] = -1\n+\n+            if itag in ('616', '235'):\n+                f['format_note'] = join_nonempty(f.get('format_note'), 'Premium', delim=' ')\n+                f['source_preference'] += 100\n+\n+            f['quality'] = q(traverse_obj(f, (\n+                'format_id', T(lambda s: itag_qualities[s.split('-')[0]])), default=-1))\n+            if try_call(lambda: f['fps'] <= 1):\n+                del f['fps']\n+\n+            if proto == 'hls' and f.get('has_drm'):\n+                f['has_drm'] = 'maybe'\n+                f['source_preference'] -= 5\n+            return True\n+\n         hls_manifest_url = streaming_data.get('hlsManifestUrl')\n         if hls_manifest_url:\n             for f in self._extract_m3u8_formats(\n                     hls_manifest_url, video_id, 'mp4', fatal=False):\n-                itag = self._search_regex(\n-                    r'/itag/(\\d+)', f['url'], 'itag', default=None)\n-                if itag:\n-                    f['format_id'] = itag\n-                formats.append(f)\n+                if process_manifest_format(\n+                        f, 'hls', None, self._search_regex(\n+                            r'/itag/(\\d+)', f['url'], 'itag', default=None)):\n+                    formats.append(f)\n \n         if self._downloader.params.get('youtube_include_dash_manifest', True):\n             dash_manifest_url = streaming_data.get('dashManifestUrl')\n             if dash_manifest_url:\n                 for f in self._extract_mpd_formats(\n                         dash_manifest_url, video_id, fatal=False):\n-                    itag = f['format_id']\n-                    if itag in itags:\n-                        continue\n-                    if itag in itag_qualities:\n-                        f['quality'] = q(itag_qualities[itag])\n-                    filesize = int_or_none(self._search_regex(\n-                        r'/clen/(\\d+)', f.get('fragment_base_url')\n-                        or f['url'], 'file size', default=None))\n-                    if filesize:\n-                        f['filesize'] = filesize\n-                    formats.append(f)\n+                    if process_manifest_format(\n+                            f, 'dash', None, f['format_id']):\n+                        f['filesize'] = traverse_obj(f, (\n+                            ('fragment_base_url', 'url'), T(lambda u: self._search_regex(\n+                                r'/clen/(\\d+)', u, 'file size', default=None)),\n+                            T(int_or_none)), get_all=False)\n+                        formats.append(f)\n+\n+        playable_formats = [f for f in formats if not f.get('has_drm')]\n+        if formats and not playable_formats:\n+            # If there are no formats that definitely don't have DRM, all have DRM\n+            self.report_drm(video_id)\n+        formats[:] = playable_formats\n \n         if not formats:\n             if streaming_data.get('licenseInfos'):\n@@ -1917,15 +2269,15 @@ def feed_entry(name):\n \n         thumbnails = []\n         for container in (video_details, microformat):\n-            for thumbnail in (try_get(\n+            for thumbnail in try_get(\n                     container,\n-                    lambda x: x['thumbnail']['thumbnails'], list) or []):\n-                thumbnail_url = thumbnail.get('url')\n+                    lambda x: x['thumbnail']['thumbnails'], list) or []:\n+                thumbnail_url = url_or_none(thumbnail.get('url'))\n                 if not thumbnail_url:\n                     continue\n                 thumbnails.append({\n                     'height': int_or_none(thumbnail.get('height')),\n-                    'url': thumbnail_url,\n+                    'url': update_url(thumbnail_url, query=None, fragment=None),\n                     'width': int_or_none(thumbnail.get('width')),\n                 })\n             if thumbnails:\n@@ -1936,19 +2288,30 @@ def feed_entry(name):\n                 thumbnails = [{'url': thumbnail}]\n \n         category = microformat.get('category') or search_meta('genre')\n-        channel_id = video_details.get('channelId') \\\n-            or microformat.get('externalChannelId') \\\n-            or search_meta('channelId')\n+        channel_id = self._extract_channel_id(\n+            webpage, videodetails=video_details, metadata=microformat)\n         duration = int_or_none(\n             video_details.get('lengthSeconds')\n             or microformat.get('lengthSeconds')) \\\n             or parse_duration(search_meta('duration'))\n+\n+        for f in formats:\n+            # Some formats may have much smaller duration than others (possibly damaged during encoding)\n+            # but avoid false positives with small duration differences.\n+            # Ref: https://github.com/yt-dlp/yt-dlp/issues/2823\n+            if try_call(lambda x: float(x.pop('_duration_ms')) / duration < 500, args=(f,)):\n+                self.report_warning(\n+                    '{0}: Some possibly damaged formats will be deprioritized'.format(video_id), only_once=True)\n+                # Strictly de-prioritize damaged formats\n+                f['preference'] = -10\n+\n         is_live = video_details.get('isLive')\n-        owner_profile_url = microformat.get('ownerProfileUrl')\n \n-        if not player_url:\n-            player_url = self._extract_player_url(webpage)\n-        self._unthrottle_format_urls(video_id, player_url, formats)\n+        owner_profile_url = self._yt_urljoin(self._extract_author_var(\n+            webpage, 'url', videodetails=video_details, metadata=microformat))\n+\n+        uploader = self._extract_author_var(\n+            webpage, 'name', videodetails=video_details, metadata=microformat)\n \n         info = {\n             'id': video_id,\n@@ -1959,11 +2322,8 @@ def feed_entry(name):\n             'upload_date': unified_strdate(\n                 microformat.get('uploadDate')\n                 or search_meta('uploadDate')),\n-            'uploader': video_details['author'],\n-            'uploader_id': self._search_regex(r'/(?:channel|user)/([^/?&#]+)', owner_profile_url, 'uploader id') if owner_profile_url else None,\n-            'uploader_url': owner_profile_url,\n+            'uploader': uploader,\n             'channel_id': channel_id,\n-            'channel_url': 'https://www.youtube.com/channel/' + channel_id if channel_id else None,\n             'duration': duration,\n             'view_count': int_or_none(\n                 video_details.get('viewCount')\n@@ -2029,6 +2389,7 @@ def process_language(container, base_url, lang_code, query):\n                         info[d_k] = parse_duration(query[k][0])\n \n         if video_description:\n+            # Youtube Music Auto-generated description\n             mobj = re.search(r'(?s)(?P<track>[^\u00b7\\n]+)\u00b7(?P<artist>[^\\n]+)\\n+(?P<album>[^\\n]+)(?:.+?\u2117\\s*(?P<release_year>\\d{4})(?!\\d))?(?:.+?Released on\\s*:\\s*(?P<release_date>\\d{4}-\\d{2}-\\d{2}))?(.+?\\nArtist\\s*:\\s*(?P<clean_artist>[^\\n]+))?.+\\nAuto-generated by YouTube\\.\\s*$', video_description)\n             if mobj:\n                 release_year = mobj.group('release_year')\n@@ -2092,6 +2453,13 @@ def chapter_time(mmlir):\n                 initial_data,\n                 lambda x: x['contents']['twoColumnWatchNextResults']['results']['results']['contents'],\n                 list) or []\n+            if not info['channel_id']:\n+                channel_id = self._extract_channel_id('', renderers=contents)\n+            if not info['uploader']:\n+                info['uploader'] = self._extract_author_var('', 'name', renderers=contents)\n+            if not owner_profile_url:\n+                owner_profile_url = self._yt_urljoin(self._extract_author_var('', 'url', renderers=contents))\n+\n             for content in contents:\n                 vpir = content.get('videoPrimaryInfoRenderer')\n                 if vpir:\n@@ -2103,7 +2471,8 @@ def chapter_time(mmlir):\n                                 lambda x: x['superTitleIcon']['iconType']) == 'LOCATION_PIN':\n                             info['location'] = stl\n                         else:\n-                            mobj = re.search(r'(.+?)\\s*S(\\d+)\\s*\u2022\\s*E(\\d+)', stl)\n+                            # \u2022? doesn't match, but [\u2022]? does; \\xa0 = non-breaking space\n+                            mobj = re.search(r'([^\\xa0\\s].*?)[\\xa0\\s]*S(\\d+)[\\xa0\\s]*[\u2022]?[\\xa0\\s]*E(\\d+)', stl)\n                             if mobj:\n                                 info.update({\n                                     'series': mobj.group(1),\n@@ -2114,7 +2483,7 @@ def chapter_time(mmlir):\n                             vpir,\n                             lambda x: x['videoActions']['menuRenderer']['topLevelButtons'],\n                             list) or []):\n-                        tbr = tlb.get('toggleButtonRenderer') or {}\n+                        tbr = traverse_obj(tlb, ('segmentedLikeDislikeButtonRenderer', 'likeButton', 'toggleButtonRenderer'), 'toggleButtonRenderer') or {}\n                         for getter, regex in [(\n                                 lambda x: x['defaultText']['accessibility']['accessibilityData'],\n                                 r'(?P<count>[\\d,]+)\\s*(?P<type>(?:dis)?like)'), ([\n@@ -2130,17 +2499,22 @@ def chapter_time(mmlir):\n                     sbr_tooltip = try_get(\n                         vpir, lambda x: x['sentimentBar']['sentimentBarRenderer']['tooltip'])\n                     if sbr_tooltip:\n+                        # however dislike_count was hidden by YT, as if there could ever be dislikable content on YT\n                         like_count, dislike_count = sbr_tooltip.split(' / ')\n                         info.update({\n                             'like_count': str_to_int(like_count),\n                             'dislike_count': str_to_int(dislike_count),\n                         })\n+                    else:\n+                        info['like_count'] = traverse_obj(vpir, (\n+                            'videoActions', 'menuRenderer', 'topLevelButtons', Ellipsis,\n+                            'segmentedLikeDislikeButtonViewModel', 'likeButtonViewModel', 'likeButtonViewModel',\n+                            'toggleButtonViewModel', 'toggleButtonViewModel', 'defaultButtonViewModel',\n+                            'buttonViewModel', (('title', ('accessibilityText', T(lambda s: s.split()), Ellipsis))), T(parse_count)),\n+                            get_all=False)\n+\n                 vsir = content.get('videoSecondaryInfoRenderer')\n                 if vsir:\n-                    info['channel'] = get_text(try_get(\n-                        vsir,\n-                        lambda x: x['owner']['videoOwnerRenderer']['title'],\n-                        dict))\n                     rows = try_get(\n                         vsir,\n                         lambda x: x['metadataRowContainer']['metadataRowContainerRenderer']['rows'],\n@@ -2167,6 +2541,30 @@ def chapter_time(mmlir):\n                             elif mrr_title == 'Song':\n                                 info['track'] = mrr_contents_text\n \n+            # this is not extraction but spelunking!\n+            carousel_lockups = traverse_obj(\n+                initial_data,\n+                ('engagementPanels', Ellipsis, 'engagementPanelSectionListRenderer',\n+                 'content', 'structuredDescriptionContentRenderer', 'items', Ellipsis,\n+                 'videoDescriptionMusicSectionRenderer', 'carouselLockups', Ellipsis),\n+                expected_type=dict) or []\n+            # try to reproduce logic from metadataRowContainerRenderer above (if it still is)\n+            fields = (('ALBUM', 'album'), ('ARTIST', 'artist'), ('SONG', 'track'), ('LICENSES', 'license'))\n+            # multiple_songs ?\n+            if len(carousel_lockups) > 1:\n+                fields = fields[-1:]\n+            for info_row in traverse_obj(\n+                    carousel_lockups,\n+                    (0, 'carouselLockupRenderer', 'infoRows', Ellipsis, 'infoRowRenderer'),\n+                    expected_type=dict):\n+                row_title = traverse_obj(info_row, ('title', 'simpleText'))\n+                row_text = traverse_obj(info_row, 'defaultMetadata', 'expandedMetadata', expected_type=get_text)\n+                if not row_text:\n+                    continue\n+                for name, field in fields:\n+                    if name == row_title and not info.get(field):\n+                        info[field] = row_text\n+\n         for s_k, d_k in [('artist', 'creator'), ('track', 'alt_title')]:\n             v = info.get(s_k)\n             if v:\n@@ -2174,7 +2572,14 @@ def chapter_time(mmlir):\n \n         self.mark_watched(video_id, player_response)\n \n-        return info\n+        return merge_dicts(\n+            info, {\n+                'uploader_id': self._extract_uploader_id(owner_profile_url),\n+                'uploader_url': owner_profile_url,\n+                'channel_id': channel_id,\n+                'channel_url': channel_id and self._yt_urljoin('/channel/' + channel_id),\n+                'channel': info['uploader'],\n+            })\n \n \n class YoutubeTabIE(YoutubeBaseInfoExtractor):\n@@ -2196,13 +2601,36 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n     IE_NAME = 'youtube:tab'\n \n     _TESTS = [{\n+        # Shorts\n+        'url': 'https://www.youtube.com/@SuperCooperShorts/shorts',\n+        'playlist_mincount': 5,\n+        'info_dict': {\n+            'description': 'Short clips from Super Cooper Sundays!',\n+            'id': 'UCKMA8kHZ8bPYpnMNaUSxfEQ',\n+            'title': 'Super Cooper Shorts - Shorts',\n+            'uploader': 'Super Cooper Shorts',\n+            'uploader_id': '@SuperCooperShorts',\n+        }\n+    }, {\n+        # Channel that does not have a Shorts tab. Test should just download videos on Home tab instead\n+        'url': 'https://www.youtube.com/@emergencyawesome/shorts',\n+        'info_dict': {\n+            'description': 'md5:592c080c06fef4de3c902c4a8eecd850',\n+            'id': 'UCDiFRMQWpcp8_KD4vwIVicw',\n+            'title': 'Emergency Awesome - Home',\n+        },\n+        'playlist_mincount': 5,\n+        'skip': 'new test page needed to replace `Emergency Awesome - Shorts`',\n+    }, {\n         # playlists, multipage\n         'url': 'https://www.youtube.com/c/\u0418\u0433\u043e\u0440\u044c\u041a\u043b\u0435\u0439\u043d\u0435\u0440/playlists?view=1&flow=grid',\n         'playlist_mincount': 94,\n         'info_dict': {\n             'id': 'UCqj7Cz7revf5maW9g5pgNcg',\n-            'title': '\u0418\u0433\u043e\u0440\u044c \u041a\u043b\u0435\u0439\u043d\u0435\u0440 - Playlists',\n+            'title': r're:Igor Kleiner(?: Ph\\.D\\.)? - Playlists',\n             'description': 'md5:be97ee0f14ee314f1f002cf187166ee2',\n+            'uploader': 'Igor Kleiner',\n+            'uploader_id': '@IgorDataScience',\n         },\n     }, {\n         # playlists, multipage, different order\n@@ -2210,8 +2638,10 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n         'playlist_mincount': 94,\n         'info_dict': {\n             'id': 'UCqj7Cz7revf5maW9g5pgNcg',\n-            'title': '\u0418\u0433\u043e\u0440\u044c \u041a\u043b\u0435\u0439\u043d\u0435\u0440 - Playlists',\n+            'title': r're:Igor Kleiner(?: Ph\\.D\\.)? - Playlists',\n             'description': 'md5:be97ee0f14ee314f1f002cf187166ee2',\n+            'uploader': 'Igor Kleiner',\n+            'uploader_id': '@IgorDataScience',\n         },\n     }, {\n         # playlists, series\n@@ -2221,6 +2651,8 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n             'id': 'UCYO_jab_esuFRV4b17AJtAw',\n             'title': '3Blue1Brown - Playlists',\n             'description': 'md5:e1384e8a133307dd10edee76e875d62f',\n+            'uploader': '3Blue1Brown',\n+            'uploader_id': '@3blue1brown',\n         },\n     }, {\n         # playlists, singlepage\n@@ -2230,6 +2662,8 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n             'id': 'UCAEtajcuhQ6an9WEzY9LEMQ',\n             'title': 'ThirstForScience - Playlists',\n             'description': 'md5:609399d937ea957b0f53cbffb747a14c',\n+            'uploader': 'ThirstForScience',\n+            'uploader_id': '@ThirstForScience',\n         }\n     }, {\n         'url': 'https://www.youtube.com/c/ChristophLaimer/playlists',\n@@ -2238,20 +2672,22 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n         # basic, single video playlist\n         'url': 'https://www.youtube.com/playlist?list=PL4lCao7KL_QFVb7Iudeipvc2BCavECqzc',\n         'info_dict': {\n-            'uploader_id': 'UCmlqkdCBesrv2Lak1mF_MxA',\n-            'uploader': 'Sergey M.',\n             'id': 'PL4lCao7KL_QFVb7Iudeipvc2BCavECqzc',\n             'title': 'youtube-dl public playlist',\n+            'uploader': 'Sergey M.',\n+            'uploader_id': '@sergeym.6173',\n+            'channel_id': 'UCmlqkdCBesrv2Lak1mF_MxA',\n         },\n         'playlist_count': 1,\n     }, {\n         # empty playlist\n         'url': 'https://www.youtube.com/playlist?list=PL4lCao7KL_QFodcLWhDpGCYnngnHtQ-Xf',\n         'info_dict': {\n-            'uploader_id': 'UCmlqkdCBesrv2Lak1mF_MxA',\n-            'uploader': 'Sergey M.',\n             'id': 'PL4lCao7KL_QFodcLWhDpGCYnngnHtQ-Xf',\n             'title': 'youtube-dl empty playlist',\n+            'uploader': 'Sergey M.',\n+            'uploader_id': '@sergeym.6173',\n+            'channel_id': 'UCmlqkdCBesrv2Lak1mF_MxA',\n         },\n         'playlist_count': 0,\n     }, {\n@@ -2261,6 +2697,8 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n             'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n             'title': 'lex will - Home',\n             'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n         },\n         'playlist_mincount': 2,\n     }, {\n@@ -2270,6 +2708,8 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n             'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n             'title': 'lex will - Videos',\n             'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n         },\n         'playlist_mincount': 975,\n     }, {\n@@ -2279,6 +2719,8 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n             'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n             'title': 'lex will - Videos',\n             'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n         },\n         'playlist_mincount': 199,\n     }, {\n@@ -2288,6 +2730,8 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n             'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n             'title': 'lex will - Playlists',\n             'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n         },\n         'playlist_mincount': 17,\n     }, {\n@@ -2297,6 +2741,8 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n             'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n             'title': 'lex will - Community',\n             'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n         },\n         'playlist_mincount': 18,\n     }, {\n@@ -2304,10 +2750,23 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n         'url': 'https://www.youtube.com/channel/UCKfVa3S1e4PHvxWcwyMMg8w/channels',\n         'info_dict': {\n             'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n-            'title': 'lex will - Channels',\n+            'title': r're:lex will - (?:Home|Channels)',\n             'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n+        },\n+        'playlist_mincount': 75,\n+    }, {\n+        # Releases tab\n+        'url': 'https://www.youtube.com/@daftpunk/releases',\n+        'info_dict': {\n+            'id': 'UC_kRDKYrUlrbtrSiyu5Tflg',\n+            'title': 'Daft Punk - Releases',\n+            'description': 'Daft Punk (1993 - 2021) - Official YouTube Channel',\n+            'uploader_id': '@daftpunk',\n+            'uploader': 'Daft Punk',\n         },\n-        'playlist_mincount': 138,\n+        'playlist_mincount': 36,\n     }, {\n         'url': 'https://invidio.us/channel/UCmlqkdCBesrv2Lak1mF_MxA',\n         'only_matching': True,\n@@ -2324,7 +2783,8 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n             'title': '29C3: Not my department',\n             'id': 'PLwP_SiAcdui0KVebT0mU9Apz359a4ubsC',\n             'uploader': 'Christiaan008',\n-            'uploader_id': 'UCEPzS1rYsrkqzSLNp76nrcg',\n+            'uploader_id': '@ChRiStIaAn008',\n+            'channel_id': 'UCEPzS1rYsrkqzSLNp76nrcg',\n         },\n         'playlist_count': 96,\n     }, {\n@@ -2334,7 +2794,8 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n             'title': 'Uploads from Cauchemar',\n             'id': 'UUBABnxM4Ar9ten8Mdjj1j0Q',\n             'uploader': 'Cauchemar',\n-            'uploader_id': 'UCBABnxM4Ar9ten8Mdjj1j0Q',\n+            'uploader_id': '@Cauchemar89',\n+            'channel_id': 'UCBABnxM4Ar9ten8Mdjj1j0Q',\n         },\n         'playlist_mincount': 1123,\n     }, {\n@@ -2348,7 +2809,8 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n             'title': 'Uploads from Interstellar Movie',\n             'id': 'UUXw-G3eDE9trcvY2sBMM_aA',\n             'uploader': 'Interstellar Movie',\n-            'uploader_id': 'UCXw-G3eDE9trcvY2sBMM_aA',\n+            'uploader_id': '@InterstellarMovie',\n+            'channel_id': 'UCXw-G3eDE9trcvY2sBMM_aA',\n         },\n         'playlist_mincount': 21,\n     }, {\n@@ -2357,8 +2819,9 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n         'info_dict': {\n             'title': 'Data Analysis with Dr Mike Pound',\n             'id': 'PLzH6n4zXuckpfMu_4Ff8E7Z1behQks5ba',\n-            'uploader_id': 'UC9-y-6csu5WGm29I7JiwpnA',\n             'uploader': 'Computerphile',\n+            'uploader_id': '@Computerphile',\n+            'channel_id': 'UC9-y-6csu5WGm29I7JiwpnA',\n         },\n         'playlist_mincount': 11,\n     }, {\n@@ -2381,7 +2844,6 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n             'tags': list,\n             'view_count': int,\n             'like_count': int,\n-            'dislike_count': int,\n         },\n         'params': {\n             'skip_download': True,\n@@ -2397,18 +2859,17 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n     }, {\n         'url': 'https://www.youtube.com/channel/UCoMdktPbSTixAyNGwb-UYkQ/live',\n         'info_dict': {\n-            'id': '9Auq9mYxFEE',\n+            'id': r're:[\\da-zA-Z_-]{8,}',\n             'ext': 'mp4',\n-            'title': 'Watch Sky News live',\n+            'title': r're:(?s)[A-Z].{20,}',\n             'uploader': 'Sky News',\n-            'uploader_id': 'skynews',\n-            'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/user/skynews',\n-            'upload_date': '20191102',\n-            'description': 'md5:78de4e1c2359d0ea3ed829678e38b662',\n+            'uploader_id': '@SkyNews',\n+            'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@SkyNews',\n+            'upload_date': r're:\\d{8}',\n+            'description': r're:(?s)(?:.*\\n)+SUBSCRIBE to our YouTube channel for more videos: http://www\\.youtube\\.com/skynews *\\n.*',\n             'categories': ['News & Politics'],\n             'tags': list,\n             'like_count': int,\n-            'dislike_count': int,\n         },\n         'params': {\n             'skip_download': True,\n@@ -2428,7 +2889,6 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n             'categories': ['News & Politics'],\n             'tags': ['Cenk Uygur (TV Program Creator)', 'The Young Turks (Award-Winning Work)', 'Talk Show (TV Genre)'],\n             'like_count': int,\n-            'dislike_count': int,\n         },\n         'params': {\n             'skip_download': True,\n@@ -2495,34 +2955,22 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n     }, {\n         'note': 'Search tab',\n         'url': 'https://www.youtube.com/c/3blue1brown/search?query=linear%20algebra',\n-        'playlist_mincount': 40,\n+        'playlist_mincount': 20,\n         'info_dict': {\n             'id': 'UCYO_jab_esuFRV4b17AJtAw',\n             'title': '3Blue1Brown - Search - linear algebra',\n             'description': 'md5:e1384e8a133307dd10edee76e875d62f',\n             'uploader': '3Blue1Brown',\n-            'uploader_id': 'UCYO_jab_esuFRV4b17AJtAw',\n+            'uploader_id': '@3blue1brown',\n+            'channel_id': 'UCYO_jab_esuFRV4b17AJtAw',\n         }\n     }]\n \n     @classmethod\n     def suitable(cls, url):\n-        return False if YoutubeIE.suitable(url) else super(\n+        return not YoutubeIE.suitable(url) and super(\n             YoutubeTabIE, cls).suitable(url)\n \n-    def _extract_channel_id(self, webpage):\n-        channel_id = self._html_search_meta(\n-            'channelId', webpage, 'channel id', default=None)\n-        if channel_id:\n-            return channel_id\n-        channel_url = self._html_search_meta(\n-            ('og:url', 'al:ios:url', 'al:android:url', 'al:web:url',\n-             'twitter:url', 'twitter:app:url:iphone', 'twitter:app:url:ipad',\n-             'twitter:app:url:googleplay'), webpage, 'channel url')\n-        return self._search_regex(\n-            r'https?://(?:www\\.)?youtube\\.com/channel/([^/?#&])+',\n-            channel_url, 'channel id')\n-\n     @staticmethod\n     def _extract_grid_item_renderer(item):\n         assert isinstance(item, dict)\n@@ -2533,6 +2981,12 @@ def _extract_grid_item_renderer(item):\n                 continue\n             return renderer\n \n+    @staticmethod\n+    def _get_text(r, k):\n+        return traverse_obj(\n+            r, (k, 'runs', 0, 'text'), (k, 'simpleText'),\n+            expected_type=txt_or_none)\n+\n     def _grid_entries(self, grid_renderer):\n         for item in grid_renderer['items']:\n             if not isinstance(item, dict):\n@@ -2540,9 +2994,7 @@ def _grid_entries(self, grid_renderer):\n             renderer = self._extract_grid_item_renderer(item)\n             if not isinstance(renderer, dict):\n                 continue\n-            title = try_get(\n-                renderer, (lambda x: x['title']['runs'][0]['text'],\n-                           lambda x: x['title']['simpleText']), compat_str)\n+            title = self._get_text(renderer, 'title')\n             # playlist\n             playlist_id = renderer.get('playlistId')\n             if playlist_id:\n@@ -2559,8 +3011,7 @@ def _grid_entries(self, grid_renderer):\n             # channel\n             channel_id = renderer.get('channelId')\n             if channel_id:\n-                title = try_get(\n-                    renderer, lambda x: x['title']['simpleText'], compat_str)\n+                title = self._get_text(renderer, 'title')\n                 yield self.url_result(\n                     'https://www.youtube.com/channel/%s' % channel_id,\n                     ie=YoutubeTabIE.ie_key(), video_title=title)\n@@ -2669,11 +3120,26 @@ def _post_thread_continuation_entries(self, post_thread_continuation):\n \n     def _rich_grid_entries(self, contents):\n         for content in contents:\n-            video_renderer = try_get(content, lambda x: x['richItemRenderer']['content']['videoRenderer'], dict)\n+            content = traverse_obj(\n+                content, ('richItemRenderer', 'content'),\n+                expected_type=dict) or {}\n+            video_renderer = traverse_obj(\n+                content, 'videoRenderer', 'reelItemRenderer',\n+                expected_type=dict)\n             if video_renderer:\n                 entry = self._video_entry(video_renderer)\n                 if entry:\n                     yield entry\n+            # playlist\n+            renderer = traverse_obj(\n+                content, 'playlistRenderer', expected_type=dict) or {}\n+            title = self._get_text(renderer, 'title')\n+            playlist_id = renderer.get('playlistId')\n+            if playlist_id:\n+                yield self.url_result(\n+                    'https://www.youtube.com/playlist?list=%s' % playlist_id,\n+                    ie=YoutubeTabIE.ie_key(), video_id=playlist_id,\n+                    video_title=title)\n \n     @staticmethod\n     def _build_continuation_query(continuation, ctp=None):\n@@ -2778,6 +3244,7 @@ def _entries(self, tab, item_id, webpage):\n                 return\n             for entry in self._rich_grid_entries(rich_grid_renderer.get('contents') or []):\n                 yield entry\n+\n             continuation = self._extract_continuation(rich_grid_renderer)\n \n         ytcfg = self._extract_ytcfg(item_id, webpage)\n@@ -2906,90 +3373,72 @@ def _extract_selected_tab(tabs):\n         else:\n             raise ExtractorError('Unable to find selected tab')\n \n-    @staticmethod\n-    def _extract_uploader(data):\n+    def _extract_uploader(self, metadata, data):\n         uploader = {}\n-        sidebar_renderer = try_get(\n-            data, lambda x: x['sidebar']['playlistSidebarRenderer']['items'], list)\n-        if sidebar_renderer:\n-            for item in sidebar_renderer:\n-                if not isinstance(item, dict):\n-                    continue\n-                renderer = item.get('playlistSidebarSecondaryInfoRenderer')\n-                if not isinstance(renderer, dict):\n-                    continue\n-                owner = try_get(\n-                    renderer, lambda x: x['videoOwner']['videoOwnerRenderer']['title']['runs'][0], dict)\n-                if owner:\n-                    uploader['uploader'] = owner.get('text')\n-                    uploader['uploader_id'] = try_get(\n-                        owner, lambda x: x['navigationEndpoint']['browseEndpoint']['browseId'], compat_str)\n-                    uploader['uploader_url'] = urljoin(\n-                        'https://www.youtube.com/',\n-                        try_get(owner, lambda x: x['navigationEndpoint']['browseEndpoint']['canonicalBaseUrl'], compat_str))\n+        renderers = traverse_obj(data,\n+                                 ('sidebar', 'playlistSidebarRenderer', 'items'))\n+        uploader['channel_id'] = self._extract_channel_id('', metadata=metadata, renderers=renderers)\n+        uploader['uploader'] = (\n+            self._extract_author_var('', 'name', renderers=renderers)\n+            or self._extract_author_var('', 'name', metadata=metadata))\n+        uploader['uploader_url'] = self._yt_urljoin(\n+            self._extract_author_var('', 'url', metadata=metadata, renderers=renderers))\n+        uploader['uploader_id'] = self._extract_uploader_id(uploader['uploader_url'])\n+        uploader['channel'] = uploader['uploader']\n         return uploader\n \n-    @staticmethod\n-    def _extract_alert(data):\n+    @classmethod\n+    def _extract_alert(cls, data):\n         alerts = []\n-        for alert in try_get(data, lambda x: x['alerts'], list) or []:\n-            if not isinstance(alert, dict):\n-                continue\n-            alert_text = try_get(\n-                alert, lambda x: x['alertRenderer']['text'], dict)\n+        for alert in traverse_obj(data, ('alerts', Ellipsis), expected_type=dict):\n+            alert_text = traverse_obj(\n+                alert, (None, lambda x: x['alertRenderer']['text']), get_all=False)\n             if not alert_text:\n                 continue\n-            text = try_get(\n-                alert_text,\n-                (lambda x: x['simpleText'], lambda x: x['runs'][0]['text']),\n-                compat_str)\n+            text = cls._get_text(alert_text, 'text')\n             if text:\n                 alerts.append(text)\n         return '\\n'.join(alerts)\n \n     def _extract_from_tabs(self, item_id, webpage, data, tabs):\n         selected_tab = self._extract_selected_tab(tabs)\n-        renderer = try_get(\n-            data, lambda x: x['metadata']['channelMetadataRenderer'], dict)\n+        renderer = traverse_obj(data, ('metadata', 'channelMetadataRenderer'),\n+                                expected_type=dict) or {}\n         playlist_id = item_id\n         title = description = None\n         if renderer:\n-            channel_title = renderer.get('title') or item_id\n-            tab_title = selected_tab.get('title')\n-            title = channel_title or item_id\n-            if tab_title:\n-                title += ' - %s' % tab_title\n-            if selected_tab.get('expandedText'):\n-                title += ' - %s' % selected_tab['expandedText']\n-            description = renderer.get('description')\n-            playlist_id = renderer.get('externalId')\n+            channel_title = txt_or_none(renderer.get('title')) or item_id\n+            tab_title = txt_or_none(selected_tab.get('title'))\n+            title = join_nonempty(\n+                channel_title or item_id, tab_title,\n+                txt_or_none(selected_tab.get('expandedText')),\n+                delim=' - ')\n+            description = txt_or_none(renderer.get('description'))\n+            playlist_id = txt_or_none(renderer.get('externalId')) or playlist_id\n         else:\n-            renderer = try_get(\n-                data, lambda x: x['metadata']['playlistMetadataRenderer'], dict)\n-            if renderer:\n-                title = renderer.get('title')\n-            else:\n-                renderer = try_get(\n-                    data, lambda x: x['header']['hashtagHeaderRenderer'], dict)\n-                if renderer:\n-                    title = try_get(renderer, lambda x: x['hashtag']['simpleText'])\n+            renderer = traverse_obj(data,\n+                                    ('metadata', 'playlistMetadataRenderer'),\n+                                    ('header', 'hashtagHeaderRenderer'),\n+                                    expected_type=dict) or {}\n+            title = traverse_obj(renderer, 'title', ('hashtag', 'simpleText'),\n+                                 expected_type=txt_or_none)\n         playlist = self.playlist_result(\n             self._entries(selected_tab, item_id, webpage),\n             playlist_id=playlist_id, playlist_title=title,\n             playlist_description=description)\n-        playlist.update(self._extract_uploader(data))\n-        return playlist\n+        return merge_dicts(playlist, self._extract_uploader(renderer, data))\n \n     def _extract_from_playlist(self, item_id, url, data, playlist):\n-        title = playlist.get('title') or try_get(\n-            data, lambda x: x['titleText']['simpleText'], compat_str)\n-        playlist_id = playlist.get('playlistId') or item_id\n+        title = traverse_obj((playlist, data),\n+                             (0, 'title'), (1, 'titleText', 'simpleText'),\n+                             expected_type=txt_or_none)\n+        playlist_id = txt_or_none(playlist.get('playlistId')) or item_id\n         # Inline playlist rendition continuation does not always work\n         # at Youtube side, so delegating regular tab-based playlist URL\n         # processing whenever possible.\n-        playlist_url = urljoin(url, try_get(\n-            playlist, lambda x: x['endpoint']['commandMetadata']['webCommandMetadata']['url'],\n-            compat_str))\n+        playlist_url = urljoin(url, traverse_obj(\n+            playlist, ('endpoint', 'commandMetadata', 'webCommandMetadata', 'url'),\n+            expected_type=url_or_none))\n         if playlist_url and playlist_url != url:\n             return self.url_result(\n                 playlist_url, ie=YoutubeTabIE.ie_key(), video_id=playlist_id,\n@@ -3009,8 +3458,7 @@ def _extract_identity_token(self, ytcfg, webpage):\n \n     def _real_extract(self, url):\n         item_id = self._match_id(url)\n-        url = compat_urlparse.urlunparse(\n-            compat_urlparse.urlparse(url)._replace(netloc='www.youtube.com'))\n+        url = update_url(url, netloc='www.youtube.com')\n         # Handle both video/playlist URLs\n         qs = parse_qs(url)\n         video_id = qs.get('v', [None])[0]\n@@ -3066,8 +3514,9 @@ class YoutubePlaylistIE(InfoExtractor):\n         'info_dict': {\n             'title': '[OLD]Team Fortress 2 (Class-based LP)',\n             'id': 'PLBB231211A4F62143',\n-            'uploader': 'Wickydoo',\n-            'uploader_id': 'UCKSpbfbl5kRQpTdL7kMc-1Q',\n+            'uploader': 'Wickman',\n+            'uploader_id': '@WickmanVT',\n+            'channel_id': 'UCKSpbfbl5kRQpTdL7kMc-1Q',\n         },\n         'playlist_mincount': 29,\n     }, {\n@@ -3081,21 +3530,25 @@ class YoutubePlaylistIE(InfoExtractor):\n     }, {\n         'note': 'embedded',\n         'url': 'https://www.youtube.com/embed/videoseries?list=PL6IaIsEjSbf96XFRuNccS_RuEXwNdsoEu',\n-        'playlist_count': 4,\n+        # TODO: full playlist requires _reload_with_unavailable_videos()\n+        # 'playlist_count': 4,\n+        'playlist_mincount': 1,\n         'info_dict': {\n             'title': 'JODA15',\n             'id': 'PL6IaIsEjSbf96XFRuNccS_RuEXwNdsoEu',\n             'uploader': 'milan',\n-            'uploader_id': 'UCEI1-PVPcYXjB73Hfelbmaw',\n+            'uploader_id': '@milan5503',\n+            'channel_id': 'UCEI1-PVPcYXjB73Hfelbmaw',\n         }\n     }, {\n         'url': 'http://www.youtube.com/embed/_xDOZElKyNU?list=PLsyOSbh5bs16vubvKePAQ1x3PhKavfBIl',\n-        'playlist_mincount': 982,\n+        'playlist_mincount': 455,\n         'info_dict': {\n             'title': '2018 Chinese New Singles (11/6 updated)',\n             'id': 'PLsyOSbh5bs16vubvKePAQ1x3PhKavfBIl',\n             'uploader': 'LBK',\n-            'uploader_id': 'UC21nz3_MesPLqtDqwdvnoxA',\n+            'uploader_id': '@music_king',\n+            'channel_id': 'UC21nz3_MesPLqtDqwdvnoxA',\n         }\n     }, {\n         'url': 'TLGGrESM50VT6acwMjAyMjAxNw',\n@@ -3110,11 +3563,7 @@ class YoutubePlaylistIE(InfoExtractor):\n     def suitable(cls, url):\n         if YoutubeTabIE.suitable(url):\n             return False\n-        # Hack for lazy extractors until more generic solution is implemented\n-        # (see #28780)\n-        from .youtube import parse_qs\n-        qs = parse_qs(url)\n-        if qs.get('v', [None])[0]:\n+        if parse_qs(url).get('v', [None])[0]:\n             return False\n         return super(YoutubePlaylistIE, cls).suitable(url)\n \n@@ -3137,14 +3586,13 @@ class YoutubeYtBeIE(InfoExtractor):\n             'ext': 'mp4',\n             'title': 'Small Scale Baler and Braiding Rugs',\n             'uploader': 'Backus-Page House Museum',\n-            'uploader_id': 'backuspagemuseum',\n-            'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/user/backuspagemuseum',\n+            'uploader_id': '@backuspagemuseum',\n+            'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@backuspagemuseum',\n             'upload_date': '20161008',\n             'description': 'md5:800c0c78d5eb128500bffd4f0b4f2e8a',\n             'categories': ['Nonprofits & Activism'],\n             'tags': list,\n             'like_count': int,\n-            'dislike_count': int,\n         },\n         'params': {\n             'noplaylist': True,\n@@ -3254,9 +3702,9 @@ class YoutubeSearchURLIE(YoutubeBaseInfoExtractor):\n     }]\n \n     def _real_extract(self, url):\n-        qs = compat_parse_qs(compat_urllib_parse_urlparse(url).query)\n-        query = (qs.get('search_query') or qs.get('q'))[0]\n-        params = qs.get('sp', ('',))[0]\n+        qs = parse_qs(url)\n+        query = (qs.get('search_query') or qs.get('q'))[-1]\n+        params = qs.get('sp', ('',))[-1]\n         return self.playlist_result(self._search_results(query, params), query, query)\n \n \ndiff --git a/youtube_dl/extractor/zdf.py b/youtube_dl/extractor/zdf.py\nindex 3d39bb33aec..fcc63ef52ca 100644\n--- a/youtube_dl/extractor/zdf.py\n+++ b/youtube_dl/extractor/zdf.py\n@@ -8,13 +8,14 @@\n from ..utils import (\n     determine_ext,\n     ExtractorError,\n+    extract_attributes,\n     float_or_none,\n     int_or_none,\n     merge_dicts,\n     NO_DEFAULT,\n-    orderedSet,\n     parse_codecs,\n     qualities,\n+    str_or_none,\n     try_get,\n     unified_timestamp,\n     update_url_query,\n@@ -57,28 +58,39 @@ def _extract_format(self, video_id, formats, format_urls, meta):\n         format_urls.add(format_url)\n         mime_type = meta.get('mimeType')\n         ext = determine_ext(format_url)\n+\n+        join_nonempty = lambda s, l: s.join(filter(None, l))\n+        meta_map = lambda t: map(lambda x: str_or_none(meta.get(x)), t)\n+\n         if mime_type == 'application/x-mpegURL' or ext == 'm3u8':\n-            formats.extend(self._extract_m3u8_formats(\n+            new_formats = self._extract_m3u8_formats(\n                 format_url, video_id, 'mp4', m3u8_id='hls',\n-                entry_protocol='m3u8_native', fatal=False))\n+                entry_protocol='m3u8_native', fatal=False)\n         elif mime_type == 'application/f4m+xml' or ext == 'f4m':\n-            formats.extend(self._extract_f4m_formats(\n-                update_url_query(format_url, {'hdcore': '3.7.0'}), video_id, f4m_id='hds', fatal=False))\n+            new_formats = self._extract_f4m_formats(\n+                update_url_query(format_url, {'hdcore': '3.7.0'}), video_id, f4m_id='hds', fatal=False)\n         else:\n             f = parse_codecs(meta.get('mimeCodec'))\n+            if not f:\n+                data = meta.get('type', '').split('_')\n+                if try_get(data, lambda x: x[2]) == ext:\n+                    f = dict(zip(('vcodec', 'acodec'), data[1]))\n+\n             format_id = ['http']\n-            for p in (meta.get('type'), meta.get('quality')):\n-                if p and isinstance(p, compat_str):\n-                    format_id.append(p)\n+            format_id.extend(join_nonempty('-', meta_map(('type', 'quality'))))\n             f.update({\n                 'url': format_url,\n                 'format_id': '-'.join(format_id),\n-                'format_note': meta.get('quality'),\n-                'language': meta.get('language'),\n-                'quality': qualities(self._QUALITIES)(meta.get('quality')),\n-                'preference': -10,\n+                'tbr': int_or_none(self._search_regex(r'_(\\d+)k_', format_url, 'tbr', default=None))\n             })\n-            formats.append(f)\n+            new_formats = [f]\n+\n+        formats.extend(merge_dicts(f, {\n+            'format_note': join_nonempty(',', meta_map(('quality', 'class'))),\n+            'language': meta.get('language'),\n+            'language_preference': 10 if meta.get('class') == 'main' else -10 if meta.get('class') == 'ad' else -1,\n+            'quality': qualities(self._QUALITIES)(meta.get('quality')),\n+        }) for f in new_formats)\n \n     def _extract_ptmd(self, ptmd_url, video_id, api_token, referrer):\n         ptmd = self._call_api(\n@@ -107,6 +119,7 @@ def _extract_ptmd(self, ptmd_url, video_id, api_token, referrer):\n                                 'type': f.get('type'),\n                                 'mimeType': f.get('mimeType'),\n                                 'quality': quality.get('quality'),\n+                                'class': track.get('class'),\n                                 'language': track.get('language'),\n                             })\n         self._sort_formats(formats)\n@@ -171,6 +184,20 @@ class ZDFIE(ZDFBaseIE):\n             'duration': 2615,\n             'timestamp': 1465021200,\n             'upload_date': '20160604',\n+            'thumbnail': 'https://www.zdf.de/assets/mauve-im-labor-100~768x432?cb=1464909117806',\n+        },\n+    }, {\n+        'url': 'https://www.zdf.de/funk/druck-11790/funk-alles-ist-verzaubert-102.html',\n+        'md5': '1b93bdec7d02fc0b703c5e7687461628',\n+        'info_dict': {\n+            'ext': 'mp4',\n+            'id': 'video_funk_1770473',\n+            'duration': 1278,\n+            'description': 'Die Neue an der Schule verdreht Ismail den Kopf.',\n+            'title': 'Alles ist verzaubert',\n+            'timestamp': 1635520560,\n+            'upload_date': '20211029',\n+            'thumbnail': 'https://www.zdf.de/assets/teaser-funk-alles-ist-verzaubert-100~1920x1080?cb=1636466431799',\n         },\n     }, {\n         # Same as https://www.phoenix.de/sendungen/dokumentationen/gesten-der-maechtigen-i-a-89468.html?ref=suche\n@@ -204,6 +231,19 @@ class ZDFIE(ZDFBaseIE):\n             'timestamp': 1641355200,\n             'upload_date': '20220105',\n         },\n+        'skip': 'No longer available \"Diese Seite wurde leider nicht gefunden\"'\n+    }, {\n+        'url': 'https://www.zdf.de/serien/soko-stuttgart/das-geld-anderer-leute-100.html',\n+        'info_dict': {\n+            'id': '191205_1800_sendung_sok8',\n+            'ext': 'mp4',\n+            'title': 'Das Geld anderer Leute',\n+            'description': 'md5:cb6f660850dc5eb7d1ab776ea094959d',\n+            'duration': 2581.0,\n+            'timestamp': 1654790700,\n+            'upload_date': '20220609',\n+            'thumbnail': 'https://epg-image.zdf.de/fotobase-webdelivery/images/e2d7e55a-09f0-424e-ac73-6cac4dd65f35?layout=2400x1350',\n+        },\n     }]\n \n     def _extract_entry(self, url, player, content, video_id):\n@@ -265,15 +305,16 @@ def _extract_mobile(self, video_id):\n             'https://zdf-cdn.live.cellular.de/mediathekV2/document/%s' % video_id,\n             video_id)\n \n-        document = video['document']\n-\n-        title = document['titel']\n-        content_id = document['basename']\n-\n         formats = []\n-        format_urls = set()\n-        for f in document['formitaeten']:\n-            self._extract_format(content_id, formats, format_urls, f)\n+        formitaeten = try_get(video, lambda x: x['document']['formitaeten'], list)\n+        document = formitaeten and video['document']\n+        if formitaeten:\n+            title = document['titel']\n+            content_id = document['basename']\n+\n+            format_urls = set()\n+            for f in formitaeten or []:\n+                self._extract_format(content_id, formats, format_urls, f)\n         self._sort_formats(formats)\n \n         thumbnails = []\n@@ -320,9 +361,9 @@ class ZDFChannelIE(ZDFBaseIE):\n         'url': 'https://www.zdf.de/sport/das-aktuelle-sportstudio',\n         'info_dict': {\n             'id': 'das-aktuelle-sportstudio',\n-            'title': 'das aktuelle sportstudio | ZDF',\n+            'title': 'das aktuelle sportstudio',\n         },\n-        'playlist_mincount': 23,\n+        'playlist_mincount': 18,\n     }, {\n         'url': 'https://www.zdf.de/dokumentation/planet-e',\n         'info_dict': {\n@@ -330,6 +371,14 @@ class ZDFChannelIE(ZDFBaseIE):\n             'title': 'planet e.',\n         },\n         'playlist_mincount': 50,\n+    }, {\n+        'url': 'https://www.zdf.de/gesellschaft/aktenzeichen-xy-ungeloest',\n+        'info_dict': {\n+            'id': 'aktenzeichen-xy-ungeloest',\n+            'title': 'Aktenzeichen XY... ungel\u00f6st',\n+            'entries': \"lambda x: not any('xy580-fall1-kindermoerder-gesucht-100' in e['url'] for e in x)\",\n+        },\n+        'playlist_mincount': 2,\n     }, {\n         'url': 'https://www.zdf.de/filme/taunuskrimi/',\n         'only_matching': True,\n@@ -339,60 +388,36 @@ class ZDFChannelIE(ZDFBaseIE):\n     def suitable(cls, url):\n         return False if ZDFIE.suitable(url) else super(ZDFChannelIE, cls).suitable(url)\n \n+    def _og_search_title(self, webpage, fatal=False):\n+        title = super(ZDFChannelIE, self)._og_search_title(webpage, fatal=fatal)\n+        return re.split(r'\\s+[-|]\\s+ZDF(?:mediathek)?$', title or '')[0] or None\n+\n     def _real_extract(self, url):\n         channel_id = self._match_id(url)\n \n         webpage = self._download_webpage(url, channel_id)\n \n-        entries = [\n-            self.url_result(item_url, ie=ZDFIE.ie_key())\n-            for item_url in orderedSet(re.findall(\n-                r'data-plusbar-url=[\"\\'](http.+?\\.html)', webpage))]\n-\n-        return self.playlist_result(\n-            entries, channel_id, self._og_search_title(webpage, fatal=False))\n-\n-        r\"\"\"\n-        player = self._extract_player(webpage, channel_id)\n-\n-        channel_id = self._search_regex(\n-            r'docId\\s*:\\s*([\"\\'])(?P<id>(?!\\1).+?)\\1', webpage,\n-            'channel id', group='id')\n-\n-        channel = self._call_api(\n-            'https://api.zdf.de/content/documents/%s.json' % channel_id,\n-            player, url, channel_id)\n-\n-        items = []\n-        for module in channel['module']:\n-            for teaser in try_get(module, lambda x: x['teaser'], list) or []:\n-                t = try_get(\n-                    teaser, lambda x: x['http://zdf.de/rels/target'], dict)\n-                if not t:\n-                    continue\n-                items.extend(try_get(\n-                    t,\n-                    lambda x: x['resultsWithVideo']['http://zdf.de/rels/search/results'],\n-                    list) or [])\n-            items.extend(try_get(\n-                module,\n-                lambda x: x['filterRef']['resultsWithVideo']['http://zdf.de/rels/search/results'],\n-                list) or [])\n-\n-        entries = []\n-        entry_urls = set()\n-        for item in items:\n-            t = try_get(item, lambda x: x['http://zdf.de/rels/target'], dict)\n-            if not t:\n-                continue\n-            sharing_url = t.get('http://zdf.de/rels/sharing-url')\n-            if not sharing_url or not isinstance(sharing_url, compat_str):\n-                continue\n-            if sharing_url in entry_urls:\n-                continue\n-            entry_urls.add(sharing_url)\n-            entries.append(self.url_result(\n-                sharing_url, ie=ZDFIE.ie_key(), video_id=t.get('id')))\n-\n-        return self.playlist_result(entries, channel_id, channel.get('title'))\n-        \"\"\"\n+        matches = re.finditer(\n+            r'''<div\\b[^>]*?\\sdata-plusbar-id\\s*=\\s*([\"'])(?P<p_id>[\\w-]+)\\1[^>]*?\\sdata-plusbar-url=\\1(?P<url>%s)\\1''' % ZDFIE._VALID_URL,\n+            webpage)\n+\n+        if self._downloader.params.get('noplaylist', False):\n+            entry = next(\n+                (self.url_result(m.group('url'), ie=ZDFIE.ie_key()) for m in matches),\n+                None)\n+            self.to_screen('Downloading just the main video because of --no-playlist')\n+            if entry:\n+                return entry\n+        else:\n+            self.to_screen('Downloading playlist %s - add --no-playlist to download just the main video' % (channel_id, ))\n+\n+        def check_video(m):\n+            v_ref = self._search_regex(\n+                r'''(<a\\b[^>]*?\\shref\\s*=[^>]+?\\sdata-target-id\\s*=\\s*([\"'])%s\\2[^>]*>)''' % (m.group('p_id'), ),\n+                webpage, 'check id', default='')\n+            v_ref = extract_attributes(v_ref)\n+            return v_ref.get('data-target-video-type') != 'novideo'\n+\n+        return self.playlist_from_matches(\n+            (m.group('url') for m in matches if check_video(m)),\n+            channel_id, self._og_search_title(webpage, fatal=False))\ndiff --git a/youtube_dl/jsinterp.py b/youtube_dl/jsinterp.py\nindex 8eaa911cdab..a616ad070b2 100644\n--- a/youtube_dl/jsinterp.py\n+++ b/youtube_dl/jsinterp.py\n@@ -1,42 +1,190 @@\n from __future__ import unicode_literals\n \n+import itertools\n import json\n import operator\n import re\n \n+from functools import update_wrapper\n+\n from .utils import (\n+    error_to_compat_str,\n     ExtractorError,\n+    js_to_json,\n     remove_quotes,\n+    unified_timestamp,\n+    variadic,\n+    write_string,\n )\n from .compat import (\n-    compat_collections_abc,\n+    compat_basestring,\n+    compat_chr,\n+    compat_collections_chain_map as ChainMap,\n+    compat_filter as filter,\n+    compat_itertools_zip_longest as zip_longest,\n+    compat_map as map,\n     compat_str,\n )\n-MutableMapping = compat_collections_abc.MutableMapping\n \n \n-class Nonlocal:\n-    pass\n+# name JS functions\n+class function_with_repr(object):\n+    # from yt_dlp/utils.py, but in this module\n+    # repr_ is always set\n+    def __init__(self, func, repr_):\n+        update_wrapper(self, func)\n+        self.func, self.__repr = func, repr_\n+\n+    def __call__(self, *args, **kwargs):\n+        return self.func(*args, **kwargs)\n+\n+    def __repr__(self):\n+        return self.__repr\n+\n+\n+# name JS operators\n+def wraps_op(op):\n+\n+    def update_and_rename_wrapper(w):\n+        f = update_wrapper(w, op)\n+        # fn names are str in both Py 2/3\n+        f.__name__ = str('JS_') + f.__name__\n+        return f\n+\n+    return update_and_rename_wrapper\n+\n+\n+# NB In principle NaN cannot be checked by membership.\n+# Here all NaN values are actually this one, so _NaN is _NaN,\n+# although _NaN != _NaN. Ditto Infinity.\n+\n+_NaN = float('nan')\n+_Infinity = float('inf')\n+\n+\n+def _js_bit_op(op):\n+\n+    def zeroise(x):\n+        return 0 if x in (None, JS_Undefined, _NaN, _Infinity) else x\n+\n+    @wraps_op(op)\n+    def wrapped(a, b):\n+        return op(zeroise(a), zeroise(b)) & 0xffffffff\n+\n+    return wrapped\n+\n+\n+def _js_arith_op(op):\n+\n+    @wraps_op(op)\n+    def wrapped(a, b):\n+        if JS_Undefined in (a, b):\n+            return _NaN\n+        return op(a or 0, b or 0)\n+\n+    return wrapped\n+\n+\n+def _js_div(a, b):\n+    if JS_Undefined in (a, b) or not (a or b):\n+        return _NaN\n+    return operator.truediv(a or 0, b) if b else _Infinity\n+\n+\n+def _js_mod(a, b):\n+    if JS_Undefined in (a, b) or not b:\n+        return _NaN\n+    return (a or 0) % b\n+\n+\n+def _js_exp(a, b):\n+    if not b:\n+        return 1  # even 0 ** 0 !!\n+    elif JS_Undefined in (a, b):\n+        return _NaN\n+    return (a or 0) ** b\n+\n+\n+def _js_eq_op(op):\n+\n+    @wraps_op(op)\n+    def wrapped(a, b):\n+        if set((a, b)) <= set((None, JS_Undefined)):\n+            return op(a, a)\n+        return op(a, b)\n+\n+    return wrapped\n+\n+\n+def _js_comp_op(op):\n+\n+    @wraps_op(op)\n+    def wrapped(a, b):\n+        if JS_Undefined in (a, b):\n+            return False\n+        if isinstance(a, compat_basestring):\n+            b = compat_str(b or 0)\n+        elif isinstance(b, compat_basestring):\n+            a = compat_str(a or 0)\n+        return op(a or 0, b or 0)\n \n+    return wrapped\n \n-_OPERATORS = [\n-    ('|', operator.or_),\n-    ('^', operator.xor),\n-    ('&', operator.and_),\n-    ('>>', operator.rshift),\n-    ('<<', operator.lshift),\n-    ('-', operator.sub),\n-    ('+', operator.add),\n-    ('%', operator.mod),\n-    ('/', operator.truediv),\n-    ('*', operator.mul),\n-]\n-_ASSIGN_OPERATORS = [(op + '=', opfunc) for op, opfunc in _OPERATORS]\n-_ASSIGN_OPERATORS.append(('=', (lambda cur, right: right)))\n \n-_NAME_RE = r'[a-zA-Z_$][a-zA-Z_$0-9]*'\n+def _js_ternary(cndn, if_true=True, if_false=False):\n+    \"\"\"Simulate JS's ternary operator (cndn?if_true:if_false)\"\"\"\n+    if cndn in (False, None, 0, '', JS_Undefined, _NaN):\n+        return if_false\n+    return if_true\n \n+\n+# (op, definition) in order of binding priority, tightest first\n+# avoid dict to maintain order\n+# definition None => Defined in JSInterpreter._operator\n+_OPERATORS = (\n+    ('>>', _js_bit_op(operator.rshift)),\n+    ('<<', _js_bit_op(operator.lshift)),\n+    ('+', _js_arith_op(operator.add)),\n+    ('-', _js_arith_op(operator.sub)),\n+    ('*', _js_arith_op(operator.mul)),\n+    ('%', _js_mod),\n+    ('/', _js_div),\n+    ('**', _js_exp),\n+)\n+\n+_COMP_OPERATORS = (\n+    ('===', operator.is_),\n+    ('!==', operator.is_not),\n+    ('==', _js_eq_op(operator.eq)),\n+    ('!=', _js_eq_op(operator.ne)),\n+    ('<=', _js_comp_op(operator.le)),\n+    ('>=', _js_comp_op(operator.ge)),\n+    ('<', _js_comp_op(operator.lt)),\n+    ('>', _js_comp_op(operator.gt)),\n+)\n+\n+_LOG_OPERATORS = (\n+    ('|', _js_bit_op(operator.or_)),\n+    ('^', _js_bit_op(operator.xor)),\n+    ('&', _js_bit_op(operator.and_)),\n+)\n+\n+_SC_OPERATORS = (\n+    ('?', None),\n+    ('??', None),\n+    ('||', None),\n+    ('&&', None),\n+)\n+\n+_OPERATOR_RE = '|'.join(map(lambda x: re.escape(x[0]), _OPERATORS + _LOG_OPERATORS))\n+\n+_NAME_RE = r'[a-zA-Z_$][\\w$]*'\n _MATCHING_PARENS = dict(zip(*zip('()', '{}', '[]')))\n+_QUOTES = '\\'\"/'\n+\n+\n+class JS_Undefined(object):\n+    pass\n \n \n class JS_Break(ExtractorError):\n@@ -49,70 +197,211 @@ def __init__(self):\n         ExtractorError.__init__(self, 'Invalid continue')\n \n \n-class LocalNameSpace(MutableMapping):\n-    def __init__(self, *stack):\n-        self.stack = tuple(stack)\n+class JS_Throw(ExtractorError):\n+    def __init__(self, e):\n+        self.error = e\n+        ExtractorError.__init__(self, 'Uncaught exception ' + error_to_compat_str(e))\n \n+\n+class LocalNameSpace(ChainMap):\n     def __getitem__(self, key):\n-        for scope in self.stack:\n-            if key in scope:\n-                return scope[key]\n-        raise KeyError(key)\n+        try:\n+            return super(LocalNameSpace, self).__getitem__(key)\n+        except KeyError:\n+            return JS_Undefined\n \n     def __setitem__(self, key, value):\n-        for scope in self.stack:\n+        for scope in self.maps:\n             if key in scope:\n                 scope[key] = value\n-                break\n-        else:\n-            self.stack[0][key] = value\n-        return value\n+                return\n+        self.maps[0][key] = value\n \n     def __delitem__(self, key):\n         raise NotImplementedError('Deleting is not supported')\n \n-    def __iter__(self):\n-        for scope in self.stack:\n-            for scope_item in iter(scope):\n-                yield scope_item\n+    def __repr__(self):\n+        return 'LocalNameSpace%s' % (self.maps, )\n \n-    def __len__(self, key):\n-        return len(iter(self))\n \n-    def __repr__(self):\n-        return 'LocalNameSpace%s' % (self.stack, )\n+class Debugger(object):\n+    ENABLED = False\n+\n+    @staticmethod\n+    def write(*args, **kwargs):\n+        level = kwargs.get('level', 100)\n+\n+        def truncate_string(s, left, right=0):\n+            if s is None or len(s) <= left + right:\n+                return s\n+            return '...'.join((s[:left - 3], s[-right:] if right else ''))\n+\n+        write_string('[debug] JS: {0}{1}\\n'.format(\n+            '  ' * (100 - level),\n+            ' '.join(truncate_string(compat_str(x), 50, 50) for x in args)))\n+\n+    @classmethod\n+    def wrap_interpreter(cls, f):\n+        def interpret_statement(self, stmt, local_vars, allow_recursion, *args, **kwargs):\n+            if cls.ENABLED and stmt.strip():\n+                cls.write(stmt, level=allow_recursion)\n+            try:\n+                ret, should_ret = f(self, stmt, local_vars, allow_recursion, *args, **kwargs)\n+            except Exception as e:\n+                if cls.ENABLED:\n+                    if isinstance(e, ExtractorError):\n+                        e = e.orig_msg\n+                    cls.write('=> Raises:', e, '<-|', stmt, level=allow_recursion)\n+                raise\n+            if cls.ENABLED and stmt.strip():\n+                if should_ret or repr(ret) != stmt:\n+                    cls.write(['->', '=>'][should_ret], repr(ret), '<-|', stmt, level=allow_recursion)\n+            return ret, should_ret\n+        return interpret_statement\n \n \n class JSInterpreter(object):\n+    __named_object_counter = 0\n+\n+    _OBJ_NAME = '__youtube_dl_jsinterp_obj'\n+\n+    OP_CHARS = None\n+\n     def __init__(self, code, objects=None):\n-        if objects is None:\n-            objects = {}\n-        self.code = code\n-        self._functions = {}\n-        self._objects = objects\n-        self.__named_object_counter = 0\n+        self.code, self._functions = code, {}\n+        self._objects = {} if objects is None else objects\n+        if type(self).OP_CHARS is None:\n+            type(self).OP_CHARS = self.OP_CHARS = self.__op_chars()\n+\n+    class Exception(ExtractorError):\n+        def __init__(self, msg, *args, **kwargs):\n+            expr = kwargs.pop('expr', None)\n+            if expr is not None:\n+                msg = '{0} in: {1!r:.100}'.format(msg.rstrip(), expr)\n+            super(JSInterpreter.Exception, self).__init__(msg, *args, **kwargs)\n+\n+    class JS_RegExp(object):\n+        RE_FLAGS = {\n+            # special knowledge: Python's re flags are bitmask values, current max 128\n+            # invent new bitmask values well above that for literal parsing\n+            # TODO: execute matches with these flags (remaining: d, y)\n+            'd': 1024,  # Generate indices for substring matches\n+            'g': 2048,  # Global search\n+            'i': re.I,  # Case-insensitive search\n+            'm': re.M,  # Multi-line search\n+            's': re.S,  # Allows . to match newline characters\n+            'u': re.U,  # Treat a pattern as a sequence of unicode code points\n+            'y': 4096,  # Perform a \"sticky\" search that matches starting at the current position in the target string\n+        }\n+\n+        def __init__(self, pattern_txt, flags=0):\n+            if isinstance(flags, compat_str):\n+                flags, _ = self.regex_flags(flags)\n+            # First, avoid https://github.com/python/cpython/issues/74534\n+            self.__self = None\n+            self.__pattern_txt = pattern_txt.replace('[[', r'[\\[')\n+            self.__flags = flags\n+\n+        def __instantiate(self):\n+            if self.__self:\n+                return\n+            self.__self = re.compile(self.__pattern_txt, self.__flags)\n+            # Thx: https://stackoverflow.com/questions/44773522/setattr-on-python2-sre-sre-pattern\n+            for name in dir(self.__self):\n+                # Only these? Obviously __class__, __init__.\n+                # PyPy creates a __weakref__ attribute with value None\n+                # that can't be setattr'd but also can't need to be copied.\n+                if name in ('__class__', '__init__', '__weakref__'):\n+                    continue\n+                setattr(self, name, getattr(self.__self, name))\n+\n+        def __getattr__(self, name):\n+            self.__instantiate()\n+            # make Py 2.6 conform to its lying documentation\n+            if name == 'flags':\n+                self.flags = self.__flags\n+                return self.flags\n+            elif name == 'pattern':\n+                self.pattern = self.__pattern_txt\n+                return self.pattern\n+            elif hasattr(self.__self, name):\n+                v = getattr(self.__self, name)\n+                setattr(self, name, v)\n+                return v\n+            elif name in ('groupindex', 'groups'):\n+                return 0 if name == 'groupindex' else {}\n+            raise AttributeError('{0} has no attribute named {1}'.format(self, name))\n+\n+        @classmethod\n+        def regex_flags(cls, expr):\n+            flags = 0\n+            if not expr:\n+                return flags, expr\n+            for idx, ch in enumerate(expr):\n+                if ch not in cls.RE_FLAGS:\n+                    break\n+                flags |= cls.RE_FLAGS[ch]\n+            return flags, expr[idx + 1:]\n+\n+    @classmethod\n+    def __op_chars(cls):\n+        op_chars = set(';,[')\n+        for op in cls._all_operators():\n+            op_chars.update(op[0])\n+        return op_chars\n \n     def _named_object(self, namespace, obj):\n         self.__named_object_counter += 1\n-        name = '__youtube_dl_jsinterp_obj%s' % (self.__named_object_counter, )\n+        name = '%s%d' % (self._OBJ_NAME, self.__named_object_counter)\n+        if callable(obj) and not isinstance(obj, function_with_repr):\n+            obj = function_with_repr(obj, 'F<%s>' % (self.__named_object_counter, ))\n         namespace[name] = obj\n         return name\n \n-    @staticmethod\n-    def _separate(expr, delim=',', max_split=None):\n+    @classmethod\n+    def _separate(cls, expr, delim=',', max_split=None, skip_delims=None):\n         if not expr:\n             return\n-        counters = {k: 0 for k in _MATCHING_PARENS.values()}\n+        # collections.Counter() is ~10% slower in both 2.7 and 3.9\n+        counters = dict((k, 0) for k in _MATCHING_PARENS.values())\n         start, splits, pos, delim_len = 0, 0, 0, len(delim) - 1\n+        in_quote, escaping, after_op, in_regex_char_group = None, False, True, False\n+        skipping = 0\n+        if skip_delims:\n+            skip_delims = variadic(skip_delims)\n         for idx, char in enumerate(expr):\n-            if char in _MATCHING_PARENS:\n-                counters[_MATCHING_PARENS[char]] += 1\n-            elif char in counters:\n-                counters[char] -= 1\n-            if char != delim[pos] or any(counters.values()):\n-                pos = 0\n+            paren_delta = 0\n+            if not in_quote:\n+                if char in _MATCHING_PARENS:\n+                    counters[_MATCHING_PARENS[char]] += 1\n+                    paren_delta = 1\n+                elif char in counters:\n+                    counters[char] -= 1\n+                    paren_delta = -1\n+            if not escaping:\n+                if char in _QUOTES and in_quote in (char, None):\n+                    if in_quote or after_op or char != '/':\n+                        in_quote = None if in_quote and not in_regex_char_group else char\n+                elif in_quote == '/' and char in '[]':\n+                    in_regex_char_group = char == '['\n+            escaping = not escaping and in_quote and char == '\\\\'\n+            after_op = not in_quote and (char in cls.OP_CHARS or paren_delta > 0 or (after_op and char.isspace()))\n+\n+            if char != delim[pos] or any(counters.values()) or in_quote:\n+                pos = skipping = 0\n                 continue\n-            elif pos != delim_len:\n+            elif skipping > 0:\n+                skipping -= 1\n+                continue\n+            elif pos == 0 and skip_delims:\n+                here = expr[idx:]\n+                for s in skip_delims:\n+                    if here.startswith(s) and s:\n+                        skipping = len(s) - 1\n+                        break\n+                if skipping > 0:\n+                    continue\n+            if pos < delim_len:\n                 pos += 1\n                 continue\n             yield expr[start: idx - delim_len]\n@@ -122,130 +411,279 @@ def _separate(expr, delim=',', max_split=None):\n                 break\n         yield expr[start:]\n \n-    @staticmethod\n-    def _separate_at_paren(expr, delim):\n-        separated = list(JSInterpreter._separate(expr, delim, 1))\n+    @classmethod\n+    def _separate_at_paren(cls, expr, delim=None):\n+        if delim is None:\n+            delim = expr and _MATCHING_PARENS[expr[0]]\n+        separated = list(cls._separate(expr, delim, 1))\n         if len(separated) < 2:\n-            raise ExtractorError('No terminating paren {0} in {1}'.format(delim, expr))\n+            raise cls.Exception('No terminating paren {delim} in {expr!r:.5500}'.format(**locals()))\n         return separated[0][1:].strip(), separated[1].strip()\n \n+    @staticmethod\n+    def _all_operators(_cached=[]):\n+        if not _cached:\n+            _cached.extend(itertools.chain(\n+                # Ref: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Operator_Precedence\n+                _SC_OPERATORS, _LOG_OPERATORS, _COMP_OPERATORS, _OPERATORS))\n+        return _cached\n+\n+    def _operator(self, op, left_val, right_expr, expr, local_vars, allow_recursion):\n+        if op in ('||', '&&'):\n+            if (op == '&&') ^ _js_ternary(left_val):\n+                return left_val  # short circuiting\n+        elif op == '??':\n+            if left_val not in (None, JS_Undefined):\n+                return left_val\n+        elif op == '?':\n+            right_expr = _js_ternary(left_val, *self._separate(right_expr, ':', 1))\n+\n+        right_val = self.interpret_expression(right_expr, local_vars, allow_recursion)\n+        opfunc = op and next((v for k, v in self._all_operators() if k == op), None)\n+        if not opfunc:\n+            return right_val\n+\n+        try:\n+            # print('Eval:', opfunc.__name__, left_val, right_val)\n+            return opfunc(left_val, right_val)\n+        except Exception as e:\n+            raise self.Exception('Failed to evaluate {left_val!r:.50} {op} {right_val!r:.50}'.format(**locals()), expr, cause=e)\n+\n+    def _index(self, obj, idx, allow_undefined=False):\n+        if idx == 'length':\n+            return len(obj)\n+        try:\n+            return obj[int(idx)] if isinstance(obj, list) else obj[idx]\n+        except Exception as e:\n+            if allow_undefined:\n+                return JS_Undefined\n+            raise self.Exception('Cannot get index {idx!r:.100}'.format(**locals()), expr=repr(obj), cause=e)\n+\n+    def _dump(self, obj, namespace):\n+        try:\n+            return json.dumps(obj)\n+        except TypeError:\n+            return self._named_object(namespace, obj)\n+\n+    # used below\n+    _VAR_RET_THROW_RE = re.compile(r'''(?x)\n+        (?P<var>(?:var|const|let)\\s)|return(?:\\s+|(?=[\"'])|$)|(?P<throw>throw\\s+)\n+        ''')\n+    _COMPOUND_RE = re.compile(r'''(?x)\n+        (?P<try>try)\\s*\\{|\n+        (?P<if>if)\\s*\\(|\n+        (?P<switch>switch)\\s*\\(|\n+        (?P<for>for)\\s*\\(|\n+        (?P<while>while)\\s*\\(\n+        ''')\n+    _FINALLY_RE = re.compile(r'finally\\s*\\{')\n+    _SWITCH_RE = re.compile(r'switch\\s*\\(')\n+\n+    @Debugger.wrap_interpreter\n     def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n         if allow_recursion < 0:\n-            raise ExtractorError('Recursion limit reached')\n+            raise self.Exception('Recursion limit reached')\n+        allow_recursion -= 1\n+\n+        # print('At: ' + stmt[:60])\n+        should_return = False\n+        # fails on (eg) if (...) stmt1; else stmt2;\n+        sub_statements = list(self._separate(stmt, ';')) or ['']\n+        expr = stmt = sub_statements.pop().strip()\n \n-        sub_statements = list(self._separate(stmt, ';'))\n-        stmt = (sub_statements or ['']).pop()\n         for sub_stmt in sub_statements:\n-            ret, should_abort = self.interpret_statement(sub_stmt, local_vars, allow_recursion - 1)\n-            if should_abort:\n-                return ret\n+            ret, should_return = self.interpret_statement(sub_stmt, local_vars, allow_recursion)\n+            if should_return:\n+                return ret, should_return\n \n-        should_abort = False\n-        stmt = stmt.lstrip()\n-        stmt_m = re.match(r'var\\s', stmt)\n-        if stmt_m:\n-            expr = stmt[len(stmt_m.group(0)):]\n-        else:\n-            return_m = re.match(r'return(?:\\s+|$)', stmt)\n-            if return_m:\n-                expr = stmt[len(return_m.group(0)):]\n-                should_abort = True\n-            else:\n-                # Try interpreting it as an expression\n-                expr = stmt\n+        m = self._VAR_RET_THROW_RE.match(stmt)\n+        if m:\n+            expr = stmt[len(m.group(0)):].strip()\n+            if m.group('throw'):\n+                raise JS_Throw(self.interpret_expression(expr, local_vars, allow_recursion))\n+            should_return = not m.group('var')\n+        if not expr:\n+            return None, should_return\n \n-        v = self.interpret_expression(expr, local_vars, allow_recursion)\n-        return v, should_abort\n+        if expr[0] in _QUOTES:\n+            inner, outer = self._separate(expr, expr[0], 1)\n+            if expr[0] == '/':\n+                flags, outer = self.JS_RegExp.regex_flags(outer)\n+                inner = self.JS_RegExp(inner[1:], flags=flags)\n+            else:\n+                inner = json.loads(js_to_json(inner + expr[0]))  # , strict=True))\n+            if not outer:\n+                return inner, should_return\n+            expr = self._named_object(local_vars, inner) + outer\n+\n+        new_kw, _, obj = expr.partition('new ')\n+        if not new_kw:\n+            for klass, konstr in (('Date', lambda x: int(unified_timestamp(x, False) * 1000)),\n+                                  ('RegExp', self.JS_RegExp),\n+                                  ('Error', self.Exception)):\n+                if not obj.startswith(klass + '('):\n+                    continue\n+                left, right = self._separate_at_paren(obj[len(klass):])\n+                argvals = self.interpret_iter(left, local_vars, allow_recursion)\n+                expr = konstr(*argvals)\n+                if expr is None:\n+                    raise self.Exception('Failed to parse {klass} {left!r:.100}'.format(**locals()), expr=expr)\n+                expr = self._dump(expr, local_vars) + right\n+                break\n+            else:\n+                raise self.Exception('Unsupported object {obj:.100}'.format(**locals()), expr=expr)\n \n-    def interpret_expression(self, expr, local_vars, allow_recursion):\n-        expr = expr.strip()\n-        if expr == '':  # Empty expression\n-            return None\n+        if expr.startswith('void '):\n+            left = self.interpret_expression(expr[5:], local_vars, allow_recursion)\n+            return None, should_return\n \n         if expr.startswith('{'):\n-            inner, outer = self._separate_at_paren(expr, '}')\n-            inner, should_abort = self.interpret_statement(inner, local_vars, allow_recursion - 1)\n+            inner, outer = self._separate_at_paren(expr)\n+            # try for object expression (Map)\n+            sub_expressions = [list(self._separate(sub_expr.strip(), ':', 1)) for sub_expr in self._separate(inner)]\n+            if all(len(sub_expr) == 2 for sub_expr in sub_expressions):\n+                return dict(\n+                    (key_expr if re.match(_NAME_RE, key_expr) else key_expr,\n+                     self.interpret_expression(val_expr, local_vars, allow_recursion))\n+                    for key_expr, val_expr in sub_expressions), should_return\n+            # or statement list\n+            inner, should_abort = self.interpret_statement(inner, local_vars, allow_recursion)\n             if not outer or should_abort:\n-                return inner\n+                return inner, should_abort or should_return\n             else:\n-                expr = json.dumps(inner) + outer\n+                expr = self._dump(inner, local_vars) + outer\n \n         if expr.startswith('('):\n-            inner, outer = self._separate_at_paren(expr, ')')\n-            inner = self.interpret_expression(inner, local_vars, allow_recursion)\n-            if not outer:\n-                return inner\n+            m = re.match(r'\\((?P<d>[a-z])%(?P<e>[a-z])\\.length\\+(?P=e)\\.length\\)%(?P=e)\\.length', expr)\n+            if m:\n+                # short-cut eval of frequently used `(d%e.length+e.length)%e.length`, worth ~6% on `pytest -k test_nsig`\n+                outer = None\n+                inner, should_abort = self._offset_e_by_d(m.group('d'), m.group('e'), local_vars)\n+            else:\n+                inner, outer = self._separate_at_paren(expr)\n+                inner, should_abort = self.interpret_statement(inner, local_vars, allow_recursion)\n+            if not outer or should_abort:\n+                return inner, should_abort or should_return\n             else:\n-                expr = json.dumps(inner) + outer\n+                expr = self._dump(inner, local_vars) + outer\n \n         if expr.startswith('['):\n-            inner, outer = self._separate_at_paren(expr, ']')\n+            inner, outer = self._separate_at_paren(expr)\n             name = self._named_object(local_vars, [\n                 self.interpret_expression(item, local_vars, allow_recursion)\n                 for item in self._separate(inner)])\n             expr = name + outer\n \n-        m = re.match(r'try\\s*', expr)\n-        if m:\n-            if expr[m.end()] == '{':\n-                try_expr, expr = self._separate_at_paren(expr[m.end():], '}')\n+        m = self._COMPOUND_RE.match(expr)\n+        md = m.groupdict() if m else {}\n+        if md.get('if'):\n+            cndn, expr = self._separate_at_paren(expr[m.end() - 1:])\n+            if expr.startswith('{'):\n+                if_expr, expr = self._separate_at_paren(expr)\n             else:\n-                try_expr, expr = expr[m.end() - 1:], ''\n-            ret, should_abort = self.interpret_statement(try_expr, local_vars, allow_recursion - 1)\n+                # may lose ... else ... because of ll.368-374\n+                if_expr, expr = self._separate_at_paren(expr, delim=';')\n+            else_expr = None\n+            m = re.match(r'else\\s*(?P<block>\\{)?', expr)\n+            if m:\n+                if m.group('block'):\n+                    else_expr, expr = self._separate_at_paren(expr[m.end() - 1:])\n+                else:\n+                    # handle subset ... else if (...) {...} else ...\n+                    # TODO: make interpret_statement do this properly, if possible\n+                    exprs = list(self._separate(expr[m.end():], delim='}', max_split=2))\n+                    if len(exprs) > 1:\n+                        if re.match(r'\\s*if\\s*\\(', exprs[0]) and re.match(r'\\s*else\\b', exprs[1]):\n+                            else_expr = exprs[0] + '}' + exprs[1]\n+                            expr = (exprs[2] + '}') if len(exprs) == 3 else None\n+                        else:\n+                            else_expr = exprs[0]\n+                            exprs.append('')\n+                            expr = '}'.join(exprs[1:])\n+                    else:\n+                        else_expr = exprs[0]\n+                        expr = None\n+                    else_expr = else_expr.lstrip() + '}'\n+            cndn = _js_ternary(self.interpret_expression(cndn, local_vars, allow_recursion))\n+            ret, should_abort = self.interpret_statement(\n+                if_expr if cndn else else_expr, local_vars, allow_recursion)\n             if should_abort:\n-                return ret\n-            return self.interpret_statement(expr, local_vars, allow_recursion - 1)[0]\n+                return ret, True\n \n-        m = re.match(r'(?:(?P<catch>catch)|(?P<for>for)|(?P<switch>switch))\\s*\\(', expr)\n-        md = m.groupdict() if m else {}\n-        if md.get('catch'):\n-            # We ignore the catch block\n-            _, expr = self._separate_at_paren(expr, '}')\n-            return self.interpret_statement(expr, local_vars, allow_recursion - 1)[0]\n+        elif md.get('try'):\n+            try_expr, expr = self._separate_at_paren(expr[m.end() - 1:])\n+            err = None\n+            try:\n+                ret, should_abort = self.interpret_statement(try_expr, local_vars, allow_recursion)\n+                if should_abort:\n+                    return ret, True\n+            except Exception as e:\n+                # XXX: This works for now, but makes debugging future issues very hard\n+                err = e\n+\n+            pending = (None, False)\n+            m = re.match(r'catch\\s*(?P<err>\\(\\s*{_NAME_RE}\\s*\\))?\\{{'.format(**globals()), expr)\n+            if m:\n+                sub_expr, expr = self._separate_at_paren(expr[m.end() - 1:])\n+                if err:\n+                    catch_vars = {}\n+                    if m.group('err'):\n+                        catch_vars[m.group('err')] = err.error if isinstance(err, JS_Throw) else err\n+                    catch_vars = local_vars.new_child(m=catch_vars)\n+                    err, pending = None, self.interpret_statement(sub_expr, catch_vars, allow_recursion)\n+\n+            m = self._FINALLY_RE.match(expr)\n+            if m:\n+                sub_expr, expr = self._separate_at_paren(expr[m.end() - 1:])\n+                ret, should_abort = self.interpret_statement(sub_expr, local_vars, allow_recursion)\n+                if should_abort:\n+                    return ret, True\n+\n+            ret, should_abort = pending\n+            if should_abort:\n+                return ret, True\n \n-        elif md.get('for'):\n-            def raise_constructor_error(c):\n-                raise ExtractorError(\n-                    'Premature return in the initialization of a for loop in {0!r}'.format(c))\n+            if err:\n+                raise err\n \n-            constructor, remaining = self._separate_at_paren(expr[m.end() - 1:], ')')\n+        elif md.get('for') or md.get('while'):\n+            init_or_cond, remaining = self._separate_at_paren(expr[m.end() - 1:])\n             if remaining.startswith('{'):\n-                body, expr = self._separate_at_paren(remaining, '}')\n+                body, expr = self._separate_at_paren(remaining)\n             else:\n-                m = re.match(r'switch\\s*\\(', remaining)  # FIXME\n-                if m:\n-                    switch_val, remaining = self._separate_at_paren(remaining[m.end() - 1:], ')')\n+                switch_m = self._SWITCH_RE.match(remaining)  # FIXME\n+                if switch_m:\n+                    switch_val, remaining = self._separate_at_paren(remaining[switch_m.end() - 1:])\n                     body, expr = self._separate_at_paren(remaining, '}')\n                     body = 'switch(%s){%s}' % (switch_val, body)\n                 else:\n                     body, expr = remaining, ''\n-            start, cndn, increment = self._separate(constructor, ';')\n-            if self.interpret_statement(start, local_vars, allow_recursion - 1)[1]:\n-                raise_constructor_error(constructor)\n-            while True:\n-                if not self.interpret_expression(cndn, local_vars, allow_recursion):\n-                    break\n+            if md.get('for'):\n+                start, cndn, increment = self._separate(init_or_cond, ';')\n+                self.interpret_expression(start, local_vars, allow_recursion)\n+            else:\n+                cndn, increment = init_or_cond, None\n+            while _js_ternary(self.interpret_expression(cndn, local_vars, allow_recursion)):\n                 try:\n-                    ret, should_abort = self.interpret_statement(body, local_vars, allow_recursion - 1)\n+                    ret, should_abort = self.interpret_statement(body, local_vars, allow_recursion)\n                     if should_abort:\n-                        return ret\n+                        return ret, True\n                 except JS_Break:\n                     break\n                 except JS_Continue:\n                     pass\n-                if self.interpret_statement(increment, local_vars, allow_recursion - 1)[1]:\n-                    raise_constructor_error(constructor)\n-            return self.interpret_statement(expr, local_vars, allow_recursion - 1)[0]\n+                if increment:\n+                    self.interpret_expression(increment, local_vars, allow_recursion)\n \n         elif md.get('switch'):\n-            switch_val, remaining = self._separate_at_paren(expr[m.end() - 1:], ')')\n+            switch_val, remaining = self._separate_at_paren(expr[m.end() - 1:])\n             switch_val = self.interpret_expression(switch_val, local_vars, allow_recursion)\n             body, expr = self._separate_at_paren(remaining, '}')\n             items = body.replace('default:', 'case default:').split('case ')[1:]\n             for default in (False, True):\n                 matched = False\n                 for item in items:\n-                    case, stmt = [i.strip() for i in self._separate(item, ':', 1)]\n+                    case, stmt = (i.strip() for i in self._separate(item, ':', 1))\n                     if default:\n                         matched = matched or case == 'default'\n                     elif not matched:\n@@ -254,24 +692,30 @@ def raise_constructor_error(c):\n                     if not matched:\n                         continue\n                     try:\n-                        ret, should_abort = self.interpret_statement(stmt, local_vars, allow_recursion - 1)\n+                        ret, should_abort = self.interpret_statement(stmt, local_vars, allow_recursion)\n                         if should_abort:\n                             return ret\n                     except JS_Break:\n                         break\n                 if matched:\n                     break\n-            return self.interpret_statement(expr, local_vars, allow_recursion - 1)[0]\n+\n+        if md:\n+            ret, should_abort = self.interpret_statement(expr, local_vars, allow_recursion)\n+            return ret, should_abort or should_return\n \n         # Comma separated statements\n         sub_expressions = list(self._separate(expr))\n-        expr = sub_expressions.pop().strip() if sub_expressions else ''\n-        for sub_expr in sub_expressions:\n-            self.interpret_expression(sub_expr, local_vars, allow_recursion)\n+        if len(sub_expressions) > 1:\n+            for sub_expr in sub_expressions:\n+                ret, should_abort = self.interpret_statement(sub_expr, local_vars, allow_recursion)\n+                if should_abort:\n+                    return ret, True\n+            return ret, False\n \n         for m in re.finditer(r'''(?x)\n-                (?P<pre_sign>\\+\\+|--)(?P<var1>%(_NAME_RE)s)|\n-                (?P<var2>%(_NAME_RE)s)(?P<post_sign>\\+\\+|--)''' % globals(), expr):\n+                (?P<pre_sign>\\+\\+|--)(?P<var1>{_NAME_RE})|\n+                (?P<var2>{_NAME_RE})(?P<post_sign>\\+\\+|--)'''.format(**globals()), expr):\n             var = m.group('var1') or m.group('var2')\n             start, end = m.span()\n             sign = m.group('pre_sign') or m.group('post_sign')\n@@ -279,129 +723,199 @@ def raise_constructor_error(c):\n             local_vars[var] += 1 if sign[0] == '+' else -1\n             if m.group('pre_sign'):\n                 ret = local_vars[var]\n-            expr = expr[:start] + json.dumps(ret) + expr[end:]\n-\n-        for op, opfunc in _ASSIGN_OPERATORS:\n-            m = re.match(r'''(?x)\n-                (?P<out>%s)(?:\\[(?P<index>[^\\]]+?)\\])?\n-                \\s*%s\n-                (?P<expr>.*)$''' % (_NAME_RE, re.escape(op)), expr)\n-            if not m:\n-                continue\n-            right_val = self.interpret_expression(m.group('expr'), local_vars, allow_recursion)\n-\n-            if m.groupdict().get('index'):\n-                lvar = local_vars[m.group('out')]\n-                idx = self.interpret_expression(m.group('index'), local_vars, allow_recursion)\n-                if not isinstance(idx, int):\n-                    raise ExtractorError('List indices must be integers: %s' % (idx, ))\n-                cur = lvar[idx]\n-                val = opfunc(cur, right_val)\n-                lvar[idx] = val\n-                return val\n-            else:\n-                cur = local_vars.get(m.group('out'))\n-                val = opfunc(cur, right_val)\n-                local_vars[m.group('out')] = val\n-                return val\n-\n-        if expr.isdigit():\n-            return int(expr)\n+            expr = expr[:start] + self._dump(ret, local_vars) + expr[end:]\n \n-        if expr == 'break':\n+        if not expr:\n+            return None, should_return\n+\n+        m = re.match(r'''(?x)\n+            (?P<assign>\n+                (?P<out>{_NAME_RE})(?:\\[(?P<index>[^\\]]+?)\\])?\\s*\n+                (?P<op>{_OPERATOR_RE})?\n+                =(?!=)(?P<expr>.*)$\n+            )|(?P<return>\n+                (?!if|return|true|false|null|undefined|NaN|Infinity)(?P<name>{_NAME_RE})$\n+            )|(?P<indexing>\n+                (?P<in>{_NAME_RE})\\[(?P<idx>.+)\\]$\n+            )|(?P<attribute>\n+                (?P<var>{_NAME_RE})(?:(?P<nullish>\\?)?\\.(?P<member>[^(]+)|\\[(?P<member2>[^\\]]+)\\])\\s*\n+            )|(?P<function>\n+                (?P<fname>{_NAME_RE})\\((?P<args>.*)\\)$\n+            )'''.format(**globals()), expr)\n+        md = m.groupdict() if m else {}\n+        if md.get('assign'):\n+            left_val = local_vars.get(m.group('out'))\n+\n+            if not m.group('index'):\n+                local_vars[m.group('out')] = self._operator(\n+                    m.group('op'), left_val, m.group('expr'), expr, local_vars, allow_recursion)\n+                return local_vars[m.group('out')], should_return\n+            elif left_val in (None, JS_Undefined):\n+                raise self.Exception('Cannot index undefined variable ' + m.group('out'), expr=expr)\n+\n+            idx = self.interpret_expression(m.group('index'), local_vars, allow_recursion)\n+            if not isinstance(idx, (int, float)):\n+                raise self.Exception('List index %s must be integer' % (idx, ), expr=expr)\n+            idx = int(idx)\n+            left_val[idx] = self._operator(\n+                m.group('op'), self._index(left_val, idx), m.group('expr'), expr, local_vars, allow_recursion)\n+            return left_val[idx], should_return\n+\n+        elif expr.isdigit():\n+            return int(expr), should_return\n+\n+        elif expr == 'break':\n             raise JS_Break()\n         elif expr == 'continue':\n             raise JS_Continue()\n+        elif expr == 'undefined':\n+            return JS_Undefined, should_return\n+        elif expr == 'NaN':\n+            return _NaN, should_return\n+        elif expr == 'Infinity':\n+            return _Infinity, should_return\n \n-        var_m = re.match(\n-            r'(?!if|return|true|false|null)(?P<name>%s)$' % _NAME_RE,\n-            expr)\n-        if var_m:\n-            return local_vars[var_m.group('name')]\n+        elif md.get('return'):\n+            return local_vars[m.group('name')], should_return\n \n         try:\n-            return json.loads(expr)\n+            ret = json.loads(js_to_json(expr))  # strict=True)\n+            if not md.get('attribute'):\n+                return ret, should_return\n         except ValueError:\n             pass\n \n-        m = re.match(\n-            r'(?P<in>%s)\\[(?P<idx>.+)\\]$' % _NAME_RE, expr)\n-        if m:\n+        if md.get('indexing'):\n             val = local_vars[m.group('in')]\n             idx = self.interpret_expression(m.group('idx'), local_vars, allow_recursion)\n-            return val[idx]\n-\n-        def raise_expr_error(where, op, exp):\n-            raise ExtractorError('Premature {0} return of {1} in {2!r}'.format(where, op, exp))\n-\n-        for op, opfunc in _OPERATORS:\n-            separated = list(self._separate(expr, op))\n+            return self._index(val, idx), should_return\n+\n+        for op, _ in self._all_operators():\n+            # hackety: </> have higher priority than <</>>, but don't confuse them\n+            skip_delim = (op + op) if op in '<>*?' else None\n+            if op == '?':\n+                skip_delim = (skip_delim, '?.')\n+            separated = list(self._separate(expr, op, skip_delims=skip_delim))\n             if len(separated) < 2:\n                 continue\n-            right_val = separated.pop()\n-            left_val = op.join(separated)\n-            left_val, should_abort = self.interpret_statement(\n-                left_val, local_vars, allow_recursion - 1)\n-            if should_abort:\n-                raise_expr_error('left-side', op, expr)\n-            right_val, should_abort = self.interpret_statement(\n-                right_val, local_vars, allow_recursion - 1)\n-            if should_abort:\n-                raise_expr_error('right-side', op, expr)\n-            return opfunc(left_val or 0, right_val)\n \n-        m = re.match(\n-            r'(?P<var>%s)(?:\\.(?P<member>[^(]+)|\\[(?P<member2>[^]]+)\\])\\s*' % _NAME_RE,\n-            expr)\n-        if m:\n-            variable = m.group('var')\n-            nl = Nonlocal()\n+            right_expr = separated.pop()\n+            # handle operators that are both unary and binary, minimal BODMAS\n+            if op in ('+', '-'):\n+                # simplify/adjust consecutive instances of these operators\n+                undone = 0\n+                separated = [s.strip() for s in separated]\n+                while len(separated) > 1 and not separated[-1]:\n+                    undone += 1\n+                    separated.pop()\n+                if op == '-' and undone % 2 != 0:\n+                    right_expr = op + right_expr\n+                elif op == '+':\n+                    while len(separated) > 1 and set(separated[-1]) <= self.OP_CHARS:\n+                        right_expr = separated.pop() + right_expr\n+                    if separated[-1][-1:] in self.OP_CHARS:\n+                        right_expr = separated.pop() + right_expr\n+                # hanging op at end of left => unary + (strip) or - (push right)\n+                left_val = separated[-1] if separated else ''\n+                for dm_op in ('*', '%', '/', '**'):\n+                    bodmas = tuple(self._separate(left_val, dm_op, skip_delims=skip_delim))\n+                    if len(bodmas) > 1 and not bodmas[-1].strip():\n+                        expr = op.join(separated) + op + right_expr\n+                        if len(separated) > 1:\n+                            separated.pop()\n+                            right_expr = op.join((left_val, right_expr))\n+                        else:\n+                            separated = [op.join((left_val, right_expr))]\n+                            right_expr = None\n+                        break\n+                if right_expr is None:\n+                    continue\n \n-            nl.member = remove_quotes(m.group('member') or m.group('member2'))\n+            left_val = self.interpret_expression(op.join(separated), local_vars, allow_recursion)\n+            return self._operator(op, left_val, right_expr, expr, local_vars, allow_recursion), should_return\n+\n+        if md.get('attribute'):\n+            variable, member, nullish = m.group('var', 'member', 'nullish')\n+            if not member:\n+                member = self.interpret_expression(m.group('member2'), local_vars, allow_recursion)\n             arg_str = expr[m.end():]\n             if arg_str.startswith('('):\n-                arg_str, remaining = self._separate_at_paren(arg_str, ')')\n+                arg_str, remaining = self._separate_at_paren(arg_str)\n             else:\n                 arg_str, remaining = None, arg_str\n \n             def assertion(cndn, msg):\n                 \"\"\" assert, but without risk of getting optimized out \"\"\"\n                 if not cndn:\n-                    raise ExtractorError('{0} {1}: {2}'.format(nl.member, msg, expr))\n-\n-            def eval_method():\n-                # nonlocal member\n-                member = nl.member\n-                if variable == 'String':\n-                    obj = compat_str\n-                elif variable in local_vars:\n-                    obj = local_vars[variable]\n-                else:\n-                    if variable not in self._objects:\n-                        self._objects[variable] = self.extract_object(variable)\n-                    obj = self._objects[variable]\n+                    memb = member\n+                    raise self.Exception('{memb} {msg}'.format(**locals()), expr=expr)\n+\n+            def eval_method(variable, member):\n+                if (variable, member) == ('console', 'debug'):\n+                    if Debugger.ENABLED:\n+                        Debugger.write(self.interpret_expression('[{}]'.format(arg_str), local_vars, allow_recursion))\n+                    return\n+                types = {\n+                    'String': compat_str,\n+                    'Math': float,\n+                    'Array': list,\n+                }\n+                obj = local_vars.get(variable)\n+                if obj in (JS_Undefined, None):\n+                    obj = types.get(variable, JS_Undefined)\n+                if obj is JS_Undefined:\n+                    try:\n+                        if variable not in self._objects:\n+                            self._objects[variable] = self.extract_object(variable)\n+                        obj = self._objects[variable]\n+                    except self.Exception:\n+                        if not nullish:\n+                            raise\n+\n+                if nullish and obj is JS_Undefined:\n+                    return JS_Undefined\n \n+                # Member access\n                 if arg_str is None:\n-                    # Member access\n-                    if member == 'length':\n-                        return len(obj)\n-                    return obj[member]\n+                    return self._index(obj, member, nullish)\n \n                 # Function call\n                 argvals = [\n                     self.interpret_expression(v, local_vars, allow_recursion)\n                     for v in self._separate(arg_str)]\n \n-                if obj == compat_str:\n+                # Fixup prototype call\n+                if isinstance(obj, type):\n+                    new_member, rest = member.partition('.')[0::2]\n+                    if new_member == 'prototype':\n+                        new_member, func_prototype = rest.partition('.')[0::2]\n+                        assertion(argvals, 'takes one or more arguments')\n+                        assertion(isinstance(argvals[0], obj), 'must bind to type {0}'.format(obj))\n+                        if func_prototype == 'call':\n+                            obj = argvals.pop(0)\n+                        elif func_prototype == 'apply':\n+                            assertion(len(argvals) == 2, 'takes two arguments')\n+                            obj, argvals = argvals\n+                            assertion(isinstance(argvals, list), 'second argument must be a list')\n+                        else:\n+                            raise self.Exception('Unsupported Function method ' + func_prototype, expr)\n+                        member = new_member\n+\n+                if obj is compat_str:\n                     if member == 'fromCharCode':\n                         assertion(argvals, 'takes one or more arguments')\n-                        return ''.join(map(chr, argvals))\n-                    raise ExtractorError('Unsupported string method %s' % (member, ))\n+                        return ''.join(map(compat_chr, argvals))\n+                    raise self.Exception('Unsupported string method ' + member, expr=expr)\n+                elif obj is float:\n+                    if member == 'pow':\n+                        assertion(len(argvals) == 2, 'takes two arguments')\n+                        return argvals[0] ** argvals[1]\n+                    raise self.Exception('Unsupported Math method ' + member, expr=expr)\n \n                 if member == 'split':\n                     assertion(argvals, 'takes one or more arguments')\n-                    assertion(argvals == [''], 'with arguments is not implemented')\n-                    return list(obj)\n+                    assertion(len(argvals) == 1, 'with limit argument is not implemented')\n+                    return obj.split(argvals[0]) if argvals[0] else list(obj)\n                 elif member == 'join':\n                     assertion(isinstance(obj, list), 'must be applied on a list')\n                     assertion(len(argvals) == 1, 'takes exactly one argument')\n@@ -411,18 +925,25 @@ def eval_method():\n                     obj.reverse()\n                     return obj\n                 elif member == 'slice':\n-                    assertion(isinstance(obj, list), 'must be applied on a list')\n-                    assertion(len(argvals) == 1, 'takes exactly one argument')\n-                    return obj[argvals[0]:]\n+                    assertion(isinstance(obj, (list, compat_str)), 'must be applied on a list or string')\n+                    # From [1]:\n+                    # .slice() - like [:]\n+                    # .slice(n) - like [n:] (not [slice(n)]\n+                    # .slice(m, n) - like [m:n] or [slice(m, n)]\n+                    # [1] https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/slice\n+                    assertion(len(argvals) <= 2, 'takes between 0 and 2 arguments')\n+                    if len(argvals) < 2:\n+                        argvals += (None,)\n+                    return obj[slice(*argvals)]\n                 elif member == 'splice':\n                     assertion(isinstance(obj, list), 'must be applied on a list')\n                     assertion(argvals, 'takes one or more arguments')\n-                    index, howMany = map(int, (argvals + [len(obj)])[:2])\n+                    index, how_many = map(int, (argvals + [len(obj)])[:2])\n                     if index < 0:\n                         index += len(obj)\n                     add_items = argvals[2:]\n                     res = []\n-                    for i in range(index, min(index + howMany, len(obj))):\n+                    for _ in range(index, min(index + how_many, len(obj))):\n                         res.append(obj.pop(index))\n                     for i, item in enumerate(add_items):\n                         obj.insert(index + i, item)\n@@ -447,7 +968,7 @@ def eval_method():\n                     assertion(argvals, 'takes one or more arguments')\n                     assertion(len(argvals) <= 2, 'takes at-most 2 arguments')\n                     f, this = (argvals + [''])[:2]\n-                    return [f((item, idx, obj), this=this) for idx, item in enumerate(obj)]\n+                    return [f((item, idx, obj), {'this': this}, allow_recursion) for idx, item in enumerate(obj)]\n                 elif member == 'indexOf':\n                     assertion(argvals, 'takes one or more arguments')\n                     assertion(len(argvals) <= 2, 'takes at-most 2 arguments')\n@@ -456,72 +977,116 @@ def eval_method():\n                         return obj.index(idx, start)\n                     except ValueError:\n                         return -1\n-\n-                if isinstance(obj, list):\n-                    member = int(member)\n-                    nl.member = member\n-                return obj[member](argvals)\n+                elif member == 'charCodeAt':\n+                    assertion(isinstance(obj, compat_str), 'must be applied on a string')\n+                    # assertion(len(argvals) == 1, 'takes exactly one argument') # but not enforced\n+                    idx = argvals[0] if isinstance(argvals[0], int) else 0\n+                    if idx >= len(obj):\n+                        return None\n+                    return ord(obj[idx])\n+                elif member in ('replace', 'replaceAll'):\n+                    assertion(isinstance(obj, compat_str), 'must be applied on a string')\n+                    assertion(len(argvals) == 2, 'takes exactly two arguments')\n+                    # TODO: argvals[1] callable, other Py vs JS edge cases\n+                    if isinstance(argvals[0], self.JS_RegExp):\n+                        count = 0 if argvals[0].flags & self.JS_RegExp.RE_FLAGS['g'] else 1\n+                        assertion(member != 'replaceAll' or count == 0,\n+                                  'replaceAll must be called with a global RegExp')\n+                        return argvals[0].sub(argvals[1], obj, count=count)\n+                    count = ('replaceAll', 'replace').index(member)\n+                    return re.sub(re.escape(argvals[0]), argvals[1], obj, count=count)\n+\n+                idx = int(member) if isinstance(obj, list) else member\n+                return obj[idx](argvals, allow_recursion=allow_recursion)\n \n             if remaining:\n-                return self.interpret_expression(\n-                    self._named_object(local_vars, eval_method()) + remaining,\n+                ret, should_abort = self.interpret_statement(\n+                    self._named_object(local_vars, eval_method(variable, member)) + remaining,\n                     local_vars, allow_recursion)\n+                return ret, should_return or should_abort\n             else:\n-                return eval_method()\n+                return eval_method(variable, member), should_return\n \n-        m = re.match(r'^(?P<func>%s)\\((?P<args>[a-zA-Z0-9_$,]*)\\)$' % _NAME_RE, expr)\n-        if m:\n-            fname = m.group('func')\n-            argvals = tuple([\n-                int(v) if v.isdigit() else local_vars[v]\n-                for v in self._separate(m.group('args'))])\n+        elif md.get('function'):\n+            fname = m.group('fname')\n+            argvals = [self.interpret_expression(v, local_vars, allow_recursion)\n+                       for v in self._separate(m.group('args'))]\n             if fname in local_vars:\n-                return local_vars[fname](argvals)\n+                return local_vars[fname](argvals, allow_recursion=allow_recursion), should_return\n             elif fname not in self._functions:\n                 self._functions[fname] = self.extract_function(fname)\n-            return self._functions[fname](argvals)\n+            return self._functions[fname](argvals, allow_recursion=allow_recursion), should_return\n+\n+        raise self.Exception(\n+            'Unsupported JS expression ' + (expr[:40] if expr != stmt else ''), expr=stmt)\n+\n+    def interpret_expression(self, expr, local_vars, allow_recursion):\n+        ret, should_return = self.interpret_statement(expr, local_vars, allow_recursion)\n+        if should_return:\n+            raise self.Exception('Cannot return from an expression', expr)\n+        return ret\n \n-        if expr:\n-            raise ExtractorError('Unsupported JS expression %r' % expr)\n+    def interpret_iter(self, list_txt, local_vars, allow_recursion):\n+        for v in self._separate(list_txt):\n+            yield self.interpret_expression(v, local_vars, allow_recursion)\n \n     def extract_object(self, objname):\n         _FUNC_NAME_RE = r'''(?:[a-zA-Z$0-9]+|\"[a-zA-Z$0-9]+\"|'[a-zA-Z$0-9]+')'''\n         obj = {}\n-        obj_m = re.search(\n-            r'''(?x)\n-                (?<!this\\.)%s\\s*=\\s*{\\s*\n-                    (?P<fields>(%s\\s*:\\s*function\\s*\\(.*?\\)\\s*{.*?}(?:,\\s*)?)*)\n-                }\\s*;\n-            ''' % (re.escape(objname), _FUNC_NAME_RE),\n-            self.code)\n-        fields = obj_m.group('fields')\n+        fields = next(filter(None, (\n+            obj_m.group('fields') for obj_m in re.finditer(\n+                r'''(?xs)\n+                    {0}\\s*\\.\\s*{1}|{1}\\s*=\\s*\\{{\\s*\n+                        (?P<fields>({2}\\s*:\\s*function\\s*\\(.*?\\)\\s*\\{{.*?}}(?:,\\s*)?)*)\n+                    }}\\s*;\n+                '''.format(_NAME_RE, re.escape(objname), _FUNC_NAME_RE),\n+                self.code))), None)\n+        if not fields:\n+            raise self.Exception('Could not find object ' + objname)\n         # Currently, it only supports function definitions\n-        fields_m = re.finditer(\n-            r'''(?x)\n-                (?P<key>%s)\\s*:\\s*function\\s*\\((?P<args>[a-z,]+)\\){(?P<code>[^}]+)}\n-            ''' % _FUNC_NAME_RE,\n-            fields)\n-        for f in fields_m:\n-            argnames = f.group('args').split(',')\n-            obj[remove_quotes(f.group('key'))] = self.build_function(argnames, f.group('code'))\n+        for f in re.finditer(\n+                r'''(?x)\n+                    (?P<key>%s)\\s*:\\s*function\\s*\\((?P<args>(?:%s|,)*)\\){(?P<code>[^}]+)}\n+                ''' % (_FUNC_NAME_RE, _NAME_RE),\n+                fields):\n+            argnames = self.build_arglist(f.group('args'))\n+            name = remove_quotes(f.group('key'))\n+            obj[name] = function_with_repr(self.build_function(argnames, f.group('code')), 'F<{0}>'.format(name))\n \n         return obj\n \n+    @staticmethod\n+    def _offset_e_by_d(d, e, local_vars):\n+        \"\"\" Short-cut eval: (d%e.length+e.length)%e.length \"\"\"\n+        try:\n+            d = local_vars[d]\n+            e = local_vars[e]\n+            e = len(e)\n+            return _js_mod(_js_mod(d, e) + e, e), False\n+        except Exception:\n+            return None, True\n+\n     def extract_function_code(self, funcname):\n         \"\"\" @returns argnames, code \"\"\"\n         func_m = re.search(\n-            r'''(?x)\n-                (?:function\\s+%(f_n)s|[{;,]\\s*%(f_n)s\\s*=\\s*function|var\\s+%(f_n)s\\s*=\\s*function)\\s*\n+            r'''(?xs)\n+                (?:\n+                    function\\s+%(name)s|\n+                    [{;,]\\s*%(name)s\\s*=\\s*function|\n+                    (?:var|const|let)\\s+%(name)s\\s*=\\s*function\n+                )\\s*\n                 \\((?P<args>[^)]*)\\)\\s*\n-                (?P<code>\\{(?:(?!};)[^\"]|\"([^\"]|\\\\\")*\")+\\})''' % {'f_n': re.escape(funcname), },\n+                (?P<code>{.+})''' % {'name': re.escape(funcname)},\n             self.code)\n-        code, _ = self._separate_at_paren(func_m.group('code'), '}')  # refine the match\n         if func_m is None:\n-            raise ExtractorError('Could not find JS function %r' % funcname)\n-        return func_m.group('args').split(','), code\n+            raise self.Exception('Could not find JS function \"{funcname}\"'.format(**locals()))\n+        code, _ = self._separate_at_paren(func_m.group('code'))  # refine the match\n+        return self.build_arglist(func_m.group('args')), code\n \n     def extract_function(self, funcname):\n-        return self.extract_function_from_code(*self.extract_function_code(funcname))\n+        return function_with_repr(\n+            self.extract_function_from_code(*self.extract_function_code(funcname)),\n+            'F<%s>' % (funcname,))\n \n     def extract_function_from_code(self, argnames, code, *global_stack):\n         local_vars = {}\n@@ -530,29 +1095,38 @@ def extract_function_from_code(self, argnames, code, *global_stack):\n             if mobj is None:\n                 break\n             start, body_start = mobj.span()\n-            body, remaining = self._separate_at_paren(code[body_start - 1:], '}')\n-            name = self._named_object(\n-                local_vars,\n-                self.extract_function_from_code(\n-                    [x.strip() for x in mobj.group('args').split(',')],\n-                    body, local_vars, *global_stack))\n+            body, remaining = self._separate_at_paren(code[body_start - 1:])\n+            name = self._named_object(local_vars, self.extract_function_from_code(\n+                [x.strip() for x in mobj.group('args').split(',')],\n+                body, local_vars, *global_stack))\n             code = code[:start] + name + remaining\n         return self.build_function(argnames, code, local_vars, *global_stack)\n \n     def call_function(self, funcname, *args):\n         return self.extract_function(funcname)(args)\n \n+    @classmethod\n+    def build_arglist(cls, arg_text):\n+        if not arg_text:\n+            return []\n+\n+        def valid_arg(y):\n+            y = y.strip()\n+            if not y:\n+                raise cls.Exception('Missing arg in \"%s\"' % (arg_text, ))\n+            return y\n+\n+        return [valid_arg(x) for x in cls._separate(arg_text)]\n+\n     def build_function(self, argnames, code, *global_stack):\n         global_stack = list(global_stack) or [{}]\n-        local_vars = global_stack.pop(0)\n-\n-        def resf(args, **kwargs):\n-            local_vars.update(dict(zip(argnames, args)))\n-            local_vars.update(kwargs)\n-            var_stack = LocalNameSpace(local_vars, *global_stack)\n-            for stmt in self._separate(code.replace('\\n', ''), ';'):\n-                ret, should_abort = self.interpret_statement(stmt, var_stack)\n-                if should_abort:\n-                    break\n-            return ret\n+        argnames = tuple(argnames)\n+\n+        def resf(args, kwargs={}, allow_recursion=100):\n+            global_stack[0].update(zip_longest(argnames, args, fillvalue=None))\n+            global_stack[0].update(kwargs)\n+            var_stack = LocalNameSpace(*global_stack)\n+            ret, should_abort = self.interpret_statement(code.replace('\\n', ' '), var_stack, allow_recursion - 1)\n+            if should_abort:\n+                return ret\n         return resf\ndiff --git a/youtube_dl/options.py b/youtube_dl/options.py\nindex 0a0641bd481..61705d1f023 100644\n--- a/youtube_dl/options.py\n+++ b/youtube_dl/options.py\n@@ -11,6 +11,7 @@\n     compat_get_terminal_size,\n     compat_getenv,\n     compat_kwargs,\n+    compat_open as open,\n     compat_shlex_split,\n )\n from .utils import (\n@@ -41,14 +42,11 @@ def _scrub_eq(o):\n def parseOpts(overrideArguments=None):\n     def _readOptions(filename_bytes, default=[]):\n         try:\n-            optionf = open(filename_bytes)\n+            optionf = open(filename_bytes, encoding=preferredencoding())\n         except IOError:\n             return default  # silently skip if file is not present\n         try:\n-            # FIXME: https://github.com/ytdl-org/youtube-dl/commit/dfe5fa49aed02cf36ba9f743b11b0903554b5e56\n             contents = optionf.read()\n-            if sys.version_info < (3,):\n-                contents = contents.decode(preferredencoding())\n             res = compat_shlex_split(contents, comments=True)\n         finally:\n             optionf.close()\n@@ -270,11 +268,11 @@ def _comma_separated_values_options_callback(option, opt_str, value, parser):\n     selection.add_option(\n         '--match-title',\n         dest='matchtitle', metavar='REGEX',\n-        help='Download only matching titles (regex or caseless sub-string)')\n+        help='Download only matching titles (case-insensitive regex or alphanumeric sub-string)')\n     selection.add_option(\n         '--reject-title',\n         dest='rejecttitle', metavar='REGEX',\n-        help='Skip download for matching titles (regex or caseless sub-string)')\n+        help='Skip download for matching titles (case-insensitive regex or alphanumeric sub-string)')\n     selection.add_option(\n         '--max-downloads',\n         dest='max_downloads', metavar='NUMBER', type=int, default=None,\n@@ -535,6 +533,10 @@ def _comma_separated_values_options_callback(option, opt_str, value, parser):\n         '--no-check-certificate',\n         action='store_true', dest='no_check_certificate', default=False,\n         help='Suppress HTTPS certificate validation')\n+    workarounds.add_option(\n+        '--no-check-extensions',\n+        action='store_true', dest='no_check_extensions', default=False,\n+        help='Suppress file extension validation')\n     workarounds.add_option(\n         '--prefer-insecure',\n         '--prefer-unsecure', action='store_true', dest='prefer_insecure',\n@@ -546,12 +548,14 @@ def _comma_separated_values_options_callback(option, opt_str, value, parser):\n     workarounds.add_option(\n         '--referer',\n         metavar='URL', dest='referer', default=None,\n-        help='Specify a custom referer, use if the video access is restricted to one domain',\n+        help='Specify a custom Referer: use if the video access is restricted to one domain',\n     )\n     workarounds.add_option(\n         '--add-header',\n         metavar='FIELD:VALUE', dest='headers', action='append',\n-        help='Specify a custom HTTP header and its value, separated by a colon \\':\\'. You can use this option multiple times',\n+        help=('Specify a custom HTTP header and its value, separated by a colon \\':\\'. You can use this option multiple times. '\n+              'NB Use --cookies rather than adding a Cookie header if its contents may be sensitive; '\n+              'data from a Cookie header will be sent to all domains, not just the one intended')\n     )\n     workarounds.add_option(\n         '--bidi-workaround',\n@@ -733,9 +737,13 @@ def _comma_separated_values_options_callback(option, opt_str, value, parser):\n         '--no-part',\n         action='store_true', dest='nopart', default=False,\n         help='Do not use .part files - write directly into output file')\n+    filesystem.add_option(\n+        '--mtime',\n+        action='store_true', dest='updatetime', default=True,\n+        help='Use the Last-modified header to set the file modification time (default)')\n     filesystem.add_option(\n         '--no-mtime',\n-        action='store_false', dest='updatetime', default=True,\n+        action='store_false', dest='updatetime',\n         help='Do not use the Last-modified header to set the file modification time')\n     filesystem.add_option(\n         '--write-description',\n@@ -801,7 +809,7 @@ def _comma_separated_values_options_callback(option, opt_str, value, parser):\n     postproc.add_option(\n         '--postprocessor-args',\n         dest='postprocessor_args', metavar='ARGS',\n-        help='Give these arguments to the postprocessor')\n+        help='Give these arguments to the postprocessor (if postprocessing is required)')\n     postproc.add_option(\n         '-k', '--keep-video',\n         action='store_true', dest='keepvideo', default=False,\ndiff --git a/youtube_dl/postprocessor/embedthumbnail.py b/youtube_dl/postprocessor/embedthumbnail.py\nindex 3990908b691..b6c60e127ee 100644\n--- a/youtube_dl/postprocessor/embedthumbnail.py\n+++ b/youtube_dl/postprocessor/embedthumbnail.py\n@@ -13,10 +13,13 @@\n     encodeFilename,\n     PostProcessingError,\n     prepend_extension,\n+    process_communicate_or_kill,\n     replace_extension,\n-    shell_quote\n+    shell_quote,\n )\n \n+from ..compat import compat_open as open\n+\n \n class EmbedThumbnailPPError(PostProcessingError):\n     pass\n@@ -109,7 +112,7 @@ def is_webp(path):\n                 self._downloader.to_screen('[debug] AtomicParsley command line: %s' % shell_quote(cmd))\n \n             p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n-            stdout, stderr = p.communicate()\n+            stdout, stderr = process_communicate_or_kill(p)\n \n             if p.returncode != 0:\n                 msg = stderr.decode('utf-8', 'replace').strip()\ndiff --git a/youtube_dl/postprocessor/ffmpeg.py b/youtube_dl/postprocessor/ffmpeg.py\nindex 9f76c9d4ed2..214825aa971 100644\n--- a/youtube_dl/postprocessor/ffmpeg.py\n+++ b/youtube_dl/postprocessor/ffmpeg.py\n@@ -1,6 +1,5 @@\n from __future__ import unicode_literals\n \n-import io\n import os\n import subprocess\n import time\n@@ -9,6 +8,7 @@\n \n from .common import AudioConversionError, PostProcessor\n \n+from ..compat import compat_open as open\n from ..utils import (\n     encodeArgument,\n     encodeFilename,\n@@ -16,6 +16,7 @@\n     is_outdated_version,\n     PostProcessingError,\n     prepend_extension,\n+    process_communicate_or_kill,\n     shell_quote,\n     subtitles_filename,\n     dfxp2srt,\n@@ -73,8 +74,11 @@ def get_versions(downloader=None):\n         return FFmpegPostProcessor(downloader)._versions\n \n     def _determine_executables(self):\n-        programs = ['avprobe', 'avconv', 'ffmpeg', 'ffprobe']\n+        # ordered to match prefer_ffmpeg!\n+        convs = ['ffmpeg', 'avconv']\n+        probes = ['ffprobe', 'avprobe']\n         prefer_ffmpeg = True\n+        programs = convs + probes\n \n         def get_ffmpeg_version(path):\n             ver = get_exe_version(path, args=['-version'])\n@@ -95,6 +99,7 @@ def get_ffmpeg_version(path):\n \n         self._paths = None\n         self._versions = None\n+        location = None\n         if self._downloader:\n             prefer_ffmpeg = self._downloader.params.get('prefer_ffmpeg', True)\n             location = self._downloader.params.get('ffmpeg_location')\n@@ -117,33 +122,21 @@ def get_ffmpeg_version(path):\n                     location = os.path.dirname(os.path.abspath(location))\n                     if basename in ('ffmpeg', 'ffprobe'):\n                         prefer_ffmpeg = True\n-\n-                self._paths = dict(\n-                    (p, os.path.join(location, p)) for p in programs)\n-                self._versions = dict(\n-                    (p, get_ffmpeg_version(self._paths[p])) for p in programs)\n-        if self._versions is None:\n-            self._versions = dict(\n-                (p, get_ffmpeg_version(p)) for p in programs)\n-            self._paths = dict((p, p) for p in programs)\n-\n-        if prefer_ffmpeg is False:\n-            prefs = ('avconv', 'ffmpeg')\n-        else:\n-            prefs = ('ffmpeg', 'avconv')\n-        for p in prefs:\n-            if self._versions[p]:\n-                self.basename = p\n-                break\n-\n-        if prefer_ffmpeg is False:\n-            prefs = ('avprobe', 'ffprobe')\n-        else:\n-            prefs = ('ffprobe', 'avprobe')\n-        for p in prefs:\n-            if self._versions[p]:\n-                self.probe_basename = p\n-                break\n+        self._paths = dict(\n+            (p, p if location is None else os.path.join(location, p))\n+            for p in programs)\n+        self._versions = dict(\n+            x for x in (\n+                (p, get_ffmpeg_version(self._paths[p])) for p in programs)\n+            if x[1] is not None)\n+\n+        basenames = [None, None]\n+        for i, progs in enumerate((convs, probes)):\n+            for p in progs[::-1 if prefer_ffmpeg is False else 1]:\n+                if self._versions.get(p):\n+                    basenames[i] = p\n+                    break\n+        self.basename, self.probe_basename = basenames\n \n     @property\n     def available(self):\n@@ -180,7 +173,7 @@ def get_audio_codec(self, path):\n             handle = subprocess.Popen(\n                 cmd, stderr=subprocess.PIPE,\n                 stdout=subprocess.PIPE, stdin=subprocess.PIPE)\n-            stdout_data, stderr_data = handle.communicate()\n+            stdout_data, stderr_data = process_communicate_or_kill(handle)\n             expected_ret = 0 if self.probe_available else 1\n             if handle.wait() != expected_ret:\n                 return None\n@@ -228,7 +221,7 @@ def run_ffmpeg_multiple_files(self, input_paths, out_path, opts):\n         if self._downloader.params.get('verbose', False):\n             self._downloader.to_screen('[debug] ffmpeg command line: %s' % shell_quote(cmd))\n         p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE)\n-        stdout, stderr = p.communicate()\n+        stdout, stderr = process_communicate_or_kill(p)\n         if p.returncode != 0:\n             stderr = stderr.decode('utf-8', 'replace')\n             msgs = stderr.strip().split('\\n')\n@@ -492,7 +485,7 @@ def add(meta_list, info_list=None):\n         chapters = info.get('chapters', [])\n         if chapters:\n             metadata_filename = replace_extension(filename, 'meta')\n-            with io.open(metadata_filename, 'wt', encoding='utf-8') as f:\n+            with open(metadata_filename, 'w', encoding='utf-8') as f:\n                 def ffmpeg_escape(text):\n                     return re.sub(r'(=|;|#|\\\\|\\n)', r'\\\\\\1', text)\n \n@@ -635,7 +628,7 @@ def run(self, info):\n                 with open(dfxp_file, 'rb') as f:\n                     srt_data = dfxp2srt(f.read())\n \n-                with io.open(srt_file, 'wt', encoding='utf-8') as f:\n+                with open(srt_file, 'w', encoding='utf-8') as f:\n                     f.write(srt_data)\n                 old_file = srt_file\n \n@@ -651,7 +644,7 @@ def run(self, info):\n \n             self.run_ffmpeg(old_file, new_file, ['-f', new_format])\n \n-            with io.open(new_file, 'rt', encoding='utf-8') as f:\n+            with open(new_file, 'r', encoding='utf-8') as f:\n                 subs[lang] = {\n                     'ext': new_ext,\n                     'data': f.read(),\ndiff --git a/youtube_dl/postprocessor/metadatafromtitle.py b/youtube_dl/postprocessor/metadatafromtitle.py\nindex f5c14d974f4..6cd5bb70f3a 100644\n--- a/youtube_dl/postprocessor/metadatafromtitle.py\n+++ b/youtube_dl/postprocessor/metadatafromtitle.py\n@@ -40,6 +40,8 @@ def run(self, info):\n                 % self._titleformat)\n             return [], info\n         for attribute, value in match.groupdict().items():\n+            if value is None:\n+                continue\n             info[attribute] = value\n             self._downloader.to_screen(\n                 '[fromtitle] parsed %s: %s'\ndiff --git a/youtube_dl/swfinterp.py b/youtube_dl/swfinterp.py\nindex 0c715857531..e79e0b17f8a 100644\n--- a/youtube_dl/swfinterp.py\n+++ b/youtube_dl/swfinterp.py\n@@ -727,7 +727,7 @@ def resfunc(args):\n                             stack.append(res)\n                             continue\n \n-                        assert isinstance(obj, (dict, _ScopeDict)),\\\n+                        assert isinstance(obj, (dict, _ScopeDict)), \\\n                             'Accessing member %r on %r' % (pname, obj)\n                         res = obj.get(pname, undefined)\n                         stack.append(res)\ndiff --git a/youtube_dl/traversal.py b/youtube_dl/traversal.py\nnew file mode 100644\nindex 00000000000..834cfef7fab\n--- /dev/null\n+++ b/youtube_dl/traversal.py\n@@ -0,0 +1,10 @@\n+# coding: utf-8\n+\n+# TODO: move these utils.fns here and move import to utils\n+# flake8: noqa\n+from .utils import (\n+    dict_get,\n+    get_first,\n+    T,\n+    traverse_obj,\n+)\ndiff --git a/youtube_dl/update.py b/youtube_dl/update.py\nindex 84c9646171e..a147b525311 100644\n--- a/youtube_dl/update.py\n+++ b/youtube_dl/update.py\n@@ -1,6 +1,5 @@\n from __future__ import unicode_literals\n \n-import io\n import json\n import traceback\n import hashlib\n@@ -9,7 +8,10 @@\n import sys\n from zipimport import zipimporter\n \n-from .compat import compat_realpath\n+from .compat import (\n+    compat_open as open,\n+    compat_realpath,\n+)\n from .utils import encode_compat_str\n \n from .version import __version__\n@@ -127,7 +129,7 @@ def version_tuple(version_str):\n \n         try:\n             bat = os.path.join(directory, 'youtube-dl-updater.bat')\n-            with io.open(bat, 'w') as batfile:\n+            with open(bat, 'w') as batfile:\n                 batfile.write('''\n @echo off\n echo Waiting for file handle to be closed ...\ndiff --git a/youtube_dl/utils.py b/youtube_dl/utils.py\nindex e722eed58de..ac1e78002b3 100644\n--- a/youtube_dl/utils.py\n+++ b/youtube_dl/utils.py\n@@ -15,7 +15,7 @@\n import email.header\n import errno\n import functools\n-import gzip\n+import inspect\n import io\n import itertools\n import json\n@@ -33,37 +33,48 @@\n import tempfile\n import time\n import traceback\n+import unicodedata\n import xml.etree.ElementTree\n import zlib\n \n from .compat import (\n     compat_HTMLParseError,\n     compat_HTMLParser,\n-    compat_HTTPError,\n     compat_basestring,\n+    compat_brotli as brotli,\n+    compat_casefold,\n     compat_chr,\n+    compat_collections_abc,\n+    compat_contextlib_suppress,\n     compat_cookiejar,\n     compat_ctypes_WINFUNCTYPE,\n+    compat_datetime_timedelta_total_seconds,\n+    compat_etree_Element,\n     compat_etree_fromstring,\n+    compat_etree_iterfind,\n     compat_expanduser,\n     compat_html_entities,\n     compat_html_entities_html5,\n     compat_http_client,\n+    compat_http_cookies,\n     compat_integer_types,\n     compat_kwargs,\n+    compat_ncompress as ncompress,\n     compat_os_name,\n-    compat_parse_qs,\n+    compat_re_Match,\n+    compat_re_Pattern,\n     compat_shlex_quote,\n     compat_str,\n     compat_struct_pack,\n     compat_struct_unpack,\n     compat_urllib_error,\n+    compat_urllib_HTTPError,\n     compat_urllib_parse,\n+    compat_urllib_parse_parse_qs as compat_parse_qs,\n     compat_urllib_parse_urlencode,\n     compat_urllib_parse_urlparse,\n     compat_urllib_parse_unquote_plus,\n     compat_urllib_request,\n-    compat_urlparse,\n     compat_xpath,\n )\n \n@@ -78,12 +89,12 @@ def register_socks_protocols():\n     # In Python < 2.6.5, urlsplit() suffers from bug https://bugs.python.org/issue7904\n     # URLs with protocols not in urlparse.uses_netloc are not handled correctly\n     for scheme in ('socks', 'socks4', 'socks4a', 'socks5'):\n-        if scheme not in compat_urlparse.uses_netloc:\n-            compat_urlparse.uses_netloc.append(scheme)\n+        if scheme not in compat_urllib_parse.uses_netloc:\n+            compat_urllib_parse.uses_netloc.append(scheme)\n \n \n-# This is not clearly defined otherwise\n-compiled_regex_type = type(re.compile(''))\n+# Unfavoured alias\n+compiled_regex_type = compat_re_Pattern\n \n \n def random_user_agent():\n@@ -1671,9 +1682,7 @@ def random_user_agent():\n \n std_headers = {\n     'User-Agent': random_user_agent(),\n-    'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.7',\n     'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n-    'Accept-Encoding': 'gzip, deflate',\n     'Accept-Language': 'en-us,en;q=0.5',\n }\n \n@@ -1684,6 +1693,7 @@ def random_user_agent():\n \n \n NO_DEFAULT = object()\n+IDENTITY = lambda x: x\n \n ENGLISH_MONTH_NAMES = [\n     'January', 'February', 'March', 'April', 'May', 'June',\n@@ -1696,20 +1706,16 @@ def random_user_agent():\n         'juillet', 'ao\u00fbt', 'septembre', 'octobre', 'novembre', 'd\u00e9cembre'],\n }\n \n-KNOWN_EXTENSIONS = (\n-    'mp4', 'm4a', 'm4p', 'm4b', 'm4r', 'm4v', 'aac',\n-    'flv', 'f4v', 'f4a', 'f4b',\n-    'webm', 'ogg', 'ogv', 'oga', 'ogx', 'spx', 'opus',\n-    'mkv', 'mka', 'mk3d',\n-    'avi', 'divx',\n-    'mov',\n-    'asf', 'wmv', 'wma',\n-    '3gp', '3g2',\n-    'mp3',\n-    'flac',\n-    'ape',\n-    'wav',\n-    'f4f', 'f4m', 'm3u8', 'smil')\n+# Timezone names for RFC2822 obs-zone\n+# From https://github.com/python/cpython/blob/3.11/Lib/email/_parseaddr.py#L36-L42\n+TIMEZONE_NAMES = {\n+    'UT': 0, 'UTC': 0, 'GMT': 0, 'Z': 0,\n+    'AST': -4, 'ADT': -3,  # Atlantic (used in Canada)\n+    'EST': -5, 'EDT': -4,  # Eastern\n+    'CST': -6, 'CDT': -5,  # Central\n+    'MST': -7, 'MDT': -6,  # Mountain\n+    'PST': -8, 'PDT': -7   # Pacific\n+}\n \n # needed for sanitizing filenames in restricted mode\n ACCENT_CHARS = dict(zip('\u00c2\u00c3\u00c4\u00c0\u00c1\u00c5\u00c6\u00c7\u00c8\u00c9\u00ca\u00cb\u00cc\u00cd\u00ce\u00cf\u00d0\u00d1\u00d2\u00d3\u00d4\u00d5\u00d6\u0150\u00d8\u0152\u00d9\u00da\u00db\u00dc\u0170\u00dd\u00de\u00df\u00e0\u00e1\u00e2\u00e3\u00e4\u00e5\u00e6\u00e7\u00e8\u00e9\u00ea\u00eb\u00ec\u00ed\u00ee\u00ef\u00f0\u00f1\u00f2\u00f3\u00f4\u00f5\u00f6\u0151\u00f8\u0153\u00f9\u00fa\u00fb\u00fc\u0171\u00fd\u00fe\u00ff',\n@@ -1735,12 +1741,17 @@ def random_user_agent():\n     '%b %dth %Y %I:%M',\n     '%Y %m %d',\n     '%Y-%m-%d',\n+    '%Y.%m.%d.',\n     '%Y/%m/%d',\n     '%Y/%m/%d %H:%M',\n     '%Y/%m/%d %H:%M:%S',\n+    '%Y%m%d%H%M',\n+    '%Y%m%d%H%M%S',\n+    '%Y%m%d',\n     '%Y-%m-%d %H:%M',\n     '%Y-%m-%d %H:%M:%S',\n     '%Y-%m-%d %H:%M:%S.%f',\n+    '%Y-%m-%d %H:%M:%S:%f',\n     '%d.%m.%Y %H:%M',\n     '%d.%m.%Y %H.%M',\n     '%Y-%m-%dT%H:%M:%SZ',\n@@ -1753,6 +1764,7 @@ def random_user_agent():\n     '%b %d %Y at %H:%M:%S',\n     '%B %d %Y at %H:%M',\n     '%B %d %Y at %H:%M:%S',\n+    '%H:%M %d-%b-%Y',\n )\n \n DATE_FORMATS_DAY_FIRST = list(DATE_FORMATS)\n@@ -1763,6 +1775,7 @@ def random_user_agent():\n     '%d/%m/%Y',\n     '%d/%m/%y',\n     '%d/%m/%Y %H:%M:%S',\n+    '%d-%m-%Y %H:%M',\n ])\n \n DATE_FORMATS_MONTH_FIRST = list(DATE_FORMATS)\n@@ -1800,11 +1813,11 @@ def write_json_file(obj, fn):\n     if sys.version_info < (3, 0) and sys.platform != 'win32':\n         encoding = get_filesystem_encoding()\n         # os.path.basename returns a bytes object, but NamedTemporaryFile\n-        # will fail if the filename contains non ascii characters unless we\n+        # will fail if the filename contains non-ascii characters unless we\n         # use a unicode object\n-        path_basename = lambda f: os.path.basename(fn).decode(encoding)\n+        path_basename = lambda f: os.path.basename(f).decode(encoding)\n         # the same for os.path.dirname\n-        path_dirname = lambda f: os.path.dirname(fn).decode(encoding)\n+        path_dirname = lambda f: os.path.dirname(f).decode(encoding)\n     else:\n         path_basename = os.path.basename\n         path_dirname = os.path.dirname\n@@ -1831,25 +1844,18 @@ def write_json_file(obj, fn):\n     try:\n         with tf:\n             json.dump(obj, tf)\n-        if sys.platform == 'win32':\n-            # Need to remove existing file on Windows, else os.rename raises\n-            # WindowsError or FileExistsError.\n-            try:\n+        with compat_contextlib_suppress(OSError):\n+            if sys.platform == 'win32':\n+                # Need to remove existing file on Windows, else os.rename raises\n+                # WindowsError or FileExistsError.\n                 os.unlink(fn)\n-            except OSError:\n-                pass\n-        try:\n             mask = os.umask(0)\n             os.umask(mask)\n             os.chmod(tf.name, 0o666 & ~mask)\n-        except OSError:\n-            pass\n         os.rename(tf.name, fn)\n     except Exception:\n-        try:\n+        with compat_contextlib_suppress(OSError):\n             os.remove(tf.name)\n-        except OSError:\n-            pass\n         raise\n \n \n@@ -1868,10 +1874,10 @@ def find_xpath_attr(node, xpath, key, val=None):\n                 return f\n         return None\n \n+\n # On python2.6 the xml.etree.ElementTree.Element methods don't support\n # the namespace parameter\n \n-\n def xpath_with_ns(path, ns_map):\n     components = [c.split(':') for c in path.split('/')]\n     replaced = []\n@@ -1888,7 +1894,7 @@ def xpath_element(node, xpath, name=None, fatal=False, default=NO_DEFAULT):\n     def _find_xpath(xpath):\n         return node.find(compat_xpath(xpath))\n \n-    if isinstance(xpath, (str, compat_str)):\n+    if isinstance(xpath, compat_basestring):\n         n = _find_xpath(xpath)\n     else:\n         for xp in xpath:\n@@ -2009,14 +2015,13 @@ def extract_attributes(html_element):\n     NB HTMLParser is stricter in Python 2.6 & 3.2 than in later versions,\n     but the cases in the unit test will work for all of 2.6, 2.7, 3.2-3.5.\n     \"\"\"\n-    parser = HTMLAttributeParser()\n-    try:\n-        parser.feed(html_element)\n-        parser.close()\n-    # Older Python may throw HTMLParseError in case of malformed HTML\n-    except compat_HTMLParseError:\n-        pass\n-    return parser.attrs\n+    ret = None\n+    # Older Python may throw HTMLParseError in case of malformed HTML (and on .close()!)\n+    with compat_contextlib_suppress(compat_HTMLParseError):\n+        with contextlib.closing(HTMLAttributeParser()) as parser:\n+            parser.feed(html_element)\n+            ret = parser.attrs\n+    return ret or {}\n \n \n def clean_html(html):\n@@ -2097,9 +2102,13 @@ def replace_insane(char):\n         if restricted and (char in '!&\\'()[]{}$;`^,#' or char.isspace()):\n             return '_'\n         if restricted and ord(char) > 127:\n-            return '_'\n+            return '' if unicodedata.category(char)[0] in 'CM' else '_'\n+\n         return char\n \n+    # Replace look-alike Unicode glyphs\n+    if restricted and not is_id:\n+        s = unicodedata.normalize('NFKC', s)\n     # Handle timestamps\n     s = re.sub(r'[0-9]+(?::[0-9]+)+', lambda m: m.group(0).replace(':', '_'), s)\n     result = ''.join(map(replace_insane, s))\n@@ -2154,8 +2163,28 @@ def sanitize_url(url):\n     return url\n \n \n+def extract_basic_auth(url):\n+    parts = compat_urllib_parse.urlsplit(url)\n+    if parts.username is None:\n+        return url, None\n+    url = compat_urllib_parse.urlunsplit(parts._replace(netloc=(\n+        parts.hostname if parts.port is None\n+        else '%s:%d' % (parts.hostname, parts.port))))\n+    auth_payload = base64.b64encode(\n+        ('%s:%s' % (parts.username, parts.password or '')).encode('utf-8'))\n+    return url, 'Basic {0}'.format(auth_payload.decode('ascii'))\n+\n+\n def sanitized_Request(url, *args, **kwargs):\n-    return compat_urllib_request.Request(sanitize_url(url), *args, **kwargs)\n+    url, auth_header = extract_basic_auth(escape_url(sanitize_url(url)))\n+    if auth_header is not None:\n+        headers = args[1] if len(args) > 1 else kwargs.get('headers')\n+        headers = headers or {}\n+        headers['Authorization'] = auth_header\n+        if len(args) <= 1 and kwargs.get('headers') is None:\n+            kwargs['headers'] = headers\n+            kwargs = compat_kwargs(kwargs)\n+    return compat_urllib_request.Request(url, *args, **kwargs)\n \n \n def expand_path(s):\n@@ -2193,7 +2222,8 @@ def _htmlentity_transform(entity_with_semicolon):\n             numstr = '0%s' % numstr\n         else:\n             base = 10\n-        # See https://github.com/ytdl-org/youtube-dl/issues/7518\n+        # See https://github.com/ytdl-org/youtube-dl/issues/7518\\\n+        # Also, weirdly, compat_contextlib_suppress fails here in 2.6\n         try:\n             return compat_chr(int(numstr, base))\n         except ValueError:\n@@ -2206,12 +2236,21 @@ def _htmlentity_transform(entity_with_semicolon):\n def unescapeHTML(s):\n     if s is None:\n         return None\n-    assert type(s) == compat_str\n+    assert isinstance(s, compat_str)\n \n     return re.sub(\n         r'&([^&;]+;)', lambda m: _htmlentity_transform(m.group(1)), s)\n \n \n+def process_communicate_or_kill(p, *args, **kwargs):\n+    try:\n+        return p.communicate(*args, **kwargs)\n+    except BaseException:  # Including KeyboardInterrupt\n+        p.kill()\n+        p.wait()\n+        raise\n+\n+\n def get_subprocess_encoding():\n     if sys.platform == 'win32' and sys.getwindowsversion()[0] >= 5:\n         # For subprocess calls, encode with locale encoding\n@@ -2224,39 +2263,32 @@ def get_subprocess_encoding():\n     return encoding\n \n \n-def encodeFilename(s, for_subprocess=False):\n-    \"\"\"\n-    @param s The name of the file\n-    \"\"\"\n-\n-    assert type(s) == compat_str\n-\n-    # Python 3 has a Unicode API\n-    if sys.version_info >= (3, 0):\n-        return s\n-\n-    # Pass '' directly to use Unicode APIs on Windows 2000 and up\n-    # (Detecting Windows NT 4 is tricky because 'major >= 4' would\n-    # match Windows 9x series as well. Besides, NT 4 is obsolete.)\n-    if not for_subprocess and sys.platform == 'win32' and sys.getwindowsversion()[0] >= 5:\n-        return s\n-\n-    # Jython assumes filenames are Unicode strings though reported as Python 2.x compatible\n-    if sys.platform.startswith('java'):\n-        return s\n+# Jython assumes filenames are Unicode strings though reported as Python 2.x compatible\n+if sys.version_info < (3, 0) and not sys.platform.startswith('java'):\n \n-    return s.encode(get_subprocess_encoding(), 'ignore')\n+    def encodeFilename(s, for_subprocess=False):\n+        \"\"\"\n+        @param s The name of the file\n+        \"\"\"\n \n+        # Pass '' directly to use Unicode APIs on Windows 2000 and up\n+        # (Detecting Windows NT 4 is tricky because 'major >= 4' would\n+        # match Windows 9x series as well. Besides, NT 4 is obsolete.)\n+        if (not for_subprocess\n+                and sys.platform == 'win32'\n+                and sys.getwindowsversion()[0] >= 5\n+                and isinstance(s, compat_str)):\n+            return s\n \n-def decodeFilename(b, for_subprocess=False):\n+        return _encode_compat_str(s, get_subprocess_encoding(), 'ignore')\n \n-    if sys.version_info >= (3, 0):\n-        return b\n+    def decodeFilename(b, for_subprocess=False):\n+        return _decode_compat_str(b, get_subprocess_encoding(), 'ignore')\n \n-    if not isinstance(b, bytes):\n-        return b\n+else:\n \n-    return b.decode(get_subprocess_encoding(), 'ignore')\n+    # Python 3 has a Unicode API\n+    encodeFilename = decodeFilename = lambda *s, **k: s[0]\n \n \n def encodeArgument(s):\n@@ -2275,11 +2307,7 @@ def decodeArgument(b):\n def decodeOption(optval):\n     if optval is None:\n         return optval\n-    if isinstance(optval, bytes):\n-        optval = optval.decode(preferredencoding())\n-\n-    assert isinstance(optval, compat_str)\n-    return optval\n+    return _decode_compat_str(optval)\n \n \n def formatSeconds(secs):\n@@ -2292,39 +2320,63 @@ def formatSeconds(secs):\n \n \n def make_HTTPS_handler(params, **kwargs):\n+\n+    # https://www.rfc-editor.org/info/rfc7301\n+    ALPN_PROTOCOLS = ['http/1.1']\n+\n+    def set_alpn_protocols(ctx):\n+        # From https://github.com/yt-dlp/yt-dlp/commit/2c6dcb65fb612fc5bc5c61937bf438d3c473d8d0\n+        # Thanks @coletdjnz\n+        # Some servers may (wrongly) reject requests if ALPN extension is not sent. See:\n+        # https://github.com/python/cpython/issues/85140\n+        # https://github.com/yt-dlp/yt-dlp/issues/3878\n+        with compat_contextlib_suppress(AttributeError, NotImplementedError):\n+            # fails for Python < 2.7.10, not ssl.HAS_ALPN\n+            ctx.set_alpn_protocols(ALPN_PROTOCOLS)\n+\n     opts_no_check_certificate = params.get('nocheckcertificate', False)\n     if hasattr(ssl, 'create_default_context'):  # Python >= 3.4 or 2.7.9\n         context = ssl.create_default_context(ssl.Purpose.SERVER_AUTH)\n+        set_alpn_protocols(context)\n         if opts_no_check_certificate:\n             context.check_hostname = False\n             context.verify_mode = ssl.CERT_NONE\n-        try:\n+\n+        with compat_contextlib_suppress(TypeError):\n+            # Fails with Python 2.7.8 (create_default_context present\n+            # but HTTPSHandler has no context=)\n             return YoutubeDLHTTPSHandler(params, context=context, **kwargs)\n-        except TypeError:\n-            # Python 2.7.8\n-            # (create_default_context present but HTTPSHandler has no context=)\n-            pass\n \n     if sys.version_info < (3, 2):\n         return YoutubeDLHTTPSHandler(params, **kwargs)\n-    else:  # Python < 3.4\n+    else:  # Python3 < 3.4\n         context = ssl.SSLContext(ssl.PROTOCOL_TLSv1)\n         context.verify_mode = (ssl.CERT_NONE\n                                if opts_no_check_certificate\n                                else ssl.CERT_REQUIRED)\n         context.set_default_verify_paths()\n+        set_alpn_protocols(context)\n         return YoutubeDLHTTPSHandler(params, context=context, **kwargs)\n \n \n-def bug_reports_message():\n+def bug_reports_message(before=';'):\n     if ytdl_is_updateable():\n         update_cmd = 'type  youtube-dl -U  to update'\n     else:\n-        update_cmd = 'see  https://yt-dl.org/update  on how to update'\n-    msg = '; please report this issue on https://yt-dl.org/bug .'\n-    msg += ' Make sure you are using the latest version; %s.' % update_cmd\n-    msg += ' Be sure to call youtube-dl with the --verbose flag and include its complete output.'\n-    return msg\n+        update_cmd = 'see  https://github.com/ytdl-org/youtube-dl/#user-content-installation  on how to update'\n+\n+    msg = (\n+        'please report this issue on https://github.com/ytdl-org/youtube-dl/issues ,'\n+        ' using the appropriate issue template.'\n+        ' Make sure you are using the latest version; %s.'\n+        ' Be sure to call youtube-dl with the --verbose option and include the complete output.'\n+    ) % update_cmd\n+\n+    before = (before or '').rstrip()\n+    if not before or before.endswith(('.', '!', '?')):\n+        msg = msg[0].title() + msg[1:]\n+\n+    return (before + ' ' if before else '') + msg\n \n \n class YoutubeDLError(Exception):\n@@ -2339,7 +2391,7 @@ def __init__(self, msg, tb=None, expected=False, cause=None, video_id=None):\n         \"\"\" tb, if given, is the original traceback (so that it can be printed out).\n         If expected is set, this is a normal error message and most likely not a bug in youtube-dl.\n         \"\"\"\n-\n+        self.orig_msg = msg\n         if sys.exc_info()[0] in (compat_urllib_error.URLError, socket.timeout, UnavailableVideoError):\n             expected = True\n         if video_id is not None:\n@@ -2540,7 +2592,7 @@ def handle_youtubedl_headers(headers):\n     filtered_headers = headers\n \n     if 'Youtubedl-no-compression' in filtered_headers:\n-        filtered_headers = dict((k, v) for k, v in filtered_headers.items() if k.lower() != 'accept-encoding')\n+        filtered_headers = filter_dict(filtered_headers, cndn=lambda k, _: k.lower() != 'accept-encoding')\n         del filtered_headers['Youtubedl-no-compression']\n \n     return filtered_headers\n@@ -2558,7 +2610,8 @@ class YoutubeDLHandler(compat_urllib_request.HTTPHandler):\n \n     Part of this code was copied from:\n \n-    http://techknack.net/python-urllib2-handlers/\n+    http://techknack.net/python-urllib2-handlers/, archived at\n+    https://web.archive.org/web/20130527205558/http://techknack.net/python-urllib2-handlers/\n \n     Andrew Rowls, the author of that code, agreed to release it to the\n     public domain.\n@@ -2581,23 +2634,91 @@ def http_open(self, req):\n             req)\n \n     @staticmethod\n-    def deflate(data):\n+    def deflate_gz(data):\n         try:\n-            return zlib.decompress(data, -zlib.MAX_WBITS)\n+            # format:zlib,gzip + windowsize:32768\n+            return data and zlib.decompress(data, 32 + zlib.MAX_WBITS)\n         except zlib.error:\n-            return zlib.decompress(data)\n+            # raw zlib * windowsize:32768 (RFC 9110: \"non-conformant\")\n+            return zlib.decompress(data, -zlib.MAX_WBITS)\n+\n+    @staticmethod\n+    def gzip(data):\n+\n+        from gzip import GzipFile\n+\n+        def _gzip(data):\n+            with io.BytesIO(data) as data_buf:\n+                gz = GzipFile(fileobj=data_buf, mode='rb')\n+                return gz.read()\n+\n+        try:\n+            return _gzip(data)\n+        except IOError as original_ioerror:\n+            # There may be junk at the end of the file\n+            # See http://stackoverflow.com/q/4928560/35070 for details\n+            for i in range(1, 1024):\n+                try:\n+                    return _gzip(data[:-i])\n+                except IOError:\n+                    continue\n+            else:\n+                raise original_ioerror\n+\n+    @staticmethod\n+    def brotli(data):\n+        return data and brotli.decompress(data)\n+\n+    @staticmethod\n+    def compress(data):\n+        return data and ncompress.decompress(data)\n+\n+    @staticmethod\n+    def _fix_path(url):\n+        # an embedded /../ or /./ sequence is not automatically handled by urllib2\n+        # see https://github.com/yt-dlp/yt-dlp/issues/3355\n+        parsed_url = compat_urllib_parse.urlsplit(url)\n+        path = parsed_url.path\n+        if not path.endswith('/'):\n+            path += '/'\n+        parts = path.partition('/./')\n+        if not parts[1]:\n+            parts = path.partition('/../')\n+        if parts[1]:\n+            path = compat_urllib_parse.urljoin(\n+                parts[0] + parts[1][:1],\n+                parts[1][1:] + (parts[2] if parsed_url.path.endswith('/') else parts[2][:-1]))\n+            url = parsed_url._replace(path=path).geturl()\n+        if '/.' in url:\n+            # worse, URL path may have initial /../ against RFCs: work-around\n+            # by stripping such prefixes, like eg Firefox\n+            path = parsed_url.path + '/'\n+            while path.startswith('/.'):\n+                if path.startswith('/../'):\n+                    path = path[3:]\n+                elif path.startswith('/./'):\n+                    path = path[2:]\n+                else:\n+                    break\n+            path = path[:-1]\n+            if not path.startswith('/') and parsed_url.path.startswith('/'):\n+                path = '/' + path\n+            url = parsed_url._replace(path=path).geturl()\n+        return url\n \n     def http_request(self, req):\n-        # According to RFC 3986, URLs can not contain non-ASCII characters, however this is not\n-        # always respected by websites, some tend to give out URLs with non percent-encoded\n+        url = req.get_full_url()\n+        # resolve embedded . and ..\n+        url_fixed = self._fix_path(url)\n+        # According to RFC 3986, URLs can not contain non-ASCII characters; however this is not\n+        # always respected by websites: some tend to give out URLs with non percent-encoded\n         # non-ASCII characters (see telemb.py, ard.py [#3412])\n         # urllib chokes on URLs with non-ASCII characters (see http://bugs.python.org/issue3991)\n         # To work around aforementioned issue we will replace request's original URL with\n         # percent-encoded one\n         # Since redirects are also affected (e.g. http://www.southpark.de/alle-episoden/s18e09)\n         # the code of this workaround has been moved here from YoutubeDL.urlopen()\n-        url = req.get_full_url()\n-        url_escaped = escape_url(url)\n+        url_escaped = escape_url(url_fixed)\n \n         # Substitute URL if any change after escaping\n         if url != url_escaped:\n@@ -2609,44 +2730,82 @@ def http_request(self, req):\n             if h.capitalize() not in req.headers:\n                 req.add_header(h, v)\n \n+        # Similarly, 'Accept-encoding'\n+        if 'Accept-encoding' not in req.headers:\n+            req.add_header(\n+                'Accept-Encoding', join_nonempty(\n+                    'gzip', 'deflate', brotli and 'br', ncompress and 'compress',\n+                    delim=', '))\n+\n         req.headers = handle_youtubedl_headers(req.headers)\n \n-        if sys.version_info < (2, 7) and '#' in req.get_full_url():\n-            # Python 2.6 is brain-dead when it comes to fragments\n-            req._Request__original = req._Request__original.partition('#')[0]\n-            req._Request__r_type = req._Request__r_type.partition('#')[0]\n+        if sys.version_info < (2, 7):\n+            # avoid possible race where __r_type may be unset\n+            req.get_type()\n+            if '#' in req.get_full_url():\n+                # Python 2.6 is brain-dead when it comes to fragments\n+                req._Request__original = req._Request__original.partition('#')[0]\n+                req._Request__r_type = req._Request__r_type.partition('#')[0]\n \n-        return req\n+        # Use the totally undocumented AbstractHTTPHandler per\n+        # https://github.com/yt-dlp/yt-dlp/pull/4158\n+        return compat_urllib_request.AbstractHTTPHandler.do_request_(self, req)\n \n     def http_response(self, req, resp):\n         old_resp = resp\n-        # gzip\n-        if resp.headers.get('Content-encoding', '') == 'gzip':\n-            content = resp.read()\n-            gz = gzip.GzipFile(fileobj=io.BytesIO(content), mode='rb')\n-            try:\n-                uncompressed = io.BytesIO(gz.read())\n-            except IOError as original_ioerror:\n-                # There may be junk add the end of the file\n-                # See http://stackoverflow.com/q/4928560/35070 for details\n-                for i in range(1, 1024):\n-                    try:\n-                        gz = gzip.GzipFile(fileobj=io.BytesIO(content[:-i]), mode='rb')\n-                        uncompressed = io.BytesIO(gz.read())\n-                    except IOError:\n-                        continue\n-                    break\n-                else:\n-                    raise original_ioerror\n-            resp = compat_urllib_request.addinfourl(uncompressed, old_resp.headers, old_resp.url, old_resp.code)\n-            resp.msg = old_resp.msg\n-            del resp.headers['Content-encoding']\n-        # deflate\n-        if resp.headers.get('Content-encoding', '') == 'deflate':\n-            gz = io.BytesIO(self.deflate(resp.read()))\n-            resp = compat_urllib_request.addinfourl(gz, old_resp.headers, old_resp.url, old_resp.code)\n+\n+        # Content-Encoding header lists the encodings in order that they were applied [1].\n+        # To decompress, we simply do the reverse.\n+        # [1]: https://datatracker.ietf.org/doc/html/rfc9110#name-content-encoding\n+        decoded_response = None\n+        decoders = {\n+            'gzip': self.deflate_gz,\n+            'deflate': self.deflate_gz,\n+        }\n+        if brotli:\n+            decoders['br'] = self.brotli\n+        if ncompress:\n+            decoders['compress'] = self.compress\n+        if sys.platform.startswith('java'):\n+            # Jython zlib implementation misses gzip\n+            decoders['gzip'] = self.gzip\n+\n+        def encodings(hdrs):\n+            # A header field that allows multiple values can have multiple instances [2].\n+            # [2]: https://datatracker.ietf.org/doc/html/rfc9110#name-fields\n+            for e in reversed(','.join(hdrs).split(',')):\n+                if e:\n+                    yield e.strip()\n+\n+        encodings_left = []\n+        try:\n+            resp.headers.get_all\n+            hdrs = resp.headers\n+        except AttributeError:\n+            # Py2 has no get_all() method: headers are rfc822.Message\n+            from email.message import Message\n+            hdrs = Message()\n+            for k, v in resp.headers.items():\n+                hdrs[k] = v\n+\n+        decoder, decoded_response = True, None\n+        for encoding in encodings(hdrs.get_all('Content-Encoding', [])):\n+            # \"SHOULD consider\" x-compress, x-gzip as compress, gzip\n+            decoder = decoder and decoders.get(remove_start(encoding, 'x-'))\n+            if not decoder:\n+                encodings_left.insert(0, encoding)\n+                continue\n+            decoded_response = decoder(decoded_response or resp.read())\n+        if decoded_response is not None:\n+            resp = compat_urllib_request.addinfourl(\n+                io.BytesIO(decoded_response), old_resp.headers, old_resp.url, old_resp.code)\n             resp.msg = old_resp.msg\n-            del resp.headers['Content-encoding']\n+            del resp.headers['Content-Length']\n+            resp.headers['Content-Length'] = '%d' % len(decoded_response)\n+        del resp.headers['Content-Encoding']\n+        if encodings_left:\n+            resp.headers['Content-Encoding'] = ', '.join(encodings_left)\n+\n         # Percent-encode redirect URL of Location HTTP header to satisfy RFC 3986 (see\n         # https://github.com/ytdl-org/youtube-dl/issues/6457).\n         if 300 <= resp.code < 400:\n@@ -2654,13 +2813,14 @@ def http_response(self, req, resp):\n             if location:\n                 # As of RFC 2616 default charset is iso-8859-1 that is respected by python 3\n                 if sys.version_info >= (3, 0):\n-                    location = location.encode('iso-8859-1').decode('utf-8')\n-                else:\n-                    location = location.decode('utf-8')\n-                location_escaped = escape_url(location)\n+                    location = location.encode('iso-8859-1')\n+                location = location.decode('utf-8')\n+                # resolve embedded . and ..\n+                location_fixed = self._fix_path(location)\n+                location_escaped = escape_url(location_fixed)\n                 if location != location_escaped:\n                     del resp.headers['Location']\n-                    if sys.version_info < (3, 0):\n+                    if not isinstance(location_escaped, str):  # Py 2 case\n                         location_escaped = location_escaped.encode('utf-8')\n                     resp.headers['Location'] = location_escaped\n         return resp\n@@ -2673,7 +2833,7 @@ def make_socks_conn_class(base_class, socks_proxy):\n     assert issubclass(base_class, (\n         compat_http_client.HTTPConnection, compat_http_client.HTTPSConnection))\n \n-    url_components = compat_urlparse.urlparse(socks_proxy)\n+    url_components = compat_urllib_parse.urlparse(socks_proxy)\n     if url_components.scheme.lower() == 'socks5':\n         socks_type = ProxyType.SOCKS5\n     elif url_components.scheme.lower() in ('socks', 'socks4'):\n@@ -2854,6 +3014,19 @@ def prepare_line(line):\n                 cookie.expires = None\n                 cookie.discard = True\n \n+    def get_cookie_header(self, url):\n+        \"\"\"Generate a Cookie HTTP header for a given url\"\"\"\n+        cookie_req = sanitized_Request(url)\n+        self.add_cookie_header(cookie_req)\n+        return cookie_req.get_header('Cookie')\n+\n+    def get_cookies_for_url(self, url):\n+        \"\"\"Generate a list of Cookie objects for a given url\"\"\"\n+        # Policy `_now` attribute must be set before calling `_cookies_for_request`\n+        # Ref: https://github.com/python/cpython/blob/3.7/Lib/http/cookiejar.py#L1360\n+        self._policy._now = self._now = int(time.time())\n+        return self._cookies_for_request(sanitized_Request(url))\n+\n \n class YoutubeDLCookieProcessor(compat_urllib_request.HTTPCookieProcessor):\n     def __init__(self, cookiejar=None):\n@@ -2884,17 +3057,16 @@ class YoutubeDLRedirectHandler(compat_urllib_request.HTTPRedirectHandler):\n \n     The code is based on HTTPRedirectHandler implementation from CPython [1].\n \n-    This redirect handler solves two issues:\n-     - ensures redirect URL is always unicode under python 2\n-     - introduces support for experimental HTTP response status code\n-       308 Permanent Redirect [2] used by some sites [3]\n+    This redirect handler fixes and improves the logic to better align with RFC7261\n+    and what browsers tend to do [2][3]\n \n     1. https://github.com/python/cpython/blob/master/Lib/urllib/request.py\n-    2. https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/308\n-    3. https://github.com/ytdl-org/youtube-dl/issues/28768\n+    2. https://datatracker.ietf.org/doc/html/rfc7231\n+    3. https://github.com/python/cpython/issues/91306\n     \"\"\"\n \n-    http_error_301 = http_error_303 = http_error_307 = http_error_308 = compat_urllib_request.HTTPRedirectHandler.http_error_302\n+    # Supply possibly missing alias\n+    http_error_308 = compat_urllib_request.HTTPRedirectHandler.http_error_302\n \n     def redirect_request(self, req, fp, code, msg, headers, newurl):\n         \"\"\"Return a Request or None in response to a redirect.\n@@ -2906,42 +3078,68 @@ def redirect_request(self, req, fp, code, msg, headers, newurl):\n         else should try to handle this url.  Return None if you can't\n         but another Handler might.\n         \"\"\"\n-        m = req.get_method()\n-        if (not (code in (301, 302, 303, 307, 308) and m in (\"GET\", \"HEAD\")\n-                 or code in (301, 302, 303) and m == \"POST\")):\n-            raise compat_HTTPError(req.full_url, code, msg, headers, fp)\n-        # Strictly (according to RFC 2616), 301 or 302 in response to\n-        # a POST MUST NOT cause a redirection without confirmation\n-        # from the user (of urllib.request, in this case).  In practice,\n-        # essentially all clients do redirect in this case, so we do\n-        # the same.\n+        if code not in (301, 302, 303, 307, 308):\n+            raise compat_urllib_HTTPError(req.full_url, code, msg, headers, fp)\n+\n+        new_method = req.get_method()\n+        new_data = req.data\n \n         # On python 2 urlh.geturl() may sometimes return redirect URL\n-        # as byte string instead of unicode. This workaround allows\n-        # to force it always return unicode.\n-        if sys.version_info[0] < 3:\n-            newurl = compat_str(newurl)\n+        # as a byte string instead of unicode. This workaround forces\n+        # it to return unicode.\n+        newurl = _decode_compat_str(newurl)\n \n         # Be conciliant with URIs containing a space.  This is mainly\n         # redundant with the more complete encoding done in http_error_302(),\n         # but it is kept for compatibility with other callers.\n         newurl = newurl.replace(' ', '%20')\n \n-        CONTENT_HEADERS = (\"content-length\", \"content-type\")\n-        # NB: don't use dict comprehension for python 2.6 compatibility\n-        newheaders = dict((k, v) for k, v in req.headers.items()\n-                          if k.lower() not in CONTENT_HEADERS)\n+        # Technically the Cookie header should be in unredirected_hdrs;\n+        # however in practice some may set it in normal headers anyway.\n+        # We will remove it here to prevent any leaks.\n+        remove_headers = ['Cookie']\n+\n+        # A 303 must either use GET or HEAD for subsequent request\n+        # https://datatracker.ietf.org/doc/html/rfc7231#section-6.4.4\n+        if code == 303 and req.get_method() != 'HEAD':\n+            new_method = 'GET'\n+        # 301 and 302 redirects are commonly turned into a GET from a POST\n+        # for subsequent requests by browsers, so we'll do the same.\n+        # https://datatracker.ietf.org/doc/html/rfc7231#section-6.4.2\n+        # https://datatracker.ietf.org/doc/html/rfc7231#section-6.4.3\n+        elif code in (301, 302) and req.get_method() == 'POST':\n+            new_method = 'GET'\n+\n+        # only remove payload if method changed (e.g. POST to GET)\n+        if new_method != req.get_method():\n+            new_data = None\n+            remove_headers.extend(['Content-Length', 'Content-Type'])\n+\n+        new_headers = filter_dict(req.headers, cndn=lambda k, _: k.title() not in remove_headers)\n+\n         return compat_urllib_request.Request(\n-            newurl, headers=newheaders, origin_req_host=req.origin_req_host,\n-            unverifiable=True)\n+            newurl, headers=new_headers, origin_req_host=req.origin_req_host,\n+            unverifiable=True, method=new_method, data=new_data)\n \n \n def extract_timezone(date_str):\n     m = re.search(\n-        r'^.{8,}?(?P<tz>Z$| ?(?P<sign>\\+|-)(?P<hours>[0-9]{2}):?(?P<minutes>[0-9]{2})$)',\n-        date_str)\n+        r'''(?x)\n+            ^.{8,}?                                              # >=8 char non-TZ prefix, if present\n+            (?P<tz>Z|                                            # just the UTC Z, or\n+                (?:(?<=.\\b\\d{4}|\\b\\d{2}:\\d\\d)|                   # preceded by 4 digits or hh:mm or\n+                   (?<!.\\b[a-zA-Z]{3}|[a-zA-Z]{4}|..\\b\\d\\d))     # not preceded by 3 alpha word or >= 4 alpha or 2 digits\n+                   [ ]?                                          # optional space\n+                (?P<sign>\\+|-)                                   # +/-\n+                (?P<hours>[0-9]{2}):?(?P<minutes>[0-9]{2})       # hh[:]mm\n+            $)\n+        ''', date_str)\n     if not m:\n-        timezone = datetime.timedelta()\n+        m = re.search(r'\\d{1,2}:\\d{1,2}(?:\\.\\d+)?(?P<tz>\\s*[A-Z]+)$', date_str)\n+        timezone = TIMEZONE_NAMES.get(m and m.group('tz').strip())\n+        if timezone is not None:\n+            date_str = date_str[:-len(m.group('tz'))]\n+        timezone = datetime.timedelta(hours=timezone or 0)\n     else:\n         date_str = date_str[:-len(m.group('tz'))]\n         if not m.group('sign'):\n@@ -2965,12 +3163,10 @@ def parse_iso8601(date_str, delimiter='T', timezone=None):\n     if timezone is None:\n         timezone, date_str = extract_timezone(date_str)\n \n-    try:\n+    with compat_contextlib_suppress(ValueError):\n         date_format = '%Y-%m-%d{0}%H:%M:%S'.format(delimiter)\n         dt = datetime.datetime.strptime(date_str, date_format) - timezone\n         return calendar.timegm(dt.timetuple())\n-    except ValueError:\n-        pass\n \n \n def date_formats(day_first=True):\n@@ -2990,17 +3186,13 @@ def unified_strdate(date_str, day_first=True):\n     _, date_str = extract_timezone(date_str)\n \n     for expression in date_formats(day_first):\n-        try:\n+        with compat_contextlib_suppress(ValueError):\n             upload_date = datetime.datetime.strptime(date_str, expression).strftime('%Y%m%d')\n-        except ValueError:\n-            pass\n     if upload_date is None:\n         timetuple = email.utils.parsedate_tz(date_str)\n         if timetuple:\n-            try:\n+            with compat_contextlib_suppress(ValueError):\n                 upload_date = datetime.datetime(*timetuple[:6]).strftime('%Y%m%d')\n-            except ValueError:\n-                pass\n     if upload_date is not None:\n         return compat_str(upload_date)\n \n@@ -3009,7 +3201,8 @@ def unified_timestamp(date_str, day_first=True):\n     if date_str is None:\n         return None\n \n-    date_str = re.sub(r'[,|]', '', date_str)\n+    date_str = re.sub(r'\\s+', ' ', re.sub(\n+        r'(?i)[,|]|(mon|tues?|wed(nes)?|thu(rs)?|fri|sat(ur)?)(day)?', '', date_str))\n \n     pm_delta = 12 if re.search(r'(?i)PM', date_str) else 0\n     timezone, date_str = extract_timezone(date_str)\n@@ -3028,14 +3221,12 @@ def unified_timestamp(date_str, day_first=True):\n         date_str = m.group(1)\n \n     for expression in date_formats(day_first):\n-        try:\n+        with compat_contextlib_suppress(ValueError):\n             dt = datetime.datetime.strptime(date_str, expression) - timezone + datetime.timedelta(hours=pm_delta)\n             return calendar.timegm(dt.timetuple())\n-        except ValueError:\n-            pass\n     timetuple = email.utils.parsedate_tz(date_str)\n     if timetuple:\n-        return calendar.timegm(timetuple) + pm_delta * 3600\n+        return calendar.timegm(timetuple) + pm_delta * 3600 - compat_datetime_timedelta_total_seconds(timezone)\n \n \n def determine_ext(url, default_ext='unknown_video'):\n@@ -3124,15 +3315,15 @@ def __contains__(self, date):\n     def __str__(self):\n         return '%s - %s' % (self.start.isoformat(), self.end.isoformat())\n \n+    def __eq__(self, other):\n+        return (isinstance(other, DateRange)\n+                and self.start == other.start and self.end == other.end)\n+\n \n def platform_name():\n     \"\"\" Returns the platform name as a compat_str \"\"\"\n     res = platform.platform()\n-    if isinstance(res, bytes):\n-        res = res.decode(preferredencoding())\n-\n-    assert isinstance(res, compat_str)\n-    return res\n+    return _decode_compat_str(res)\n \n \n def _windows_write_string(s, out):\n@@ -3213,7 +3404,7 @@ def next_nonbmp_pos(s):\n def write_string(s, out=None, encoding=None):\n     if out is None:\n         out = sys.stderr\n-    assert type(s) == compat_str\n+    assert isinstance(s, compat_str)\n \n     if sys.platform == 'win32' and encoding is None and hasattr(out, 'fileno'):\n         if _windows_write_string(s, out):\n@@ -3362,9 +3553,8 @@ def shell_quote(args):\n     quoted_args = []\n     encoding = get_filesystem_encoding()\n     for a in args:\n-        if isinstance(a, bytes):\n-            # We may get a filename encoded with 'encodeFilename'\n-            a = a.decode(encoding)\n+        # We may get a filename encoded with 'encodeFilename'\n+        a = _decode_compat_str(a, encoding)\n         quoted_args.append(compat_shlex_quote(a))\n     return ' '.join(quoted_args)\n \n@@ -3528,8 +3718,9 @@ def parse_resolution(s):\n \n \n def parse_bitrate(s):\n-    if not isinstance(s, compat_str):\n-        return\n+    s = txt_or_none(s)\n+    if not s:\n+        return None\n     mobj = re.search(r'\\b(\\d+)\\s*kbps', s)\n     if mobj:\n         return int(mobj.group(1))\n@@ -3608,7 +3799,7 @@ def remove_quotes(s):\n \n \n def url_basename(url):\n-    path = compat_urlparse.urlparse(url).path\n+    path = compat_urllib_parse.urlparse(url).path\n     return path.strip('/').split('/')[-1]\n \n \n@@ -3617,18 +3808,17 @@ def base_url(url):\n \n \n def urljoin(base, path):\n-    if isinstance(path, bytes):\n-        path = path.decode('utf-8')\n-    if not isinstance(path, compat_str) or not path:\n+    path = _decode_compat_str(path, encoding='utf-8', or_none=True)\n+    if not path:\n         return None\n     if re.match(r'^(?:[a-zA-Z][a-zA-Z0-9+-.]*:)?//', path):\n         return path\n-    if isinstance(base, bytes):\n-        base = base.decode('utf-8')\n-    if not isinstance(base, compat_str) or not re.match(\n-            r'^(?:https?:)?//', base):\n+    base = _decode_compat_str(base, encoding='utf-8', or_none=True)\n+    if not base:\n         return None\n-    return compat_urlparse.urljoin(base, path)\n+    return (\n+        re.match(r'^(?:https?:)?//', base)\n+        and compat_urllib_parse.urljoin(base, path))\n \n \n class HEADRequest(compat_urllib_request.Request):\n@@ -3641,17 +3831,16 @@ def get_method(self):\n         return 'PUT'\n \n \n-def int_or_none(v, scale=1, default=None, get_attr=None, invscale=1):\n+def int_or_none(v, scale=1, default=None, get_attr=None, invscale=1, base=None):\n     if get_attr:\n         if v is not None:\n             v = getattr(v, get_attr, None)\n-    if v == '':\n-        v = None\n-    if v is None:\n+    if v in (None, ''):\n         return default\n     try:\n-        return int(v) * invscale // scale\n-    except (ValueError, TypeError):\n+        # like int, raise if base is specified and v is not a string\n+        return (int(v) if base is None else int(v, base=base)) * invscale // scale\n+    except (ValueError, TypeError, OverflowError):\n         return default\n \n \n@@ -3685,6 +3874,11 @@ def strip_or_none(v, default=None):\n     return v.strip() if isinstance(v, compat_str) else default\n \n \n+def txt_or_none(v, default=None):\n+    \"\"\" Combine str/strip_or_none, disallow blank value (for traverse_obj) \"\"\"\n+    return default if v is None else (compat_str(v).strip() or default)\n+\n+\n def url_or_none(url):\n     if not url or not isinstance(url, compat_str):\n         return None\n@@ -3750,26 +3944,30 @@ def parse_duration(s):\n     return duration\n \n \n-def prepend_extension(filename, ext, expected_real_ext=None):\n+def _change_extension(prepend, filename, ext, expected_real_ext=None):\n     name, real_ext = os.path.splitext(filename)\n-    return (\n-        '{0}.{1}{2}'.format(name, ext, real_ext)\n-        if not expected_real_ext or real_ext[1:] == expected_real_ext\n-        else '{0}.{1}'.format(filename, ext))\n+    sanitize_extension = _UnsafeExtensionError.sanitize_extension\n \n+    if not expected_real_ext or real_ext.partition('.')[0::2] == ('', expected_real_ext):\n+        filename = name\n+        if prepend and real_ext:\n+            sanitize_extension(ext, prepend=prepend)\n+            return ''.join((filename, '.', ext, real_ext))\n+\n+    # Mitigate path traversal and file impersonation attacks\n+    return '.'.join((filename, sanitize_extension(ext)))\n \n-def replace_extension(filename, ext, expected_real_ext=None):\n-    name, real_ext = os.path.splitext(filename)\n-    return '{0}.{1}'.format(\n-        name if not expected_real_ext or real_ext[1:] == expected_real_ext else filename,\n-        ext)\n+\n+prepend_extension = functools.partial(_change_extension, True)\n+replace_extension = functools.partial(_change_extension, False)\n \n \n def check_executable(exe, args=[]):\n     \"\"\" Checks if the given binary is installed somewhere in PATH, and returns its name.\n     args can be a list of arguments for a short output (like -version) \"\"\"\n     try:\n-        subprocess.Popen([exe] + args, stdout=subprocess.PIPE, stderr=subprocess.PIPE).communicate()\n+        process_communicate_or_kill(subprocess.Popen(\n+            [exe] + args, stdout=subprocess.PIPE, stderr=subprocess.PIPE))\n     except OSError:\n         return False\n     return exe\n@@ -3783,14 +3981,13 @@ def get_exe_version(exe, args=['--version'],\n         # STDIN should be redirected too. On UNIX-like systems, ffmpeg triggers\n         # SIGTTOU if youtube-dl is run in the background.\n         # See https://github.com/ytdl-org/youtube-dl/issues/955#issuecomment-209789656\n-        out, _ = subprocess.Popen(\n+        out, _ = process_communicate_or_kill(subprocess.Popen(\n             [encodeArgument(exe)] + args,\n             stdin=subprocess.PIPE,\n-            stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()\n+            stdout=subprocess.PIPE, stderr=subprocess.STDOUT))\n     except OSError:\n         return False\n-    if isinstance(out, bytes):  # Python 2.x\n-        out = out.decode('ascii', 'ignore')\n+    out = _decode_compat_str(out, 'ascii', 'ignore')\n     return detect_exe_version(out, version_re, unrecognized)\n \n \n@@ -3805,6 +4002,105 @@ def detect_exe_version(output, version_re=None, unrecognized='present'):\n         return unrecognized\n \n \n+class LazyList(compat_collections_abc.Iterable):\n+    \"\"\"Lazy immutable list from an iterable\n+    Note that slices of a LazyList are lists and not LazyList\"\"\"\n+\n+    class IndexError(IndexError):\n+        def __init__(self, cause=None):\n+            if cause:\n+                # reproduce `raise from`\n+                self.__cause__ = cause\n+            super(IndexError, self).__init__()\n+\n+    def __init__(self, iterable, **kwargs):\n+        # kwarg-only\n+        reverse = kwargs.get('reverse', False)\n+        _cache = kwargs.get('_cache')\n+\n+        self._iterable = iter(iterable)\n+        self._cache = [] if _cache is None else _cache\n+        self._reversed = reverse\n+\n+    def __iter__(self):\n+        if self._reversed:\n+            # We need to consume the entire iterable to iterate in reverse\n+            for item in self.exhaust():\n+                yield item\n+            return\n+        for item in self._cache:\n+            yield item\n+        for item in self._iterable:\n+            self._cache.append(item)\n+            yield item\n+\n+    def _exhaust(self):\n+        self._cache.extend(self._iterable)\n+        self._iterable = []  # Discard the emptied iterable to make it pickle-able\n+        return self._cache\n+\n+    def exhaust(self):\n+        \"\"\"Evaluate the entire iterable\"\"\"\n+        return self._exhaust()[::-1 if self._reversed else 1]\n+\n+    @staticmethod\n+    def _reverse_index(x):\n+        return None if x is None else ~x\n+\n+    def __getitem__(self, idx):\n+        if isinstance(idx, slice):\n+            if self._reversed:\n+                idx = slice(self._reverse_index(idx.start), self._reverse_index(idx.stop), -(idx.step or 1))\n+            start, stop, step = idx.start, idx.stop, idx.step or 1\n+        elif isinstance(idx, int):\n+            if self._reversed:\n+                idx = self._reverse_index(idx)\n+            start, stop, step = idx, idx, 0\n+        else:\n+            raise TypeError('indices must be integers or slices')\n+        if ((start or 0) < 0 or (stop or 0) < 0\n+                or (start is None and step < 0)\n+                or (stop is None and step > 0)):\n+            # We need to consume the entire iterable to be able to slice from the end\n+            # Obviously, never use this with infinite iterables\n+            self._exhaust()\n+            try:\n+                return self._cache[idx]\n+            except IndexError as e:\n+                raise self.IndexError(e)\n+        n = max(start or 0, stop or 0) - len(self._cache) + 1\n+        if n > 0:\n+            self._cache.extend(itertools.islice(self._iterable, n))\n+        try:\n+            return self._cache[idx]\n+        except IndexError as e:\n+            raise self.IndexError(e)\n+\n+    def __bool__(self):\n+        try:\n+            self[-1] if self._reversed else self[0]\n+        except self.IndexError:\n+            return False\n+        return True\n+\n+    def __len__(self):\n+        self._exhaust()\n+        return len(self._cache)\n+\n+    def __reversed__(self):\n+        return type(self)(self._iterable, reverse=not self._reversed, _cache=self._cache)\n+\n+    def __copy__(self):\n+        return type(self)(self._iterable, reverse=self._reversed, _cache=self._cache)\n+\n+    def __repr__(self):\n+        # repr and str should mimic a list. So we exhaust the iterable\n+        return repr(self.exhaust())\n+\n+    def __str__(self):\n+        return repr(self.exhaust())\n+\n+\n class PagedList(object):\n     def __len__(self):\n         # This is only useful for tests\n@@ -3910,9 +4206,10 @@ def lowercase_escape(s):\n \n def escape_rfc3986(s):\n     \"\"\"Escape non-ASCII characters as suggested by RFC 3986\"\"\"\n-    if sys.version_info < (3, 0) and isinstance(s, compat_str):\n-        s = s.encode('utf-8')\n-    return compat_urllib_parse.quote(s, b\"%/;:@&=+$,!~*'()?#[]\")\n+    if sys.version_info < (3, 0):\n+        s = _encode_compat_str(s, 'utf-8')\n+    # ensure unicode: after quoting, it can always be converted\n+    return compat_str(compat_urllib_parse.quote(s, b\"%/;:@&=+$,!~*'()?#[]\"))\n \n \n def escape_url(url):\n@@ -3927,10 +4224,13 @@ def escape_url(url):\n     ).geturl()\n \n \n+def parse_qs(url, **kwargs):\n+    return compat_parse_qs(compat_urllib_parse.urlparse(url).query, **kwargs)\n+\n+\n def read_batch_urls(batch_fd):\n     def fixup(url):\n-        if not isinstance(url, compat_str):\n-            url = url.decode('utf-8', 'replace')\n+        url = _decode_compat_str(url, 'utf-8', 'replace')\n         BOM_UTF8 = '\\xef\\xbb\\xbf'\n         if url.startswith(BOM_UTF8):\n             url = url[len(BOM_UTF8):]\n@@ -3947,28 +4247,38 @@ def urlencode_postdata(*args, **kargs):\n     return compat_urllib_parse_urlencode(*args, **kargs).encode('ascii')\n \n \n+def update_url(url, **kwargs):\n+    \"\"\"Replace URL components specified by kwargs\n+       url: compat_str or parsed URL tuple\n+       if query_update is in kwargs, update query with\n+       its value instead of replacing (overrides any `query`)\n+       NB: query_update expects parse_qs() format: [key: value_list, ...]\n+       returns: compat_str\n+    \"\"\"\n+    if not kwargs:\n+        return compat_urllib_parse.urlunparse(url) if isinstance(url, tuple) else url\n+    if not isinstance(url, tuple):\n+        url = compat_urllib_parse.urlparse(url)\n+    query = kwargs.pop('query_update', None)\n+    if query:\n+        qs = compat_parse_qs(url.query)\n+        qs.update(query)\n+        kwargs['query'] = compat_urllib_parse_urlencode(qs, True)\n+        kwargs = compat_kwargs(kwargs)\n+    return compat_urllib_parse.urlunparse(url._replace(**kwargs))\n+\n+\n def update_url_query(url, query):\n-    if not query:\n-        return url\n-    parsed_url = compat_urlparse.urlparse(url)\n-    qs = compat_parse_qs(parsed_url.query)\n-    qs.update(query)\n-    return compat_urlparse.urlunparse(parsed_url._replace(\n-        query=compat_urllib_parse_urlencode(qs, True)))\n+    return update_url(url, query_update=query)\n \n \n def update_Request(req, url=None, data=None, headers={}, query={}):\n     req_headers = req.headers.copy()\n     req_headers.update(headers)\n-    req_data = data or req.data\n+    req_data = data if data is not None else req.data\n     req_url = update_url_query(url or req.get_full_url(), query)\n-    req_get_method = req.get_method()\n-    if req_get_method == 'HEAD':\n-        req_type = HEADRequest\n-    elif req_get_method == 'PUT':\n-        req_type = PUTRequest\n-    else:\n-        req_type = compat_urllib_request.Request\n+    req_type = {'HEAD': HEADRequest, 'PUT': PUTRequest}.get(\n+        req.get_method(), compat_urllib_request.Request)\n     new_req = req_type(\n         req_url, data=req_data, headers=req_headers,\n         origin_req_host=req.origin_req_host, unverifiable=req.unverifiable)\n@@ -3983,10 +4293,8 @@ def _multipart_encode_impl(data, boundary):\n     out = b''\n     for k, v in data.items():\n         out += b'--' + boundary.encode('ascii') + b'\\r\\n'\n-        if isinstance(k, compat_str):\n-            k = k.encode('utf-8')\n-        if isinstance(v, compat_str):\n-            v = v.encode('utf-8')\n+        k = _encode_compat_str(k, 'utf-8')\n+        v = _encode_compat_str(v, 'utf-8')\n         # RFC 2047 requires non-ASCII field names to be encoded, while RFC 7578\n         # suggests sending UTF-8 directly. Firefox sends UTF-8, too\n         content = b'Content-Disposition: form-data; name=\"' + k + b'\"\\r\\n\\r\\n' + v + b'\\r\\n'\n@@ -4029,14 +4337,39 @@ def multipart_encode(data, boundary=None):\n     return out, content_type\n \n \n+def is_iterable_like(x, allowed_types=compat_collections_abc.Iterable, blocked_types=NO_DEFAULT):\n+    if blocked_types is NO_DEFAULT:\n+        blocked_types = (compat_str, bytes, compat_collections_abc.Mapping)\n+    return isinstance(x, allowed_types) and not isinstance(x, blocked_types)\n+\n+\n+def variadic(x, allowed_types=NO_DEFAULT):\n+    if isinstance(allowed_types, compat_collections_abc.Iterable):\n+        allowed_types = tuple(allowed_types)\n+    return x if is_iterable_like(x, blocked_types=allowed_types) else (x,)\n+\n+\n def dict_get(d, key_or_keys, default=None, skip_false_values=True):\n-    if isinstance(key_or_keys, (list, tuple)):\n-        for key in key_or_keys:\n-            if key not in d or d[key] is None or skip_false_values and not d[key]:\n-                continue\n-            return d[key]\n-        return default\n-    return d.get(key_or_keys, default)\n+    exp = (lambda x: x or None) if skip_false_values else IDENTITY\n+    return traverse_obj(d, *variadic(key_or_keys), expected_type=exp,\n+                        default=default, get_all=False)\n+\n+\n+def try_call(*funcs, **kwargs):\n+\n+    # parameter defaults\n+    expected_type = kwargs.get('expected_type')\n+    fargs = kwargs.get('args', [])\n+    fkwargs = kwargs.get('kwargs', {})\n+\n+    for f in funcs:\n+        try:\n+            val = f(*fargs, **fkwargs)\n+        except (AttributeError, KeyError, TypeError, IndexError, ZeroDivisionError):\n+            pass\n+        else:\n+            if expected_type is None or isinstance(val, expected_type):\n+                return val\n \n \n def try_get(src, getter, expected_type=None):\n@@ -4052,22 +4385,67 @@ def try_get(src, getter, expected_type=None):\n                 return v\n \n \n-def merge_dicts(*dicts):\n+def filter_dict(dct, cndn=lambda _, v: v is not None):\n+    # NB: don't use dict comprehension for python 2.6 compatibility\n+    return dict((k, v) for k, v in dct.items() if cndn(k, v))\n+\n+\n+def merge_dicts(*dicts, **kwargs):\n+    \"\"\"\n+        Merge the `dict`s in `dicts` using the first valid value for each key.\n+        Normally valid: not None and not an empty string\n+\n+        Keyword-only args:\n+        unblank:    allow empty string if False (default True)\n+        rev:        merge dicts in reverse order (default False)\n+\n+        merge_dicts(dct1, dct2, ..., unblank=False, rev=True)\n+        matches {**dct1, **dct2, ...}\n+\n+        However, merge_dicts(dct1, dct2, ..., rev=True) may often be better.\n+    \"\"\"\n+\n+    unblank = kwargs.get('unblank', True)\n+    rev = kwargs.get('rev', False)\n+\n+    if unblank:\n+        def can_merge_str(k, v, to_dict):\n+            return (isinstance(v, compat_str) and v\n+                    and isinstance(to_dict[k], compat_str)\n+                    and not to_dict[k])\n+    else:\n+        can_merge_str = lambda k, v, to_dict: False\n+\n     merged = {}\n-    for a_dict in dicts:\n+    for a_dict in reversed(dicts) if rev else dicts:\n         for k, v in a_dict.items():\n             if v is None:\n                 continue\n-            if (k not in merged\n-                    or (isinstance(v, compat_str) and v\n-                        and isinstance(merged[k], compat_str)\n-                        and not merged[k])):\n+            if (k not in merged) or can_merge_str(k, v, merged):\n                 merged[k] = v\n     return merged\n \n \n-def encode_compat_str(string, encoding=preferredencoding(), errors='strict'):\n-    return string if isinstance(string, compat_str) else compat_str(string, encoding, errors)\n+# very poor choice of name, as if Python string encodings weren't confusing enough\n+def encode_compat_str(s, encoding=preferredencoding(), errors='strict'):\n+    assert isinstance(s, compat_basestring)\n+    return s if isinstance(s, compat_str) else compat_str(s, encoding, errors)\n+\n+\n+# what it could have been\n+def _decode_compat_str(s, encoding=preferredencoding(), errors='strict', or_none=False):\n+    if not or_none:\n+        assert isinstance(s, compat_basestring)\n+    return (\n+        s if isinstance(s, compat_str)\n+        else compat_str(s, encoding, errors) if isinstance(s, compat_basestring)\n+        else None)\n+\n+\n+# the real encode_compat_str, but only for internal use\n+def _encode_compat_str(s, encoding=preferredencoding(), errors='strict'):\n+    assert isinstance(s, compat_basestring)\n+    return s.encode(encoding, errors) if isinstance(s, compat_str) else s\n \n \n US_RATINGS = {\n@@ -4090,8 +4468,10 @@ def encode_compat_str(string, encoding=preferredencoding(), errors='strict'):\n \n \n def parse_age_limit(s):\n-    if type(s) == int:\n-        return s if 0 <= s <= 21 else None\n+    if not isinstance(s, bool):\n+        age = int_or_none(s)\n+        if age is not None:\n+            return age if 0 <= age <= 21 else None\n     if not isinstance(s, compat_basestring):\n         return None\n     m = re.match(r'^(?P<age>\\d{1,2})\\+?$', s)\n@@ -4115,46 +4495,108 @@ def strip_jsonp(code):\n         r'\\g<callback_data>', code)\n \n \n-def js_to_json(code):\n-    COMMENT_RE = r'/\\*(?:(?!\\*/).)*?\\*/|//[^\\n]*'\n+def js_to_json(code, *args, **kwargs):\n+\n+    # vars is a dict of (var, val) pairs to substitute\n+    vars = args[0] if len(args) > 0 else kwargs.get('vars', {})\n+    strict = kwargs.get('strict', False)\n+\n+    STRING_QUOTES = '\\'\"`'\n+    STRING_RE = '|'.join(r'{0}(?:\\\\.|[^\\\\{0}])*{0}'.format(q) for q in STRING_QUOTES)\n+    COMMENT_RE = r'/\\*(?:(?!\\*/).)*?\\*/|//[^\\n]*\\n'\n     SKIP_RE = r'\\s*(?:{comment})?\\s*'.format(comment=COMMENT_RE)\n     INTEGER_TABLE = (\n         (r'(?s)^(0[xX][0-9a-fA-F]+){skip}:?$'.format(skip=SKIP_RE), 16),\n         (r'(?s)^(0+[0-7]+){skip}:?$'.format(skip=SKIP_RE), 8),\n+        (r'(?s)^(\\d+){skip}:?$'.format(skip=SKIP_RE), 10),\n     )\n+    # compat candidate\n+    JSONDecodeError = json.JSONDecodeError if 'JSONDecodeError' in dir(json) else ValueError\n+\n+    def process_escape(match):\n+        JSON_PASSTHROUGH_ESCAPES = r'\"\\bfnrtu'\n+        escape = match.group(1) or match.group(2)\n+\n+        return ('\\\\' + escape if escape in JSON_PASSTHROUGH_ESCAPES\n+                else '\\\\u00' if escape == 'x'\n+                else '' if escape == '\\n'\n+                else escape)\n+\n+    def template_substitute(match):\n+        evaluated = js_to_json(match.group(1), vars, strict=strict)\n+        if evaluated[0] == '\"':\n+            return json.loads(evaluated)\n+        return evaluated\n \n     def fix_kv(m):\n         v = m.group(0)\n         if v in ('true', 'false', 'null'):\n             return v\n-        elif v.startswith('/*') or v.startswith('//') or v.startswith('!') or v == ',':\n-            return \"\"\n-\n-        if v[0] in (\"'\", '\"'):\n-            v = re.sub(r'(?s)\\\\.|\"', lambda m: {\n-                '\"': '\\\\\"',\n-                \"\\\\'\": \"'\",\n-                '\\\\\\n': '',\n-                '\\\\x': '\\\\u00',\n-            }.get(m.group(0), m.group(0)), v[1:-1])\n-        else:\n-            for regex, base in INTEGER_TABLE:\n-                im = re.match(regex, v)\n-                if im:\n-                    i = int(im.group(1), base)\n-                    return '\"%d\":' % i if v.endswith(':') else '%d' % i\n+        elif v in ('undefined', 'void 0'):\n+            return 'null'\n+        elif v.startswith('/*') or v.startswith('//') or v == ',':\n+            return ''\n+\n+        if v[0] in STRING_QUOTES:\n+            v = re.sub(r'(?s)\\${([^}]+)}', template_substitute, v[1:-1]) if v[0] == '`' else v[1:-1]\n+            escaped = re.sub(r'(?s)(\")|\\\\(.)', process_escape, v)\n+            return '\"{0}\"'.format(escaped)\n+\n+        inv = IDENTITY\n+        im = re.split(r'^!+', v)\n+        if len(im) > 1 and not im[-1].endswith(':'):\n+            if (len(v) - len(im[1])) % 2 == 1:\n+                inv = lambda x: 'true' if x == 0 else 'false'\n+            else:\n+                inv = lambda x: 'false' if x == 0 else 'true'\n+        if not any(x for x in im):\n+            return\n+        v = im[-1]\n \n-        return '\"%s\"' % v\n+        for regex, base in INTEGER_TABLE:\n+            im = re.match(regex, v)\n+            if im:\n+                i = int(im.group(1), base)\n+                return ('\"%s\":' if v.endswith(':') else '%s') % inv(i)\n+\n+        if v in vars:\n+            try:\n+                if not strict:\n+                    json.loads(vars[v])\n+            except JSONDecodeError:\n+                return inv(json.dumps(vars[v]))\n+            else:\n+                return inv(vars[v])\n+\n+        if not strict:\n+            v = try_call(inv, args=(v,), default=v)\n+            if v in ('true', 'false'):\n+                return v\n+            return '\"{0}\"'.format(v)\n+\n+        raise ValueError('Unknown value: ' + v)\n+\n+    def create_map(mobj):\n+        return json.dumps(dict(json.loads(js_to_json(mobj.group(1) or '[]', vars=vars))))\n+\n+    code = re.sub(r'new Map\\((\\[.*?\\])?\\)', create_map, code)\n+    if not strict:\n+        code = re.sub(r'new Date\\((\".+\")\\)', r'\\g<1>', code)\n+        code = re.sub(r'new \\w+\\((.*?)\\)', lambda m: json.dumps(m.group(0)), code)\n+        code = re.sub(r'parseInt\\([^\\d]+(\\d+)[^\\d]+\\)', r'\\1', code)\n+        code = re.sub(r'\\(function\\([^)]*\\)\\s*\\{[^}]*\\}\\s*\\)\\s*\\(\\s*([\"\\'][^)]*[\"\\'])\\s*\\)', r'\\1', code)\n \n     return re.sub(r'''(?sx)\n-        \"(?:[^\"\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^\"\\\\]*\"|\n-        '(?:[^'\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^'\\\\]*'|\n-        {comment}|,(?={skip}[\\]}}])|\n-        (?:(?<![0-9])[eE]|[a-df-zA-DF-Z_])[.a-zA-Z_0-9]*|\n-        \\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:{skip}:)?|\n-        [0-9]+(?={skip}:)|\n+        {str_}|\n+        {comment}|\n+        ,(?={skip}[\\]}}])|\n+        void\\s0|\n+        !*(?:(?<!\\d)[eE]|[a-df-zA-DF-Z_$])[.a-zA-Z_$0-9]*|\n+        (?:\\b|!+)0(?:[xX][\\da-fA-F]+|[0-7]+)(?:{skip}:)?|\n+        !+\\d+(?:\\.\\d*)?(?:{skip}:)?|\n+        [0-9]+(?:{skip}:)|\n         !+\n-        '''.format(comment=COMMENT_RE, skip=SKIP_RE), fix_kv, code)\n+        '''.format(comment=COMMENT_RE, skip=SKIP_RE, str_=STRING_RE), fix_kv, code)\n \n \n def qualities(quality_ids):\n@@ -4206,12 +4648,7 @@ def args_to_str(args):\n \n \n def error_to_compat_str(err):\n-    err_str = str(err)\n-    # On python 2 error byte string must be decoded with proper\n-    # encoding rather than ascii\n-    if sys.version_info[0] < 3:\n-        err_str = err_str.decode(preferredencoding())\n-    return err_str\n+    return _decode_compat_str(str(err))\n \n \n def mimetype2ext(mt):\n@@ -5401,7 +5838,7 @@ def proxy_open(self, req, proxy, type):\n \n         if proxy == '__noproxy__':\n             return None  # No Proxy\n-        if compat_urlparse.urlparse(proxy).scheme.lower() in ('socks', 'socks4', 'socks4a', 'socks5'):\n+        if compat_urllib_parse.urlparse(proxy).scheme.lower() in ('socks', 'socks4', 'socks4a', 'socks5'):\n             req.add_header('Ytdl-socks-proxy', proxy)\n             # youtube-dl's http/https handlers do wrapping the socket with socks\n             return None\n@@ -5725,7 +6162,7 @@ def write_xattr(path, key, value):\n                         cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE)\n                 except EnvironmentError as e:\n                     raise XAttrMetadataError(e.errno, e.strerror)\n-                stdout, stderr = p.communicate()\n+                stdout, stderr = process_communicate_or_kill(p)\n                 stderr = stderr.decode('utf-8', 'replace')\n                 if p.returncode != 0:\n                     raise XAttrMetadataError(p.returncode, stderr)\n@@ -5772,3 +6209,509 @@ def clean_podcast_url(url):\n                 st\\.fm # https://podsights.com/docs/\n             )/e\n         )/''', '', url)\n+\n+\n+if __debug__:\n+    # Raise TypeError if args can't be bound\n+    # needs compat owing to unstable inspect API, thanks PSF :-(\n+    try:\n+        inspect.signature\n+\n+        def _try_bind_args(fn, *args, **kwargs):\n+            inspect.signature(fn).bind(*args, **kwargs)\n+    except AttributeError:\n+        # Py < 3.3\n+        def _try_bind_args(fn, *args, **kwargs):\n+            fn_args = inspect.getargspec(fn)\n+            # Py2: ArgInfo(args, varargs, keywords, defaults)\n+            # Py3: ArgSpec(args, varargs, keywords, defaults)\n+            if not fn_args.keywords:\n+                for k in kwargs:\n+                    if k not in (fn_args.args or []):\n+                        raise TypeError(\"got an unexpected keyword argument: '{0}'\".format(k))\n+            if not fn_args.varargs:\n+                args_to_bind = len(args)\n+                bindable = len(fn_args.args or [])\n+                if args_to_bind > bindable:\n+                    raise TypeError('too many positional arguments')\n+                bindable -= len(fn_args.defaults or [])\n+                if args_to_bind < bindable:\n+                    if kwargs:\n+                        bindable -= len(set(fn_args.args or []) & set(kwargs))\n+                    if bindable > args_to_bind:\n+                        raise TypeError(\"missing a required argument: '{0}'\".format(fn_args.args[args_to_bind]))\n+\n+\n+def traverse_obj(obj, *paths, **kwargs):\n+    \"\"\"\n+    Safely traverse nested `dict`s and `Iterable`s, etc\n+\n+    >>> obj = [{}, {\"key\": \"value\"}]\n+    >>> traverse_obj(obj, (1, \"key\"))\n+    'value'\n+\n+    Each of the provided `paths` is tested and the first producing a valid result will be returned.\n+    The next path will also be tested if the path branched but no results could be found.\n+    Supported values for traversal are `Mapping`, `Iterable`, `re.Match`, `xml.etree.ElementTree`\n+    (xpath) and `http.cookies.Morsel`.\n+    Unhelpful values (`{}`, `None`) are treated as the absence of a value and discarded.\n+\n+    The paths will be wrapped in `variadic`, so that `'key'` is conveniently the same as `('key', )`.\n+\n+    The keys in the path can be one of:\n+        - `None`:           Return the current object.\n+        - `set`:            Requires the only item in the set to be a type or function,\n+                            like `{type}`/`{type, type, ...}`/`{func}`. If one or more `type`s,\n+                            return only values that have one of the types. If a function,\n+                            return `func(obj)`.\n+        - `str`/`int`:      Return `obj[key]`. For `re.Match`, return `obj.group(key)`.\n+        - `slice`:          Branch out and return all values in `obj[key]`.\n+        - `Ellipsis`:       Branch out and return a list of all values.\n+        - `tuple`/`list`:   Branch out and return a list of all matching values.\n+                            Read as: `[traverse_obj(obj, branch) for branch in branches]`.\n+        - `function`:       Branch out and return values filtered by the function.\n+                            Read as: `[value for key, value in obj if function(key, value)]`.\n+                            For `Sequence`s, `key` is the index of the value.\n+                            For `Iterable`s, `key` is the enumeration count of the value.\n+                            For `re.Match`es, `key` is the group number (0 = full match)\n+                            as well as additionally any group names, if given.\n+        - `dict`:           Transform the current object and return a matching dict.\n+                            Read as: `{key: traverse_obj(obj, path) for key, path in dct.items()}`.\n+        - `any`-builtin:    Take the first matching object and return it, resetting branching.\n+        - `all`-builtin:    Take all matching objects and return them as a list, resetting branching.\n+\n+        `tuple`, `list`, and `dict` all support nested paths and branches.\n+\n+    @params paths           Paths which to traverse by.\n+    Keyword arguments:\n+    @param default          Value to return if the paths do not match.\n+                            If the last key in the path is a `dict`, it will apply to each value inside\n+                            the dict instead, depth first. Try to avoid if using nested `dict` keys.\n+    @param expected_type    If a `type`, only accept final values of this type.\n+                            If any other callable, try to call the function on each result.\n+                            If the last key in the path is a `dict`, it will apply to each value inside\n+                            the dict instead, recursively. This does respect branching paths.\n+    @param get_all          If `False`, return the first matching result, otherwise all matching ones.\n+    @param casesense        If `False`, consider string dictionary keys as case insensitive.\n+\n+    The following is only meant to be used by YoutubeDL.prepare_outtmpl and is not part of the API\n+\n+    @param _traverse_string  Whether to traverse into objects as strings.\n+                            If `True`, any non-compatible object will first be\n+                            converted into a string and then traversed into.\n+                            The return value of that path will be a string instead,\n+                            not respecting any further branching.\n+\n+\n+    @returns                The result of the object traversal.\n+                            If successful, `get_all=True`, and the path branches at least once,\n+                            then a list of results is returned instead.\n+                            A list is always returned if the last path branches and no `default` is given.\n+                            If a path ends on a `dict` that result will always be a `dict`.\n+    \"\"\"\n+\n+    # parameter defaults\n+    default = kwargs.get('default', NO_DEFAULT)\n+    expected_type = kwargs.get('expected_type')\n+    get_all = kwargs.get('get_all', True)\n+    casesense = kwargs.get('casesense', True)\n+    _traverse_string = kwargs.get('_traverse_string', False)\n+\n+    # instant compat\n+    str = compat_str\n+\n+    casefold = lambda k: compat_casefold(k) if isinstance(k, str) else k\n+\n+    if isinstance(expected_type, type):\n+        type_test = lambda val: val if isinstance(val, expected_type) else None\n+    else:\n+        type_test = lambda val: try_call(expected_type or IDENTITY, args=(val,))\n+\n+    def lookup_or_none(v, k, getter=None):\n+        with compat_contextlib_suppress(LookupError):\n+            return getter(v, k) if getter else v[k]\n+\n+    def from_iterable(iterables):\n+        # chain.from_iterable(['ABC', 'DEF']) --> A B C D E F\n+        for it in iterables:\n+            for item in it:\n+                yield item\n+\n+    def apply_key(key, obj, is_last):\n+        branching = False\n+\n+        if obj is None and _traverse_string:\n+            if key is Ellipsis or callable(key) or isinstance(key, slice):\n+                branching = True\n+                result = ()\n+            else:\n+                result = None\n+\n+        elif key is None:\n+            result = obj\n+\n+        elif isinstance(key, set):\n+            assert len(key) >= 1, 'At least one item is required in a `set` key'\n+            if all(isinstance(item, type) for item in key):\n+                result = obj if isinstance(obj, tuple(key)) else None\n+            else:\n+                item = next(iter(key))\n+                assert len(key) == 1, 'Multiple items in a `set` key must all be types'\n+                result = try_call(item, args=(obj,)) if not isinstance(item, type) else None\n+\n+        elif isinstance(key, (list, tuple)):\n+            branching = True\n+            result = from_iterable(\n+                apply_path(obj, branch, is_last)[0] for branch in key)\n+\n+        elif key is Ellipsis:\n+            branching = True\n+            if isinstance(obj, compat_http_cookies.Morsel):\n+                obj = dict(obj, key=obj.key, value=obj.value)\n+            if isinstance(obj, compat_collections_abc.Mapping):\n+                result = obj.values()\n+            elif is_iterable_like(obj, (compat_collections_abc.Iterable, compat_etree_Element)):\n+                result = obj\n+            elif isinstance(obj, compat_re_Match):\n+                result = obj.groups()\n+            elif _traverse_string:\n+                branching = False\n+                result = str(obj)\n+            else:\n+                result = ()\n+\n+        elif callable(key):\n+            branching = True\n+            if isinstance(obj, compat_http_cookies.Morsel):\n+                obj = dict(obj, key=obj.key, value=obj.value)\n+            if isinstance(obj, compat_collections_abc.Mapping):\n+                iter_obj = obj.items()\n+            elif is_iterable_like(obj, (compat_collections_abc.Iterable, compat_etree_Element)):\n+                iter_obj = enumerate(obj)\n+            elif isinstance(obj, compat_re_Match):\n+                iter_obj = itertools.chain(\n+                    enumerate(itertools.chain((obj.group(),), obj.groups())),\n+                    obj.groupdict().items())\n+            elif _traverse_string:\n+                branching = False\n+                iter_obj = enumerate(str(obj))\n+            else:\n+                iter_obj = ()\n+\n+            result = (v for k, v in iter_obj if try_call(key, args=(k, v)))\n+            if not branching:  # string traversal\n+                result = ''.join(result)\n+\n+        elif isinstance(key, dict):\n+            iter_obj = ((k, _traverse_obj(obj, v, False, is_last)) for k, v in key.items())\n+            result = dict((k, v if v is not None else default) for k, v in iter_obj\n+                          if v is not None or default is not NO_DEFAULT) or None\n+\n+        elif isinstance(obj, compat_collections_abc.Mapping):\n+            if isinstance(obj, compat_http_cookies.Morsel):\n+                obj = dict(obj, key=obj.key, value=obj.value)\n+            result = (try_call(obj.get, args=(key,))\n+                      if casesense or try_call(obj.__contains__, args=(key,))\n+                      else next((v for k, v in obj.items() if casefold(k) == key), None))\n+\n+        elif isinstance(obj, compat_re_Match):\n+            result = None\n+            if isinstance(key, int) or casesense:\n+                # Py 2.6 doesn't have methods in the Match class/type\n+                result = lookup_or_none(obj, key, getter=lambda _, k: obj.group(k))\n+\n+            elif isinstance(key, str):\n+                result = next((v for k, v in obj.groupdict().items()\n+                              if casefold(k) == key), None)\n+\n+        else:\n+            result = None\n+            if isinstance(key, (int, slice)):\n+                if is_iterable_like(obj, (compat_collections_abc.Sequence, compat_etree_Element)):\n+                    branching = isinstance(key, slice)\n+                    result = lookup_or_none(obj, key)\n+                elif _traverse_string:\n+                    result = lookup_or_none(str(obj), key)\n+\n+            elif isinstance(obj, compat_etree_Element) and isinstance(key, str):\n+                xpath, _, special = key.rpartition('/')\n+                if not special.startswith('@') and not special.endswith('()'):\n+                    xpath = key\n+                    special = None\n+\n+                # Allow abbreviations of relative paths, absolute paths error\n+                if xpath.startswith('/'):\n+                    xpath = '.' + xpath\n+                elif xpath and not xpath.startswith('./'):\n+                    xpath = './' + xpath\n+\n+                def apply_specials(element):\n+                    if special is None:\n+                        return element\n+                    if special == '@':\n+                        return element.attrib\n+                    if special.startswith('@'):\n+                        return try_call(element.attrib.get, args=(special[1:],))\n+                    if special == 'text()':\n+                        return element.text\n+                    raise SyntaxError('apply_specials is missing case for {0!r}'.format(special))\n+\n+                if xpath:\n+                    result = list(map(apply_specials, compat_etree_iterfind(obj, xpath)))\n+                else:\n+                    result = apply_specials(obj)\n+\n+        return branching, result if branching else (result,)\n+\n+    def lazy_last(iterable):\n+        iterator = iter(iterable)\n+        prev = next(iterator, NO_DEFAULT)\n+        if prev is NO_DEFAULT:\n+            return\n+\n+        for item in iterator:\n+            yield False, prev\n+            prev = item\n+\n+        yield True, prev\n+\n+    def apply_path(start_obj, path, test_type):\n+        objs = (start_obj,)\n+        has_branched = False\n+\n+        key = None\n+        for last, key in lazy_last(variadic(path, (str, bytes, dict, set))):\n+            if not casesense and isinstance(key, str):\n+                key = compat_casefold(key)\n+\n+            if key in (any, all):\n+                has_branched = False\n+                filtered_objs = (obj for obj in objs if obj not in (None, {}))\n+                if key is any:\n+                    objs = (next(filtered_objs, None),)\n+                else:\n+                    objs = (list(filtered_objs),)\n+                continue\n+\n+            if __debug__ and callable(key):\n+                # Verify function signature\n+                _try_bind_args(key, None, None)\n+\n+            new_objs = []\n+            for obj in objs:\n+                branching, results = apply_key(key, obj, last)\n+                has_branched |= branching\n+                new_objs.append(results)\n+\n+            objs = from_iterable(new_objs)\n+\n+        if test_type and not isinstance(key, (dict, list, tuple)):\n+            objs = map(type_test, objs)\n+\n+        return objs, has_branched, isinstance(key, dict)\n+\n+    def _traverse_obj(obj, path, allow_empty, test_type):\n+        results, has_branched, is_dict = apply_path(obj, path, test_type)\n+        results = LazyList(x for x in results if x not in (None, {}))\n+\n+        if get_all and has_branched:\n+            if results:\n+                return results.exhaust()\n+            if allow_empty:\n+                return [] if default is NO_DEFAULT else default\n+            return None\n+\n+        return results[0] if results else {} if allow_empty and is_dict else None\n+\n+    for index, path in enumerate(paths, 1):\n+        result = _traverse_obj(obj, path, index == len(paths), True)\n+        if result is not None:\n+            return result\n+\n+    return None if default is NO_DEFAULT else default\n+\n+\n+def T(*x):\n+    \"\"\" For use in yt-dl instead of {type, ...} or set((type, ...)) \"\"\"\n+    return set(x)\n+\n+\n+def get_first(obj, keys, **kwargs):\n+    return traverse_obj(obj, (Ellipsis,) + tuple(variadic(keys)), get_all=False, **kwargs)\n+\n+\n+def join_nonempty(*values, **kwargs):\n+\n+    # parameter defaults\n+    delim = kwargs.get('delim', '-')\n+    from_dict = kwargs.get('from_dict')\n+\n+    if from_dict is not None:\n+        values = (traverse_obj(from_dict, variadic(v)) for v in values)\n+    return delim.join(map(compat_str, filter(None, values)))\n+\n+\n+class Namespace(object):\n+    \"\"\"Immutable namespace\"\"\"\n+\n+    def __init__(self, **kw_attr):\n+        self.__dict__.update(kw_attr)\n+\n+    def __iter__(self):\n+        return iter(self.__dict__.values())\n+\n+    @property\n+    def items_(self):\n+        return self.__dict__.items()\n+\n+\n+MEDIA_EXTENSIONS = Namespace(\n+    common_video=('avi', 'flv', 'mkv', 'mov', 'mp4', 'webm'),\n+    video=('3g2', '3gp', 'f4v', 'mk3d', 'divx', 'mpg', 'ogv', 'm4v', 'wmv'),\n+    common_audio=('aiff', 'alac', 'flac', 'm4a', 'mka', 'mp3', 'ogg', 'opus', 'wav'),\n+    audio=('aac', 'ape', 'asf', 'f4a', 'f4b', 'm4b', 'm4p', 'm4r', 'oga', 'ogx', 'spx', 'vorbis', 'wma', 'weba'),\n+    thumbnails=('jpg', 'png', 'webp'),\n+    # storyboards=('mhtml', ),\n+    subtitles=('srt', 'vtt', 'ass', 'lrc', 'ttml'),\n+    manifests=('f4f', 'f4m', 'm3u8', 'smil', 'mpd'),\n+)\n+MEDIA_EXTENSIONS.video = MEDIA_EXTENSIONS.common_video + MEDIA_EXTENSIONS.video\n+MEDIA_EXTENSIONS.audio = MEDIA_EXTENSIONS.common_audio + MEDIA_EXTENSIONS.audio\n+\n+KNOWN_EXTENSIONS = (\n+    MEDIA_EXTENSIONS.video + MEDIA_EXTENSIONS.audio\n+    + MEDIA_EXTENSIONS.manifests\n+)\n+\n+\n+class _UnsafeExtensionError(Exception):\n+    \"\"\"\n+    Mitigation exception for unwanted file overwrite/path traversal\n+\n+    Ref: https://github.com/yt-dlp/yt-dlp/security/advisories/GHSA-79w7-vh3h-8g4j\n+    \"\"\"\n+    _ALLOWED_EXTENSIONS = frozenset(itertools.chain(\n+        (   # internal\n+            'description',\n+            'json',\n+            'meta',\n+            'orig',\n+            'part',\n+            'temp',\n+            'uncut',\n+            'unknown_video',\n+            'ytdl',\n+        ),\n+        # video\n+        MEDIA_EXTENSIONS.video, (\n+            'asx',\n+            'ismv',\n+            'm2t',\n+            'm2ts',\n+            'm2v',\n+            'm4s',\n+            'mng',\n+            'mp2v',\n+            'mp4v',\n+            'mpe',\n+            'mpeg',\n+            'mpeg1',\n+            'mpeg2',\n+            'mpeg4',\n+            'mxf',\n+            'ogm',\n+            'qt',\n+            'rm',\n+            'swf',\n+            'ts',\n+            'vob',\n+            'vp9',\n+        ),\n+        # audio\n+        MEDIA_EXTENSIONS.audio, (\n+            '3ga',\n+            'ac3',\n+            'adts',\n+            'aif',\n+            'au',\n+            'dts',\n+            'isma',\n+            'it',\n+            'mid',\n+            'mod',\n+            'mpga',\n+            'mp1',\n+            'mp2',\n+            'mp4a',\n+            'mpa',\n+            'ra',\n+            'shn',\n+            'xm',\n+        ),\n+        # image\n+        MEDIA_EXTENSIONS.thumbnails, (\n+            'avif',\n+            'bmp',\n+            'gif',\n+            'ico',\n+            'heic',\n+            'jng',\n+            'jpeg',\n+            'jxl',\n+            'svg',\n+            'tif',\n+            'tiff',\n+            'wbmp',\n+        ),\n+        # subtitle\n+        MEDIA_EXTENSIONS.subtitles, (\n+            'dfxp',\n+            'fs',\n+            'ismt',\n+            'json3',\n+            'sami',\n+            'scc',\n+            'srv1',\n+            'srv2',\n+            'srv3',\n+            'ssa',\n+            'tt',\n+            'xml',\n+        ),\n+        # others\n+        MEDIA_EXTENSIONS.manifests,\n+        (\n+            # not used in yt-dl\n+            # *MEDIA_EXTENSIONS.storyboards,\n+            # 'desktop',\n+            # 'ism',\n+            # 'm3u',\n+            # 'sbv',\n+            # 'swp',\n+            # 'url',\n+            # 'webloc',\n+        )))\n+\n+    def __init__(self, extension):\n+        super(_UnsafeExtensionError, self).__init__('unsafe file extension: {0!r}'.format(extension))\n+        self.extension = extension\n+\n+    # support --no-check-extensions\n+    lenient = False\n+\n+    @classmethod\n+    def sanitize_extension(cls, extension, **kwargs):\n+        # ... /, *, prepend=False\n+        prepend = kwargs.get('prepend', False)\n+\n+        if '/' in extension or '\\\\' in extension:\n+            raise cls(extension)\n+\n+        if not prepend:\n+            last = extension.rpartition('.')[-1]\n+            if last == 'bin':\n+                extension = last = 'unknown_video'\n+            if not (cls.lenient or last.lower() in cls._ALLOWED_EXTENSIONS):\n+                raise cls(extension)\n+\n+        return extension\n", "test_patch": "diff --git a/test/helper.py b/test/helper.py\nindex e62aab11e77..6f2129efff3 100644\n--- a/test/helper.py\n+++ b/test/helper.py\n@@ -1,22 +1,24 @@\n from __future__ import unicode_literals\n \n import errno\n-import io\n import hashlib\n import json\n import os.path\n import re\n-import types\n import ssl\n import sys\n+import types\n+import unittest\n \n import youtube_dl.extractor\n from youtube_dl import YoutubeDL\n from youtube_dl.compat import (\n+    compat_open as open,\n     compat_os_name,\n     compat_str,\n )\n from youtube_dl.utils import (\n+    IDENTITY,\n     preferredencoding,\n     write_string,\n )\n@@ -27,10 +29,10 @@ def get_params(override=None):\n                                    \"parameters.json\")\n     LOCAL_PARAMETERS_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)),\n                                          \"local_parameters.json\")\n-    with io.open(PARAMETERS_FILE, encoding='utf-8') as pf:\n+    with open(PARAMETERS_FILE, encoding='utf-8') as pf:\n         parameters = json.load(pf)\n     if os.path.exists(LOCAL_PARAMETERS_FILE):\n-        with io.open(LOCAL_PARAMETERS_FILE, encoding='utf-8') as pf:\n+        with open(LOCAL_PARAMETERS_FILE, encoding='utf-8') as pf:\n             parameters.update(json.load(pf))\n     if override:\n         parameters.update(override)\n@@ -72,7 +74,8 @@ def __init__(self, override=None):\n     def to_screen(self, s, skip_eol=None):\n         print(s)\n \n-    def trouble(self, s, tb=None):\n+    def trouble(self, *args, **kwargs):\n+        s = args[0] if len(args) > 0 else kwargs.get('message', 'Missing message')\n         raise Exception(s)\n \n     def download(self, x):\n@@ -89,6 +92,17 @@ def report_warning(self, message):\n         self.report_warning = types.MethodType(report_warning, self)\n \n \n+class FakeLogger(object):\n+    def debug(self, msg):\n+        pass\n+\n+    def warning(self, msg):\n+        pass\n+\n+    def error(self, msg):\n+        pass\n+\n+\n def gettestcases(include_onlymatching=False):\n     for ie in youtube_dl.extractor.gen_extractors():\n         for tc in ie.get_testcases(include_onlymatching):\n@@ -128,6 +142,12 @@ def expect_value(self, got, expected, field):\n         self.assertTrue(\n             contains_str in got,\n             'field %s (value: %r) should contain %r' % (field, got, contains_str))\n+    elif isinstance(expected, compat_str) and re.match(r'lambda \\w+:', expected):\n+        fn = eval(expected)\n+        suite = expected.split(':', 1)[1].strip()\n+        self.assertTrue(\n+            fn(got),\n+            'Expected field %s to meet condition %s, but value %r failed ' % (field, suite, got))\n     elif isinstance(expected, type):\n         self.assertTrue(\n             isinstance(got, expected),\n@@ -137,7 +157,7 @@ def expect_value(self, got, expected, field):\n     elif isinstance(expected, list) and isinstance(got, list):\n         self.assertEqual(\n             len(expected), len(got),\n-            'Expect a list of length %d, but got a list of length %d for field %s' % (\n+            'Expected a list of length %d, but got a list of length %d for field %s' % (\n                 len(expected), len(got), field))\n         for index, (item_got, item_expected) in enumerate(zip(got, expected)):\n             type_got = type(item_got)\n@@ -161,18 +181,18 @@ def expect_value(self, got, expected, field):\n             op, _, expected_num = expected.partition(':')\n             expected_num = int(expected_num)\n             if op == 'mincount':\n-                assert_func = assertGreaterEqual\n+                assert_func = self.assertGreaterEqual\n                 msg_tmpl = 'Expected %d items in field %s, but only got %d'\n             elif op == 'maxcount':\n-                assert_func = assertLessEqual\n+                assert_func = self.assertLessEqual\n                 msg_tmpl = 'Expected maximum %d items in field %s, but got %d'\n             elif op == 'count':\n-                assert_func = assertEqual\n+                assert_func = self.assertEqual\n                 msg_tmpl = 'Expected exactly %d items in field %s, but got %d'\n             else:\n                 assert False\n             assert_func(\n-                self, len(got), expected_num,\n+                len(got), expected_num,\n                 msg_tmpl % (expected_num, field, len(got)))\n             return\n         self.assertEqual(\n@@ -242,27 +262,6 @@ def assertRegexpMatches(self, text, regexp, msg=None):\n             self.assertTrue(m, msg)\n \n \n-def assertGreaterEqual(self, got, expected, msg=None):\n-    if not (got >= expected):\n-        if msg is None:\n-            msg = '%r not greater than or equal to %r' % (got, expected)\n-        self.assertTrue(got >= expected, msg)\n-\n-\n-def assertLessEqual(self, got, expected, msg=None):\n-    if not (got <= expected):\n-        if msg is None:\n-            msg = '%r not less than or equal to %r' % (got, expected)\n-        self.assertTrue(got <= expected, msg)\n-\n-\n-def assertEqual(self, got, expected, msg=None):\n-    if not (got == expected):\n-        if msg is None:\n-            msg = '%r not equal to %r' % (got, expected)\n-        self.assertTrue(got == expected, msg)\n-\n-\n def expect_warnings(ydl, warnings_re):\n     real_warning = ydl.report_warning\n \n@@ -280,3 +279,7 @@ def http_server_port(httpd):\n     else:\n         sock = httpd.socket\n     return sock.getsockname()[1]\n+\n+\n+def expectedFailureIf(cond):\n+    return unittest.expectedFailure if cond else IDENTITY\ndiff --git a/test/test_InfoExtractor.py b/test/test_InfoExtractor.py\nindex dd69a681bef..09100a1d667 100644\n--- a/test/test_InfoExtractor.py\n+++ b/test/test_InfoExtractor.py\n@@ -3,19 +3,37 @@\n from __future__ import unicode_literals\n \n # Allow direct execution\n-import io\n import os\n import sys\n import unittest\n+\n sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n \n-from test.helper import FakeYDL, expect_dict, expect_value, http_server_port\n-from youtube_dl.compat import compat_etree_fromstring, compat_http_server\n-from youtube_dl.extractor.common import InfoExtractor\n-from youtube_dl.extractor import YoutubeIE, get_info_extractor\n-from youtube_dl.utils import encode_data_uri, strip_jsonp, ExtractorError, RegexNotFoundError\n import threading\n \n+from test.helper import (\n+    expect_dict,\n+    expect_value,\n+    FakeYDL,\n+    http_server_port,\n+)\n+from youtube_dl.compat import (\n+    compat_etree_fromstring,\n+    compat_http_server,\n+    compat_open as open,\n+)\n+from youtube_dl.extractor.common import InfoExtractor\n+from youtube_dl.extractor import (\n+    get_info_extractor,\n+    YoutubeIE,\n+)\n+from youtube_dl.utils import (\n+    encode_data_uri,\n+    ExtractorError,\n+    RegexNotFoundError,\n+    strip_jsonp,\n+)\n+\n \n TEAPOT_RESPONSE_STATUS = 418\n TEAPOT_RESPONSE_BODY = \"<h1>418 I'm a teapot</h1>\"\n@@ -35,13 +53,13 @@ def do_GET(self):\n             assert False\n \n \n-class TestIE(InfoExtractor):\n+class DummyIE(InfoExtractor):\n     pass\n \n \n class TestInfoExtractor(unittest.TestCase):\n     def setUp(self):\n-        self.ie = TestIE(FakeYDL())\n+        self.ie = DummyIE(FakeYDL())\n \n     def test_ie_key(self):\n         self.assertEqual(get_info_extractor(YoutubeIE.ie_key()), YoutubeIE)\n@@ -62,6 +80,7 @@ def test_opengraph(self):\n             <meta name=\"og:test1\" content='foo > < bar'/>\n             <meta name=\"og:test2\" content=\"foo >//< bar\"/>\n             <meta property=og-test3 content='Ill-formatted opengraph'/>\n+            <meta property=og:test4 content=unquoted-value/>\n             '''\n         self.assertEqual(ie._og_search_title(html), 'Foo')\n         self.assertEqual(ie._og_search_description(html), 'Some video\\'s description ')\n@@ -74,6 +93,7 @@ def test_opengraph(self):\n         self.assertEqual(ie._og_search_property(('test0', 'test1'), html), 'foo > < bar')\n         self.assertRaises(RegexNotFoundError, ie._og_search_property, 'test0', html, None, fatal=True)\n         self.assertRaises(RegexNotFoundError, ie._og_search_property, ('test0', 'test00'), html, None, fatal=True)\n+        self.assertEqual(ie._og_search_property('test4', html), 'unquoted-value')\n \n     def test_html_search_meta(self):\n         ie = self.ie\n@@ -98,6 +118,74 @@ def test_html_search_meta(self):\n         self.assertRaises(RegexNotFoundError, ie._html_search_meta, 'z', html, None, fatal=True)\n         self.assertRaises(RegexNotFoundError, ie._html_search_meta, ('z', 'x'), html, None, fatal=True)\n \n+    def test_search_nextjs_data(self):\n+        html = '''\n+<!DOCTYPE html>\n+<html>\n+<head>\n+  <meta http-equiv=\"content-type\" content=\n+  \"text/html; charset=utf-8\">\n+  <meta name=\"viewport\" content=\"width=device-width\">\n+  <title>Test _search_nextjs_data()</title>\n+</head>\n+<body>\n+  <div id=\"__next\">\n+    <div style=\"background-color:#17171E\" class=\"FU\" dir=\"ltr\">\n+      <div class=\"sc-93de261d-0 dyzzYE\">\n+        <div>\n+          <header class=\"HD\"></header>\n+          <main class=\"MN\">\n+            <div style=\"height:0\" class=\"HT0\">\n+              <div style=\"width:NaN%\" data-testid=\n+              \"stream-container\" class=\"WDN\"></div>\n+            </div>\n+          </main>\n+        </div>\n+        <footer class=\"sc-6e5faf91-0 dEGaHS\"></footer>\n+      </div>\n+    </div>\n+  </div>\n+  <script id=\"__NEXT_DATA__\" type=\"application/json\">\n+  {\"props\":{\"pageProps\":{\"video\":{\"id\":\"testid\"}}}}\n+  </script>\n+</body>\n+</html>\n+'''\n+        search = self.ie._search_nextjs_data(html, 'testID')\n+        self.assertEqual(search['props']['pageProps']['video']['id'], 'testid')\n+        search = self.ie._search_nextjs_data(\n+            'no next.js data here, move along', 'testID', default={'status': 0})\n+        self.assertEqual(search['status'], 0)\n+\n+    def test_search_nuxt_data(self):\n+        html = '''\n+<!DOCTYPE html>\n+<html>\n+<head>\n+  <meta http-equiv=\"content-type\" content=\n+  \"text/html; charset=utf-8\">\n+  <title>Nuxt.js Test Page</title>\n+  <meta name=\"viewport\" content=\n+  \"width=device-width, initial-scale=1\">\n+  <meta data-hid=\"robots\" name=\"robots\" content=\"all\">\n+</head>\n+<body class=\"BD\">\n+  <div id=\"__layout\">\n+    <h1 class=\"H1\">Example heading</h1>\n+    <div class=\"IN\">\n+      <p>Decoy text</p>\n+    </div>\n+  </div>\n+  <script>\n+  window.__NUXT__=(function(a,b,c,d,e,f,g,h){return {decoy:\" default\",data:[{track:{id:f,title:g}}]}}(null,null,\"c\",null,null,\"testid\",\"Nuxt.js title\",null));\n+  </script>\n+  <script src=\"/_nuxt/a12345b.js\" defer=\"defer\"></script>\n+</body>\n+</html>\n+'''\n+        search = self.ie._search_nuxt_data(html, 'testID')\n+        self.assertEqual(search['track']['id'], 'testid')\n+\n     def test_search_json_ld_realworld(self):\n         # https://github.com/ytdl-org/youtube-dl/issues/23306\n         expect_dict(\n@@ -346,6 +434,24 @@ def test_parse_html5_media_entries(self):\n                 }],\n             })\n \n+        # from https://0000.studio/\n+        # with type attribute but without extension in URL\n+        expect_dict(\n+            self,\n+            self.ie._parse_html5_media_entries(\n+                'https://0000.studio',\n+                r'''\n+                <video src=\"https://d1ggyt9m8pwf3g.cloudfront.net/protected/ap-northeast-1:1864af40-28d5-492b-b739-b32314b1a527/archive/clip/838db6a7-8973-4cd6-840d-8517e4093c92\"\n+                    controls=\"controls\" type=\"video/mp4\" preload=\"metadata\" autoplay=\"autoplay\" playsinline class=\"object-contain\">\n+                </video>\n+                ''', None)[0],\n+            {\n+                'formats': [{\n+                    'url': 'https://d1ggyt9m8pwf3g.cloudfront.net/protected/ap-northeast-1:1864af40-28d5-492b-b739-b32314b1a527/archive/clip/838db6a7-8973-4cd6-840d-8517e4093c92',\n+                    'ext': 'mp4',\n+                }],\n+            })\n+\n     def test_extract_jwplayer_data_realworld(self):\n         # from http://www.suffolk.edu/sjc/\n         expect_dict(\n@@ -799,8 +905,8 @@ def test_parse_m3u8_formats(self):\n         ]\n \n         for m3u8_file, m3u8_url, expected_formats in _TEST_CASES:\n-            with io.open('./test/testdata/m3u8/%s.m3u8' % m3u8_file,\n-                         mode='r', encoding='utf-8') as f:\n+            with open('./test/testdata/m3u8/%s.m3u8' % m3u8_file,\n+                      mode='r', encoding='utf-8') as f:\n                 formats = self.ie._parse_m3u8_formats(\n                     f.read(), m3u8_url, ext='mp4')\n                 self.ie._sort_formats(formats)\n@@ -890,7 +996,8 @@ def test_parse_mpd_formats(self):\n                     'tbr': 5997.485,\n                     'width': 1920,\n                     'height': 1080,\n-                }]\n+                }],\n+                {},\n             ), (\n                 # https://github.com/ytdl-org/youtube-dl/pull/14844\n                 'urls_only',\n@@ -973,7 +1080,8 @@ def test_parse_mpd_formats(self):\n                     'tbr': 4400,\n                     'width': 1920,\n                     'height': 1080,\n-                }]\n+                }],\n+                {},\n             ), (\n                 # https://github.com/ytdl-org/youtube-dl/issues/20346\n                 # Media considered unfragmented even though it contains\n@@ -1019,18 +1127,185 @@ def test_parse_mpd_formats(self):\n                     'width': 360,\n                     'height': 360,\n                     'fps': 30,\n-                }]\n+                }],\n+                {},\n+            ), (\n+                # https://github.com/ytdl-org/youtube-dl/issues/30235\n+                # Bento4 generated test mpd\n+                # mp4dash --mpd-name=manifest.mpd --no-split --use-segment-list mediafiles\n+                'url_and_range',\n+                'http://unknown/manifest.mpd',  # mpd_url\n+                'http://unknown/',  # mpd_base_url\n+                [{\n+                    'manifest_url': 'http://unknown/manifest.mpd',\n+                    'fragment_base_url': 'http://unknown/',\n+                    'ext': 'm4a',\n+                    'format_id': 'audio-und-mp4a.40.2',\n+                    'format_note': 'DASH audio',\n+                    'container': 'm4a_dash',\n+                    'protocol': 'http_dash_segments',\n+                    'acodec': 'mp4a.40.2',\n+                    'vcodec': 'none',\n+                    'tbr': 98.808,\n+                }, {\n+                    'manifest_url': 'http://unknown/manifest.mpd',\n+                    'fragment_base_url': 'http://unknown/',\n+                    'ext': 'mp4',\n+                    'format_id': 'video-avc1',\n+                    'format_note': 'DASH video',\n+                    'container': 'mp4_dash',\n+                    'protocol': 'http_dash_segments',\n+                    'acodec': 'none',\n+                    'vcodec': 'avc1.4D401E',\n+                    'tbr': 699.597,\n+                    'width': 768,\n+                    'height': 432\n+                }],\n+                {},\n+            ), (\n+                # https://github.com/ytdl-org/youtube-dl/issues/27575\n+                # GPAC generated test mpd\n+                # MP4Box -dash 10000 -single-file -out manifest.mpd mediafiles\n+                'range_only',\n+                'http://unknown/manifest.mpd',  # mpd_url\n+                'http://unknown/',  # mpd_base_url\n+                [{\n+                    'manifest_url': 'http://unknown/manifest.mpd',\n+                    'fragment_base_url': 'http://unknown/audio_dashinit.mp4',\n+                    'ext': 'm4a',\n+                    'format_id': '2',\n+                    'format_note': 'DASH audio',\n+                    'container': 'm4a_dash',\n+                    'protocol': 'http_dash_segments',\n+                    'acodec': 'mp4a.40.2',\n+                    'vcodec': 'none',\n+                    'tbr': 98.096,\n+                }, {\n+                    'manifest_url': 'http://unknown/manifest.mpd',\n+                    'fragment_base_url': 'http://unknown/video_dashinit.mp4',\n+                    'ext': 'mp4',\n+                    'format_id': '1',\n+                    'format_note': 'DASH video',\n+                    'container': 'mp4_dash',\n+                    'protocol': 'http_dash_segments',\n+                    'acodec': 'none',\n+                    'vcodec': 'avc1.4D401E',\n+                    'tbr': 526.987,\n+                    'width': 768,\n+                    'height': 432\n+                }],\n+                {},\n+            ), (\n+                'subtitles',\n+                'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n+                'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/',\n+                [{\n+                    'format_id': 'audio=128001',\n+                    'manifest_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n+                    'ext': 'm4a',\n+                    'tbr': 128.001,\n+                    'asr': 48000,\n+                    'format_note': 'DASH audio',\n+                    'container': 'm4a_dash',\n+                    'vcodec': 'none',\n+                    'acodec': 'mp4a.40.2',\n+                    'url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n+                    'fragment_base_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/dash/',\n+                    'protocol': 'http_dash_segments',\n+                }, {\n+                    'format_id': 'video=100000',\n+                    'manifest_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n+                    'ext': 'mp4',\n+                    'width': 336,\n+                    'height': 144,\n+                    'tbr': 100,\n+                    'format_note': 'DASH video',\n+                    'container': 'mp4_dash',\n+                    'vcodec': 'avc1.4D401F',\n+                    'acodec': 'none',\n+                    'url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n+                    'fragment_base_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/dash/',\n+                    'protocol': 'http_dash_segments',\n+                }, {\n+                    'format_id': 'video=326000',\n+                    'manifest_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n+                    'ext': 'mp4',\n+                    'width': 562,\n+                    'height': 240,\n+                    'tbr': 326,\n+                    'format_note': 'DASH video',\n+                    'container': 'mp4_dash',\n+                    'vcodec': 'avc1.4D401F',\n+                    'acodec': 'none',\n+                    'url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n+                    'fragment_base_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/dash/',\n+                    'protocol': 'http_dash_segments',\n+                }, {\n+                    'format_id': 'video=698000',\n+                    'manifest_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n+                    'ext': 'mp4',\n+                    'width': 844,\n+                    'height': 360,\n+                    'tbr': 698,\n+                    'format_note': 'DASH video',\n+                    'container': 'mp4_dash',\n+                    'vcodec': 'avc1.4D401F',\n+                    'acodec': 'none',\n+                    'url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n+                    'fragment_base_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/dash/',\n+                    'protocol': 'http_dash_segments',\n+                }, {\n+                    'format_id': 'video=1493000',\n+                    'manifest_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n+                    'ext': 'mp4',\n+                    'width': 1126,\n+                    'height': 480,\n+                    'tbr': 1493,\n+                    'format_note': 'DASH video',\n+                    'container': 'mp4_dash',\n+                    'vcodec': 'avc1.4D401F',\n+                    'acodec': 'none',\n+                    'url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n+                    'fragment_base_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/dash/',\n+                    'protocol': 'http_dash_segments',\n+                }, {\n+                    'format_id': 'video=4482000',\n+                    'manifest_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n+                    'ext': 'mp4',\n+                    'width': 1688,\n+                    'height': 720,\n+                    'tbr': 4482,\n+                    'format_note': 'DASH video',\n+                    'container': 'mp4_dash',\n+                    'vcodec': 'avc1.4D401F',\n+                    'acodec': 'none',\n+                    'url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n+                    'fragment_base_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/dash/',\n+                    'protocol': 'http_dash_segments',\n+                }],\n+                {\n+                    'en': [\n+                        {\n+                            'ext': 'mp4',\n+                            'manifest_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n+                            'url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n+                            'fragment_base_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/dash/',\n+                            'protocol': 'http_dash_segments',\n+                        }\n+                    ]\n+                },\n             )\n         ]\n \n-        for mpd_file, mpd_url, mpd_base_url, expected_formats in _TEST_CASES:\n-            with io.open('./test/testdata/mpd/%s.mpd' % mpd_file,\n-                         mode='r', encoding='utf-8') as f:\n-                formats = self.ie._parse_mpd_formats(\n+        for mpd_file, mpd_url, mpd_base_url, expected_formats, expected_subtitles in _TEST_CASES:\n+            with open('./test/testdata/mpd/%s.mpd' % mpd_file,\n+                      mode='r', encoding='utf-8') as f:\n+                formats, subtitles = self.ie._parse_mpd_formats_and_subtitles(\n                     compat_etree_fromstring(f.read().encode('utf-8')),\n                     mpd_base_url=mpd_base_url, mpd_url=mpd_url)\n                 self.ie._sort_formats(formats)\n                 expect_value(self, formats, expected_formats, None)\n+                expect_value(self, subtitles, expected_subtitles, None)\n \n     def test_parse_f4m_formats(self):\n         _TEST_CASES = [\n@@ -1051,8 +1326,8 @@ def test_parse_f4m_formats(self):\n         ]\n \n         for f4m_file, f4m_url, expected_formats in _TEST_CASES:\n-            with io.open('./test/testdata/f4m/%s.f4m' % f4m_file,\n-                         mode='r', encoding='utf-8') as f:\n+            with open('./test/testdata/f4m/%s.f4m' % f4m_file,\n+                      mode='r', encoding='utf-8') as f:\n                 formats = self.ie._parse_f4m_formats(\n                     compat_etree_fromstring(f.read().encode('utf-8')),\n                     f4m_url, None)\n@@ -1099,8 +1374,8 @@ def test_parse_xspf(self):\n         ]\n \n         for xspf_file, xspf_url, expected_entries in _TEST_CASES:\n-            with io.open('./test/testdata/xspf/%s.xspf' % xspf_file,\n-                         mode='r', encoding='utf-8') as f:\n+            with open('./test/testdata/xspf/%s.xspf' % xspf_file,\n+                      mode='r', encoding='utf-8') as f:\n                 entries = self.ie._parse_xspf(\n                     compat_etree_fromstring(f.read().encode('utf-8')),\n                     xspf_file, xspf_url=xspf_url, xspf_base_url=xspf_url)\ndiff --git a/test/test_YoutubeDL.py b/test/test_YoutubeDL.py\nindex a35effe0e4a..d994682b249 100644\n--- a/test/test_YoutubeDL.py\n+++ b/test/test_YoutubeDL.py\n@@ -10,14 +10,31 @@\n sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n \n import copy\n+import json\n \n-from test.helper import FakeYDL, assertRegexpMatches\n+from test.helper import (\n+    FakeYDL,\n+    assertRegexpMatches,\n+    try_rm,\n+)\n from youtube_dl import YoutubeDL\n-from youtube_dl.compat import compat_str, compat_urllib_error\n+from youtube_dl.compat import (\n+    compat_http_cookiejar_Cookie,\n+    compat_http_cookies_SimpleCookie,\n+    compat_kwargs,\n+    compat_open as open,\n+    compat_str,\n+    compat_urllib_error,\n+)\n+\n from youtube_dl.extractor import YoutubeIE\n from youtube_dl.extractor.common import InfoExtractor\n from youtube_dl.postprocessor.common import PostProcessor\n-from youtube_dl.utils import ExtractorError, match_filter_func\n+from youtube_dl.utils import (\n+    ExtractorError,\n+    match_filter_func,\n+    traverse_obj,\n+)\n \n TEST_URL = 'http://localhost/sample.mp4'\n \n@@ -29,11 +46,14 @@ def __init__(self, *args, **kwargs):\n         self.msgs = []\n \n     def process_info(self, info_dict):\n-        self.downloaded_info_dicts.append(info_dict)\n+        self.downloaded_info_dicts.append(info_dict.copy())\n \n     def to_screen(self, msg):\n         self.msgs.append(msg)\n \n+    def dl(self, *args, **kwargs):\n+        assert False, 'Downloader must not be invoked for test_YoutubeDL'\n+\n \n def _make_result(formats, **kwargs):\n     res = {\n@@ -42,8 +62,9 @@ def _make_result(formats, **kwargs):\n         'title': 'testttitle',\n         'extractor': 'testex',\n         'extractor_key': 'TestEx',\n+        'webpage_url': 'http://example.com/watch?v=shenanigans',\n     }\n-    res.update(**kwargs)\n+    res.update(**compat_kwargs(kwargs))\n     return res\n \n \n@@ -681,12 +702,12 @@ def test_postprocessors(self):\n \n         class SimplePP(PostProcessor):\n             def run(self, info):\n-                with open(audiofile, 'wt') as f:\n+                with open(audiofile, 'w') as f:\n                     f.write('EXAMPLE')\n                 return [info['filepath']], info\n \n         def run_pp(params, PP):\n-            with open(filename, 'wt') as f:\n+            with open(filename, 'w') as f:\n                 f.write('EXAMPLE')\n             ydl = YoutubeDL(params)\n             ydl.add_post_processor(PP())\n@@ -705,7 +726,7 @@ def run_pp(params, PP):\n \n         class ModifierPP(PostProcessor):\n             def run(self, info):\n-                with open(info['filepath'], 'wt') as f:\n+                with open(info['filepath'], 'w') as f:\n                     f.write('MODIFIED')\n                 return [], info\n \n@@ -930,17 +951,11 @@ def _real_extract(self, url):\n     # Test case for https://github.com/ytdl-org/youtube-dl/issues/27064\n     def test_ignoreerrors_for_playlist_with_url_transparent_iterable_entries(self):\n \n-        class _YDL(YDL):\n-            def __init__(self, *args, **kwargs):\n-                super(_YDL, self).__init__(*args, **kwargs)\n-\n-            def trouble(self, s, tb=None):\n-                pass\n-\n-        ydl = _YDL({\n+        ydl = YDL({\n             'format': 'extra',\n             'ignoreerrors': True,\n         })\n+        ydl.trouble = lambda *_, **__: None\n \n         class VideoIE(InfoExtractor):\n             _VALID_URL = r'video:(?P<id>\\d+)'\n@@ -997,6 +1012,180 @@ def _real_extract(self, url):\n         self.assertEqual(downloaded['extractor'], 'Video')\n         self.assertEqual(downloaded['extractor_key'], 'Video')\n \n+    def test_default_times(self):\n+        \"\"\"Test addition of missing upload/release/_date from /release_/timestamp\"\"\"\n+        info = {\n+            'id': '1234',\n+            'url': TEST_URL,\n+            'title': 'Title',\n+            'ext': 'mp4',\n+            'timestamp': 1631352900,\n+            'release_timestamp': 1632995931,\n+        }\n+\n+        params = {'simulate': True, }\n+        ydl = FakeYDL(params)\n+        out_info = ydl.process_ie_result(info)\n+        self.assertTrue(isinstance(out_info['upload_date'], compat_str))\n+        self.assertEqual(out_info['upload_date'], '20210911')\n+        self.assertTrue(isinstance(out_info['release_date'], compat_str))\n+        self.assertEqual(out_info['release_date'], '20210930')\n+\n+\n+class TestYoutubeDLCookies(unittest.TestCase):\n+\n+    @staticmethod\n+    def encode_cookie(cookie):\n+        if not isinstance(cookie, dict):\n+            cookie = vars(cookie)\n+        for name, value in cookie.items():\n+            yield name, compat_str(value)\n+\n+    @classmethod\n+    def comparable_cookies(cls, cookies):\n+        # Work around cookiejar cookies not being unicode strings\n+        return sorted(map(tuple, map(sorted, map(cls.encode_cookie, cookies))))\n+\n+    def assertSameCookies(self, c1, c2, msg=None):\n+        return self.assertEqual(\n+            *map(self.comparable_cookies, (c1, c2)),\n+            msg=msg)\n+\n+    def assertSameCookieStrings(self, c1, c2, msg=None):\n+        return self.assertSameCookies(\n+            *map(lambda c: compat_http_cookies_SimpleCookie(c).values(), (c1, c2)),\n+            msg=msg)\n+\n+    def test_header_cookies(self):\n+\n+        ydl = FakeYDL()\n+        ydl.report_warning = lambda *_, **__: None\n+\n+        def cookie(name, value, version=None, domain='', path='', secure=False, expires=None):\n+            return compat_http_cookiejar_Cookie(\n+                version or 0, name, value, None, False,\n+                domain, bool(domain), bool(domain), path, bool(path),\n+                secure, expires, False, None, None, rest={})\n+\n+        test_url, test_domain = (t % ('yt.dl',) for t in ('https://%s/test', '.%s'))\n+\n+        def test(encoded_cookies, cookies, headers=False, round_trip=None, error_re=None):\n+            def _test():\n+                ydl.cookiejar.clear()\n+                ydl._load_cookies(encoded_cookies, autoscope=headers)\n+                if headers:\n+                    ydl._apply_header_cookies(test_url)\n+                data = {'url': test_url}\n+                ydl._calc_headers(data)\n+                self.assertSameCookies(\n+                    cookies, ydl.cookiejar,\n+                    'Extracted cookiejar.Cookie is not the same')\n+                if not headers:\n+                    self.assertSameCookieStrings(\n+                        data.get('cookies'), round_trip or encoded_cookies,\n+                        msg='Cookie is not the same as round trip')\n+                ydl.__dict__['_YoutubeDL__header_cookies'] = []\n+\n+            try:\n+                _test()\n+            except AssertionError:\n+                raise\n+            except Exception as e:\n+                if not error_re:\n+                    raise\n+                assertRegexpMatches(self, e.args[0], error_re.join(('.*',) * 2))\n+\n+        test('test=value; Domain=' + test_domain, [cookie('test', 'value', domain=test_domain)])\n+        test('test=value', [cookie('test', 'value')], error_re='Unscoped cookies are not allowed')\n+        test('cookie1=value1; Domain={0}; Path=/test; cookie2=value2; Domain={0}; Path=/'.format(test_domain), [\n+            cookie('cookie1', 'value1', domain=test_domain, path='/test'),\n+            cookie('cookie2', 'value2', domain=test_domain, path='/')])\n+        cookie_kw = compat_kwargs(\n+            {'domain': test_domain, 'path': '/test', 'secure': True, 'expires': '9999999999', })\n+        test('test=value; Domain={domain}; Path={path}; Secure; Expires={expires}'.format(**cookie_kw), [\n+            cookie('test', 'value', **cookie_kw)])\n+        test('test=\"value; \"; path=/test; domain=' + test_domain, [\n+            cookie('test', 'value; ', domain=test_domain, path='/test')],\n+            round_trip='test=\"value\\\\073 \"; Domain={0}; Path=/test'.format(test_domain))\n+        test('name=; Domain=' + test_domain, [cookie('name', '', domain=test_domain)],\n+             round_trip='name=\"\"; Domain=' + test_domain)\n+        test('test=value', [cookie('test', 'value', domain=test_domain)], headers=True)\n+        test('cookie1=value; Domain={0}; cookie2=value'.format(test_domain), [],\n+             headers=True, error_re='Invalid syntax')\n+        ydl.report_warning = ydl.report_error\n+        test('test=value', [], headers=True, error_re='Passing cookies as a header is a potential security risk')\n+\n+    def test_infojson_cookies(self):\n+        TEST_FILE = 'test_infojson_cookies.info.json'\n+        TEST_URL = 'https://example.com/example.mp4'\n+        COOKIES = 'a=b; Domain=.example.com; c=d; Domain=.example.com'\n+        COOKIE_HEADER = {'Cookie': 'a=b; c=d'}\n+\n+        ydl = FakeYDL()\n+        ydl.process_info = lambda x: ydl._write_info_json('test', x, TEST_FILE)\n+\n+        def make_info(info_header_cookies=False, fmts_header_cookies=False, cookies_field=False):\n+            fmt = {'url': TEST_URL}\n+            if fmts_header_cookies:\n+                fmt['http_headers'] = COOKIE_HEADER\n+            if cookies_field:\n+                fmt['cookies'] = COOKIES\n+            return _make_result([fmt], http_headers=COOKIE_HEADER if info_header_cookies else None)\n+\n+        def test(initial_info, note):\n+\n+            def failure_msg(why):\n+                return ' when '.join((why, note))\n+\n+            result = {}\n+            result['processed'] = ydl.process_ie_result(initial_info)\n+            self.assertTrue(ydl.cookiejar.get_cookies_for_url(TEST_URL),\n+                            msg=failure_msg('No cookies set in cookiejar after initial process'))\n+            ydl.cookiejar.clear()\n+            with open(TEST_FILE) as infojson:\n+                result['loaded'] = ydl.sanitize_info(json.load(infojson), True)\n+            result['final'] = ydl.process_ie_result(result['loaded'].copy(), download=False)\n+            self.assertTrue(ydl.cookiejar.get_cookies_for_url(TEST_URL),\n+                            msg=failure_msg('No cookies set in cookiejar after final process'))\n+            ydl.cookiejar.clear()\n+            for key in ('processed', 'loaded', 'final'):\n+                info = result[key]\n+                self.assertIsNone(\n+                    traverse_obj(info, ((None, ('formats', 0)), 'http_headers', 'Cookie'), casesense=False, get_all=False),\n+                    msg=failure_msg('Cookie header not removed in {0} result'.format(key)))\n+                self.assertSameCookieStrings(\n+                    traverse_obj(info, ((None, ('formats', 0)), 'cookies'), get_all=False), COOKIES,\n+                    msg=failure_msg('No cookies field found in {0} result'.format(key)))\n+\n+        test({'url': TEST_URL, 'http_headers': COOKIE_HEADER, 'id': '1', 'title': 'x'}, 'no formats field')\n+        test(make_info(info_header_cookies=True), 'info_dict header cokies')\n+        test(make_info(fmts_header_cookies=True), 'format header cookies')\n+        test(make_info(info_header_cookies=True, fmts_header_cookies=True), 'info_dict and format header cookies')\n+        test(make_info(info_header_cookies=True, fmts_header_cookies=True, cookies_field=True), 'all cookies fields')\n+        test(make_info(cookies_field=True), 'cookies format field')\n+        test({'url': TEST_URL, 'cookies': COOKIES, 'id': '1', 'title': 'x'}, 'info_dict cookies field only')\n+\n+        try_rm(TEST_FILE)\n+\n+    def test_add_headers_cookie(self):\n+        def check_for_cookie_header(result):\n+            return traverse_obj(result, ((None, ('formats', 0)), 'http_headers', 'Cookie'), casesense=False, get_all=False)\n+\n+        ydl = FakeYDL({'http_headers': {'Cookie': 'a=b'}})\n+        ydl._apply_header_cookies(_make_result([])['webpage_url'])  # Scope to input webpage URL: .example.com\n+\n+        fmt = {'url': 'https://example.com/video.mp4'}\n+        result = ydl.process_ie_result(_make_result([fmt]), download=False)\n+        self.assertIsNone(check_for_cookie_header(result), msg='http_headers cookies in result info_dict')\n+        self.assertEqual(result.get('cookies'), 'a=b; Domain=.example.com', msg='No cookies were set in cookies field')\n+        self.assertIn('a=b', ydl.cookiejar.get_cookie_header(fmt['url']), msg='No cookies were set in cookiejar')\n+\n+        fmt = {'url': 'https://wrong.com/video.mp4'}\n+        result = ydl.process_ie_result(_make_result([fmt]), download=False)\n+        self.assertIsNone(check_for_cookie_header(result), msg='http_headers cookies for wrong domain')\n+        self.assertFalse(result.get('cookies'), msg='Cookies set in cookies field for wrong domain')\n+        self.assertFalse(ydl.cookiejar.get_cookie_header(fmt['url']), msg='Cookies set in cookiejar for wrong domain')\n+\n \n if __name__ == '__main__':\n     unittest.main()\ndiff --git a/test/test_YoutubeDLCookieJar.py b/test/test_YoutubeDLCookieJar.py\nindex 05f48bd7417..4f9dd71ae6e 100644\n--- a/test/test_YoutubeDLCookieJar.py\n+++ b/test/test_YoutubeDLCookieJar.py\n@@ -46,6 +46,20 @@ def test_malformed_cookies(self):\n         # will be ignored\n         self.assertFalse(cookiejar._cookies)\n \n+    def test_get_cookie_header(self):\n+        cookiejar = YoutubeDLCookieJar('./test/testdata/cookies/httponly_cookies.txt')\n+        cookiejar.load(ignore_discard=True, ignore_expires=True)\n+        header = cookiejar.get_cookie_header('https://www.foobar.foobar')\n+        self.assertIn('HTTPONLY_COOKIE', header)\n+\n+    def test_get_cookies_for_url(self):\n+        cookiejar = YoutubeDLCookieJar('./test/testdata/cookies/session_cookies.txt')\n+        cookiejar.load(ignore_discard=True, ignore_expires=True)\n+        cookies = cookiejar.get_cookies_for_url('https://www.foobar.foobar/')\n+        self.assertEqual(len(cookies), 2)\n+        cookies = cookiejar.get_cookies_for_url('https://foobar.foobar/')\n+        self.assertFalse(cookies)\n+\n \n if __name__ == '__main__':\n     unittest.main()\ndiff --git a/test/test_aes.py b/test/test_aes.py\nindex cc89fb6ab27..0f181466bcf 100644\n--- a/test/test_aes.py\n+++ b/test/test_aes.py\n@@ -8,7 +8,7 @@\n import unittest\n sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n \n-from youtube_dl.aes import aes_decrypt, aes_encrypt, aes_cbc_decrypt, aes_cbc_encrypt, aes_decrypt_text\n+from youtube_dl.aes import aes_decrypt, aes_encrypt, aes_cbc_decrypt, aes_cbc_encrypt, aes_decrypt_text, aes_ecb_encrypt\n from youtube_dl.utils import bytes_to_intlist, intlist_to_bytes\n import base64\n \n@@ -58,6 +58,13 @@ def test_decrypt_text(self):\n         decrypted = (aes_decrypt_text(encrypted, password, 32))\n         self.assertEqual(decrypted, self.secret_msg)\n \n+    def test_ecb_encrypt(self):\n+        data = bytes_to_intlist(self.secret_msg)\n+        encrypted = intlist_to_bytes(aes_ecb_encrypt(data, self.key))\n+        self.assertEqual(\n+            encrypted,\n+            b'\\xaa\\x86]\\x81\\x97>\\x02\\x92\\x9d\\x1bR[[L/u\\xd3&\\xd1(h\\xde{\\x81\\x94\\xba\\x02\\xae\\xbd\\xa6\\xd0:')\n+\n \n if __name__ == '__main__':\n     unittest.main()\ndiff --git a/test/test_age_restriction.py b/test/test_age_restriction.py\nindex 6f5513faa2c..db98494ab85 100644\n--- a/test/test_age_restriction.py\n+++ b/test/test_age_restriction.py\n@@ -11,6 +11,7 @@\n \n \n from youtube_dl import YoutubeDL\n+from youtube_dl.utils import DownloadError\n \n \n def _download_restricted(url, filename, age):\n@@ -26,7 +27,10 @@ def _download_restricted(url, filename, age):\n     ydl.add_default_info_extractors()\n     json_filename = os.path.splitext(filename)[0] + '.info.json'\n     try_rm(json_filename)\n-    ydl.download([url])\n+    try:\n+        ydl.download([url])\n+    except DownloadError:\n+        try_rm(json_filename)\n     res = os.path.exists(json_filename)\n     try_rm(json_filename)\n     return res\n@@ -38,12 +42,12 @@ def _assert_restricted(self, url, filename, age, old_age=None):\n         self.assertFalse(_download_restricted(url, filename, age))\n \n     def test_youtube(self):\n-        self._assert_restricted('07FYdnEawAQ', '07FYdnEawAQ.mp4', 10)\n+        self._assert_restricted('HtVdAasjOgU', 'HtVdAasjOgU.mp4', 10)\n \n     def test_youporn(self):\n         self._assert_restricted(\n-            'http://www.youporn.com/watch/505835/sex-ed-is-it-safe-to-masturbate-daily/',\n-            '505835.mp4', 2, old_age=25)\n+            'https://www.youporn.com/watch/16715086/sex-ed-in-detention-18-asmr/',\n+            '16715086.mp4', 2, old_age=25)\n \n \n if __name__ == '__main__':\ndiff --git a/test/test_cache.py b/test/test_cache.py\nindex a161601420d..931074aa1de 100644\n--- a/test/test_cache.py\n+++ b/test/test_cache.py\n@@ -3,17 +3,18 @@\n \n from __future__ import unicode_literals\n \n-import shutil\n-\n # Allow direct execution\n import os\n import sys\n import unittest\n sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n \n+import shutil\n \n from test.helper import FakeYDL\n from youtube_dl.cache import Cache\n+from youtube_dl.utils import version_tuple\n+from youtube_dl.version import __version__\n \n \n def _is_empty(d):\n@@ -54,6 +55,17 @@ def test_cache(self):\n         self.assertFalse(os.path.exists(self.test_dir))\n         self.assertEqual(c.load('test_cache', 'k.'), None)\n \n+    def test_cache_validation(self):\n+        ydl = FakeYDL({\n+            'cachedir': self.test_dir,\n+        })\n+        c = Cache(ydl)\n+        obj = {'x': 1, 'y': ['\u00e4', '\\\\a', True]}\n+        c.store('test_cache', 'k.', obj)\n+        self.assertEqual(c.load('test_cache', 'k.', min_ver='1970.01.01'), obj)\n+        new_version = '.'.join(('%d' % ((v + 1) if i == 0 else v, )) for i, v in enumerate(version_tuple(__version__)))\n+        self.assertIs(c.load('test_cache', 'k.', min_ver=new_version), None)\n+\n \n if __name__ == '__main__':\n     unittest.main()\ndiff --git a/test/test_compat.py b/test/test_compat.py\nindex 86ff389fdfc..b83c8cb4100 100644\n--- a/test/test_compat.py\n+++ b/test/test_compat.py\n@@ -11,6 +11,7 @@\n \n \n from youtube_dl.compat import (\n+    compat_casefold,\n     compat_getenv,\n     compat_setenv,\n     compat_etree_Element,\n@@ -22,6 +23,7 @@\n     compat_urllib_parse_unquote,\n     compat_urllib_parse_unquote_plus,\n     compat_urllib_parse_urlencode,\n+    compat_urllib_request,\n )\n \n \n@@ -47,10 +49,11 @@ def test_compat_expanduser(self):\n \n     def test_all_present(self):\n         import youtube_dl.compat\n-        all_names = youtube_dl.compat.__all__\n-        present_names = set(filter(\n+        all_names = sorted(\n+            youtube_dl.compat.__all__ + youtube_dl.compat.legacy)\n+        present_names = set(map(compat_str, filter(\n             lambda c: '_' in c and not c.startswith('_'),\n-            dir(youtube_dl.compat))) - set(['unicode_literals'])\n+            dir(youtube_dl.compat)))) - set(['unicode_literals'])\n         self.assertEqual(all_names, sorted(present_names))\n \n     def test_compat_urllib_parse_unquote(self):\n@@ -118,9 +121,34 @@ def test_compat_etree_fromstring_doctype(self):\n <smil xmlns=\"http://www.w3.org/2001/SMIL20/Language\"></smil>'''\n         compat_etree_fromstring(xml)\n \n-    def test_struct_unpack(self):\n+    def test_compat_struct_unpack(self):\n         self.assertEqual(compat_struct_unpack('!B', b'\\x00'), (0,))\n \n+    def test_compat_casefold(self):\n+        if hasattr(compat_str, 'casefold'):\n+            # don't bother to test str.casefold() (again)\n+            return\n+        # thanks https://bugs.python.org/file24232/casefolding.patch\n+        self.assertEqual(compat_casefold('hello'), 'hello')\n+        self.assertEqual(compat_casefold('hELlo'), 'hello')\n+        self.assertEqual(compat_casefold('\u00df'), 'ss')\n+        self.assertEqual(compat_casefold('\ufb01'), 'fi')\n+        self.assertEqual(compat_casefold('\\u03a3'), '\\u03c3')\n+        self.assertEqual(compat_casefold('A\\u0345\\u03a3'), 'a\\u03b9\\u03c3')\n+\n+    def test_compat_urllib_request_Request(self):\n+        self.assertEqual(\n+            compat_urllib_request.Request('http://127.0.0.1', method='PUT').get_method(),\n+            'PUT')\n+\n+        class PUTrequest(compat_urllib_request.Request):\n+            def get_method(self):\n+                return 'PUT'\n+\n+        self.assertEqual(\n+            PUTrequest('http://127.0.0.1').get_method(),\n+            'PUT')\n+\n \n if __name__ == '__main__':\n     unittest.main()\ndiff --git a/test/test_download.py b/test/test_download.py\nindex 8e43cfa1270..f7d6a23bc95 100644\n--- a/test/test_download.py\n+++ b/test/test_download.py\n@@ -9,7 +9,6 @@\n sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n \n from test.helper import (\n-    assertGreaterEqual,\n     expect_warnings,\n     get_params,\n     gettestcases,\n@@ -20,26 +19,35 @@\n \n \n import hashlib\n-import io\n import json\n import socket\n \n import youtube_dl.YoutubeDL\n from youtube_dl.compat import (\n     compat_http_client,\n-    compat_urllib_error,\n     compat_HTTPError,\n+    compat_open as open,\n+    compat_urllib_error,\n )\n from youtube_dl.utils import (\n     DownloadError,\n     ExtractorError,\n+    error_to_compat_str,\n     format_bytes,\n+    IDENTITY,\n+    preferredencoding,\n     UnavailableVideoError,\n )\n from youtube_dl.extractor import get_info_extractor\n \n RETRIES = 3\n \n+# Some unittest APIs require actual str\n+if not isinstance('TEST', str):\n+    _encode_str = lambda s: s.encode(preferredencoding())\n+else:\n+    _encode_str = IDENTITY\n+\n \n class YoutubeDL(youtube_dl.YoutubeDL):\n     def __init__(self, *args, **kwargs):\n@@ -100,28 +108,31 @@ def test_template(self):\n \n         def print_skipping(reason):\n             print('Skipping %s: %s' % (test_case['name'], reason))\n+            self.skipTest(_encode_str(reason))\n+\n         if not ie.working():\n             print_skipping('IE marked as not _WORKING')\n-            return\n \n         for tc in test_cases:\n             info_dict = tc.get('info_dict', {})\n             if not (info_dict.get('id') and info_dict.get('ext')):\n-                raise Exception('Test definition incorrect. The output file cannot be known. Are both \\'id\\' and \\'ext\\' keys present?')\n+                raise Exception('Test definition (%s) requires both \\'id\\' and \\'ext\\' keys present to define the output file' % (tname, ))\n \n         if 'skip' in test_case:\n             print_skipping(test_case['skip'])\n-            return\n+\n         for other_ie in other_ies:\n             if not other_ie.working():\n                 print_skipping('test depends on %sIE, marked as not WORKING' % other_ie.ie_key())\n-                return\n \n         params = get_params(test_case.get('params', {}))\n         params['outtmpl'] = tname + '_' + params['outtmpl']\n         if is_playlist and 'playlist' not in test_case:\n             params.setdefault('extract_flat', 'in_playlist')\n-            params.setdefault('playlistend', test_case.get('playlist_mincount'))\n+            params.setdefault('playlistend',\n+                              test_case['playlist_maxcount'] + 1\n+                              if test_case.get('playlist_maxcount')\n+                              else test_case.get('playlist_mincount'))\n             params.setdefault('skip_download', True)\n \n         ydl = YoutubeDL(params, auto_init=False)\n@@ -147,6 +158,7 @@ def try_rm_tcs_files(tcs=None):\n                 try_rm(tc_filename)\n                 try_rm(tc_filename + '.part')\n                 try_rm(os.path.splitext(tc_filename)[0] + '.info.json')\n+\n         try_rm_tcs_files()\n         try:\n             try_num = 1\n@@ -161,7 +173,9 @@ def try_rm_tcs_files(tcs=None):\n                 except (DownloadError, ExtractorError) as err:\n                     # Check if the exception is not a network related one\n                     if not err.exc_info[0] in (compat_urllib_error.URLError, socket.timeout, UnavailableVideoError, compat_http_client.BadStatusLine) or (err.exc_info[0] == compat_HTTPError and err.exc_info[1].code == 503):\n-                        raise\n+                        msg = getattr(err, 'msg', error_to_compat_str(err))\n+                        err.msg = '%s (%s)' % (msg, tname, )\n+                        raise err\n \n                     if try_num == RETRIES:\n                         report_warning('%s failed due to network errors, skipping...' % tname)\n@@ -179,13 +193,19 @@ def try_rm_tcs_files(tcs=None):\n                 expect_info_dict(self, res_dict, test_case.get('info_dict', {}))\n \n             if 'playlist_mincount' in test_case:\n-                assertGreaterEqual(\n-                    self,\n+                self.assertGreaterEqual(\n                     len(res_dict['entries']),\n                     test_case['playlist_mincount'],\n                     'Expected at least %d in playlist %s, but got only %d' % (\n                         test_case['playlist_mincount'], test_case['url'],\n                         len(res_dict['entries'])))\n+            if 'playlist_maxcount' in test_case:\n+                self.assertLessEqual(\n+                    len(res_dict['entries']),\n+                    test_case['playlist_maxcount'],\n+                    'Expected at most %d in playlist %s, but got %d' % (\n+                        test_case['playlist_maxcount'], test_case['url'],\n+                        len(res_dict['entries'])))\n             if 'playlist_count' in test_case:\n                 self.assertEqual(\n                     len(res_dict['entries']),\n@@ -210,7 +230,15 @@ def try_rm_tcs_files(tcs=None):\n                 # First, check test cases' data against extracted data alone\n                 expect_info_dict(self, tc_res_dict, tc.get('info_dict', {}))\n                 # Now, check downloaded file consistency\n+                # support test-case with volatile ID, signalled by regexp value\n+                if tc.get('info_dict', {}).get('id', '').startswith('re:'):\n+                    test_id = tc['info_dict']['id']\n+                    tc['info_dict']['id'] = tc_res_dict['id']\n+                else:\n+                    test_id = None\n                 tc_filename = get_tc_filename(tc)\n+                if test_id:\n+                    tc['info_dict']['id'] = test_id\n                 if not test_case.get('params', {}).get('skip_download', False):\n                     self.assertTrue(os.path.exists(tc_filename), msg='Missing file ' + tc_filename)\n                     self.assertTrue(tc_filename in finished_hook_called)\n@@ -219,8 +247,8 @@ def try_rm_tcs_files(tcs=None):\n                         if params.get('test'):\n                             expected_minsize = max(expected_minsize, 10000)\n                         got_fsize = os.path.getsize(tc_filename)\n-                        assertGreaterEqual(\n-                            self, got_fsize, expected_minsize,\n+                        self.assertGreaterEqual(\n+                            got_fsize, expected_minsize,\n                             'Expected %s to be at least %s, but it\\'s only %s ' %\n                             (tc_filename, format_bytes(expected_minsize),\n                                 format_bytes(got_fsize)))\n@@ -233,7 +261,7 @@ def try_rm_tcs_files(tcs=None):\n                 self.assertTrue(\n                     os.path.exists(info_json_fn),\n                     'Missing info file %s' % info_json_fn)\n-                with io.open(info_json_fn, encoding='utf-8') as infof:\n+                with open(info_json_fn, encoding='utf-8') as infof:\n                     info_dict = json.load(infof)\n                 expect_info_dict(self, info_dict, tc.get('info_dict', {}))\n         finally:\ndiff --git a/test/test_downloader_external.py b/test/test_downloader_external.py\nnew file mode 100644\nindex 00000000000..4491bd9dee6\n--- /dev/null\n+++ b/test/test_downloader_external.py\n@@ -0,0 +1,272 @@\n+#!/usr/bin/env python\n+# coding: utf-8\n+from __future__ import unicode_literals\n+\n+# Allow direct execution\n+import os\n+import re\n+import sys\n+import subprocess\n+import unittest\n+sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n+\n+from test.helper import (\n+    FakeLogger,\n+    FakeYDL,\n+    http_server_port,\n+    try_rm,\n+)\n+from youtube_dl import YoutubeDL\n+from youtube_dl.compat import (\n+    compat_contextlib_suppress,\n+    compat_http_cookiejar_Cookie,\n+    compat_http_server,\n+    compat_kwargs,\n+)\n+from youtube_dl.utils import (\n+    encodeFilename,\n+    join_nonempty,\n+)\n+from youtube_dl.downloader.external import (\n+    Aria2cFD,\n+    Aria2pFD,\n+    AxelFD,\n+    CurlFD,\n+    FFmpegFD,\n+    HttpieFD,\n+    WgetFD,\n+)\n+from youtube_dl.postprocessor import (\n+    FFmpegPostProcessor,\n+)\n+import threading\n+\n+TEST_SIZE = 10 * 1024\n+\n+TEST_COOKIE = {\n+    'version': 0,\n+    'name': 'test',\n+    'value': 'ytdlp',\n+    'port': None,\n+    'port_specified': False,\n+    'domain': '.example.com',\n+    'domain_specified': True,\n+    'domain_initial_dot': False,\n+    'path': '/',\n+    'path_specified': True,\n+    'secure': False,\n+    'expires': None,\n+    'discard': False,\n+    'comment': None,\n+    'comment_url': None,\n+    'rest': {},\n+}\n+\n+TEST_COOKIE_VALUE = join_nonempty('name', 'value', delim='=', from_dict=TEST_COOKIE)\n+\n+TEST_INFO = {'url': 'http://www.example.com/'}\n+\n+\n+def cookiejar_Cookie(**cookie_args):\n+    return compat_http_cookiejar_Cookie(**compat_kwargs(cookie_args))\n+\n+\n+def ifExternalFDAvailable(externalFD):\n+    return unittest.skipUnless(externalFD.available(),\n+                               externalFD.get_basename() + ' not found')\n+\n+\n+class HTTPTestRequestHandler(compat_http_server.BaseHTTPRequestHandler):\n+    def log_message(self, format, *args):\n+        pass\n+\n+    def send_content_range(self, total=None):\n+        range_header = self.headers.get('Range')\n+        start = end = None\n+        if range_header:\n+            mobj = re.match(r'bytes=(\\d+)-(\\d+)', range_header)\n+            if mobj:\n+                start, end = (int(mobj.group(i)) for i in (1, 2))\n+        valid_range = start is not None and end is not None\n+        if valid_range:\n+            content_range = 'bytes %d-%d' % (start, end)\n+            if total:\n+                content_range += '/%d' % total\n+            self.send_header('Content-Range', content_range)\n+        return (end - start + 1) if valid_range else total\n+\n+    def serve(self, range=True, content_length=True):\n+        self.send_response(200)\n+        self.send_header('Content-Type', 'video/mp4')\n+        size = TEST_SIZE\n+        if range:\n+            size = self.send_content_range(TEST_SIZE)\n+        if content_length:\n+            self.send_header('Content-Length', size)\n+        self.end_headers()\n+        self.wfile.write(b'#' * size)\n+\n+    def do_GET(self):\n+        if self.path == '/regular':\n+            self.serve()\n+        elif self.path == '/no-content-length':\n+            self.serve(content_length=False)\n+        elif self.path == '/no-range':\n+            self.serve(range=False)\n+        elif self.path == '/no-range-no-content-length':\n+            self.serve(range=False, content_length=False)\n+        else:\n+            assert False, 'unrecognised server path'\n+\n+\n+@ifExternalFDAvailable(Aria2pFD)\n+class TestAria2pFD(unittest.TestCase):\n+    def setUp(self):\n+        self.httpd = compat_http_server.HTTPServer(\n+            ('127.0.0.1', 0), HTTPTestRequestHandler)\n+        self.port = http_server_port(self.httpd)\n+        self.server_thread = threading.Thread(target=self.httpd.serve_forever)\n+        self.server_thread.daemon = True\n+        self.server_thread.start()\n+\n+    def download(self, params, ep):\n+        with subprocess.Popen(\n+            ['aria2c', '--enable-rpc'],\n+            stdout=subprocess.DEVNULL,\n+            stderr=subprocess.DEVNULL\n+        ) as process:\n+            if not process.poll():\n+                filename = 'testfile.mp4'\n+                params['logger'] = FakeLogger()\n+                params['outtmpl'] = filename\n+                ydl = YoutubeDL(params)\n+                try_rm(encodeFilename(filename))\n+                self.assertEqual(ydl.download(['http://127.0.0.1:%d/%s' % (self.port, ep)]), 0)\n+                self.assertEqual(os.path.getsize(encodeFilename(filename)), TEST_SIZE)\n+                try_rm(encodeFilename(filename))\n+            process.kill()\n+\n+    def download_all(self, params):\n+        for ep in ('regular', 'no-content-length', 'no-range', 'no-range-no-content-length'):\n+            self.download(params, ep)\n+\n+    def test_regular(self):\n+        self.download_all({'external_downloader': 'aria2p'})\n+\n+    def test_chunked(self):\n+        self.download_all({\n+            'external_downloader': 'aria2p',\n+            'http_chunk_size': 1000,\n+        })\n+\n+\n+@ifExternalFDAvailable(HttpieFD)\n+class TestHttpieFD(unittest.TestCase):\n+    def test_make_cmd(self):\n+        with FakeYDL() as ydl:\n+            downloader = HttpieFD(ydl, {})\n+            self.assertEqual(\n+                downloader._make_cmd('test', TEST_INFO),\n+                ['http', '--download', '--output', 'test', 'http://www.example.com/'])\n+\n+            # Test cookie header is added\n+            ydl.cookiejar.set_cookie(cookiejar_Cookie(**TEST_COOKIE))\n+            self.assertEqual(\n+                downloader._make_cmd('test', TEST_INFO),\n+                ['http', '--download', '--output', 'test',\n+                 'http://www.example.com/', 'Cookie:' + TEST_COOKIE_VALUE])\n+\n+\n+@ifExternalFDAvailable(AxelFD)\n+class TestAxelFD(unittest.TestCase):\n+    def test_make_cmd(self):\n+        with FakeYDL() as ydl:\n+            downloader = AxelFD(ydl, {})\n+            self.assertEqual(\n+                downloader._make_cmd('test', TEST_INFO),\n+                ['axel', '-o', 'test', '--', 'http://www.example.com/'])\n+\n+            # Test cookie header is added\n+            ydl.cookiejar.set_cookie(cookiejar_Cookie(**TEST_COOKIE))\n+            self.assertEqual(\n+                downloader._make_cmd('test', TEST_INFO),\n+                ['axel', '-o', 'test', '-H', 'Cookie: ' + TEST_COOKIE_VALUE,\n+                 '--max-redirect=0', '--', 'http://www.example.com/'])\n+\n+\n+@ifExternalFDAvailable(WgetFD)\n+class TestWgetFD(unittest.TestCase):\n+    def test_make_cmd(self):\n+        with FakeYDL() as ydl:\n+            downloader = WgetFD(ydl, {})\n+            self.assertNotIn('--load-cookies', downloader._make_cmd('test', TEST_INFO))\n+            # Test cookiejar tempfile arg is added\n+            ydl.cookiejar.set_cookie(cookiejar_Cookie(**TEST_COOKIE))\n+            self.assertIn('--load-cookies', downloader._make_cmd('test', TEST_INFO))\n+\n+\n+@ifExternalFDAvailable(CurlFD)\n+class TestCurlFD(unittest.TestCase):\n+    def test_make_cmd(self):\n+        with FakeYDL() as ydl:\n+            downloader = CurlFD(ydl, {})\n+            self.assertNotIn('--cookie', downloader._make_cmd('test', TEST_INFO))\n+            # Test cookie header is added\n+            ydl.cookiejar.set_cookie(cookiejar_Cookie(**TEST_COOKIE))\n+            self.assertIn('--cookie', downloader._make_cmd('test', TEST_INFO))\n+            self.assertIn(TEST_COOKIE_VALUE, downloader._make_cmd('test', TEST_INFO))\n+\n+\n+@ifExternalFDAvailable(Aria2cFD)\n+class TestAria2cFD(unittest.TestCase):\n+    def test_make_cmd(self):\n+        with FakeYDL() as ydl:\n+            downloader = Aria2cFD(ydl, {})\n+            downloader._make_cmd('test', TEST_INFO)\n+            self.assertFalse(hasattr(downloader, '_cookies_tempfile'))\n+\n+            # Test cookiejar tempfile arg is added\n+            ydl.cookiejar.set_cookie(cookiejar_Cookie(**TEST_COOKIE))\n+            cmd = downloader._make_cmd('test', TEST_INFO)\n+            self.assertIn('--load-cookies=%s' % downloader._cookies_tempfile, cmd)\n+\n+\n+# Handle delegated availability\n+def ifFFmpegFDAvailable(externalFD):\n+    # raise SkipTest, or set False!\n+    avail = ifExternalFDAvailable(externalFD) and False\n+    with compat_contextlib_suppress(Exception):\n+        avail = FFmpegPostProcessor(downloader=None).available\n+    return unittest.skipUnless(\n+        avail, externalFD.get_basename() + ' not found')\n+\n+\n+@ifFFmpegFDAvailable(FFmpegFD)\n+class TestFFmpegFD(unittest.TestCase):\n+    _args = []\n+\n+    def _test_cmd(self, args):\n+        self._args = args\n+\n+    def test_make_cmd(self):\n+        with FakeYDL() as ydl:\n+            downloader = FFmpegFD(ydl, {})\n+            downloader._debug_cmd = self._test_cmd\n+            info_dict = TEST_INFO.copy()\n+            info_dict['ext'] = 'mp4'\n+\n+            downloader._call_downloader('test', info_dict)\n+            self.assertEqual(self._args, [\n+                'ffmpeg', '-y', '-i', 'http://www.example.com/',\n+                '-c', 'copy', '-f', 'mp4', 'file:test'])\n+\n+            # Test cookies arg is added\n+            ydl.cookiejar.set_cookie(cookiejar_Cookie(**TEST_COOKIE))\n+            downloader._call_downloader('test', info_dict)\n+            self.assertEqual(self._args, [\n+                'ffmpeg', '-y', '-cookies', TEST_COOKIE_VALUE + '; path=/; domain=.example.com;\\r\\n',\n+                '-i', 'http://www.example.com/', '-c', 'copy', '-f', 'mp4', 'file:test'])\n+\n+\n+if __name__ == '__main__':\n+    unittest.main()\ndiff --git a/test/test_downloader_http.py b/test/test_downloader_http.py\nindex 7504722810b..6af86ae4862 100644\n--- a/test/test_downloader_http.py\n+++ b/test/test_downloader_http.py\n@@ -9,7 +9,11 @@\n import unittest\n sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n \n-from test.helper import http_server_port, try_rm\n+from test.helper import (\n+    FakeLogger,\n+    http_server_port,\n+    try_rm,\n+)\n from youtube_dl import YoutubeDL\n from youtube_dl.compat import compat_http_server\n from youtube_dl.downloader.http import HttpFD\n@@ -66,17 +70,6 @@ def do_GET(self):\n             assert False\n \n \n-class FakeLogger(object):\n-    def debug(self, msg):\n-        pass\n-\n-    def warning(self, msg):\n-        pass\n-\n-    def error(self, msg):\n-        pass\n-\n-\n class TestHttpFD(unittest.TestCase):\n     def setUp(self):\n         self.httpd = compat_http_server.HTTPServer(\n@@ -95,7 +88,7 @@ def download(self, params, ep):\n         self.assertTrue(downloader.real_download(filename, {\n             'url': 'http://127.0.0.1:%d/%s' % (self.port, ep),\n         }))\n-        self.assertEqual(os.path.getsize(encodeFilename(filename)), TEST_SIZE)\n+        self.assertEqual(os.path.getsize(encodeFilename(filename)), TEST_SIZE, ep)\n         try_rm(encodeFilename(filename))\n \n     def download_all(self, params):\ndiff --git a/test/test_execution.py b/test/test_execution.py\nindex 32948d93e25..9daaafa6ccb 100644\n--- a/test/test_execution.py\n+++ b/test/test_execution.py\n@@ -8,46 +8,54 @@\n import sys\n import os\n import subprocess\n-sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n \n+rootDir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n+\n+sys.path.insert(0, rootDir)\n+\n+from youtube_dl.compat import compat_register_utf8, compat_subprocess_get_DEVNULL\n from youtube_dl.utils import encodeArgument\n \n-rootDir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n+compat_register_utf8()\n \n \n-try:\n-    _DEV_NULL = subprocess.DEVNULL\n-except AttributeError:\n-    _DEV_NULL = open(os.devnull, 'wb')\n+_DEV_NULL = compat_subprocess_get_DEVNULL()\n \n \n class TestExecution(unittest.TestCase):\n+    def setUp(self):\n+        self.module = 'youtube_dl'\n+        if sys.version_info < (2, 7):\n+            self.module += '.__main__'\n+\n     def test_import(self):\n         subprocess.check_call([sys.executable, '-c', 'import youtube_dl'], cwd=rootDir)\n \n     def test_module_exec(self):\n-        if sys.version_info >= (2, 7):  # Python 2.6 doesn't support package execution\n-            subprocess.check_call([sys.executable, '-m', 'youtube_dl', '--version'], cwd=rootDir, stdout=_DEV_NULL)\n+        subprocess.check_call([sys.executable, '-m', self.module, '--version'], cwd=rootDir, stdout=_DEV_NULL)\n \n     def test_main_exec(self):\n-        subprocess.check_call([sys.executable, 'youtube_dl/__main__.py', '--version'], cwd=rootDir, stdout=_DEV_NULL)\n+        subprocess.check_call([sys.executable, os.path.normpath('youtube_dl/__main__.py'), '--version'], cwd=rootDir, stdout=_DEV_NULL)\n \n     def test_cmdline_umlauts(self):\n+        os.environ['PYTHONIOENCODING'] = 'utf-8'\n         p = subprocess.Popen(\n-            [sys.executable, 'youtube_dl/__main__.py', encodeArgument('\u00e4'), '--version'],\n+            [sys.executable, '-m', self.module, encodeArgument('\u00e4'), '--version'],\n             cwd=rootDir, stdout=_DEV_NULL, stderr=subprocess.PIPE)\n         _, stderr = p.communicate()\n         self.assertFalse(stderr)\n \n     def test_lazy_extractors(self):\n+        lazy_extractors = os.path.normpath('youtube_dl/extractor/lazy_extractors.py')\n         try:\n-            subprocess.check_call([sys.executable, 'devscripts/make_lazy_extractors.py', 'youtube_dl/extractor/lazy_extractors.py'], cwd=rootDir, stdout=_DEV_NULL)\n-            subprocess.check_call([sys.executable, 'test/test_all_urls.py'], cwd=rootDir, stdout=_DEV_NULL)\n+            subprocess.check_call([sys.executable, os.path.normpath('devscripts/make_lazy_extractors.py'), lazy_extractors], cwd=rootDir, stdout=_DEV_NULL)\n+            subprocess.check_call([sys.executable, os.path.normpath('test/test_all_urls.py')], cwd=rootDir, stdout=_DEV_NULL)\n         finally:\n-            try:\n-                os.remove('youtube_dl/extractor/lazy_extractors.py')\n-            except (IOError, OSError):\n-                pass\n+            for x in ('', 'c') if sys.version_info[0] < 3 else ('',):\n+                try:\n+                    os.remove(lazy_extractors + x)\n+                except OSError:\n+                    pass\n \n \n if __name__ == '__main__':\ndiff --git a/test/test_http.py b/test/test_http.py\nindex 3ee0a5dda8d..485c4c6fcc6 100644\n--- a/test/test_http.py\n+++ b/test/test_http.py\n@@ -8,30 +8,163 @@\n import unittest\n sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n \n-from test.helper import http_server_port\n-from youtube_dl import YoutubeDL\n-from youtube_dl.compat import compat_http_server, compat_urllib_request\n+import contextlib\n+import gzip\n+import io\n import ssl\n+import tempfile\n import threading\n+import zlib\n+\n+# avoid deprecated alias assertRaisesRegexp\n+if hasattr(unittest.TestCase, 'assertRaisesRegex'):\n+    unittest.TestCase.assertRaisesRegexp = unittest.TestCase.assertRaisesRegex\n+\n+try:\n+    import brotli\n+except ImportError:\n+    brotli = None\n+try:\n+    from urllib.request import pathname2url\n+except ImportError:\n+    from urllib import pathname2url\n+\n+from youtube_dl.compat import (\n+    compat_http_cookiejar_Cookie,\n+    compat_http_server,\n+    compat_str as str,\n+    compat_urllib_error,\n+    compat_urllib_HTTPError,\n+    compat_urllib_parse,\n+    compat_urllib_request,\n+)\n+\n+from youtube_dl.utils import (\n+    sanitized_Request,\n+    update_Request,\n+    urlencode_postdata,\n+)\n+\n+from test.helper import (\n+    expectedFailureIf,\n+    FakeYDL,\n+    FakeLogger,\n+    http_server_port,\n+)\n+from youtube_dl import YoutubeDL\n \n TEST_DIR = os.path.dirname(os.path.abspath(__file__))\n \n \n class HTTPTestRequestHandler(compat_http_server.BaseHTTPRequestHandler):\n+    protocol_version = 'HTTP/1.1'\n+\n+    # work-around old/new -style class inheritance\n+    def super(self, meth_name, *args, **kwargs):\n+        from types import MethodType\n+        try:\n+            super()\n+            fn = lambda s, m, *a, **k: getattr(super(), m)(*a, **k)\n+        except TypeError:\n+            fn = lambda s, m, *a, **k: getattr(compat_http_server.BaseHTTPRequestHandler, m)(s, *a, **k)\n+        self.super = MethodType(fn, self)\n+        return self.super(meth_name, *args, **kwargs)\n+\n     def log_message(self, format, *args):\n         pass\n \n+    def _headers(self):\n+        payload = str(self.headers).encode('utf-8')\n+        self.send_response(200)\n+        self.send_header('Content-Type', 'application/json')\n+        self.send_header('Content-Length', str(len(payload)))\n+        self.end_headers()\n+        self.wfile.write(payload)\n+\n+    def _redirect(self):\n+        self.send_response(int(self.path[len('/redirect_'):]))\n+        self.send_header('Location', '/method')\n+        self.send_header('Content-Length', '0')\n+        self.end_headers()\n+\n+    def _method(self, method, payload=None):\n+        self.send_response(200)\n+        self.send_header('Content-Length', str(len(payload or '')))\n+        self.send_header('Method', method)\n+        self.end_headers()\n+        if payload:\n+            self.wfile.write(payload)\n+\n+    def _status(self, status):\n+        payload = '<html>{0} NOT FOUND</html>'.format(status).encode('utf-8')\n+        self.send_response(int(status))\n+        self.send_header('Content-Type', 'text/html; charset=utf-8')\n+        self.send_header('Content-Length', str(len(payload)))\n+        self.end_headers()\n+        self.wfile.write(payload)\n+\n+    def _read_data(self):\n+        if 'Content-Length' in self.headers:\n+            return self.rfile.read(int(self.headers['Content-Length']))\n+\n+    def _test_url(self, path, host='127.0.0.1', scheme='http', port=None):\n+        return '{0}://{1}:{2}/{3}'.format(\n+            scheme, host,\n+            port if port is not None\n+            else http_server_port(self.server), path)\n+\n+    def do_POST(self):\n+        data = self._read_data()\n+        if self.path.startswith('/redirect_'):\n+            self._redirect()\n+        elif self.path.startswith('/method'):\n+            self._method('POST', data)\n+        elif self.path.startswith('/headers'):\n+            self._headers()\n+        else:\n+            self._status(404)\n+\n+    def do_HEAD(self):\n+        if self.path.startswith('/redirect_'):\n+            self._redirect()\n+        elif self.path.startswith('/method'):\n+            self._method('HEAD')\n+        else:\n+            self._status(404)\n+\n+    def do_PUT(self):\n+        data = self._read_data()\n+        if self.path.startswith('/redirect_'):\n+            self._redirect()\n+        elif self.path.startswith('/method'):\n+            self._method('PUT', data)\n+        else:\n+            self._status(404)\n+\n     def do_GET(self):\n-        if self.path == '/video.html':\n-            self.send_response(200)\n-            self.send_header('Content-Type', 'text/html; charset=utf-8')\n+\n+        def respond(payload=b'<html><video src=\"/vid.mp4\" /></html>',\n+                    payload_type='text/html; charset=utf-8',\n+                    payload_encoding=None,\n+                    resp_code=200):\n+            self.send_response(resp_code)\n+            self.send_header('Content-Type', payload_type)\n+            if payload_encoding:\n+                self.send_header('Content-Encoding', payload_encoding)\n+            self.send_header('Content-Length', str(len(payload)))  # required for persistent connections\n             self.end_headers()\n-            self.wfile.write(b'<html><video src=\"/vid.mp4\" /></html>')\n+            self.wfile.write(payload)\n+\n+        def gzip_compress(p):\n+            buf = io.BytesIO()\n+            with contextlib.closing(gzip.GzipFile(fileobj=buf, mode='wb')) as f:\n+                f.write(p)\n+            return buf.getvalue()\n+\n+        if self.path == '/video.html':\n+            respond()\n         elif self.path == '/vid.mp4':\n-            self.send_response(200)\n-            self.send_header('Content-Type', 'video/mp4')\n-            self.end_headers()\n-            self.wfile.write(b'\\x00\\x00\\x00\\x00\\x20\\x66\\x74[video]')\n+            respond(b'\\x00\\x00\\x00\\x00\\x20\\x66\\x74[video]', 'video/mp4')\n         elif self.path == '/302':\n             if sys.version_info[0] == 3:\n                 # XXX: Python 3 http server does not allow non-ASCII header values\n@@ -39,71 +172,336 @@ def do_GET(self):\n                 self.end_headers()\n                 return\n \n-            new_url = 'http://127.0.0.1:%d/\u4e2d\u6587.html' % http_server_port(self.server)\n+            new_url = self._test_url('\u4e2d\u6587.html')\n             self.send_response(302)\n             self.send_header(b'Location', new_url.encode('utf-8'))\n             self.end_headers()\n         elif self.path == '/%E4%B8%AD%E6%96%87.html':\n-            self.send_response(200)\n-            self.send_header('Content-Type', 'text/html; charset=utf-8')\n+            respond()\n+        elif self.path == '/%c7%9f':\n+            respond()\n+        elif self.path == '/redirect_dotsegments':\n+            self.send_response(301)\n+            # redirect to /headers but with dot segments before\n+            self.send_header('Location', '/a/b/./../../headers')\n+            self.send_header('Content-Length', '0')\n             self.end_headers()\n-            self.wfile.write(b'<html><video src=\"/vid.mp4\" /></html>')\n+        elif self.path.startswith('/redirect_'):\n+            self._redirect()\n+        elif self.path.startswith('/method'):\n+            self._method('GET')\n+        elif self.path.startswith('/headers'):\n+            self._headers()\n+        elif self.path.startswith('/308-to-headers'):\n+            self.send_response(308)\n+            self.send_header('Location', '/headers')\n+            self.send_header('Content-Length', '0')\n+            self.end_headers()\n+        elif self.path == '/trailing_garbage':\n+            payload = b'<html><video src=\"/vid.mp4\" /></html>'\n+            compressed = gzip_compress(payload) + b'trailing garbage'\n+            respond(compressed, payload_encoding='gzip')\n+        elif self.path == '/302-non-ascii-redirect':\n+            new_url = self._test_url('\u4e2d\u6587.html')\n+            # actually respond with permanent redirect\n+            self.send_response(301)\n+            self.send_header('Location', new_url)\n+            self.send_header('Content-Length', '0')\n+            self.end_headers()\n+        elif self.path == '/content-encoding':\n+            encodings = self.headers.get('ytdl-encoding', '')\n+            payload = b'<html><video src=\"/vid.mp4\" /></html>'\n+            for encoding in filter(None, (e.strip() for e in encodings.split(','))):\n+                if encoding == 'br' and brotli:\n+                    payload = brotli.compress(payload)\n+                elif encoding == 'gzip':\n+                    payload = gzip_compress(payload)\n+                elif encoding == 'deflate':\n+                    payload = zlib.compress(payload)\n+                elif encoding == 'unsupported':\n+                    payload = b'raw'\n+                    break\n+                else:\n+                    self._status(415)\n+                    return\n+            respond(payload, payload_encoding=encodings)\n+\n         else:\n-            assert False\n+            self._status(404)\n \n+    def send_header(self, keyword, value):\n+        \"\"\"\n+        Forcibly allow HTTP server to send non percent-encoded non-ASCII characters in headers.\n+        This is against what is defined in RFC 3986: but we need to test that we support this\n+        since some sites incorrectly do this.\n+        \"\"\"\n+        if keyword.lower() == 'connection':\n+            return self.super('send_header', keyword, value)\n \n-class FakeLogger(object):\n-    def debug(self, msg):\n-        pass\n+        if not hasattr(self, '_headers_buffer'):\n+            self._headers_buffer = []\n \n-    def warning(self, msg):\n-        pass\n+        self._headers_buffer.append('{0}: {1}\\r\\n'.format(keyword, value).encode('utf-8'))\n \n-    def error(self, msg):\n-        pass\n+    def end_headers(self):\n+        if hasattr(self, '_headers_buffer'):\n+            self.wfile.write(b''.join(self._headers_buffer))\n+            self._headers_buffer = []\n+        self.super('end_headers')\n \n \n class TestHTTP(unittest.TestCase):\n+    # when does it make sense to check the SSL certificate?\n+    _check_cert = (\n+        sys.version_info >= (3, 2)\n+        or (sys.version_info[0] == 2 and sys.version_info[1:] >= (7, 19)))\n+\n     def setUp(self):\n-        self.httpd = compat_http_server.HTTPServer(\n+        # HTTP server\n+        self.http_httpd = compat_http_server.HTTPServer(\n             ('127.0.0.1', 0), HTTPTestRequestHandler)\n-        self.port = http_server_port(self.httpd)\n-        self.server_thread = threading.Thread(target=self.httpd.serve_forever)\n-        self.server_thread.daemon = True\n-        self.server_thread.start()\n+        self.http_port = http_server_port(self.http_httpd)\n \n-    def test_unicode_path_redirection(self):\n-        # XXX: Python 3 http server does not allow non-ASCII header values\n-        if sys.version_info[0] == 3:\n-            return\n+        self.http_server_thread = threading.Thread(target=self.http_httpd.serve_forever)\n+        self.http_server_thread.daemon = True\n+        self.http_server_thread.start()\n \n-        ydl = YoutubeDL({'logger': FakeLogger()})\n-        r = ydl.extract_info('http://127.0.0.1:%d/302' % self.port)\n-        self.assertEqual(r['entries'][0]['url'], 'http://127.0.0.1:%d/vid.mp4' % self.port)\n+        try:\n+            from http.server import ThreadingHTTPServer\n+        except ImportError:\n+            try:\n+                from socketserver import ThreadingMixIn\n+            except ImportError:\n+                from SocketServer import ThreadingMixIn\n \n+            class ThreadingHTTPServer(ThreadingMixIn, compat_http_server.HTTPServer):\n+                pass\n \n-class TestHTTPS(unittest.TestCase):\n-    def setUp(self):\n+        # HTTPS server\n         certfn = os.path.join(TEST_DIR, 'testcert.pem')\n-        self.httpd = compat_http_server.HTTPServer(\n+        self.https_httpd = ThreadingHTTPServer(\n             ('127.0.0.1', 0), HTTPTestRequestHandler)\n-        self.httpd.socket = ssl.wrap_socket(\n-            self.httpd.socket, certfile=certfn, server_side=True)\n-        self.port = http_server_port(self.httpd)\n-        self.server_thread = threading.Thread(target=self.httpd.serve_forever)\n-        self.server_thread.daemon = True\n-        self.server_thread.start()\n+        try:\n+            sslctx = ssl.SSLContext(ssl.PROTOCOL_SSLv23)\n+            sslctx.verify_mode = ssl.CERT_NONE\n+            sslctx.check_hostname = False\n+            sslctx.load_cert_chain(certfn, None)\n+            self.https_httpd.socket = sslctx.wrap_socket(\n+                self.https_httpd.socket, server_side=True)\n+        except AttributeError:\n+            self.https_httpd.socket = ssl.wrap_socket(\n+                self.https_httpd.socket, certfile=certfn, server_side=True)\n+\n+        self.https_port = http_server_port(self.https_httpd)\n+        self.https_server_thread = threading.Thread(target=self.https_httpd.serve_forever)\n+        self.https_server_thread.daemon = True\n+        self.https_server_thread.start()\n \n+    def tearDown(self):\n+\n+        def closer(svr):\n+            def _closer():\n+                svr.shutdown()\n+                svr.server_close()\n+            return _closer\n+\n+        shutdown_thread = threading.Thread(target=closer(self.http_httpd))\n+        shutdown_thread.start()\n+        self.http_server_thread.join(2.0)\n+\n+        shutdown_thread = threading.Thread(target=closer(self.https_httpd))\n+        shutdown_thread.start()\n+        self.https_server_thread.join(2.0)\n+\n+    def _test_url(self, path, host='127.0.0.1', scheme='http', port=None):\n+        return '{0}://{1}:{2}/{3}'.format(\n+            scheme, host,\n+            port if port is not None\n+            else self.https_port if scheme == 'https'\n+            else self.http_port, path)\n+\n+    @unittest.skipUnless(_check_cert, 'No support for certificate check in SSL')\n     def test_nocheckcertificate(self):\n-        if sys.version_info >= (2, 7, 9):  # No certificate checking anyways\n-            ydl = YoutubeDL({'logger': FakeLogger()})\n-            self.assertRaises(\n-                Exception,\n-                ydl.extract_info, 'https://127.0.0.1:%d/video.html' % self.port)\n+        with FakeYDL({'logger': FakeLogger()}) as ydl:\n+            with self.assertRaises(compat_urllib_error.URLError):\n+                ydl.urlopen(sanitized_Request(self._test_url('headers', scheme='https')))\n+\n+        with FakeYDL({'logger': FakeLogger(), 'nocheckcertificate': True}) as ydl:\n+            r = ydl.urlopen(sanitized_Request(self._test_url('headers', scheme='https')))\n+            self.assertEqual(r.getcode(), 200)\n+            r.close()\n+\n+    def test_percent_encode(self):\n+        with FakeYDL() as ydl:\n+            # Unicode characters should be encoded with uppercase percent-encoding\n+            res = ydl.urlopen(sanitized_Request(self._test_url('\u4e2d\u6587.html')))\n+            self.assertEqual(res.getcode(), 200)\n+            res.close()\n+            # don't normalize existing percent encodings\n+            res = ydl.urlopen(sanitized_Request(self._test_url('%c7%9f')))\n+            self.assertEqual(res.getcode(), 200)\n+            res.close()\n+\n+    def test_unicode_path_redirection(self):\n+        with FakeYDL() as ydl:\n+            r = ydl.urlopen(sanitized_Request(self._test_url('302-non-ascii-redirect')))\n+            self.assertEqual(r.url, self._test_url('%E4%B8%AD%E6%96%87.html'))\n+            r.close()\n+\n+    def test_redirect(self):\n+        with FakeYDL() as ydl:\n+            def do_req(redirect_status, method, check_no_content=False):\n+                data = b'testdata' if method in ('POST', 'PUT') else None\n+                res = ydl.urlopen(sanitized_Request(\n+                    self._test_url('redirect_{0}'.format(redirect_status)),\n+                    method=method, data=data))\n+                if check_no_content:\n+                    self.assertNotIn('Content-Type', res.headers)\n+                return res.read().decode('utf-8'), res.headers.get('method', '')\n+            # A 303 must either use GET or HEAD for subsequent request\n+            self.assertEqual(do_req(303, 'POST'), ('', 'GET'))\n+            self.assertEqual(do_req(303, 'HEAD'), ('', 'HEAD'))\n+\n+            self.assertEqual(do_req(303, 'PUT'), ('', 'GET'))\n+\n+            # 301 and 302 turn POST only into a GET, with no Content-Type\n+            self.assertEqual(do_req(301, 'POST', True), ('', 'GET'))\n+            self.assertEqual(do_req(301, 'HEAD'), ('', 'HEAD'))\n+            self.assertEqual(do_req(302, 'POST', True), ('', 'GET'))\n+            self.assertEqual(do_req(302, 'HEAD'), ('', 'HEAD'))\n+\n+            self.assertEqual(do_req(301, 'PUT'), ('testdata', 'PUT'))\n+            self.assertEqual(do_req(302, 'PUT'), ('testdata', 'PUT'))\n+\n+            # 307 and 308 should not change method\n+            for m in ('POST', 'PUT'):\n+                self.assertEqual(do_req(307, m), ('testdata', m))\n+                self.assertEqual(do_req(308, m), ('testdata', m))\n+\n+            self.assertEqual(do_req(307, 'HEAD'), ('', 'HEAD'))\n+            self.assertEqual(do_req(308, 'HEAD'), ('', 'HEAD'))\n+\n+            # These should not redirect and instead raise an HTTPError\n+            for code in (300, 304, 305, 306):\n+                with self.assertRaises(compat_urllib_HTTPError):\n+                    do_req(code, 'GET')\n+\n+    # Jython 2.7.1 times out for some reason\n+    @expectedFailureIf(sys.platform.startswith('java') and sys.version_info < (2, 7, 2))\n+    def test_content_type(self):\n+        # https://github.com/yt-dlp/yt-dlp/commit/379a4f161d4ad3e40932dcf5aca6e6fb9715ab28\n+        with FakeYDL({'nocheckcertificate': True}) as ydl:\n+            # method should be auto-detected as POST\n+            r = sanitized_Request(self._test_url('headers', scheme='https'), data=urlencode_postdata({'test': 'test'}))\n+\n+            headers = ydl.urlopen(r).read().decode('utf-8')\n+            self.assertIn('Content-Type: application/x-www-form-urlencoded', headers)\n+\n+            # test http\n+            r = sanitized_Request(self._test_url('headers'), data=urlencode_postdata({'test': 'test'}))\n+            headers = ydl.urlopen(r).read().decode('utf-8')\n+            self.assertIn('Content-Type: application/x-www-form-urlencoded', headers)\n+\n+    def test_update_req(self):\n+        req = sanitized_Request('http://example.com')\n+        assert req.data is None\n+        assert req.get_method() == 'GET'\n+        assert not req.has_header('Content-Type')\n+        # Test that zero-byte payloads will be sent\n+        req = update_Request(req, data=b'')\n+        assert req.data == b''\n+        assert req.get_method() == 'POST'\n+        # yt-dl expects data to be encoded and Content-Type to be added by sender\n+        # assert req.get_header('Content-Type') == 'application/x-www-form-urlencoded'\n \n-        ydl = YoutubeDL({'logger': FakeLogger(), 'nocheckcertificate': True})\n-        r = ydl.extract_info('https://127.0.0.1:%d/video.html' % self.port)\n-        self.assertEqual(r['entries'][0]['url'], 'https://127.0.0.1:%d/vid.mp4' % self.port)\n+    def test_cookiejar(self):\n+        with FakeYDL() as ydl:\n+            ydl.cookiejar.set_cookie(compat_http_cookiejar_Cookie(\n+                0, 'test', 'ytdl', None, False, '127.0.0.1', True,\n+                False, '/headers', True, False, None, False, None, None, {}))\n+            data = ydl.urlopen(sanitized_Request(\n+                self._test_url('headers'))).read().decode('utf-8')\n+            self.assertIn('Cookie: test=ytdl', data)\n+\n+    def test_passed_cookie_header(self):\n+        # We should accept a Cookie header being passed as in normal headers and handle it appropriately.\n+        with FakeYDL() as ydl:\n+            # Specified Cookie header should be used\n+            res = ydl.urlopen(sanitized_Request(\n+                self._test_url('headers'), headers={'Cookie': 'test=test'})).read().decode('utf-8')\n+            self.assertIn('Cookie: test=test', res)\n+\n+            # Specified Cookie header should be removed on any redirect\n+            res = ydl.urlopen(sanitized_Request(\n+                self._test_url('308-to-headers'), headers={'Cookie': 'test=test'})).read().decode('utf-8')\n+            self.assertNotIn('Cookie: test=test', res)\n+\n+            # Specified Cookie header should override global cookiejar for that request\n+            ydl.cookiejar.set_cookie(compat_http_cookiejar_Cookie(\n+                0, 'test', 'ytdlp', None, False, '127.0.0.1', True,\n+                False, '/headers', True, False, None, False, None, None, {}))\n+            data = ydl.urlopen(sanitized_Request(\n+                self._test_url('headers'), headers={'Cookie': 'test=test'})).read().decode('utf-8')\n+            self.assertNotIn('Cookie: test=ytdlp', data)\n+            self.assertIn('Cookie: test=test', data)\n+\n+    def test_no_compression_compat_header(self):\n+        with FakeYDL() as ydl:\n+            data = ydl.urlopen(\n+                sanitized_Request(\n+                    self._test_url('headers'),\n+                    headers={'Youtubedl-no-compression': True})).read()\n+            self.assertIn(b'Accept-Encoding: identity', data)\n+            self.assertNotIn(b'youtubedl-no-compression', data.lower())\n+\n+    def test_gzip_trailing_garbage(self):\n+        # https://github.com/ytdl-org/youtube-dl/commit/aa3e950764337ef9800c936f4de89b31c00dfcf5\n+        # https://github.com/ytdl-org/youtube-dl/commit/6f2ec15cee79d35dba065677cad9da7491ec6e6f\n+        with FakeYDL() as ydl:\n+            data = ydl.urlopen(sanitized_Request(self._test_url('trailing_garbage'))).read().decode('utf-8')\n+            self.assertEqual(data, '<html><video src=\"/vid.mp4\" /></html>')\n+\n+    def __test_compression(self, encoding):\n+        with FakeYDL() as ydl:\n+            res = ydl.urlopen(\n+                sanitized_Request(\n+                    self._test_url('content-encoding'),\n+                    headers={'ytdl-encoding': encoding}))\n+            # decoded encodings are removed: only check for valid decompressed data\n+            self.assertEqual(res.read(), b'<html><video src=\"/vid.mp4\" /></html>')\n+\n+    @unittest.skipUnless(brotli, 'brotli support is not installed')\n+    def test_brotli(self):\n+        self.__test_compression('br')\n+\n+    def test_deflate(self):\n+        self.__test_compression('deflate')\n+\n+    def test_gzip(self):\n+        self.__test_compression('gzip')\n+\n+    def test_multiple_encodings(self):\n+        # https://www.rfc-editor.org/rfc/rfc9110.html#section-8.4\n+        for pair in ('gzip,deflate', 'deflate, gzip', 'gzip, gzip', 'deflate, deflate'):\n+            self.__test_compression(pair)\n+\n+    def test_unsupported_encoding(self):\n+        # it should return the raw content\n+        with FakeYDL() as ydl:\n+            res = ydl.urlopen(\n+                sanitized_Request(\n+                    self._test_url('content-encoding'),\n+                    headers={'ytdl-encoding': 'unsupported'}))\n+            self.assertEqual(res.headers.get('Content-Encoding'), 'unsupported')\n+            self.assertEqual(res.read(), b'raw')\n+\n+    def test_remove_dot_segments(self):\n+        with FakeYDL() as ydl:\n+            res = ydl.urlopen(sanitized_Request(self._test_url('a/b/./../../headers')))\n+            self.assertEqual(compat_urllib_parse.urlparse(res.geturl()).path, '/headers')\n+\n+            res = ydl.urlopen(sanitized_Request(self._test_url('redirect_dotsegments')))\n+            self.assertEqual(compat_urllib_parse.urlparse(res.geturl()).path, '/headers')\n \n \n def _build_proxy_handler(name):\n@@ -117,7 +515,7 @@ def do_GET(self):\n             self.send_response(200)\n             self.send_header('Content-Type', 'text/plain; charset=utf-8')\n             self.end_headers()\n-            self.wfile.write('{self.proxy_name}: {self.path}'.format(self=self).encode('utf-8'))\n+            self.wfile.write('{0}: {1}'.format(self.proxy_name, self.path).encode('utf-8'))\n     return HTTPTestRequestHandler\n \n \n@@ -137,10 +535,30 @@ def setUp(self):\n         self.geo_proxy_thread.daemon = True\n         self.geo_proxy_thread.start()\n \n+    def tearDown(self):\n+\n+        def closer(svr):\n+            def _closer():\n+                svr.shutdown()\n+                svr.server_close()\n+            return _closer\n+\n+        shutdown_thread = threading.Thread(target=closer(self.proxy))\n+        shutdown_thread.start()\n+        self.proxy_thread.join(2.0)\n+\n+        shutdown_thread = threading.Thread(target=closer(self.geo_proxy))\n+        shutdown_thread.start()\n+        self.geo_proxy_thread.join(2.0)\n+\n+    def _test_proxy(self, host='127.0.0.1', port=None):\n+        return '{0}:{1}'.format(\n+            host, port if port is not None else self.port)\n+\n     def test_proxy(self):\n-        geo_proxy = '127.0.0.1:{0}'.format(self.geo_port)\n+        geo_proxy = self._test_proxy(port=self.geo_port)\n         ydl = YoutubeDL({\n-            'proxy': '127.0.0.1:{0}'.format(self.port),\n+            'proxy': self._test_proxy(),\n             'geo_verification_proxy': geo_proxy,\n         })\n         url = 'http://foo.com/bar'\n@@ -154,7 +572,7 @@ def test_proxy(self):\n \n     def test_proxy_with_idn(self):\n         ydl = YoutubeDL({\n-            'proxy': '127.0.0.1:{0}'.format(self.port),\n+            'proxy': self._test_proxy(),\n         })\n         url = 'http://\u4e2d\u6587.tw/'\n         response = ydl.urlopen(url).read().decode('utf-8')\n@@ -162,5 +580,25 @@ def test_proxy_with_idn(self):\n         self.assertEqual(response, 'normal: http://xn--fiq228c.tw/')\n \n \n+class TestFileURL(unittest.TestCase):\n+    # See https://github.com/ytdl-org/youtube-dl/issues/8227\n+    def test_file_urls(self):\n+        tf = tempfile.NamedTemporaryFile(delete=False)\n+        tf.write(b'foobar')\n+        tf.close()\n+        url = compat_urllib_parse.urljoin('file://', pathname2url(tf.name))\n+        with FakeYDL() as ydl:\n+            self.assertRaisesRegexp(\n+                compat_urllib_error.URLError, 'file:// scheme is explicitly disabled in youtube-dl for security reasons', ydl.urlopen, url)\n+        # not yet implemented\n+        \"\"\"\n+        with FakeYDL({'enable_file_urls': True}) as ydl:\n+            res = ydl.urlopen(url)\n+            self.assertEqual(res.read(), b'foobar')\n+            res.close()\n+        \"\"\"\n+        os.unlink(tf.name)\n+\n+\n if __name__ == '__main__':\n     unittest.main()\ndiff --git a/test/test_jsinterp.py b/test/test_jsinterp.py\nindex acdabffb1f8..c7a4f2cbf23 100644\n--- a/test/test_jsinterp.py\n+++ b/test/test_jsinterp.py\n@@ -8,175 +8,450 @@\n import unittest\n sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n \n-from youtube_dl.jsinterp import JSInterpreter\n+import math\n+import re\n \n+from youtube_dl.compat import compat_str\n+from youtube_dl.jsinterp import JS_Undefined, JSInterpreter\n \n-class TestJSInterpreter(unittest.TestCase):\n-    def test_basic(self):\n-        jsi = JSInterpreter('function x(){;}')\n-        self.assertEqual(jsi.call_function('x'), None)\n+NaN = object()\n \n-        jsi = JSInterpreter('function x3(){return 42;}')\n-        self.assertEqual(jsi.call_function('x3'), 42)\n \n-        jsi = JSInterpreter('var x5 = function(){return 42;}')\n-        self.assertEqual(jsi.call_function('x5'), 42)\n+class TestJSInterpreter(unittest.TestCase):\n+    def _test(self, jsi_or_code, expected, func='f', args=()):\n+        if isinstance(jsi_or_code, compat_str):\n+            jsi_or_code = JSInterpreter(jsi_or_code)\n+        got = jsi_or_code.call_function(func, *args)\n+        if expected is NaN:\n+            self.assertTrue(math.isnan(got), '{0} is not NaN'.format(got))\n+        else:\n+            self.assertEqual(got, expected)\n+\n+    def test_basic(self):\n+        jsi = JSInterpreter('function f(){;}')\n+        self.assertEqual(repr(jsi.extract_function('f')), 'F<f>')\n+        self._test(jsi, None)\n+\n+        self._test('function f(){return 42;}', 42)\n+        self._test('function f(){42}', None)\n+        self._test('var f = function(){return 42;}', 42)\n+\n+    def test_add(self):\n+        self._test('function f(){return 42 + 7;}', 49)\n+        self._test('function f(){return 42 + undefined;}', NaN)\n+        self._test('function f(){return 42 + null;}', 42)\n+\n+    def test_sub(self):\n+        self._test('function f(){return 42 - 7;}', 35)\n+        self._test('function f(){return 42 - undefined;}', NaN)\n+        self._test('function f(){return 42 - null;}', 42)\n+\n+    def test_mul(self):\n+        self._test('function f(){return 42 * 7;}', 294)\n+        self._test('function f(){return 42 * undefined;}', NaN)\n+        self._test('function f(){return 42 * null;}', 0)\n+\n+    def test_div(self):\n+        jsi = JSInterpreter('function f(a, b){return a / b;}')\n+        self._test(jsi, NaN, args=(0, 0))\n+        self._test(jsi, NaN, args=(JS_Undefined, 1))\n+        self._test(jsi, float('inf'), args=(2, 0))\n+        self._test(jsi, 0, args=(0, 3))\n+\n+    def test_mod(self):\n+        self._test('function f(){return 42 % 7;}', 0)\n+        self._test('function f(){return 42 % 0;}', NaN)\n+        self._test('function f(){return 42 % undefined;}', NaN)\n+\n+    def test_exp(self):\n+        self._test('function f(){return 42 ** 2;}', 1764)\n+        self._test('function f(){return 42 ** undefined;}', NaN)\n+        self._test('function f(){return 42 ** null;}', 1)\n+        self._test('function f(){return undefined ** 42;}', NaN)\n \n     def test_calc(self):\n-        jsi = JSInterpreter('function x4(a){return 2*a+1;}')\n-        self.assertEqual(jsi.call_function('x4', 3), 7)\n+        self._test('function f(a){return 2*a+1;}', 7, args=[3])\n \n     def test_empty_return(self):\n-        jsi = JSInterpreter('function f(){return; y()}')\n-        self.assertEqual(jsi.call_function('f'), None)\n+        self._test('function f(){return; y()}', None)\n \n     def test_morespace(self):\n-        jsi = JSInterpreter('function x (a) { return 2 * a + 1 ; }')\n-        self.assertEqual(jsi.call_function('x', 3), 7)\n-\n-        jsi = JSInterpreter('function f () { x =  2  ; return x; }')\n-        self.assertEqual(jsi.call_function('f'), 2)\n+        self._test('function f (a) { return 2 * a + 1 ; }', 7, args=[3])\n+        self._test('function f () { x =  2  ; return x; }', 2)\n \n     def test_strange_chars(self):\n-        jsi = JSInterpreter('function $_xY1 ($_axY1) { var $_axY2 = $_axY1 + 1; return $_axY2; }')\n-        self.assertEqual(jsi.call_function('$_xY1', 20), 21)\n+        self._test('function $_xY1 ($_axY1) { var $_axY2 = $_axY1 + 1; return $_axY2; }',\n+                   21, args=[20], func='$_xY1')\n \n     def test_operators(self):\n-        jsi = JSInterpreter('function f(){return 1 << 5;}')\n-        self.assertEqual(jsi.call_function('f'), 32)\n-\n-        jsi = JSInterpreter('function f(){return 19 & 21;}')\n-        self.assertEqual(jsi.call_function('f'), 17)\n-\n-        jsi = JSInterpreter('function f(){return 11 >> 2;}')\n-        self.assertEqual(jsi.call_function('f'), 2)\n+        self._test('function f(){return 1 << 5;}', 32)\n+        self._test('function f(){return 2 ** 5}', 32)\n+        self._test('function f(){return 19 & 21;}', 17)\n+        self._test('function f(){return 11 >> 2;}', 2)\n+        self._test('function f(){return []? 2+3: 4;}', 5)\n+        self._test('function f(){return 1 == 2}', False)\n+        self._test('function f(){return 0 && 1 || 2;}', 2)\n+        self._test('function f(){return 0 ?? 42;}', 0)\n+        self._test('function f(){return \"life, the universe and everything\" < 42;}', False)\n+        # https://github.com/ytdl-org/youtube-dl/issues/32815\n+        self._test('function f(){return 0  - 7 * - 6;}', 42)\n \n     def test_array_access(self):\n-        jsi = JSInterpreter('function f(){var x = [1,2,3]; x[0] = 4; x[0] = 5; x[2] = 7; return x;}')\n-        self.assertEqual(jsi.call_function('f'), [5, 2, 7])\n+        self._test('function f(){var x = [1,2,3]; x[0] = 4; x[0] = 5; x[2.0] = 7; return x;}', [5, 2, 7])\n \n     def test_parens(self):\n-        jsi = JSInterpreter('function f(){return (1) + (2) * ((( (( (((((3)))))) )) ));}')\n-        self.assertEqual(jsi.call_function('f'), 7)\n+        self._test('function f(){return (1) + (2) * ((( (( (((((3)))))) )) ));}', 7)\n+        self._test('function f(){return (1 + 2) * 3;}', 9)\n \n-        jsi = JSInterpreter('function f(){return (1 + 2) * 3;}')\n-        self.assertEqual(jsi.call_function('f'), 9)\n+    def test_quotes(self):\n+        self._test(r'function f(){return \"a\\\"\\\\(\"}', r'a\"\\(')\n \n     def test_assignments(self):\n-        jsi = JSInterpreter('function f(){var x = 20; x = 30 + 1; return x;}')\n-        self.assertEqual(jsi.call_function('f'), 31)\n-\n-        jsi = JSInterpreter('function f(){var x = 20; x += 30 + 1; return x;}')\n-        self.assertEqual(jsi.call_function('f'), 51)\n-\n-        jsi = JSInterpreter('function f(){var x = 20; x -= 30 + 1; return x;}')\n-        self.assertEqual(jsi.call_function('f'), -11)\n+        self._test('function f(){var x = 20; x = 30 + 1; return x;}', 31)\n+        self._test('function f(){var x = 20; x += 30 + 1; return x;}', 51)\n+        self._test('function f(){var x = 20; x -= 30 + 1; return x;}', -11)\n \n+    @unittest.skip('Not yet fully implemented')\n     def test_comments(self):\n-        'Skipping: Not yet fully implemented'\n-        return\n-        jsi = JSInterpreter('''\n-        function x() {\n-            var x = /* 1 + */ 2;\n-            var y = /* 30\n-            * 40 */ 50;\n-            return x + y;\n-        }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 52)\n-\n-        jsi = JSInterpreter('''\n-        function f() {\n-            var x = \"/*\";\n-            var y = 1 /* comment */ + 2;\n-            return y;\n-        }\n-        ''')\n-        self.assertEqual(jsi.call_function('f'), 3)\n+        self._test('''\n+            function f() {\n+                var x = /* 1 + */ 2;\n+                var y = /* 30\n+                * 40 */ 50;\n+                return x + y;\n+            }\n+        ''', 52)\n+\n+        self._test('''\n+            function f() {\n+                var x = \"/*\";\n+                var y = 1 /* comment */ + 2;\n+                return y;\n+            }\n+        ''', 3)\n \n     def test_precedence(self):\n-        jsi = JSInterpreter('''\n-        function x() {\n-            var a = [10, 20, 30, 40, 50];\n-            var b = 6;\n-            a[0]=a[b%a.length];\n-            return a;\n-        }''')\n-        self.assertEqual(jsi.call_function('x'), [20, 20, 30, 40, 50])\n+        self._test('''\n+            function f() {\n+                var a = [10, 20, 30, 40, 50];\n+                var b = 6;\n+                a[0]=a[b%a.length];\n+                return a;\n+            }\n+        ''', [20, 20, 30, 40, 50])\n+\n+    def test_builtins(self):\n+        self._test('function f() { return NaN }', NaN)\n+\n+    def test_Date(self):\n+        self._test('function f() { return new Date(\"Wednesday 31 December 1969 18:01:26 MDT\") - 0; }', 86000)\n+\n+        jsi = JSInterpreter('function f(dt) { return new Date(dt) - 0; }')\n+        # date format m/d/y\n+        self._test(jsi, 86000, args=['12/31/1969 18:01:26 MDT'])\n+        # epoch 0\n+        self._test(jsi, 0, args=['1 January 1970 00:00:00 UTC'])\n \n     def test_call(self):\n         jsi = JSInterpreter('''\n         function x() { return 2; }\n-        function y(a) { return x() + a; }\n+        function y(a) { return x() + (a?a:0); }\n         function z() { return y(3); }\n         ''')\n-        self.assertEqual(jsi.call_function('z'), 5)\n+        self._test(jsi, 5, func='z')\n+        self._test(jsi, 2, func='y')\n+\n+    def test_if(self):\n+        self._test('''\n+            function f() {\n+            let a = 9;\n+            if (0==0) {a++}\n+            return a\n+            }\n+        ''', 10)\n+\n+        self._test('''\n+            function f() {\n+            if (0==0) {return 10}\n+            }\n+        ''', 10)\n+\n+        self._test('''\n+            function f() {\n+            if (0!=0) {return 1}\n+            else {return 10}\n+            }\n+        ''', 10)\n+\n+    def test_elseif(self):\n+        self._test('''\n+            function f() {\n+                if (0!=0) {return 1}\n+                else if (1==0) {return 2}\n+                else {return 10}\n+            }\n+        ''', 10)\n \n     def test_for_loop(self):\n-        # function x() { a=0; for (i=0; i-10; i++) {a++} a }\n-        jsi = JSInterpreter('''\n-        function x() { a=0; for (i=0; i-10; i = i + 1) {a++} a }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 10)\n+        self._test('function f() { a=0; for (i=0; i-10; i++) {a++} return a }', 10)\n+\n+    def test_while_loop(self):\n+        self._test('function f() { a=0; while (a<10) {a++} return a }', 10)\n \n     def test_switch(self):\n         jsi = JSInterpreter('''\n-        function x(f) { switch(f){\n-            case 1:f+=1;\n-            case 2:f+=2;\n-            case 3:f+=3;break;\n-            case 4:f+=4;\n-            default:f=0;\n-        } return f }\n+            function f(x) { switch(x){\n+                case 1:x+=1;\n+                case 2:x+=2;\n+                case 3:x+=3;break;\n+                case 4:x+=4;\n+                default:x=0;\n+            } return x }\n         ''')\n-        self.assertEqual(jsi.call_function('x', 1), 7)\n-        self.assertEqual(jsi.call_function('x', 3), 6)\n-        self.assertEqual(jsi.call_function('x', 5), 0)\n+        self._test(jsi, 7, args=[1])\n+        self._test(jsi, 6, args=[3])\n+        self._test(jsi, 0, args=[5])\n \n     def test_switch_default(self):\n         jsi = JSInterpreter('''\n-        function x(f) { switch(f){\n-            case 2: f+=2;\n-            default: f-=1;\n-            case 5:\n-            case 6: f+=6;\n-            case 0: break;\n-            case 1: f+=1;\n-        } return f }\n+            function f(x) { switch(x){\n+                case 2: x+=2;\n+                default: x-=1;\n+                case 5:\n+                case 6: x+=6;\n+                case 0: break;\n+                case 1: x+=1;\n+            } return x }\n         ''')\n-        self.assertEqual(jsi.call_function('x', 1), 2)\n-        self.assertEqual(jsi.call_function('x', 5), 11)\n-        self.assertEqual(jsi.call_function('x', 9), 14)\n+        self._test(jsi, 2, args=[1])\n+        self._test(jsi, 11, args=[5])\n+        self._test(jsi, 14, args=[9])\n \n     def test_try(self):\n-        jsi = JSInterpreter('''\n-        function x() { try{return 10} catch(e){return 5} }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 10)\n+        self._test('function f() { try{return 10} catch(e){return 5} }', 10)\n+\n+    def test_catch(self):\n+        self._test('function f() { try{throw 10} catch(e){return 5} }', 5)\n+\n+    def test_finally(self):\n+        self._test('function f() { try{throw 10} finally {return 42} }', 42)\n+        self._test('function f() { try{throw 10} catch(e){return 5} finally {return 42} }', 42)\n+\n+    def test_nested_try(self):\n+        self._test('''\n+            function f() {try {\n+                try{throw 10} finally {throw 42}\n+            } catch(e){return 5} }\n+        ''', 5)\n \n     def test_for_loop_continue(self):\n+        self._test('function f() { a=0; for (i=0; i-10; i++) { continue; a++ } return a }', 0)\n+\n+    def test_for_loop_break(self):\n+        self._test('function f() { a=0; for (i=0; i-10; i++) { break; a++ } return a }', 0)\n+\n+    def test_for_loop_try(self):\n+        self._test('''\n+            function f() {\n+                for (i=0; i-10; i++) { try { if (i == 5) throw i} catch {return 10} finally {break} };\n+                return 42 }\n+        ''', 42)\n+\n+    def test_literal_list(self):\n+        self._test('function f() { return [1, 2, \"asdf\", [5, 6, 7]][3] }', [5, 6, 7])\n+\n+    def test_comma(self):\n+        self._test('function f() { a=5; a -= 1, a+=3; return a }', 7)\n+        self._test('function f() { a=5; return (a -= 1, a+=3, a); }', 7)\n+        self._test('function f() { return (l=[0,1,2,3], function(a, b){return a+b})((l[1], l[2]), l[3]) }', 5)\n+\n+    def test_void(self):\n+        self._test('function f() { return void 42; }', None)\n+\n+    def test_return_function(self):\n         jsi = JSInterpreter('''\n-        function x() { a=0; for (i=0; i-10; i++) { continue; a++ } a }\n+        function x() { return [1, function(){return 1}][1] }\n         ''')\n-        self.assertEqual(jsi.call_function('x'), 0)\n+        self.assertEqual(jsi.call_function('x')([]), 1)\n+\n+    def test_null(self):\n+        self._test('function f() { return null; }', None)\n+        self._test('function f() { return [null > 0, null < 0, null == 0, null === 0]; }',\n+                   [False, False, False, False])\n+        self._test('function f() { return [null >= 0, null <= 0]; }', [True, True])\n+\n+    def test_undefined(self):\n+        self._test('function f() { return undefined === undefined; }', True)\n+        self._test('function f() { return undefined; }', JS_Undefined)\n+        self._test('function f() {return undefined ?? 42; }', 42)\n+        self._test('function f() { let v; return v; }', JS_Undefined)\n+        self._test('function f() { let v; return v**0; }', 1)\n+        self._test('function f() { let v; return [v>42, v<=42, v&&42, 42&&v]; }',\n+                   [False, False, JS_Undefined, JS_Undefined])\n+\n+        self._test('''\n+            function f() { return [\n+                undefined === undefined,\n+                undefined == undefined,\n+                undefined == null\n+            ]; }\n+        ''', [True] * 3)\n+        self._test('''\n+            function f() { return [\n+                undefined < undefined,\n+                undefined > undefined,\n+                undefined === 0,\n+                undefined == 0,\n+                undefined < 0,\n+                undefined > 0,\n+                undefined >= 0,\n+                undefined <= 0,\n+                undefined > null,\n+                undefined < null,\n+                undefined === null\n+            ]; }\n+        ''', [False] * 11)\n \n-    def test_for_loop_break(self):\n         jsi = JSInterpreter('''\n-        function x() { a=0; for (i=0; i-10; i++) { break; a++ } a }\n+            function x() { let v; return [42+v, v+42, v**42, 42**v, 0**v]; }\n         ''')\n-        self.assertEqual(jsi.call_function('x'), 0)\n+        for y in jsi.call_function('x'):\n+            self.assertTrue(math.isnan(y))\n+\n+    def test_object(self):\n+        self._test('function f() { return {}; }', {})\n+        self._test('function f() { let a = {m1: 42, m2: 0 }; return [a[\"m1\"], a.m2]; }', [42, 0])\n+        self._test('function f() { let a; return a?.qq; }', JS_Undefined)\n+        self._test('function f() { let a = {m1: 42, m2: 0 }; return a?.qq; }', JS_Undefined)\n+\n+    def test_regex(self):\n+        self._test('function f() { let a=/,,[/,913,/](,)}/; }', None)\n \n-    def test_literal_list(self):\n         jsi = JSInterpreter('''\n-        function x() { [1, 2, \"asdf\", [5, 6, 7]][3] }\n+            function x() { let a=/,,[/,913,/](,)}/; \"\".replace(a, \"\"); return a; }\n         ''')\n-        self.assertEqual(jsi.call_function('x'), [5, 6, 7])\n+        attrs = set(('findall', 'finditer', 'match', 'scanner', 'search',\n+                     'split', 'sub', 'subn'))\n+        if sys.version_info >= (2, 7):\n+            # documented for 2.6 but may not be found\n+            attrs.update(('flags', 'groupindex', 'groups', 'pattern'))\n+        self.assertSetEqual(set(dir(jsi.call_function('x'))) & attrs, attrs)\n \n-    def test_comma(self):\n         jsi = JSInterpreter('''\n-        function x() { a=5; a -= 1, a+=3; return a }\n+            function x() { let a=/,,[/,913,/](,)}/i; return a; }\n         ''')\n-        self.assertEqual(jsi.call_function('x'), 7)\n+        self.assertEqual(jsi.call_function('x').flags & ~re.U, re.I)\n+\n+        jsi = JSInterpreter(r'function f() { let a=/,][}\",],()}(\\[)/; return a; }')\n+        self.assertEqual(jsi.call_function('f').pattern, r',][}\",],()}(\\[)')\n+\n+        jsi = JSInterpreter(r'function f() { let a=[/[)\\\\]/]; return a[0]; }')\n+        self.assertEqual(jsi.call_function('f').pattern, r'[)\\\\]')\n+\n+    def test_replace(self):\n+        self._test('function f() { let a=\"data-name\".replace(\"data-\", \"\"); return a }',\n+                   'name')\n+        self._test('function f() { let a=\"data-name\".replace(new RegExp(\"^.+-\"), \"\"); return a; }',\n+                   'name')\n+        self._test('function f() { let a=\"data-name\".replace(/^.+-/, \"\"); return a; }',\n+                   'name')\n+        self._test('function f() { let a=\"data-name\".replace(/a/g, \"o\"); return a; }',\n+                   'doto-nome')\n+        self._test('function f() { let a=\"data-name\".replaceAll(\"a\", \"o\"); return a; }',\n+                   'doto-nome')\n+\n+    def test_char_code_at(self):\n+        jsi = JSInterpreter('function f(i){return \"test\".charCodeAt(i)}')\n+        self._test(jsi, 116, args=[0])\n+        self._test(jsi, 101, args=[1])\n+        self._test(jsi, 115, args=[2])\n+        self._test(jsi, 116, args=[3])\n+        self._test(jsi, None, args=[4])\n+        self._test(jsi, 116, args=['not_a_number'])\n+\n+    def test_bitwise_operators_overflow(self):\n+        self._test('function f(){return -524999584 << 5}', 379882496)\n+        self._test('function f(){return 1236566549 << 5}', 915423904)\n+\n+    def test_bitwise_operators_typecast(self):\n+        # madness\n+        self._test('function f(){return null << 5}', 0)\n+        self._test('function f(){return undefined >> 5}', 0)\n+        self._test('function f(){return 42 << NaN}', 42)\n+        self._test('function f(){return 42 << Infinity}', 42)\n+\n+    def test_negative(self):\n+        self._test('function f(){return 2    *    -2.0    ;}', -4)\n+        self._test('function f(){return 2    -    - -2    ;}', 0)\n+        self._test('function f(){return 2    -    - - -2  ;}', 4)\n+        self._test('function f(){return 2    -    + + - -2;}', 0)\n+        self._test('function f(){return 2    +    - + - -2;}', 0)\n+\n+    def test_32066(self):\n+        self._test(\n+            \"function f(){return Math.pow(3, 5) + new Date('1970-01-01T08:01:42.000+08:00') / 1000 * -239 - -24205;}\",\n+            70)\n+\n+    @unittest.skip('Not yet working')\n+    def test_packed(self):\n+        self._test(\n+            '''function f(p,a,c,k,e,d){while(c--)if(k[c])p=p.replace(new RegExp('\\\\b'+c.toString(a)+'\\\\b','g'),k[c]);return p}''',\n+            '''h 7=g(\"1j\");7.7h({7g:[{33:\"w://7f-7e-7d-7c.v.7b/7a/79/78/77/76.74?t=73&s=2s&e=72&f=2t&71=70.0.0.1&6z=6y&6x=6w\"}],6v:\"w://32.v.u/6u.31\",16:\"r%\",15:\"r%\",6t:\"6s\",6r:\"\",6q:\"l\",6p:\"l\",6o:\"6n\",6m:\\'6l\\',6k:\"6j\",9:[{33:\"/2u?b=6i&n=50&6h=w://32.v.u/6g.31\",6f:\"6e\"}],1y:{6d:1,6c:\\'#6b\\',6a:\\'#69\\',68:\"67\",66:30,65:r,},\"64\":{63:\"%62 2m%m%61%5z%5y%5x.u%5w%5v%5u.2y%22 2k%m%1o%22 5t%m%1o%22 5s%m%1o%22 2j%m%5r%22 16%m%5q%22 15%m%5p%22 5o%2z%5n%5m%2z\",5l:\"w://v.u/d/1k/5k.2y\",5j:[]},\\'5i\\':{\"5h\":\"5g\"},5f:\"5e\",5d:\"w://v.u\",5c:{},5b:l,1x:[0.25,0.50,0.75,1,1.25,1.5,2]});h 1m,1n,5a;h 59=0,58=0;h 7=g(\"1j\");h 2x=0,57=0,56=0;$.55({54:{\\'53-52\\':\\'2i-51\\'}});7.j(\\'4z\\',6(x){c(5>0&&x.1l>=5&&1n!=1){1n=1;$(\\'q.4y\\').4x(\\'4w\\')}});7.j(\\'13\\',6(x){2x=x.1l});7.j(\\'2g\\',6(x){2w(x)});7.j(\\'4v\\',6(){$(\\'q.2v\\').4u()});6 2w(x){$(\\'q.2v\\').4t();c(1m)19;1m=1;17=0;c(4s.4r===l){17=1}$.4q(\\'/2u?b=4p&2l=1k&4o=2t-4n-4m-2s-4l&4k=&4j=&4i=&17=\\'+17,6(2r){$(\\'#4h\\').4g(2r)});$(\\'.3-8-4f-4e:4d(\"4c\")\\').2h(6(e){2q();g().4b(0);g().4a(l)});6 2q(){h $14=$(\"<q />\").2p({1l:\"49\",16:\"r%\",15:\"r%\",48:0,2n:0,2o:47,46:\"45(10%, 10%, 10%, 0.4)\",\"44-43\":\"42\"});$(\"<41 />\").2p({16:\"60%\",15:\"60%\",2o:40,\"3z-2n\":\"3y\"}).3x({\\'2m\\':\\'/?b=3w&2l=1k\\',\\'2k\\':\\'0\\',\\'2j\\':\\'2i\\'}).2f($14);$14.2h(6(){$(3v).3u();g().2g()});$14.2f($(\\'#1j\\'))}g().13(0);}6 3t(){h 9=7.1b(2e);2d.2c(9);c(9.n>1){1r(i=0;i<9.n;i++){c(9[i].1a==2e){2d.2c(\\'!!=\\'+i);7.1p(i)}}}}7.j(\\'3s\\',6(){g().1h(\"/2a/3r.29\",\"3q 10 28\",6(){g().13(g().27()+10)},\"2b\");$(\"q[26=2b]\").23().21(\\'.3-20-1z\\');g().1h(\"/2a/3p.29\",\"3o 10 28\",6(){h 12=g().27()-10;c(12<0)12=0;g().13(12)},\"24\");$(\"q[26=24]\").23().21(\\'.3-20-1z\\');});6 1i(){}7.j(\\'3n\\',6(){1i()});7.j(\\'3m\\',6(){1i()});7.j(\"k\",6(y){h 9=7.1b();c(9.n<2)19;$(\\'.3-8-3l-3k\\').3j(6(){$(\\'#3-8-a-k\\').1e(\\'3-8-a-z\\');$(\\'.3-a-k\\').p(\\'o-1f\\',\\'11\\')});7.1h(\"/3i/3h.3g\",\"3f 3e\",6(){$(\\'.3-1w\\').3d(\\'3-8-1v\\');$(\\'.3-8-1y, .3-8-1x\\').p(\\'o-1g\\',\\'11\\');c($(\\'.3-1w\\').3c(\\'3-8-1v\\')){$(\\'.3-a-k\\').p(\\'o-1g\\',\\'l\\');$(\\'.3-a-k\\').p(\\'o-1f\\',\\'l\\');$(\\'.3-8-a\\').1e(\\'3-8-a-z\\');$(\\'.3-8-a:1u\\').3b(\\'3-8-a-z\\')}3a{$(\\'.3-a-k\\').p(\\'o-1g\\',\\'11\\');$(\\'.3-a-k\\').p(\\'o-1f\\',\\'11\\');$(\\'.3-8-a:1u\\').1e(\\'3-8-a-z\\')}},\"39\");7.j(\"38\",6(y){1d.37(\\'1c\\',y.9[y.36].1a)});c(1d.1t(\\'1c\\')){35(\"1s(1d.1t(\\'1c\\'));\",34)}});h 18;6 1s(1q){h 9=7.1b();c(9.n>1){1r(i=0;i<9.n;i++){c(9[i].1a==1q){c(i==18){19}18=i;7.1p(i)}}}}',36,270,'|||jw|||function|player|settings|tracks|submenu||if||||jwplayer|var||on|audioTracks|true|3D|length|aria|attr|div|100|||sx|filemoon|https||event|active||false|tt|seek|dd|height|width|adb|current_audio|return|name|getAudioTracks|default_audio|localStorage|removeClass|expanded|checked|addButton|callMeMaybe|vplayer|0fxcyc2ajhp1|position|vvplay|vvad|220|setCurrentAudioTrack|audio_name|for|audio_set|getItem|last|open|controls|playbackRates|captions|rewind|icon|insertAfter||detach|ff00||button|getPosition|sec|png|player8|ff11|log|console|track_name|appendTo|play|click|no|scrolling|frameborder|file_code|src|top|zIndex|css|showCCform|data|1662367683|383371|dl|video_ad|doPlay|prevt|mp4|3E||jpg|thumbs|file|300|setTimeout|currentTrack|setItem|audioTrackChanged|dualSound|else|addClass|hasClass|toggleClass|Track|Audio|svg|dualy|images|mousedown|buttons|topbar|playAttemptFailed|beforePlay|Rewind|fr|Forward|ff|ready|set_audio_track|remove|this|upload_srt|prop|50px|margin|1000001|iframe|center|align|text|rgba|background|1000000|left|absolute|pause|setCurrentCaptions|Upload|contains|item|content|html|fviews|referer|prem|embed|3e57249ef633e0d03bf76ceb8d8a4b65|216|83|hash|view|get|TokenZir|window|hide|show|complete|slow|fadeIn|video_ad_fadein|time||cache|Cache|Content|headers|ajaxSetup|v2done|tott|vastdone2|vastdone1|vvbefore|playbackRateControls|cast|aboutlink|FileMoon|abouttext|UHD|1870|qualityLabels|sites|GNOME_POWER|link|2Fiframe|3C|allowfullscreen|22360|22640|22no|marginheight|marginwidth|2FGNOME_POWER|2F0fxcyc2ajhp1|2Fe|2Ffilemoon|2F|3A||22https|3Ciframe|code|sharing|fontOpacity|backgroundOpacity|Tahoma|fontFamily|303030|backgroundColor|FFFFFF|color|userFontScale|thumbnails|kind|0fxcyc2ajhp10000|url|get_slides|start|startparam|none|preload|html5|primary|hlshtml|androidhls|duration|uniform|stretching|0fxcyc2ajhp1_xt|image|2048|sp|6871|asn|127|srv|43200|_g3XlBcu2lmD9oDexD2NLWSmah2Nu3XcDrl93m9PwXY|m3u8||master|0fxcyc2ajhp1_x|00076|01|hls2|to|s01|delivery|storage|moon|sources|setup'''.split('|'))\n+\n+    def test_join(self):\n+        test_input = list('test')\n+        tests = [\n+            'function f(a, b){return a.join(b)}',\n+            'function f(a, b){return Array.prototype.join.call(a, b)}',\n+            'function f(a, b){return Array.prototype.join.apply(a, [b])}',\n+        ]\n+        for test in tests:\n+            jsi = JSInterpreter(test)\n+            self._test(jsi, 'test', args=[test_input, ''])\n+            self._test(jsi, 't-e-s-t', args=[test_input, '-'])\n+            self._test(jsi, '', args=[[], '-'])\n+\n+    def test_split(self):\n+        test_result = list('test')\n+        tests = [\n+            'function f(a, b){return a.split(b)}',\n+            'function f(a, b){return String.prototype.split.call(a, b)}',\n+            'function f(a, b){return String.prototype.split.apply(a, [b])}',\n+        ]\n+        for test in tests:\n+            jsi = JSInterpreter(test)\n+            self._test(jsi, test_result, args=['test', ''])\n+            self._test(jsi, test_result, args=['t-e-s-t', '-'])\n+            self._test(jsi, [''], args=['', '-'])\n+            self._test(jsi, [], args=['', ''])\n+\n+    def test_slice(self):\n+        self._test('function f(){return [0, 1, 2, 3, 4, 5, 6, 7, 8].slice()}', [0, 1, 2, 3, 4, 5, 6, 7, 8])\n+        self._test('function f(){return [0, 1, 2, 3, 4, 5, 6, 7, 8].slice(0)}', [0, 1, 2, 3, 4, 5, 6, 7, 8])\n+        self._test('function f(){return [0, 1, 2, 3, 4, 5, 6, 7, 8].slice(5)}', [5, 6, 7, 8])\n+        self._test('function f(){return [0, 1, 2, 3, 4, 5, 6, 7, 8].slice(99)}', [])\n+        self._test('function f(){return [0, 1, 2, 3, 4, 5, 6, 7, 8].slice(-2)}', [7, 8])\n+        self._test('function f(){return [0, 1, 2, 3, 4, 5, 6, 7, 8].slice(-99)}', [0, 1, 2, 3, 4, 5, 6, 7, 8])\n+        self._test('function f(){return [0, 1, 2, 3, 4, 5, 6, 7, 8].slice(0, 0)}', [])\n+        self._test('function f(){return [0, 1, 2, 3, 4, 5, 6, 7, 8].slice(1, 0)}', [])\n+        self._test('function f(){return [0, 1, 2, 3, 4, 5, 6, 7, 8].slice(0, 1)}', [0])\n+        self._test('function f(){return [0, 1, 2, 3, 4, 5, 6, 7, 8].slice(3, 6)}', [3, 4, 5])\n+        self._test('function f(){return [0, 1, 2, 3, 4, 5, 6, 7, 8].slice(1, -1)}', [1, 2, 3, 4, 5, 6, 7])\n+        self._test('function f(){return [0, 1, 2, 3, 4, 5, 6, 7, 8].slice(-1, 1)}', [])\n+        self._test('function f(){return [0, 1, 2, 3, 4, 5, 6, 7, 8].slice(-3, -1)}', [6, 7])\n+        self._test('function f(){return \"012345678\".slice()}', '012345678')\n+        self._test('function f(){return \"012345678\".slice(0)}', '012345678')\n+        self._test('function f(){return \"012345678\".slice(5)}', '5678')\n+        self._test('function f(){return \"012345678\".slice(99)}', '')\n+        self._test('function f(){return \"012345678\".slice(-2)}', '78')\n+        self._test('function f(){return \"012345678\".slice(-99)}', '012345678')\n+        self._test('function f(){return \"012345678\".slice(0, 0)}', '')\n+        self._test('function f(){return \"012345678\".slice(1, 0)}', '')\n+        self._test('function f(){return \"012345678\".slice(0, 1)}', '0')\n+        self._test('function f(){return \"012345678\".slice(3, 6)}', '345')\n+        self._test('function f(){return \"012345678\".slice(1, -1)}', '1234567')\n+        self._test('function f(){return \"012345678\".slice(-1, 1)}', '')\n+        self._test('function f(){return \"012345678\".slice(-3, -1)}', '67')\n \n \n if __name__ == '__main__':\ndiff --git a/test/test_subtitles.py b/test/test_subtitles.py\nindex 550e0ca0081..e005c78fc2a 100644\n--- a/test/test_subtitles.py\n+++ b/test/test_subtitles.py\n@@ -38,6 +38,9 @@ def setUp(self):\n         self.DL = FakeYDL()\n         self.ie = self.IE()\n         self.DL.add_info_extractor(self.ie)\n+        if not self.IE.working():\n+            print('Skipping: %s marked as not _WORKING' % self.IE.ie_key())\n+            self.skipTest('IE marked as not _WORKING')\n \n     def getInfoDict(self):\n         info_dict = self.DL.extract_info(self.url, download=False)\n@@ -56,6 +59,21 @@ def getSubtitles(self):\n \n \n class TestYoutubeSubtitles(BaseTestSubtitles):\n+    # Available subtitles for QRS8MkLhQmM:\n+    # Language formats\n+    # ru       vtt, ttml, srv3, srv2, srv1, json3\n+    # fr       vtt, ttml, srv3, srv2, srv1, json3\n+    # en       vtt, ttml, srv3, srv2, srv1, json3\n+    # nl       vtt, ttml, srv3, srv2, srv1, json3\n+    # de       vtt, ttml, srv3, srv2, srv1, json3\n+    # ko       vtt, ttml, srv3, srv2, srv1, json3\n+    # it       vtt, ttml, srv3, srv2, srv1, json3\n+    # zh-Hant  vtt, ttml, srv3, srv2, srv1, json3\n+    # hi       vtt, ttml, srv3, srv2, srv1, json3\n+    # pt-BR    vtt, ttml, srv3, srv2, srv1, json3\n+    # es-MX    vtt, ttml, srv3, srv2, srv1, json3\n+    # ja       vtt, ttml, srv3, srv2, srv1, json3\n+    # pl       vtt, ttml, srv3, srv2, srv1, json3\n     url = 'QRS8MkLhQmM'\n     IE = YoutubeIE\n \n@@ -64,41 +82,60 @@ def test_youtube_allsubtitles(self):\n         self.DL.params['allsubtitles'] = True\n         subtitles = self.getSubtitles()\n         self.assertEqual(len(subtitles.keys()), 13)\n-        self.assertEqual(md5(subtitles['en']), '3cb210999d3e021bd6c7f0ea751eab06')\n-        self.assertEqual(md5(subtitles['it']), '6d752b98c31f1cf8d597050c7a2cb4b5')\n+        self.assertEqual(md5(subtitles['en']), 'ae1bd34126571a77aabd4d276b28044d')\n+        self.assertEqual(md5(subtitles['it']), '0e0b667ba68411d88fd1c5f4f4eab2f9')\n         for lang in ['fr', 'de']:\n             self.assertTrue(subtitles.get(lang) is not None, 'Subtitles for \\'%s\\' not extracted' % lang)\n \n-    def test_youtube_subtitles_ttml_format(self):\n+    def _test_subtitles_format(self, fmt, md5_hash, lang='en'):\n         self.DL.params['writesubtitles'] = True\n-        self.DL.params['subtitlesformat'] = 'ttml'\n+        self.DL.params['subtitlesformat'] = fmt\n         subtitles = self.getSubtitles()\n-        self.assertEqual(md5(subtitles['en']), 'e306f8c42842f723447d9f63ad65df54')\n+        self.assertEqual(md5(subtitles[lang]), md5_hash)\n+\n+    def test_youtube_subtitles_ttml_format(self):\n+        self._test_subtitles_format('ttml', 'c97ddf1217390906fa9fbd34901f3da2')\n \n     def test_youtube_subtitles_vtt_format(self):\n-        self.DL.params['writesubtitles'] = True\n-        self.DL.params['subtitlesformat'] = 'vtt'\n-        subtitles = self.getSubtitles()\n-        self.assertEqual(md5(subtitles['en']), '3cb210999d3e021bd6c7f0ea751eab06')\n+        self._test_subtitles_format('vtt', 'ae1bd34126571a77aabd4d276b28044d')\n \n-    def test_youtube_automatic_captions(self):\n-        self.url = '8YoUxe5ncPo'\n+    def test_youtube_subtitles_json3_format(self):\n+        self._test_subtitles_format('json3', '688dd1ce0981683867e7fe6fde2a224b')\n+\n+    def _test_automatic_captions(self, url, lang):\n+        self.url = url\n         self.DL.params['writeautomaticsub'] = True\n-        self.DL.params['subtitleslangs'] = ['it']\n+        self.DL.params['subtitleslangs'] = [lang]\n         subtitles = self.getSubtitles()\n-        self.assertTrue(subtitles['it'] is not None)\n+        self.assertTrue(subtitles[lang] is not None)\n \n+    def test_youtube_automatic_captions(self):\n+        # Available automatic captions for 8YoUxe5ncPo:\n+        # Language formats (all in vtt, ttml, srv3, srv2, srv1, json3)\n+        # gu, zh-Hans, zh-Hant, gd, ga, gl, lb, la, lo, tt, tr,\n+        # lv, lt, tk, th, tg, te, fil, haw, yi, ceb, yo, de, da,\n+        # el, eo, en, eu, et, es, ru, rw, ro, bn, be, bg, uk, jv,\n+        # bs, ja, or, xh, co, ca, cy, cs, ps, pt, pa, vi, pl, hy,\n+        # hr, ht, hu, hmn, hi, ha, mg, uz, ml, mn, mi, mk, ur,\n+        # mt, ms, mr, ug, ta, my, af, sw, is, am,\n+        #                                         *it*, iw, sv, ar,\n+        # su, zu, az, id, ig, nl, no, ne, ny, fr, ku, fy, fa, fi,\n+        # ka, kk, sr, sq, ko, kn, km, st, sk, si, so, sn, sm, sl,\n+        # ky, sd\n+        # ...\n+        self._test_automatic_captions('8YoUxe5ncPo', 'it')\n+\n+    @unittest.skip('ASR subs all in all supported langs now')\n     def test_youtube_translated_subtitles(self):\n-        # This video has a subtitles track, which can be translated\n-        self.url = 'Ky9eprVWzlI'\n-        self.DL.params['writeautomaticsub'] = True\n-        self.DL.params['subtitleslangs'] = ['it']\n-        subtitles = self.getSubtitles()\n-        self.assertTrue(subtitles['it'] is not None)\n+        # This video has a subtitles track, which can be translated (#4555)\n+        self._test_automatic_captions('Ky9eprVWzlI', 'it')\n \n     def test_youtube_nosubtitles(self):\n         self.DL.expect_warning('video doesn\\'t have subtitles')\n-        self.url = 'n5BB19UTcdA'\n+        # Available automatic captions for 8YoUxe5ncPo:\n+        # ...\n+        # 8YoUxe5ncPo has no subtitles\n+        self.url = '8YoUxe5ncPo'\n         self.DL.params['writesubtitles'] = True\n         self.DL.params['allsubtitles'] = True\n         subtitles = self.getSubtitles()\n@@ -128,6 +165,7 @@ def test_nosubtitles(self):\n         self.assertFalse(subtitles)\n \n \n+@unittest.skip('IE broken')\n class TestTedSubtitles(BaseTestSubtitles):\n     url = 'http://www.ted.com/talks/dan_dennett_on_our_consciousness.html'\n     IE = TEDIE\n@@ -152,18 +190,19 @@ def test_allsubtitles(self):\n         self.DL.params['allsubtitles'] = True\n         subtitles = self.getSubtitles()\n         self.assertEqual(set(subtitles.keys()), set(['de', 'en', 'es', 'fr']))\n-        self.assertEqual(md5(subtitles['en']), '8062383cf4dec168fc40a088aa6d5888')\n-        self.assertEqual(md5(subtitles['fr']), 'b6191146a6c5d3a452244d853fde6dc8')\n+        self.assertEqual(md5(subtitles['en']), '386cbc9320b94e25cb364b97935e5dd1')\n+        self.assertEqual(md5(subtitles['fr']), 'c9b69eef35bc6641c0d4da8a04f9dfac')\n \n     def test_nosubtitles(self):\n         self.DL.expect_warning('video doesn\\'t have subtitles')\n-        self.url = 'http://vimeo.com/56015672'\n+        self.url = 'http://vimeo.com/68093876'\n         self.DL.params['writesubtitles'] = True\n         self.DL.params['allsubtitles'] = True\n         subtitles = self.getSubtitles()\n         self.assertFalse(subtitles)\n \n \n+@unittest.skip('IE broken')\n class TestWallaSubtitles(BaseTestSubtitles):\n     url = 'http://vod.walla.co.il/movie/2705958/the-yes-men'\n     IE = WallaIE\n@@ -185,6 +224,7 @@ def test_nosubtitles(self):\n         self.assertFalse(subtitles)\n \n \n+@unittest.skip('IE broken')\n class TestCeskaTelevizeSubtitles(BaseTestSubtitles):\n     url = 'http://www.ceskatelevize.cz/ivysilani/10600540290-u6-uzasny-svet-techniky'\n     IE = CeskaTelevizeIE\n@@ -206,6 +246,7 @@ def test_nosubtitles(self):\n         self.assertFalse(subtitles)\n \n \n+@unittest.skip('IE broken')\n class TestLyndaSubtitles(BaseTestSubtitles):\n     url = 'http://www.lynda.com/Bootstrap-tutorials/Using-exercise-files/110885/114408-4.html'\n     IE = LyndaIE\n@@ -218,6 +259,7 @@ def test_allsubtitles(self):\n         self.assertEqual(md5(subtitles['en']), '09bbe67222259bed60deaa26997d73a7')\n \n \n+@unittest.skip('IE broken')\n class TestNPOSubtitles(BaseTestSubtitles):\n     url = 'http://www.npo.nl/nos-journaal/28-08-2014/POW_00722860'\n     IE = NPOIE\n@@ -230,6 +272,7 @@ def test_allsubtitles(self):\n         self.assertEqual(md5(subtitles['nl']), 'fc6435027572b63fb4ab143abd5ad3f4')\n \n \n+@unittest.skip('IE broken')\n class TestMTVSubtitles(BaseTestSubtitles):\n     url = 'http://www.cc.com/video-clips/p63lk0/adam-devine-s-house-party-chasing-white-swans'\n     IE = ComedyCentralIE\n@@ -252,9 +295,10 @@ class TestNRKSubtitles(BaseTestSubtitles):\n     def test_allsubtitles(self):\n         self.DL.params['writesubtitles'] = True\n         self.DL.params['allsubtitles'] = True\n+        self.DL.params['format'] = 'best/bestvideo'\n         subtitles = self.getSubtitles()\n-        self.assertEqual(set(subtitles.keys()), set(['no']))\n-        self.assertEqual(md5(subtitles['no']), '544fa917d3197fcbee64634559221cc2')\n+        self.assertEqual(set(subtitles.keys()), set(['nb-ttv']))\n+        self.assertEqual(md5(subtitles['nb-ttv']), '67e06ff02d0deaf975e68f6cb8f6a149')\n \n \n class TestRaiPlaySubtitles(BaseTestSubtitles):\n@@ -277,6 +321,7 @@ def test_subtitles_array_key(self):\n         self.assertEqual(md5(subtitles['it']), '4b3264186fbb103508abe5311cfcb9cd')\n \n \n+@unittest.skip('IE broken - DRM only')\n class TestVikiSubtitles(BaseTestSubtitles):\n     url = 'http://www.viki.com/videos/1060846v-punch-episode-18'\n     IE = VikiIE\n@@ -303,6 +348,7 @@ def test_allsubtitles(self):\n         self.assertEqual(md5(subtitles['en']), '97e7670cbae3c4d26ae8bcc7fdd78d4b')\n \n \n+@unittest.skip('IE broken')\n class TestThePlatformFeedSubtitles(BaseTestSubtitles):\n     url = 'http://feed.theplatform.com/f/7wvmTC/msnbc_video-p-test?form=json&pretty=true&range=-40&byGuid=n_hardball_5biden_140207'\n     IE = ThePlatformFeedIE\n@@ -338,7 +384,7 @@ def test_allsubtitles(self):\n         self.DL.params['allsubtitles'] = True\n         subtitles = self.getSubtitles()\n         self.assertEqual(set(subtitles.keys()), set(['en']))\n-        self.assertEqual(md5(subtitles['en']), 'acaca989e24a9e45a6719c9b3d60815c')\n+        self.assertEqual(md5(subtitles['en']), 'a3cc4c0b5eadd74d9974f1c1f5101045')\n \n     def test_subtitles_in_page(self):\n         self.url = 'http://www.democracynow.org/2015/7/3/this_flag_comes_down_today_bree'\n@@ -346,7 +392,7 @@ def test_subtitles_in_page(self):\n         self.DL.params['allsubtitles'] = True\n         subtitles = self.getSubtitles()\n         self.assertEqual(set(subtitles.keys()), set(['en']))\n-        self.assertEqual(md5(subtitles['en']), 'acaca989e24a9e45a6719c9b3d60815c')\n+        self.assertEqual(md5(subtitles['en']), 'a3cc4c0b5eadd74d9974f1c1f5101045')\n \n \n if __name__ == '__main__':\ndiff --git a/test/test_swfinterp.py b/test/test_swfinterp.py\nindex 9f18055e629..7c282ee0087 100644\n--- a/test/test_swfinterp.py\n+++ b/test/test_swfinterp.py\n@@ -5,16 +5,18 @@\n import os\n import sys\n import unittest\n-sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n \n+dirn = os.path.dirname\n+\n+sys.path.insert(0, dirn(dirn(os.path.abspath(__file__))))\n \n import errno\n-import io\n import json\n import re\n import subprocess\n \n from youtube_dl.swfinterp import SWFInterpreter\n+from youtube_dl.compat import compat_open as open\n \n \n TEST_DIR = os.path.join(\n@@ -43,7 +45,7 @@ def test_func(self):\n                     '-static-link-runtime-shared-libraries', as_file])\n             except OSError as ose:\n                 if ose.errno == errno.ENOENT:\n-                    print('mxmlc not found! Skipping test.')\n+                    self.skipTest('mxmlc not found!')\n                     return\n                 raise\n \n@@ -51,7 +53,7 @@ def test_func(self):\n             swf_content = swf_f.read()\n         swfi = SWFInterpreter(swf_content)\n \n-        with io.open(as_file, 'r', encoding='utf-8') as as_f:\n+        with open(as_file, 'r', encoding='utf-8') as as_f:\n             as_content = as_f.read()\n \n         def _find_spec(key):\ndiff --git a/test/test_traversal.py b/test/test_traversal.py\nnew file mode 100644\nindex 00000000000..00a428edb7b\n--- /dev/null\n+++ b/test/test_traversal.py\n@@ -0,0 +1,509 @@\n+#!/usr/bin/env python\n+# coding: utf-8\n+from __future__ import unicode_literals\n+\n+# Allow direct execution\n+import os\n+import sys\n+import unittest\n+sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n+\n+\n+import re\n+\n+from youtube_dl.traversal import (\n+    dict_get,\n+    get_first,\n+    T,\n+    traverse_obj,\n+)\n+from youtube_dl.compat import (\n+    compat_etree_fromstring,\n+    compat_http_cookies,\n+    compat_str,\n+)\n+from youtube_dl.utils import (\n+    int_or_none,\n+    str_or_none,\n+)\n+\n+_TEST_DATA = {\n+    100: 100,\n+    1.2: 1.2,\n+    'str': 'str',\n+    'None': None,\n+    '...': Ellipsis,\n+    'urls': [\n+        {'index': 0, 'url': 'https://www.example.com/0'},\n+        {'index': 1, 'url': 'https://www.example.com/1'},\n+    ],\n+    'data': (\n+        {'index': 2},\n+        {'index': 3},\n+    ),\n+    'dict': {},\n+}\n+\n+\n+if sys.version_info < (3, 0):\n+    class _TestCase(unittest.TestCase):\n+\n+        def assertCountEqual(self, *args, **kwargs):\n+            return self.assertItemsEqual(*args, **kwargs)\n+else:\n+    _TestCase = unittest.TestCase\n+\n+\n+class TestTraversal(_TestCase):\n+    def assertMaybeCountEqual(self, *args, **kwargs):\n+        if sys.version_info < (3, 7):\n+            # random dict order\n+            return self.assertCountEqual(*args, **kwargs)\n+        else:\n+            return self.assertEqual(*args, **kwargs)\n+\n+    def test_traverse_obj(self):\n+        # instant compat\n+        str = compat_str\n+\n+        # define a pukka Iterable\n+        def iter_range(stop):\n+            for from_ in range(stop):\n+                yield from_\n+\n+        # Test base functionality\n+        self.assertEqual(traverse_obj(_TEST_DATA, ('str',)), 'str',\n+                         msg='allow tuple path')\n+        self.assertEqual(traverse_obj(_TEST_DATA, ['str']), 'str',\n+                         msg='allow list path')\n+        self.assertEqual(traverse_obj(_TEST_DATA, (value for value in (\"str\",))), 'str',\n+                         msg='allow iterable path')\n+        self.assertEqual(traverse_obj(_TEST_DATA, 'str'), 'str',\n+                         msg='single items should be treated as a path')\n+        self.assertEqual(traverse_obj(_TEST_DATA, None), _TEST_DATA)\n+        self.assertEqual(traverse_obj(_TEST_DATA, 100), 100)\n+        self.assertEqual(traverse_obj(_TEST_DATA, 1.2), 1.2)\n+\n+        # Test Ellipsis behavior\n+        self.assertCountEqual(traverse_obj(_TEST_DATA, Ellipsis),\n+                              (item for item in _TEST_DATA.values() if item not in (None, {})),\n+                              msg='`...` should give all non-discarded values')\n+        self.assertCountEqual(traverse_obj(_TEST_DATA, ('urls', 0, Ellipsis)), _TEST_DATA['urls'][0].values(),\n+                              msg='`...` selection for dicts should select all values')\n+        self.assertEqual(traverse_obj(_TEST_DATA, (Ellipsis, Ellipsis, 'url')),\n+                         ['https://www.example.com/0', 'https://www.example.com/1'],\n+                         msg='nested `...` queries should work')\n+        self.assertCountEqual(traverse_obj(_TEST_DATA, (Ellipsis, Ellipsis, 'index')), iter_range(4),\n+                              msg='`...` query result should be flattened')\n+        self.assertEqual(traverse_obj(iter(range(4)), Ellipsis), list(range(4)),\n+                         msg='`...` should accept iterables')\n+\n+        # Test function as key\n+        self.assertEqual(traverse_obj(_TEST_DATA, lambda x, y: x == 'urls' and isinstance(y, list)),\n+                         [_TEST_DATA['urls']],\n+                         msg='function as query key should perform a filter based on (key, value)')\n+        self.assertCountEqual(traverse_obj(_TEST_DATA, lambda _, x: isinstance(x[0], str)), set(('str',)),\n+                              msg='exceptions in the query function should be caught')\n+        self.assertEqual(traverse_obj(iter(range(4)), lambda _, x: x % 2 == 0), [0, 2],\n+                         msg='function key should accept iterables')\n+        if __debug__:\n+            with self.assertRaises(Exception, msg='Wrong function signature should raise in debug'):\n+                traverse_obj(_TEST_DATA, lambda a: Ellipsis)\n+            with self.assertRaises(Exception, msg='Wrong function signature should raise in debug'):\n+                traverse_obj(_TEST_DATA, lambda a, b, c: Ellipsis)\n+\n+        # Test set as key (transformation/type, like `expected_type`)\n+        self.assertEqual(traverse_obj(_TEST_DATA, (Ellipsis, T(str.upper), )), ['STR'],\n+                         msg='Function in set should be a transformation')\n+        self.assertEqual(traverse_obj(_TEST_DATA, ('fail', T(lambda _: 'const'))), 'const',\n+                         msg='Function in set should always be called')\n+        self.assertEqual(traverse_obj(_TEST_DATA, (Ellipsis, T(str))), ['str'],\n+                         msg='Type in set should be a type filter')\n+        self.assertMaybeCountEqual(traverse_obj(_TEST_DATA, (Ellipsis, T(str, int))), [100, 'str'],\n+                                   msg='Multiple types in set should be a type filter')\n+        self.assertEqual(traverse_obj(_TEST_DATA, T(dict)), _TEST_DATA,\n+                         msg='A single set should be wrapped into a path')\n+        self.assertEqual(traverse_obj(_TEST_DATA, (Ellipsis, T(str.upper))), ['STR'],\n+                         msg='Transformation function should not raise')\n+        self.assertMaybeCountEqual(traverse_obj(_TEST_DATA, (Ellipsis, T(str_or_none))),\n+                                   [item for item in map(str_or_none, _TEST_DATA.values()) if item is not None],\n+                                   msg='Function in set should be a transformation')\n+        if __debug__:\n+            with self.assertRaises(Exception, msg='Sets with length != 1 should raise in debug'):\n+                traverse_obj(_TEST_DATA, set())\n+            with self.assertRaises(Exception, msg='Sets with length != 1 should raise in debug'):\n+                traverse_obj(_TEST_DATA, set((str.upper, str)))\n+\n+        # Test `slice` as a key\n+        _SLICE_DATA = [0, 1, 2, 3, 4]\n+        self.assertEqual(traverse_obj(_TEST_DATA, ('dict', slice(1))), None,\n+                         msg='slice on a dictionary should not throw')\n+        self.assertEqual(traverse_obj(_SLICE_DATA, slice(1)), _SLICE_DATA[:1],\n+                         msg='slice key should apply slice to sequence')\n+        self.assertEqual(traverse_obj(_SLICE_DATA, slice(1, 2)), _SLICE_DATA[1:2],\n+                         msg='slice key should apply slice to sequence')\n+        self.assertEqual(traverse_obj(_SLICE_DATA, slice(1, 4, 2)), _SLICE_DATA[1:4:2],\n+                         msg='slice key should apply slice to sequence')\n+\n+        # Test alternative paths\n+        self.assertEqual(traverse_obj(_TEST_DATA, 'fail', 'str'), 'str',\n+                         msg='multiple `paths` should be treated as alternative paths')\n+        self.assertEqual(traverse_obj(_TEST_DATA, 'str', 100), 'str',\n+                         msg='alternatives should exit early')\n+        self.assertEqual(traverse_obj(_TEST_DATA, 'fail', 'fail'), None,\n+                         msg='alternatives should return `default` if exhausted')\n+        self.assertEqual(traverse_obj(_TEST_DATA, (Ellipsis, 'fail'), 100), 100,\n+                         msg='alternatives should track their own branching return')\n+        self.assertEqual(traverse_obj(_TEST_DATA, ('dict', Ellipsis), ('data', Ellipsis)), list(_TEST_DATA['data']),\n+                         msg='alternatives on empty objects should search further')\n+\n+        # Test branch and path nesting\n+        self.assertEqual(traverse_obj(_TEST_DATA, ('urls', (3, 0), 'url')), ['https://www.example.com/0'],\n+                         msg='tuple as key should be treated as branches')\n+        self.assertEqual(traverse_obj(_TEST_DATA, ('urls', [3, 0], 'url')), ['https://www.example.com/0'],\n+                         msg='list as key should be treated as branches')\n+        self.assertEqual(traverse_obj(_TEST_DATA, ('urls', ((1, 'fail'), (0, 'url')))), ['https://www.example.com/0'],\n+                         msg='double nesting in path should be treated as paths')\n+        self.assertEqual(traverse_obj(['0', [1, 2]], [(0, 1), 0]), [1],\n+                         msg='do not fail early on branching')\n+        self.assertCountEqual(traverse_obj(_TEST_DATA, ('urls', ((1, ('fail', 'url')), (0, 'url')))),\n+                              ['https://www.example.com/0', 'https://www.example.com/1'],\n+                              msg='triple nesting in path should be treated as branches')\n+        self.assertEqual(traverse_obj(_TEST_DATA, ('urls', ('fail', (Ellipsis, 'url')))),\n+                         ['https://www.example.com/0', 'https://www.example.com/1'],\n+                         msg='ellipsis as branch path start gets flattened')\n+\n+        # Test dictionary as key\n+        self.assertEqual(traverse_obj(_TEST_DATA, {0: 100, 1: 1.2}), {0: 100, 1: 1.2},\n+                         msg='dict key should result in a dict with the same keys')\n+        self.assertEqual(traverse_obj(_TEST_DATA, {0: ('urls', 0, 'url')}),\n+                         {0: 'https://www.example.com/0'},\n+                         msg='dict key should allow paths')\n+        self.assertEqual(traverse_obj(_TEST_DATA, {0: ('urls', (3, 0), 'url')}),\n+                         {0: ['https://www.example.com/0']},\n+                         msg='tuple in dict path should be treated as branches')\n+        self.assertEqual(traverse_obj(_TEST_DATA, {0: ('urls', ((1, 'fail'), (0, 'url')))}),\n+                         {0: ['https://www.example.com/0']},\n+                         msg='double nesting in dict path should be treated as paths')\n+        self.assertEqual(traverse_obj(_TEST_DATA, {0: ('urls', ((1, ('fail', 'url')), (0, 'url')))}),\n+                         {0: ['https://www.example.com/1', 'https://www.example.com/0']},\n+                         msg='triple nesting in dict path should be treated as branches')\n+        self.assertEqual(traverse_obj(_TEST_DATA, {0: 'fail'}), {},\n+                         msg='remove `None` values when top level dict key fails')\n+        self.assertEqual(traverse_obj(_TEST_DATA, {0: 'fail'}, default=Ellipsis), {0: Ellipsis},\n+                         msg='use `default` if key fails and `default`')\n+        self.assertEqual(traverse_obj(_TEST_DATA, {0: 'dict'}), {},\n+                         msg='remove empty values when dict key')\n+        self.assertEqual(traverse_obj(_TEST_DATA, {0: 'dict'}, default=Ellipsis), {0: Ellipsis},\n+                         msg='use `default` when dict key and a default')\n+        self.assertEqual(traverse_obj(_TEST_DATA, {0: {0: 'fail'}}), {},\n+                         msg='remove empty values when nested dict key fails')\n+        self.assertEqual(traverse_obj(None, {0: 'fail'}), {},\n+                         msg='default to dict if pruned')\n+        self.assertEqual(traverse_obj(None, {0: 'fail'}, default=Ellipsis), {0: Ellipsis},\n+                         msg='default to dict if pruned and default is given')\n+        self.assertEqual(traverse_obj(_TEST_DATA, {0: {0: 'fail'}}, default=Ellipsis), {0: {0: Ellipsis}},\n+                         msg='use nested `default` when nested dict key fails and `default`')\n+        self.assertEqual(traverse_obj(_TEST_DATA, {0: ('dict', Ellipsis)}), {},\n+                         msg='remove key if branch in dict key not successful')\n+\n+        # Testing default parameter behavior\n+        _DEFAULT_DATA = {'None': None, 'int': 0, 'list': []}\n+        self.assertEqual(traverse_obj(_DEFAULT_DATA, 'fail'), None,\n+                         msg='default value should be `None`')\n+        self.assertEqual(traverse_obj(_DEFAULT_DATA, 'fail', 'fail', default=Ellipsis), Ellipsis,\n+                         msg='chained fails should result in default')\n+        self.assertEqual(traverse_obj(_DEFAULT_DATA, 'None', 'int'), 0,\n+                         msg='should not short cirquit on `None`')\n+        self.assertEqual(traverse_obj(_DEFAULT_DATA, 'fail', default=1), 1,\n+                         msg='invalid dict key should result in `default`')\n+        self.assertEqual(traverse_obj(_DEFAULT_DATA, 'None', default=1), 1,\n+                         msg='`None` is a deliberate sentinel and should become `default`')\n+        self.assertEqual(traverse_obj(_DEFAULT_DATA, ('list', 10)), None,\n+                         msg='`IndexError` should result in `default`')\n+        self.assertEqual(traverse_obj(_DEFAULT_DATA, (Ellipsis, 'fail'), default=1), 1,\n+                         msg='if branched but not successful return `default` if defined, not `[]`')\n+        self.assertEqual(traverse_obj(_DEFAULT_DATA, (Ellipsis, 'fail'), default=None), None,\n+                         msg='if branched but not successful return `default` even if `default` is `None`')\n+        self.assertEqual(traverse_obj(_DEFAULT_DATA, (Ellipsis, 'fail')), [],\n+                         msg='if branched but not successful return `[]`, not `default`')\n+        self.assertEqual(traverse_obj(_DEFAULT_DATA, ('list', Ellipsis)), [],\n+                         msg='if branched but object is empty return `[]`, not `default`')\n+        self.assertEqual(traverse_obj(None, Ellipsis), [],\n+                         msg='if branched but object is `None` return `[]`, not `default`')\n+        self.assertEqual(traverse_obj({0: None}, (0, Ellipsis)), [],\n+                         msg='if branched but state is `None` return `[]`, not `default`')\n+\n+        branching_paths = [\n+            ('fail', Ellipsis),\n+            (Ellipsis, 'fail'),\n+            100 * ('fail',) + (Ellipsis,),\n+            (Ellipsis,) + 100 * ('fail',),\n+        ]\n+        for branching_path in branching_paths:\n+            self.assertEqual(traverse_obj({}, branching_path), [],\n+                             msg='if branched but state is `None`, return `[]` (not `default`)')\n+            self.assertEqual(traverse_obj({}, 'fail', branching_path), [],\n+                             msg='if branching in last alternative and previous did not match, return `[]` (not `default`)')\n+            self.assertEqual(traverse_obj({0: 'x'}, 0, branching_path), 'x',\n+                             msg='if branching in last alternative and previous did match, return single value')\n+            self.assertEqual(traverse_obj({0: 'x'}, branching_path, 0), 'x',\n+                             msg='if branching in first alternative and non-branching path does match, return single value')\n+            self.assertEqual(traverse_obj({}, branching_path, 'fail'), None,\n+                             msg='if branching in first alternative and non-branching path does not match, return `default`')\n+\n+        # Testing expected_type behavior\n+        _EXPECTED_TYPE_DATA = {'str': 'str', 'int': 0}\n+        self.assertEqual(traverse_obj(_EXPECTED_TYPE_DATA, 'str', expected_type=str),\n+                         'str', msg='accept matching `expected_type` type')\n+        self.assertEqual(traverse_obj(_EXPECTED_TYPE_DATA, 'str', expected_type=int),\n+                         None, msg='reject non-matching `expected_type` type')\n+        self.assertEqual(traverse_obj(_EXPECTED_TYPE_DATA, 'int', expected_type=lambda x: str(x)),\n+                         '0', msg='transform type using type function')\n+        self.assertEqual(traverse_obj(_EXPECTED_TYPE_DATA, 'str', expected_type=lambda _: 1 / 0),\n+                         None, msg='wrap expected_type function in try_call')\n+        self.assertEqual(traverse_obj(_EXPECTED_TYPE_DATA, Ellipsis, expected_type=str),\n+                         ['str'], msg='eliminate items that expected_type fails on')\n+        self.assertEqual(traverse_obj(_TEST_DATA, {0: 100, 1: 1.2}, expected_type=int),\n+                         {0: 100}, msg='type as expected_type should filter dict values')\n+        self.assertEqual(traverse_obj(_TEST_DATA, {0: 100, 1: 1.2, 2: 'None'}, expected_type=str_or_none),\n+                         {0: '100', 1: '1.2'}, msg='function as expected_type should transform dict values')\n+        self.assertEqual(traverse_obj(_TEST_DATA, ({0: 1.2}, 0, set((int_or_none,))), expected_type=int),\n+                         1, msg='expected_type should not filter non-final dict values')\n+        self.assertEqual(traverse_obj(_TEST_DATA, {0: {0: 100, 1: 'str'}}, expected_type=int),\n+                         {0: {0: 100}}, msg='expected_type should transform deep dict values')\n+        self.assertEqual(traverse_obj(_TEST_DATA, [({0: '...'}, {0: '...'})], expected_type=type(Ellipsis)),\n+                         [{0: Ellipsis}, {0: Ellipsis}], msg='expected_type should transform branched dict values')\n+        self.assertEqual(traverse_obj({1: {3: 4}}, [(1, 2), 3], expected_type=int),\n+                         [4], msg='expected_type regression for type matching in tuple branching')\n+        self.assertEqual(traverse_obj(_TEST_DATA, ['data', Ellipsis], expected_type=int),\n+                         [], msg='expected_type regression for type matching in dict result')\n+\n+        # Test get_all behavior\n+        _GET_ALL_DATA = {'key': [0, 1, 2]}\n+        self.assertEqual(traverse_obj(_GET_ALL_DATA, ('key', Ellipsis), get_all=False), 0,\n+                         msg='if not `get_all`, return only first matching value')\n+        self.assertEqual(traverse_obj(_GET_ALL_DATA, Ellipsis, get_all=False), [0, 1, 2],\n+                         msg='do not overflatten if not `get_all`')\n+\n+        # Test casesense behavior\n+        _CASESENSE_DATA = {\n+            'KeY': 'value0',\n+            0: {\n+                'KeY': 'value1',\n+                0: {'KeY': 'value2'},\n+            },\n+            # FULLWIDTH LATIN CAPITAL LETTER K\n+            '\\uff2bey': 'value3',\n+        }\n+        self.assertEqual(traverse_obj(_CASESENSE_DATA, 'key'), None,\n+                         msg='dict keys should be case sensitive unless `casesense`')\n+        self.assertEqual(traverse_obj(_CASESENSE_DATA, 'keY',\n+                                      casesense=False), 'value0',\n+                         msg='allow non matching key case if `casesense`')\n+        self.assertEqual(traverse_obj(_CASESENSE_DATA, '\\uff4bey',  # FULLWIDTH LATIN SMALL LETTER K\n+                                      casesense=False), 'value3',\n+                         msg='allow non matching Unicode key case if `casesense`')\n+        self.assertEqual(traverse_obj(_CASESENSE_DATA, (0, ('keY',)),\n+                                      casesense=False), ['value1'],\n+                         msg='allow non matching key case in branch if `casesense`')\n+        self.assertEqual(traverse_obj(_CASESENSE_DATA, (0, ((0, 'keY'),)),\n+                                      casesense=False), ['value2'],\n+                         msg='allow non matching key case in branch path if `casesense`')\n+\n+        # Test traverse_string behavior\n+        _TRAVERSE_STRING_DATA = {'str': 'str', 1.2: 1.2}\n+        self.assertEqual(traverse_obj(_TRAVERSE_STRING_DATA, ('str', 0)), None,\n+                         msg='do not traverse into string if not `traverse_string`')\n+        self.assertEqual(traverse_obj(_TRAVERSE_STRING_DATA, ('str', 0),\n+                                      _traverse_string=True), 's',\n+                         msg='traverse into string if `traverse_string`')\n+        self.assertEqual(traverse_obj(_TRAVERSE_STRING_DATA, (1.2, 1),\n+                                      _traverse_string=True), '.',\n+                         msg='traverse into converted data if `traverse_string`')\n+        self.assertEqual(traverse_obj(_TRAVERSE_STRING_DATA, ('str', Ellipsis),\n+                                      _traverse_string=True), 'str',\n+                         msg='`...` should result in string (same value) if `traverse_string`')\n+        self.assertEqual(traverse_obj(_TRAVERSE_STRING_DATA, ('str', slice(0, None, 2)),\n+                                      _traverse_string=True), 'sr',\n+                         msg='`slice` should result in string if `traverse_string`')\n+        self.assertEqual(traverse_obj(_TRAVERSE_STRING_DATA, ('str', lambda i, v: i or v == 's'),\n+                                      _traverse_string=True), 'str',\n+                         msg='function should result in string if `traverse_string`')\n+        self.assertEqual(traverse_obj(_TRAVERSE_STRING_DATA, ('str', (0, 2)),\n+                                      _traverse_string=True), ['s', 'r'],\n+                         msg='branching should result in list if `traverse_string`')\n+        self.assertEqual(traverse_obj({}, (0, Ellipsis), _traverse_string=True), [],\n+                         msg='branching should result in list if `traverse_string`')\n+        self.assertEqual(traverse_obj({}, (0, lambda x, y: True), _traverse_string=True), [],\n+                         msg='branching should result in list if `traverse_string`')\n+        self.assertEqual(traverse_obj({}, (0, slice(1)), _traverse_string=True), [],\n+                         msg='branching should result in list if `traverse_string`')\n+\n+        # Test re.Match as input obj\n+        mobj = re.match(r'^0(12)(?P<group>3)(4)?$', '0123')\n+        self.assertEqual(traverse_obj(mobj, Ellipsis), [x for x in mobj.groups() if x is not None],\n+                         msg='`...` on a `re.Match` should give its `groups()`')\n+        self.assertEqual(traverse_obj(mobj, lambda k, _: k in (0, 2)), ['0123', '3'],\n+                         msg='function on a `re.Match` should give groupno, value starting at 0')\n+        self.assertEqual(traverse_obj(mobj, 'group'), '3',\n+                         msg='str key on a `re.Match` should give group with that name')\n+        self.assertEqual(traverse_obj(mobj, 2), '3',\n+                         msg='int key on a `re.Match` should give group with that name')\n+        self.assertEqual(traverse_obj(mobj, 'gRoUp', casesense=False), '3',\n+                         msg='str key on a `re.Match` should respect casesense')\n+        self.assertEqual(traverse_obj(mobj, 'fail'), None,\n+                         msg='failing str key on a `re.Match` should return `default`')\n+        self.assertEqual(traverse_obj(mobj, 'gRoUpS', casesense=False), None,\n+                         msg='failing str key on a `re.Match` should return `default`')\n+        self.assertEqual(traverse_obj(mobj, 8), None,\n+                         msg='failing int key on a `re.Match` should return `default`')\n+        self.assertEqual(traverse_obj(mobj, lambda k, _: k in (0, 'group')), ['0123', '3'],\n+                         msg='function on a `re.Match` should give group name as well')\n+\n+        # Test xml.etree.ElementTree.Element as input obj\n+        etree = compat_etree_fromstring('''<?xml version=\"1.0\"?>\n+        <data>\n+            <country name=\"Liechtenstein\">\n+                <rank>1</rank>\n+                <year>2008</year>\n+                <gdppc>141100</gdppc>\n+                <neighbor name=\"Austria\" direction=\"E\"/>\n+                <neighbor name=\"Switzerland\" direction=\"W\"/>\n+            </country>\n+            <country name=\"Singapore\">\n+                <rank>4</rank>\n+                <year>2011</year>\n+                <gdppc>59900</gdppc>\n+                <neighbor name=\"Malaysia\" direction=\"N\"/>\n+            </country>\n+            <country name=\"Panama\">\n+                <rank>68</rank>\n+                <year>2011</year>\n+                <gdppc>13600</gdppc>\n+                <neighbor name=\"Costa Rica\" direction=\"W\"/>\n+                <neighbor name=\"Colombia\" direction=\"E\"/>\n+            </country>\n+        </data>''')\n+        self.assertEqual(traverse_obj(etree, ''), etree,\n+                         msg='empty str key should return the element itself')\n+        self.assertEqual(traverse_obj(etree, 'country'), list(etree),\n+                         msg='str key should return all children with that tag name')\n+        self.assertEqual(traverse_obj(etree, Ellipsis), list(etree),\n+                         msg='`...` as key should return all children')\n+        self.assertEqual(traverse_obj(etree, lambda _, x: x[0].text == '4'), [etree[1]],\n+                         msg='function as key should get element as value')\n+        self.assertEqual(traverse_obj(etree, lambda i, _: i == 1), [etree[1]],\n+                         msg='function as key should get index as key')\n+        self.assertEqual(traverse_obj(etree, 0), etree[0],\n+                         msg='int key should return the nth child')\n+        self.assertEqual(traverse_obj(etree, './/neighbor/@name'),\n+                         ['Austria', 'Switzerland', 'Malaysia', 'Costa Rica', 'Colombia'],\n+                         msg='`@<attribute>` at end of path should give that attribute')\n+        self.assertEqual(traverse_obj(etree, '//neighbor/@fail'), [None, None, None, None, None],\n+                         msg='`@<nonexistent>` at end of path should give `None`')\n+        self.assertEqual(traverse_obj(etree, ('//neighbor/@', 2)), {'name': 'Malaysia', 'direction': 'N'},\n+                         msg='`@` should give the full attribute dict')\n+        self.assertEqual(traverse_obj(etree, '//year/text()'), ['2008', '2011', '2011'],\n+                         msg='`text()` at end of path should give the inner text')\n+        self.assertEqual(traverse_obj(etree, '//*[@direction]/@direction'), ['E', 'W', 'N', 'W', 'E'],\n+                         msg='full python xpath features should be supported')\n+        self.assertEqual(traverse_obj(etree, (0, '@name')), 'Liechtenstein',\n+                         msg='special transformations should act on current element')\n+        self.assertEqual(traverse_obj(etree, ('country', 0, Ellipsis, 'text()', T(int_or_none))), [1, 2008, 141100],\n+                         msg='special transformations should act on current element')\n+\n+    def test_traversal_unbranching(self):\n+        self.assertEqual(traverse_obj(_TEST_DATA, [(100, 1.2), all]), [100, 1.2],\n+                         msg='`all` should give all results as list')\n+        self.assertEqual(traverse_obj(_TEST_DATA, [(100, 1.2), any]), 100,\n+                         msg='`any` should give the first result')\n+        self.assertEqual(traverse_obj(_TEST_DATA, [100, all]), [100],\n+                         msg='`all` should give list if non branching')\n+        self.assertEqual(traverse_obj(_TEST_DATA, [100, any]), 100,\n+                         msg='`any` should give single item if non branching')\n+        self.assertEqual(traverse_obj(_TEST_DATA, [('dict', 'None', 100), all]), [100],\n+                         msg='`all` should filter `None` and empty dict')\n+        self.assertEqual(traverse_obj(_TEST_DATA, [('dict', 'None', 100), any]), 100,\n+                         msg='`any` should filter `None` and empty dict')\n+        self.assertEqual(traverse_obj(_TEST_DATA, [{\n+            'all': [('dict', 'None', 100, 1.2), all],\n+            'any': [('dict', 'None', 100, 1.2), any],\n+        }]), {'all': [100, 1.2], 'any': 100},\n+            msg='`all`/`any` should apply to each dict path separately')\n+        self.assertEqual(traverse_obj(_TEST_DATA, [{\n+            'all': [('dict', 'None', 100, 1.2), all],\n+            'any': [('dict', 'None', 100, 1.2), any],\n+        }], get_all=False), {'all': [100, 1.2], 'any': 100},\n+            msg='`all`/`any` should apply to dict regardless of `get_all`')\n+        self.assertIs(traverse_obj(_TEST_DATA, [('dict', 'None', 100, 1.2), all, T(float)]), None,\n+                      msg='`all` should reset branching status')\n+        self.assertIs(traverse_obj(_TEST_DATA, [('dict', 'None', 100, 1.2), any, T(float)]), None,\n+                      msg='`any` should reset branching status')\n+        self.assertEqual(traverse_obj(_TEST_DATA, [('dict', 'None', 100, 1.2), all, Ellipsis, T(float)]), [1.2],\n+                         msg='`all` should allow further branching')\n+        self.assertEqual(traverse_obj(_TEST_DATA, [('dict', 'None', 'urls', 'data'), any, Ellipsis, 'index']), [0, 1],\n+                         msg='`any` should allow further branching')\n+\n+    def test_traversal_morsel(self):\n+        values = {\n+            'expires': 'a',\n+            'path': 'b',\n+            'comment': 'c',\n+            'domain': 'd',\n+            'max-age': 'e',\n+            'secure': 'f',\n+            'httponly': 'g',\n+            'version': 'h',\n+            'samesite': 'i',\n+        }\n+        # SameSite added in Py3.8, breaks .update for 3.5-3.7\n+        if sys.version_info < (3, 8):\n+            del values['samesite']\n+        morsel = compat_http_cookies.Morsel()\n+        morsel.set(str('item_key'), 'item_value', 'coded_value')\n+        morsel.update(values)\n+        values['key'] = str('item_key')\n+        values['value'] = 'item_value'\n+        values = dict((str(k), v) for k, v in values.items())\n+        # make test pass even without ordered dict\n+        value_set = set(values.values())\n+\n+        for key, value in values.items():\n+            self.assertEqual(traverse_obj(morsel, key), value,\n+                             msg='Morsel should provide access to all values')\n+        self.assertEqual(set(traverse_obj(morsel, Ellipsis)), value_set,\n+                         msg='`...` should yield all values')\n+        self.assertEqual(set(traverse_obj(morsel, lambda k, v: True)), value_set,\n+                         msg='function key should yield all values')\n+        self.assertIs(traverse_obj(morsel, [(None,), any]), morsel,\n+                      msg='Morsel should not be implicitly changed to dict on usage')\n+\n+    def test_get_first(self):\n+        self.assertEqual(get_first([{'a': None}, {'a': 'spam'}], 'a'), 'spam')\n+\n+    def test_dict_get(self):\n+        FALSE_VALUES = {\n+            'none': None,\n+            'false': False,\n+            'zero': 0,\n+            'empty_string': '',\n+            'empty_list': [],\n+        }\n+        d = FALSE_VALUES.copy()\n+        d['a'] = 42\n+        self.assertEqual(dict_get(d, 'a'), 42)\n+        self.assertEqual(dict_get(d, 'b'), None)\n+        self.assertEqual(dict_get(d, 'b', 42), 42)\n+        self.assertEqual(dict_get(d, ('a', )), 42)\n+        self.assertEqual(dict_get(d, ('b', 'a', )), 42)\n+        self.assertEqual(dict_get(d, ('b', 'c', 'a', 'd', )), 42)\n+        self.assertEqual(dict_get(d, ('b', 'c', )), None)\n+        self.assertEqual(dict_get(d, ('b', 'c', ), 42), 42)\n+        for key, false_value in FALSE_VALUES.items():\n+            self.assertEqual(dict_get(d, ('b', 'c', key, )), None)\n+            self.assertEqual(dict_get(d, ('b', 'c', key, ), skip_false_values=False), false_value)\n+\n+\n+if __name__ == '__main__':\n+    unittest.main()\ndiff --git a/test/test_unicode_literals.py b/test/test_unicode_literals.py\nindex 6c1b7ec915c..0c83f2a0ce9 100644\n--- a/test/test_unicode_literals.py\n+++ b/test/test_unicode_literals.py\n@@ -2,19 +2,21 @@\n \n # Allow direct execution\n import os\n+import re\n import sys\n import unittest\n-sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n \n-import io\n-import re\n+dirn = os.path.dirname\n+\n+rootDir = dirn(dirn(os.path.abspath(__file__)))\n \n-rootDir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n+sys.path.insert(0, rootDir)\n \n IGNORED_FILES = [\n     'setup.py',  # http://bugs.python.org/issue13943\n     'conf.py',\n     'buildserver.py',\n+    'get-pip.py',\n ]\n \n IGNORED_DIRS = [\n@@ -23,6 +25,7 @@\n ]\n \n from test.helper import assertRegexpMatches\n+from youtube_dl.compat import compat_open as open\n \n \n class TestUnicodeLiterals(unittest.TestCase):\n@@ -40,7 +43,7 @@ def test_all_files(self):\n                     continue\n \n                 fn = os.path.join(dirpath, basename)\n-                with io.open(fn, encoding='utf-8') as inf:\n+                with open(fn, encoding='utf-8') as inf:\n                     code = inf.read()\n \n                 if \"'\" not in code and '\"' not in code:\ndiff --git a/test/test_utils.py b/test/test_utils.py\nindex 259c4763e1e..2947cce7eb3 100644\n--- a/test/test_utils.py\n+++ b/test/test_utils.py\n@@ -12,13 +12,16 @@\n \n # Various small unit tests\n import io\n+import itertools\n import json\n+import types\n import xml.etree.ElementTree\n \n from youtube_dl.utils import (\n+    _UnsafeExtensionError,\n     age_restricted,\n     args_to_str,\n-    encode_base_n,\n+    base_url,\n     caesar,\n     clean_html,\n     clean_podcast_url,\n@@ -26,11 +29,12 @@\n     DateRange,\n     detect_exe_version,\n     determine_ext,\n-    dict_get,\n+    encode_base_n,\n     encode_compat_str,\n     encodeFilename,\n     escape_rfc3986,\n     escape_url,\n+    expand_path,\n     extract_attributes,\n     ExtractorError,\n     find_xpath_attr,\n@@ -44,8 +48,11 @@\n     int_or_none,\n     intlist_to_bytes,\n     is_html,\n+    join_nonempty,\n     js_to_json,\n+    LazyList,\n     limit_length,\n+    lowercase_escape,\n     merge_dicts,\n     mimetype2ext,\n     month_by_name,\n@@ -54,24 +61,26 @@\n     OnDemandPagedList,\n     orderedSet,\n     parse_age_limit,\n+    parse_bitrate,\n     parse_duration,\n     parse_filesize,\n+    parse_codecs,\n     parse_count,\n     parse_iso8601,\n     parse_resolution,\n-    parse_bitrate,\n+    parse_qs,\n     pkcs1pad,\n-    read_batch_urls,\n-    sanitize_filename,\n-    sanitize_path,\n-    sanitize_url,\n-    expand_path,\n     prepend_extension,\n-    replace_extension,\n+    read_batch_urls,\n     remove_start,\n     remove_end,\n     remove_quotes,\n+    replace_extension,\n     rot47,\n+    sanitize_filename,\n+    sanitize_path,\n+    sanitize_url,\n+    sanitized_Request,\n     shell_quote,\n     smuggle_url,\n     str_to_int,\n@@ -79,19 +88,19 @@\n     strip_or_none,\n     subtitles_filename,\n     timeconvert,\n+    try_call,\n     unescapeHTML,\n     unified_strdate,\n     unified_timestamp,\n     unsmuggle_url,\n     uppercase_escape,\n-    lowercase_escape,\n     url_basename,\n     url_or_none,\n-    base_url,\n     urljoin,\n     urlencode_postdata,\n     urshift,\n     update_url_query,\n+    variadic,\n     version_tuple,\n     xpath_with_ns,\n     xpath_element,\n@@ -104,7 +113,7 @@\n     cli_option,\n     cli_valueless_option,\n     cli_bool_option,\n-    parse_codecs,\n+    YoutubeDLHandler,\n )\n from youtube_dl.compat import (\n     compat_chr,\n@@ -112,12 +121,13 @@\n     compat_getenv,\n     compat_os_name,\n     compat_setenv,\n+    compat_str,\n     compat_urlparse,\n-    compat_parse_qs,\n )\n \n \n class TestUtil(unittest.TestCase):\n+\n     def test_timeconvert(self):\n         self.assertTrue(timeconvert('') is None)\n         self.assertTrue(timeconvert('bougrg') is None)\n@@ -236,6 +246,19 @@ def test_sanitize_url(self):\n         self.assertEqual(sanitize_url('httpss://foo.bar'), 'https://foo.bar')\n         self.assertEqual(sanitize_url('rmtps://foo.bar'), 'rtmps://foo.bar')\n         self.assertEqual(sanitize_url('https://foo.bar'), 'https://foo.bar')\n+        self.assertEqual(sanitize_url('foo bar'), 'foo bar')\n+\n+    def test_sanitized_Request(self):\n+        self.assertFalse(sanitized_Request('http://foo.bar').has_header('Authorization'))\n+        self.assertFalse(sanitized_Request('http://:foo.bar').has_header('Authorization'))\n+        self.assertEqual(sanitized_Request('http://@foo.bar').get_header('Authorization'),\n+                         'Basic Og==')\n+        self.assertEqual(sanitized_Request('http://:pass@foo.bar').get_header('Authorization'),\n+                         'Basic OnBhc3M=')\n+        self.assertEqual(sanitized_Request('http://user:@foo.bar').get_header('Authorization'),\n+                         'Basic dXNlcjo=')\n+        self.assertEqual(sanitized_Request('http://user:pass@foo.bar').get_header('Authorization'),\n+                         'Basic dXNlcjpwYXNz')\n \n     def test_expand_path(self):\n         def env(var):\n@@ -249,6 +272,27 @@ def env(var):\n             expand_path('~/%s' % env('YOUTUBE_DL_EXPATH_PATH')),\n             '%s/expanded' % compat_getenv('HOME'))\n \n+    _uncommon_extensions = [\n+        ('exe', 'abc.exe.ext'),\n+        ('de', 'abc.de.ext'),\n+        ('../.mp4', None),\n+        ('..\\\\.mp4', None),\n+    ]\n+\n+    def assertUnsafeExtension(self, ext=None):\n+        assert_raises = self.assertRaises(_UnsafeExtensionError)\n+        assert_raises.ext = ext\n+        orig_exit = assert_raises.__exit__\n+\n+        def my_exit(self_, exc_type, exc_val, exc_tb):\n+            did_raise = orig_exit(exc_type, exc_val, exc_tb)\n+            if did_raise and assert_raises.ext is not None:\n+                self.assertEqual(assert_raises.ext, assert_raises.exception.extension, 'Unsafe extension  not as unexpected')\n+            return did_raise\n+\n+        assert_raises.__exit__ = types.MethodType(my_exit, assert_raises)\n+        return assert_raises\n+\n     def test_prepend_extension(self):\n         self.assertEqual(prepend_extension('abc.ext', 'temp'), 'abc.temp.ext')\n         self.assertEqual(prepend_extension('abc.ext', 'temp', 'ext'), 'abc.temp.ext')\n@@ -257,6 +301,19 @@ def test_prepend_extension(self):\n         self.assertEqual(prepend_extension('.abc', 'temp'), '.abc.temp')\n         self.assertEqual(prepend_extension('.abc.ext', 'temp'), '.abc.temp.ext')\n \n+        # Test uncommon extensions\n+        self.assertEqual(prepend_extension('abc.ext', 'bin'), 'abc.bin.ext')\n+        for ext, result in self._uncommon_extensions:\n+            with self.assertUnsafeExtension(ext):\n+                prepend_extension('abc', ext)\n+            if result:\n+                self.assertEqual(prepend_extension('abc.ext', ext, 'ext'), result)\n+            else:\n+                with self.assertUnsafeExtension(ext):\n+                    prepend_extension('abc.ext', ext, 'ext')\n+            with self.assertUnsafeExtension(ext):\n+                prepend_extension('abc.unexpected_ext', ext, 'ext')\n+\n     def test_replace_extension(self):\n         self.assertEqual(replace_extension('abc.ext', 'temp'), 'abc.temp')\n         self.assertEqual(replace_extension('abc.ext', 'temp', 'ext'), 'abc.temp')\n@@ -265,6 +322,16 @@ def test_replace_extension(self):\n         self.assertEqual(replace_extension('.abc', 'temp'), '.abc.temp')\n         self.assertEqual(replace_extension('.abc.ext', 'temp'), '.abc.temp')\n \n+        # Test uncommon extensions\n+        self.assertEqual(replace_extension('abc.ext', 'bin'), 'abc.unknown_video')\n+        for ext, _ in self._uncommon_extensions:\n+            with self.assertUnsafeExtension(ext):\n+                replace_extension('abc', ext)\n+            with self.assertUnsafeExtension(ext):\n+                replace_extension('abc.ext', ext, 'ext')\n+            with self.assertUnsafeExtension(ext):\n+                replace_extension('abc.unexpected_ext', ext, 'ext')\n+\n     def test_subtitles_filename(self):\n         self.assertEqual(subtitles_filename('abc.ext', 'en', 'vtt'), 'abc.en.vtt')\n         self.assertEqual(subtitles_filename('abc.ext', 'en', 'vtt', 'ext'), 'abc.en.vtt')\n@@ -370,6 +437,9 @@ def test_unified_timestamps(self):\n         self.assertEqual(unified_timestamp('Sep 11, 2013 | 5:49 AM'), 1378878540)\n         self.assertEqual(unified_timestamp('December 15, 2017 at 7:49 am'), 1513324140)\n         self.assertEqual(unified_timestamp('2018-03-14T08:32:43.1493874+00:00'), 1521016363)\n+        self.assertEqual(unified_timestamp('December 31 1969 20:00:01 EDT'), 1)\n+        self.assertEqual(unified_timestamp('Wednesday 31 December 1969 18:01:26 MDT'), 86)\n+        self.assertEqual(unified_timestamp('12/31/1969 20:01:18 EDT', False), 78)\n \n     def test_determine_ext(self):\n         self.assertEqual(determine_ext('http://example.com/foo/bar.mp4/?download'), 'mp4')\n@@ -491,11 +561,14 @@ def test_float_or_none(self):\n         self.assertEqual(float_or_none(set()), None)\n \n     def test_int_or_none(self):\n+        self.assertEqual(int_or_none(42), 42)\n         self.assertEqual(int_or_none('42'), 42)\n         self.assertEqual(int_or_none(''), None)\n         self.assertEqual(int_or_none(None), None)\n         self.assertEqual(int_or_none([]), None)\n         self.assertEqual(int_or_none(set()), None)\n+        self.assertEqual(int_or_none('42', base=8), 34)\n+        self.assertRaises(TypeError, int_or_none(42, base=8))\n \n     def test_str_to_int(self):\n         self.assertEqual(str_to_int('123,456'), 123456)\n@@ -662,38 +735,36 @@ def test_urlencode_postdata(self):\n         self.assertTrue(isinstance(data, bytes))\n \n     def test_update_url_query(self):\n-        def query_dict(url):\n-            return compat_parse_qs(compat_urlparse.urlparse(url).query)\n-        self.assertEqual(query_dict(update_url_query(\n+        self.assertEqual(parse_qs(update_url_query(\n             'http://example.com/path', {'quality': ['HD'], 'format': ['mp4']})),\n-            query_dict('http://example.com/path?quality=HD&format=mp4'))\n-        self.assertEqual(query_dict(update_url_query(\n+            parse_qs('http://example.com/path?quality=HD&format=mp4'))\n+        self.assertEqual(parse_qs(update_url_query(\n             'http://example.com/path', {'system': ['LINUX', 'WINDOWS']})),\n-            query_dict('http://example.com/path?system=LINUX&system=WINDOWS'))\n-        self.assertEqual(query_dict(update_url_query(\n+            parse_qs('http://example.com/path?system=LINUX&system=WINDOWS'))\n+        self.assertEqual(parse_qs(update_url_query(\n             'http://example.com/path', {'fields': 'id,formats,subtitles'})),\n-            query_dict('http://example.com/path?fields=id,formats,subtitles'))\n-        self.assertEqual(query_dict(update_url_query(\n+            parse_qs('http://example.com/path?fields=id,formats,subtitles'))\n+        self.assertEqual(parse_qs(update_url_query(\n             'http://example.com/path', {'fields': ('id,formats,subtitles', 'thumbnails')})),\n-            query_dict('http://example.com/path?fields=id,formats,subtitles&fields=thumbnails'))\n-        self.assertEqual(query_dict(update_url_query(\n+            parse_qs('http://example.com/path?fields=id,formats,subtitles&fields=thumbnails'))\n+        self.assertEqual(parse_qs(update_url_query(\n             'http://example.com/path?manifest=f4m', {'manifest': []})),\n-            query_dict('http://example.com/path'))\n-        self.assertEqual(query_dict(update_url_query(\n+            parse_qs('http://example.com/path'))\n+        self.assertEqual(parse_qs(update_url_query(\n             'http://example.com/path?system=LINUX&system=WINDOWS', {'system': 'LINUX'})),\n-            query_dict('http://example.com/path?system=LINUX'))\n-        self.assertEqual(query_dict(update_url_query(\n+            parse_qs('http://example.com/path?system=LINUX'))\n+        self.assertEqual(parse_qs(update_url_query(\n             'http://example.com/path', {'fields': b'id,formats,subtitles'})),\n-            query_dict('http://example.com/path?fields=id,formats,subtitles'))\n-        self.assertEqual(query_dict(update_url_query(\n+            parse_qs('http://example.com/path?fields=id,formats,subtitles'))\n+        self.assertEqual(parse_qs(update_url_query(\n             'http://example.com/path', {'width': 1080, 'height': 720})),\n-            query_dict('http://example.com/path?width=1080&height=720'))\n-        self.assertEqual(query_dict(update_url_query(\n+            parse_qs('http://example.com/path?width=1080&height=720'))\n+        self.assertEqual(parse_qs(update_url_query(\n             'http://example.com/path', {'bitrate': 5020.43})),\n-            query_dict('http://example.com/path?bitrate=5020.43'))\n-        self.assertEqual(query_dict(update_url_query(\n+            parse_qs('http://example.com/path?bitrate=5020.43'))\n+        self.assertEqual(parse_qs(update_url_query(\n             'http://example.com/path', {'test': '\u7b2c\u4e8c\u884c\u0442\u0435\u0441\u0442'})),\n-            query_dict('http://example.com/path?test=%E7%AC%AC%E4%BA%8C%E8%A1%8C%D1%82%D0%B5%D1%81%D1%82'))\n+            parse_qs('http://example.com/path?test=%E7%AC%AC%E4%BA%8C%E8%A1%8C%D1%82%D0%B5%D1%81%D1%82'))\n \n     def test_multipart_encode(self):\n         self.assertEqual(\n@@ -705,28 +776,6 @@ def test_multipart_encode(self):\n         self.assertRaises(\n             ValueError, multipart_encode, {b'field': b'value'}, boundary='value')\n \n-    def test_dict_get(self):\n-        FALSE_VALUES = {\n-            'none': None,\n-            'false': False,\n-            'zero': 0,\n-            'empty_string': '',\n-            'empty_list': [],\n-        }\n-        d = FALSE_VALUES.copy()\n-        d['a'] = 42\n-        self.assertEqual(dict_get(d, 'a'), 42)\n-        self.assertEqual(dict_get(d, 'b'), None)\n-        self.assertEqual(dict_get(d, 'b', 42), 42)\n-        self.assertEqual(dict_get(d, ('a', )), 42)\n-        self.assertEqual(dict_get(d, ('b', 'a', )), 42)\n-        self.assertEqual(dict_get(d, ('b', 'c', 'a', 'd', )), 42)\n-        self.assertEqual(dict_get(d, ('b', 'c', )), None)\n-        self.assertEqual(dict_get(d, ('b', 'c', ), 42), 42)\n-        for key, false_value in FALSE_VALUES.items():\n-            self.assertEqual(dict_get(d, ('b', 'c', key, )), None)\n-            self.assertEqual(dict_get(d, ('b', 'c', key, ), skip_false_values=False), false_value)\n-\n     def test_merge_dicts(self):\n         self.assertEqual(merge_dicts({'a': 1}, {'b': 2}), {'a': 1, 'b': 2})\n         self.assertEqual(merge_dicts({'a': 1}, {'a': 2}), {'a': 1})\n@@ -885,6 +934,111 @@ def test_escape_url(self):\n         )\n         self.assertEqual(escape_url('http://vimeo.com/56015672#at=0'), 'http://vimeo.com/56015672#at=0')\n \n+    def test_remove_dot_segments(self):\n+\n+        def remove_dot_segments(p):\n+            q = '' if p.startswith('/') else '/'\n+            p = 'http://example.com' + q + p\n+            p = compat_urlparse.urlsplit(YoutubeDLHandler._fix_path(p)).path\n+            return p[1:] if q else p\n+\n+        self.assertEqual(remove_dot_segments('/a/b/c/./../../g'), '/a/g')\n+        self.assertEqual(remove_dot_segments('mid/content=5/../6'), 'mid/6')\n+        self.assertEqual(remove_dot_segments('/ad/../cd'), '/cd')\n+        self.assertEqual(remove_dot_segments('/ad/../cd/'), '/cd/')\n+        self.assertEqual(remove_dot_segments('/..'), '/')\n+        self.assertEqual(remove_dot_segments('/./'), '/')\n+        self.assertEqual(remove_dot_segments('/./a'), '/a')\n+        self.assertEqual(remove_dot_segments('/abc/./.././d/././e/.././f/./../../ghi'), '/ghi')\n+        self.assertEqual(remove_dot_segments('/'), '/')\n+        self.assertEqual(remove_dot_segments('/t'), '/t')\n+        self.assertEqual(remove_dot_segments('t'), 't')\n+        self.assertEqual(remove_dot_segments(''), '')\n+        self.assertEqual(remove_dot_segments('/../a/b/c'), '/a/b/c')\n+        self.assertEqual(remove_dot_segments('../a'), 'a')\n+        self.assertEqual(remove_dot_segments('./a'), 'a')\n+        self.assertEqual(remove_dot_segments('.'), '')\n+        self.assertEqual(remove_dot_segments('////'), '////')\n+\n+    def test_js_to_json_vars_strings(self):\n+        self.assertDictEqual(\n+            json.loads(js_to_json(\n+                '''{\n+                    'null': a,\n+                    'nullStr': b,\n+                    'true': c,\n+                    'trueStr': d,\n+                    'false': e,\n+                    'falseStr': f,\n+                    'unresolvedVar': g,\n+                }''',\n+                {\n+                    'a': 'null',\n+                    'b': '\"null\"',\n+                    'c': 'true',\n+                    'd': '\"true\"',\n+                    'e': 'false',\n+                    'f': '\"false\"',\n+                    'g': 'var',\n+                }\n+            )),\n+            {\n+                'null': None,\n+                'nullStr': 'null',\n+                'true': True,\n+                'trueStr': 'true',\n+                'false': False,\n+                'falseStr': 'false',\n+                'unresolvedVar': 'var'\n+            }\n+        )\n+\n+        self.assertDictEqual(\n+            json.loads(js_to_json(\n+                '''{\n+                    'int': a,\n+                    'intStr': b,\n+                    'float': c,\n+                    'floatStr': d,\n+                }''',\n+                {\n+                    'a': '123',\n+                    'b': '\"123\"',\n+                    'c': '1.23',\n+                    'd': '\"1.23\"',\n+                }\n+            )),\n+            {\n+                'int': 123,\n+                'intStr': '123',\n+                'float': 1.23,\n+                'floatStr': '1.23',\n+            }\n+        )\n+\n+        self.assertDictEqual(\n+            json.loads(js_to_json(\n+                '''{\n+                    'object': a,\n+                    'objectStr': b,\n+                    'array': c,\n+                    'arrayStr': d,\n+                }''',\n+                {\n+                    'a': '{}',\n+                    'b': '\"{}\"',\n+                    'c': '[]',\n+                    'd': '\"[]\"',\n+                }\n+            )),\n+            {\n+                'object': {},\n+                'objectStr': '{}',\n+                'array': [],\n+                'arrayStr': '[]',\n+            }\n+        )\n+\n     def test_js_to_json_realworld(self):\n         inp = '''{\n             'clip':{'provider':'pseudo'}\n@@ -955,10 +1109,10 @@ def test_js_to_json_edgecases(self):\n             !42: 42\n         }''')\n         self.assertEqual(json.loads(on), {\n-            'a': 0,\n-            'b': 1,\n-            'c': 0,\n-            'd': 42.42,\n+            'a': True,\n+            'b': False,\n+            'c': False,\n+            'd': True,\n             'e': [],\n             'f': \"abc\",\n             'g': \"\",\n@@ -1028,10 +1182,26 @@ def test_js_to_json_edgecases(self):\n         on = js_to_json('{ \"040\": \"040\" }')\n         self.assertEqual(json.loads(on), {'040': '040'})\n \n+        on = js_to_json('[1,//{},\\n2]')\n+        self.assertEqual(json.loads(on), [1, 2])\n+\n+        on = js_to_json(r'\"\\^\\$\\#\"')\n+        self.assertEqual(json.loads(on), R'^$#', msg='Unnecessary escapes should be stripped')\n+\n+        on = js_to_json('\\'\"\\\\\"\"\\'')\n+        self.assertEqual(json.loads(on), '\"\"\"', msg='Unnecessary quote escape should be escaped')\n+\n     def test_js_to_json_malformed(self):\n         self.assertEqual(js_to_json('42a1'), '42\"a1\"')\n         self.assertEqual(js_to_json('42a-1'), '42\"a\"-1')\n \n+    def test_js_to_json_template_literal(self):\n+        self.assertEqual(js_to_json('`Hello ${name}`', {'name': '\"world\"'}), '\"Hello world\"')\n+        self.assertEqual(js_to_json('`${name}${name}`', {'name': '\"X\"'}), '\"XX\"')\n+        self.assertEqual(js_to_json('`${name}${name}`', {'name': '5'}), '\"55\"')\n+        self.assertEqual(js_to_json('`${name}\"${name}\"`', {'name': '5'}), '\"5\\\\\"5\\\\\"\"')\n+        self.assertEqual(js_to_json('`${name}`', {}), '\"name\"')\n+\n     def test_extract_attributes(self):\n         self.assertEqual(extract_attributes('<e x=\"y\">'), {'x': 'y'})\n         self.assertEqual(extract_attributes(\"<e x='y'>\"), {'x': 'y'})\n@@ -1475,6 +1645,84 @@ def test_clean_podcast_url(self):\n         self.assertEqual(clean_podcast_url('https://www.podtrac.com/pts/redirect.mp3/chtbl.com/track/5899E/traffic.megaphone.fm/HSW7835899191.mp3'), 'https://traffic.megaphone.fm/HSW7835899191.mp3')\n         self.assertEqual(clean_podcast_url('https://play.podtrac.com/npr-344098539/edge1.pod.npr.org/anon.npr-podcasts/podcast/npr/waitwait/2020/10/20201003_waitwait_wwdtmpodcast201003-015621a5-f035-4eca-a9a1-7c118d90bc3c.mp3'), 'https://edge1.pod.npr.org/anon.npr-podcasts/podcast/npr/waitwait/2020/10/20201003_waitwait_wwdtmpodcast201003-015621a5-f035-4eca-a9a1-7c118d90bc3c.mp3')\n \n+    def test_LazyList(self):\n+        it = list(range(10))\n+\n+        self.assertEqual(list(LazyList(it)), it)\n+        self.assertEqual(LazyList(it).exhaust(), it)\n+        self.assertEqual(LazyList(it)[5], it[5])\n+\n+        self.assertEqual(LazyList(it)[5:], it[5:])\n+        self.assertEqual(LazyList(it)[:5], it[:5])\n+        self.assertEqual(LazyList(it)[::2], it[::2])\n+        self.assertEqual(LazyList(it)[1::2], it[1::2])\n+        self.assertEqual(LazyList(it)[5::-1], it[5::-1])\n+        self.assertEqual(LazyList(it)[6:2:-2], it[6:2:-2])\n+        self.assertEqual(LazyList(it)[::-1], it[::-1])\n+\n+        self.assertTrue(LazyList(it))\n+        self.assertFalse(LazyList(range(0)))\n+        self.assertEqual(len(LazyList(it)), len(it))\n+        self.assertEqual(repr(LazyList(it)), repr(it))\n+        self.assertEqual(compat_str(LazyList(it)), compat_str(it))\n+\n+        self.assertEqual(list(LazyList(it, reverse=True)), it[::-1])\n+        self.assertEqual(list(reversed(LazyList(it))[::-1]), it)\n+        self.assertEqual(list(reversed(LazyList(it))[1:3:7]), it[::-1][1:3:7])\n+\n+    def test_LazyList_laziness(self):\n+\n+        def test(ll, idx, val, cache):\n+            self.assertEqual(ll[idx], val)\n+            self.assertEqual(ll._cache, list(cache))\n+\n+        ll = LazyList(range(10))\n+        test(ll, 0, 0, range(1))\n+        test(ll, 5, 5, range(6))\n+        test(ll, -3, 7, range(10))\n+\n+        ll = LazyList(range(10), reverse=True)\n+        test(ll, -1, 0, range(1))\n+        test(ll, 3, 6, range(10))\n+\n+        ll = LazyList(itertools.count())\n+        test(ll, 10, 10, range(11))\n+        ll = reversed(ll)\n+        test(ll, -15, 14, range(15))\n+\n+    def test_try_call(self):\n+        def total(*x, **kwargs):\n+            return sum(x) + sum(kwargs.values())\n+\n+        self.assertEqual(try_call(None), None,\n+                         msg='not a fn should give None')\n+        self.assertEqual(try_call(lambda: 1), 1,\n+                         msg='int fn with no expected_type should give int')\n+        self.assertEqual(try_call(lambda: 1, expected_type=int), 1,\n+                         msg='int fn with expected_type int should give int')\n+        self.assertEqual(try_call(lambda: 1, expected_type=dict), None,\n+                         msg='int fn with wrong expected_type should give None')\n+        self.assertEqual(try_call(total, args=(0, 1, 0, ), expected_type=int), 1,\n+                         msg='fn should accept arglist')\n+        self.assertEqual(try_call(total, kwargs={'a': 0, 'b': 1, 'c': 0}, expected_type=int), 1,\n+                         msg='fn should accept kwargs')\n+        self.assertEqual(try_call(lambda: 1, expected_type=dict), None,\n+                         msg='int fn with no expected_type should give None')\n+        self.assertEqual(try_call(lambda x: {}, total, args=(42, ), expected_type=int), 42,\n+                         msg='expect first int result with expected_type int')\n+\n+    def test_variadic(self):\n+        self.assertEqual(variadic(None), (None, ))\n+        self.assertEqual(variadic('spam'), ('spam', ))\n+        self.assertEqual(variadic('spam', allowed_types=dict), 'spam')\n+        self.assertEqual(variadic('spam', allowed_types=[dict]), 'spam')\n+\n+    def test_join_nonempty(self):\n+        self.assertEqual(join_nonempty('a', 'b'), 'a-b')\n+        self.assertEqual(join_nonempty(\n+            'a', 'b', 'c', 'd',\n+            from_dict={'a': 'c', 'c': [], 'b': 'd', 'd': None}), 'c-d')\n+\n \n if __name__ == '__main__':\n     unittest.main()\ndiff --git a/test/test_write_annotations.py b/test/test_write_annotations.py\nindex 41abdfe3b99..68e0a391d7b 100644\n--- a/test/test_write_annotations.py\n+++ b/test/test_write_annotations.py\n@@ -11,12 +11,11 @@\n from test.helper import get_params, try_rm\n \n \n-import io\n-\n import xml.etree.ElementTree\n \n import youtube_dl.YoutubeDL\n import youtube_dl.extractor\n+from youtube_dl.compat import compat_open as open\n \n \n class YoutubeDL(youtube_dl.YoutubeDL):\n@@ -51,7 +50,7 @@ def test_info_json(self):\n         ydl.download([TEST_ID])\n         self.assertTrue(os.path.exists(ANNOTATIONS_FILE))\n         annoxml = None\n-        with io.open(ANNOTATIONS_FILE, 'r', encoding='utf-8') as annof:\n+        with open(ANNOTATIONS_FILE, 'r', encoding='utf-8') as annof:\n             annoxml = xml.etree.ElementTree.parse(annof)\n         self.assertTrue(annoxml is not None, 'Failed to parse annotations XML')\n         root = annoxml.getroot()\ndiff --git a/test/test_youtube_signature.py b/test/test_youtube_signature.py\nindex fc5e9828e2c..56e92fac5df 100644\n--- a/test/test_youtube_signature.py\n+++ b/test/test_youtube_signature.py\n@@ -8,14 +8,18 @@\n import unittest\n sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n \n-import io\n import re\n import string\n \n+from youtube_dl.compat import (\n+    compat_open as open,\n+    compat_str,\n+    compat_urlretrieve,\n+)\n+\n from test.helper import FakeYDL\n from youtube_dl.extractor import YoutubeIE\n from youtube_dl.jsinterp import JSInterpreter\n-from youtube_dl.compat import compat_str, compat_urlretrieve\n \n _SIG_TESTS = [\n     (\n@@ -66,6 +70,10 @@\n ]\n \n _NSIG_TESTS = [\n+    (\n+        'https://www.youtube.com/s/player/7862ca1f/player_ias.vflset/en_US/base.js',\n+        'X_LCxVDjAavgE5t', 'yxJ1dM6iz5ogUg',\n+    ),\n     (\n         'https://www.youtube.com/s/player/9216d1f7/player_ias.vflset/en_US/base.js',\n         'SLp9F5bwjAdhE9F-', 'gWnb9IK2DJ8Q1w',\n@@ -90,12 +98,97 @@\n         'https://www.youtube.com/s/player/e06dea74/player_ias.vflset/en_US/base.js',\n         'AiuodmaDDYw8d3y4bf', 'ankd8eza2T6Qmw',\n     ),\n+    (\n+        'https://www.youtube.com/s/player/5dd88d1d/player-plasma-ias-phone-en_US.vflset/base.js',\n+        'kSxKFLeqzv_ZyHSAt', 'n8gS8oRlHOxPFA',\n+    ),\n+    (\n+        'https://www.youtube.com/s/player/324f67b9/player_ias.vflset/en_US/base.js',\n+        'xdftNy7dh9QGnhW', '22qLGxrmX8F1rA',\n+    ),\n+    (\n+        'https://www.youtube.com/s/player/4c3f79c5/player_ias.vflset/en_US/base.js',\n+        'TDCstCG66tEAO5pR9o', 'dbxNtZ14c-yWyw',\n+    ),\n+    (\n+        'https://www.youtube.com/s/player/c81bbb4a/player_ias.vflset/en_US/base.js',\n+        'gre3EcLurNY2vqp94', 'Z9DfGxWP115WTg',\n+    ),\n+    (\n+        'https://www.youtube.com/s/player/1f7d5369/player_ias.vflset/en_US/base.js',\n+        'batNX7sYqIJdkJ', 'IhOkL_zxbkOZBw',\n+    ),\n+    (\n+        'https://www.youtube.com/s/player/009f1d77/player_ias.vflset/en_US/base.js',\n+        '5dwFHw8aFWQUQtffRq', 'audescmLUzI3jw',\n+    ),\n+    (\n+        'https://www.youtube.com/s/player/dc0c6770/player_ias.vflset/en_US/base.js',\n+        '5EHDMgYLV6HPGk_Mu-kk', 'n9lUJLHbxUI0GQ',\n+    ),\n+    (\n+        'https://www.youtube.com/s/player/c2199353/player_ias.vflset/en_US/base.js',\n+        '5EHDMgYLV6HPGk_Mu-kk', 'AD5rgS85EkrE7',\n+    ),\n+    (\n+        'https://www.youtube.com/s/player/113ca41c/player_ias.vflset/en_US/base.js',\n+        'cgYl-tlYkhjT7A', 'hI7BBr2zUgcmMg',\n+    ),\n+    (\n+        'https://www.youtube.com/s/player/c57c113c/player_ias.vflset/en_US/base.js',\n+        '-Txvy6bT5R6LqgnQNx', 'dcklJCnRUHbgSg',\n+    ),\n+    (\n+        'https://www.youtube.com/s/player/5a3b6271/player_ias.vflset/en_US/base.js',\n+        'B2j7f_UPT4rfje85Lu_e', 'm5DmNymaGQ5RdQ',\n+    ),\n+    (\n+        'https://www.youtube.com/s/player/dac945fd/player_ias.vflset/en_US/base.js',\n+        'o8BkRxXhuYsBCWi6RplPdP', '3Lx32v_hmzTm6A',\n+    ),\n+    (\n+        'https://www.youtube.com/s/player/6f20102c/player_ias.vflset/en_US/base.js',\n+        'lE8DhoDmKqnmJJ', 'pJTTX6XyJP2BYw',\n+    ),\n+    (\n+        'https://www.youtube.com/s/player/cfa9e7cb/player_ias.vflset/en_US/base.js',\n+        'qO0NiMtYQ7TeJnfFG2', 'k9cuJDHNS5O7kQ',\n+    ),\n+    (\n+        'https://www.youtube.com/s/player/b7910ca8/player_ias.vflset/en_US/base.js',\n+        '_hXMCwMt9qE310D', 'LoZMgkkofRMCZQ',\n+    ),\n+    (\n+        'https://www.youtube.com/s/player/590f65a6/player_ias.vflset/en_US/base.js',\n+        '1tm7-g_A9zsI8_Lay_', 'xI4Vem4Put_rOg',\n+    ),\n+    (\n+        'https://www.youtube.com/s/player/b22ef6e7/player_ias.vflset/en_US/base.js',\n+        'b6HcntHGkvBLk_FRf', 'kNPW6A7FyP2l8A',\n+    ),\n+    (\n+        'https://www.youtube.com/s/player/3400486c/player_ias.vflset/en_US/base.js',\n+        'lL46g3XifCKUZn1Xfw', 'z767lhet6V2Skl',\n+    ),\n+    (\n+        'https://www.youtube.com/s/player/5604538d/player_ias.vflset/en_US/base.js',\n+        '7X-he4jjvMx7BCX', 'sViSydX8IHtdWA',\n+    ),\n+    (\n+        'https://www.youtube.com/s/player/20dfca59/player_ias.vflset/en_US/base.js',\n+        '-fLCxedkAk4LUTK2', 'O8kfRq1y1eyHGw',\n+    ),\n+    (\n+        'https://www.youtube.com/s/player/b12cc44b/player_ias.vflset/en_US/base.js',\n+        'keLa5R2U00sR9SQK', 'N1OGyujjEwMnLw',\n+    ),\n ]\n \n \n class TestPlayerInfo(unittest.TestCase):\n     def test_youtube_extract_player_info(self):\n         PLAYER_URLS = (\n+            ('https://www.youtube.com/s/player/4c3f79c5/player_ias.vflset/en_US/base.js', '4c3f79c5'),\n             ('https://www.youtube.com/s/player/64dddad9/player_ias.vflset/en_US/base.js', '64dddad9'),\n             ('https://www.youtube.com/s/player/64dddad9/player_ias.vflset/fr_FR/base.js', '64dddad9'),\n             ('https://www.youtube.com/s/player/64dddad9/player-plasma-ias-phone-en_US.vflset/base.js', '64dddad9'),\n@@ -142,7 +235,7 @@ def test_func(self):\n \n             if not os.path.exists(fn):\n                 compat_urlretrieve(url, fn)\n-            with io.open(fn, encoding='utf-8') as testf:\n+            with open(fn, encoding='utf-8') as testf:\n                 jscode = testf.read()\n             self.assertEqual(sig_func(jscode, sig_input), expected_sig)\n \ndiff --git a/test/testdata/mpd/range_only.mpd b/test/testdata/mpd/range_only.mpd\nnew file mode 100644\nindex 00000000000..e0c2152d1a5\n--- /dev/null\n+++ b/test/testdata/mpd/range_only.mpd\n@@ -0,0 +1,35 @@\n+<?xml version=\"1.0\"?>\n+<!-- MPD file Generated with GPAC version 1.0.1-revrelease at 2021-11-27T20:53:11.690Z -->\n+<MPD xmlns=\"urn:mpeg:dash:schema:mpd:2011\" minBufferTime=\"PT1.500S\" type=\"static\" mediaPresentationDuration=\"PT0H0M30.196S\" maxSegmentDuration=\"PT0H0M10.027S\" profiles=\"urn:mpeg:dash:profile:full:2011\">\n+ <ProgramInformation moreInformationURL=\"http://gpac.io\">\n+  <Title>manifest.mpd generated by GPAC</Title>\n+ </ProgramInformation>\n+\n+ <Period duration=\"PT0H0M30.196S\">\n+  <AdaptationSet segmentAlignment=\"true\" maxWidth=\"768\" maxHeight=\"432\" maxFrameRate=\"30000/1001\" par=\"16:9\" lang=\"und\" startWithSAP=\"1\">\n+   <Representation id=\"1\" mimeType=\"video/mp4\" codecs=\"avc1.4D401E\" width=\"768\" height=\"432\" frameRate=\"30000/1001\" sar=\"1:1\" bandwidth=\"526987\">\n+    <BaseURL>video_dashinit.mp4</BaseURL>\n+    <SegmentList timescale=\"90000\" duration=\"900000\">\n+     <Initialization range=\"0-881\"/>\n+     <SegmentURL mediaRange=\"882-876094\" indexRange=\"882-925\"/>\n+     <SegmentURL mediaRange=\"876095-1466732\" indexRange=\"876095-876138\"/>\n+     <SegmentURL mediaRange=\"1466733-1953615\" indexRange=\"1466733-1466776\"/>\n+     <SegmentURL mediaRange=\"1953616-1994211\" indexRange=\"1953616-1953659\"/>\n+    </SegmentList>\n+   </Representation>\n+  </AdaptationSet>\n+  <AdaptationSet segmentAlignment=\"true\" lang=\"und\" startWithSAP=\"1\">\n+   <Representation id=\"2\" mimeType=\"audio/mp4\" codecs=\"mp4a.40.2\" audioSamplingRate=\"48000\" bandwidth=\"98096\">\n+    <AudioChannelConfiguration schemeIdUri=\"urn:mpeg:dash:23003:3:audio_channel_configuration:2011\" value=\"2\"/>\n+    <BaseURL>audio_dashinit.mp4</BaseURL>\n+    <SegmentList timescale=\"48000\" duration=\"480000\">\n+     <Initialization range=\"0-752\"/>\n+     <SegmentURL mediaRange=\"753-124129\" indexRange=\"753-796\"/>\n+     <SegmentURL mediaRange=\"124130-250544\" indexRange=\"124130-124173\"/>\n+     <SegmentURL mediaRange=\"250545-374929\" indexRange=\"250545-250588\"/>\n+    </SegmentList>\n+   </Representation>\n+  </AdaptationSet>\n+ </Period>\n+</MPD>\n+\ndiff --git a/test/testdata/mpd/subtitles.mpd b/test/testdata/mpd/subtitles.mpd\nnew file mode 100644\nindex 00000000000..6f948adba92\n--- /dev/null\n+++ b/test/testdata/mpd/subtitles.mpd\n@@ -0,0 +1,351 @@\n+<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+<!-- Created with Unified Streaming Platform (version=1.10.18-20255) -->\n+<MPD\n+  xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n+  xmlns=\"urn:mpeg:dash:schema:mpd:2011\"\n+  xsi:schemaLocation=\"urn:mpeg:dash:schema:mpd:2011 http://standards.iso.org/ittf/PubliclyAvailableStandards/MPEG-DASH_schema_files/DASH-MPD.xsd\"\n+  type=\"static\"\n+  mediaPresentationDuration=\"PT14M48S\"\n+  maxSegmentDuration=\"PT1M\"\n+  minBufferTime=\"PT10S\"\n+  profiles=\"urn:mpeg:dash:profile:isoff-live:2011\">\n+  <Period\n+    id=\"1\"\n+    duration=\"PT14M48S\">\n+    <BaseURL>dash/</BaseURL>\n+    <AdaptationSet\n+      id=\"1\"\n+      group=\"1\"\n+      contentType=\"audio\"\n+      segmentAlignment=\"true\"\n+      audioSamplingRate=\"48000\"\n+      mimeType=\"audio/mp4\"\n+      codecs=\"mp4a.40.2\"\n+      startWithSAP=\"1\">\n+      <AudioChannelConfiguration\n+        schemeIdUri=\"urn:mpeg:dash:23003:3:audio_channel_configuration:2011\"\n+        value=\"2\" />\n+      <Role schemeIdUri=\"urn:mpeg:dash:role:2011\" value=\"main\" />\n+      <SegmentTemplate\n+        timescale=\"48000\"\n+        initialization=\"3144-kZT4LWMQw6Rh7Kpd-$RepresentationID$.dash\"\n+        media=\"3144-kZT4LWMQw6Rh7Kpd-$RepresentationID$-$Time$.dash\">\n+        <SegmentTimeline>\n+          <S t=\"0\" d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"3584\" />\n+        </SegmentTimeline>\n+      </SegmentTemplate>\n+      <Representation\n+        id=\"audio=128001\"\n+        bandwidth=\"128001\">\n+      </Representation>\n+    </AdaptationSet>\n+    <AdaptationSet\n+      id=\"2\"\n+      group=\"3\"\n+      contentType=\"text\"\n+      lang=\"en\"\n+      mimeType=\"application/mp4\"\n+      codecs=\"stpp\"\n+      startWithSAP=\"1\">\n+      <Role schemeIdUri=\"urn:mpeg:dash:role:2011\" value=\"subtitle\" />\n+      <SegmentTemplate\n+        timescale=\"1000\"\n+        initialization=\"3144-kZT4LWMQw6Rh7Kpd-$RepresentationID$.dash\"\n+        media=\"3144-kZT4LWMQw6Rh7Kpd-$RepresentationID$-$Time$.dash\">\n+        <SegmentTimeline>\n+          <S t=\"0\" d=\"60000\" r=\"9\" />\n+          <S d=\"24000\" />\n+        </SegmentTimeline>\n+      </SegmentTemplate>\n+      <Representation\n+        id=\"textstream_eng=1000\"\n+        bandwidth=\"1000\">\n+      </Representation>\n+    </AdaptationSet>\n+    <AdaptationSet\n+      id=\"3\"\n+      group=\"2\"\n+      contentType=\"video\"\n+      par=\"960:409\"\n+      minBandwidth=\"100000\"\n+      maxBandwidth=\"4482000\"\n+      maxWidth=\"1689\"\n+      maxHeight=\"720\"\n+      segmentAlignment=\"true\"\n+      mimeType=\"video/mp4\"\n+      codecs=\"avc1.4D401F\"\n+      startWithSAP=\"1\">\n+      <Role schemeIdUri=\"urn:mpeg:dash:role:2011\" value=\"main\" />\n+      <SegmentTemplate\n+        timescale=\"12288\"\n+        initialization=\"3144-kZT4LWMQw6Rh7Kpd-$RepresentationID$.dash\"\n+        media=\"3144-kZT4LWMQw6Rh7Kpd-$RepresentationID$-$Time$.dash\">\n+        <SegmentTimeline>\n+          <S t=\"0\" d=\"24576\" r=\"443\" />\n+        </SegmentTimeline>\n+      </SegmentTemplate>\n+      <Representation\n+        id=\"video=100000\"\n+        bandwidth=\"100000\"\n+        width=\"336\"\n+        height=\"144\"\n+        sar=\"2880:2863\"\n+        scanType=\"progressive\">\n+      </Representation>\n+      <Representation\n+        id=\"video=326000\"\n+        bandwidth=\"326000\"\n+        width=\"562\"\n+        height=\"240\"\n+        sar=\"115200:114929\"\n+        scanType=\"progressive\">\n+      </Representation>\n+      <Representation\n+        id=\"video=698000\"\n+        bandwidth=\"698000\"\n+        width=\"844\"\n+        height=\"360\"\n+        sar=\"86400:86299\"\n+        scanType=\"progressive\">\n+      </Representation>\n+      <Representation\n+        id=\"video=1493000\"\n+        bandwidth=\"1493000\"\n+        width=\"1126\"\n+        height=\"480\"\n+        sar=\"230400:230267\"\n+        scanType=\"progressive\">\n+      </Representation>\n+      <Representation\n+        id=\"video=4482000\"\n+        bandwidth=\"4482000\"\n+        width=\"1688\"\n+        height=\"720\"\n+        sar=\"86400:86299\"\n+        scanType=\"progressive\">\n+      </Representation>\n+    </AdaptationSet>\n+  </Period>\n+</MPD>\ndiff --git a/test/testdata/mpd/url_and_range.mpd b/test/testdata/mpd/url_and_range.mpd\nnew file mode 100644\nindex 00000000000..b8c68aad2e6\n--- /dev/null\n+++ b/test/testdata/mpd/url_and_range.mpd\n@@ -0,0 +1,32 @@\n+<?xml version=\"1.0\" ?>\n+<MPD xmlns=\"urn:mpeg:dash:schema:mpd:2011\" profiles=\"urn:mpeg:dash:profile:isoff-live:2011\" minBufferTime=\"PT10.01S\" mediaPresentationDuration=\"PT30.097S\" type=\"static\">\n+  <!-- Created with Bento4 mp4-dash.py, VERSION=2.0.0-639 -->\n+  <Period>\n+    <!-- Video -->\n+    <AdaptationSet mimeType=\"video/mp4\" segmentAlignment=\"true\" startWithSAP=\"1\" maxWidth=\"768\" maxHeight=\"432\">\n+      <Representation id=\"video-avc1\" codecs=\"avc1.4D401E\" width=\"768\" height=\"432\" scanType=\"progressive\" frameRate=\"30000/1001\" bandwidth=\"699597\">\n+        <SegmentList timescale=\"1000\" duration=\"10010\">\n+          <Initialization sourceURL=\"video-frag.mp4\" range=\"36-746\"/>\n+          <SegmentURL media=\"video-frag.mp4\" mediaRange=\"747-876117\"/>\n+          <SegmentURL media=\"video-frag.mp4\" mediaRange=\"876118-1466913\"/>\n+          <SegmentURL media=\"video-frag.mp4\" mediaRange=\"1466914-1953954\"/>\n+          <SegmentURL media=\"video-frag.mp4\" mediaRange=\"1953955-1994652\"/>\n+        </SegmentList>\n+      </Representation>\n+    </AdaptationSet>\n+    <!-- Audio -->\n+    <AdaptationSet mimeType=\"audio/mp4\" startWithSAP=\"1\" segmentAlignment=\"true\">\n+      <Representation id=\"audio-und-mp4a.40.2\" codecs=\"mp4a.40.2\" bandwidth=\"98808\" audioSamplingRate=\"48000\">\n+        <AudioChannelConfiguration schemeIdUri=\"urn:mpeg:mpegB:cicp:ChannelConfiguration\" value=\"2\"/>\n+        <SegmentList timescale=\"1000\" duration=\"10010\">\n+          <Initialization sourceURL=\"audio-frag.mp4\" range=\"32-623\"/>\n+          <SegmentURL media=\"audio-frag.mp4\" mediaRange=\"624-124199\"/>\n+          <SegmentURL media=\"audio-frag.mp4\" mediaRange=\"124200-250303\"/>\n+          <SegmentURL media=\"audio-frag.mp4\" mediaRange=\"250304-374365\"/>\n+          <SegmentURL media=\"audio-frag.mp4\" mediaRange=\"374366-374836\"/>\n+        </SegmentList>\n+      </Representation>\n+    </AdaptationSet>\n+  </Period>\n+</MPD>\n+\ndiff --git a/youtube_dl/extractor/americastestkitchen.py b/youtube_dl/extractor/americastestkitchen.py\nindex be960c0f93b..08d3604e91b 100644\n--- a/youtube_dl/extractor/americastestkitchen.py\n+++ b/youtube_dl/extractor/americastestkitchen.py\n@@ -15,7 +15,7 @@\n \n \n class AmericasTestKitchenIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?(?:americastestkitchen|cooks(?:country|illustrated))\\.com/(?P<resource_type>episode|videos)/(?P<id>\\d+)'\n+    _VALID_URL = r'https?://(?:www\\.)?(?:americastestkitchen|cooks(?:country|illustrated))\\.com/(?:cooks(?:country|illustrated)/)?(?P<resource_type>episode|videos)/(?P<id>\\d+)'\n     _TESTS = [{\n         'url': 'https://www.americastestkitchen.com/episode/582-weeknight-japanese-suppers',\n         'md5': 'b861c3e365ac38ad319cfd509c30577f',\n@@ -23,15 +23,20 @@ class AmericasTestKitchenIE(InfoExtractor):\n             'id': '5b400b9ee338f922cb06450c',\n             'title': 'Japanese Suppers',\n             'ext': 'mp4',\n+            'display_id': 'weeknight-japanese-suppers',\n             'description': 'md5:64e606bfee910627efc4b5f050de92b3',\n-            'thumbnail': r're:^https?://',\n-            'timestamp': 1523318400,\n-            'upload_date': '20180410',\n-            'release_date': '20180410',\n+            'timestamp': 1523304000,\n+            'upload_date': '20180409',\n+            'release_date': '20180409',\n             'series': \"America's Test Kitchen\",\n+            'season': 'Season 18',\n             'season_number': 18,\n             'episode': 'Japanese Suppers',\n             'episode_number': 15,\n+            'duration': 1376,\n+            'thumbnail': r're:^https?://',\n+            'average_rating': 0,\n+            'view_count': int,\n         },\n         'params': {\n             'skip_download': True,\n@@ -44,15 +49,20 @@ class AmericasTestKitchenIE(InfoExtractor):\n             'id': '5fbe8c61bda2010001c6763b',\n             'title': 'Simple Chicken Dinner',\n             'ext': 'mp4',\n+            'display_id': 'atktv_2103_simple-chicken-dinner_full-episode_web-mp4',\n             'description': 'md5:eb68737cc2fd4c26ca7db30139d109e7',\n-            'thumbnail': r're:^https?://',\n-            'timestamp': 1610755200,\n-            'upload_date': '20210116',\n-            'release_date': '20210116',\n+            'timestamp': 1610737200,\n+            'upload_date': '20210115',\n+            'release_date': '20210115',\n             'series': \"America's Test Kitchen\",\n+            'season': 'Season 21',\n             'season_number': 21,\n             'episode': 'Simple Chicken Dinner',\n             'episode_number': 3,\n+            'duration': 1397,\n+            'thumbnail': r're:^https?://',\n+            'view_count': int,\n+            'average_rating': 0,\n         },\n         'params': {\n             'skip_download': True,\n@@ -60,6 +70,12 @@ class AmericasTestKitchenIE(InfoExtractor):\n     }, {\n         'url': 'https://www.americastestkitchen.com/videos/3420-pan-seared-salmon',\n         'only_matching': True,\n+    }, {\n+        'url': 'https://www.americastestkitchen.com/cookscountry/episode/564-when-only-chocolate-will-do',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.americastestkitchen.com/cooksillustrated/videos/4478-beef-wellington',\n+        'only_matching': True,\n     }, {\n         'url': 'https://www.cookscountry.com/episode/564-when-only-chocolate-will-do',\n         'only_matching': True,\n@@ -94,7 +110,7 @@ def _real_extract(self, url):\n \n \n class AmericasTestKitchenSeasonIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?(?P<show>americastestkitchen|cookscountry)\\.com/episodes/browse/season_(?P<id>\\d+)'\n+    _VALID_URL = r'https?://(?:www\\.)?(?P<show>americastestkitchen|(?P<cooks>cooks(?:country|illustrated)))\\.com(?:(?:/(?P<show2>cooks(?:country|illustrated)))?(?:/?$|(?<!ated)(?<!ated\\.com)/episodes/browse/season_(?P<season>\\d+)))'\n     _TESTS = [{\n         # ATK Season\n         'url': 'https://www.americastestkitchen.com/episodes/browse/season_1',\n@@ -105,48 +121,93 @@ class AmericasTestKitchenSeasonIE(InfoExtractor):\n         'playlist_count': 13,\n     }, {\n         # Cooks Country Season\n-        'url': 'https://www.cookscountry.com/episodes/browse/season_12',\n+        'url': 'https://www.americastestkitchen.com/cookscountry/episodes/browse/season_12',\n         'info_dict': {\n             'id': 'season_12',\n             'title': 'Season 12',\n         },\n         'playlist_count': 13,\n+    }, {\n+        # America's Test Kitchen Series\n+        'url': 'https://www.americastestkitchen.com/',\n+        'info_dict': {\n+            'id': 'americastestkitchen',\n+            'title': 'America\\'s Test Kitchen',\n+        },\n+        'playlist_count': 558,\n+    }, {\n+        # Cooks Country Series\n+        'url': 'https://www.americastestkitchen.com/cookscountry',\n+        'info_dict': {\n+            'id': 'cookscountry',\n+            'title': 'Cook\\'s Country',\n+        },\n+        'playlist_count': 199,\n+    }, {\n+        'url': 'https://www.americastestkitchen.com/cookscountry/',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.cookscountry.com/episodes/browse/season_12',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.cookscountry.com',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.americastestkitchen.com/cooksillustrated/',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://www.cooksillustrated.com',\n+        'only_matching': True,\n     }]\n \n     def _real_extract(self, url):\n-        show_name, season_number = re.match(self._VALID_URL, url).groups()\n-        season_number = int(season_number)\n+        match = re.match(self._VALID_URL, url).groupdict()\n+        show = match.get('show2')\n+        show_path = ('/' + show) if show else ''\n+        show = show or match['show']\n+        season_number = int_or_none(match.get('season'))\n+\n+        slug, title = {\n+            'americastestkitchen': ('atk', 'America\\'s Test Kitchen'),\n+            'cookscountry': ('cco', 'Cook\\'s Country'),\n+            'cooksillustrated': ('cio', 'Cook\\'s Illustrated'),\n+        }[show]\n \n-        slug = 'atk' if show_name == 'americastestkitchen' else 'cco'\n+        facet_filters = [\n+            'search_document_klass:episode',\n+            'search_show_slug:' + slug,\n+        ]\n \n-        season = 'Season %d' % season_number\n+        if season_number:\n+            playlist_id = 'season_%d' % season_number\n+            playlist_title = 'Season %d' % season_number\n+            facet_filters.append('search_season_list:' + playlist_title)\n+        else:\n+            playlist_id = show\n+            playlist_title = title\n \n         season_search = self._download_json(\n             'https://y1fnzxui30-dsn.algolia.net/1/indexes/everest_search_%s_season_desc_production' % slug,\n-            season, headers={\n-                'Origin': 'https://www.%s.com' % show_name,\n+            playlist_id, headers={\n+                'Origin': 'https://www.americastestkitchen.com',\n                 'X-Algolia-API-Key': '8d504d0099ed27c1b73708d22871d805',\n                 'X-Algolia-Application-Id': 'Y1FNZXUI30',\n             }, query={\n-                'facetFilters': json.dumps([\n-                    'search_season_list:' + season,\n-                    'search_document_klass:episode',\n-                    'search_show_slug:' + slug,\n-                ]),\n-                'attributesToRetrieve': 'description,search_%s_episode_number,search_document_date,search_url,title' % slug,\n+                'facetFilters': json.dumps(facet_filters),\n+                'attributesToRetrieve': 'description,search_%s_episode_number,search_document_date,search_url,title,search_atk_episode_season' % slug,\n                 'attributesToHighlight': '',\n                 'hitsPerPage': 1000,\n             })\n \n         def entries():\n             for episode in (season_search.get('hits') or []):\n-                search_url = episode.get('search_url')\n+                search_url = episode.get('search_url')  # always formatted like '/episode/123-title-of-episode'\n                 if not search_url:\n                     continue\n                 yield {\n                     '_type': 'url',\n-                    'url': 'https://www.%s.com%s' % (show_name, search_url),\n-                    'id': try_get(episode, lambda e: e['objectID'].split('_')[-1]),\n+                    'url': 'https://www.americastestkitchen.com%s%s' % (show_path, search_url),\n+                    'id': try_get(episode, lambda e: e['objectID'].rsplit('_', 1)[-1]),\n                     'title': episode.get('title'),\n                     'description': episode.get('description'),\n                     'timestamp': unified_timestamp(episode.get('search_document_date')),\n@@ -156,4 +217,4 @@ def entries():\n                 }\n \n         return self.playlist_result(\n-            entries(), 'season_%d' % season_number, season)\n+            entries(), playlist_id, playlist_title)\n", "problem_statement": "External-downloader \"ffmpeg\" does not understand ffmpeg-location parameter\nYoutubeDownloader does not use `ffmpeg-location`  path for an `external-downloader` argument I think? Full folder value did not work in an external args.\r\n\r\n```\r\nyoutube-dl.exe --verbose ^\r\n --ffmpeg-location \"c:/apps/ffmpeg\" ^\r\n --format \"(bestvideo[height<=1080][ext=mp4])+(bestaudio[asr=48000][ext=webm])\" ^\r\n --external-downloader ffmpeg --external-downloader-args \"-ss 00:00:00.00 -to 00:01:00.00\" ^\r\n \"https://www.youtube.com/watch?v=1JWEb2uKZ28\" ^\r\n --merge-output-format mp4 -o \"wildlife.mp4\"\r\n```\r\n\r\nI had to put ffmpeg folder to PATH then external downloader worked.\r\n`set path=%path%;c:\\apps\\ffmpeg\"`\r\n\r\n**Feature Request** If external download is ffmpeg then try to use `ffmpeg-location` folder.\r\n\r\nps: Or is there a downlod time limit parameter already without using an external ffmpeg trick?\nyoutube_dl.jsinterp.JSInterpreter.Exception: slice must be applied on a list in: 'a.slice(0,0)\n<!--\r\n\r\n######################################################################\r\n  WARNING!\r\n  IGNORING THE FOLLOWING TEMPLATE WILL RESULT IN ISSUE CLOSED AS INCOMPLETE\r\n######################################################################\r\n\r\n-->\r\n\r\n\r\n## Checklist\r\n\r\n<!--\r\nCarefully read and work through this check list in order to prevent the most common mistakes and misuse of youtube-dl:\r\n- First of, make sure you are using the latest version of youtube-dl. Run `youtube-dl --version` and ensure your version is 2021.12.17. If it's not, see https://yt-dl.org/update on how to update. Issues with outdated version will be REJECTED.\r\n- Make sure that all provided video/audio/playlist URLs (if any) are alive and playable in a browser.\r\n- Make sure that all URLs and arguments with special characters are properly quoted or escaped as explained in http://yt-dl.org/escape.\r\n- Search the bugtracker for similar issues: http://yt-dl.org/search-issues. DO NOT post duplicates.\r\n- Read bugs section in FAQ: http://yt-dl.org/reporting\r\n- Finally, put x into all relevant boxes (like this [x])\r\n-->\r\n\r\n- [x] I'm reporting a broken site support issue\r\n- [x] I've verified that I'm running youtube-dl version **2024.08.02**\r\n- [x] I've checked that all provided URLs are alive and playable in a browser\r\n- [x] I've checked that all URLs and arguments with special characters are properly quoted or escaped\r\n- [x] I've searched the bugtracker for similar bug reports including closed ones\r\n- [x] I've read bugs section in FAQ\r\n\r\n\r\n## Verbose log\r\n\r\n<!--\r\nProvide the complete verbose output of youtube-dl that clearly demonstrates the problem.\r\nAdd the `-v` flag to your command line you run youtube-dl with (`youtube-dl -v <your command line>`), copy the WHOLE output and insert it below. It should look similar to this:\r\n [debug] System config: []\r\n [debug] User config: []\r\n [debug] Command-line args: [u'-v', u'http://www.youtube.com/watch?v=BaW_jenozKcj']\r\n [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251\r\n [debug] youtube-dl version 2021.12.17\r\n [debug] Python version 2.7.11 - Windows-2003Server-5.2.3790-SP2\r\n [debug] exe versions: ffmpeg N-75573-g1d0487f, ffprobe N-75573-g1d0487f, rtmpdump 2.4\r\n [debug] Proxy map: {}\r\n <more lines>\r\n-->\r\n\r\n```\r\nLatest version: 2024.08.02, Current version: 2024.08.02\r\nyoutube-dl is up to date (2024.08.02)\r\n[youtube] LG9G4aA28rU: Downloading webpage\r\n[youtube] LG9G4aA28rU: Downloading player b12cc44b\r\nWARNING: [youtube] Falling back to generic n function search\r\nWARNING: [youtube] LG9G4aA28rU: Unable to decode n-parameter: expect download to be blocked or throttled (slice must be applied on a list in: 'a.slice(0,0)'; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output. Traceback (most recent call last):\r\n  File \"/__w/speech2text/speech2text/youtube/./youtube-dl/youtube_dl/extractor/youtube.py\", line 1646, in _decrypt_nsig\r\n    ret = extract_nsig(jsi, func_code)(n)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/speech2text/speech2text/youtube/./youtube-dl/youtube_dl/extractor/youtube.py\", line 1722, in extract_nsig\r\n    ret = func([s])\r\n          ^^^^^^^^^\r\n  File \"/__w/speech2text/speech2text/youtube/./youtube-dl/youtube_dl/jsinterp.py\", line 1122, in resf\r\n    ret, should_abort = self.interpret_statement(code.replace('\\n', ' '), var_stack, allow_recursion - 1)\r\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/speech2text/speech2text/youtube/./youtube-dl/youtube_dl/jsinterp.py\", line 249, in interpret_statement\r\n  File \"/__w/speech2text/speech2text/youtube/./youtube-dl/youtube_dl/extractor/youtube.py\", line 1646, in _decrypt_nsig\r\n    ret = extract_nsig(jsi, func_code)(n)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/speech2text/speech2text/youtube/./youtube-dl/youtube_dl/extractor/youtube.py\", line 1722, in extract_nsig\r\n    ret = func([s])\r\n          ^^^^^^^^^\r\n  File \"/__w/speech2text/speech2text/youtube/./youtube-dl/youtube_dl/jsinterp.py\", line 1122, in resf\r\n    ret, should_abort = self.interpret_statement(code.replace('\\n', ' '), var_stack, allow_recursion - 1)\r\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/speech2text/speech2text/youtube/./youtube-dl/youtube_dl/jsinterp.py\", line 249, in interpret_statement\r\n    ret, should_ret = f(self, stmt, local_vars, allow_recursion, *args, **kwargs)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/speech2text/speech2text/youtube/./youtube-dl/youtube_dl/jsinterp.py\", line 495, in interpret_statement\r\n    ret, should_return = self.interpret_statement(sub_stmt, local_vars, allow_recursion)\r\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/speech2text/speech2text/youtube/./youtube-dl/youtube_dl/jsinterp.py\", line 249, in interpret_statement\r\n    ret, should_ret = f(self, stmt, local_vars, allow_recursion, *args, **kwargs)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/speech2text/speech2text/youtube/./youtube-dl/youtube_dl/jsinterp.py\", line 711, in interpret_statement\r\n    ret, should_abort = self.interpret_statement(sub_expr, local_vars, allow_recursion)\r\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/speech2text/speech2text/youtube/./youtube-dl/youtube_dl/jsinterp.py\", line 249, in interpret_statement\r\n    ret, should_ret = f(self, stmt, local_vars, allow_recursion, *args, **kwargs)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/speech2text/speech2text/youtube/./youtube-dl/youtube_dl/jsinterp.py\", line 750, in interpret_statement\r\n    local_vars[m.group('out')] = self._operator(\r\n                                 ^^^^^^^^^^^^^^^\r\n  File \"/__w/speech2text/speech2text/youtube/./youtube-dl/youtube_dl/jsinterp.py\", line 441, in _operator\r\n    right_val = self.interpret_expression(right_expr, local_vars, allow_recursion)\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/speech2text/speech2text/youtube/./youtube-dl/youtube_dl/jsinterp.py\", line 1017, in interpret_expression\r\n    ret, should_return = self.interpret_statement(expr, local_vars, allow_recursion)\r\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/speech2text/speech2text/youtube/./youtube-dl/youtube_dl/jsinterp.py\", line 249, in interpret_statement\r\n    ret, should_ret = f(self, stmt, local_vars, allow_recursion, *args, **kwargs)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/speech2text/speech2text/youtube/./youtube-dl/youtube_dl/jsinterp.py\", line 1001, in interpret_statement\r\n    return eval_method(variable, member), should_return\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/speech2text/speech2text/youtube/./youtube-dl/youtube_dl/jsinterp.py\", line 883, in eval_method\r\n    argvals = [\r\n              ^\r\n  File \"/__w/speech2text/speech2text/youtube/./youtube-dl/youtube_dl/jsinterp.py\", line 884, in <listcomp>\r\n    self.interpret_expression(v, local_vars, allow_recursion)\r\n  File \"/__w/speech2text/speech2text/youtube/./youtube-dl/youtube_dl/jsinterp.py\", line 1017, in interpret_expression\r\n    ret, should_return = self.interpret_statement(expr, local_vars, allow_recursion)\r\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/speech2text/speech2text/youtube/./youtube-dl/youtube_dl/jsinterp.py\", line 249, in interpret_statement\r\n    ret, should_ret = f(self, stmt, local_vars, allow_recursion, *args, **kwargs)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/speech2text/speech2text/youtube/./youtube-dl/youtube_dl/jsinterp.py\", line 1001, in interpret_statement\r\n    return eval_method(variable, member), should_return\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/speech2text/speech2text/youtube/./youtube-dl/youtube_dl/jsinterp.py\", line 928, in eval_method\r\n    assertion(isinstance(obj, list), 'must be applied on a list')\r\n  File \"/__w/speech2text/speech2text/youtube/./youtube-dl/youtube_dl/jsinterp.py\", line [85](https://github.com/xihajun/speech2text/actions/runs/10256086421/job/28374451507#step:4:86)1, in assertion\r\n    raise self.Exception('{memb} {msg}'.format(**locals()), expr=expr)\r\nyoutube_dl.jsinterp.JSInterpreter.Exception: slice must be applied on a list in: 'a.slice(0,0)'; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n```\r\n\r\n\r\n## Description\r\n\r\n<!--\r\nProvide an explanation of your issue in an arbitrary form. Please make sure the description is worded well enough to be understood, see https://github.com/ytdl-org/youtube-dl#is-the-description-of-the-issue-itself-sufficient. Provide any additional information, suggested solution and as much context and examples as possible.\r\nIf work on your issue requires account credentials please provide them or explain how one can obtain them.\r\n-->\r\n\r\nWRITE DESCRIPTION HERE\r\n\nvbox7 extract url problem\n<!--\r\n\r\n######################################################################\r\n  WARNING!\r\n  IGNORING THE FOLLOWING TEMPLATE WILL RESULT IN ISSUE CLOSED AS INCOMPLETE\r\n######################################################################\r\n\r\n-->\r\n\r\n\r\n## Checklist\r\n\r\n<!--\r\nCarefully read and work through this check list in order to prevent the most common mistakes and misuse of youtube-dl:\r\n- First of, make sure you are using the latest version of youtube-dl. Run `youtube-dl --version` and ensure your version is 2020.07.28. If it's not, see https://yt-dl.org/update on how to update. Issues with outdated version will be REJECTED.\r\n- Make sure that all provided video/audio/playlist URLs (if any) are alive and playable in a browser.\r\n- Make sure that all URLs and arguments with special characters are properly quoted or escaped as explained in http://yt-dl.org/escape.\r\n- Search the bugtracker for similar issues: http://yt-dl.org/search-issues. DO NOT post duplicates.\r\n- Read bugs section in FAQ: http://yt-dl.org/reporting\r\n- Finally, put x into all relevant boxes (like this [x])\r\n-->\r\n\r\n- [x] I'm reporting a broken site support issue\r\n- [x] I've verified that I'm running youtube-dl version **2020.07.28**\r\n- [x] I've checked that all provided URLs are alive and playable in a browser\r\n- [x] I've checked that all URLs and arguments with special characters are properly quoted or escaped\r\n- [x] I've searched the bugtracker for similar bug reports including closed ones\r\n- [x] I've read bugs section in FAQ\r\n\r\n\r\n## Verbose log\r\n\r\n<!--\r\nProvide the complete verbose output of youtube-dl that clearly demonstrates the problem.\r\nAdd the `-v` flag to your command line you run youtube-dl with (`youtube-dl -v <your command line>`), copy the WHOLE output and insert it below. It should look similar to this:\r\n [debug] System config: []\r\n [debug] User config: []\r\n [debug] Command-line args: [u'-v', u'http://www.youtube.com/watch?v=BaW_jenozKcj']\r\n [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251\r\n [debug] youtube-dl version 2020.07.28\r\n [debug] Python version 2.7.11 - Windows-2003Server-5.2.3790-SP2\r\n [debug] exe versions: ffmpeg N-75573-g1d0487f, ffprobe N-75573-g1d0487f, rtmpdump 2.4\r\n [debug] Proxy map: {}\r\n <more lines>\r\n-->\r\n\r\n```\r\nC:\\Users\\PanteliX\\Desktop>C:\\Users\\PanteliX\\Desktop\\youtube-dl.exe --verbose -g\r\nhttps://www.vbox7.com/play:e96500c7cd >>sdasdasdsd.txt\r\n[debug] System config: []\r\n[debug] User config: []\r\n[debug] Custom config: []\r\n[debug] Command-line args: ['--verbose', '-g', 'https://www.vbox7.com/play:e9650\r\n0c7cd']\r\n[debug] Encodings: locale cp1251, fs mbcs, out cp1251, pref cp1251\r\n[debug] youtube-dl version 2020.07.28\r\n[debug] Python version 3.4.4 (CPython) - Windows-7-6.1.7601-SP1\r\n[debug] exe versions: none\r\n[debug] Proxy map: {}\r\nWARNING: [Vbox7] e96500c7cd: Failed to parse JSON Invalid control character at:\r\nline 6 column 173 (char 286)\r\nWARNING: unable to extract JSON-LD ; please report this issue on https://yt-dl.o\r\nrg/bug . Make sure you are using the latest version; type  youtube-dl -U  to upd\r\nate. Be sure to call youtube-dl with the --verbose flag and include its complete\r\n output.\r\n```\r\n\r\n\r\n## Description\r\n\r\n<!--\r\nProvide an explanation of your issue in an arbitrary form. Please make sure the description is worded well enough to be understood, see https://github.com/ytdl-org/youtube-dl#is-the-description-of-the-issue-itself-sufficient. Provide any additional information, suggested solution and as much context and examples as possible.\r\nIf work on your issue requires account credentials please provide them or explain how one can obtain them.\r\n-->\r\n\r\nWhen I try to download a video it gives me the following error\r\n\n Unable to decode n-parameter: download likely to be throttled\n<!--\r\n\r\n######################################################################\r\n  WARNING!\r\n  IGNORING THE FOLLOWING TEMPLATE WILL RESULT IN ISSUE CLOSED AS INCOMPLETE\r\n######################################################################\r\n\r\n-->\r\n\r\n\r\n## Checklist\r\n\r\n<!--\r\nCarefully read and work through this check list in order to prevent the most common mistakes and misuse of youtube-dl:\r\n- First of, make sure you are using the latest version of youtube-dl. Run `youtube-dl --version` and ensure your version is 2021.12.17. If it's not, see https://yt-dl.org/update on how to update. Issues with outdated version will be REJECTED.\r\n- Make sure that all provided video/audio/playlist URLs (if any) are alive and playable in a browser.\r\n- Make sure that all URLs and arguments with special characters are properly quoted or escaped as explained in http://yt-dl.org/escape.\r\n- Search the bugtracker for similar issues: http://yt-dl.org/search-issues. DO NOT post duplicates.\r\n- Read bugs section in FAQ: http://yt-dl.org/reporting\r\n- Finally, put x into all relevant boxes (like this [x])\r\n-->\r\n\r\n- [x] I'm reporting a broken site support issue\r\n- [x] I've verified that I'm running youtube-dl version **2021.12.17**\r\n- [x] I've checked that all provided URLs are alive and playable in a browser\r\n- [x] I've checked that all URLs and arguments with special characters are properly quoted or escaped\r\n- [x] I've searched the bugtracker for similar bug reports including closed ones\r\n- [x] I've read bugs section in FAQ\r\n\r\n\r\n## Verbose log\r\n\r\n<!--\r\nProvide the complete verbose output of youtube-dl that clearly demonstrates the problem.\r\nAdd the `-v` flag to your command line you run youtube-dl with (`youtube-dl -v <your command line>`), copy the WHOLE output and insert it below. It should look similar to this:\r\n [debug] System config: []\r\n [debug] User config: []\r\n [debug] Command-line args: [u'-v', u'http://www.youtube.com/watch?v=BaW_jenozKcj']\r\n [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251\r\n [debug] youtube-dl version 2021.12.17\r\n [debug] Python version 2.7.11 - Windows-2003Server-5.2.3790-SP2\r\n [debug] exe versions: ffmpeg N-75573-g1d0487f, ffprobe N-75573-g1d0487f, rtmpdump 2.4\r\n [debug] Proxy map: {}\r\n <more lines>\r\n-->\r\n\r\n```\r\n[debug] System config: []\r\n[debug] User config: []\r\n[debug] Custom config: []\r\n[debug] Command-line args: ['--verbose', 'https://www.youtube.com/watch?v=03Gug8rMTSI']\r\n[debug] Encodings: locale UTF-8, fs utf-8, out utf-8, pref UTF-8\r\n[debug] youtube-dl version 2021.12.17\r\n[debug] Python version 3.10.9 (CPython) - Linux-5.10.102.1-microsoft-standard-WSL2-x86_64-with\r\n[debug] exe versions: ffmpeg 5.1.2, ffprobe 5.1.2\r\n[debug] Proxy map: {}\r\n[youtube] 03Gug8rMTSI: Downloading webpage\r\n[youtube] 03Gug8rMTSI: Downloading player dac945fd\r\nWARNING: [youtube] Unable to decode n-parameter: download likely to be throttled (Unhandled exception in decode; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose flag and include its complete output. Traceback (most recent call last):\r\n  File \"/usr/local/bin/youtube-dl/youtube_dl/extractor/youtube.py\", line 1524, in _n_descramble\r\n    raise ExtractorError('Unhandled exception in decode')\r\nyoutube_dl.utils.ExtractorError: Unhandled exception in decode; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.\r\n)\r\n[debug] Default format spec: bestvideo+bestaudio/best\r\nWARNING: Requested formats are incompatible for merge and will be merged into mkv.\r\n```\r\n\r\n\r\n## Description\r\n\r\n<!--\r\nProvide an explanation of your issue in an arbitrary form. Please make sure the description is worded well enough to be understood, see https://github.com/ytdl-org/youtube-dl#is-the-description-of-the-issue-itself-sufficient. Provide any additional information, suggested solution and as much context and examples as possible.\r\nIf work on your issue requires account credentials please provide them or explain how one can obtain them.\r\n-->\r\n\r\nI was trying to download a simple short you tube video, nothing fancy. The tool issued the above output and requested that I submit a bug report, and as the good netizen I am, I have just submitted.\r\n\r\nRegards\r\n\nOption --restrict-filenames should implicitly de-zalgo the text.\n## Checklist\r\n\r\n<!--\r\nCarefully read and work through this check list in order to prevent the most common mistakes and misuse of youtube-dl:\r\n- First of, make sure you are using the latest version of youtube-dl. Run `youtube-dl --version` and ensure your version is 2021.12.17. If it's not, see https://yt-dl.org/update on how to update. Issues with outdated version will be REJECTED.\r\n- Search the bugtracker for similar feature requests: http://yt-dl.org/search-issues. DO NOT post duplicates.\r\n- Finally, put x into all relevant boxes (like this [x])\r\n-->\r\n\r\n- [x] I'm reporting a feature request\r\n- [x] I've verified that I'm running youtube-dl version **2021.12.17**\r\n- [x] I've searched the bugtracker for similar feature requests including closed ones\r\n\r\n\r\n## Problem\r\n\r\nI had to manually fix the output filename for a video whose title contains lots of character decorations ([Zalgo](https://en.wikipedia.org/wiki/Zalgo_text)):\r\n\r\n```text\r\n$ ytdl-rg3 --format=18 --get-title -- i41M0no6ff4\r\nFilm Theory: There Is Only DAD DaD \u0358\u0348DAd \u0312\u035e\u0307d\u0344\u031a\u033baD dAD\r\n\r\n$ ytdl-rg3 --format=18 --output='%(title)s' --get-filename -- i41M0no6ff4\r\nFilm_Theory_-_There_Is_Only_DAD_D_a_D_DA_d_d_a_D_d_A_D\r\n```\r\n\r\n(Why I need `--format=` to read the title can be left for another day.)\r\n\r\n\r\n## Proposed solution\r\n\r\nLet's automatically dezalgo the input before restricting filename. On PyPI I found [zalgolib](https://pypi.org/project/zalgolib/) but it may be easier to just regexp-replace the unicode ranges mentioned there.\r\n\r\nFortunately, for this specific video I found a good enough work-around:\r\n\r\n```text\r\n$ ytdl-rg3 --format=18 --get-title i41M0no6ff4 | tr -cd 'A-Za-z \\n' | tr ' ' _\r\nFilm_Theory_There_Is_Only_DAD_DaD_DAd_daD_dAD\r\n```\n[downloader/external] fix wget proxy\n## Please follow the guide below\r\n\r\n- You will be asked some questions, please read them **carefully** and answer honestly\r\n- Put an `x` into all the boxes [ ] relevant to your *pull request* (like that [x])\r\n- Use *Preview* tab to see how your *pull request* will actually look like\r\n\r\n---\r\n\r\n### Before submitting a *pull request* make sure you have:\r\n- [x] [Searched](https://github.com/ytdl-org/youtube-dl/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests\r\n- [x] Read [adding new extractor tutorial](https://github.com/ytdl-org/youtube-dl#adding-support-for-a-new-site)\r\n- [x] Read [youtube-dl coding conventions](https://github.com/ytdl-org/youtube-dl#youtube-dl-coding-conventions) and adjusted the code to meet them\r\n- [ ] Covered the code with tests (note that PRs without tests will be REJECTED)\r\nI couldn't find test cases for external downloaders so I didn't run python test/test_.... Did execution test to this change of course.\r\n- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8)\r\n\r\n### In order to be accepted and merged into youtube-dl each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check one of the following options:\r\n- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)\r\n- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)\r\n\r\n### What is the purpose of your *pull request*?\r\n- [x] Bug fix\r\n- [ ] Improvement\r\n- [ ] New extractor\r\n- [ ] New feature\r\n\r\n---\r\n\r\n### Description of your *pull request* and other information\r\n\r\nI noticed that youtube-dl passes proxy to wget incorrectly by command line `--proxy`. The standard way (as in 'man wget') is to use environment variables `http_proxy` and `https_proxy` which this PR fixes.\r\n\r\nActually, if I run youtube-dl with --external-downloader wget and --proxy ..., I get error. The full log is too long so I quote only the part of it.\r\n\r\n> $ youtube-dl h<span></span>ttps://www.youtube.com/watch?v=U7qKJi4elro -o '%(id)s.%(ext)s' -v -f18 --external-downloader wget --external-downloader-args --no-config --proxy h<span></span>ttp://127.0.0.1:8080 --no-check-certificate \r\n> ...\r\n> [youtube] U7qKJi4elro: Downloading webpage\r\n> [debug] Invoking downloader on ...\r\n> [download] Destination: U7qKJi4elro.mp4\r\n> [debug] wget command line: wget -O U7qKJi4elro.mp4.part ...\r\n> h<span></span>ttp://127.0.0.1:8080/:\r\n> 2021-06-20 13:45:25 ERROR 400: Bad Request.\r\n> 2021-06-20 13:45:29 URL:... [5004778/5004778] -> \"U7qKJi4elro.mp4.part\" [1]\r\n> FINISHED --2021-06-20 13:45:29--\r\n> Total wall clock time: 4.2s\r\n> Downloaded: 1 files, 4.8M in 1.3s (3.78 MB/s)\r\n> \r\n> ERROR: wget exited with code 8\r\n> \r\n\r\nAnd to be sure, I checked old wget around commit bf812ef71438036c23640f29bd7ae955289720ed which youtube-dl started to pass proxy to wget. Environment variables seem to have been needed for wget since before.\r\n\n", "hints_text": "\n\n\n\n\nI'm sorry to bother you and I'm not sure if I've understood your comments correctly, but I changed the code to pass proxy in command line not environment variables.\r\n", "created_at": "2024-10-14T13:08:52Z"}
{"repo": "ytdl-org/youtube-dl", "pull_number": 32845, "instance_id": "ytdl-org__youtube-dl-32845", "issue_numbers": ["32842", "32843"], "base_commit": "a452f9437c8a3048f75fc12f75bcfd3eed78430f", "patch": "diff --git a/youtube_dl/extractor/youtube.py b/youtube_dl/extractor/youtube.py\nindex 90c16e172bd..2e31a89798e 100644\n--- a/youtube_dl/extractor/youtube.py\n+++ b/youtube_dl/extractor/youtube.py\n@@ -1636,7 +1636,7 @@ def _decrypt_nsig(self, n, video_id, player_url):\n         try:\n             jsi, player_id, func_code = self._extract_n_function_code(video_id, player_url)\n         except ExtractorError as e:\n-            raise ExtractorError('Unable to extract nsig jsi, player_id, func_codefunction code', cause=e)\n+            raise ExtractorError('Unable to extract nsig function code', cause=e)\n         if self.get_param('youtube_print_sig_code'):\n             self.to_screen('Extracted nsig function from {0}:\\n{1}\\n'.format(\n                 player_id, func_code[1]))\n@@ -1658,8 +1658,14 @@ def _decrypt_nsig(self, n, video_id, player_url):\n \n     def _extract_n_function_name(self, jscode):\n         func_name, idx = self._search_regex(\n-            r'\\.get\\(\"n\"\\)\\)&&\\(b=(?P<nfunc>[a-zA-Z_$][\\w$]*)(?:\\[(?P<idx>\\d+)\\])?\\([\\w$]+\\)',\n-            jscode, 'Initial JS player n function name', group=('nfunc', 'idx'))\n+            # new: (b=String.fromCharCode(110),c=a.get(b))&&c=nfunc[idx](c)\n+            # old: .get(\"n\"))&&(b=nfunc[idx](b)\n+            # older: .get(\"n\"))&&(b=nfunc(b)\n+            r'''(?x)\n+                (?:\\(\\s*(?P<b>[a-z])\\s*=\\s*String\\s*\\.\\s*fromCharCode\\s*\\(\\s*110\\s*\\)\\s*,(?P<c>[a-z])\\s*=\\s*[a-z]\\s*)?\n+                \\.\\s*get\\s*\\(\\s*(?(b)(?P=b)|\"n\")(?:\\s*\\)){2}\\s*&&\\s*\\(\\s*(?(c)(?P=c)|b)\\s*=\\s*\n+                (?P<nfunc>[a-zA-Z_$][\\w$]*)(?:\\s*\\[(?P<idx>\\d+)\\])?\\s*\\(\\s*[\\w$]+\\s*\\)\n+            ''', jscode, 'Initial JS player n function name', group=('nfunc', 'idx'))\n         if not idx:\n             return func_name\n \n@@ -1679,17 +1685,7 @@ def _extract_n_function_code(self, video_id, player_url):\n \n         func_name = self._extract_n_function_name(jscode)\n \n-        # For redundancy\n-        func_code = self._search_regex(\n-            r'''(?xs)%s\\s*=\\s*function\\s*\\((?P<var>[\\w$]+)\\)\\s*\n-                     # NB: The end of the regex is intentionally kept strict\n-                     {(?P<code>.+?}\\s*return\\ [\\w$]+.join\\(\"\"\\))};''' % func_name,\n-            jscode, 'nsig function', group=('var', 'code'), default=None)\n-        if func_code:\n-            func_code = ([func_code[0]], func_code[1])\n-        else:\n-            self.write_debug('Extracting nsig function with jsinterp')\n-            func_code = jsi.extract_function_code(func_name)\n+        func_code = jsi.extract_function_code(func_name)\n \n         self.cache.store('youtube-nsig', player_id, func_code)\n         return jsi, player_id, func_code\ndiff --git a/youtube_dl/jsinterp.py b/youtube_dl/jsinterp.py\nindex 02adf667846..949f77775e8 100644\n--- a/youtube_dl/jsinterp.py\n+++ b/youtube_dl/jsinterp.py\n@@ -20,7 +20,9 @@\n     compat_basestring,\n     compat_chr,\n     compat_collections_chain_map as ChainMap,\n+    compat_filter as filter,\n     compat_itertools_zip_longest as zip_longest,\n+    compat_map as map,\n     compat_str,\n )\n \n@@ -252,7 +254,7 @@ def interpret_statement(self, stmt, local_vars, allow_recursion, *args, **kwargs\n                     cls.write('=> Raises:', e, '<-|', stmt, level=allow_recursion)\n                 raise\n             if cls.ENABLED and stmt.strip():\n-                if should_ret or not repr(ret) == stmt:\n+                if should_ret or repr(ret) != stmt:\n                     cls.write(['->', '=>'][should_ret], repr(ret), '<-|', stmt, level=allow_recursion)\n             return ret, should_ret\n         return interpret_statement\n@@ -365,6 +367,8 @@ def _separate(cls, expr, delim=',', max_split=None, skip_delims=None):\n         start, splits, pos, delim_len = 0, 0, 0, len(delim) - 1\n         in_quote, escaping, after_op, in_regex_char_group = None, False, True, False\n         skipping = 0\n+        if skip_delims:\n+            skip_delims = variadic(skip_delims)\n         for idx, char in enumerate(expr):\n             paren_delta = 0\n             if not in_quote:\n@@ -391,7 +395,7 @@ def _separate(cls, expr, delim=',', max_split=None, skip_delims=None):\n                 continue\n             elif pos == 0 and skip_delims:\n                 here = expr[idx:]\n-                for s in variadic(skip_delims):\n+                for s in skip_delims:\n                     if here.startswith(s) and s:\n                         skipping = len(s) - 1\n                         break\n@@ -412,7 +416,6 @@ def _separate_at_paren(cls, expr, delim=None):\n         if delim is None:\n             delim = expr and _MATCHING_PARENS[expr[0]]\n         separated = list(cls._separate(expr, delim, 1))\n-\n         if len(separated) < 2:\n             raise cls.Exception('No terminating paren {delim} in {expr!r:.5500}'.format(**locals()))\n         return separated[0][1:].strip(), separated[1].strip()\n@@ -487,6 +490,7 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n         # fails on (eg) if (...) stmt1; else stmt2;\n         sub_statements = list(self._separate(stmt, ';')) or ['']\n         expr = stmt = sub_statements.pop().strip()\n+\n         for sub_stmt in sub_statements:\n             ret, should_return = self.interpret_statement(sub_stmt, local_vars, allow_recursion)\n             if should_return:\n@@ -626,8 +630,7 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n                     if m.group('err'):\n                         catch_vars[m.group('err')] = err.error if isinstance(err, JS_Throw) else err\n                     catch_vars = local_vars.new_child(m=catch_vars)\n-                    err = None\n-                    pending = self.interpret_statement(sub_expr, catch_vars, allow_recursion)\n+                    err, pending = None, self.interpret_statement(sub_expr, catch_vars, allow_recursion)\n \n             m = self._FINALLY_RE.match(expr)\n             if m:\n@@ -801,16 +804,19 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n             if op in ('+', '-'):\n                 # simplify/adjust consecutive instances of these operators\n                 undone = 0\n-                while len(separated) > 1 and not separated[-1].strip():\n+                separated = [s.strip() for s in separated]\n+                while len(separated) > 1 and not separated[-1]:\n                     undone += 1\n                     separated.pop()\n                 if op == '-' and undone % 2 != 0:\n                     right_expr = op + right_expr\n                 elif op == '+':\n-                    while len(separated) > 1 and separated[-1].strip() in self.OP_CHARS:\n+                    while len(separated) > 1 and set(separated[-1]) <= self.OP_CHARS:\n+                        right_expr = separated.pop() + right_expr\n+                    if separated[-1][-1:] in self.OP_CHARS:\n                         right_expr = separated.pop() + right_expr\n                 # hanging op at end of left => unary + (strip) or - (push right)\n-                left_val = separated[-1]\n+                left_val = separated[-1] if separated else ''\n                 for dm_op in ('*', '%', '/', '**'):\n                     bodmas = tuple(self._separate(left_val, dm_op, skip_delims=skip_delim))\n                     if len(bodmas) > 1 and not bodmas[-1].strip():\n@@ -844,7 +850,7 @@ def assertion(cndn, msg):\n                     memb = member\n                     raise self.Exception('{memb} {msg}'.format(**locals()), expr=expr)\n \n-            def eval_method():\n+            def eval_method(variable, member):\n                 if (variable, member) == ('console', 'debug'):\n                     if Debugger.ENABLED:\n                         Debugger.write(self.interpret_expression('[{}]'.format(arg_str), local_vars, allow_recursion))\n@@ -852,6 +858,7 @@ def eval_method():\n                 types = {\n                     'String': compat_str,\n                     'Math': float,\n+                    'Array': list,\n                 }\n                 obj = local_vars.get(variable)\n                 if obj in (JS_Undefined, None):\n@@ -877,12 +884,29 @@ def eval_method():\n                     self.interpret_expression(v, local_vars, allow_recursion)\n                     for v in self._separate(arg_str)]\n \n-                if obj == compat_str:\n+                # Fixup prototype call\n+                if isinstance(obj, type):\n+                    new_member, rest = member.partition('.')[0::2]\n+                    if new_member == 'prototype':\n+                        new_member, func_prototype = rest.partition('.')[0::2]\n+                        assertion(argvals, 'takes one or more arguments')\n+                        assertion(isinstance(argvals[0], obj), 'must bind to type {0}'.format(obj))\n+                        if func_prototype == 'call':\n+                            obj = argvals.pop(0)\n+                        elif func_prototype == 'apply':\n+                            assertion(len(argvals) == 2, 'takes two arguments')\n+                            obj, argvals = argvals\n+                            assertion(isinstance(argvals, list), 'second argument must be a list')\n+                        else:\n+                            raise self.Exception('Unsupported Function method ' + func_prototype, expr)\n+                        member = new_member\n+\n+                if obj is compat_str:\n                     if member == 'fromCharCode':\n                         assertion(argvals, 'takes one or more arguments')\n                         return ''.join(map(compat_chr, argvals))\n                     raise self.Exception('Unsupported string method ' + member, expr=expr)\n-                elif obj == float:\n+                elif obj is float:\n                     if member == 'pow':\n                         assertion(len(argvals) == 2, 'takes two arguments')\n                         return argvals[0] ** argvals[1]\n@@ -907,12 +931,12 @@ def eval_method():\n                 elif member == 'splice':\n                     assertion(isinstance(obj, list), 'must be applied on a list')\n                     assertion(argvals, 'takes one or more arguments')\n-                    index, howMany = map(int, (argvals + [len(obj)])[:2])\n+                    index, how_many = map(int, (argvals + [len(obj)])[:2])\n                     if index < 0:\n                         index += len(obj)\n                     add_items = argvals[2:]\n                     res = []\n-                    for i in range(index, min(index + howMany, len(obj))):\n+                    for _ in range(index, min(index + how_many, len(obj))):\n                         res.append(obj.pop(index))\n                     for i, item in enumerate(add_items):\n                         obj.insert(index + i, item)\n@@ -970,11 +994,11 @@ def eval_method():\n \n             if remaining:\n                 ret, should_abort = self.interpret_statement(\n-                    self._named_object(local_vars, eval_method()) + remaining,\n+                    self._named_object(local_vars, eval_method(variable, member)) + remaining,\n                     local_vars, allow_recursion)\n                 return ret, should_return or should_abort\n             else:\n-                return eval_method(), should_return\n+                return eval_method(variable, member), should_return\n \n         elif md.get('function'):\n             fname = m.group('fname')\n@@ -1002,28 +1026,25 @@ def interpret_iter(self, list_txt, local_vars, allow_recursion):\n     def extract_object(self, objname):\n         _FUNC_NAME_RE = r'''(?:[a-zA-Z$0-9]+|\"[a-zA-Z$0-9]+\"|'[a-zA-Z$0-9]+')'''\n         obj = {}\n-        fields = None\n-        for obj_m in re.finditer(\n+        fields = next(filter(None, (\n+            obj_m.group('fields') for obj_m in re.finditer(\n                 r'''(?xs)\n                     {0}\\s*\\.\\s*{1}|{1}\\s*=\\s*\\{{\\s*\n                         (?P<fields>({2}\\s*:\\s*function\\s*\\(.*?\\)\\s*\\{{.*?}}(?:,\\s*)?)*)\n                     }}\\s*;\n                 '''.format(_NAME_RE, re.escape(objname), _FUNC_NAME_RE),\n-                self.code):\n-            fields = obj_m.group('fields')\n-            if fields:\n-                break\n-        else:\n+                self.code))), None)\n+        if not fields:\n             raise self.Exception('Could not find object ' + objname)\n         # Currently, it only supports function definitions\n-        fields_m = re.finditer(\n-            r'''(?x)\n-                (?P<key>%s)\\s*:\\s*function\\s*\\((?P<args>(?:%s|,)*)\\){(?P<code>[^}]+)}\n-            ''' % (_FUNC_NAME_RE, _NAME_RE),\n-            fields)\n-        for f in fields_m:\n+        for f in re.finditer(\n+                r'''(?x)\n+                    (?P<key>%s)\\s*:\\s*function\\s*\\((?P<args>(?:%s|,)*)\\){(?P<code>[^}]+)}\n+                ''' % (_FUNC_NAME_RE, _NAME_RE),\n+                fields):\n             argnames = self.build_arglist(f.group('args'))\n-            obj[remove_quotes(f.group('key'))] = self.build_function(argnames, f.group('code'))\n+            name = remove_quotes(f.group('key'))\n+            obj[name] = function_with_repr(self.build_function(argnames, f.group('code')), 'F<{0}>'.format(name))\n \n         return obj\n \n@@ -1058,7 +1079,7 @@ def extract_function_code(self, funcname):\n     def extract_function(self, funcname):\n         return function_with_repr(\n             self.extract_function_from_code(*self.extract_function_code(funcname)),\n-            'F<%s>' % (funcname, ))\n+            'F<%s>' % (funcname,))\n \n     def extract_function_from_code(self, argnames, code, *global_stack):\n         local_vars = {}\n@@ -1067,7 +1088,7 @@ def extract_function_from_code(self, argnames, code, *global_stack):\n             if mobj is None:\n                 break\n             start, body_start = mobj.span()\n-            body, remaining = self._separate_at_paren(code[body_start - 1:], '}')\n+            body, remaining = self._separate_at_paren(code[body_start - 1:])\n             name = self._named_object(local_vars, self.extract_function_from_code(\n                 [x.strip() for x in mobj.group('args').split(',')],\n                 body, local_vars, *global_stack))\n@@ -1095,8 +1116,7 @@ def build_function(self, argnames, code, *global_stack):\n         argnames = tuple(argnames)\n \n         def resf(args, kwargs={}, allow_recursion=100):\n-            global_stack[0].update(\n-                zip_longest(argnames, args, fillvalue=None))\n+            global_stack[0].update(zip_longest(argnames, args, fillvalue=None))\n             global_stack[0].update(kwargs)\n             var_stack = LocalNameSpace(*global_stack)\n             ret, should_abort = self.interpret_statement(code.replace('\\n', ' '), var_stack, allow_recursion - 1)\ndiff --git a/youtube_dl/utils.py b/youtube_dl/utils.py\nindex 3ec9d381190..ac1e78002b3 100644\n--- a/youtube_dl/utils.py\n+++ b/youtube_dl/utils.py\n@@ -6604,27 +6604,53 @@ class _UnsafeExtensionError(Exception):\n         ),\n         # video\n         MEDIA_EXTENSIONS.video, (\n-            'avif',\n+            'asx',\n             'ismv',\n+            'm2t',\n             'm2ts',\n+            'm2v',\n             'm4s',\n             'mng',\n+            'mp2v',\n+            'mp4v',\n+            'mpe',\n             'mpeg',\n+            'mpeg1',\n+            'mpeg2',\n+            'mpeg4',\n+            'mxf',\n+            'ogm',\n             'qt',\n+            'rm',\n             'swf',\n             'ts',\n+            'vob',\n             'vp9',\n-            'wvm',\n         ),\n         # audio\n         MEDIA_EXTENSIONS.audio, (\n+            '3ga',\n+            'ac3',\n+            'adts',\n+            'aif',\n+            'au',\n+            'dts',\n             'isma',\n+            'it',\n             'mid',\n+            'mod',\n             'mpga',\n+            'mp1',\n+            'mp2',\n+            'mp4a',\n+            'mpa',\n             'ra',\n+            'shn',\n+            'xm',\n         ),\n         # image\n         MEDIA_EXTENSIONS.thumbnails, (\n+            'avif',\n             'bmp',\n             'gif',\n             'ico',\n@@ -6634,6 +6660,7 @@ class _UnsafeExtensionError(Exception):\n             'jxl',\n             'svg',\n             'tif',\n+            'tiff',\n             'wbmp',\n         ),\n         # subtitle\n@@ -6641,10 +6668,15 @@ class _UnsafeExtensionError(Exception):\n             'dfxp',\n             'fs',\n             'ismt',\n+            'json3',\n             'sami',\n             'scc',\n+            'srv1',\n+            'srv2',\n+            'srv3',\n             'ssa',\n             'tt',\n+            'xml',\n         ),\n         # others\n         MEDIA_EXTENSIONS.manifests,\n@@ -6658,7 +6690,6 @@ class _UnsafeExtensionError(Exception):\n             # 'swp',\n             # 'url',\n             # 'webloc',\n-            # 'xml',\n         )))\n \n     def __init__(self, extension):\n", "test_patch": "diff --git a/test/test_jsinterp.py b/test/test_jsinterp.py\nindex da8e980207a..104e766be36 100644\n--- a/test/test_jsinterp.py\n+++ b/test/test_jsinterp.py\n@@ -11,194 +11,146 @@\n import math\n import re\n \n+from youtube_dl.compat import compat_str\n from youtube_dl.jsinterp import JS_Undefined, JSInterpreter\n \n+NaN = object()\n \n-class TestJSInterpreter(unittest.TestCase):\n-    def test_basic(self):\n-        jsi = JSInterpreter('function x(){;}')\n-        self.assertEqual(jsi.call_function('x'), None)\n-        self.assertEqual(repr(jsi.extract_function('x')), 'F<x>')\n-\n-        jsi = JSInterpreter('function x3(){return 42;}')\n-        self.assertEqual(jsi.call_function('x3'), 42)\n \n-        jsi = JSInterpreter('function x3(){42}')\n-        self.assertEqual(jsi.call_function('x3'), None)\n+class TestJSInterpreter(unittest.TestCase):\n+    def _test(self, jsi_or_code, expected, func='f', args=()):\n+        if isinstance(jsi_or_code, compat_str):\n+            jsi_or_code = JSInterpreter(jsi_or_code)\n+        got = jsi_or_code.call_function(func, *args)\n+        if expected is NaN:\n+            self.assertTrue(math.isnan(got), '{0} is not NaN'.format(got))\n+        else:\n+            self.assertEqual(got, expected)\n \n-        jsi = JSInterpreter('var x5 = function(){return 42;}')\n-        self.assertEqual(jsi.call_function('x5'), 42)\n+    def test_basic(self):\n+        jsi = JSInterpreter('function f(){;}')\n+        self.assertEqual(repr(jsi.extract_function('f')), 'F<f>')\n+        self._test(jsi, None)\n \n-    def test_calc(self):\n-        jsi = JSInterpreter('function x4(a){return 2*a+1;}')\n-        self.assertEqual(jsi.call_function('x4', 3), 7)\n+        self._test('function f(){return 42;}', 42)\n+        self._test('function f(){42}', None)\n+        self._test('var f = function(){return 42;}', 42)\n \n     def test_add(self):\n-        jsi = JSInterpreter('function f(){return 42 + 7;}')\n-        self.assertEqual(jsi.call_function('f'), 49)\n-        jsi = JSInterpreter('function f(){return 42 + undefined;}')\n-        self.assertTrue(math.isnan(jsi.call_function('f')))\n-        jsi = JSInterpreter('function f(){return 42 + null;}')\n-        self.assertEqual(jsi.call_function('f'), 42)\n+        self._test('function f(){return 42 + 7;}', 49)\n+        self._test('function f(){return 42 + undefined;}', NaN)\n+        self._test('function f(){return 42 + null;}', 42)\n \n     def test_sub(self):\n-        jsi = JSInterpreter('function f(){return 42 - 7;}')\n-        self.assertEqual(jsi.call_function('f'), 35)\n-        jsi = JSInterpreter('function f(){return 42 - undefined;}')\n-        self.assertTrue(math.isnan(jsi.call_function('f')))\n-        jsi = JSInterpreter('function f(){return 42 - null;}')\n-        self.assertEqual(jsi.call_function('f'), 42)\n+        self._test('function f(){return 42 - 7;}', 35)\n+        self._test('function f(){return 42 - undefined;}', NaN)\n+        self._test('function f(){return 42 - null;}', 42)\n \n     def test_mul(self):\n-        jsi = JSInterpreter('function f(){return 42 * 7;}')\n-        self.assertEqual(jsi.call_function('f'), 294)\n-        jsi = JSInterpreter('function f(){return 42 * undefined;}')\n-        self.assertTrue(math.isnan(jsi.call_function('f')))\n-        jsi = JSInterpreter('function f(){return 42 * null;}')\n-        self.assertEqual(jsi.call_function('f'), 0)\n+        self._test('function f(){return 42 * 7;}', 294)\n+        self._test('function f(){return 42 * undefined;}', NaN)\n+        self._test('function f(){return 42 * null;}', 0)\n \n     def test_div(self):\n         jsi = JSInterpreter('function f(a, b){return a / b;}')\n-        self.assertTrue(math.isnan(jsi.call_function('f', 0, 0)))\n-        self.assertTrue(math.isnan(jsi.call_function('f', JS_Undefined, 1)))\n-        self.assertTrue(math.isinf(jsi.call_function('f', 2, 0)))\n-        self.assertEqual(jsi.call_function('f', 0, 3), 0)\n+        self._test(jsi, NaN, args=(0, 0))\n+        self._test(jsi, NaN, args=(JS_Undefined, 1))\n+        self._test(jsi, float('inf'), args=(2, 0))\n+        self._test(jsi, 0, args=(0, 3))\n \n     def test_mod(self):\n-        jsi = JSInterpreter('function f(){return 42 % 7;}')\n-        self.assertEqual(jsi.call_function('f'), 0)\n-        jsi = JSInterpreter('function f(){return 42 % 0;}')\n-        self.assertTrue(math.isnan(jsi.call_function('f')))\n-        jsi = JSInterpreter('function f(){return 42 % undefined;}')\n-        self.assertTrue(math.isnan(jsi.call_function('f')))\n+        self._test('function f(){return 42 % 7;}', 0)\n+        self._test('function f(){return 42 % 0;}', NaN)\n+        self._test('function f(){return 42 % undefined;}', NaN)\n \n     def test_exp(self):\n-        jsi = JSInterpreter('function f(){return 42 ** 2;}')\n-        self.assertEqual(jsi.call_function('f'), 1764)\n-        jsi = JSInterpreter('function f(){return 42 ** undefined;}')\n-        self.assertTrue(math.isnan(jsi.call_function('f')))\n-        jsi = JSInterpreter('function f(){return 42 ** null;}')\n-        self.assertEqual(jsi.call_function('f'), 1)\n-        jsi = JSInterpreter('function f(){return undefined ** 42;}')\n-        self.assertTrue(math.isnan(jsi.call_function('f')))\n+        self._test('function f(){return 42 ** 2;}', 1764)\n+        self._test('function f(){return 42 ** undefined;}', NaN)\n+        self._test('function f(){return 42 ** null;}', 1)\n+        self._test('function f(){return undefined ** 42;}', NaN)\n+\n+    def test_calc(self):\n+        self._test('function f(a){return 2*a+1;}', 7, args=[3])\n \n     def test_empty_return(self):\n-        jsi = JSInterpreter('function f(){return; y()}')\n-        self.assertEqual(jsi.call_function('f'), None)\n+        self._test('function f(){return; y()}', None)\n \n     def test_morespace(self):\n-        jsi = JSInterpreter('function x (a) { return 2 * a + 1 ; }')\n-        self.assertEqual(jsi.call_function('x', 3), 7)\n-\n-        jsi = JSInterpreter('function f () { x =  2  ; return x; }')\n-        self.assertEqual(jsi.call_function('f'), 2)\n+        self._test('function f (a) { return 2 * a + 1 ; }', 7, args=[3])\n+        self._test('function f () { x =  2  ; return x; }', 2)\n \n     def test_strange_chars(self):\n-        jsi = JSInterpreter('function $_xY1 ($_axY1) { var $_axY2 = $_axY1 + 1; return $_axY2; }')\n-        self.assertEqual(jsi.call_function('$_xY1', 20), 21)\n+        self._test('function $_xY1 ($_axY1) { var $_axY2 = $_axY1 + 1; return $_axY2; }',\n+                   21, args=[20], func='$_xY1')\n \n     def test_operators(self):\n-        jsi = JSInterpreter('function f(){return 1 << 5;}')\n-        self.assertEqual(jsi.call_function('f'), 32)\n-\n-        jsi = JSInterpreter('function f(){return 2 ** 5}')\n-        self.assertEqual(jsi.call_function('f'), 32)\n-\n-        jsi = JSInterpreter('function f(){return 19 & 21;}')\n-        self.assertEqual(jsi.call_function('f'), 17)\n-\n-        jsi = JSInterpreter('function f(){return 11 >> 2;}')\n-        self.assertEqual(jsi.call_function('f'), 2)\n-\n-        jsi = JSInterpreter('function f(){return []? 2+3: 4;}')\n-        self.assertEqual(jsi.call_function('f'), 5)\n-\n-        jsi = JSInterpreter('function f(){return 1 == 2}')\n-        self.assertEqual(jsi.call_function('f'), False)\n-\n-        jsi = JSInterpreter('function f(){return 0 && 1 || 2;}')\n-        self.assertEqual(jsi.call_function('f'), 2)\n-\n-        jsi = JSInterpreter('function f(){return 0 ?? 42;}')\n-        self.assertEqual(jsi.call_function('f'), 0)\n-\n-        jsi = JSInterpreter('function f(){return \"life, the universe and everything\" < 42;}')\n-        self.assertFalse(jsi.call_function('f'))\n+        self._test('function f(){return 1 << 5;}', 32)\n+        self._test('function f(){return 2 ** 5}', 32)\n+        self._test('function f(){return 19 & 21;}', 17)\n+        self._test('function f(){return 11 >> 2;}', 2)\n+        self._test('function f(){return []? 2+3: 4;}', 5)\n+        self._test('function f(){return 1 == 2}', False)\n+        self._test('function f(){return 0 && 1 || 2;}', 2)\n+        self._test('function f(){return 0 ?? 42;}', 0)\n+        self._test('function f(){return \"life, the universe and everything\" < 42;}', False)\n+        # https://github.com/ytdl-org/youtube-dl/issues/32815\n+        self._test('function f(){return 0  - 7 * - 6;}', 42)\n \n     def test_array_access(self):\n-        jsi = JSInterpreter('function f(){var x = [1,2,3]; x[0] = 4; x[0] = 5; x[2.0] = 7; return x;}')\n-        self.assertEqual(jsi.call_function('f'), [5, 2, 7])\n+        self._test('function f(){var x = [1,2,3]; x[0] = 4; x[0] = 5; x[2.0] = 7; return x;}', [5, 2, 7])\n \n     def test_parens(self):\n-        jsi = JSInterpreter('function f(){return (1) + (2) * ((( (( (((((3)))))) )) ));}')\n-        self.assertEqual(jsi.call_function('f'), 7)\n-\n-        jsi = JSInterpreter('function f(){return (1 + 2) * 3;}')\n-        self.assertEqual(jsi.call_function('f'), 9)\n+        self._test('function f(){return (1) + (2) * ((( (( (((((3)))))) )) ));}', 7)\n+        self._test('function f(){return (1 + 2) * 3;}', 9)\n \n     def test_quotes(self):\n-        jsi = JSInterpreter(r'function f(){return \"a\\\"\\\\(\"}')\n-        self.assertEqual(jsi.call_function('f'), r'a\"\\(')\n+        self._test(r'function f(){return \"a\\\"\\\\(\"}', r'a\"\\(')\n \n     def test_assignments(self):\n-        jsi = JSInterpreter('function f(){var x = 20; x = 30 + 1; return x;}')\n-        self.assertEqual(jsi.call_function('f'), 31)\n-\n-        jsi = JSInterpreter('function f(){var x = 20; x += 30 + 1; return x;}')\n-        self.assertEqual(jsi.call_function('f'), 51)\n-\n-        jsi = JSInterpreter('function f(){var x = 20; x -= 30 + 1; return x;}')\n-        self.assertEqual(jsi.call_function('f'), -11)\n+        self._test('function f(){var x = 20; x = 30 + 1; return x;}', 31)\n+        self._test('function f(){var x = 20; x += 30 + 1; return x;}', 51)\n+        self._test('function f(){var x = 20; x -= 30 + 1; return x;}', -11)\n \n+    @unittest.skip('Not yet fully implemented')\n     def test_comments(self):\n-        'Skipping: Not yet fully implemented'\n-        return\n-        jsi = JSInterpreter('''\n-        function x() {\n-            var x = /* 1 + */ 2;\n-            var y = /* 30\n-            * 40 */ 50;\n-            return x + y;\n-        }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 52)\n-\n-        jsi = JSInterpreter('''\n-        function f() {\n-            var x = \"/*\";\n-            var y = 1 /* comment */ + 2;\n-            return y;\n-        }\n-        ''')\n-        self.assertEqual(jsi.call_function('f'), 3)\n+        self._test('''\n+            function f() {\n+                var x = /* 1 + */ 2;\n+                var y = /* 30\n+                * 40 */ 50;\n+                return x + y;\n+            }\n+        ''', 52)\n+\n+        self._test('''\n+            function f() {\n+                var x = \"/*\";\n+                var y = 1 /* comment */ + 2;\n+                return y;\n+            }\n+        ''', 3)\n \n     def test_precedence(self):\n-        jsi = JSInterpreter('''\n-        function x() {\n-            var a = [10, 20, 30, 40, 50];\n-            var b = 6;\n-            a[0]=a[b%a.length];\n-            return a;\n-        }''')\n-        self.assertEqual(jsi.call_function('x'), [20, 20, 30, 40, 50])\n+        self._test('''\n+            function f() {\n+                var a = [10, 20, 30, 40, 50];\n+                var b = 6;\n+                a[0]=a[b%a.length];\n+                return a;\n+            }\n+        ''', [20, 20, 30, 40, 50])\n \n     def test_builtins(self):\n-        jsi = JSInterpreter('''\n-        function x() { return NaN }\n-        ''')\n-        self.assertTrue(math.isnan(jsi.call_function('x')))\n+        self._test('function f() { return NaN }', NaN)\n \n     def test_Date(self):\n-        jsi = JSInterpreter('''\n-        function x(dt) { return new Date(dt) - 0; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x', 'Wednesday 31 December 1969 18:01:26 MDT'), 86000)\n+        self._test('function f() { return new Date(\"Wednesday 31 December 1969 18:01:26 MDT\") - 0; }', 86000)\n \n+        jsi = JSInterpreter('function f(dt) { return new Date(dt) - 0; }')\n         # date format m/d/y\n-        self.assertEqual(jsi.call_function('x', '12/31/1969 18:01:26 MDT'), 86000)\n-\n+        self._test(jsi, 86000, args=['12/31/1969 18:01:26 MDT'])\n         # epoch 0\n-        self.assertEqual(jsi.call_function('x', '1 January 1970 00:00:00 UTC'), 0)\n+        self._test(jsi, 0, args=['1 January 1970 00:00:00 UTC'])\n \n     def test_call(self):\n         jsi = JSInterpreter('''\n@@ -206,179 +158,115 @@ def test_call(self):\n         function y(a) { return x() + (a?a:0); }\n         function z() { return y(3); }\n         ''')\n-        self.assertEqual(jsi.call_function('z'), 5)\n-        self.assertEqual(jsi.call_function('y'), 2)\n+        self._test(jsi, 5, func='z')\n+        self._test(jsi, 2, func='y')\n \n     def test_if(self):\n-        jsi = JSInterpreter('''\n-        function x() {\n+        self._test('''\n+            function f() {\n             let a = 9;\n             if (0==0) {a++}\n             return a\n-        }''')\n-        self.assertEqual(jsi.call_function('x'), 10)\n+            }\n+        ''', 10)\n \n-        jsi = JSInterpreter('''\n-        function x() {\n+        self._test('''\n+            function f() {\n             if (0==0) {return 10}\n-        }''')\n-        self.assertEqual(jsi.call_function('x'), 10)\n+            }\n+        ''', 10)\n \n-        jsi = JSInterpreter('''\n-        function x() {\n+        self._test('''\n+            function f() {\n             if (0!=0) {return 1}\n             else {return 10}\n-        }''')\n-        self.assertEqual(jsi.call_function('x'), 10)\n-\n-        \"\"\"  # Unsupported\n-        jsi = JSInterpreter('''\n-        function x() {\n-            if (0!=0) return 1;\n-            else {return 10}\n-        }''')\n-        self.assertEqual(jsi.call_function('x'), 10)\n-        \"\"\"\n+            }\n+        ''', 10)\n \n     def test_elseif(self):\n-        jsi = JSInterpreter('''\n-        function x() {\n-            if (0!=0) {return 1}\n-            else if (1==0) {return 2}\n-            else {return 10}\n-        }''')\n-        self.assertEqual(jsi.call_function('x'), 10)\n-\n-        \"\"\"  # Unsupported\n-        jsi = JSInterpreter('''\n-        function x() {\n-            if (0!=0) return 1;\n-            else if (1==0) {return 2}\n-            else {return 10}\n-        }''')\n-        self.assertEqual(jsi.call_function('x'), 10)\n-        # etc\n-        \"\"\"\n+        self._test('''\n+            function f() {\n+                if (0!=0) {return 1}\n+                else if (1==0) {return 2}\n+                else {return 10}\n+            }\n+        ''', 10)\n \n     def test_for_loop(self):\n-        # function x() { a=0; for (i=0; i-10; i++) {a++} a }\n-        jsi = JSInterpreter('''\n-        function x() { a=0; for (i=0; i-10; i++) {a++} return a }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 10)\n+        self._test('function f() { a=0; for (i=0; i-10; i++) {a++} return a }', 10)\n \n     def test_while_loop(self):\n-        # function x() { a=0; while (a<10) {a++} a }\n-        jsi = JSInterpreter('''\n-        function x() { a=0; while (a<10) {a++} return a }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 10)\n+        self._test('function f() { a=0; while (a<10) {a++} return a }', 10)\n \n     def test_switch(self):\n         jsi = JSInterpreter('''\n-        function x(f) { switch(f){\n-            case 1:f+=1;\n-            case 2:f+=2;\n-            case 3:f+=3;break;\n-            case 4:f+=4;\n-            default:f=0;\n-        } return f }\n+            function f(x) { switch(x){\n+                case 1:x+=1;\n+                case 2:x+=2;\n+                case 3:x+=3;break;\n+                case 4:x+=4;\n+                default:x=0;\n+            } return x }\n         ''')\n-        self.assertEqual(jsi.call_function('x', 1), 7)\n-        self.assertEqual(jsi.call_function('x', 3), 6)\n-        self.assertEqual(jsi.call_function('x', 5), 0)\n+        self._test(jsi, 7, args=[1])\n+        self._test(jsi, 6, args=[3])\n+        self._test(jsi, 0, args=[5])\n \n     def test_switch_default(self):\n         jsi = JSInterpreter('''\n-        function x(f) { switch(f){\n-            case 2: f+=2;\n-            default: f-=1;\n-            case 5:\n-            case 6: f+=6;\n-            case 0: break;\n-            case 1: f+=1;\n-        } return f }\n+            function f(x) { switch(x){\n+                case 2: x+=2;\n+                default: x-=1;\n+                case 5:\n+                case 6: x+=6;\n+                case 0: break;\n+                case 1: x+=1;\n+            } return x }\n         ''')\n-        self.assertEqual(jsi.call_function('x', 1), 2)\n-        self.assertEqual(jsi.call_function('x', 5), 11)\n-        self.assertEqual(jsi.call_function('x', 9), 14)\n+        self._test(jsi, 2, args=[1])\n+        self._test(jsi, 11, args=[5])\n+        self._test(jsi, 14, args=[9])\n \n     def test_try(self):\n-        jsi = JSInterpreter('''\n-        function x() { try{return 10} catch(e){return 5} }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 10)\n+        self._test('function f() { try{return 10} catch(e){return 5} }', 10)\n \n     def test_catch(self):\n-        jsi = JSInterpreter('''\n-        function x() { try{throw 10} catch(e){return 5} }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 5)\n+        self._test('function f() { try{throw 10} catch(e){return 5} }', 5)\n \n     def test_finally(self):\n-        jsi = JSInterpreter('''\n-        function x() { try{throw 10} finally {return 42} }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 42)\n-        jsi = JSInterpreter('''\n-        function x() { try{throw 10} catch(e){return 5} finally {return 42} }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 42)\n+        self._test('function f() { try{throw 10} finally {return 42} }', 42)\n+        self._test('function f() { try{throw 10} catch(e){return 5} finally {return 42} }', 42)\n \n     def test_nested_try(self):\n-        jsi = JSInterpreter('''\n-        function x() {try {\n-            try{throw 10} finally {throw 42}\n+        self._test('''\n+            function f() {try {\n+                try{throw 10} finally {throw 42}\n             } catch(e){return 5} }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 5)\n+        ''', 5)\n \n     def test_for_loop_continue(self):\n-        jsi = JSInterpreter('''\n-        function x() { a=0; for (i=0; i-10; i++) { continue; a++ } return a }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 0)\n+        self._test('function f() { a=0; for (i=0; i-10; i++) { continue; a++ } return a }', 0)\n \n     def test_for_loop_break(self):\n-        jsi = JSInterpreter('''\n-        function x() { a=0; for (i=0; i-10; i++) { break; a++ } return a }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 0)\n+        self._test('function f() { a=0; for (i=0; i-10; i++) { break; a++ } return a }', 0)\n \n     def test_for_loop_try(self):\n-        jsi = JSInterpreter('''\n-        function x() {\n-            for (i=0; i-10; i++) { try { if (i == 5) throw i} catch {return 10} finally {break} };\n-            return 42 }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 42)\n+        self._test('''\n+            function f() {\n+                for (i=0; i-10; i++) { try { if (i == 5) throw i} catch {return 10} finally {break} };\n+                return 42 }\n+        ''', 42)\n \n     def test_literal_list(self):\n-        jsi = JSInterpreter('''\n-        function x() { return [1, 2, \"asdf\", [5, 6, 7]][3] }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), [5, 6, 7])\n+        self._test('function f() { return [1, 2, \"asdf\", [5, 6, 7]][3] }', [5, 6, 7])\n \n     def test_comma(self):\n-        jsi = JSInterpreter('''\n-        function x() { a=5; a -= 1, a+=3; return a }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 7)\n-        jsi = JSInterpreter('''\n-        function x() { a=5; return (a -= 1, a+=3, a); }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 7)\n-\n-        jsi = JSInterpreter('''\n-        function x() { return (l=[0,1,2,3], function(a, b){return a+b})((l[1], l[2]), l[3]) }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 5)\n+        self._test('function f() { a=5; a -= 1, a+=3; return a }', 7)\n+        self._test('function f() { a=5; return (a -= 1, a+=3, a); }', 7)\n+        self._test('function f() { return (l=[0,1,2,3], function(a, b){return a+b})((l[1], l[2]), l[3]) }', 5)\n \n     def test_void(self):\n-        jsi = JSInterpreter('''\n-        function x() { return void 42; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), None)\n+        self._test('function f() { return void 42; }', None)\n \n     def test_return_function(self):\n         jsi = JSInterpreter('''\n@@ -387,110 +275,60 @@ def test_return_function(self):\n         self.assertEqual(jsi.call_function('x')([]), 1)\n \n     def test_null(self):\n-        jsi = JSInterpreter('''\n-        function x() { return null; }\n-        ''')\n-        self.assertIs(jsi.call_function('x'), None)\n-\n-        jsi = JSInterpreter('''\n-        function x() { return [null > 0, null < 0, null == 0, null === 0]; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), [False, False, False, False])\n-\n-        jsi = JSInterpreter('''\n-        function x() { return [null >= 0, null <= 0]; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), [True, True])\n+        self._test('function f() { return null; }', None)\n+        self._test('function f() { return [null > 0, null < 0, null == 0, null === 0]; }',\n+                   [False, False, False, False])\n+        self._test('function f() { return [null >= 0, null <= 0]; }', [True, True])\n \n     def test_undefined(self):\n-        jsi = JSInterpreter('''\n-        function x() { return undefined === undefined; }\n-        ''')\n-        self.assertTrue(jsi.call_function('x'))\n-\n-        jsi = JSInterpreter('''\n-        function x() { return undefined; }\n-        ''')\n-        self.assertIs(jsi.call_function('x'), JS_Undefined)\n-\n-        jsi = JSInterpreter('''\n-        function x() { let v; return v; }\n-        ''')\n-        self.assertIs(jsi.call_function('x'), JS_Undefined)\n-\n-        jsi = JSInterpreter('''\n-        function x() { return [undefined === undefined, undefined == undefined, undefined < undefined, undefined > undefined]; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), [True, True, False, False])\n-\n-        jsi = JSInterpreter('''\n-        function x() { return [undefined === 0, undefined == 0, undefined < 0, undefined > 0]; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), [False, False, False, False])\n-\n-        jsi = JSInterpreter('''\n-        function x() { return [undefined >= 0, undefined <= 0]; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), [False, False])\n-\n-        jsi = JSInterpreter('''\n-        function x() { return [undefined > null, undefined < null, undefined == null, undefined === null]; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), [False, False, True, False])\n-\n-        jsi = JSInterpreter('''\n-        function x() { return [undefined === null, undefined == null, undefined < null, undefined > null]; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), [False, True, False, False])\n-\n-        jsi = JSInterpreter('''\n-        function x() { let v; return [42+v, v+42, v**42, 42**v, 0**v]; }\n+        self._test('function f() { return undefined === undefined; }', True)\n+        self._test('function f() { return undefined; }', JS_Undefined)\n+        self._test('function f() {return undefined ?? 42; }', 42)\n+        self._test('function f() { let v; return v; }', JS_Undefined)\n+        self._test('function f() { let v; return v**0; }', 1)\n+        self._test('function f() { let v; return [v>42, v<=42, v&&42, 42&&v]; }',\n+                   [False, False, JS_Undefined, JS_Undefined])\n+\n+        self._test('''\n+            function f() { return [\n+                undefined === undefined,\n+                undefined == undefined,\n+                undefined == null\n+            ]; }\n+        ''', [True] * 3)\n+        self._test('''\n+            function f() { return [\n+                undefined < undefined,\n+                undefined > undefined,\n+                undefined === 0,\n+                undefined == 0,\n+                undefined < 0,\n+                undefined > 0,\n+                undefined >= 0,\n+                undefined <= 0,\n+                undefined > null,\n+                undefined < null,\n+                undefined === null\n+            ]; }\n+        ''', [False] * 11)\n+\n+        jsi = JSInterpreter('''\n+            function x() { let v; return [42+v, v+42, v**42, 42**v, 0**v]; }\n         ''')\n         for y in jsi.call_function('x'):\n             self.assertTrue(math.isnan(y))\n \n-        jsi = JSInterpreter('''\n-        function x() { let v; return v**0; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 1)\n-\n-        jsi = JSInterpreter('''\n-        function x() { let v; return [v>42, v<=42, v&&42, 42&&v]; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), [False, False, JS_Undefined, JS_Undefined])\n-\n-        jsi = JSInterpreter('function x(){return undefined ?? 42; }')\n-        self.assertEqual(jsi.call_function('x'), 42)\n-\n     def test_object(self):\n-        jsi = JSInterpreter('''\n-        function x() { return {}; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), {})\n-\n-        jsi = JSInterpreter('''\n-        function x() { let a = {m1: 42, m2: 0 }; return [a[\"m1\"], a.m2]; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), [42, 0])\n-\n-        jsi = JSInterpreter('''\n-        function x() { let a; return a?.qq; }\n-        ''')\n-        self.assertIs(jsi.call_function('x'), JS_Undefined)\n-\n-        jsi = JSInterpreter('''\n-        function x() { let a = {m1: 42, m2: 0 }; return a?.qq; }\n-        ''')\n-        self.assertIs(jsi.call_function('x'), JS_Undefined)\n+        self._test('function f() { return {}; }', {})\n+        self._test('function f() { let a = {m1: 42, m2: 0 }; return [a[\"m1\"], a.m2]; }', [42, 0])\n+        self._test('function f() { let a; return a?.qq; }', JS_Undefined)\n+        self._test('function f() { let a = {m1: 42, m2: 0 }; return a?.qq; }', JS_Undefined)\n \n     def test_regex(self):\n-        jsi = JSInterpreter('''\n-        function x() { let a=/,,[/,913,/](,)}/; }\n-        ''')\n-        self.assertIs(jsi.call_function('x'), None)\n+        self._test('function f() { let a=/,,[/,913,/](,)}/; }', None)\n \n         jsi = JSInterpreter('''\n-        function x() { let a=/,,[/,913,/](,)}/; \"\".replace(a, \"\"); return a; }\n+            function x() { let a=/,,[/,913,/](,)}/; \"\".replace(a, \"\"); return a; }\n         ''')\n         attrs = set(('findall', 'finditer', 'match', 'scanner', 'search',\n                      'split', 'sub', 'subn'))\n@@ -500,94 +338,92 @@ def test_regex(self):\n         self.assertSetEqual(set(dir(jsi.call_function('x'))) & attrs, attrs)\n \n         jsi = JSInterpreter('''\n-        function x() { let a=/,,[/,913,/](,)}/i; return a; }\n+            function x() { let a=/,,[/,913,/](,)}/i; return a; }\n         ''')\n         self.assertEqual(jsi.call_function('x').flags & ~re.U, re.I)\n \n-        jsi = JSInterpreter(r'''\n-        function x() { let a=\"data-name\".replace(\"data-\", \"\"); return a }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 'name')\n-\n-        jsi = JSInterpreter(r'''\n-        function x() { let a=\"data-name\".replace(new RegExp(\"^.+-\"), \"\"); return a; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 'name')\n-\n-        jsi = JSInterpreter(r'''\n-        function x() { let a=\"data-name\".replace(/^.+-/, \"\"); return a; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 'name')\n-\n-        jsi = JSInterpreter(r'''\n-        function x() { let a=\"data-name\".replace(/a/g, \"o\"); return a; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 'doto-nome')\n-\n-        jsi = JSInterpreter(r'''\n-        function x() { let a=\"data-name\".replaceAll(\"a\", \"o\"); return a; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 'doto-nome')\n+        jsi = JSInterpreter(r'function f() { let a=/,][}\",],()}(\\[)/; return a; }')\n+        self.assertEqual(jsi.call_function('f').pattern, r',][}\",],()}(\\[)')\n \n-        jsi = JSInterpreter(r'''\n-        function x() { let a=[/[)\\\\]/]; return a[0]; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x').pattern, r'[)\\\\]')\n+        jsi = JSInterpreter(r'function f() { let a=[/[)\\\\]/]; return a[0]; }')\n+        self.assertEqual(jsi.call_function('f').pattern, r'[)\\\\]')\n \n-        \"\"\"  # fails\n-        jsi = JSInterpreter(r'''\n-        function x() { let a=100; a/=/[0-9]+/.exec('divide by 20 today')[0]; }\n-        ''')\n-        self.assertEqual(jsi.call_function('x'), 5)\n-        \"\"\"\n+    def test_replace(self):\n+        self._test('function f() { let a=\"data-name\".replace(\"data-\", \"\"); return a }',\n+                   'name')\n+        self._test('function f() { let a=\"data-name\".replace(new RegExp(\"^.+-\"), \"\"); return a; }',\n+                   'name')\n+        self._test('function f() { let a=\"data-name\".replace(/^.+-/, \"\"); return a; }',\n+                   'name')\n+        self._test('function f() { let a=\"data-name\".replace(/a/g, \"o\"); return a; }',\n+                   'doto-nome')\n+        self._test('function f() { let a=\"data-name\".replaceAll(\"a\", \"o\"); return a; }',\n+                   'doto-nome')\n \n     def test_char_code_at(self):\n-        jsi = JSInterpreter('function x(i){return \"test\".charCodeAt(i)}')\n-        self.assertEqual(jsi.call_function('x', 0), 116)\n-        self.assertEqual(jsi.call_function('x', 1), 101)\n-        self.assertEqual(jsi.call_function('x', 2), 115)\n-        self.assertEqual(jsi.call_function('x', 3), 116)\n-        self.assertEqual(jsi.call_function('x', 4), None)\n-        self.assertEqual(jsi.call_function('x', 'not_a_number'), 116)\n+        jsi = JSInterpreter('function f(i){return \"test\".charCodeAt(i)}')\n+        self._test(jsi, 116, args=[0])\n+        self._test(jsi, 101, args=[1])\n+        self._test(jsi, 115, args=[2])\n+        self._test(jsi, 116, args=[3])\n+        self._test(jsi, None, args=[4])\n+        self._test(jsi, 116, args=['not_a_number'])\n \n     def test_bitwise_operators_overflow(self):\n-        jsi = JSInterpreter('function x(){return -524999584 << 5}')\n-        self.assertEqual(jsi.call_function('x'), 379882496)\n-\n-        jsi = JSInterpreter('function x(){return 1236566549 << 5}')\n-        self.assertEqual(jsi.call_function('x'), 915423904)\n-\n-    def test_bitwise_operators_madness(self):\n-        jsi = JSInterpreter('function x(){return null << 5}')\n-        self.assertEqual(jsi.call_function('x'), 0)\n-\n-        jsi = JSInterpreter('function x(){return undefined >> 5}')\n-        self.assertEqual(jsi.call_function('x'), 0)\n-\n-        jsi = JSInterpreter('function x(){return 42 << NaN}')\n-        self.assertEqual(jsi.call_function('x'), 42)\n-\n-        jsi = JSInterpreter('function x(){return 42 << Infinity}')\n-        self.assertEqual(jsi.call_function('x'), 42)\n+        self._test('function f(){return -524999584 << 5}', 379882496)\n+        self._test('function f(){return 1236566549 << 5}', 915423904)\n+\n+    def test_bitwise_operators_typecast(self):\n+        # madness\n+        self._test('function f(){return null << 5}', 0)\n+        self._test('function f(){return undefined >> 5}', 0)\n+        self._test('function f(){return 42 << NaN}', 42)\n+        self._test('function f(){return 42 << Infinity}', 42)\n+\n+    def test_negative(self):\n+        self._test('function f(){return 2    *    -2.0    ;}', -4)\n+        self._test('function f(){return 2    -    - -2    ;}', 0)\n+        self._test('function f(){return 2    -    - - -2  ;}', 4)\n+        self._test('function f(){return 2    -    + + - -2;}', 0)\n+        self._test('function f(){return 2    +    - + - -2;}', 0)\n \n     def test_32066(self):\n-        jsi = JSInterpreter(\"function x(){return Math.pow(3, 5) + new Date('1970-01-01T08:01:42.000+08:00') / 1000 * -239 - -24205;}\")\n-        self.assertEqual(jsi.call_function('x'), 70)\n-\n-    def test_unary_operators(self):\n-        jsi = JSInterpreter('function f(){return 2  -  - - 2;}')\n-        self.assertEqual(jsi.call_function('f'), 0)\n-        jsi = JSInterpreter('function f(){return 2 + - + - - 2;}')\n-        self.assertEqual(jsi.call_function('f'), 0)\n-        # https://github.com/ytdl-org/youtube-dl/issues/32815\n-        jsi = JSInterpreter('function f(){return 0  - 7 * - 6;}')\n-        self.assertEqual(jsi.call_function('f'), 42)\n+        self._test(\n+            \"function f(){return Math.pow(3, 5) + new Date('1970-01-01T08:01:42.000+08:00') / 1000 * -239 - -24205;}\",\n+            70)\n \n-    \"\"\" # fails so far\n+    @unittest.skip('Not yet working')\n     def test_packed(self):\n-        jsi = JSInterpreter('''function x(p,a,c,k,e,d){while(c--)if(k[c])p=p.replace(new RegExp('\\\\b'+c.toString(a)+'\\\\b','g'),k[c]);return p}''')\n-        self.assertEqual(jsi.call_function('x', '''h 7=g(\"1j\");7.7h({7g:[{33:\"w://7f-7e-7d-7c.v.7b/7a/79/78/77/76.74?t=73&s=2s&e=72&f=2t&71=70.0.0.1&6z=6y&6x=6w\"}],6v:\"w://32.v.u/6u.31\",16:\"r%\",15:\"r%\",6t:\"6s\",6r:\"\",6q:\"l\",6p:\"l\",6o:\"6n\",6m:\\'6l\\',6k:\"6j\",9:[{33:\"/2u?b=6i&n=50&6h=w://32.v.u/6g.31\",6f:\"6e\"}],1y:{6d:1,6c:\\'#6b\\',6a:\\'#69\\',68:\"67\",66:30,65:r,},\"64\":{63:\"%62 2m%m%61%5z%5y%5x.u%5w%5v%5u.2y%22 2k%m%1o%22 5t%m%1o%22 5s%m%1o%22 2j%m%5r%22 16%m%5q%22 15%m%5p%22 5o%2z%5n%5m%2z\",5l:\"w://v.u/d/1k/5k.2y\",5j:[]},\\'5i\\':{\"5h\":\"5g\"},5f:\"5e\",5d:\"w://v.u\",5c:{},5b:l,1x:[0.25,0.50,0.75,1,1.25,1.5,2]});h 1m,1n,5a;h 59=0,58=0;h 7=g(\"1j\");h 2x=0,57=0,56=0;$.55({54:{\\'53-52\\':\\'2i-51\\'}});7.j(\\'4z\\',6(x){c(5>0&&x.1l>=5&&1n!=1){1n=1;$(\\'q.4y\\').4x(\\'4w\\')}});7.j(\\'13\\',6(x){2x=x.1l});7.j(\\'2g\\',6(x){2w(x)});7.j(\\'4v\\',6(){$(\\'q.2v\\').4u()});6 2w(x){$(\\'q.2v\\').4t();c(1m)19;1m=1;17=0;c(4s.4r===l){17=1}$.4q(\\'/2u?b=4p&2l=1k&4o=2t-4n-4m-2s-4l&4k=&4j=&4i=&17=\\'+17,6(2r){$(\\'#4h\\').4g(2r)});$(\\'.3-8-4f-4e:4d(\"4c\")\\').2h(6(e){2q();g().4b(0);g().4a(l)});6 2q(){h $14=$(\"<q />\").2p({1l:\"49\",16:\"r%\",15:\"r%\",48:0,2n:0,2o:47,46:\"45(10%, 10%, 10%, 0.4)\",\"44-43\":\"42\"});$(\"<41 />\").2p({16:\"60%\",15:\"60%\",2o:40,\"3z-2n\":\"3y\"}).3x({\\'2m\\':\\'/?b=3w&2l=1k\\',\\'2k\\':\\'0\\',\\'2j\\':\\'2i\\'}).2f($14);$14.2h(6(){$(3v).3u();g().2g()});$14.2f($(\\'#1j\\'))}g().13(0);}6 3t(){h 9=7.1b(2e);2d.2c(9);c(9.n>1){1r(i=0;i<9.n;i++){c(9[i].1a==2e){2d.2c(\\'!!=\\'+i);7.1p(i)}}}}7.j(\\'3s\\',6(){g().1h(\"/2a/3r.29\",\"3q 10 28\",6(){g().13(g().27()+10)},\"2b\");$(\"q[26=2b]\").23().21(\\'.3-20-1z\\');g().1h(\"/2a/3p.29\",\"3o 10 28\",6(){h 12=g().27()-10;c(12<0)12=0;g().13(12)},\"24\");$(\"q[26=24]\").23().21(\\'.3-20-1z\\');});6 1i(){}7.j(\\'3n\\',6(){1i()});7.j(\\'3m\\',6(){1i()});7.j(\"k\",6(y){h 9=7.1b();c(9.n<2)19;$(\\'.3-8-3l-3k\\').3j(6(){$(\\'#3-8-a-k\\').1e(\\'3-8-a-z\\');$(\\'.3-a-k\\').p(\\'o-1f\\',\\'11\\')});7.1h(\"/3i/3h.3g\",\"3f 3e\",6(){$(\\'.3-1w\\').3d(\\'3-8-1v\\');$(\\'.3-8-1y, .3-8-1x\\').p(\\'o-1g\\',\\'11\\');c($(\\'.3-1w\\').3c(\\'3-8-1v\\')){$(\\'.3-a-k\\').p(\\'o-1g\\',\\'l\\');$(\\'.3-a-k\\').p(\\'o-1f\\',\\'l\\');$(\\'.3-8-a\\').1e(\\'3-8-a-z\\');$(\\'.3-8-a:1u\\').3b(\\'3-8-a-z\\')}3a{$(\\'.3-a-k\\').p(\\'o-1g\\',\\'11\\');$(\\'.3-a-k\\').p(\\'o-1f\\',\\'11\\');$(\\'.3-8-a:1u\\').1e(\\'3-8-a-z\\')}},\"39\");7.j(\"38\",6(y){1d.37(\\'1c\\',y.9[y.36].1a)});c(1d.1t(\\'1c\\')){35(\"1s(1d.1t(\\'1c\\'));\",34)}});h 18;6 1s(1q){h 9=7.1b();c(9.n>1){1r(i=0;i<9.n;i++){c(9[i].1a==1q){c(i==18){19}18=i;7.1p(i)}}}}',36,270,'|||jw|||function|player|settings|tracks|submenu||if||||jwplayer|var||on|audioTracks|true|3D|length|aria|attr|div|100|||sx|filemoon|https||event|active||false|tt|seek|dd|height|width|adb|current_audio|return|name|getAudioTracks|default_audio|localStorage|removeClass|expanded|checked|addButton|callMeMaybe|vplayer|0fxcyc2ajhp1|position|vvplay|vvad|220|setCurrentAudioTrack|audio_name|for|audio_set|getItem|last|open|controls|playbackRates|captions|rewind|icon|insertAfter||detach|ff00||button|getPosition|sec|png|player8|ff11|log|console|track_name|appendTo|play|click|no|scrolling|frameborder|file_code|src|top|zIndex|css|showCCform|data|1662367683|383371|dl|video_ad|doPlay|prevt|mp4|3E||jpg|thumbs|file|300|setTimeout|currentTrack|setItem|audioTrackChanged|dualSound|else|addClass|hasClass|toggleClass|Track|Audio|svg|dualy|images|mousedown|buttons|topbar|playAttemptFailed|beforePlay|Rewind|fr|Forward|ff|ready|set_audio_track|remove|this|upload_srt|prop|50px|margin|1000001|iframe|center|align|text|rgba|background|1000000|left|absolute|pause|setCurrentCaptions|Upload|contains|item|content|html|fviews|referer|prem|embed|3e57249ef633e0d03bf76ceb8d8a4b65|216|83|hash|view|get|TokenZir|window|hide|show|complete|slow|fadeIn|video_ad_fadein|time||cache|Cache|Content|headers|ajaxSetup|v2done|tott|vastdone2|vastdone1|vvbefore|playbackRateControls|cast|aboutlink|FileMoon|abouttext|UHD|1870|qualityLabels|sites|GNOME_POWER|link|2Fiframe|3C|allowfullscreen|22360|22640|22no|marginheight|marginwidth|2FGNOME_POWER|2F0fxcyc2ajhp1|2Fe|2Ffilemoon|2F|3A||22https|3Ciframe|code|sharing|fontOpacity|backgroundOpacity|Tahoma|fontFamily|303030|backgroundColor|FFFFFF|color|userFontScale|thumbnails|kind|0fxcyc2ajhp10000|url|get_slides|start|startparam|none|preload|html5|primary|hlshtml|androidhls|duration|uniform|stretching|0fxcyc2ajhp1_xt|image|2048|sp|6871|asn|127|srv|43200|_g3XlBcu2lmD9oDexD2NLWSmah2Nu3XcDrl93m9PwXY|m3u8||master|0fxcyc2ajhp1_x|00076|01|hls2|to|s01|delivery|storage|moon|sources|setup'''.split('|')))\n-    \"\"\"\n+        self._test(\n+            '''function f(p,a,c,k,e,d){while(c--)if(k[c])p=p.replace(new RegExp('\\\\b'+c.toString(a)+'\\\\b','g'),k[c]);return p}''',\n+            '''h 7=g(\"1j\");7.7h({7g:[{33:\"w://7f-7e-7d-7c.v.7b/7a/79/78/77/76.74?t=73&s=2s&e=72&f=2t&71=70.0.0.1&6z=6y&6x=6w\"}],6v:\"w://32.v.u/6u.31\",16:\"r%\",15:\"r%\",6t:\"6s\",6r:\"\",6q:\"l\",6p:\"l\",6o:\"6n\",6m:\\'6l\\',6k:\"6j\",9:[{33:\"/2u?b=6i&n=50&6h=w://32.v.u/6g.31\",6f:\"6e\"}],1y:{6d:1,6c:\\'#6b\\',6a:\\'#69\\',68:\"67\",66:30,65:r,},\"64\":{63:\"%62 2m%m%61%5z%5y%5x.u%5w%5v%5u.2y%22 2k%m%1o%22 5t%m%1o%22 5s%m%1o%22 2j%m%5r%22 16%m%5q%22 15%m%5p%22 5o%2z%5n%5m%2z\",5l:\"w://v.u/d/1k/5k.2y\",5j:[]},\\'5i\\':{\"5h\":\"5g\"},5f:\"5e\",5d:\"w://v.u\",5c:{},5b:l,1x:[0.25,0.50,0.75,1,1.25,1.5,2]});h 1m,1n,5a;h 59=0,58=0;h 7=g(\"1j\");h 2x=0,57=0,56=0;$.55({54:{\\'53-52\\':\\'2i-51\\'}});7.j(\\'4z\\',6(x){c(5>0&&x.1l>=5&&1n!=1){1n=1;$(\\'q.4y\\').4x(\\'4w\\')}});7.j(\\'13\\',6(x){2x=x.1l});7.j(\\'2g\\',6(x){2w(x)});7.j(\\'4v\\',6(){$(\\'q.2v\\').4u()});6 2w(x){$(\\'q.2v\\').4t();c(1m)19;1m=1;17=0;c(4s.4r===l){17=1}$.4q(\\'/2u?b=4p&2l=1k&4o=2t-4n-4m-2s-4l&4k=&4j=&4i=&17=\\'+17,6(2r){$(\\'#4h\\').4g(2r)});$(\\'.3-8-4f-4e:4d(\"4c\")\\').2h(6(e){2q();g().4b(0);g().4a(l)});6 2q(){h $14=$(\"<q />\").2p({1l:\"49\",16:\"r%\",15:\"r%\",48:0,2n:0,2o:47,46:\"45(10%, 10%, 10%, 0.4)\",\"44-43\":\"42\"});$(\"<41 />\").2p({16:\"60%\",15:\"60%\",2o:40,\"3z-2n\":\"3y\"}).3x({\\'2m\\':\\'/?b=3w&2l=1k\\',\\'2k\\':\\'0\\',\\'2j\\':\\'2i\\'}).2f($14);$14.2h(6(){$(3v).3u();g().2g()});$14.2f($(\\'#1j\\'))}g().13(0);}6 3t(){h 9=7.1b(2e);2d.2c(9);c(9.n>1){1r(i=0;i<9.n;i++){c(9[i].1a==2e){2d.2c(\\'!!=\\'+i);7.1p(i)}}}}7.j(\\'3s\\',6(){g().1h(\"/2a/3r.29\",\"3q 10 28\",6(){g().13(g().27()+10)},\"2b\");$(\"q[26=2b]\").23().21(\\'.3-20-1z\\');g().1h(\"/2a/3p.29\",\"3o 10 28\",6(){h 12=g().27()-10;c(12<0)12=0;g().13(12)},\"24\");$(\"q[26=24]\").23().21(\\'.3-20-1z\\');});6 1i(){}7.j(\\'3n\\',6(){1i()});7.j(\\'3m\\',6(){1i()});7.j(\"k\",6(y){h 9=7.1b();c(9.n<2)19;$(\\'.3-8-3l-3k\\').3j(6(){$(\\'#3-8-a-k\\').1e(\\'3-8-a-z\\');$(\\'.3-a-k\\').p(\\'o-1f\\',\\'11\\')});7.1h(\"/3i/3h.3g\",\"3f 3e\",6(){$(\\'.3-1w\\').3d(\\'3-8-1v\\');$(\\'.3-8-1y, .3-8-1x\\').p(\\'o-1g\\',\\'11\\');c($(\\'.3-1w\\').3c(\\'3-8-1v\\')){$(\\'.3-a-k\\').p(\\'o-1g\\',\\'l\\');$(\\'.3-a-k\\').p(\\'o-1f\\',\\'l\\');$(\\'.3-8-a\\').1e(\\'3-8-a-z\\');$(\\'.3-8-a:1u\\').3b(\\'3-8-a-z\\')}3a{$(\\'.3-a-k\\').p(\\'o-1g\\',\\'11\\');$(\\'.3-a-k\\').p(\\'o-1f\\',\\'11\\');$(\\'.3-8-a:1u\\').1e(\\'3-8-a-z\\')}},\"39\");7.j(\"38\",6(y){1d.37(\\'1c\\',y.9[y.36].1a)});c(1d.1t(\\'1c\\')){35(\"1s(1d.1t(\\'1c\\'));\",34)}});h 18;6 1s(1q){h 9=7.1b();c(9.n>1){1r(i=0;i<9.n;i++){c(9[i].1a==1q){c(i==18){19}18=i;7.1p(i)}}}}',36,270,'|||jw|||function|player|settings|tracks|submenu||if||||jwplayer|var||on|audioTracks|true|3D|length|aria|attr|div|100|||sx|filemoon|https||event|active||false|tt|seek|dd|height|width|adb|current_audio|return|name|getAudioTracks|default_audio|localStorage|removeClass|expanded|checked|addButton|callMeMaybe|vplayer|0fxcyc2ajhp1|position|vvplay|vvad|220|setCurrentAudioTrack|audio_name|for|audio_set|getItem|last|open|controls|playbackRates|captions|rewind|icon|insertAfter||detach|ff00||button|getPosition|sec|png|player8|ff11|log|console|track_name|appendTo|play|click|no|scrolling|frameborder|file_code|src|top|zIndex|css|showCCform|data|1662367683|383371|dl|video_ad|doPlay|prevt|mp4|3E||jpg|thumbs|file|300|setTimeout|currentTrack|setItem|audioTrackChanged|dualSound|else|addClass|hasClass|toggleClass|Track|Audio|svg|dualy|images|mousedown|buttons|topbar|playAttemptFailed|beforePlay|Rewind|fr|Forward|ff|ready|set_audio_track|remove|this|upload_srt|prop|50px|margin|1000001|iframe|center|align|text|rgba|background|1000000|left|absolute|pause|setCurrentCaptions|Upload|contains|item|content|html|fviews|referer|prem|embed|3e57249ef633e0d03bf76ceb8d8a4b65|216|83|hash|view|get|TokenZir|window|hide|show|complete|slow|fadeIn|video_ad_fadein|time||cache|Cache|Content|headers|ajaxSetup|v2done|tott|vastdone2|vastdone1|vvbefore|playbackRateControls|cast|aboutlink|FileMoon|abouttext|UHD|1870|qualityLabels|sites|GNOME_POWER|link|2Fiframe|3C|allowfullscreen|22360|22640|22no|marginheight|marginwidth|2FGNOME_POWER|2F0fxcyc2ajhp1|2Fe|2Ffilemoon|2F|3A||22https|3Ciframe|code|sharing|fontOpacity|backgroundOpacity|Tahoma|fontFamily|303030|backgroundColor|FFFFFF|color|userFontScale|thumbnails|kind|0fxcyc2ajhp10000|url|get_slides|start|startparam|none|preload|html5|primary|hlshtml|androidhls|duration|uniform|stretching|0fxcyc2ajhp1_xt|image|2048|sp|6871|asn|127|srv|43200|_g3XlBcu2lmD9oDexD2NLWSmah2Nu3XcDrl93m9PwXY|m3u8||master|0fxcyc2ajhp1_x|00076|01|hls2|to|s01|delivery|storage|moon|sources|setup'''.split('|'))\n+\n+    def test_join(self):\n+        test_input = list('test')\n+        tests = [\n+            'function f(a, b){return a.join(b)}',\n+            'function f(a, b){return Array.prototype.join.call(a, b)}',\n+            'function f(a, b){return Array.prototype.join.apply(a, [b])}',\n+        ]\n+        for test in tests:\n+            jsi = JSInterpreter(test)\n+            self._test(jsi, 'test', args=[test_input, ''])\n+            self._test(jsi, 't-e-s-t', args=[test_input, '-'])\n+            self._test(jsi, '', args=[[], '-'])\n+\n+    def test_split(self):\n+        test_result = list('test')\n+        tests = [\n+            'function f(a, b){return a.split(b)}',\n+            'function f(a, b){return String.prototype.split.call(a, b)}',\n+            'function f(a, b){return String.prototype.split.apply(a, [b])}',\n+        ]\n+        for test in tests:\n+            jsi = JSInterpreter(test)\n+            self._test(jsi, test_result, args=['test', ''])\n+            self._test(jsi, test_result, args=['t-e-s-t', '-'])\n+            self._test(jsi, [''], args=['', '-'])\n+            self._test(jsi, [], args=['', ''])\n \n \n if __name__ == '__main__':\ndiff --git a/test/test_youtube_signature.py b/test/test_youtube_signature.py\nindex cafba7a5cdd..cc18d0f7be3 100644\n--- a/test/test_youtube_signature.py\n+++ b/test/test_youtube_signature.py\n@@ -162,6 +162,10 @@\n         'https://www.youtube.com/s/player/590f65a6/player_ias.vflset/en_US/base.js',\n         '1tm7-g_A9zsI8_Lay_', 'xI4Vem4Put_rOg',\n     ),\n+    (\n+        'https://www.youtube.com/s/player/b22ef6e7/player_ias.vflset/en_US/base.js',\n+        'b6HcntHGkvBLk_FRf', 'kNPW6A7FyP2l8A',\n+    ),\n ]\n \n \n", "problem_statement": "[YouTube] Unable to extract nsig jsi ...\n<!--\r\n\r\n######################################################################\r\n  WARNING!\r\n  IGNORING THE FOLLOWING TEMPLATE WILL RESULT IN ISSUE CLOSED AS INCOMPLETE\r\n######################################################################\r\n\r\n-->\r\n\r\n\r\n## Checklist\r\n\r\n<!--\r\nCarefully read and work through this check list in order to prevent the most common mistakes and misuse of youtube-dl:\r\n- First of, make sure you are using the latest version of youtube-dl. Run `youtube-dl --version` and ensure your version is 2021.12.17. If it's not, see https://yt-dl.org/update on how to update. Issues with outdated version will be REJECTED.\r\n- Make sure that all provided video/audio/playlist URLs (if any) are alive and playable in a browser.\r\n- Make sure that all URLs and arguments with special characters are properly quoted or escaped as explained in http://yt-dl.org/escape.\r\n- Search the bugtracker for similar issues: http://yt-dl.org/search-issues. DO NOT post duplicates.\r\n- Finally, put x into all relevant boxes (like this [x])\r\n-->\r\n\r\n- [x] I'm reporting a broken site support\r\n- [x] I've verified that I'm running youtube-dl version **2021.12.17**\r\n- [x] I've checked that all provided URLs are alive and playable in a browser\r\n- [x] I've checked that all URLs and arguments with special characters are properly quoted or escaped\r\n- [x] I've searched the bugtracker for similar issues including closed ones\r\n\r\n\r\n## Verbose log\r\n\r\n<!--\r\nProvide the complete verbose output of youtube-dl that clearly demonstrates the problem.\r\nAdd the `-v` flag to your command line you run youtube-dl with (`youtube-dl -v <your command line>`), copy the WHOLE output and insert it below. It should look similar to this:\r\n [debug] System config: []\r\n [debug] User config: []\r\n [debug] Command-line args: [u'-v', u'http://www.youtube.com/watch?v=BaW_jenozKcj']\r\n [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251\r\n [debug] youtube-dl version 2021.12.17\r\n [debug] Python version 2.7.11 - Windows-2003Server-5.2.3790-SP2\r\n [debug] exe versions: ffmpeg N-75573-g1d0487f, ffprobe N-75573-g1d0487f, rtmpdump 2.4\r\n [debug] Proxy map: {}\r\n <more lines>\r\n-->\r\n\r\n```\r\n\r\n==========================\r\nTESTING NORMAL YOUTUBE-DL:\r\n==========================\r\n\r\n\r\n[debug] System config: []\r\n[debug] User config: ['--no-mtime', '--match-filter', '!is_live', '--retries', 'infinite', '--fragment-retries', '3', '--skip-unavailable-fragments', '--restrict-filenames', '-i', '-o', '/home/gregorius/home/pending/videos/%(title)s___%(id)s.webm', '-f', '(bestvideo[height<=360]+worstaudio/best[height<=360])[protocol!=http_dash_segments][container!^=dash]', '--console-title', '--hls-prefer-native', '--no-cache-dir', '--http-chunk-size', '100M', '--cookies', '/home/gregorius/home/scripts/video/youtube-dl-cookies']\r\n[debug] Custom config: []\r\n[debug] Command-line args: ['https://www.youtube.com/watch?v=zPHM0q0xgFg', '-vf', '18', '--no-playlist', '-o', '/home/gregorius/home/scripts/video/TEST_NORMAL_%(title)s___%(id)s.webm']\r\n[debug] Encodings: locale UTF-8, fs utf-8, out utf-8, pref UTF-8\r\n[debug] youtube-dl version 2021.12.17\r\n[debug] Single file build\r\n[debug] Python 3.10.12 (CPython x86_64 64bit) - Linux-5.15.0-112-generic-x86_64-with-glibc2.35 - OpenSSL 3.0.2 15 Mar 2022 - glibc 2.35\r\n[debug] exe versions: ffmpeg 4.4.2, ffprobe 4.4.2, rtmpdump 2.4\r\n[debug] Proxy map: {}\r\n[youtube] zPHM0q0xgFg: Downloading webpage\r\n[youtube] Downloading just video zPHM0q0xgFg because of --no-playlist\r\n[youtube] zPHM0q0xgFg: Downloading player b22ef6e7\r\nERROR: Unable to extract nsig jsi, player_id, func_codefunction code (caused by RegexNotFoundError('Unable to extract \\x1b[0;34mInitial JS player n function name\\x1b[0m; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.')); please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\nTraceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1637, in _decrypt_nsig\r\n    jsi, player_id, func_code = self._extract_n_function_code(video_id, player_url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1680, in _extract_n_function_code\r\n    func_name = self._extract_n_function_name(jscode)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1660, in _extract_n_function_name\r\n    func_name, idx = self._search_regex(\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/common.py\", line 1101, in _search_regex\r\n    raise RegexNotFoundError('Unable to extract %s' % _name)\r\nyoutube_dl.utils.RegexNotFoundError: Unable to extract Initial JS player n function name; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\nTraceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1637, in _decrypt_nsig\r\n    jsi, player_id, func_code = self._extract_n_function_code(video_id, player_url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1680, in _extract_n_function_code\r\n    func_name = self._extract_n_function_name(jscode)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1660, in _extract_n_function_name\r\n    func_name, idx = self._search_regex(\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/common.py\", line 1101, in _search_regex\r\n    raise RegexNotFoundError('Unable to extract %s' % _name)\r\nyoutube_dl.utils.RegexNotFoundError: Unable to extract Initial JS player n function name; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/YoutubeDL.py\", line 875, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/YoutubeDL.py\", line 971, in __extract_info\r\n    ie_result = ie.extract(url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/common.py\", line 571, in extract\r\n    ie_result = self._real_extract(url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 2108, in _real_extract\r\n    self._unthrottle_format_urls(video_id, player_url, dct)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1725, in _unthrottle_format_urls\r\n    n_response = decrypt_nsig(n_param)(n_param, video_id, player_url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1614, in inner\r\n    raise ret\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1606, in inner\r\n    self._player_cache[cache_id] = func(*args, **kwargs)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1639, in _decrypt_nsig\r\n    raise ExtractorError('Unable to extract nsig jsi, player_id, func_codefunction code', cause=e)\r\nyoutube_dl.utils.ExtractorError: Unable to extract nsig jsi, player_id, func_codefunction code (caused by RegexNotFoundError('Unable to extract \\x1b[0;34mInitial JS player n function name\\x1b[0m; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.')); please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n\r\n\r\n===========================\r\nTESTING NIGHTLY YOUTUBE-DL:\r\n===========================\r\n\r\n\r\n[debug] System config: []\r\n[debug] User config: ['--no-mtime', '--match-filter', '!is_live', '--retries', 'infinite', '--fragment-retries', '3', '--skip-unavailable-fragments', '--restrict-filenames', '-i', '-o', '/home/gregorius/home/pending/videos/%(title)s___%(id)s.webm', '-f', '(bestvideo[height<=360]+worstaudio/best[height<=360])[protocol!=http_dash_segments][container!^=dash]', '--console-title', '--hls-prefer-native', '--no-cache-dir', '--http-chunk-size', '100M', '--cookies', '/home/gregorius/home/scripts/video/youtube-dl-cookies']\r\n[debug] Custom config: []\r\n[debug] Command-line args: ['https://www.youtube.com/watch?v=zPHM0q0xgFg', '-vf', '18', '--no-playlist', '-o', '/home/gregorius/home/scripts/video/TEST_NIGHTLY_%(title)s___%(id)s.webm']\r\n[debug] Encodings: locale UTF-8, fs utf-8, out utf-8, pref UTF-8\r\n[debug] youtube-dl version 2024.07.08 [a452f9437] (single file build)\r\n[debug] ** This version was built from the latest master code at https://github.com/ytdl-org/youtube-dl.\r\n[debug] ** For support, visit the main site.\r\n[debug] Python 3.10.12 (CPython x86_64 64bit) - Linux-5.15.0-112-generic-x86_64-with-glibc2.35 - OpenSSL 3.0.2 15 Mar 2022 - glibc 2.35\r\n[debug] exe versions: ffmpeg 4.4.2, ffprobe 4.4.2, rtmpdump 2.4\r\n[debug] Proxy map: {}\r\n[youtube] zPHM0q0xgFg: Downloading webpage\r\n[youtube] Downloading just video zPHM0q0xgFg because of --no-playlist\r\n[youtube] zPHM0q0xgFg: Downloading player b22ef6e7\r\nERROR: Unable to extract nsig jsi, player_id, func_codefunction code (caused by RegexNotFoundError('Unable to extract \\x1b[0;34mInitial JS player n function name\\x1b[0m; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.')); please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\nTraceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1637, in _decrypt_nsig\r\n    jsi, player_id, func_code = self._extract_n_function_code(video_id, player_url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1680, in _extract_n_function_code\r\n    func_name = self._extract_n_function_name(jscode)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1660, in _extract_n_function_name\r\n    func_name, idx = self._search_regex(\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/common.py\", line 1101, in _search_regex\r\n    raise RegexNotFoundError('Unable to extract %s' % _name)\r\nyoutube_dl.utils.RegexNotFoundError: Unable to extract Initial JS player n function name; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\nTraceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1637, in _decrypt_nsig\r\n    jsi, player_id, func_code = self._extract_n_function_code(video_id, player_url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1680, in _extract_n_function_code\r\n    func_name = self._extract_n_function_name(jscode)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1660, in _extract_n_function_name\r\n    func_name, idx = self._search_regex(\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/common.py\", line 1101, in _search_regex\r\n    raise RegexNotFoundError('Unable to extract %s' % _name)\r\nyoutube_dl.utils.RegexNotFoundError: Unable to extract Initial JS player n function name; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/YoutubeDL.py\", line 879, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/YoutubeDL.py\", line 975, in __extract_info\r\n    ie_result = ie.extract(url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/common.py\", line 571, in extract\r\n    ie_result = self._real_extract(url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 2108, in _real_extract\r\n    self._unthrottle_format_urls(video_id, player_url, dct)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1725, in _unthrottle_format_urls\r\n    n_response = decrypt_nsig(n_param)(n_param, video_id, player_url)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1614, in inner\r\n    raise ret\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1606, in inner\r\n    self._player_cache[cache_id] = func(*args, **kwargs)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1639, in _decrypt_nsig\r\n    raise ExtractorError('Unable to extract nsig jsi, player_id, func_codefunction code', cause=e)\r\nyoutube_dl.utils.ExtractorError: Unable to extract nsig jsi, player_id, func_codefunction code (caused by RegexNotFoundError('Unable to extract \\x1b[0;34mInitial JS player n function name\\x1b[0m; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.')); please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n```\r\n\r\n\r\n## Description\r\n\r\n<!--\r\nProvide an explanation of your issue in an arbitrary form. Provide any additional information, suggested solution and as much context and examples as possible.\r\nIf work on your issue requires account credentials please provide them or explain how one can obtain them.\r\n-->\r\n\r\nNew Error Message on Youtube, yay, time to provide the Devs with the Logs!\r\n\r\nUnable to extract nsig jsi, player_id, func_codefunction code (caused by RegexNotFoundError('Unable to extract \\x1b[0;34mInitial JS player n function name\\x1b[0m; \r\n\nfix to 'Unable to extract nsig jsi ... #32842'\nfix to 'Unable to extract nsig jsi ... #32842'\r\nthanks to @Duster98 \\@#issuecomment-2220376175\r\n\r\n## Please follow the guide below\r\n\r\n- You will be asked some questions, please read them **carefully** and answer honestly\r\n- Put an `x` into all the boxes [ ] relevant to your *pull request* (like that [x])\r\n- Use *Preview* tab to see how your *pull request* will actually look like\r\n\r\n---\r\n\r\n### Before submitting a *pull request* make sure you have:\r\n- [ x] [Searched](https://github.com/ytdl-org/youtube-dl/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests\r\n- [ n/a] Read [adding new extractor tutorial](https://github.com/ytdl-org/youtube-dl#adding-support-for-a-new-site)\r\n- [ x] Read [youtube-dl coding conventions](https://github.com/ytdl-org/youtube-dl#youtube-dl-coding-conventions) and adjusted the code to meet them\r\n- [ x] Covered the code with tests (note that PRs without tests will be REJECTED)\r\n- [ x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8)\r\n\r\n### In order to be accepted and merged into youtube-dl each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check one of the following options:\r\n- [ -] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)\r\n- [ x] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)\r\n\r\n### What is the purpose of your *pull request*?\r\n- [ x] Bug fix\r\n- [ ] Improvement\r\n- [ ] New extractor\r\n- [ ] New feature\r\n\r\n---\r\n\r\n### Description of your *pull request* and other information\r\n\r\nExplanation of your *pull request* in arbitrary form goes here. Please make sure the description explains the purpose and effect of your *pull request* and is worded well enough to be understood. Provide as much context and examples as possible.\r\n\r\nFix to issue #32842 [posted](https://github.com/ytdl-org/youtube-dl/issues/32842#issuecomment-2220376175) by @Duster98.\r\n\r\nCode checked and tested.\r\n\r\n```\r\n$ python youtube-dl.py -F -v https://www.youtube.com/watch?v=NjCVZ2TBlkw\r\n[debug] System config: []\r\n[debug] User config: []\r\n[debug] Custom config: []\r\n[debug] Command-line args: [u'-F', u'-v', u'https://www.youtube.com/watch?v=NjCVZ2TBlkw']\r\n[debug] Encodings: locale UTF-8, fs UTF-8, out UTF-8, pref UTF-8\r\n[debug] youtube-dl version 2024.07.08 [a452f9437]\r\n[debug] ** This version was built from the latest master code at https://github.com/ytdl-org/youtube-dl.\r\n[debug] ** For support, visit the main site.\r\n[debug] Python 2.7.3 (CPython i686 32bit) - Linux-i686 - OpenSSL 1.0.1e - glibc 2.0\r\n[debug] exe versions: none\r\n[debug] Proxy map: {}\r\n[youtube] NjCVZ2TBlkw: Downloading webpage\r\n[debug] [youtube] Decrypted nsig GGMy0_8ADhuvb3QiC => HhLGoGWp5YkFLQ\r\n[debug] [youtube] Decrypted nsig g_flXTUre97dIvcKl => kBjCgNdd7NUQcQ\r\n[info] Available formats for NjCVZ2TBlkw:\r\nformat code  extension  resolution note\r\n251          webm       audio only audio_quality_medium    3k , webm_dash container, opus  (48000Hz), 2.57MiB\r\n251-drc      webm       audio only audio_quality_medium    3k , webm_dash container, opus  (48000Hz), 2.58MiB\r\n140          m4a        audio only audio_quality_medium  129k , m4a_dash container, mp4a.40.2 (44100Hz), 91.90MiB\r\n140-drc      m4a        audio only audio_quality_medium  129k , m4a_dash container, mp4a.40.2 (44100Hz), 91.90MiB\r\n160          mp4        256x144    144p    6k , mp4_dash container, avc1.4d400c, 30fps, video only, 4.70MiB\r\n134          mp4        640x360    360p   10k , mp4_dash container, avc1.4d401e, 30fps, video only, 7.64MiB\r\n136          mp4        1280x720   720p   21k , mp4_dash container, avc1.64001f, 30fps, video only, 15.16MiB\r\n137          mp4        1920x1080  1080p   32k , mp4_dash container, avc1.640028, 30fps, video only, 23.11MiB\r\n18           mp4        640x360    360p  139k , avc1.42001E, 30fps, mp4a.40.2 (44100Hz) (best)\r\n\r\n```\r\n\n", "hints_text": "\n", "created_at": "2024-07-10T17:53:59Z"}
{"repo": "ytdl-org/youtube-dl", "pull_number": 32820, "instance_id": "ytdl-org__youtube-dl-32820", "issue_numbers": ["32815"], "base_commit": "0153b387e57e0bb8e580f1869f85596d2767fb0d", "patch": "diff --git a/youtube_dl/extractor/common.py b/youtube_dl/extractor/common.py\nindex b10e844168c..9b0016d07ec 100644\n--- a/youtube_dl/extractor/common.py\n+++ b/youtube_dl/extractor/common.py\n@@ -3033,7 +3033,6 @@ def _find_jwplayer_data(self, webpage, video_id=None, transform_source=js_to_jso\n             transform_source=transform_source, default=None)\n \n     def _extract_jwplayer_data(self, webpage, video_id, *args, **kwargs):\n-\n         # allow passing `transform_source` through to _find_jwplayer_data()\n         transform_source = kwargs.pop('transform_source', None)\n         kwfind = compat_kwargs({'transform_source': transform_source}) if transform_source else {}\ndiff --git a/youtube_dl/extractor/palcomp3.py b/youtube_dl/extractor/palcomp3.py\nindex fb29d83f9f2..60f7a4d48db 100644\n--- a/youtube_dl/extractor/palcomp3.py\n+++ b/youtube_dl/extractor/palcomp3.py\n@@ -8,7 +8,7 @@\n from ..utils import (\n     int_or_none,\n     str_or_none,\n-    try_get,\n+    traverse_obj,\n )\n \n \n@@ -109,7 +109,7 @@ class PalcoMP3ArtistIE(PalcoMP3BaseIE):\n     }\n     name'''\n \n-    @ classmethod\n+    @classmethod\n     def suitable(cls, url):\n         return False if re.match(PalcoMP3IE._VALID_URL, url) else super(PalcoMP3ArtistIE, cls).suitable(url)\n \n@@ -118,7 +118,8 @@ def _real_extract(self, url):\n         artist = self._call_api(artist_slug, self._ARTIST_FIELDS_TMPL)['artist']\n \n         def entries():\n-            for music in (try_get(artist, lambda x: x['musics']['nodes'], list) or []):\n+            for music in traverse_obj(artist, (\n+                    'musics', 'nodes', lambda _, m: m['musicID'])):\n                 yield self._parse_music(music)\n \n         return self.playlist_result(\n@@ -137,7 +138,7 @@ class PalcoMP3VideoIE(PalcoMP3BaseIE):\n             'title': 'Maiara e Maraisa - Voc\u00ea Faz Falta Aqui - DVD Ao Vivo Em Campo Grande',\n             'description': 'md5:7043342c09a224598e93546e98e49282',\n             'upload_date': '20161107',\n-            'uploader_id': 'maiaramaraisaoficial',\n+            'uploader_id': '@maiaramaraisaoficial',\n             'uploader': 'Maiara e Maraisa',\n         }\n     }]\ndiff --git a/youtube_dl/jsinterp.py b/youtube_dl/jsinterp.py\nindex 86d902248da..02adf667846 100644\n--- a/youtube_dl/jsinterp.py\n+++ b/youtube_dl/jsinterp.py\n@@ -14,6 +14,7 @@\n     remove_quotes,\n     unified_timestamp,\n     variadic,\n+    write_string,\n )\n from .compat import (\n     compat_basestring,\n@@ -53,15 +54,16 @@ def update_and_rename_wrapper(w):\n \n # NB In principle NaN cannot be checked by membership.\n # Here all NaN values are actually this one, so _NaN is _NaN,\n-# although _NaN != _NaN.\n+# although _NaN != _NaN. Ditto Infinity.\n \n _NaN = float('nan')\n+_Infinity = float('inf')\n \n \n def _js_bit_op(op):\n \n     def zeroise(x):\n-        return 0 if x in (None, JS_Undefined, _NaN) else x\n+        return 0 if x in (None, JS_Undefined, _NaN, _Infinity) else x\n \n     @wraps_op(op)\n     def wrapped(a, b):\n@@ -84,7 +86,7 @@ def wrapped(a, b):\n def _js_div(a, b):\n     if JS_Undefined in (a, b) or not (a or b):\n         return _NaN\n-    return operator.truediv(a or 0, b) if b else float('inf')\n+    return operator.truediv(a or 0, b) if b else _Infinity\n \n \n def _js_mod(a, b):\n@@ -220,6 +222,42 @@ def __repr__(self):\n         return 'LocalNameSpace%s' % (self.maps, )\n \n \n+class Debugger(object):\n+    ENABLED = False\n+\n+    @staticmethod\n+    def write(*args, **kwargs):\n+        level = kwargs.get('level', 100)\n+\n+        def truncate_string(s, left, right=0):\n+            if s is None or len(s) <= left + right:\n+                return s\n+            return '...'.join((s[:left - 3], s[-right:] if right else ''))\n+\n+        write_string('[debug] JS: {0}{1}\\n'.format(\n+            '  ' * (100 - level),\n+            ' '.join(truncate_string(compat_str(x), 50, 50) for x in args)))\n+\n+    @classmethod\n+    def wrap_interpreter(cls, f):\n+        def interpret_statement(self, stmt, local_vars, allow_recursion, *args, **kwargs):\n+            if cls.ENABLED and stmt.strip():\n+                cls.write(stmt, level=allow_recursion)\n+            try:\n+                ret, should_ret = f(self, stmt, local_vars, allow_recursion, *args, **kwargs)\n+            except Exception as e:\n+                if cls.ENABLED:\n+                    if isinstance(e, ExtractorError):\n+                        e = e.orig_msg\n+                    cls.write('=> Raises:', e, '<-|', stmt, level=allow_recursion)\n+                raise\n+            if cls.ENABLED and stmt.strip():\n+                if should_ret or not repr(ret) == stmt:\n+                    cls.write(['->', '=>'][should_ret], repr(ret), '<-|', stmt, level=allow_recursion)\n+            return ret, should_ret\n+        return interpret_statement\n+\n+\n class JSInterpreter(object):\n     __named_object_counter = 0\n \n@@ -307,8 +345,7 @@ def regex_flags(cls, expr):\n     def __op_chars(cls):\n         op_chars = set(';,[')\n         for op in cls._all_operators():\n-            for c in op[0]:\n-                op_chars.add(c)\n+            op_chars.update(op[0])\n         return op_chars\n \n     def _named_object(self, namespace, obj):\n@@ -326,9 +363,8 @@ def _separate(cls, expr, delim=',', max_split=None, skip_delims=None):\n         # collections.Counter() is ~10% slower in both 2.7 and 3.9\n         counters = dict((k, 0) for k in _MATCHING_PARENS.values())\n         start, splits, pos, delim_len = 0, 0, 0, len(delim) - 1\n-        in_quote, escaping, skipping = None, False, 0\n-        after_op, in_regex_char_group = True, False\n-\n+        in_quote, escaping, after_op, in_regex_char_group = None, False, True, False\n+        skipping = 0\n         for idx, char in enumerate(expr):\n             paren_delta = 0\n             if not in_quote:\n@@ -382,10 +418,12 @@ def _separate_at_paren(cls, expr, delim=None):\n         return separated[0][1:].strip(), separated[1].strip()\n \n     @staticmethod\n-    def _all_operators():\n-        return itertools.chain(\n-            # Ref: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Operator_Precedence\n-            _SC_OPERATORS, _LOG_OPERATORS, _COMP_OPERATORS, _OPERATORS)\n+    def _all_operators(_cached=[]):\n+        if not _cached:\n+            _cached.extend(itertools.chain(\n+                # Ref: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Operator_Precedence\n+                _SC_OPERATORS, _LOG_OPERATORS, _COMP_OPERATORS, _OPERATORS))\n+        return _cached\n \n     def _operator(self, op, left_val, right_expr, expr, local_vars, allow_recursion):\n         if op in ('||', '&&'):\n@@ -416,7 +454,7 @@ def _index(self, obj, idx, allow_undefined=False):\n         except Exception as e:\n             if allow_undefined:\n                 return JS_Undefined\n-            raise self.Exception('Cannot get index {idx:.100}'.format(**locals()), expr=repr(obj), cause=e)\n+            raise self.Exception('Cannot get index {idx!r:.100}'.format(**locals()), expr=repr(obj), cause=e)\n \n     def _dump(self, obj, namespace):\n         try:\n@@ -438,6 +476,7 @@ def _dump(self, obj, namespace):\n     _FINALLY_RE = re.compile(r'finally\\s*\\{')\n     _SWITCH_RE = re.compile(r'switch\\s*\\(')\n \n+    @Debugger.wrap_interpreter\n     def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n         if allow_recursion < 0:\n             raise self.Exception('Recursion limit reached')\n@@ -511,7 +550,6 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n                 expr = self._dump(inner, local_vars) + outer\n \n         if expr.startswith('('):\n-\n             m = re.match(r'\\((?P<d>[a-z])%(?P<e>[a-z])\\.length\\+(?P=e)\\.length\\)%(?P=e)\\.length', expr)\n             if m:\n                 # short-cut eval of frequently used `(d%e.length+e.length)%e.length`, worth ~6% on `pytest -k test_nsig`\n@@ -693,7 +731,7 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n                 (?P<op>{_OPERATOR_RE})?\n                 =(?!=)(?P<expr>.*)$\n             )|(?P<return>\n-                (?!if|return|true|false|null|undefined)(?P<name>{_NAME_RE})$\n+                (?!if|return|true|false|null|undefined|NaN|Infinity)(?P<name>{_NAME_RE})$\n             )|(?P<indexing>\n                 (?P<in>{_NAME_RE})\\[(?P<idx>.+)\\]$\n             )|(?P<attribute>\n@@ -727,11 +765,12 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n             raise JS_Break()\n         elif expr == 'continue':\n             raise JS_Continue()\n-\n         elif expr == 'undefined':\n             return JS_Undefined, should_return\n         elif expr == 'NaN':\n             return _NaN, should_return\n+        elif expr == 'Infinity':\n+            return _Infinity, should_return\n \n         elif md.get('return'):\n             return local_vars[m.group('name')], should_return\n@@ -760,18 +799,28 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n             right_expr = separated.pop()\n             # handle operators that are both unary and binary, minimal BODMAS\n             if op in ('+', '-'):\n+                # simplify/adjust consecutive instances of these operators\n                 undone = 0\n                 while len(separated) > 1 and not separated[-1].strip():\n                     undone += 1\n                     separated.pop()\n                 if op == '-' and undone % 2 != 0:\n                     right_expr = op + right_expr\n+                elif op == '+':\n+                    while len(separated) > 1 and separated[-1].strip() in self.OP_CHARS:\n+                        right_expr = separated.pop() + right_expr\n+                # hanging op at end of left => unary + (strip) or - (push right)\n                 left_val = separated[-1]\n                 for dm_op in ('*', '%', '/', '**'):\n                     bodmas = tuple(self._separate(left_val, dm_op, skip_delims=skip_delim))\n                     if len(bodmas) > 1 and not bodmas[-1].strip():\n                         expr = op.join(separated) + op + right_expr\n-                        right_expr = None\n+                        if len(separated) > 1:\n+                            separated.pop()\n+                            right_expr = op.join((left_val, right_expr))\n+                        else:\n+                            separated = [op.join((left_val, right_expr))]\n+                            right_expr = None\n                         break\n                 if right_expr is None:\n                     continue\n@@ -797,6 +846,8 @@ def assertion(cndn, msg):\n \n             def eval_method():\n                 if (variable, member) == ('console', 'debug'):\n+                    if Debugger.ENABLED:\n+                        Debugger.write(self.interpret_expression('[{}]'.format(arg_str), local_vars, allow_recursion))\n                     return\n                 types = {\n                     'String': compat_str,\ndiff --git a/youtube_dl/utils.py b/youtube_dl/utils.py\nindex cd43035667e..113c913df54 100644\n--- a/youtube_dl/utils.py\n+++ b/youtube_dl/utils.py\n@@ -2406,7 +2406,7 @@ def __init__(self, msg, tb=None, expected=False, cause=None, video_id=None):\n         \"\"\" tb, if given, is the original traceback (so that it can be printed out).\n         If expected is set, this is a normal error message and most likely not a bug in youtube-dl.\n         \"\"\"\n-\n+        self.orig_msg = msg\n         if sys.exc_info()[0] in (compat_urllib_error.URLError, socket.timeout, UnavailableVideoError):\n             expected = True\n         if video_id is not None:\n", "test_patch": "diff --git a/test/test_jsinterp.py b/test/test_jsinterp.py\nindex 91b12f5441b..da8e980207a 100644\n--- a/test/test_jsinterp.py\n+++ b/test/test_jsinterp.py\n@@ -577,9 +577,11 @@ def test_32066(self):\n     def test_unary_operators(self):\n         jsi = JSInterpreter('function f(){return 2  -  - - 2;}')\n         self.assertEqual(jsi.call_function('f'), 0)\n-        # fails\n-        # jsi = JSInterpreter('function f(){return 2 + - + - - 2;}')\n-        # self.assertEqual(jsi.call_function('f'), 0)\n+        jsi = JSInterpreter('function f(){return 2 + - + - - 2;}')\n+        self.assertEqual(jsi.call_function('f'), 0)\n+        # https://github.com/ytdl-org/youtube-dl/issues/32815\n+        jsi = JSInterpreter('function f(){return 0  - 7 * - 6;}')\n+        self.assertEqual(jsi.call_function('f'), 42)\n \n     \"\"\" # fails so far\n     def test_packed(self):\ndiff --git a/test/test_youtube_signature.py b/test/test_youtube_signature.py\nindex f45dfec7cff..cafba7a5cdd 100644\n--- a/test/test_youtube_signature.py\n+++ b/test/test_youtube_signature.py\n@@ -158,6 +158,10 @@\n         'https://www.youtube.com/s/player/b7910ca8/player_ias.vflset/en_US/base.js',\n         '_hXMCwMt9qE310D', 'LoZMgkkofRMCZQ',\n     ),\n+    (\n+        'https://www.youtube.com/s/player/590f65a6/player_ias.vflset/en_US/base.js',\n+        '1tm7-g_A9zsI8_Lay_', 'xI4Vem4Put_rOg',\n+    ),\n ]\n \n \n", "problem_statement": "[YouTube] Error 403 when downloading, with \"Unable to decode n-parameter\", not solved by new cookies\n<!--\r\n\r\n######################################################################\r\n  WARNING!\r\n  IGNORING THE FOLLOWING TEMPLATE WILL RESULT IN ISSUE CLOSED AS INCOMPLETE\r\n######################################################################\r\n\r\n-->\r\n\r\n\r\n## Checklist\r\n\r\n<!--\r\nCarefully read and work through this check list in order to prevent the most common mistakes and misuse of youtube-dl:\r\n- First of, make sure you are using the latest version of youtube-dl. Run `youtube-dl --version` and ensure your version is 2021.12.17. If it's not, see https://yt-dl.org/update on how to update. Issues with outdated version will be REJECTED.\r\n- Make sure that all provided video/audio/playlist URLs (if any) are alive and playable in a browser.\r\n- Make sure that all URLs and arguments with special characters are properly quoted or escaped as explained in http://yt-dl.org/escape.\r\n- Search the bugtracker for similar issues: http://yt-dl.org/search-issues. DO NOT post duplicates.\r\n- Finally, put x into all relevant boxes (like this [x])\r\n-->\r\n\r\n- [x] I'm reporting a broken site support\r\n- [x] I've verified that I'm running youtube-dl version **2021.12.17**\r\n- [x] I've checked that all provided URLs are alive and playable in a browser\r\n- [x] I've checked that all URLs and arguments with special characters are properly quoted or escaped\r\n- [x] I've searched the bugtracker for similar issues including closed ones\r\n\r\n\r\n## Verbose log\r\n\r\n<!--\r\nProvide the complete verbose output of youtube-dl that clearly demonstrates the problem.\r\nAdd the `-v` flag to your command line you run youtube-dl with (`youtube-dl -v <your command line>`), copy the WHOLE output and insert it below. It should look similar to this:\r\n [debug] System config: []\r\n [debug] User config: []\r\n [debug] Command-line args: [u'-v', u'http://www.youtube.com/watch?v=BaW_jenozKcj']\r\n [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251\r\n [debug] youtube-dl version 2021.12.17\r\n [debug] Python version 2.7.11 - Windows-2003Server-5.2.3790-SP2\r\n [debug] exe versions: ffmpeg N-75573-g1d0487f, ffprobe N-75573-g1d0487f, rtmpdump 2.4\r\n [debug] Proxy map: {}\r\n <more lines>\r\n-->\r\n\r\n```\r\n[debug] System config: []\r\n[debug] User config: ['--no-mtime', '--match-filter', '!is_live', '--retries', 'infinite', '--fragment-retries', '2', '--skip-unavailable-fragments', '--restrict-filenames', '-i', '-o', '/home/gregorius/home/pending/videos/%(title)s___%(id)s.webm', '-f', '(bestvideo[height<=360]+worstaudio/best[height<=360])[protocol!=http_dash_segments][container!^=dash]', '--console-title', '--hls-prefer-native', '--no-cache-dir', '--http-chunk-size', '100M', '--cookies', '/home/gregorius/home/scripts/video/youtube-dl-cookies']\r\n[debug] Custom config: []\r\n[debug] Command-line args: ['https://www.youtube.com/watch?v=I0EmyNep1pE', '-vf', '(242+249/242+250/242+171/242+251)/(243+249/243+250/243+171/243+251)/18', '--no-playlist']\r\n[debug] Encodings: locale UTF-8, fs utf-8, out utf-8, pref UTF-8\r\n[debug] youtube-dl version 2021.12.17\r\n[debug] Single file build\r\n[debug] Python 3.10.12 (CPython x86_64 64bit) - Linux-5.15.0-112-generic-x86_64-with-glibc2.35 - OpenSSL 3.0.2 15 Mar 2022 - glibc 2.35\r\n[debug] exe versions: ffmpeg 4.4.2, ffprobe 4.4.2, rtmpdump 2.4\r\n[debug] Proxy map: {}\r\n[youtube] I0EmyNep1pE: Downloading webpage\r\n[youtube] Downloading just video I0EmyNep1pE because of --no-playlist\r\n[youtube] I0EmyNep1pE: Downloading player 590f65a6\r\nWARNING: [youtube] I0EmyNep1pE: Unable to decode n-parameter: download likely to be throttled (Signature function returned an exception; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output. Traceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/extractor/youtube.py\", line 1646, in _decrypt_nsig\r\n    ret = extract_nsig(jsi, func_code)(n)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/extractor/youtube.py\", line 1709, in extract_nsig\r\n    raise JSInterpreter.Exception('Signature function returned an exception')\r\nyoutube_dl.jsinterp.JSInterpreter.Exception: Signature function returned an exception; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n)\r\nWARNING: [youtube] I0EmyNep1pE: Unable to decode n-parameter: download likely to be throttled (Signature function returned an exception; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output. Traceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/extractor/youtube.py\", line 1646, in _decrypt_nsig\r\n    ret = extract_nsig(jsi, func_code)(n)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/extractor/youtube.py\", line 1709, in extract_nsig\r\n    raise JSInterpreter.Exception('Signature function returned an exception')\r\nyoutube_dl.jsinterp.JSInterpreter.Exception: Signature function returned an exception; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n)\r\n[debug] Invoking downloader on 'https://rr2---sn-4g5lznes.googlevideo.com/videoplayback?expire=1718665474&ei=omxwZvKCJ_iai9oPrPytsAk&ip=47.69.137.137&id=o-AKm1dRgby9sFrDGVX0bJ3KgKGN1PyHSx-q_f9KRGSC_4&itag=242&aitags=133%2C134%2C135%2C136%2C160%2C242%2C243%2C244%2C247%2C278%2C298%2C299%2C302%2C303&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&mh=zO&mm=31%2C26&mn=sn-4g5lznes%2Csn-f5f7lne6&ms=au%2Conr&mv=m&mvi=2&pl=19&pcm2=yes&initcwndbps=1846250&bui=AbKP-1O3YVe9fGtsxyDHU7TNneKRCgDUDHJwh-mOCN2NGJSkTd1WUBz8ZDI7BYY3UcprCDw-3wyQylOT&spc=UWF9f4QfSfv8_z5FTqrKIgbd1Z6DAne5l6xi3J5VDxJUjQ-o3xFEAi01qfSo&vprv=1&svpuc=1&mime=video%2Fwebm&ns=yYmogIIQrRok7xzlewj6aUcQ&rqh=1&gir=yes&clen=1016042&dur=191.725&lmt=1718607835104876&mt=1718643418&fvip=5&keepalive=yes&c=WEB&sefc=1&txp=630F224&n=qe5ibzBzTm3VA9bmZa&sparams=expire%2Cei%2Cip%2Cid%2Caitags%2Csource%2Crequiressl%2Cxpc%2Cpcm2%2Cbui%2Cspc%2Cvprv%2Csvpuc%2Cmime%2Cns%2Crqh%2Cgir%2Cclen%2Cdur%2Clmt&sig=AJfQdSswRQIgfyH-ksbDbOyac3rkRTnpiA_Nl3s2qIu4ZaefHXj3F5kCIQCzK-IImJCscIaBUVePhfrOHydqrVCjL0XQK11dlUvY7A%3D%3D&lsparams=mh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Cinitcwndbps&lsig=AHlkHjAwRQIgNUY_e59Q8v2uPdD5xDihqT72BNgDpYP1DwWLHcSH0UMCIQCzBNlexMB54rPi1BsBVeLCNQxyrz_g9jsimTokpp4HEA%3D%3D'\r\n[dashsegments] Total fragments: 1\r\n[download] Destination: /home/gregorius/home/pending/videos/This_Guy_Uses_ROOK___I0EmyNep1pE.f242.webm\r\n[download] Got server HTTP error: HTTP Error 403: Forbidden. Retrying fragment 1 (attempt 1 of 2)...\r\n[download] Got server HTTP error: HTTP Error 403: Forbidden. Retrying fragment 1 (attempt 2 of 2)...\r\nERROR: giving up after 2 fragment retries\r\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/__main__.py\", line 19, in <module>\r\n    youtube_dl.main()\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/__init__.py\", line 473, in main\r\n    _real_main(argv)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/__init__.py\", line 463, in _real_main\r\n    retcode = ydl.download(all_urls)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 2227, in download\r\n    res = self.extract_info(\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 852, in extract_info\r\n    return self.__extract_info(url, ie, download, extra_info, process)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 859, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 966, in __extract_info\r\n    return self.process_ie_result(ie_result, download, extra_info)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 1000, in process_ie_result\r\n    return self.process_video_result(ie_result, download=download)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 1844, in process_video_result\r\n    self.process_info(new_info)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 2126, in process_info\r\n    partial_success = dl(fname, new_info)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 2071, in dl\r\n    return fd.download(name, new_info)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/downloader/common.py\", line 380, in download\r\n    return self.real_download(filename, info_dict)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/downloader/dash.py\", line 78, in real_download\r\n    self.report_error('giving up after %s fragment retries' % count)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/downloader/common.py\", line 175, in report_error\r\n    self.ydl.report_error(*args, **kargs)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 662, in report_error\r\n    self.trouble(*args, **kwargs)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 614, in trouble\r\n    tb_data = traceback.format_list(traceback.extract_stack())\r\n\r\n[debug] Invoking downloader on 'https://rr2---sn-4g5lznes.googlevideo.com/videoplayback?expire=1718665474&ei=omxwZvKCJ_iai9oPrPytsAk&ip=47.69.137.137&id=o-AKm1dRgby9sFrDGVX0bJ3KgKGN1PyHSx-q_f9KRGSC_4&itag=249&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&mh=zO&mm=31%2C26&mn=sn-4g5lznes%2Csn-f5f7lne6&ms=au%2Conr&mv=m&mvi=2&pl=19&pcm2=yes&initcwndbps=1846250&bui=AbKP-1O3YVe9fGtsxyDHU7TNneKRCgDUDHJwh-mOCN2NGJSkTd1WUBz8ZDI7BYY3UcprCDw-3wyQylOT&spc=UWF9f4QfSfv8_z5FTqrKIgbd1Z6DAne5l6xi3J5VDxJUjQ-o3xFEAi01qfSo&vprv=1&svpuc=1&mime=audio%2Fwebm&ns=yYmogIIQrRok7xzlewj6aUcQ&rqh=1&gir=yes&clen=1161452&dur=191.761&lmt=1718607847881176&mt=1718643418&fvip=5&keepalive=yes&c=WEB&sefc=1&txp=6308224&n=qe5ibzBzTm3VA9bmZa&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cpcm2%2Cbui%2Cspc%2Cvprv%2Csvpuc%2Cmime%2Cns%2Crqh%2Cgir%2Cclen%2Cdur%2Clmt&sig=AJfQdSswRQIhAI2_T6krM2pajKeKzXsbDjrszD9mLTIrH02Ms4TWM0FdAiBbGDPlRd7zNnZn86UT1A4xMbQthP6xzZulcyn_HqBJsg%3D%3D&lsparams=mh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Cinitcwndbps&lsig=AHlkHjAwRQIgNUY_e59Q8v2uPdD5xDihqT72BNgDpYP1DwWLHcSH0UMCIQCzBNlexMB54rPi1BsBVeLCNQxyrz_g9jsimTokpp4HEA%3D%3D'\r\n[dashsegments] Total fragments: 1\r\n[download] Destination: /home/gregorius/home/pending/videos/This_Guy_Uses_ROOK___I0EmyNep1pE.f249.webm\r\n[download] Got server HTTP error: HTTP Error 403: Forbidden. Retrying fragment 1 (attempt 1 of 2)...\r\n[download] Got server HTTP error: HTTP Error 403: Forbidden. Retrying fragment 1 (attempt 2 of 2)...\r\nERROR: giving up after 2 fragment retries\r\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/__main__.py\", line 19, in <module>\r\n    youtube_dl.main()\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/__init__.py\", line 473, in main\r\n    _real_main(argv)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/__init__.py\", line 463, in _real_main\r\n    retcode = ydl.download(all_urls)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 2227, in download\r\n    res = self.extract_info(\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 852, in extract_info\r\n    return self.__extract_info(url, ie, download, extra_info, process)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 859, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 966, in __extract_info\r\n    return self.process_ie_result(ie_result, download, extra_info)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 1000, in process_ie_result\r\n    return self.process_video_result(ie_result, download=download)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 1844, in process_video_result\r\n    self.process_info(new_info)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 2126, in process_info\r\n    partial_success = dl(fname, new_info)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 2071, in dl\r\n    return fd.download(name, new_info)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/downloader/common.py\", line 380, in download\r\n    return self.real_download(filename, info_dict)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/downloader/dash.py\", line 78, in real_download\r\n    self.report_error('giving up after %s fragment retries' % count)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/downloader/common.py\", line 175, in report_error\r\n    self.ydl.report_error(*args, **kargs)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 662, in report_error\r\n    self.trouble(*args, **kwargs)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 614, in trouble\r\n    tb_data = traceback.format_list(traceback.extract_stack())\r\n```\r\n\r\n\r\nand from the youtube-dl-nightly the same type of error\r\n\r\n```\r\n[debug] System config: []\r\n[debug] User config: ['--no-mtime', '--match-filter', '!is_live', '--retries', 'infinite', '--fragment-retries', '2', '--skip-unavailable-fragments', '--restrict-filenames', '-i', '-o', '/home/gregorius/home/pending/videos/%(title)s___%(id)s.webm', '-f', '(bestvideo[height<=360]+worstaudio/best[height<=360])[protocol!=http_dash_segments][container!^=dash]', '--console-title', '--hls-prefer-native', '--no-cache-dir', '--http-chunk-size', '100M', '--cookies', '/home/gregorius/home/scripts/video/youtube-dl-cookies']\r\n[debug] Custom config: []\r\n[debug] Command-line args: ['https://www.youtube.com/watch?v=I0EmyNep1pE', '-vf', '(242+249/242+250/242+171/242+251)/(243+249/243+250/243+171/243+251)/18', '--no-playlist']\r\n[debug] Encodings: locale UTF-8, fs utf-8, out utf-8, pref UTF-8\r\n[debug] youtube-dl version 2024.06.12 [0153b387e] (single file build)\r\n[debug] ** This version was built from the latest master code at https://github.com/ytdl-org/youtube-dl.\r\n[debug] ** For support, visit the main site.\r\n[debug] Python 3.10.12 (CPython x86_64 64bit) - Linux-5.15.0-112-generic-x86_64-with-glibc2.35 - OpenSSL 3.0.2 15 Mar 2022 - glibc 2.35\r\n[debug] exe versions: ffmpeg 4.4.2, ffprobe 4.4.2, rtmpdump 2.4\r\n[debug] Proxy map: {}\r\n[youtube] I0EmyNep1pE: Downloading webpage\r\n[youtube] Downloading just video I0EmyNep1pE because of --no-playlist\r\n[youtube] I0EmyNep1pE: Downloading player 590f65a6\r\nWARNING: [youtube] I0EmyNep1pE: Unable to decode n-parameter: download likely to be throttled (Signature function returned an exception; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output. Traceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/extractor/youtube.py\", line 1646, in _decrypt_nsig\r\n    ret = extract_nsig(jsi, func_code)(n)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/extractor/youtube.py\", line 1709, in extract_nsig\r\n    raise JSInterpreter.Exception('Signature function returned an exception')\r\nyoutube_dl.jsinterp.JSInterpreter.Exception: Signature function returned an exception; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n)\r\nWARNING: [youtube] I0EmyNep1pE: Unable to decode n-parameter: download likely to be throttled (Signature function returned an exception; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output. Traceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/extractor/youtube.py\", line 1646, in _decrypt_nsig\r\n    ret = extract_nsig(jsi, func_code)(n)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/extractor/youtube.py\", line 1709, in extract_nsig\r\n    raise JSInterpreter.Exception('Signature function returned an exception')\r\nyoutube_dl.jsinterp.JSInterpreter.Exception: Signature function returned an exception; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n)\r\n[debug] Invoking downloader on 'https://rr1---sn-4g5edns6.googlevideo.com/videoplayback?expire=1718665573&ei=BW1wZr65K-rD6dsPguar4A4&ip=47.69.137.137&id=o-AGWiYhnnFi2gYwMjiEngFFG_mTkLEY5tw7LLQ8vZOxra&itag=242&aitags=133%2C134%2C135%2C136%2C160%2C242%2C243%2C244%2C247%2C278%2C298%2C299%2C302%2C303&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&mh=zO&mm=31%2C29&mn=sn-4g5edns6%2Csn-4g5lznes&ms=au%2Crdu&mv=m&mvi=1&pl=19&pcm2=yes&initcwndbps=1705000&bui=AbKP-1OuyfqSvr0ru3007fM3N7DxkqOFTDTFs5Peea0Lbj1kTJE3h_CMSJ_nEb1H8YpyRyYBuAygB1c0&spc=UWF9f1molBRRIO4AR05dXDWq6-AEI-3dzzsNSNboAQbRWJ7bEXUMXYlz-XFX&vprv=1&svpuc=1&mime=video%2Fwebm&ns=ST2xa86mQoTRTzJyy5F-mTgQ&rqh=1&gir=yes&clen=1016042&dur=191.725&lmt=1718607835104876&mt=1718643663&fvip=2&keepalive=yes&c=WEB&sefc=1&txp=630F224&n=e-sOBZGG27nbx3xUSI&sparams=expire%2Cei%2Cip%2Cid%2Caitags%2Csource%2Crequiressl%2Cxpc%2Cpcm2%2Cbui%2Cspc%2Cvprv%2Csvpuc%2Cmime%2Cns%2Crqh%2Cgir%2Cclen%2Cdur%2Clmt&sig=AJfQdSswRAIgJW0G2uflvnWao3WKRmrjsLSueCjacv2WU3BUkeMm5cwCIByTIOw4jdAaSVtw4i50PkKIQxEPlt7d6En5cZZ4OFbx&lsparams=mh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Cinitcwndbps&lsig=AHlkHjAwRgIhANRsG6osuyhOxcD9hxgdSoPuOFleBB9PHSSmqMyEj0qLAiEA8fKD4p4xgzZlk2S6ceZD1QhkOc-mXtjWyg3E1xqxVMg%3D'\r\n[dashsegments] Total fragments: 1\r\n[download] Destination: /home/gregorius/home/pending/videos/This_Guy_Uses_ROOK___I0EmyNep1pE.f242.webm\r\n[download] Got server HTTP error: HTTP Error 403: Forbidden. Retrying fragment 1 (attempt 1 of 2)...\r\n[download] Got server HTTP error: HTTP Error 403: Forbidden. Retrying fragment 1 (attempt 2 of 2)...\r\nERROR: giving up after 2 fragment retries\r\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/__main__.py\", line 19, in <module>\r\n    youtube_dl.main()\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/__init__.py\", line 473, in main\r\n    _real_main(argv)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/__init__.py\", line 463, in _real_main\r\n    retcode = ydl.download(all_urls)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 2231, in download\r\n    res = self.extract_info(\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 856, in extract_info\r\n    return self.__extract_info(url, ie, download, extra_info, process)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 863, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 970, in __extract_info\r\n    return self.process_ie_result(ie_result, download, extra_info)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 1004, in process_ie_result\r\n    return self.process_video_result(ie_result, download=download)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 1848, in process_video_result\r\n    self.process_info(new_info)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 2130, in process_info\r\n    partial_success = dl(fname, new_info)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 2075, in dl\r\n    return fd.download(name, new_info)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/downloader/common.py\", line 380, in download\r\n    return self.real_download(filename, info_dict)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/downloader/dash.py\", line 78, in real_download\r\n    self.report_error('giving up after %s fragment retries' % count)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/downloader/common.py\", line 175, in report_error\r\n    self.ydl.report_error(*args, **kargs)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 666, in report_error\r\n    self.trouble(*args, **kwargs)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 618, in trouble\r\n    tb_data = traceback.format_list(traceback.extract_stack())\r\n\r\n[debug] Invoking downloader on 'https://rr1---sn-4g5edns6.googlevideo.com/videoplayback?expire=1718665573&ei=BW1wZr65K-rD6dsPguar4A4&ip=47.69.137.137&id=o-AGWiYhnnFi2gYwMjiEngFFG_mTkLEY5tw7LLQ8vZOxra&itag=249&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&mh=zO&mm=31%2C29&mn=sn-4g5edns6%2Csn-4g5lznes&ms=au%2Crdu&mv=m&mvi=1&pl=19&pcm2=yes&initcwndbps=1705000&bui=AbKP-1OuyfqSvr0ru3007fM3N7DxkqOFTDTFs5Peea0Lbj1kTJE3h_CMSJ_nEb1H8YpyRyYBuAygB1c0&spc=UWF9f1molBRRIO4AR05dXDWq6-AEI-3dzzsNSNboAQbRWJ7bEXUMXYlz-XFX&vprv=1&svpuc=1&mime=audio%2Fwebm&ns=ST2xa86mQoTRTzJyy5F-mTgQ&rqh=1&gir=yes&clen=1161452&dur=191.761&lmt=1718607847881176&mt=1718643663&fvip=2&keepalive=yes&c=WEB&sefc=1&txp=6308224&n=e-sOBZGG27nbx3xUSI&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cpcm2%2Cbui%2Cspc%2Cvprv%2Csvpuc%2Cmime%2Cns%2Crqh%2Cgir%2Cclen%2Cdur%2Clmt&sig=AJfQdSswRAIgeVy4plOeya9-a-P6aLWEhRr1WfGSZytZhUt3puSuQEECIGFUyjsOSHpyUaNn6Otxq-GYH4_zOBhjh-v6ceKe4IZi&lsparams=mh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Cinitcwndbps&lsig=AHlkHjAwRgIhANRsG6osuyhOxcD9hxgdSoPuOFleBB9PHSSmqMyEj0qLAiEA8fKD4p4xgzZlk2S6ceZD1QhkOc-mXtjWyg3E1xqxVMg%3D'\r\n[dashsegments] Total fragments: 1\r\n[download] Destination: /home/gregorius/home/pending/videos/This_Guy_Uses_ROOK___I0EmyNep1pE.f249.webm\r\n[download] Got server HTTP error: HTTP Error 403: Forbidden. Retrying fragment 1 (attempt 1 of 2)...\r\n[download] Got server HTTP error: HTTP Error 403: Forbidden. Retrying fragment 1 (attempt 2 of 2)...\r\nERROR: giving up after 2 fragment retries\r\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/__main__.py\", line 19, in <module>\r\n    youtube_dl.main()\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/__init__.py\", line 473, in main\r\n    _real_main(argv)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/__init__.py\", line 463, in _real_main\r\n    retcode = ydl.download(all_urls)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 2231, in download\r\n    res = self.extract_info(\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 856, in extract_info\r\n    return self.__extract_info(url, ie, download, extra_info, process)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 863, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 970, in __extract_info\r\n    return self.process_ie_result(ie_result, download, extra_info)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 1004, in process_ie_result\r\n    return self.process_video_result(ie_result, download=download)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 1848, in process_video_result\r\n    self.process_info(new_info)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 2130, in process_info\r\n    partial_success = dl(fname, new_info)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 2075, in dl\r\n    return fd.download(name, new_info)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/downloader/common.py\", line 380, in download\r\n    return self.real_download(filename, info_dict)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/downloader/dash.py\", line 78, in real_download\r\n    self.report_error('giving up after %s fragment retries' % count)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/downloader/common.py\", line 175, in report_error\r\n    self.ydl.report_error(*args, **kargs)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 666, in report_error\r\n    self.trouble(*args, **kwargs)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl/youtube-dl/youtube_dl/YoutubeDL.py\", line 618, in trouble\r\n    tb_data = traceback.format_list(traceback.extract_stack())\r\n```\r\n\r\n## Description\r\n\r\n<!--\r\nProvide an explanation of your issue in an arbitrary form. Provide any additional information, suggested solution and as much context and examples as possible.\r\nIf work on your issue requires account credentials please provide them or explain how one can obtain them.\r\n-->\r\n\r\nusing both youtube-dl and youtube-dl-nightly as compiled from their respective github master repos!\r\n\r\nError 403 when downloading from youtube, resetting cookies and grabbing new cookies did not solve it this time. It worked like 3 hours or so ago, though i DID have issues two days ago where I could fix it myself with the cookie reset. Them Googel Devs trying to mess with downloaders and ad blockers again, I'm guessing.\r\n\r\nExtra Info, VPN in Canada does not help, so I am certain this is not only a Germany Issue.\n", "hints_text": "I'm seeing the same error, same version (installed directly from repo), \r\n\r\nVerbose logs:\r\n```\r\n$ youtube-dl --verbose https://www.youtube.com/watch?v=Di42xlwKNpM\r\n[debug] System config: []\r\n[debug] User config: []\r\n[debug] Custom config: []\r\n[debug] Command-line args: ['--verbose', 'https://www.youtube.com/watch?v=Di42xlwKNpM']\r\n[debug] Encodings: locale UTF-8, fs utf-8, out utf-8, pref UTF-8\r\n[debug] youtube-dl version 2021.12.17\r\n[debug] Python 3.10.12 (CPython x86_64 64bit) - Linux-5.15.0-112-generic-x86_64-with-glibc2.35 - OpenSSL 3.0.2 15 Mar 2022 - glibc 2.35\r\n[debug] exe versions: ffmpeg 4.4.2, ffprobe 4.4.2\r\n[debug] Proxy map: {}\r\n[youtube] Di42xlwKNpM: Downloading webpage\r\nWARNING: [youtube] Di42xlwKNpM: Unable to decode n-parameter: download likely to be throttled (Signature function returned an exception; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; see  https://github.com/ytdl-org/youtube-dl/#user-content-installation  on how to update. Be sure to call youtube-dl with the --verbose option and include the complete output. Traceback (most recent call last):\r\n  File \"/home/facundo/.local/share/fades/e8e62b47-d978-4f96-89e3-044007ddde30/lib/python3.10/site-packages/youtube_dl/extractor/youtube.py\", line 1646, in _decrypt_nsig\r\n    ret = extract_nsig(jsi, func_code)(n)\r\n  File \"/home/facundo/.local/share/fades/e8e62b47-d978-4f96-89e3-044007ddde30/lib/python3.10/site-packages/youtube_dl/extractor/youtube.py\", line 1709, in extract_nsig\r\n    raise JSInterpreter.Exception('Signature function returned an exception')\r\nyoutube_dl.jsinterp.JSInterpreter.Exception: Signature function returned an exception; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; see  https://github.com/ytdl-org/youtube-dl/#user-content-installation  on how to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n)\r\nWARNING: [youtube] Di42xlwKNpM: Unable to decode n-parameter: download likely to be throttled (Signature function returned an exception; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; see  https://github.com/ytdl-org/youtube-dl/#user-content-installation  on how to update. Be sure to call youtube-dl with the --verbose option and include the complete output. Traceback (most recent call last):\r\n  File \"/home/facundo/.local/share/fades/e8e62b47-d978-4f96-89e3-044007ddde30/lib/python3.10/site-packages/youtube_dl/extractor/youtube.py\", line 1646, in _decrypt_nsig\r\n    ret = extract_nsig(jsi, func_code)(n)\r\n  File \"/home/facundo/.local/share/fades/e8e62b47-d978-4f96-89e3-044007ddde30/lib/python3.10/site-packages/youtube_dl/extractor/youtube.py\", line 1709, in extract_nsig\r\n    raise JSInterpreter.Exception('Signature function returned an exception')\r\nyoutube_dl.jsinterp.JSInterpreter.Exception: Signature function returned an exception; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; see  https://github.com/ytdl-org/youtube-dl/#user-content-installation  on how to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n)\r\n[debug] Default format spec: bestvideo+bestaudio/best\r\n[debug] Invoking downloader on 'https://rr1---sn-x1xe7n76.googlevideo.com/videoplayback?expire=1718666275&ei=w29wZvOkAqCOobIPu5u3yAU&ip=181.117.11.18&id=o-AMTwa_TiFGo1qIev-PWUemETHTPUfEFf5jaqu9FZSnGq&itag=136&aitags=133%2C134%2C135%2C136%2C160%2C242%2C243%2C244%2C247%2C278&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&mh=bR&mm=31%2C29&mn=sn-x1xe7n76%2Csn-x1x7dnez&ms=au%2Crdu&mv=m&mvi=1&pl=22&initcwndbps=825000&bui=AbKP-1NAIffWqRbWB2NuJSNiN9smnQRcS1yK11iBNt5XINpC3S5YLwv35pyc3DvjW5LelFIDUH6iM_xE&spc=UWF9fzzcNRdprxblYrh4aLGVwO1OKJEEmISWPFlgQngg1SS6owxJS9wEFw9V&vprv=1&svpuc=1&mime=video%2Fmp4&ns=eT0GHgT7XoDfPw6-aNMW3lEQ&rqh=1&gir=yes&clen=80080955&dur=2247.041&lmt=1718537359256407&mt=1718644382&fvip=5&keepalive=yes&c=WEB&sefc=1&txp=5309224&n=TPAiQsgP23jKbn51DN&sparams=expire%2Cei%2Cip%2Cid%2Caitags%2Csource%2Crequiressl%2Cxpc%2Cbui%2Cspc%2Cvprv%2Csvpuc%2Cmime%2Cns%2Crqh%2Cgir%2Cclen%2Cdur%2Clmt&sig=AJfQdSswRgIhAPGt-d411vDGWdAVIR8GylNywF_AzCbG8OdQx4PBnLUJAiEArFOtCgKxeeG7Bjr-A0mqPGqDw9JAA6OMfhsW38QmpBI%3D&lsparams=mh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Cinitcwndbps&lsig=AHlkHjAwRQIhALOu__tKd8Wtns3dzC8tcIwRYLMI1mcV9IL78XBVvRD_AiA3qHoAUy6eyNliBv_Ve1U-rEKTq4B-oY9eLkIT8MSRTw%3D%3D'\r\n[dashsegments] Total fragments: 8\r\n[download] Destination: David Byrne, bicicletas, Piramidal, Charli XCX y periodistas en el cine-Di42xlwKNpM.f136.mp4\r\n[download] Got server HTTP error: HTTP Error 403: Forbidden. Retrying fragment 1 (attempt 1 of 10)...\r\n[download] Got server HTTP error: HTTP Error 403: Forbidden. Retrying fragment 1 (attempt 2 of 10)...\r\n[download] Got server HTTP error: HTTP Error 403: Forbidden. Retrying fragment 1 (attempt 3 of 10)...\r\n[download] Got server HTTP error: HTTP Error 403: Forbidden. Retrying fragment 1 (attempt 4 of 10)...\r\n[download] Got server HTTP error: HTTP Error 403: Forbidden. Retrying fragment 1 (attempt 5 of 10)...\r\n[download] Got server HTTP error: HTTP Error 403: Forbidden. Retrying fragment 1 (attempt 6 of 10)...\r\n[download] Got server HTTP error: HTTP Error 403: Forbidden. Retrying fragment 1 (attempt 7 of 10)...\r\n[download] Got server HTTP error: HTTP Error 403: Forbidden. Retrying fragment 1 (attempt 8 of 10)...\r\n[download] Got server HTTP error: HTTP Error 403: Forbidden. Retrying fragment 1 (attempt 9 of 10)...\r\n[download] Got server HTTP error: HTTP Error 403: Forbidden. Retrying fragment 1 (attempt 10 of 10)...\r\nERROR: giving up after 10 fragment retries\r\n  File \"/home/facundo/.local/share/fades/e8e62b47-d978-4f96-89e3-044007ddde30/bin/youtube-dl\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/home/facundo/.local/share/fades/e8e62b47-d978-4f96-89e3-044007ddde30/lib/python3.10/site-packages/youtube_dl/__init__.py\", line 473, in main\r\n    _real_main(argv)\r\n  File \"/home/facundo/.local/share/fades/e8e62b47-d978-4f96-89e3-044007ddde30/lib/python3.10/site-packages/youtube_dl/__init__.py\", line 463, in _real_main\r\n    retcode = ydl.download(all_urls)\r\n  File \"/home/facundo/.local/share/fades/e8e62b47-d978-4f96-89e3-044007ddde30/lib/python3.10/site-packages/youtube_dl/YoutubeDL.py\", line 2227, in download\r\n    res = self.extract_info(\r\n  File \"/home/facundo/.local/share/fades/e8e62b47-d978-4f96-89e3-044007ddde30/lib/python3.10/site-packages/youtube_dl/YoutubeDL.py\", line 852, in extract_info\r\n    return self.__extract_info(url, ie, download, extra_info, process)\r\n  File \"/home/facundo/.local/share/fades/e8e62b47-d978-4f96-89e3-044007ddde30/lib/python3.10/site-packages/youtube_dl/YoutubeDL.py\", line 859, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"/home/facundo/.local/share/fades/e8e62b47-d978-4f96-89e3-044007ddde30/lib/python3.10/site-packages/youtube_dl/YoutubeDL.py\", line 966, in __extract_info\r\n    return self.process_ie_result(ie_result, download, extra_info)\r\n  File \"/home/facundo/.local/share/fades/e8e62b47-d978-4f96-89e3-044007ddde30/lib/python3.10/site-packages/youtube_dl/YoutubeDL.py\", line 1000, in process_ie_result\r\n    return self.process_video_result(ie_result, download=download)\r\n  File \"/home/facundo/.local/share/fades/e8e62b47-d978-4f96-89e3-044007ddde30/lib/python3.10/site-packages/youtube_dl/YoutubeDL.py\", line 1844, in process_video_result\r\n    self.process_info(new_info)\r\n  File \"/home/facundo/.local/share/fades/e8e62b47-d978-4f96-89e3-044007ddde30/lib/python3.10/site-packages/youtube_dl/YoutubeDL.py\", line 2126, in process_info\r\n    partial_success = dl(fname, new_info)\r\n  File \"/home/facundo/.local/share/fades/e8e62b47-d978-4f96-89e3-044007ddde30/lib/python3.10/site-packages/youtube_dl/YoutubeDL.py\", line 2071, in dl\r\n    return fd.download(name, new_info)\r\n  File \"/home/facundo/.local/share/fades/e8e62b47-d978-4f96-89e3-044007ddde30/lib/python3.10/site-packages/youtube_dl/downloader/common.py\", line 380, in download\r\n    return self.real_download(filename, info_dict)\r\n  File \"/home/facundo/.local/share/fades/e8e62b47-d978-4f96-89e3-044007ddde30/lib/python3.10/site-packages/youtube_dl/downloader/dash.py\", line 78, in real_download\r\n    self.report_error('giving up after %s fragment retries' % count)\r\n  File \"/home/facundo/.local/share/fades/e8e62b47-d978-4f96-89e3-044007ddde30/lib/python3.10/site-packages/youtube_dl/downloader/common.py\", line 175, in report_error\r\n    self.ydl.report_error(*args, **kargs)\r\n  File \"/home/facundo/.local/share/fades/e8e62b47-d978-4f96-89e3-044007ddde30/lib/python3.10/site-packages/youtube_dl/YoutubeDL.py\", line 662, in report_error\r\n    self.trouble(*args, **kwargs)\r\n  File \"/home/facundo/.local/share/fades/e8e62b47-d978-4f96-89e3-044007ddde30/lib/python3.10/site-packages/youtube_dl/YoutubeDL.py\", line 614, in trouble\r\n    tb_data = traceback.format_list(traceback.extract_stack())\r\n```\r\n\nWait NOW it is working again, i just tested YOUR youtube link and it works now with mine too...\n> Wait NOW it is working again, i just tested YOUR youtube link and it works now with mine too...\r\n\r\nDo you have the verbose log from when it worked? Do you know if it was using the same player id `590f65a6`?\r\n\r\nA couple of videos I am trying to pull are not pulling at the moment with this error and using the same player Id as your verbose log for the failure, `590f65a6`.\nNo I dont have it but I can MAKE ONE! *redownloads video*\r\n\r\n```\r\n[debug] System config: []\r\n[debug] User config: ['--no-mtime', '--match-filter', '!is_live', '--retries', 'infinite', '--fragment-retries', '2', '--skip-unavailable-fragments', '--restrict-filenames', '-i', '-o', '/home/gregorius/home/pending/videos/%(title)s___%(id)s.webm', '-f', '(bestvideo[height<=360]+worstaudio/best[height<=360])[protocol!=http_dash_segments][container!^=dash]', '--console-title', '--hls-prefer-native', '--no-cache-dir', '--http-chunk-size', '100M', '--cookies', '/home/gregorius/home/scripts/video/youtube-dl-cookies']\r\n[debug] Custom config: []\r\n[debug] Command-line args: ['https://www.youtube.com/watch?v=I0EmyNep1pE', '-vf', '(242+249/242+250/242+171/242+251)/(243+249/243+250/243+171/243+251)/18', '--no-playlist']\r\n[debug] Encodings: locale UTF-8, fs utf-8, out utf-8, pref UTF-8\r\n[debug] youtube-dl version 2024.06.12 [0153b387e] (single file build)\r\n[debug] ** This version was built from the latest master code at https://github.com/ytdl-org/youtube-dl.\r\n[debug] ** For support, visit the main site.\r\n[debug] Python 3.10.12 (CPython x86_64 64bit) - Linux-5.15.0-112-generic-x86_64-with-glibc2.35 - OpenSSL 3.0.2 15 Mar 2022 - glibc 2.35\r\n[debug] exe versions: ffmpeg 4.4.2, ffprobe 4.4.2, rtmpdump 2.4\r\n[debug] Proxy map: {}\r\n[youtube] I0EmyNep1pE: Downloading webpage\r\n[youtube] Downloading just video I0EmyNep1pE because of --no-playlist\r\n[youtube] I0EmyNep1pE: Downloading player 84314bef\r\n[debug] [youtube] Decrypted nsig Q8M2qqYzwfYVCSffYx => H5Oh-7-CbNUKzg\r\n[debug] [youtube] Decrypted nsig mry5phGrNQ4Z-140dA => 9V0Qv9eOFODk5g\r\n[debug] Invoking downloader on 'https://rr2---sn-4g5lznes.googlevideo.com/videoplayback?expire=1718667532&ei=rHRwZomwDLXE6dsP7p-FgAI&ip=47.69.137.137&id=o-AKVn8Xp4mFVDiSw6Xqxu_ltPK2qfKGBx-mg37TKGjQWT&itag=242&aitags=133%2C134%2C135%2C136%2C160%2C242%2C243%2C244%2C247%2C278%2C298%2C299%2C302%2C303&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&mh=zO&mm=31%2C26&mn=sn-4g5lznes%2Csn-f5f7lne6&ms=au%2Conr&mv=m&mvi=2&pl=19&pcm2=yes&initcwndbps=1607500&bui=AbKP-1NHA94gcB9G6dDwG2Jqpb8Dwscp6kPFC9tVS3PeyuM9AnupAXjNuTzZsR1QV7ob2Y8ALRn369yK&spc=UWF9fySpyqOECqZK2l0FsHX9SwfHSlfVv54BD9k_2xw1v6r3g1LmYqpSFXRN&vprv=1&svpuc=1&mime=video%2Fwebm&ns=oQ47xPQq799NE5PjYPO0OcYQ&rqh=1&gir=yes&clen=1016042&dur=191.725&lmt=1718607835104876&mt=1718645576&fvip=5&keepalive=yes&c=WEB&sefc=1&txp=630F224&n=9V0Qv9eOFODk5g&sparams=expire%2Cei%2Cip%2Cid%2Caitags%2Csource%2Crequiressl%2Cxpc%2Cpcm2%2Cbui%2Cspc%2Cvprv%2Csvpuc%2Cmime%2Cns%2Crqh%2Cgir%2Cclen%2Cdur%2Clmt&sig=AJfQdSswRQIgTwJOodNmtkabWPyUOUuaOQHgGT11pR90l5fIKDloXqYCIQDit2olRgTA7G2gTFU19aRZX4P5lyyhS6qTlDXk7STPKQ%3D%3D&lsparams=mh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Cinitcwndbps&lsig=AHlkHjAwRQIgKx6HGCFPgy4n0hfh638Ln4W-jtgITXZ6wbXmtfNzviACIQDdcitWojgz7QBWLHvNg4563ymSW1uQxL960iT5TAKnQw%3D%3D'\r\n[dashsegments] Total fragments: 1\r\n[download] Destination: /home/gregorius/home/pending/videos/This_Guy_Uses_ROOK___I0EmyNep1pE.f242.webm\r\n[download] 100% of 992.23KiB in 00:00\r\n[debug] Invoking downloader on 'https://rr2---sn-4g5lznes.googlevideo.com/videoplayback?expire=1718667532&ei=rHRwZomwDLXE6dsP7p-FgAI&ip=47.69.137.137&id=o-AKVn8Xp4mFVDiSw6Xqxu_ltPK2qfKGBx-mg37TKGjQWT&itag=249&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&mh=zO&mm=31%2C26&mn=sn-4g5lznes%2Csn-f5f7lne6&ms=au%2Conr&mv=m&mvi=2&pl=19&pcm2=yes&initcwndbps=1607500&bui=AbKP-1NHA94gcB9G6dDwG2Jqpb8Dwscp6kPFC9tVS3PeyuM9AnupAXjNuTzZsR1QV7ob2Y8ALRn369yK&spc=UWF9fySpyqOECqZK2l0FsHX9SwfHSlfVv54BD9k_2xw1v6r3g1LmYqpSFXRN&vprv=1&svpuc=1&mime=audio%2Fwebm&ns=oQ47xPQq799NE5PjYPO0OcYQ&rqh=1&gir=yes&clen=1161452&dur=191.761&lmt=1718607847881176&mt=1718645576&fvip=5&keepalive=yes&c=WEB&sefc=1&txp=6308224&n=9V0Qv9eOFODk5g&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cpcm2%2Cbui%2Cspc%2Cvprv%2Csvpuc%2Cmime%2Cns%2Crqh%2Cgir%2Cclen%2Cdur%2Clmt&sig=AJfQdSswRQIgG8BruVg_fAUCrYbyCRmJ79AxjhHN4WaqNAxTIczItH0CIQCFtjJ11spRyfsE3h43xTJjW4p17e-Dq2CpOW9dn2tlFA%3D%3D&lsparams=mh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Cinitcwndbps&lsig=AHlkHjAwRQIgKx6HGCFPgy4n0hfh638Ln4W-jtgITXZ6wbXmtfNzviACIQDdcitWojgz7QBWLHvNg4563ymSW1uQxL960iT5TAKnQw%3D%3D'\r\n[dashsegments] Total fragments: 1\r\n[download] Destination: /home/gregorius/home/pending/videos/This_Guy_Uses_ROOK___I0EmyNep1pE.f249.webm\r\n[download] 100% of 1.11MiB in 00:00\r\n[ffmpeg] Merging formats into \"/home/gregorius/home/pending/videos/This_Guy_Uses_ROOK___I0EmyNep1pE.webm\"\r\n[debug] ffmpeg command line: ffmpeg -y -loglevel repeat+info -i file:/home/gregorius/home/pending/videos/This_Guy_Uses_ROOK___I0EmyNep1pE.f242.webm -i file:/home/gregorius/home/pending/videos/This_Guy_Uses_ROOK___I0EmyNep1pE.f249.webm -c copy -map 0:v:0 -map 1:a:0 file:/home/gregorius/home/pending/videos/This_Guy_Uses_ROOK___I0EmyNep1pE.temp.webm\r\nDeleting original file /home/gregorius/home/pending/videos/This_Guy_Uses_ROOK___I0EmyNep1pE.f242.webm (pass -k to keep)\r\nDeleting original file /home/gregorius/home/pending/videos/This_Guy_Uses_ROOK___I0EmyNep1pE.f249.webm (pass -k to keep)\r\n```\r\n\n> Downloading player 590f65a6\r\n\r\nYeah, it looks like it changed to a different player ID when it worked.\r\n\r\nPlayer Id when error: 590f65a6\r\nPlayer Id when work: 84314bef\nWe can acquire the [problem player JS](https://www.youtube.com/s/player/590f65a6/player_ias.vflset/en_US/base.js) and find out what feature breaks the n-sig descrambling. If this is a roll-out to `590f65a6` doing so becomes important. If not, it's for the back-burner in case YT does the same again.\nIndeed, I am getting `590f65a6` in the UK.\r\n\r\nAs usual there is the definition of <details><summary>the pointless obfuscated descrambling function</summary>\r\n\r\n```js\r\nIma=function(a){var b=a.split(\"\"),c=[481196748,function(d,e){d.splice(d.length,0,e)},\r\n-1184122307,-1844585791,-1517999176,-1838315746,-391786256,function(d,e,f,h,l){return e(f,h,l)},\r\n311082973,777402095,-513835569,-1517999176,b,777402095,b,function(d,e){e.length!=0&&(d=(d%e.length+e.length)%e.length,e.splice(0,1,e.splice(d,1,e[0])[0]))},\r\n\"/([\\\\/,[\",function(d,e){d.push(e)},\r\n1836568272,\",54];c[51]=c;\",-132471649,function(d,e,f,h,l,m,n,p,q){return f(h,l,m,n,p,q)},\r\n2099923865,-1844585791,function(d,e,f,h,l,m,n,p){return e(f,h,l,m,n,p)},\r\n-1177328935,1637869400,1340863434,1591384456,function(d){throw d;},\r\n-893696909,-1785346148,186785386,512500232,976315399,55621845,function(d,e,f,h,l,m,n){return d(l,m,n)},\r\n1968846220,-120562974,1415280244,function(){for(var d=64,e=[];++d-e.length-32;){switch(d){case 91:d=44;continue;case 123:d=65;break;case 65:d-=18;continue;case 58:d=96;continue;case 46:d=95}e.push(String.fromCharCode(d))}return e},\r\n-1617495598,null,function(d){d.reverse()},\r\nfunction(d,e){e=(e%d.length+d.length)%d.length;d.splice(e,1)},\r\n-439060673,2108931577,function(d,e,f){var h=d.length;e.forEach(function(l,m,n){this.push(n[m]=d[(d.indexOf(l)-d.indexOf(this[m])+m+h--)%d.length])},f.split(\"\"))},\r\n-206894781,\"o6LTxqq\",-530102236,-785373294,function(d,e,f,h,l,m){return e(h,l,m)},\r\nnull,null,1096979497,\"\\u2210]}(/\",1867530696,196808569,-1759560780,-120562974,-939717184,1123017595,function(){for(var d=64,e=[];++d-e.length-32;)switch(d){case 58:d=96;continue;case 91:d=44;break;case 65:d=47;continue;case 46:d=153;case 123:d-=58;default:e.push(String.fromCharCode(d))}return e},\r\n976315399,886367998,1481614131,-1632737085,1506052226,-1259426209,function(d){for(var e=d.length;e;)d.push(d.splice(--e,1)[0])},\r\nfunction(d,e){e=(e%d.length+d.length)%d.length;d.splice(-e).reverse().forEach(function(f){d.unshift(f)})},\r\n-963596628,1599531971,1653761992,193670587,-565480227,b,-1733454587,function(){for(var d=64,e=[];++d-e.length-32;){switch(d){case 58:d-=14;case 91:case 92:case 93:continue;case 123:d=47;case 94:case 95:case 96:continue;case 46:d=95}e.push(String.fromCharCode(d))}return e},\r\n-147499041,-414478713,444508829,302113098,2140193951,714051216,1812860928,function(){for(var d=64,e=[];++d-e.length-32;)switch(d){case 46:d=95;default:e.push(String.fromCharCode(d));case 94:case 95:case 96:break;case 123:d-=76;case 92:case 93:continue;case 58:d=44;case 91:}return e},\r\nfunction(d,e){if(d.length!=0){e=(e%d.length+d.length)%d.length;var f=d[0];d[0]=d[e];d[e]=f}},\r\nfunction(d,e){for(d=(d%e.length+e.length)%e.length;d--;)e.unshift(e.pop())},\r\n-949341690,-2000214115,2049985941];c[42]=c;c[53]=c;c[54]=c;try{try{c[84]<6?((0,c[44])(c[new Date(\"1969-12-31T14:45:53.000-09:15\")/1E3],c[62]),c[71])(c[Math.pow(2,2)+7600-7592],c[new Date(\"1970-01-01T09:01:20.000+09:00\")/1E3]):((0,c[88])(c[42],c[0]),(0,c[88])(c[0],c[28])),(0,c[7])(((0,c[29])(c[77]),c[47])((0,c[63])(),c[14],c[49]),c[11+Math.pow(6,new Date(\"1970-01-01T05:15:02.000+05:15\")/1E3)-3],c[76],c[26])}catch(d){c[54]>new Date(\"1970-01-01T07:00:00.000+07:00\")/1E3&&(c[51]>-9||((0,c[52])((0,c[43])(c[12]),\r\nc[15],(0,c[89])(c[86],c[53]),c[22],c[66]),(0,c[71])((0,c[10])(),c[8],c[73]),0))&&(((0,c[31])((0,c[71])((0,c[10])(),c[36],c[73]),c[1],c[77]),c[19])(c[66],c[57]),c[71])((0,c[18])(),c[8],c[73]),c[72]===7?(0,c[48])(((0,c[68])(c[8],c[79]),c[68])(c[38],c[62]),c[76],(0,c[68])(c[8],c[4]),c[20],(0,c[71])((0,c[18])(),c[38],c[73]),c[44],c[Math.pow(1,1)-7200+7235]):(0,c[76])((0,c[68])(c[38],c[3])+(0,c[1])(c[8]),c[20],(0,c[31])((0,c[19])(c[7],c[42]),c[20],c[63],c[8]),c[29],c[66]),c[72]>=9?(0,c[70])(c[10],c[83]):\r\n(0,c[188%Math.pow(4,4)-118])(c[10],c[25]),c[14]>9?((0,c[70])(c[40],c[30+Math.pow(6,1)- -31]),c[73])((0,c[2])(),c[10],c[75]):(0,c[33])((0,c[4])(c[68],c[89]),c[40],c[29],c[70]),c[79]==-1?(0,c[52])((0,c[40])(c[57],c[33]),c[22-Math.pow(7,1)- -74],c[57],c[72]):((0,c[60])(c[53],c[Math.pow(6,2)-69+38]),c[88])(c[59])}finally{c[55]===-8+Math.pow(4,3)%17?((((0,c[52])((0,c[41])(c[45],c[28]),c[89],c[57],c[79]),c[-503+105*Math.pow(5,1)])(c[new Date(\"1970-01-01T05:16:27.000+05:15\")/1E3]),c[40])(c[29],c[26]),c[92])((0,c[31])(),\r\nc[29],c[1]):(0,c[52])(((0,c[4])((0,c[41])(c[new Date(\"1969-12-31T21:16:17.000-02:45\")/1E3],c[59]),c[Math.pow(3,4)+-31725- -31704],(0,c[88])(c[5]),c[56],c[35]),c[70])(c[64]),c[40],c[28],c[67])}try{c[3]==-5&&((0,c[89])(c[29],c[35]),\"NaN\")||(0,c[88])(c[28]),c[42]<5&&(c[73]<=0||(((0,c[51])(c[82],c[63]),c[52])(c[87],c[2]),\"\"))&&(0,c[40])((0,c[32])(c[new Date(\"1970-01-01T05:01:05.000+05:00\")/1E3],c[64]),c[3],c[63],c[41]),(0,c[18])(c[87]),(0,c[40])((0,c[53])(),c[63],c[91]),(0,c[40])((0,c[53])(),c[35],c[91])}catch(d){c[57]!==\r\n8&&(c[60-196%Math.pow(1,4)]>=59-Math.pow(1,2)+-50||((0,c[26])((0,c[0])((0,c[69])(c[33],c[58])|(0,c[0])((0,c[69])(c[87],c[62]),c[64],c[5],c[88]),c[38],c[69]),(0,c[85])(c[22],c[5]),c[57],(0,c[19])(c[72],c[32]),c[0],(0,c[19])(c[2],c[33]),c[46],c[6]),0))&&((((0,c[new Date(\"1970-01-01T06:30:13.000+06:30\")/1E3])((0,c[26])(),c[6],c[65]),c[24])(c[17],c[38]),((0,c[13])((0,c[74])(),c[37],c[65]),c[25])(c[38],c[47]),c[62])((0,c[70])(c[6],c[64]),c[43],(0,c[13])((0,c[74])(),c[37],c[65]),c[6],c[16]),c[13])((0,c[74])(),\r\nc[37],c[65]),c[57]!=4&&(c[51]==-7&&((0,c[24])(c[10],c[72]),1)||(0,c[24])(c[83],c[37]))}finally{c[49]>=4&&(c[22]!==-9?((0,c[78])(c[5],(0,c[25])(c[6],c[66]),(0,c[5])(c[28],c[8]),(0,c[44])(c[37]),c[53],c[38]),c[67])((0,c[-62-Math.pow(7,1)*-19])(c[6]),c[70],c[6],c[Math.pow(5,5)+-65280- -62178]):((0,c[67])(((((0,c[35-Math.pow(8,2)+42])((0,c[74])(),c[8],c[65]),c[70])(c[37],c[64]),c[70])(c[38],c[84]),c[25])(c[Math.pow(4,1)-16+73],c[48]),c[13],(0,c[45])(),c[6],c[65]),c[70])(c[37],c[63]))}try{c[68]!=9&&(c[29]==\r\n-9||(((0,c[67])((0,c[19])(c[61],c[75]),c[24],c[68],c[38]),c[48])(c[38]),null))&&(((0,c[73])(c[Math.pow(8,1)*-149+1237]),c[47])(c[20-308%Math.pow(4,2)],c[68]),c[47])(c[80],c[38])}catch(d){(0,c[46])(c[16])}}catch(d){return\"enhanced_except_r5sB0OT-_w8_\"+a}return b.join(\"\")};\r\n```\r\n</details>\r\n\r\nand a pointless redirection\r\n```js\r\nvar HRa=[Ima];\r\n```\r\nand the code that calls the descrambler (`b` is the n-parameter)\r\n```js\r\na.D&&(b=a.get(\"n\"))&&(b=HRa[0](b),a.set(\"n\",b),HRa.length||Ima(\"\"))\r\n```\r\nPresumably the children who committed this nonsense bike or EV and search out \"environmentally-friendly\" products and yet they acquiesce in the waste of electricity by their billions of users who aren't savvy enough to avoid YT's JS.\r\n\nI have the same trouble\nI'm getting this error too:\r\n```\r\n$ ../youtube-dl --verbose --format=18 \"https://www.youtube.com/watch?v=qCvqZPf3smI\" -r 100k\r\n[debug] System config: []\r\n[debug] User config: []\r\n[debug] Custom config: []\r\n[debug] Command-line args: ['--verbose', '--format=18', 'https://www.youtube.com/watch?v=qCvqZPf3smI', '-r', '100k']\r\n[debug] Encodings: locale UTF-8, fs utf-8, out utf-8, pref UTF-8\r\n[debug] youtube-dl version 2024.06.12 [0153b387e] (single file build)\r\n[debug] ** This version was built from the latest master code at https://github.com/ytdl-org/youtube-dl.\r\n[debug] ** For support, visit the main site.\r\n[debug] Python 3.10.7 (CPython x86_64 64bit) - Linux-6.8.8-gnu-x86_64-with-glibc2.35 - OpenSSL 3.0.8 7 Feb 2023 - glibc 2.35\r\n[debug] exe versions: ffmpeg 6.1.1, ffprobe 6.1.1\r\n[debug] Proxy map: {}\r\n[youtube] qCvqZPf3smI: Downloading webpage\r\nWARNING: [youtube] qCvqZPf3smI: Unable to decode n-parameter: download likely to be throttled (Signature function returned an exception; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output. Traceback (most recent call last):\r\n  File \"/home/zacchae/tmp/vids/../youtube-dl/youtube_dl/extractor/youtube.py\", line 1646, in _decrypt_nsig\r\n    ret = extract_nsig(jsi, func_code)(n)\r\n  File \"/home/zacchae/tmp/vids/../youtube-dl/youtube_dl/extractor/youtube.py\", line 1709, in extract_nsig\r\n    raise JSInterpreter.Exception('Signature function returned an exception')\r\nyoutube_dl.jsinterp.JSInterpreter.Exception: Signature function returned an exception; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n)\r\nWARNING: [youtube] qCvqZPf3smI: Unable to decode n-parameter: download likely to be throttled (Signature function returned an exception; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output. Traceback (most recent call last):\r\n  File \"/home/zacchae/tmp/vids/../youtube-dl/youtube_dl/extractor/youtube.py\", line 1646, in _decrypt_nsig\r\n    ret = extract_nsig(jsi, func_code)(n)\r\n  File \"/home/zacchae/tmp/vids/../youtube-dl/youtube_dl/extractor/youtube.py\", line 1709, in extract_nsig\r\n    raise JSInterpreter.Exception('Signature function returned an exception')\r\nyoutube_dl.jsinterp.JSInterpreter.Exception: Signature function returned an exception; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n)\r\n[debug] Invoking downloader on 'https://rr3---sn-o097znsk.googlevideo.com/videoplayback?expire=1718680265&ei=aaZwZqXEFOzosfIPlrCj8Ac&ip=192.147.44.15&id=o-AHrcaP0CtzSnhKK9oCPswB0Mgb7D6ioDlGdDoDhrOaCu&itag=18&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&mh=94&mm=31%2C29&mn=sn-o097znsk%2Csn-n4v7snlr&ms=au%2Crdu&mv=m&mvi=3&pl=24&initcwndbps=7681250&bui=AbKP-1M6Y7_ZJfPtf6HeYtQ6JCt6coBSpmC_6-A1Q-mt7m43HVsvsTl6vmKv_usr3-c0DJ_Et_Uq9F0Y&spc=UWF9f-OGHzvs3XPOTMgbQkX5jhJBdo3PYjZizrDhGRfy8VsyLXnB5FqbITzh&vprv=1&svpuc=1&mime=video%2Fmp4&ns=uviRDdrPrAaNvd34t5_kJscQ&rqh=1&gir=yes&clen=270741677&ratebypass=yes&dur=7915.659&lmt=1718385543727535&mt=1718658290&fvip=1&c=WEB&sefc=1&txp=7209224&n=Y3iqaxF9noQUd0eCCw&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cbui%2Cspc%2Cvprv%2Csvpuc%2Cmime%2Cns%2Crqh%2Cgir%2Cclen%2Cratebypass%2Cdur%2Clmt&sig=AJfQdSswRQIhAPWrdP6or4ois_TBf9OlL5g1Q-R2o2SnJYproScqiI3kAiAJ9L7pNtWcUeqnW65qk6E4SrWRTn0YYBUcNnOtkB8jiA%3D%3D&lsparams=mh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Cinitcwndbps&lsig=AHlkHjAwRQIgWJIrtReMtuOiz5K9NdOIimcW_SgSFGAnQuF8XGkAle0CIQDkOkAi54SfNQcsxDS7j25Rikjar0hW_Dp2cmmlpndoqw%3D%3D'\r\nERROR: unable to download video data: HTTP Error 403: Forbidden\r\nTraceback (most recent call last):\r\n  File \"/home/zacchae/tmp/vids/../youtube-dl/youtube_dl/YoutubeDL.py\", line 2136, in process_info\r\n    success = dl(filename, info_dict)\r\n  File \"/home/zacchae/tmp/vids/../youtube-dl/youtube_dl/YoutubeDL.py\", line 2075, in dl\r\n    return fd.download(name, new_info)\r\n  File \"/home/zacchae/tmp/vids/../youtube-dl/youtube_dl/downloader/common.py\", line 380, in download\r\n    return self.real_download(filename, info_dict)\r\n  File \"/home/zacchae/tmp/vids/../youtube-dl/youtube_dl/downloader/http.py\", line 349, in real_download\r\n    establish_connection()\r\n  File \"/home/zacchae/tmp/vids/../youtube-dl/youtube_dl/downloader/http.py\", line 116, in establish_connection\r\n    raise err\r\n  File \"/home/zacchae/tmp/vids/../youtube-dl/youtube_dl/downloader/http.py\", line 110, in establish_connection\r\n    ctx.data = self.ydl.urlopen(request)\r\n  File \"/home/zacchae/tmp/vids/../youtube-dl/youtube_dl/YoutubeDL.py\", line 2474, in urlopen\r\n    return self._opener.open(req, timeout=self._socket_timeout)\r\n  File \"/gnu/store/1w5v338qk5m8khcazwclprs3znqp6f7f-python-3.10.7/lib/python3.10/urllib/request.py\", line 525, in open\r\n    response = meth(req, response)\r\n  File \"/gnu/store/1w5v338qk5m8khcazwclprs3znqp6f7f-python-3.10.7/lib/python3.10/urllib/request.py\", line 634, in http_response\r\n    response = self.parent.error(\r\n  File \"/gnu/store/1w5v338qk5m8khcazwclprs3znqp6f7f-python-3.10.7/lib/python3.10/urllib/request.py\", line 563, in error\r\n    return self._call_chain(*args)\r\n  File \"/gnu/store/1w5v338qk5m8khcazwclprs3znqp6f7f-python-3.10.7/lib/python3.10/urllib/request.py\", line 496, in _call_chain\r\n    result = func(*args)\r\n  File \"/gnu/store/1w5v338qk5m8khcazwclprs3znqp6f7f-python-3.10.7/lib/python3.10/urllib/request.py\", line 643, in http_error_default\r\n    raise HTTPError(req.full_url, code, msg, hdrs, fp)\r\nurllib.error.HTTPError: HTTP Error 403: Forbidden\r\n```\r\nIs there a work around?\nI'm getting same error (same problematic player ID `590f65a6`):\r\n\r\n```\r\n$ youtube-dl --verbose -f251 youtu.be/nVmaUlPHEuc\r\n[debug] System config: []\r\n[debug] User config: []\r\n[debug] Custom config: []\r\n[debug] Command-line args: ['--verbose', '-f251', 'youtu.be/nVmaUlPHEuc']\r\n[debug] Encodings: locale UTF-8, fs utf-8, out utf-8, pref UTF-8\r\n[debug] youtube-dl version 2021.12.17\r\n[debug] Git HEAD: be008e657\r\n[debug] Python 3.12.3 (CPython x86_64 64bit) - Linux-6.8.0-35-generic-x86_64-with-glibc2.39 - OpenSSL 3.0.13 30 Jan 2024 - glibc 2.39\r\n[debug] exe versions: ffmpeg 6.1.1, ffprobe 6.1.1\r\n[debug] Proxy map: {}\r\nWARNING: The url doesn't specify the protocol, trying with http\r\n[youtube] nVmaUlPHEuc: Downloading webpage\r\nWARNING: [youtube] Unable to decode n-parameter: download likely to be throttled (Unhandled exception in decode; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see  https://yt-dl.org/update  on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output. Traceback (most recent call last):\r\n  File \"/home/ek/PycharmProjects/youtube-dl/youtube_dl/extractor/youtube.py\", line 1677, in _n_descramble\r\n    raise ExtractorError('Unhandled exception in decode')\r\nyoutube_dl.utils.ExtractorError: Unhandled exception in decode; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see  https://yt-dl.org/update  on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.\r\n)\r\n[debug] Invoking downloader on 'https://rr1---sn-a5mlrnll.googlevideo.com/videoplayback?expire=1718687135&ei=P8FwZvr9Gb6psfIPo7q_kAU&ip=2600%3A8801%3A7100%3A1b1%3A94b8%3A1124%3A8247%3Aae96&id=o-AN1qHbHALflbpvUnAAQVR05d8gXcRM0RmPfUHH5S-ZHS&itag=251&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&mh=Y2&mm=31%2C29&mn=sn-a5mlrnll%2Csn-a5meknzk&ms=au%2Crdu&mv=m&mvi=1&pl=44&initcwndbps=1745000&bui=AbKP-1O6ytsgDa3rSOv-UVcs5eR_fP4hqDYlqTD_ZBtlIdk-raYV3EjT7ByG_luTKJmBbh4B5o1rk7B9&spc=UWF9f9rYu_d5FgsWpssUxk1-1bKcFzjGUteTdHiSPLRyGPisXl-PTu9LedSx&vprv=1&svpuc=1&mime=audio%2Fwebm&ns=O6oxW6O2iJoYIKaCPusXVS8Q&rqh=1&gir=yes&clen=79400050&dur=4440.061&lmt=1696726699824311&mt=1718665260&fvip=2&keepalive=yes&c=WEB&sefc=1&txp=4532434&n=bDlmQuYiHQDaGLz2Vd&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cbui%2Cspc%2Cvprv%2Csvpuc%2Cmime%2Cns%2Crqh%2Cgir%2Cclen%2Cdur%2Clmt&sig=AJfQdSswRAIgWZwcc6vchS9j7TJqT0KPhk66Xxo7MgR6jwZmujF4jIICIBJi-aZfzgcq1DEWPZh5ahBdQxhMzb2iq9gSYTBXkqIs&lsparams=mh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Cinitcwndbps&lsig=AHlkHjAwRgIhAOO6xH7uiXRJ5iOBAWrWt-cwxhE_7LKYQcHjmhmTc_4uAiEAx7NgWb3BcVKqy6HSNqS761qtpuja_f0TK9FPSIKHYV0%3D'\r\n[dashsegments] Total fragments: 8\r\n[download] Destination: Shigatsu wa Kimi no Uso (Your Lie in April) OST - Disc 1 [Marathon]-nVmaUlPHEuc.webm\r\n[download] Got server HTTP error: HTTP Error 403: Forbidden. Retrying fragment 1 (attempt 1 of 10)...\r\n[download] Got server HTTP error: HTTP Error 403: Forbidden. Retrying fragment 1 (attempt 2 of 10)...\r\n[download] Got server HTTP error: HTTP Error 403: Forbidden. Retrying fragment 1 (attempt 3 of 10)...\r\n[download] Got server HTTP error: HTTP Error 403: Forbidden. Retrying fragment 1 (attempt 4 of 10)...\r\n[download] Got server HTTP error: HTTP Error 403: Forbidden. Retrying fragment 1 (attempt 5 of 10)...\r\n[download] Got server HTTP error: HTTP Error 403: Forbidden. Retrying fragment 1 (attempt 6 of 10)...\r\n[download] Got server HTTP error: HTTP Error 403: Forbidden. Retrying fragment 1 (attempt 7 of 10)...\r\n[download] Got server HTTP error: HTTP Error 403: Forbidden. Retrying fragment 1 (attempt 8 of 10)...\r\n[download] Got server HTTP error: HTTP Error 403: Forbidden. Retrying fragment 1 (attempt 9 of 10)...\r\n[download] Got server HTTP error: HTTP Error 403: Forbidden. Retrying fragment 1 (attempt 10 of 10)...\r\nERROR: giving up after 10 fragment retries\r\n  File \"/home/ek/bin/youtube-dl\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/home/ek/PycharmProjects/youtube-dl/youtube_dl/__init__.py\", line 473, in main\r\n    _real_main(argv)\r\n  File \"/home/ek/PycharmProjects/youtube-dl/youtube_dl/__init__.py\", line 463, in _real_main\r\n    retcode = ydl.download(all_urls)\r\n  File \"/home/ek/PycharmProjects/youtube-dl/youtube_dl/YoutubeDL.py\", line 2218, in download\r\n    res = self.extract_info(\r\n  File \"/home/ek/PycharmProjects/youtube-dl/youtube_dl/YoutubeDL.py\", line 852, in extract_info\r\n    return self.__extract_info(url, ie, download, extra_info, process)\r\n  File \"/home/ek/PycharmProjects/youtube-dl/youtube_dl/YoutubeDL.py\", line 859, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"/home/ek/PycharmProjects/youtube-dl/youtube_dl/YoutubeDL.py\", line 966, in __extract_info\r\n    return self.process_ie_result(ie_result, download, extra_info)\r\n  File \"/home/ek/PycharmProjects/youtube-dl/youtube_dl/YoutubeDL.py\", line 1004, in process_ie_result\r\n    return self.extract_info(ie_result['url'],\r\n  File \"/home/ek/PycharmProjects/youtube-dl/youtube_dl/YoutubeDL.py\", line 852, in extract_info\r\n    return self.__extract_info(url, ie, download, extra_info, process)\r\n  File \"/home/ek/PycharmProjects/youtube-dl/youtube_dl/YoutubeDL.py\", line 859, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"/home/ek/PycharmProjects/youtube-dl/youtube_dl/YoutubeDL.py\", line 966, in __extract_info\r\n    return self.process_ie_result(ie_result, download, extra_info)\r\n  File \"/home/ek/PycharmProjects/youtube-dl/youtube_dl/YoutubeDL.py\", line 1000, in process_ie_result\r\n    return self.process_video_result(ie_result, download=download)\r\n  File \"/home/ek/PycharmProjects/youtube-dl/youtube_dl/YoutubeDL.py\", line 1835, in process_video_result\r\n    self.process_info(new_info)\r\n  File \"/home/ek/PycharmProjects/youtube-dl/youtube_dl/YoutubeDL.py\", line 2123, in process_info\r\n    success = dl(filename, info_dict)\r\n  File \"/home/ek/PycharmProjects/youtube-dl/youtube_dl/YoutubeDL.py\", line 2062, in dl\r\n    return fd.download(name, new_info)\r\n  File \"/home/ek/PycharmProjects/youtube-dl/youtube_dl/downloader/common.py\", line 380, in download\r\n    return self.real_download(filename, info_dict)\r\n  File \"/home/ek/PycharmProjects/youtube-dl/youtube_dl/downloader/dash.py\", line 73, in real_download\r\n    self.report_error('giving up after %s fragment retries' % count)\r\n  File \"/home/ek/PycharmProjects/youtube-dl/youtube_dl/downloader/common.py\", line 175, in report_error\r\n    self.ydl.report_error(*args, **kargs)\r\n  File \"/home/ek/PycharmProjects/youtube-dl/youtube_dl/YoutubeDL.py\", line 662, in report_error\r\n    self.trouble(*args, **kwargs)\r\n  File \"/home/ek/PycharmProjects/youtube-dl/youtube_dl/YoutubeDL.py\", line 614, in trouble\r\n    tb_data = traceback.format_list(traceback.extract_stack())\r\n```\nI was mucking around and got a different player ID `84314bef` once (pure luck, or pure unluck because that opportunity was lost on the `-F` query ;_;); all the other 100+ attempts returned the same problematic player ID `590f65a6`.  I believe it is safe to assume that a rollout is in progress.\nA brute-force workaround:\r\n\r\n```diff\r\ndiff --git a/youtube_dl/extractor/youtube.py b/youtube_dl/extractor/youtube.py\r\nindex 3bf483c1c..87fb14124 100644\r\n--- a/youtube_dl/extractor/youtube.py\r\n+++ b/youtube_dl/extractor/youtube.py\r\n@@ -1472,6 +1472,8 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\r\n                 break\r\n         else:\r\n             raise ExtractorError('Cannot identify player %r' % player_url)\r\n+        player_id = id_m.group('id')\r\n+        assert player_id != '590f65a6'\r\n         return id_m.group('id')\r\n\r\n     def _get_player_code(self, video_id, player_url, player_id=None):\r\n@@ -1680,6 +1682,7 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\r\n                 self._downloader.to_screen('[debug] [%s] %s' % (self.IE_NAME, 'Decrypted nsig {0} => {1}'.format(n_param, self._player_cache[sig_id])))\r\n             return self._player_cache[sig_id]\r\n         except Exception as e:\r\n+            raise\r\n             self._downloader.report_warning(\r\n                 '[%s] %s (%s %s)' % (\r\n                     self.IE_NAME,\r\n```\r\n\r\nAnd run youtube-dl until it succeeds:\r\n\r\n```sh\r\nwhile ! youtube-dl -f251 --verbose https://youtu.be/nVmaUlPHEuc; do sleep 1; done\r\n```\r\n\r\nAt least this worked for me, with 50+ retries.  \ud83e\udd23\nI'm getting same error, how to fix it elegantly\nIs there a way to get a specific player in ytdl? That would probably be helpful. Also I got the  bad player RNG again...\nmaybe the trouble is related to https://techcrunch.com/2024/06/16/youtube-is-testing-another-way-to-combat-ad-blockers/\nIronically, a page that doesn't display without a pointless interstitial page.\r\n\r\nIt's not clear how what TC describes would be having this effect.\r\n\r\nThere are two issues:\r\n* failing to descramble the n-parameter, which should be fixable\r\n* getting 403 instead of throttling when the descrambling fails -- will that also happen with the descrambled URL?\r\n\r\nAt yt-dlp, people have observed getting [403 with Web formats when using browser cookies](https://github.com/yt-dlp/yt-dlp/issues/10046), but not generally without cookies, unless YT bans your data centre, VPN, or bulk downloading IP address.\r\n\r\n\nNeed me to do something? I got the Issue again, with the same Player, so I might be able to reproduce it again if need be, unless youtube randomly decides to give me a working player again.\nThe descrambling problem is a parsing error that will be patched soon.\r\n\r\nAs to \"getting 403 with Web formats when using browser cookies\", I can't see any reason why that should succeed when it fails with yt-dlp. Review the linked yt-dlp issue (and its related issues) for guidance.\r\n\r\nMaybe we'll be able pull the experimental OAuth2 implementation from https://github.com/coletdjnz/yt-dlp-youtube-oauth2.\nWait you said web formats? does the mp4 format at ID 18 count as that?\r\n\r\nAlso I did wipe the cookies File in my case, it was a File I needed a few months/years ago to get youtubedl to work at all.\r\n```\r\n==========================\r\nTESTING NORMAL YOUTUBE-DL:\r\n==========================\r\n\r\n\r\n[debug] System config: []\r\n[debug] User config: ['--no-mtime', '--match-filter', '!is_live', '--retries', 'infinite', '--fragment-retries', '2', '--skip-unavailable-fragments', '--restrict-filenames', '-i', '-o', '/home/gregorius/home/pending/videos/%(title)s___%(id)s.webm', '-f', '(bestvideo[height<=360]+worstaudio/best[height<=360])[protocol!=http_dash_segments][container!^=dash]', '--console-title', '--hls-prefer-native', '--no-cache-dir', '--http-chunk-size', '100M', '--cookies', '/home/gregorius/home/scripts/video/youtube-dl-cookies']\r\n[debug] Custom config: []\r\n[debug] Command-line args: ['https://www.youtube.com/watch?v=I0EmyNep1pE', '-vf', '18', '--no-playlist', '-o', '/home/gregorius/home/scripts/video/TEST_NORMAL_%(title)s___%(id)s.webm']\r\n[debug] Encodings: locale UTF-8, fs utf-8, out utf-8, pref UTF-8\r\n[debug] youtube-dl version 2021.12.17\r\n[debug] Single file build\r\n[debug] Python 3.10.12 (CPython x86_64 64bit) - Linux-5.15.0-112-generic-x86_64-with-glibc2.35 - OpenSSL 3.0.2 15 Mar 2022 - glibc 2.35\r\n[debug] exe versions: ffmpeg 4.4.2, ffprobe 4.4.2, rtmpdump 2.4\r\n[debug] Proxy map: {}\r\n[youtube] I0EmyNep1pE: Downloading webpage\r\n[youtube] Downloading just video I0EmyNep1pE because of --no-playlist\r\n[youtube] I0EmyNep1pE: Downloading player 590f65a6\r\nWARNING: [youtube] I0EmyNep1pE: Unable to decode n-parameter: download likely to be throttled (Signature function returned an exception; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output. Traceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1646, in _decrypt_nsig\r\n    ret = extract_nsig(jsi, func_code)(n)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1709, in extract_nsig\r\n    raise JSInterpreter.Exception('Signature function returned an exception')\r\nyoutube_dl.jsinterp.JSInterpreter.Exception: Signature function returned an exception; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n)\r\nWARNING: [youtube] I0EmyNep1pE: Unable to decode n-parameter: download likely to be throttled (Signature function returned an exception; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output. Traceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1646, in _decrypt_nsig\r\n    ret = extract_nsig(jsi, func_code)(n)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/extractor/youtube.py\", line 1709, in extract_nsig\r\n    raise JSInterpreter.Exception('Signature function returned an exception')\r\nyoutube_dl.jsinterp.JSInterpreter.Exception: Signature function returned an exception; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n)\r\n[debug] Invoking downloader on 'https://rr1---sn-4g5edns6.googlevideo.com/videoplayback?expire=1718743429&ei=JZ1xZsexE_TPi9oPhqGUmAw&ip=47.69.137.137&id=o-ALpP8_kfzlAoOw4OghmoqjXRForiH0Sq2MSj6etIKepD&itag=18&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&mh=zO&mm=31%2C26&mn=sn-4g5edns6%2Csn-f5f7lne6&ms=au%2Conr&mv=m&mvi=1&pl=19&initcwndbps=1608750&bui=AbKP-1OaqwPYcHbvgkOW_dF3v8n5-JoiKYXx2azwutwpyoRKfjnerSNcMqtNB9yDKmTsMBH5mk4fkbxe&spc=UWF9fxLno6Zu2tOYOM82-ZIS0ERcWPQYRsTA-_6zGaegxbD0BjUGxsiRsk_5&vprv=1&svpuc=1&mime=video%2Fmp4&ns=hobLoLXK120Bm1r7fCpNi6EQ&rqh=1&gir=yes&clen=4071365&ratebypass=yes&dur=191.796&lmt=1718607831904259&mt=1718721412&fvip=5&c=WEB&sefc=1&txp=6309224&n=eVGeGrQ4clzgWXjUQU&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cbui%2Cspc%2Cvprv%2Csvpuc%2Cmime%2Cns%2Crqh%2Cgir%2Cclen%2Cratebypass%2Cdur%2Clmt&sig=AJfQdSswRAIgcEPNINjUgTO2mQtU4PyTEeImVLz3CppSnmrb2rPHJR8CICMO3c6HBHuO_7vUwPZeUFia_zp4b6VbpIV9FT6sdTBc&lsparams=mh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Cinitcwndbps&lsig=AHlkHjAwRQIhAI-e1T-chmIZNaAODqhlp4ri_qBY_2wMrba0j13M1_iTAiALMdun_YI9k8ypoox8lCPWj6D4_HJiP_zmvVUmwkY3qA%3D%3D'\r\nERROR: unable to download video data: HTTP Error 403: Forbidden\r\nTraceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/YoutubeDL.py\", line 2132, in process_info\r\n    success = dl(filename, info_dict)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/YoutubeDL.py\", line 2071, in dl\r\n    return fd.download(name, new_info)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/downloader/common.py\", line 380, in download\r\n    return self.real_download(filename, info_dict)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/downloader/http.py\", line 349, in real_download\r\n    establish_connection()\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/downloader/http.py\", line 116, in establish_connection\r\n    raise err\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/downloader/http.py\", line 110, in establish_connection\r\n    ctx.data = self.ydl.urlopen(request)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-normal/youtube-dl/youtube_dl/YoutubeDL.py\", line 2470, in urlopen\r\n    return self._opener.open(req, timeout=self._socket_timeout)\r\n  File \"/usr/lib/python3.10/urllib/request.py\", line 525, in open\r\n    response = meth(req, response)\r\n  File \"/usr/lib/python3.10/urllib/request.py\", line 634, in http_response\r\n    response = self.parent.error(\r\n  File \"/usr/lib/python3.10/urllib/request.py\", line 563, in error\r\n    return self._call_chain(*args)\r\n  File \"/usr/lib/python3.10/urllib/request.py\", line 496, in _call_chain\r\n    result = func(*args)\r\n  File \"/usr/lib/python3.10/urllib/request.py\", line 643, in http_error_default\r\n    raise HTTPError(req.full_url, code, msg, hdrs, fp)\r\nurllib.error.HTTPError: HTTP Error 403: Forbidden\r\n\r\n\r\n\r\n===========================\r\nTESTING NIGHTLY YOUTUBE-DL:\r\n===========================\r\n\r\n\r\n[debug] System config: []\r\n[debug] User config: ['--no-mtime', '--match-filter', '!is_live', '--retries', 'infinite', '--fragment-retries', '2', '--skip-unavailable-fragments', '--restrict-filenames', '-i', '-o', '/home/gregorius/home/pending/videos/%(title)s___%(id)s.webm', '-f', '(bestvideo[height<=360]+worstaudio/best[height<=360])[protocol!=http_dash_segments][container!^=dash]', '--console-title', '--hls-prefer-native', '--no-cache-dir', '--http-chunk-size', '100M', '--cookies', '/home/gregorius/home/scripts/video/youtube-dl-cookies']\r\n[debug] Custom config: []\r\n[debug] Command-line args: ['https://www.youtube.com/watch?v=I0EmyNep1pE', '-vf', '18', '--no-playlist', '-o', '/home/gregorius/home/scripts/video/TEST_NIGHTLY_%(title)s___%(id)s.webm']\r\n[debug] Encodings: locale UTF-8, fs utf-8, out utf-8, pref UTF-8\r\n[debug] youtube-dl version 2024.06.12 [0153b387e] (single file build)\r\n[debug] ** This version was built from the latest master code at https://github.com/ytdl-org/youtube-dl.\r\n[debug] ** For support, visit the main site.\r\n[debug] Python 3.10.12 (CPython x86_64 64bit) - Linux-5.15.0-112-generic-x86_64-with-glibc2.35 - OpenSSL 3.0.2 15 Mar 2022 - glibc 2.35\r\n[debug] exe versions: ffmpeg 4.4.2, ffprobe 4.4.2, rtmpdump 2.4\r\n[debug] Proxy map: {}\r\n[youtube] I0EmyNep1pE: Downloading webpage\r\n[youtube] Downloading just video I0EmyNep1pE because of --no-playlist\r\n[youtube] I0EmyNep1pE: Downloading player 590f65a6\r\nWARNING: [youtube] I0EmyNep1pE: Unable to decode n-parameter: download likely to be throttled (Signature function returned an exception; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output. Traceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1646, in _decrypt_nsig\r\n    ret = extract_nsig(jsi, func_code)(n)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1709, in extract_nsig\r\n    raise JSInterpreter.Exception('Signature function returned an exception')\r\nyoutube_dl.jsinterp.JSInterpreter.Exception: Signature function returned an exception; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n)\r\nWARNING: [youtube] I0EmyNep1pE: Unable to decode n-parameter: download likely to be throttled (Signature function returned an exception; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output. Traceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1646, in _decrypt_nsig\r\n    ret = extract_nsig(jsi, func_code)(n)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/extractor/youtube.py\", line 1709, in extract_nsig\r\n    raise JSInterpreter.Exception('Signature function returned an exception')\r\nyoutube_dl.jsinterp.JSInterpreter.Exception: Signature function returned an exception; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n)\r\n[debug] Invoking downloader on 'https://rr2---sn-4g5lznes.googlevideo.com/videoplayback?expire=1718743435&ei=K51xZpCsHsWFi9oPyPGJmAU&ip=47.69.137.137&id=o-AMJwsnXuS5Sj_IxmcMfEG9jb2QL7ZZDmOcT52yff_gQy&itag=18&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&mh=zO&mm=31%2C29&mn=sn-4g5lznes%2Csn-4g5edns6&ms=au%2Crdu&mv=m&mvi=2&pl=19&initcwndbps=1551250&bui=AbKP-1Mt1GYyAFk4RQokMazLpBZmh669se4lCe2MabcVCM-adEbQLktkvmVSpBTnpEJxsMgvMcnvChih&spc=UWF9f1mpWhpaiqMPXn-i1OhoT70i4fn_FIjU9oysFRLUCeCPkll2jujR_SE3&vprv=1&svpuc=1&mime=video%2Fmp4&ns=XTh292yl-K0JCktb9QQ9hFgQ&rqh=1&gir=yes&clen=4071365&ratebypass=yes&dur=191.796&lmt=1718607831904259&mt=1718721672&fvip=1&c=WEB&sefc=1&txp=6309224&n=cUQ1Yq2Psv9Sk8RY8p&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cbui%2Cspc%2Cvprv%2Csvpuc%2Cmime%2Cns%2Crqh%2Cgir%2Cclen%2Cratebypass%2Cdur%2Clmt&sig=AJfQdSswRQIgLxo5vNb8qwAJYT_Xf1NNY6H9DRt1valskQ514uHcJH4CIQC_tvhYhEAS5YeqhXIzWWVaU1wuvZih2fH0KEutbLbPBw%3D%3D&lsparams=mh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Cinitcwndbps&lsig=AHlkHjAwRQIgJDb9SYXTro7u0vqeR9TXN2p5TRUJJohnQTqsnraBkKkCIQC1IdTnbOeA6LVUQyesjnV2TizZh_j6ecV6Djha_hfbog%3D%3D'\r\nERROR: unable to download video data: HTTP Error 403: Forbidden\r\nTraceback (most recent call last):\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/YoutubeDL.py\", line 2136, in process_info\r\n    success = dl(filename, info_dict)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/YoutubeDL.py\", line 2075, in dl\r\n    return fd.download(name, new_info)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/downloader/common.py\", line 380, in download\r\n    return self.real_download(filename, info_dict)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/downloader/http.py\", line 349, in real_download\r\n    establish_connection()\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/downloader/http.py\", line 116, in establish_connection\r\n    raise err\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/downloader/http.py\", line 110, in establish_connection\r\n    ctx.data = self.ydl.urlopen(request)\r\n  File \"/home/gregorius/home/scripts/video/youtube-dl-nightly/youtube-dl/youtube_dl/YoutubeDL.py\", line 2474, in urlopen\r\n    return self._opener.open(req, timeout=self._socket_timeout)\r\n  File \"/usr/lib/python3.10/urllib/request.py\", line 525, in open\r\n    response = meth(req, response)\r\n  File \"/usr/lib/python3.10/urllib/request.py\", line 634, in http_response\r\n    response = self.parent.error(\r\n  File \"/usr/lib/python3.10/urllib/request.py\", line 563, in error\r\n    return self._call_chain(*args)\r\n  File \"/usr/lib/python3.10/urllib/request.py\", line 496, in _call_chain\r\n    result = func(*args)\r\n  File \"/usr/lib/python3.10/urllib/request.py\", line 643, in http_error_default\r\n    raise HTTPError(req.full_url, code, msg, hdrs, fp)\r\nurllib.error.HTTPError: HTTP Error 403: Forbidden\r\n\r\n```\r\n\r\n\nyt-dl only extracts Web formats, except for age-gate bypassing.\r\n\r\nSo we can't (without changing the extractor code) use the work-around of fetching the formats as if we were an iThing that yt-dlp users have (since the yt-dlp extractor can get any or all of the various format groups that the YT webpage and APIs offer).\nI have the same problem, using new, [nightly build](https://github.com/ytdl-org/ytdl-nightly/releases) of youtube-dl 2024.06.12 (latest).\r\n\r\n```\r\n$ youtube-dl -f 242 https://www.youtube.com/watch?v=tIiG-XqHvUQ\r\n[youtube] tIiG-XqHvUQ: Downloading webpage\r\nWARNING: [youtube] tIiG-XqHvUQ: Unable to decode n-parameter: download likely to be throttled (Signature function returned an exception; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output. Traceback (most recent call last):\r\n  File \"/home/user/bin/youtube-dl/youtube_dl/extractor/youtube.py\", line 1646, in _decrypt_nsig\r\n    ret = extract_nsig(jsi, func_code)(n)\r\n  File \"/home/user/bin/youtube-dl/youtube_dl/extractor/youtube.py\", line 1709, in extract_nsig\r\n    raise JSInterpreter.Exception('Signature function returned an exception')\r\nException: Signature function returned an exception; please report this issue on https://github.com/ytdl-org/youtube-dl/issues , using the appropriate issue template. Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose option and include the complete output.\r\n)\r\nWARNING: [youtube] tIiG-XqHvUQ:\r\n\r\n[dashsegments] Total fragments: 2\r\n[download] Destination: YDB Extinction - Killing the Overkill Hype with Catastrophe _ -Cosmography101-33.3 w_Randall Carlson-tIiG-XqHvUQ.webm\r\n[download] Got server HTTP error: HTTP Error 403: Forbidden. Retrying fragment 1 (attempt 1 of 10)...\r\n...\r\n[download] Got server HTTP error: HTTP Error 403: Forbidden. Retrying fragment 1 (attempt 10 of 10)...\r\nERROR: giving up after 10 fragment retries\r\n```\r\n[Verbose log here](https://pastebin.com/g2nfTCsu).\r\n\r\nAnything can be done to help?\nhttps://github.com/ytdl-org/youtube-dl/issues/32815#issuecomment-2176253831\n> A brute-force workaround:\r\n\r\n@astralblue: Which version it patches?\r\n\r\nAlso, due to inability to figure out exactly which version it applies, I tried nightly build without the change and it succeeded after 30+ tries.\r\n\r\n\r\n\nCuriously, yt-dlp does not have this issue. I thought the jsinterp in yt-dlp and youtube-dl were pretty much on par?\r\n```\r\nyt-dlp \"https://www.youtube.com/watch?v=2yJgwwDcgV8\" -v --simulate\r\n[debug] Command-line config: ['https://www.youtube.com/watch?v=2yJgwwDcgV8', '-v', '--simulate']\r\n[debug] Encodings: locale cp1252, fs utf-8, pref cp1252, out utf-8, error utf-8, screen utf-8\r\n[debug] yt-dlp version master@2024.06.17.163732 from yt-dlp/yt-dlp-master-builds [90c3721a3] (win_exe)\r\n[debug] Python 3.8.10 (CPython AMD64 64bit) - Windows-10-10.0.22631-SP0 (OpenSSL 1.1.1k  25 Mar 2021)\r\n[debug] exe versions: ffmpeg 7.0.1-full_build-www.gyan.dev (setts), ffprobe 7.0.1-full_build-www.gyan.dev, phantomjs 2.5.0, rtmpdump 2.4\r\n[debug] Optional libraries: Cryptodome-3.20.0, brotli-1.1.0, certifi-2024.06.02, curl_cffi-0.5.10, mutagen-1.47.0, requests-2.32.3, sqlite3-3.35.5, urllib3-2.2.2, websockets-12.0\r\n[debug] Proxy map: {}\r\n[debug] Request Handlers: urllib, requests, websockets, curl_cffi\r\n[debug] Loaded 1823 extractors\r\n[youtube] Extracting URL: https://www.youtube.com/watch?v=2yJgwwDcgV8\r\n[youtube] 2yJgwwDcgV8: Downloading webpage\r\n[youtube] 2yJgwwDcgV8: Downloading ios player API JSON\r\n[debug] Loading youtube-nsig.590f65a6 from cache\r\n[debug] [youtube] Decrypted nsig PC2qd-W-Wif3efRhhd => oA-qNAnWAPC7KQ\r\n[debug] Loading youtube-nsig.590f65a6 from cache\r\n[debug] [youtube] Decrypted nsig oJ7dw8OPYRU6qUJYtI => rZ20QpzNLP-d8g\r\n[youtube] 2yJgwwDcgV8: Downloading m3u8 information\r\n[debug] Sort order given by extractor: quality, res, fps, hdr:12, source, vcodec:vp9.2, channels, acodec, lang, proto\r\n[debug] Formats sorted by: hasvid, ie_pref, quality, res, fps, hdr:12(7), source, vcodec:vp9.2(10), channels, acodec, lang, proto, size, br, asr, vext, aext, hasaud, id\r\n[debug] Default format spec: bestvideo*+bestaudio/best\r\n[info] 2yJgwwDcgV8: Downloading 1 format(s): 244+251\r\n```\nYes, but there is a bug in the evaluation of `-62-Math.pow(7, 1)*-19` here where the multiplication is not getting precedence -> 1311 vs. 71.\r\n\r\nBut more significantly, the links from the web player are getting 403 regardless of cookies, unlike in https://github.com/yt-dlp/yt-dlp/issues/10046.\nAh okay makes sense, so it seems like incorrect/missing nsig is no longer punished by throttling, but by 403.\nOr perhaps not once the descrambling is fixed:\r\n```console\r\n$ python -m youtube_dl -v -f 18 --test  \"https://www.youtube.com/watch?v=qCvqZPf3smI\"\r\n[debug] System config: [u'--prefer-ffmpeg']\r\n[debug] User config: []\r\n[debug] Custom config: []\r\n[debug] Command-line args: [u'-v', u'-f', u'18', u'--test', u'https://www.youtube.com/watch?v=qCvqZPf3smI']\r\n[debug] Encodings: locale UTF-8, fs UTF-8, out UTF-8, pref UTF-8\r\n[debug] youtube-dl version 2021.12.17\r\n[debug] Git HEAD: 0eafa09a3\r\n[debug] Python 2.7.18 (CPython i686 32bit) - Linux-4.4.0-210-generic-i686-with-Ubuntu-16.04-xenial - OpenSSL 1.1.1w  11 Sep 2023 - glibc 2.15\r\n[debug] exe versions: avconv 4.3, avprobe 4.3, ffmpeg 4.3, ffprobe 4.3\r\n[debug] Proxy map: {}\r\n[youtube] qCvqZPf3smI: Downloading webpage\r\n[youtube] qCvqZPf3smI: Downloading player 0cbdb0bb\r\n[debug] [youtube] Decrypted nsig pS7nuoyHbD53b1Ogg5 => hJuTwgtHbJ7KNw\r\n[debug] [youtube] Decrypted nsig LJanpw1rKj_ErFkGnX => AJlxnCkg4YIgVA\r\n[debug] Invoking downloader on u'https://rr5---sn-cu-aigss.googlevideo.com/videoplayback?sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cbui%2Cspc%2Cvprv%2Csvpuc%2Cmime%2Cns%2Crqh%2Cgir%2Cclen%2Cratebypass%2Cdur%2Clmt&ei=4NdxZu6pOuG6vdIP78qomAM&ip=46.208.6.91&clen=270741677&spc=UWF9fzCMgE5BLW_zLwwxJRScw7NGuV3twVCPUn3eAY514F1rrgeBL-1d4voC&id=o-AP3SKVgbJGwcSKnEmAyKjq2NMA-ojDhdElUs9OJTsuPO&txp=7209224&svpuc=1&gir=yes&xpc=EgVo2aDSNQ%3D%3D&requiressl=yes&ratebypass=yes&source=youtube&mv=m&sig=AJfQdSswRAIgAt-1nRr0gUNENkkHYv5UtUugo2XyVF31falJ5D8GK0ACICx7-MqdNYepmrySWKDwZ2ddQOVkGRiZB4PqxRyHBTxZ&dur=7915.659&ns=eogiHzTeG8t8rGibJ9Ed9BcQ&initcwndbps=1637500&vprv=1&lsig=AHlkHjAwRQIgOtzZ9Ftrvs5OyPilYNUGWeiIXsiYjsXnkb_OtUT_TV4CIQDWR0rn8VzpBnpP_Cwed2YWHzmTR-NLY4Uf9s1utibUyg%3D%3D&lsparams=mh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Cinitcwndbps&lmt=1718385543727535&c=WEB&sefc=1&bui=AbKP-1Pn_KA2jKDL1neyFLmuv91AKPbt-Pu40GLa0IoUNlGAcJpQ8drZn_2YWMVGi5vTV7Pa3sbdT6Jh&mime=video%2Fmp4&fvip=1&rqh=1&itag=18&mm=31%2C29&mn=sn-cu-aigss%2Csn-cu-c9i6&mh=94&n=hJuTwgtHbJ7KNw&mt=1718736314&expire=1718758464&pl=25&ms=au%2Crdu&mvi=5'\r\n[download] Destination: Crafting 3 Guile Scheme Programs in 2 Hours - System Crafters Live!-qCvqZPf3smI.mp4\r\n[download] 100% of 10.00KiB in 00:00\r\n$\r\n```\r\n\r\nAbove got player `0cbdb0bb` which doesn't trigger the arithmetic bug, but it was fixed anyway:\r\n```console\r\n$ python test/test_youtube_signature.py TestSignature.test_nsig_js_590f65a6\r\n.\r\n----------------------------------------------------------------------\r\nRan 1 test in 1.966s\r\n\r\nOK\r\n$ \r\n```", "created_at": "2024-06-20T03:03:11Z"}
{"repo": "ytdl-org/youtube-dl", "pull_number": 32802, "instance_id": "ytdl-org__youtube-dl-32802", "issue_numbers": ["29394"], "base_commit": "21924742f79ccbd62d16ef4120518c6a5da8614e", "patch": "diff --git a/youtube_dl/YoutubeDL.py b/youtube_dl/YoutubeDL.py\nindex 6f2aba5ac8e..dad44435f0a 100755\n--- a/youtube_dl/YoutubeDL.py\n+++ b/youtube_dl/YoutubeDL.py\n@@ -1039,8 +1039,8 @@ def process_ie_result(self, ie_result, download=True, extra_info={}):\n         elif result_type in ('playlist', 'multi_video'):\n             # Protect from infinite recursion due to recursively nested playlists\n             # (see https://github.com/ytdl-org/youtube-dl/issues/27833)\n-            webpage_url = ie_result['webpage_url']\n-            if webpage_url in self._playlist_urls:\n+            webpage_url = ie_result.get('webpage_url')  # not all pl/mv have this\n+            if webpage_url and webpage_url in self._playlist_urls:\n                 self.to_screen(\n                     '[download] Skipping already downloaded playlist: %s'\n                     % ie_result.get('title') or ie_result.get('id'))\n@@ -1048,6 +1048,10 @@ def process_ie_result(self, ie_result, download=True, extra_info={}):\n \n             self._playlist_level += 1\n             self._playlist_urls.add(webpage_url)\n+            new_result = dict((k, v) for k, v in extra_info.items() if k not in ie_result)\n+            if new_result:\n+                new_result.update(ie_result)\n+                ie_result = new_result\n             try:\n                 return self.__process_playlist(ie_result, download)\n             finally:\n@@ -1593,6 +1597,28 @@ def _calc_cookies(self, info_dict):\n         self.cookiejar.add_cookie_header(pr)\n         return pr.get_header('Cookie')\n \n+    def _fill_common_fields(self, info_dict, final=True):\n+\n+        for ts_key, date_key in (\n+                ('timestamp', 'upload_date'),\n+                ('release_timestamp', 'release_date'),\n+        ):\n+            if info_dict.get(date_key) is None and info_dict.get(ts_key) is not None:\n+                # Working around out-of-range timestamp values (e.g. negative ones on Windows,\n+                # see http://bugs.python.org/issue1646728)\n+                try:\n+                    upload_date = datetime.datetime.utcfromtimestamp(info_dict[ts_key])\n+                    info_dict[date_key] = compat_str(upload_date.strftime('%Y%m%d'))\n+                except (ValueError, OverflowError, OSError):\n+                    pass\n+\n+        # Auto generate title fields corresponding to the *_number fields when missing\n+        # in order to always have clean titles. This is very common for TV series.\n+        if final:\n+            for field in ('chapter', 'season', 'episode'):\n+                if info_dict.get('%s_number' % field) is not None and not info_dict.get(field):\n+                    info_dict[field] = '%s %d' % (field.capitalize(), info_dict['%s_number' % field])\n+\n     def process_video_result(self, info_dict, download=True):\n         assert info_dict.get('_type', 'video') == 'video'\n \n@@ -1660,24 +1686,7 @@ def sanitize_numeric_fields(info):\n         if 'display_id' not in info_dict and 'id' in info_dict:\n             info_dict['display_id'] = info_dict['id']\n \n-        for ts_key, date_key in (\n-                ('timestamp', 'upload_date'),\n-                ('release_timestamp', 'release_date'),\n-        ):\n-            if info_dict.get(date_key) is None and info_dict.get(ts_key) is not None:\n-                # Working around out-of-range timestamp values (e.g. negative ones on Windows,\n-                # see http://bugs.python.org/issue1646728)\n-                try:\n-                    upload_date = datetime.datetime.utcfromtimestamp(info_dict[ts_key])\n-                    info_dict[date_key] = compat_str(upload_date.strftime('%Y%m%d'))\n-                except (ValueError, OverflowError, OSError):\n-                    pass\n-\n-        # Auto generate title fields corresponding to the *_number fields when missing\n-        # in order to always have clean titles. This is very common for TV series.\n-        for field in ('chapter', 'season', 'episode'):\n-            if info_dict.get('%s_number' % field) is not None and not info_dict.get(field):\n-                info_dict[field] = '%s %d' % (field.capitalize(), info_dict['%s_number' % field])\n+        self._fill_common_fields(info_dict)\n \n         for cc_kind in ('subtitles', 'automatic_captions'):\n             cc = info_dict.get(cc_kind)\ndiff --git a/youtube_dl/extractor/extractors.py b/youtube_dl/extractor/extractors.py\nindex 03d035a275d..3da5f802093 100644\n--- a/youtube_dl/extractor/extractors.py\n+++ b/youtube_dl/extractor/extractors.py\n@@ -898,21 +898,13 @@\n )\n from .ora import OraTVIE\n from .orf import (\n-    ORFTVthekIE,\n-    ORFFM4IE,\n+    ORFONIE,\n+    ORFONLiveIE,\n     ORFFM4StoryIE,\n-    ORFOE1IE,\n-    ORFOE3IE,\n-    ORFNOEIE,\n-    ORFWIEIE,\n-    ORFBGLIE,\n-    ORFOOEIE,\n-    ORFSTMIE,\n-    ORFKTNIE,\n-    ORFSBGIE,\n-    ORFTIRIE,\n-    ORFVBGIE,\n     ORFIPTVIE,\n+    ORFPodcastIE,\n+    ORFRadioIE,\n+    ORFRadioCollectionIE,\n )\n from .outsidetv import OutsideTVIE\n from .packtpub import (\ndiff --git a/youtube_dl/extractor/orf.py b/youtube_dl/extractor/orf.py\nindex 8d537d7ae71..1ee78edbc17 100644\n--- a/youtube_dl/extractor/orf.py\n+++ b/youtube_dl/extractor/orf.py\n@@ -1,407 +1,394 @@\n # coding: utf-8\n from __future__ import unicode_literals\n \n+import base64\n+import functools\n import re\n \n from .common import InfoExtractor\n-from ..compat import compat_str\n+from .youtube import YoutubeIE\n from ..utils import (\n     clean_html,\n     determine_ext,\n+    ExtractorError,\n     float_or_none,\n-    HEADRequest,\n     int_or_none,\n-    orderedSet,\n-    remove_end,\n-    str_or_none,\n+    merge_dicts,\n+    mimetype2ext,\n+    parse_age_limit,\n+    parse_iso8601,\n     strip_jsonp,\n-    unescapeHTML,\n+    txt_or_none,\n     unified_strdate,\n+    update_url_query,\n     url_or_none,\n )\n-\n-\n-class ORFTVthekIE(InfoExtractor):\n-    IE_NAME = 'orf:tvthek'\n-    IE_DESC = 'ORF TVthek'\n-    _VALID_URL = r'https?://tvthek\\.orf\\.at/(?:[^/]+/)+(?P<id>\\d+)'\n+from ..traversal import T, traverse_obj\n+\n+k_float_or_none = functools.partial(float_or_none, scale=1000)\n+\n+\n+class ORFRadioBase(InfoExtractor):\n+    STATION_INFO = {\n+        'fm4': ('fm4', 'fm4', 'orffm4'),\n+        'noe': ('noe', 'oe2n', 'orfnoe'),\n+        'wien': ('wie', 'oe2w', 'orfwie'),\n+        'burgenland': ('bgl', 'oe2b', 'orfbgl'),\n+        'ooe': ('ooe', 'oe2o', 'orfooe'),\n+        'steiermark': ('stm', 'oe2st', 'orfstm'),\n+        'kaernten': ('ktn', 'oe2k', 'orfktn'),\n+        'salzburg': ('sbg', 'oe2s', 'orfsbg'),\n+        'tirol': ('tir', 'oe2t', 'orftir'),\n+        'vorarlberg': ('vbg', 'oe2v', 'orfvbg'),\n+        'oe3': ('oe3', 'oe3', 'orfoe3'),\n+        'oe1': ('oe1', 'oe1', 'orfoe1'),\n+    }\n+    _ID_NAMES = ('id', 'guid', 'program')\n+\n+    @classmethod\n+    def _get_item_id(cls, data):\n+        return traverse_obj(data, *cls._ID_NAMES, expected_type=txt_or_none)\n+\n+    @classmethod\n+    def _get_api_payload(cls, data, expected_id, in_payload=False):\n+        if expected_id not in traverse_obj(data, ('payload',)[:1 if in_payload else 0] + (cls._ID_NAMES, T(txt_or_none))):\n+            raise ExtractorError('Unexpected API data result', video_id=expected_id)\n+        return data['payload']\n+\n+    @staticmethod\n+    def _extract_podcast_upload(data):\n+        return traverse_obj(data, {\n+            'url': ('enclosures', 0, 'url'),\n+            'ext': ('enclosures', 0, 'type', T(mimetype2ext)),\n+            'filesize': ('enclosures', 0, 'length', T(int_or_none)),\n+            'title': ('title', T(txt_or_none)),\n+            'description': ('description', T(clean_html)),\n+            'timestamp': (('published', 'postDate'), T(parse_iso8601)),\n+            'duration': ('duration', T(k_float_or_none)),\n+            'series': ('podcast', 'title'),\n+            'uploader': ((('podcast', 'author'), 'station'), T(txt_or_none)),\n+            'uploader_id': ('podcast', 'channel', T(txt_or_none)),\n+        }, get_all=False)\n+\n+    @classmethod\n+    def _entries(cls, data, station, item_type=None):\n+        if item_type in ('upload', 'podcast-episode'):\n+            yield merge_dicts({\n+                'id': cls._get_item_id(data),\n+                'ext': 'mp3',\n+                'vcodec': 'none',\n+            }, cls._extract_podcast_upload(data), rev=True)\n+            return\n+\n+        loop_station = cls.STATION_INFO[station][1]\n+        for info in traverse_obj(data, ((('streams', Ellipsis), 'stream'), T(lambda v: v if v['loopStreamId'] else None))):\n+            item_id = info['loopStreamId']\n+            host = info.get('host') or 'loopstream01.apa.at'\n+            yield merge_dicts({\n+                'id': item_id.replace('.mp3', ''),\n+                'ext': 'mp3',\n+                'url': update_url_query('https://{0}/'.format(host), {\n+                    'channel': loop_station,\n+                    'id': item_id,\n+                }),\n+                'vcodec': 'none',\n+                # '_old_archive_ids': [make_archive_id(old_ie, video_id)],\n+            }, traverse_obj(data, {\n+                'title': ('title', T(txt_or_none)),\n+                'description': ('subtitle', T(clean_html)),\n+                'uploader': 'station',\n+                'series': ('programTitle', T(txt_or_none)),\n+            }), traverse_obj(info, {\n+                'duration': (('duration',\n+                              (None, T(lambda x: x['end'] - x['start']))),\n+                             T(k_float_or_none), any),\n+                'timestamp': (('start', 'startISO'), T(parse_iso8601), any),\n+            }))\n+\n+\n+class ORFRadioIE(ORFRadioBase):\n+    IE_NAME = 'orf:sound'\n+    _STATION_RE = '|'.join(map(re.escape, ORFRadioBase.STATION_INFO.keys()))\n+\n+    _VALID_URL = (\n+        r'https?://sound\\.orf\\.at/radio/(?P<station>{0})/sendung/(?P<id>\\d+)(?:/(?P<show>\\w+))?'.format(_STATION_RE),\n+        r'https?://(?P<station>{0})\\.orf\\.at/player/(?P<date>\\d{{8}})/(?P<id>\\d+)'.format(_STATION_RE),\n+    )\n \n     _TESTS = [{\n-        'url': 'http://tvthek.orf.at/program/Aufgetischt/2745173/Aufgetischt-Mit-der-Steirischen-Tafelrunde/8891389',\n+        'url': 'https://sound.orf.at/radio/ooe/sendung/37802/guten-morgen-oberoesterreich-am-feiertag',\n+        'info_dict': {\n+            'id': '37802',\n+            'title': 'Guten Morgen Ober\u00f6sterreich am Feiertag',\n+            'description': 'Ober\u00f6sterreichs meistgeh\u00f6rte regionale Fr\u00fchsendung.\\nRegionale Nachrichten zu jeder halben Stunde.\\nModeration: Wolfgang Lehner\\nNachrichten:  Stephan Schnabl',\n+        },\n         'playlist': [{\n-            'md5': '2942210346ed779588f428a92db88712',\n+            'md5': 'f9ff8517dd681b642a2c900e2c9e6085',\n             'info_dict': {\n-                'id': '8896777',\n-                'ext': 'mp4',\n-                'title': 'Aufgetischt: Mit der Steirischen Tafelrunde',\n-                'description': 'md5:c1272f0245537812d4e36419c207b67d',\n-                'duration': 2668,\n-                'upload_date': '20141208',\n-            },\n+                'id': '2024-05-30_0559_tl_66_7DaysThu1_443862',\n+                'ext': 'mp3',\n+                'title': 'Guten Morgen Ober\u00f6sterreich am Feiertag',\n+                'description': 'Ober\u00f6sterreichs meistgeh\u00f6rte regionale Fr\u00fchsendung.\\nRegionale Nachrichten zu jeder halben Stunde.\\nModeration: Wolfgang Lehner\\nNachrichten:  Stephan Schnabl',\n+                'timestamp': 1717041587,\n+                'upload_date': '20240530',\n+                'uploader': 'ooe',\n+                'duration': 14413.0,\n+            }\n         }],\n-        'skip': 'Blocked outside of Austria / Germany',\n+        'skip': 'Shows from ORF Sound are only available for 30 days.'\n     }, {\n-        'url': 'http://tvthek.orf.at/topic/Im-Wandel-der-Zeit/8002126/Best-of-Ingrid-Thurnher/7982256',\n+        'url': 'https://oe1.orf.at/player/20240531/758136',\n+        'md5': '2397717aaf3ae9c22a4f090ee3b8d374',\n         'info_dict': {\n-            'id': '7982259',\n-            'ext': 'mp4',\n-            'title': 'Best of Ingrid Thurnher',\n-            'upload_date': '20140527',\n-            'description': 'Viele Jahre war Ingrid Thurnher das \"Gesicht\" der ZIB 2. Vor ihrem Wechsel zur ZIB 2 im Jahr 1995 moderierte sie unter anderem \"Land und Leute\", \"\u00d6sterreich-Bild\" und \"Nieder\u00f6sterreich heute\".',\n-        },\n-        'params': {\n-            'skip_download': True,  # rtsp downloads\n+            'id': '2024-05-31_1905_tl_51_7DaysFri35_2413387',\n+            'ext': 'mp3',\n+            'title': '\"Who Cares?\"',\n+            'description': 'Europas gr\u00f6\u00dfte Netzkonferenz re:publica 2024',\n+            'timestamp': 1717175100,\n+            'upload_date': '20240531',\n+            'uploader': 'oe1',\n+            'duration': 1500,\n         },\n-        'skip': 'Blocked outside of Austria / Germany',\n-    }, {\n-        'url': 'http://tvthek.orf.at/topic/Fluechtlingskrise/10463081/Heimat-Fremde-Heimat/13879132/Senioren-betreuen-Migrantenkinder/13879141',\n-        'only_matching': True,\n-    }, {\n-        'url': 'http://tvthek.orf.at/profile/Universum/35429',\n-        'only_matching': True,\n+        'skip': 'Shows from ORF Sound are only available for 30 days.'\n     }]\n \n     def _real_extract(self, url):\n-        playlist_id = self._match_id(url)\n-        webpage = self._download_webpage(url, playlist_id)\n+        m = self._match_valid_url(url)\n+        station, show_id = m.group('station', 'id')\n+        api_station, _, _ = self.STATION_INFO[station]\n+        if 'date' in m.groupdict():\n+            data = self._download_json(\n+                'https://audioapi.orf.at/{0}/json/4.0/broadcast/{1}/{2}?_o={3}.orf.at'.format(\n+                    api_station, show_id, m.group('date'), station), show_id)\n+            show_id = data['id']\n+        else:\n+            data = self._download_json(\n+                'https://audioapi.orf.at/{0}/api/json/5.0/broadcast/{1}?_o=sound.orf.at'.format(\n+                    api_station, show_id), show_id)\n \n-        data_jsb = self._parse_json(\n-            self._search_regex(\n-                r'<div[^>]+class=([\"\\']).*?VideoPlaylist.*?\\1[^>]+data-jsb=([\"\\'])(?P<json>.+?)\\2',\n-                webpage, 'playlist', group='json'),\n-            playlist_id, transform_source=unescapeHTML)['playlist']['videos']\n+            data = self._get_api_payload(data, show_id, in_payload=True)\n \n-        entries = []\n-        for sd in data_jsb:\n-            video_id, title = sd.get('id'), sd.get('title')\n-            if not video_id or not title:\n-                continue\n-            video_id = compat_str(video_id)\n-            formats = []\n-            for fd in sd['sources']:\n-                src = url_or_none(fd.get('src'))\n-                if not src:\n-                    continue\n-                format_id_list = []\n-                for key in ('delivery', 'quality', 'quality_string'):\n-                    value = fd.get(key)\n-                    if value:\n-                        format_id_list.append(value)\n-                format_id = '-'.join(format_id_list)\n-                ext = determine_ext(src)\n-                if ext == 'm3u8':\n-                    m3u8_formats = self._extract_m3u8_formats(\n-                        src, video_id, 'mp4', m3u8_id=format_id, fatal=False)\n-                    if any('/geoprotection' in f['url'] for f in m3u8_formats):\n-                        self.raise_geo_restricted()\n-                    formats.extend(m3u8_formats)\n-                elif ext == 'f4m':\n-                    formats.extend(self._extract_f4m_formats(\n-                        src, video_id, f4m_id=format_id, fatal=False))\n-                elif ext == 'mpd':\n-                    formats.extend(self._extract_mpd_formats(\n-                        src, video_id, mpd_id=format_id, fatal=False))\n-                else:\n-                    formats.append({\n-                        'format_id': format_id,\n-                        'url': src,\n-                        'protocol': fd.get('protocol'),\n-                    })\n-\n-            # Check for geoblocking.\n-            # There is a property is_geoprotection, but that's always false\n-            geo_str = sd.get('geoprotection_string')\n-            if geo_str:\n-                try:\n-                    http_url = next(\n-                        f['url']\n-                        for f in formats\n-                        if re.match(r'^https?://.*\\.mp4$', f['url']))\n-                except StopIteration:\n-                    pass\n-                else:\n-                    req = HEADRequest(http_url)\n-                    self._request_webpage(\n-                        req, video_id,\n-                        note='Testing for geoblocking',\n-                        errnote=((\n-                            'This video seems to be blocked outside of %s. '\n-                            'You may want to try the streaming-* formats.')\n-                            % geo_str),\n-                        fatal=False)\n-\n-            self._check_formats(formats, video_id)\n-            self._sort_formats(formats)\n+        # site sends ISO8601 GMT date-times with separate TZ offset, ignored\n+        # TODO: should `..._date` be calculated relative to TZ?\n \n-            subtitles = {}\n-            for sub in sd.get('subtitles', []):\n-                sub_src = sub.get('src')\n-                if not sub_src:\n-                    continue\n-                subtitles.setdefault(sub.get('lang', 'de-AT'), []).append({\n-                    'url': sub_src,\n-                })\n-\n-            upload_date = unified_strdate(sd.get('created_date'))\n+        return merge_dicts(\n+            {'_type': 'multi_video'},\n+            self.playlist_result(\n+                self._entries(data, station), show_id,\n+                txt_or_none(data.get('title')),\n+                clean_html(data.get('subtitle'))))\n \n-            thumbnails = []\n-            preview = sd.get('preview_image_url')\n-            if preview:\n-                thumbnails.append({\n-                    'id': 'preview',\n-                    'url': preview,\n-                    'preference': 0,\n-                })\n-            image = sd.get('image_full_url')\n-            if not image and len(data_jsb) == 1:\n-                image = self._og_search_thumbnail(webpage)\n-            if image:\n-                thumbnails.append({\n-                    'id': 'full',\n-                    'url': image,\n-                    'preference': 1,\n-                })\n \n-            entries.append({\n-                '_type': 'video',\n-                'id': video_id,\n-                'title': title,\n-                'formats': formats,\n-                'subtitles': subtitles,\n-                'description': sd.get('description'),\n-                'duration': int_or_none(sd.get('duration_in_seconds')),\n-                'upload_date': upload_date,\n-                'thumbnails': thumbnails,\n-            })\n-\n-        return {\n-            '_type': 'playlist',\n-            'entries': entries,\n-            'id': playlist_id,\n-        }\n-\n-\n-class ORFRadioIE(InfoExtractor):\n-    def _real_extract(self, url):\n-        mobj = re.match(self._VALID_URL, url)\n-        show_date = mobj.group('date')\n-        show_id = mobj.group('show')\n+class ORFRadioCollectionIE(ORFRadioBase):\n+    IE_NAME = 'orf:collection'\n+    _VALID_URL = r'https?://sound\\.orf\\.at/collection/(?P<coll_id>\\d+)(?:/(?P<item_id>\\d+))?'\n \n-        data = self._download_json(\n-            'http://audioapi.orf.at/%s/api/json/current/broadcast/%s/%s'\n-            % (self._API_STATION, show_id, show_date), show_id)\n-\n-        entries = []\n-        for info in data['streams']:\n-            loop_stream_id = str_or_none(info.get('loopStreamId'))\n-            if not loop_stream_id:\n-                continue\n-            title = str_or_none(data.get('title'))\n-            if not title:\n-                continue\n-            start = int_or_none(info.get('start'), scale=1000)\n-            end = int_or_none(info.get('end'), scale=1000)\n-            duration = end - start if end and start else None\n-            entries.append({\n-                'id': loop_stream_id.replace('.mp3', ''),\n-                'url': 'https://loopstream01.apa.at/?channel=%s&id=%s' % (self._LOOP_STATION, loop_stream_id),\n-                'title': title,\n-                'description': clean_html(data.get('subtitle')),\n-                'duration': duration,\n-                'timestamp': start,\n+    _TESTS = [{\n+        'url': 'https://sound.orf.at/collection/4/61908/was-das-uberschreiten-des-15-limits-bedeutet',\n+        'info_dict': {\n+            'id': '2577582',\n+        },\n+        'playlist': [{\n+            'md5': '5789cec7d75575ff58d19c0428c80eb3',\n+            'info_dict': {\n+                'id': '2024-06-06_1659_tl_54_7DaysThu6_153926',\n                 'ext': 'mp3',\n-                'series': data.get('programTitle'),\n-            })\n-\n-        return {\n-            '_type': 'playlist',\n-            'id': show_id,\n-            'title': data.get('title'),\n-            'description': clean_html(data.get('subtitle')),\n-            'entries': entries,\n-        }\n-\n-\n-class ORFFM4IE(ORFRadioIE):\n-    IE_NAME = 'orf:fm4'\n-    IE_DESC = 'radio FM4'\n-    _VALID_URL = r'https?://(?P<station>fm4)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>4\\w+)'\n-    _API_STATION = 'fm4'\n-    _LOOP_STATION = 'fm4'\n-\n-    _TEST = {\n-        'url': 'http://fm4.orf.at/player/20170107/4CC',\n-        'md5': '2b0be47375432a7ef104453432a19212',\n+                'title': 'Klimakrise: Was das \u00dcberschreiten des 1,5\u00b0-Limits bedeutet',\n+                'timestamp': 1717686674,\n+                'upload_date': '20240606',\n+                'uploader': 'fm4',\n+            },\n+        }],\n+        'skip': 'Shows from ORF Sound are only available for 30 days.'\n+    }, {\n+        # persistent playlist (FM4 Highlights)\n+        'url': 'https://sound.orf.at/collection/4/',\n         'info_dict': {\n-            'id': '2017-01-07_2100_tl_54_7DaysSat18_31295',\n-            'ext': 'mp3',\n-            'title': 'Solid Steel Radioshow',\n-            'description': 'Die Mixshow von Coldcut und Ninja Tune.',\n-            'duration': 3599,\n-            'timestamp': 1483819257,\n-            'upload_date': '20170107',\n+            'id': '4',\n         },\n-        'skip': 'Shows from ORF radios are only available for 7 days.',\n-        'only_matching': True,\n-    }\n-\n-\n-class ORFNOEIE(ORFRadioIE):\n-    IE_NAME = 'orf:noe'\n-    IE_DESC = 'Radio Nieder\u00f6sterreich'\n-    _VALID_URL = r'https?://(?P<station>noe)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>\\w+)'\n-    _API_STATION = 'noe'\n-    _LOOP_STATION = 'oe2n'\n-\n-    _TEST = {\n-        'url': 'https://noe.orf.at/player/20200423/NGM',\n-        'only_matching': True,\n-    }\n-\n-\n-class ORFWIEIE(ORFRadioIE):\n-    IE_NAME = 'orf:wien'\n-    IE_DESC = 'Radio Wien'\n-    _VALID_URL = r'https?://(?P<station>wien)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>\\w+)'\n-    _API_STATION = 'wie'\n-    _LOOP_STATION = 'oe2w'\n-\n-    _TEST = {\n-        'url': 'https://wien.orf.at/player/20200423/WGUM',\n-        'only_matching': True,\n-    }\n-\n-\n-class ORFBGLIE(ORFRadioIE):\n-    IE_NAME = 'orf:burgenland'\n-    IE_DESC = 'Radio Burgenland'\n-    _VALID_URL = r'https?://(?P<station>burgenland)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>\\w+)'\n-    _API_STATION = 'bgl'\n-    _LOOP_STATION = 'oe2b'\n-\n-    _TEST = {\n-        'url': 'https://burgenland.orf.at/player/20200423/BGM',\n-        'only_matching': True,\n-    }\n-\n-\n-class ORFOOEIE(ORFRadioIE):\n-    IE_NAME = 'orf:oberoesterreich'\n-    IE_DESC = 'Radio Ober\u00f6sterreich'\n-    _VALID_URL = r'https?://(?P<station>ooe)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>\\w+)'\n-    _API_STATION = 'ooe'\n-    _LOOP_STATION = 'oe2o'\n+        'playlist_mincount': 10,\n+        'playlist_maxcount': 13,\n+    }]\n \n-    _TEST = {\n-        'url': 'https://ooe.orf.at/player/20200423/OGMO',\n-        'only_matching': True,\n-    }\n+    def _real_extract(self, url):\n+        coll_id, item_id = self._match_valid_url(url).group('coll_id', 'item_id')\n+        data = self._download_json(\n+            'https://collector.orf.at/api/frontend/collections/{0}?_o=sound.orf.at'.format(\n+                coll_id), coll_id)\n+        data = self._get_api_payload(data, coll_id, in_payload=True)\n+\n+        def yield_items():\n+            for item in traverse_obj(data, (\n+                    'content', 'items', lambda _, v: any(k in v['target']['params'] for k in self._ID_NAMES))):\n+                if item_id is None or item_id == txt_or_none(item.get('id')):\n+                    target = item['target']\n+                    typed_item_id = self._get_item_id(target['params'])\n+                    station = target['params'].get('station')\n+                    item_type = target.get('type')\n+                    if typed_item_id and (station or item_type):\n+                        yield station, typed_item_id, item_type\n+                    if item_id is not None:\n+                        break\n+            else:\n+                if item_id is not None:\n+                    raise ExtractorError('Item not found in collection',\n+                                         video_id=coll_id, expected=True)\n+\n+        def item_playlist(station, typed_item_id, item_type):\n+            if item_type == 'upload':\n+                item_data = self._download_json('https://audioapi.orf.at/radiothek/api/2.0/upload/{0}?_o=sound.orf.at'.format(\n+                    typed_item_id), typed_item_id)\n+            elif item_type == 'podcast-episode':\n+                item_data = self._download_json('https://audioapi.orf.at/radiothek/api/2.0/episode/{0}?_o=sound.orf.at'.format(\n+                    typed_item_id), typed_item_id)\n+            else:\n+                api_station, _, _ = self.STATION_INFO[station]\n+                item_data = self._download_json(\n+                    'https://audioapi.orf.at/{0}/api/json/5.0/{1}/{2}?_o=sound.orf.at'.format(\n+                        api_station, item_type or 'broadcastitem', typed_item_id), typed_item_id)\n \n+            item_data = self._get_api_payload(item_data, typed_item_id, in_payload=True)\n \n-class ORFSTMIE(ORFRadioIE):\n-    IE_NAME = 'orf:steiermark'\n-    IE_DESC = 'Radio Steiermark'\n-    _VALID_URL = r'https?://(?P<station>steiermark)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>\\w+)'\n-    _API_STATION = 'stm'\n-    _LOOP_STATION = 'oe2st'\n+            return merge_dicts(\n+                {'_type': 'multi_video'},\n+                self.playlist_result(\n+                    self._entries(item_data, station, item_type), typed_item_id,\n+                    txt_or_none(data.get('title')),\n+                    clean_html(data.get('subtitle'))))\n \n-    _TEST = {\n-        'url': 'https://steiermark.orf.at/player/20200423/STGMS',\n-        'only_matching': True,\n-    }\n+        def yield_item_entries():\n+            for station, typed_id, item_type in yield_items():\n+                yield item_playlist(station, typed_id, item_type)\n \n+        if item_id is not None:\n+            # coll_id = '/'.join((coll_id, item_id))\n+            return next(yield_item_entries())\n \n-class ORFKTNIE(ORFRadioIE):\n-    IE_NAME = 'orf:kaernten'\n-    IE_DESC = 'Radio K\u00e4rnten'\n-    _VALID_URL = r'https?://(?P<station>kaernten)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>\\w+)'\n-    _API_STATION = 'ktn'\n-    _LOOP_STATION = 'oe2k'\n+        return self.playlist_result(yield_item_entries(), coll_id, data.get('title'))\n \n-    _TEST = {\n-        'url': 'https://kaernten.orf.at/player/20200423/KGUMO',\n-        'only_matching': True,\n-    }\n \n+class ORFPodcastIE(ORFRadioBase):\n+    IE_NAME = 'orf:podcast'\n+    _STATION_RE = '|'.join(map(re.escape, (x[0] for x in ORFRadioBase.STATION_INFO.values()))) + '|tv'\n+    _VALID_URL = r'https?://sound\\.orf\\.at/podcast/(?P<station>{0})/(?P<show>[\\w-]+)/(?P<id>[\\w-]+)'.format(_STATION_RE)\n+    _TESTS = [{\n+        'url': 'https://sound.orf.at/podcast/stm/der-kraeutertipp-von-christine-lackner/rotklee',\n+        'md5': '1f2bab2ba90c2ce0c2754196ea78b35f',\n+        'info_dict': {\n+            'id': 'der-kraeutertipp-von-christine-lackner/rotklee',\n+            'ext': 'mp3',\n+            'title': 'Rotklee',\n+            'description': 'In der Natur weit verbreitet - in der Medizin l\u00e4ngst anerkennt: Rotklee. Dieser Podcast begleitet die Sendung \"Radio Steiermark am Vormittag\", Radio Steiermark, 28. Mai 2024.',\n+            'timestamp': 1716891761,\n+            'upload_date': '20240528',\n+            'uploader_id': 'stm_kraeutertipp',\n+            'uploader': 'ORF Radio Steiermark',\n+            'duration': 101,\n+            'series': 'Der Kr\u00e4utertipp von Christine Lackner',\n+        },\n+        'skip': 'ORF podcasts are only available for a limited time'\n+    }]\n \n-class ORFSBGIE(ORFRadioIE):\n-    IE_NAME = 'orf:salzburg'\n-    IE_DESC = 'Radio Salzburg'\n-    _VALID_URL = r'https?://(?P<station>salzburg)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>\\w+)'\n-    _API_STATION = 'sbg'\n-    _LOOP_STATION = 'oe2s'\n+    _ID_NAMES = ('slug', 'guid')\n \n-    _TEST = {\n-        'url': 'https://salzburg.orf.at/player/20200423/SGUM',\n-        'only_matching': True,\n-    }\n+    def _real_extract(self, url):\n+        station, show, show_id = self._match_valid_url(url).group('station', 'show', 'id')\n+        data = self._download_json(\n+            'https://audioapi.orf.at/radiothek/api/2.0/podcast/{0}/{1}/{2}'.format(\n+                station, show, show_id), show_id)\n+        data = self._get_api_payload(data, show_id, in_payload=True)\n \n+        return merge_dicts({\n+            'id': '/'.join((show, show_id)),\n+            'ext': 'mp3',\n+            'vcodec': 'none',\n+        }, self._extract_podcast_upload(data), rev=True)\n \n-class ORFTIRIE(ORFRadioIE):\n-    IE_NAME = 'orf:tirol'\n-    IE_DESC = 'Radio Tirol'\n-    _VALID_URL = r'https?://(?P<station>tirol)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>\\w+)'\n-    _API_STATION = 'tir'\n-    _LOOP_STATION = 'oe2t'\n \n-    _TEST = {\n-        'url': 'https://tirol.orf.at/player/20200423/TGUMO',\n-        'only_matching': True,\n-    }\n+class ORFIPTVBase(InfoExtractor):\n+    _TITLE_STRIP_RE = ''\n \n+    def _extract_video(self, video_id, webpage, fatal=False):\n \n-class ORFVBGIE(ORFRadioIE):\n-    IE_NAME = 'orf:vorarlberg'\n-    IE_DESC = 'Radio Vorarlberg'\n-    _VALID_URL = r'https?://(?P<station>vorarlberg)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>\\w+)'\n-    _API_STATION = 'vbg'\n-    _LOOP_STATION = 'oe2v'\n+        data = self._download_json(\n+            'http://bits.orf.at/filehandler/static-api/json/current/data.json?file=%s' % video_id,\n+            video_id)[0]\n \n-    _TEST = {\n-        'url': 'https://vorarlberg.orf.at/player/20200423/VGUM',\n-        'only_matching': True,\n-    }\n+        video = traverse_obj(data, (\n+            'sources', ('default', 'q8c'),\n+            T(lambda x: x if x['loadBalancerUrl'] else None),\n+            any))\n \n+        load_balancer_url = video['loadBalancerUrl']\n \n-class ORFOE3IE(ORFRadioIE):\n-    IE_NAME = 'orf:oe3'\n-    IE_DESC = 'Radio \u00d6sterreich 3'\n-    _VALID_URL = r'https?://(?P<station>oe3)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>\\w+)'\n-    _API_STATION = 'oe3'\n-    _LOOP_STATION = 'oe3'\n+        try:\n+            rendition = self._download_json(\n+                load_balancer_url, video_id, transform_source=strip_jsonp)\n+        except ExtractorError:\n+            rendition = None\n+\n+        if not rendition:\n+            rendition = {\n+                'redirect': {\n+                    'smil': re.sub(\n+                        r'(/)jsonp(/.+\\.)mp4$', r'\\1dash\\2smil/manifest.mpd',\n+                        load_balancer_url),\n+                },\n+            }\n \n-    _TEST = {\n-        'url': 'https://oe3.orf.at/player/20200424/3WEK',\n-        'only_matching': True,\n-    }\n+        f = traverse_obj(video, {\n+            'abr': ('audioBitrate', T(int_or_none)),\n+            'vbr': ('bitrate', T(int_or_none)),\n+            'fps': ('videoFps', T(int_or_none)),\n+            'width': ('videoWidth', T(int_or_none)),\n+            'height': ('videoHeight', T(int_or_none)),\n+        })\n \n+        formats = []\n+        for format_id, format_url in traverse_obj(rendition, (\n+                'redirect', T(dict.items), Ellipsis)):\n+            if format_id == 'rtmp':\n+                ff = f.copy()\n+                ff.update({\n+                    'url': format_url,\n+                    'format_id': format_id,\n+                })\n+                formats.append(ff)\n+            elif determine_ext(format_url) == 'f4m':\n+                formats.extend(self._extract_f4m_formats(\n+                    format_url, video_id, f4m_id=format_id))\n+            elif determine_ext(format_url) == 'm3u8':\n+                formats.extend(self._extract_m3u8_formats(\n+                    format_url, video_id, 'mp4', m3u8_id=format_id,\n+                    entry_protocol='m3u8_native'))\n+            elif determine_ext(format_url) == 'mpd':\n+                formats.extend(self._extract_mpd_formats(\n+                    format_url, video_id, mpd_id=format_id))\n \n-class ORFOE1IE(ORFRadioIE):\n-    IE_NAME = 'orf:oe1'\n-    IE_DESC = 'Radio \u00d6sterreich 1'\n-    _VALID_URL = r'https?://(?P<station>oe1)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>\\w+)'\n-    _API_STATION = 'oe1'\n-    _LOOP_STATION = 'oe1'\n+        if formats or fatal:\n+            self._sort_formats(formats)\n+        else:\n+            return\n \n-    _TEST = {\n-        'url': 'http://oe1.orf.at/player/20170108/456544',\n-        'md5': '34d8a6e67ea888293741c86a099b745b',\n-        'info_dict': {\n-            'id': '2017-01-08_0759_tl_51_7DaysSun6_256141',\n-            'ext': 'mp3',\n-            'title': 'Morgenjournal',\n-            'duration': 609,\n-            'timestamp': 1483858796,\n-            'upload_date': '20170108',\n-        },\n-        'skip': 'Shows from ORF radios are only available for 7 days.'\n-    }\n+        return merge_dicts({\n+            'id': video_id,\n+            'title': re.sub(self._TITLE_STRIP_RE, '', self._og_search_title(webpage)),\n+            'description': self._og_search_description(webpage),\n+            'upload_date': unified_strdate(self._html_search_meta(\n+                'dc.date', webpage, 'upload date', fatal=False)),\n+            'formats': formats,\n+        }, traverse_obj(data, {\n+            'duration': ('duration', T(k_float_or_none)),\n+            'thumbnail': ('sources', 'default', 'preview', T(url_or_none)),\n+        }), rev=True)\n \n \n-class ORFIPTVIE(InfoExtractor):\n+class ORFIPTVIE(ORFIPTVBase):\n     IE_NAME = 'orf:iptv'\n     IE_DESC = 'iptv.ORF.at'\n+    _WORKING = False  # URLs redirect to orf.at/\n     _VALID_URL = r'https?://iptv\\.orf\\.at/(?:#/)?stories/(?P<id>\\d+)'\n+    _TITLE_STRIP_RE = r'\\s+-\\s+iptv\\.ORF\\.at\\S*$'\n \n     _TEST = {\n         'url': 'http://iptv.orf.at/stories/2275236/',\n@@ -426,74 +413,32 @@ def _real_extract(self, url):\n         video_id = self._search_regex(\n             r'data-video(?:id)?=\"(\\d+)\"', webpage, 'video id')\n \n-        data = self._download_json(\n-            'http://bits.orf.at/filehandler/static-api/json/current/data.json?file=%s' % video_id,\n-            video_id)[0]\n-\n-        duration = float_or_none(data['duration'], 1000)\n+        return self._extract_video(video_id, webpage)\n \n-        video = data['sources']['default']\n-        load_balancer_url = video['loadBalancerUrl']\n-        abr = int_or_none(video.get('audioBitrate'))\n-        vbr = int_or_none(video.get('bitrate'))\n-        fps = int_or_none(video.get('videoFps'))\n-        width = int_or_none(video.get('videoWidth'))\n-        height = int_or_none(video.get('videoHeight'))\n-        thumbnail = video.get('preview')\n-\n-        rendition = self._download_json(\n-            load_balancer_url, video_id, transform_source=strip_jsonp)\n-\n-        f = {\n-            'abr': abr,\n-            'vbr': vbr,\n-            'fps': fps,\n-            'width': width,\n-            'height': height,\n-        }\n \n-        formats = []\n-        for format_id, format_url in rendition['redirect'].items():\n-            if format_id == 'rtmp':\n-                ff = f.copy()\n-                ff.update({\n-                    'url': format_url,\n-                    'format_id': format_id,\n-                })\n-                formats.append(ff)\n-            elif determine_ext(format_url) == 'f4m':\n-                formats.extend(self._extract_f4m_formats(\n-                    format_url, video_id, f4m_id=format_id))\n-            elif determine_ext(format_url) == 'm3u8':\n-                formats.extend(self._extract_m3u8_formats(\n-                    format_url, video_id, 'mp4', m3u8_id=format_id))\n-            else:\n-                continue\n-        self._sort_formats(formats)\n-\n-        title = remove_end(self._og_search_title(webpage), ' - iptv.ORF.at')\n-        description = self._og_search_description(webpage)\n-        upload_date = unified_strdate(self._html_search_meta(\n-            'dc.date', webpage, 'upload date'))\n-\n-        return {\n-            'id': video_id,\n-            'title': title,\n-            'description': description,\n-            'duration': duration,\n-            'thumbnail': thumbnail,\n-            'upload_date': upload_date,\n-            'formats': formats,\n-        }\n-\n-\n-class ORFFM4StoryIE(InfoExtractor):\n+class ORFFM4StoryIE(ORFIPTVBase):\n     IE_NAME = 'orf:fm4:story'\n     IE_DESC = 'fm4.orf.at stories'\n     _VALID_URL = r'https?://fm4\\.orf\\.at/stories/(?P<id>\\d+)'\n+    _TITLE_STRIP_RE = r'\\s+-\\s+fm4\\.ORF\\.at\\s*$'\n \n-    _TEST = {\n+    _TESTS = [{\n+        'url': 'https://fm4.orf.at/stories/3041554/',\n+        'add_ie': ['Youtube'],\n+        'info_dict': {\n+            'id': '3041554',\n+            'title': 'Is The EU Green Deal In Mortal Danger?',\n+        },\n+        'playlist_count': 4,\n+        'params': {\n+            'format': 'bestvideo',\n+        },\n+    }, {\n         'url': 'http://fm4.orf.at/stories/2865738/',\n+        'info_dict': {\n+            'id': '2865738',\n+            'title': 'Manu Delago und Inner Tongue live',\n+        },\n         'playlist': [{\n             'md5': 'e1c2c706c45c7b34cf478bbf409907ca',\n             'info_dict': {\n@@ -510,83 +455,311 @@ class ORFFM4StoryIE(InfoExtractor):\n             'info_dict': {\n                 'id': '547798',\n                 'ext': 'flv',\n-                'title': 'Manu Delago und Inner Tongue live (2)',\n+                'title': 'Manu Delago und Inner Tongue https://vod-ww.mdn.ors.at/cms-worldwide_episodes_nas/_definst_/nas/cms-worldwide_episodes/online/14228823_0005.smil/chunklist_b992000_vo.m3u8live (2)',\n                 'duration': 1504.08,\n                 'thumbnail': r're:^https?://.*\\.jpg$',\n                 'upload_date': '20170913',\n                 'description': 'Manu Delago und Inner Tongue haben bei der FM4 Soundpark Session live alles gegeben. Hier gibt es Fotos und die gesamte Session als Video.',\n             },\n         }],\n-    }\n+        'skip': 'Videos gone',\n+    }]\n \n     def _real_extract(self, url):\n         story_id = self._match_id(url)\n         webpage = self._download_webpage(url, story_id)\n \n         entries = []\n-        all_ids = orderedSet(re.findall(r'data-video(?:id)?=\"(\\d+)\"', webpage))\n-        for idx, video_id in enumerate(all_ids):\n-            data = self._download_json(\n-                'http://bits.orf.at/filehandler/static-api/json/current/data.json?file=%s' % video_id,\n-                video_id)[0]\n+        seen_ids = set()\n+        for idx, video_id in enumerate(re.findall(r'data-video(?:id)?=\"(\\d+)\"', webpage)):\n+            if video_id in seen_ids:\n+                continue\n+            seen_ids.add(video_id)\n+            entry = self._extract_video(video_id, webpage, fatal=False)\n+            if not entry:\n+                continue\n+\n+            if idx >= 1:\n+                # Titles are duplicates, make them unique\n+                entry['title'] = '%s (%d)' % (entry['title'], idx)\n \n-            duration = float_or_none(data['duration'], 1000)\n+            entries.append(entry)\n \n-            video = data['sources']['q8c']\n-            load_balancer_url = video['loadBalancerUrl']\n-            abr = int_or_none(video.get('audioBitrate'))\n-            vbr = int_or_none(video.get('bitrate'))\n-            fps = int_or_none(video.get('videoFps'))\n-            width = int_or_none(video.get('videoWidth'))\n-            height = int_or_none(video.get('videoHeight'))\n-            thumbnail = video.get('preview')\n+        seen_ids = set()\n+        for yt_id in re.findall(\n+                r'data-id\\s*=\\s*[\"\\']([\\w-]+)[^>]+\\bclass\\s*=\\s*[\"\\']youtube\\b',\n+                webpage):\n+            if yt_id in seen_ids:\n+                continue\n+            seen_ids.add(yt_id)\n+            if YoutubeIE.suitable(yt_id):\n+                entries.append(self.url_result(yt_id, ie='Youtube', video_id=yt_id))\n+\n+        return self.playlist_result(\n+            entries, story_id,\n+            re.sub(self._TITLE_STRIP_RE, '', self._og_search_title(webpage, default='') or None))\n+\n+\n+class ORFONBase(InfoExtractor):\n+    _ENC_PFX = '3dSlfek03nsLKdj4Jsd'\n+    _API_PATH = 'episode'\n+\n+    def _call_api(self, video_id, **kwargs):\n+        encrypted_id = base64.b64encode('{0}{1}'.format(\n+            self._ENC_PFX, video_id).encode('utf-8')).decode('ascii')\n+        return self._download_json(\n+            'https://api-tvthek.orf.at/api/v4.3/public/{0}/encrypted/{1}'.format(\n+                self._API_PATH, encrypted_id),\n+            video_id, **kwargs)\n+\n+    @classmethod\n+    def _parse_metadata(cls, api_json):\n+        return traverse_obj(api_json, {\n+            'id': ('id', T(int), T(txt_or_none)),\n+            'age_limit': ('age_classification', T(parse_age_limit)),\n+            'duration': ((('exact_duration', T(k_float_or_none)),\n+                          ('duration_second', T(float_or_none))),),\n+            'title': (('title', 'headline'), T(txt_or_none)),\n+            'description': (('description', 'teaser_text'), T(txt_or_none)),\n+            # 'media_type': ('video_type', T(txt_or_none)),\n+            'thumbnail': ('_embedded', 'image', 'public_urls', 'highlight_teaser', 'url', T(url_or_none)),\n+            'timestamp': (('date', 'episode_date'), T(parse_iso8601)),\n+            'release_timestamp': ('release_date', T(parse_iso8601)),\n+            # 'modified_timestamp': ('updated_at', T(parse_iso8601)),\n+        }, get_all=False)\n+\n+    def _extract_video(self, video_id, segment_id):\n+        # Not a segmented episode: return single video\n+        # Segmented episode without valid segment id: return entire playlist\n+        # Segmented episode with valid segment id and yes-playlist: return entire playlist\n+        # Segmented episode with valid segment id and no-playlist: return single video corresponding to segment id\n+        # If a multi_video playlist would be returned, but an unsegmented source exists, that source is chosen instead.\n+\n+        api_json = self._call_api(video_id)\n+\n+        if traverse_obj(api_json, 'is_drm_protected'):\n+            self.report_drm(video_id)\n+\n+        # updates formats, subtitles\n+        def extract_sources(src_json, video_id):\n+            for manifest_type in traverse_obj(src_json, ('sources', T(dict.keys), Ellipsis)):\n+                for manifest_url in traverse_obj(src_json, ('sources', manifest_type, Ellipsis, 'src', T(url_or_none))):\n+                    if manifest_type == 'hls':\n+                        fmts, subs = self._extract_m3u8_formats(\n+                            manifest_url, video_id, fatal=False, m3u8_id='hls',\n+                            ext='mp4', entry_protocol='m3u8_native'), {}\n+                        for f in fmts:\n+                            if '_vo.' in f['url']:\n+                                f['acodec'] = 'none'\n+                    elif manifest_type == 'dash':\n+                        fmts, subs = self._extract_mpd_formats_and_subtitles(\n+                            manifest_url, video_id, fatal=False, mpd_id='dash')\n+                    else:\n+                        continue\n+                    formats.extend(fmts)\n+                    self._merge_subtitles(subs, target=subtitles)\n+\n+        formats, subtitles = [], {}\n+        if segment_id is None:\n+            extract_sources(api_json, video_id)\n+        if not formats:\n+            segments = traverse_obj(api_json, (\n+                '_embedded', 'segments', lambda _, v: v['id']))\n+            if len(segments) > 1 and segment_id is not None:\n+                if not self._yes_playlist(video_id, segment_id, playlist_label='collection', video_label='segment'):\n+                    segments = [next(s for s in segments if txt_or_none(s['id']) == segment_id)]\n+\n+            entries = []\n+            for seg in segments:\n+                formats, subtitles = [], {}\n+                extract_sources(seg, segment_id)\n+                self._sort_formats(formats)\n+                entries.append(merge_dicts({\n+                    'formats': formats,\n+                    'subtitles': subtitles,\n+                }, self._parse_metadata(seg), rev=True))\n+            result = merge_dicts(\n+                {'_type': 'multi_video' if len(entries) > 1 else 'playlist'},\n+                self._parse_metadata(api_json),\n+                self.playlist_result(entries, video_id))\n+            # not yet processed in core for playlist/multi\n+            self._downloader._fill_common_fields(result)\n+            return result\n+        else:\n+            self._sort_formats(formats)\n \n-            rendition = self._download_json(\n-                load_balancer_url, video_id, transform_source=strip_jsonp)\n+        for sub_url in traverse_obj(api_json, (\n+                '_embedded', 'subtitle',\n+                ('xml_url', 'sami_url', 'stl_url', 'ttml_url', 'srt_url', 'vtt_url'),\n+                T(url_or_none))):\n+            self._merge_subtitles({'de': [{'url': sub_url}]}, target=subtitles)\n \n-            f = {\n-                'abr': abr,\n-                'vbr': vbr,\n-                'fps': fps,\n-                'width': width,\n-                'height': height,\n-            }\n+        return merge_dicts({\n+            'id': video_id,\n+            'formats': formats,\n+            'subtitles': subtitles,\n+            # '_old_archive_ids': [self._downloader._make_archive_id({'ie_key': 'ORFTVthek', 'id': video_id})],\n+        }, self._parse_metadata(api_json), rev=True)\n \n-            formats = []\n-            for format_id, format_url in rendition['redirect'].items():\n-                if format_id == 'rtmp':\n-                    ff = f.copy()\n-                    ff.update({\n-                        'url': format_url,\n-                        'format_id': format_id,\n-                    })\n-                    formats.append(ff)\n-                elif determine_ext(format_url) == 'f4m':\n-                    formats.extend(self._extract_f4m_formats(\n-                        format_url, video_id, f4m_id=format_id))\n-                elif determine_ext(format_url) == 'm3u8':\n-                    formats.extend(self._extract_m3u8_formats(\n-                        format_url, video_id, 'mp4', m3u8_id=format_id))\n-                else:\n-                    continue\n-            self._sort_formats(formats)\n+    def _real_extract(self, url):\n+        video_id, segment_id = self._match_valid_url(url).group('id', 'segment')\n+        webpage = self._download_webpage(url, video_id)\n \n-            title = remove_end(self._og_search_title(webpage), ' - fm4.ORF.at')\n-            if idx >= 1:\n-                # Titles are duplicates, make them unique\n-                title += ' (' + str(idx + 1) + ')'\n-            description = self._og_search_description(webpage)\n-            upload_date = unified_strdate(self._html_search_meta(\n-                'dc.date', webpage, 'upload date'))\n-\n-            entries.append({\n-                'id': video_id,\n-                'title': title,\n-                'description': description,\n-                'duration': duration,\n-                'thumbnail': thumbnail,\n-                'upload_date': upload_date,\n-                'formats': formats,\n-            })\n-\n-        return self.playlist_result(entries)\n+        # ORF doesn't like 410 or 404\n+        if self._search_regex(r'<div\\b[^>]*>\\s*(Nicht mehr verf\u00fcgbar)\\s*</div>', webpage, 'Availability', default=False):\n+            raise ExtractorError('Content is no longer available', expected=True, video_id=video_id)\n+\n+        return merge_dicts({\n+            'id': video_id,\n+            'title': self._html_search_meta(['og:title', 'twitter:title'], webpage, default=None),\n+            'description': self._html_search_meta(\n+                ['description', 'og:description', 'twitter:description'], webpage, default=None),\n+        }, self._search_json_ld(webpage, video_id, default={}),\n+            self._extract_video(video_id, segment_id),\n+            rev=True)\n+\n+\n+class ORFONIE(ORFONBase):\n+    IE_NAME = 'orf:on'\n+    _VALID_URL = r'https?://on\\.orf\\.at/video/(?P<id>\\d+)(?:/(?P<segment>\\d+))?'\n+    _TESTS = [{\n+        'url': 'https://on.orf.at/video/14210000/school-of-champions-48',\n+        'info_dict': {\n+            'id': '14210000',\n+            'ext': 'mp4',\n+            'duration': 2651.08,\n+            'thumbnail': 'https://api-tvthek.orf.at/assets/segments/0167/98/thumb_16697671_segments_highlight_teaser.jpeg',\n+            'title': 'School of Champions (4/8)',\n+            'description': r're:(?s)Luca hat sein ganzes Leben in den Bergen S\u00fcdtirols verbracht und ist bei seiner Mutter aufgewachsen, .{1029} Leo$',\n+            # 'media_type': 'episode',\n+            'timestamp': 1706558922,\n+            'upload_date': '20240129',\n+            'release_timestamp': 1706472362,\n+            'release_date': '20240128',\n+            # 'modified_timestamp': 1712756663,\n+            # 'modified_date': '20240410',\n+            # '_old_archive_ids': ['orftvthek 14210000'],\n+        },\n+        'params': {\n+            'format': 'bestvideo',\n+        },\n+        'skip': 'Available until 2024-08-12',\n+    }, {\n+        'url': 'https://on.orf.at/video/3220355',\n+        'md5': '925a93b2b9a37da5c9b979d7cf71aa2e',\n+        'info_dict': {\n+            'id': '3220355',\n+            'ext': 'mp4',\n+            'duration': 445.04,\n+            'thumbnail': 'https://api-tvthek.orf.at/assets/segments/0002/60/thumb_159573_segments_highlight_teaser.png',\n+            'title': '50 Jahre Burgenland: Der Festumzug',\n+            'description': r're:(?s)Aus allen Landesteilen zogen festlich geschm\u00fcckte Wagen und Musikkapellen .{270} Jenakowitsch$',\n+            # 'media_type': 'episode',\n+            'timestamp': 52916400,\n+            'upload_date': '19710905',\n+            'release_timestamp': 52916400,\n+            'release_date': '19710905',\n+            # 'modified_timestamp': 1498536049,\n+            # 'modified_date': '20170627',\n+            # '_old_archive_ids': ['orftvthek 3220355'],\n+        },\n+    }, {\n+        # Video with multiple segments selecting the second segment\n+        'url': 'https://on.orf.at/video/14226549/15639808/jugendbande-einbrueche-aus-langeweile',\n+        'md5': 'fc151bba8c05ea77ab5693617e4a33d3',\n+        'info_dict': {\n+            'id': '15639808',\n+            'ext': 'mp4',\n+            'duration': 97.707,\n+            'thumbnail': 'https://api-tvthek.orf.at/assets/segments/0175/43/thumb_17442704_segments_highlight_teaser.jpg',\n+            'title': 'Jugendbande: Einbr\u00fcche aus Langeweile',\n+            'description': r're:Jugendbande: Einbr\u00fcche aus Langeweile \\| Neuer Kinder- und .{259} Wanda$',\n+            # 'media_type': 'segment',\n+            'timestamp': 1715792400,\n+            'upload_date': '20240515',\n+            # 'modified_timestamp': 1715794394,\n+            # 'modified_date': '20240515',\n+            # '_old_archive_ids': ['orftvthek 15639808'],\n+        },\n+        'params': {\n+            'noplaylist': True,\n+            'format': 'bestvideo',\n+        },\n+        'skip': 'Available until 2024-06-14',\n+    }, {\n+        # Video with multiple segments and no combined version\n+        'url': 'https://on.orf.at/video/14227864/formel-1-grosser-preis-von-monaco-2024',\n+        'info_dict': {\n+            '_type': 'multi_video',\n+            'id': '14227864',\n+            'duration': 18410.52,\n+            'thumbnail': 'https://api-tvthek.orf.at/assets/segments/0176/04/thumb_17503881_segments_highlight_teaser.jpg',\n+            'title': 'Formel 1: Gro\u00dfer Preis von Monaco 2024',\n+            'description': 'md5:aeeb010710ccf70ce28ccb4482243d4f',\n+            # 'media_type': 'episode',\n+            'timestamp': 1716721200,\n+            'upload_date': '20240526',\n+            'release_timestamp': 1716721802,\n+            'release_date': '20240526',\n+            # 'modified_timestamp': 1716884702,\n+            # 'modified_date': '20240528',\n+        },\n+        'playlist_count': 42,\n+        'skip': 'Gone: Nicht mehr verf\u00fcgbar',\n+    }, {\n+        # Video with multiple segments, but with combined version\n+        'url': 'https://on.orf.at/video/14228172',\n+        'info_dict': {\n+            'id': '14228172',\n+            'ext': 'mp4',\n+            'duration': 3294.878,\n+            'thumbnail': 'https://api-tvthek.orf.at/assets/segments/0176/29/thumb_17528242_segments_highlight_teaser.jpg',\n+            'title': 'Willkommen \u00d6sterreich mit Stermann & Grissemann',\n+            'description': r're:Zum Saisonfinale freuen sich die urlaubsreifen Gastgeber Stermann und .{1863} Geschichten\\.$',\n+            # 'media_type': 'episode',\n+            'timestamp': 1716926584,\n+            'upload_date': '20240528',\n+            'release_timestamp': 1716919202,\n+            'release_date': '20240528',\n+            # 'modified_timestamp': 1716968045,\n+            # 'modified_date': '20240529',\n+            # '_old_archive_ids': ['orftvthek 14228172'],\n+        },\n+        'params': {\n+            'format': 'bestvideo',\n+        },\n+        'skip': 'Gone: Nicht mehr verf\u00fcgbar',\n+    }]\n+\n+\n+class ORFONLiveIE(ORFONBase):\n+    _ENC_PFX = '8876324jshjd7293ktd'\n+    _API_PATH = 'livestream'\n+    _VALID_URL = r'https?://on\\.orf\\.at/livestream/(?P<id>\\d+)(?:/(?P<segment>\\d+))?'\n+    _TESTS = [{\n+        'url': 'https://on.orf.at/livestream/14320204/pressekonferenz-neos-zu-aktuellen-entwicklungen',\n+        'info_dict': {\n+            'id': '14320204',\n+            'ext': 'mp4',\n+            'title': 'Pressekonferenz: Neos zu aktuellen Entwicklungen',\n+            'description': r're:(?s)Neos-Chefin Beate Meinl-Reisinger informi.{598}ng\\.\"',\n+            'timestamp': 1716886335,\n+            'upload_date': '20240528',\n+            # 'modified_timestamp': 1712756663,\n+            # 'modified_date': '20240410',\n+            # '_old_archive_ids': ['orftvthek 14210000'],\n+        },\n+        'params': {\n+            'format': 'bestvideo',\n+        },\n+    }]\n+\n+    @classmethod\n+    def _parse_metadata(cls, api_json):\n+        return merge_dicts(\n+            super(ORFONLiveIE, cls)._parse_metadata(api_json),\n+            traverse_obj(api_json, {\n+                'timestamp': ('updated_at', T(parse_iso8601)),\n+                'release_timestamp': ('start', T(parse_iso8601)),\n+                'is_live': True,\n+            }))\n", "test_patch": "diff --git a/test/helper.py b/test/helper.py\nindex 5b7e3dfe20b..6f2129efff3 100644\n--- a/test/helper.py\n+++ b/test/helper.py\n@@ -5,9 +5,9 @@\n import json\n import os.path\n import re\n-import types\n import ssl\n import sys\n+import types\n import unittest\n \n import youtube_dl.extractor\n@@ -181,18 +181,18 @@ def expect_value(self, got, expected, field):\n             op, _, expected_num = expected.partition(':')\n             expected_num = int(expected_num)\n             if op == 'mincount':\n-                assert_func = assertGreaterEqual\n+                assert_func = self.assertGreaterEqual\n                 msg_tmpl = 'Expected %d items in field %s, but only got %d'\n             elif op == 'maxcount':\n-                assert_func = assertLessEqual\n+                assert_func = self.assertLessEqual\n                 msg_tmpl = 'Expected maximum %d items in field %s, but got %d'\n             elif op == 'count':\n-                assert_func = assertEqual\n+                assert_func = self.assertEqual\n                 msg_tmpl = 'Expected exactly %d items in field %s, but got %d'\n             else:\n                 assert False\n             assert_func(\n-                self, len(got), expected_num,\n+                len(got), expected_num,\n                 msg_tmpl % (expected_num, field, len(got)))\n             return\n         self.assertEqual(\n@@ -262,27 +262,6 @@ def assertRegexpMatches(self, text, regexp, msg=None):\n             self.assertTrue(m, msg)\n \n \n-def assertGreaterEqual(self, got, expected, msg=None):\n-    if not (got >= expected):\n-        if msg is None:\n-            msg = '%r not greater than or equal to %r' % (got, expected)\n-        self.assertTrue(got >= expected, msg)\n-\n-\n-def assertLessEqual(self, got, expected, msg=None):\n-    if not (got <= expected):\n-        if msg is None:\n-            msg = '%r not less than or equal to %r' % (got, expected)\n-        self.assertTrue(got <= expected, msg)\n-\n-\n-def assertEqual(self, got, expected, msg=None):\n-    if not (got == expected):\n-        if msg is None:\n-            msg = '%r not equal to %r' % (got, expected)\n-        self.assertTrue(got == expected, msg)\n-\n-\n def expect_warnings(ydl, warnings_re):\n     real_warning = ydl.report_warning\n \ndiff --git a/test/test_download.py b/test/test_download.py\nindex df8b370cfdc..f7d6a23bc95 100644\n--- a/test/test_download.py\n+++ b/test/test_download.py\n@@ -9,8 +9,6 @@\n sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n \n from test.helper import (\n-    assertGreaterEqual,\n-    assertLessEqual,\n     expect_warnings,\n     get_params,\n     gettestcases,\n@@ -36,12 +34,20 @@\n     ExtractorError,\n     error_to_compat_str,\n     format_bytes,\n+    IDENTITY,\n+    preferredencoding,\n     UnavailableVideoError,\n )\n from youtube_dl.extractor import get_info_extractor\n \n RETRIES = 3\n \n+# Some unittest APIs require actual str\n+if not isinstance('TEST', str):\n+    _encode_str = lambda s: s.encode(preferredencoding())\n+else:\n+    _encode_str = IDENTITY\n+\n \n class YoutubeDL(youtube_dl.YoutubeDL):\n     def __init__(self, *args, **kwargs):\n@@ -102,7 +108,7 @@ def test_template(self):\n \n         def print_skipping(reason):\n             print('Skipping %s: %s' % (test_case['name'], reason))\n-            self.skipTest(reason)\n+            self.skipTest(_encode_str(reason))\n \n         if not ie.working():\n             print_skipping('IE marked as not _WORKING')\n@@ -187,16 +193,14 @@ def try_rm_tcs_files(tcs=None):\n                 expect_info_dict(self, res_dict, test_case.get('info_dict', {}))\n \n             if 'playlist_mincount' in test_case:\n-                assertGreaterEqual(\n-                    self,\n+                self.assertGreaterEqual(\n                     len(res_dict['entries']),\n                     test_case['playlist_mincount'],\n                     'Expected at least %d in playlist %s, but got only %d' % (\n                         test_case['playlist_mincount'], test_case['url'],\n                         len(res_dict['entries'])))\n             if 'playlist_maxcount' in test_case:\n-                assertLessEqual(\n-                    self,\n+                self.assertLessEqual(\n                     len(res_dict['entries']),\n                     test_case['playlist_maxcount'],\n                     'Expected at most %d in playlist %s, but got %d' % (\n@@ -243,8 +247,8 @@ def try_rm_tcs_files(tcs=None):\n                         if params.get('test'):\n                             expected_minsize = max(expected_minsize, 10000)\n                         got_fsize = os.path.getsize(tc_filename)\n-                        assertGreaterEqual(\n-                            self, got_fsize, expected_minsize,\n+                        self.assertGreaterEqual(\n+                            got_fsize, expected_minsize,\n                             'Expected %s to be at least %s, but it\\'s only %s ' %\n                             (tc_filename, format_bytes(expected_minsize),\n                                 format_bytes(got_fsize)))\n", "problem_statement": "ORF Radiothek has changed the URLs\nyoutube-dl version 2021.06.06\r\n\r\nORF has changed the URLs for Radiothek.\r\nI have tested the URLs in the file \"orf.py.new\" and the downloads are working now without problems.\r\nBut how these URLs can be changed in the official youtube-dl version?\r\n\r\n**diff orf.py orf.py.new**\r\n**191c191**\r\n**<             'http://audioapi.orf.at/%s/api/json/current/broadcast/%s/%s'**\r\n**---**\r\n**>             'https://audioapi.orf.at/%s/api/json/current/broadcast/%s/%s'**\r\n**228c228**\r\n**<     _VALID_URL = r'https?://(?P<station>fm4)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>4\\w+)'**\r\n**---**\r\n**>     _VALID_URL = r'https?://radiothek\\.orf\\.at/(?P<station>fm4)/(?P<date>[0-9]+)/(?P<show>4\\w+)'**\r\n**252c252**\r\n**<     _VALID_URL = r'https?://(?P<station>noe)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>\\w+)'**\r\n**---**\r\n**>     _VALID_URL = r'https?://radiothek\\.orf\\.at/(?P<station>noe)/(?P<date>[0-9]+)/(?P<show>\\w+)'**\r\n**265c265**\r\n**<     _VALID_URL = r'https?://(?P<station>wien)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>\\w+)'**\r\n**---**\r\n**>     _VALID_URL = r'https?://radiothek\\.orf\\.at/(?P<station>wie)/(?P<date>[0-9]+)/(?P<show>\\w+)'**\r\n**278c278**\r\n**<     _VALID_URL = r'https?://(?P<station>burgenland)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>\\w+)'**\r\n**---**\r\n**>     _VALID_URL = r'https?://radiothek\\.orf\\.at/(?P<station>bgl)/(?P<date>[0-9]+)/(?P<show>\\w+)'**\r\n**291c291**\r\n**<     _VALID_URL = r'https?://(?P<station>ooe)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>\\w+)'**\r\n**---**\r\n**>     _VALID_URL = r'https?://radiothek\\.orf\\.at/(?P<station>ooe)/(?P<date>[0-9]+)/(?P<show>\\w+)'**\r\n**304c304**\r\n**<     _VALID_URL = r'https?://(?P<station>steiermark)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>\\w+)'**\r\n**---**\r\n**>     _VALID_URL = r'https?://radiothek\\.orf\\.at/(?P<station>stm)/(?P<date>[0-9]+)/(?P<show>\\w+)'**\r\n**317c317**\r\n**<     _VALID_URL = r'https?://(?P<station>kaernten)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>\\w+)'**\r\n**---**\r\n**>     _VALID_URL = r'https?://radiothek\\.orf\\.at/(?P<station>ktn)/(?P<date>[0-9]+)/(?P<show>\\w+)'**\r\n**330c330**\r\n**<     _VALID_URL = r'https?://(?P<station>salzburg)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>\\w+)'**\r\n**---**\r\n**>     _VALID_URL = r'https?://radiothek\\.orf\\.at/(?P<station>sbg)/(?P<date>[0-9]+)/(?P<show>\\w+)'**\r\n**343c343**\r\n**<     _VALID_URL = r'https?://(?P<station>tirol)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>\\w+)'**\r\n**---**\r\n**>     _VALID_URL = r'https?://radiothek\\.orf\\.at/(?P<station>tir)/(?P<date>[0-9]+)/(?P<show>\\w+)'**\r\n**356c356**\r\n**<     _VALID_URL = r'https?://(?P<station>vorarlberg)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>\\w+)'**\r\n**---**\r\n**>     _VALID_URL = r'https?://radiothek\\.orf\\.at/(?P<station>vbg)/(?P<date>[0-9]+)/(?P<show>\\w+)'**\r\n**369c369**\r\n**<     _VALID_URL = r'https?://(?P<station>oe3)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>\\w+)'**\r\n**---**\r\n**>     _VALID_URL = r'https?://radiothek\\.orf\\.at/(?P<station>oe3)/(?P<date>[0-9]+)/(?P<show>\\w+)'**\r\n**382c382**\r\n**<     _VALID_URL = r'https?://(?P<station>oe1)\\.orf\\.at/player/(?P<date>[0-9]+)/(?P<show>\\w+)'**\r\n**---**\r\n**>     _VALID_URL = r'https?://radiothek\\.orf\\.at/(?P<station>oe1)/(?P<date>[0-9]+)/(?P<show>\\w+)'**\n", "hints_text": "Actually the _station_ URLs still work fine, but you are right: the _radiothek_ URLs are not supported (I am working on it).\r\n\r\nFor example, this _station_ URL (currently) works: https://oe1.orf.at/player/20210723/645522 this _radiothek_ URL does not: https://radiothek.orf.at/oe1/20210723/645522 (both URLs provide the same mp3 file).\r\n\r\nSo your patch would break the ability to download the _station_ URLs (although I was not able to apply your patch, because of missing white spaces. You should use [fenced code blocks](https://docs.github.com/en/github/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) to paste your code).\r\n\r\nBut you inspired me to look at an older issue (#26043) where we already talked about the _radiothek_ URLs and now try to implement the extractor.\r\n\r\nYou also asked:\r\n> But how these URLs can be changed in the official youtube-dl version?\r\n\r\nYou might want to read the [GitHub Docs](https://docs.github.com/en) first, but basically you [fork and clone](https://docs.github.com/en/get-started/quickstart/fork-a-repo) the project, make changes and [submit a pull request](https://docs.github.com/en/github/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request).\r\n\nThanks @dirkf for the clean up!\r\n\r\nThis triggered me to take another look into this issue.\r\nI've tested an ORF OE1 (station) URL with `youtube-dl \"https://oe1.orf.at/player/20210723/645522\"` (with youtube-dl 2021.12.17, I haven't tested the latest version) . This still works.\r\n\r\nORF Radiothek does not exist anymore. It seems it has been superseded by ORF Sound: https://sound.orf.at/.\r\n\r\nI think this issue can be closed; I will close my PR: https://github.com/ytdl-org/youtube-dl/pull/29679.\r\n\nIs https://oe1.orf.at/player/20210723/645522 an actual URL that can be found from sound.orf.at or any other orf.at site?\r\n\r\nCurrently I'm just finding `sound.orf.at/radio/{station}/sendung/{id}` which may also have a final `/{slug}`. However I see that the player URL is still valid (with no redirect) and playable, so I'll have to make sure it still gets extracted.\r\n\r\nThere are also `/collection/...` URLs where `/collection/{id}` brings up a playlist page and `/collection/{collection_id}/{item_id}` is one item in the playlist. Such an item may be an `upload` or `podcast-episode`, or it may be a `broadcast`, `broadcastitem` or `program` associated with a `station`, each with its own API.\r\n\r\nI'll close the issue when an new ORF extractor module gets merged.", "created_at": "2024-06-01T23:55:36Z"}
{"repo": "ytdl-org/youtube-dl", "pull_number": 32741, "instance_id": "ytdl-org__youtube-dl-32741", "issue_numbers": ["32735"], "base_commit": "820fae3b3a8587a6f57afbe803b4f91de7d4e086", "patch": "diff --git a/youtube_dl/compat.py b/youtube_dl/compat.py\nindex 818ccebd0a6..53ff2a892af 100644\n--- a/youtube_dl/compat.py\n+++ b/youtube_dl/compat.py\n@@ -2421,29 +2421,26 @@ def load(self, rawdata):\n compat_urllib_request_urlretrieve = compat_urlretrieve\n \n try:\n+    from HTMLParser import (\n+        HTMLParser as compat_HTMLParser,\n+        HTMLParseError as compat_HTMLParseError)\n+except ImportError:  # Python 3\n     from html.parser import HTMLParser as compat_HTMLParser\n-except ImportError:  # Python 2\n-    from HTMLParser import HTMLParser as compat_HTMLParser\n-compat_html_parser_HTMLParser = compat_HTMLParser\n-\n-try:  # Python 2\n-    from HTMLParser import HTMLParseError as compat_HTMLParseError\n-except ImportError:  # Python <3.4\n     try:\n         from html.parser import HTMLParseError as compat_HTMLParseError\n     except ImportError:  # Python >3.4\n-\n-        # HTMLParseError has been deprecated in Python 3.3 and removed in\n+        # HTMLParseError was deprecated in Python 3.3 and removed in\n         # Python 3.5. Introducing dummy exception for Python >3.5 for compatible\n         # and uniform cross-version exception handling\n         class compat_HTMLParseError(Exception):\n             pass\n+compat_html_parser_HTMLParser = compat_HTMLParser\n compat_html_parser_HTMLParseError = compat_HTMLParseError\n \n try:\n-    from subprocess import DEVNULL\n-    compat_subprocess_get_DEVNULL = lambda: DEVNULL\n-except ImportError:\n+    _DEVNULL = subprocess.DEVNULL\n+    compat_subprocess_get_DEVNULL = lambda: _DEVNULL\n+except AttributeError:\n     compat_subprocess_get_DEVNULL = lambda: open(os.path.devnull, 'w')\n \n try:\n@@ -2943,6 +2940,51 @@ def compat_socket_create_connection(address, timeout, source_address=None):\n     compat_socket_create_connection = socket.create_connection\n \n \n+try:\n+    from contextlib import suppress as compat_contextlib_suppress\n+except ImportError:\n+    class compat_contextlib_suppress(object):\n+        _exceptions = None\n+\n+        def __init__(self, *exceptions):\n+            super(compat_contextlib_suppress, self).__init__()\n+            # TODO: [Base]ExceptionGroup (3.12+)\n+            self._exceptions = exceptions\n+\n+        def __enter__(self):\n+            return self\n+\n+        def __exit__(self, exc_type, exc_val, exc_tb):\n+            return exc_val is not None and isinstance(exc_val, self._exceptions or tuple())\n+\n+\n+# subprocess.Popen context manager\n+# avoids leaking handles if .communicate() is not called\n+try:\n+    _Popen = subprocess.Popen\n+    # check for required context manager attributes\n+    _Popen.__enter__ and _Popen.__exit__\n+    compat_subprocess_Popen = _Popen\n+except AttributeError:\n+    # not a context manager - make one\n+    from contextlib import contextmanager\n+\n+    @contextmanager\n+    def compat_subprocess_Popen(*args, **kwargs):\n+        popen = None\n+        try:\n+            popen = _Popen(*args, **kwargs)\n+            yield popen\n+        finally:\n+            if popen:\n+                for f in (popen.stdin, popen.stdout, popen.stderr):\n+                    if f:\n+                        # repeated .close() is OK, but just in case\n+                        with compat_contextlib_suppress(EnvironmentError):\n+                            f.close()\n+                popen.wait()\n+\n+\n # Fix https://github.com/ytdl-org/youtube-dl/issues/4223\n # See http://bugs.python.org/issue9161 for what is broken\n def workaround_optparse_bug9161():\n@@ -3263,6 +3305,7 @@ def compat_datetime_timedelta_total_seconds(td):\n     'compat_http_cookiejar_Cookie',\n     'compat_http_cookies',\n     'compat_http_cookies_SimpleCookie',\n+    'compat_contextlib_suppress',\n     'compat_ctypes_WINFUNCTYPE',\n     'compat_etree_fromstring',\n     'compat_filter',\n@@ -3298,6 +3341,7 @@ def compat_datetime_timedelta_total_seconds(td):\n     'compat_struct_pack',\n     'compat_struct_unpack',\n     'compat_subprocess_get_DEVNULL',\n+    'compat_subprocess_Popen',\n     'compat_tokenize_tokenize',\n     'compat_urllib_error',\n     'compat_urllib_parse',\ndiff --git a/youtube_dl/downloader/external.py b/youtube_dl/downloader/external.py\nindex bc228960efe..4fbc0f520e0 100644\n--- a/youtube_dl/downloader/external.py\n+++ b/youtube_dl/downloader/external.py\n@@ -11,8 +11,14 @@\n from ..compat import (\n     compat_setenv,\n     compat_str,\n+    compat_subprocess_Popen,\n )\n-from ..postprocessor.ffmpeg import FFmpegPostProcessor, EXT_TO_OUT_FORMATS\n+\n+try:\n+    from ..postprocessor.ffmpeg import FFmpegPostProcessor, EXT_TO_OUT_FORMATS\n+except ImportError:\n+    FFmpegPostProcessor = None\n+\n from ..utils import (\n     cli_option,\n     cli_valueless_option,\n@@ -361,13 +367,14 @@ def supports(cls, info_dict):\n \n     @classmethod\n     def available(cls):\n-        return FFmpegPostProcessor().available\n+        # actual availability can only be confirmed for an instance\n+        return bool(FFmpegPostProcessor)\n \n     def _call_downloader(self, tmpfilename, info_dict):\n-        url = info_dict['url']\n-        ffpp = FFmpegPostProcessor(downloader=self)\n+        # `downloader` means the parent `YoutubeDL`\n+        ffpp = FFmpegPostProcessor(downloader=self.ydl)\n         if not ffpp.available:\n-            self.report_error('m3u8 download detected but ffmpeg or avconv could not be found. Please install one.')\n+            self.report_error('ffmpeg required for download but no ffmpeg (nor avconv) executable could be found. Please install one.')\n             return False\n         ffpp.check_version()\n \n@@ -396,6 +403,7 @@ def _call_downloader(self, tmpfilename, info_dict):\n         # if end_time:\n         #     args += ['-t', compat_str(end_time - start_time)]\n \n+        url = info_dict['url']\n         cookies = self.ydl.cookiejar.get_cookies_for_url(url)\n         if cookies:\n             args.extend(['-cookies', ''.join(\n@@ -483,21 +491,25 @@ def _call_downloader(self, tmpfilename, info_dict):\n \n         self._debug_cmd(args)\n \n-        proc = subprocess.Popen(args, stdin=subprocess.PIPE, env=env)\n-        try:\n-            retval = proc.wait()\n-        except BaseException as e:\n-            # subprocess.run would send the SIGKILL signal to ffmpeg and the\n-            # mp4 file couldn't be played, but if we ask ffmpeg to quit it\n-            # produces a file that is playable (this is mostly useful for live\n-            # streams). Note that Windows is not affected and produces playable\n-            # files (see https://github.com/ytdl-org/youtube-dl/issues/8300).\n-            if isinstance(e, KeyboardInterrupt) and sys.platform != 'win32':\n-                process_communicate_or_kill(proc, b'q')\n-            else:\n-                proc.kill()\n-                proc.wait()\n-            raise\n+        # From [1], a PIPE opened in Popen() should be closed, unless\n+        # .communicate() is called. Avoid leaking any PIPEs by using Popen\n+        # as a context manager (newer Python 3.x and compat)\n+        # Fixes \"Resource Warning\" in test/test_downloader_external.py\n+        # [1] https://devpress.csdn.net/python/62fde12d7e66823466192e48.html\n+        with compat_subprocess_Popen(args, stdin=subprocess.PIPE, env=env) as proc:\n+            try:\n+                retval = proc.wait()\n+            except BaseException as e:\n+                # subprocess.run would send the SIGKILL signal to ffmpeg and the\n+                # mp4 file couldn't be played, but if we ask ffmpeg to quit it\n+                # produces a file that is playable (this is mostly useful for live\n+                # streams). Note that Windows is not affected and produces playable\n+                # files (see https://github.com/ytdl-org/youtube-dl/issues/8300).\n+                if isinstance(e, KeyboardInterrupt) and sys.platform != 'win32':\n+                    process_communicate_or_kill(proc, b'q')\n+                else:\n+                    proc.kill()\n+                raise\n         return retval\n \n \ndiff --git a/youtube_dl/postprocessor/ffmpeg.py b/youtube_dl/postprocessor/ffmpeg.py\nindex 801160e6c84..e5ffdf37882 100644\n--- a/youtube_dl/postprocessor/ffmpeg.py\n+++ b/youtube_dl/postprocessor/ffmpeg.py\n@@ -96,6 +96,7 @@ def get_ffmpeg_version(path):\n \n         self._paths = None\n         self._versions = None\n+        location = None\n         if self._downloader:\n             prefer_ffmpeg = self._downloader.params.get('prefer_ffmpeg', True)\n             location = self._downloader.params.get('ffmpeg_location')\n@@ -118,32 +119,17 @@ def get_ffmpeg_version(path):\n                     location = os.path.dirname(os.path.abspath(location))\n                     if basename in ('ffmpeg', 'ffprobe'):\n                         prefer_ffmpeg = True\n-\n-                self._paths = dict(\n-                    (p, os.path.join(location, p)) for p in programs)\n-                self._versions = dict(\n-                    (p, get_ffmpeg_version(self._paths[p])) for p in programs)\n-        if self._versions is None:\n-            self._versions = dict(\n-                (p, get_ffmpeg_version(p)) for p in programs)\n-            self._paths = dict((p, p) for p in programs)\n-\n-        if prefer_ffmpeg is False:\n-            prefs = ('avconv', 'ffmpeg')\n-        else:\n-            prefs = ('ffmpeg', 'avconv')\n-        for p in prefs:\n-            if self._versions[p]:\n-                self.basename = p\n-                break\n-\n-        if prefer_ffmpeg is False:\n-            prefs = ('avprobe', 'ffprobe')\n-        else:\n-            prefs = ('ffprobe', 'avprobe')\n-        for p in prefs:\n-            if self._versions[p]:\n-                self.probe_basename = p\n+        self._paths = dict(\n+            (p, p if location is None else os.path.join(location, p))\n+            for p in programs)\n+        self._versions = dict(\n+            x for x in (\n+                (p, get_ffmpeg_version(self._paths[p])) for p in programs)\n+            if x[1] is not None)\n+\n+        for p in ('ffmpeg', 'avconv')[::-1 if prefer_ffmpeg is False else 1]:\n+            if self._versions.get(p):\n+                self.basename = self.probe_basename = p\n                 break\n \n     @property\ndiff --git a/youtube_dl/utils.py b/youtube_dl/utils.py\nindex 03c73dff39d..083446342b0 100644\n--- a/youtube_dl/utils.py\n+++ b/youtube_dl/utils.py\n@@ -45,6 +45,7 @@\n     compat_casefold,\n     compat_chr,\n     compat_collections_abc,\n+    compat_contextlib_suppress,\n     compat_cookiejar,\n     compat_ctypes_WINFUNCTYPE,\n     compat_datetime_timedelta_total_seconds,\n@@ -1855,25 +1856,18 @@ def write_json_file(obj, fn):\n     try:\n         with tf:\n             json.dump(obj, tf)\n-        if sys.platform == 'win32':\n-            # Need to remove existing file on Windows, else os.rename raises\n-            # WindowsError or FileExistsError.\n-            try:\n+        with compat_contextlib_suppress(OSError):\n+            if sys.platform == 'win32':\n+                # Need to remove existing file on Windows, else os.rename raises\n+                # WindowsError or FileExistsError.\n                 os.unlink(fn)\n-            except OSError:\n-                pass\n-        try:\n             mask = os.umask(0)\n             os.umask(mask)\n             os.chmod(tf.name, 0o666 & ~mask)\n-        except OSError:\n-            pass\n         os.rename(tf.name, fn)\n     except Exception:\n-        try:\n+        with compat_contextlib_suppress(OSError):\n             os.remove(tf.name)\n-        except OSError:\n-            pass\n         raise\n \n \n@@ -2033,14 +2027,13 @@ def extract_attributes(html_element):\n     NB HTMLParser is stricter in Python 2.6 & 3.2 than in later versions,\n     but the cases in the unit test will work for all of 2.6, 2.7, 3.2-3.5.\n     \"\"\"\n-    parser = HTMLAttributeParser()\n-    try:\n-        parser.feed(html_element)\n-        parser.close()\n-    # Older Python may throw HTMLParseError in case of malformed HTML\n-    except compat_HTMLParseError:\n-        pass\n-    return parser.attrs\n+    ret = None\n+    # Older Python may throw HTMLParseError in case of malformed HTML (and on .close()!)\n+    with compat_contextlib_suppress(compat_HTMLParseError):\n+        with contextlib.closing(HTMLAttributeParser()) as parser:\n+            parser.feed(html_element)\n+            ret = parser.attrs\n+    return ret or {}\n \n \n def clean_html(html):\n@@ -2241,7 +2234,8 @@ def _htmlentity_transform(entity_with_semicolon):\n             numstr = '0%s' % numstr\n         else:\n             base = 10\n-        # See https://github.com/ytdl-org/youtube-dl/issues/7518\n+        # See https://github.com/ytdl-org/youtube-dl/issues/7518\\\n+        # Also, weirdly, compat_contextlib_suppress fails here in 2.6\n         try:\n             return compat_chr(int(numstr, base))\n         except ValueError:\n@@ -2348,11 +2342,9 @@ def set_alpn_protocols(ctx):\n         # Some servers may (wrongly) reject requests if ALPN extension is not sent. See:\n         # https://github.com/python/cpython/issues/85140\n         # https://github.com/yt-dlp/yt-dlp/issues/3878\n-        try:\n+        with compat_contextlib_suppress(AttributeError, NotImplementedError):\n+            # fails for Python < 2.7.10, not ssl.HAS_ALPN\n             ctx.set_alpn_protocols(ALPN_PROTOCOLS)\n-        except (AttributeError, NotImplementedError):\n-            # Python < 2.7.10, not ssl.HAS_ALPN\n-            pass\n \n     opts_no_check_certificate = params.get('nocheckcertificate', False)\n     if hasattr(ssl, 'create_default_context'):  # Python >= 3.4 or 2.7.9\n@@ -2362,12 +2354,10 @@ def set_alpn_protocols(ctx):\n             context.check_hostname = False\n             context.verify_mode = ssl.CERT_NONE\n \n-        try:\n+        with compat_contextlib_suppress(TypeError):\n+            # Fails with Python 2.7.8 (create_default_context present\n+            # but HTTPSHandler has no context=)\n             return YoutubeDLHTTPSHandler(params, context=context, **kwargs)\n-        except TypeError:\n-            # Python 2.7.8\n-            # (create_default_context present but HTTPSHandler has no context=)\n-            pass\n \n     if sys.version_info < (3, 2):\n         return YoutubeDLHTTPSHandler(params, **kwargs)\n@@ -3176,12 +3166,10 @@ def parse_iso8601(date_str, delimiter='T', timezone=None):\n     if timezone is None:\n         timezone, date_str = extract_timezone(date_str)\n \n-    try:\n+    with compat_contextlib_suppress(ValueError):\n         date_format = '%Y-%m-%d{0}%H:%M:%S'.format(delimiter)\n         dt = datetime.datetime.strptime(date_str, date_format) - timezone\n         return calendar.timegm(dt.timetuple())\n-    except ValueError:\n-        pass\n \n \n def date_formats(day_first=True):\n@@ -3201,17 +3189,13 @@ def unified_strdate(date_str, day_first=True):\n     _, date_str = extract_timezone(date_str)\n \n     for expression in date_formats(day_first):\n-        try:\n+        with compat_contextlib_suppress(ValueError):\n             upload_date = datetime.datetime.strptime(date_str, expression).strftime('%Y%m%d')\n-        except ValueError:\n-            pass\n     if upload_date is None:\n         timetuple = email.utils.parsedate_tz(date_str)\n         if timetuple:\n-            try:\n+            with compat_contextlib_suppress(ValueError):\n                 upload_date = datetime.datetime(*timetuple[:6]).strftime('%Y%m%d')\n-            except ValueError:\n-                pass\n     if upload_date is not None:\n         return compat_str(upload_date)\n \n@@ -3240,11 +3224,9 @@ def unified_timestamp(date_str, day_first=True):\n         date_str = m.group(1)\n \n     for expression in date_formats(day_first):\n-        try:\n+        with compat_contextlib_suppress(ValueError):\n             dt = datetime.datetime.strptime(date_str, expression) - timezone + datetime.timedelta(hours=pm_delta)\n             return calendar.timegm(dt.timetuple())\n-        except ValueError:\n-            pass\n     timetuple = email.utils.parsedate_tz(date_str)\n     if timetuple:\n         return calendar.timegm(timetuple) + pm_delta * 3600 - compat_datetime_timedelta_total_seconds(timezone)\n", "test_patch": "diff --git a/test/test_downloader_external.py b/test/test_downloader_external.py\nindex 029f9b05f64..4491bd9dee6 100644\n--- a/test/test_downloader_external.py\n+++ b/test/test_downloader_external.py\n@@ -18,6 +18,7 @@\n )\n from youtube_dl import YoutubeDL\n from youtube_dl.compat import (\n+    compat_contextlib_suppress,\n     compat_http_cookiejar_Cookie,\n     compat_http_server,\n     compat_kwargs,\n@@ -35,6 +36,9 @@\n     HttpieFD,\n     WgetFD,\n )\n+from youtube_dl.postprocessor import (\n+    FFmpegPostProcessor,\n+)\n import threading\n \n TEST_SIZE = 10 * 1024\n@@ -227,7 +231,17 @@ def test_make_cmd(self):\n             self.assertIn('--load-cookies=%s' % downloader._cookies_tempfile, cmd)\n \n \n-@ifExternalFDAvailable(FFmpegFD)\n+# Handle delegated availability\n+def ifFFmpegFDAvailable(externalFD):\n+    # raise SkipTest, or set False!\n+    avail = ifExternalFDAvailable(externalFD) and False\n+    with compat_contextlib_suppress(Exception):\n+        avail = FFmpegPostProcessor(downloader=None).available\n+    return unittest.skipUnless(\n+        avail, externalFD.get_basename() + ' not found')\n+\n+\n+@ifFFmpegFDAvailable(FFmpegFD)\n class TestFFmpegFD(unittest.TestCase):\n     _args = []\n \n", "problem_statement": "External-downloader \"ffmpeg\" does not understand ffmpeg-location parameter\nYoutubeDownloader does not use `ffmpeg-location`  path for an `external-downloader` argument I think? Full folder value did not work in an external args.\r\n\r\n```\r\nyoutube-dl.exe --verbose ^\r\n --ffmpeg-location \"c:/apps/ffmpeg\" ^\r\n --format \"(bestvideo[height<=1080][ext=mp4])+(bestaudio[asr=48000][ext=webm])\" ^\r\n --external-downloader ffmpeg --external-downloader-args \"-ss 00:00:00.00 -to 00:01:00.00\" ^\r\n \"https://www.youtube.com/watch?v=1JWEb2uKZ28\" ^\r\n --merge-output-format mp4 -o \"wildlife.mp4\"\r\n```\r\n\r\nI had to put ffmpeg folder to PATH then external downloader worked.\r\n`set path=%path%;c:\\apps\\ffmpeg\"`\r\n\r\n**Feature Request** If external download is ffmpeg then try to use `ffmpeg-location` folder.\r\n\r\nps: Or is there a downlod time limit parameter already without using an external ffmpeg trick?\n", "hints_text": "I don't have a Windows test setup to hand but this has always worked before.\r\n\r\nSetting `--external-downloader ffmpeg --ffmpeg-location ./foo`, where `./foo` contains a (a link to) the _ffmpeg_ binary leads to this downloader output:\r\n```\r\n...\r\n[debug] ffmpeg command line: ./foo/ffmpeg -y -loglevel verbose -headers 'Accept-Lan\r\nguage: en-us,en;q=0.5\r\n...\r\n```\r\nPlease post the text log of your failing command, with `-v`/`--verbose`.\r\n\r\nI suppose `\"c:/dir1/dir2\"` is understood and shouldn't be `\"c:\\\\dir1\\\\dir2\"` or some such?\nExact error is `Requested external downloader cannot be used: ignoring --external-downloader-args` and app fallback to an internal downloader. It cannot use a duration limitter so 1h of video is downloaded and then must be trimmed post-process.\r\n\r\nI tried a combination of ffmpeg-location values nothing helps.\r\nExternal downloader works **if I put ffmpeg folder to dos PATH variable**  `set path=%path%;c:\\apps\\ffmpeg`\r\n\r\n```\r\n--ffmpeg-location \"c:/apps/ffmpeg\"\r\n--ffmpeg-location \"c:\\apps\\ffmpeg\"\r\n--ffmpeg-location \"c:\\\\apps\\\\ffmpeg\"\r\n--ffmpeg-location \"c:/apps/ffmpeg/\"\r\n--ffmpeg-location \"c:/apps/ffmpeg/ffmpeg.exe\"\r\n--external-downloader \"c:/apps/ffmpeg.exe\"\r\n--external-downloader \"c:\\apps\\ffmpeg.exe\"\r\n--external-downloader \"c:/apps/ffmpeg\"\r\n--external-downloader \"c:\\apps\\ffmpeg\"\r\n\r\nyoutube-dl.exe --verbose ^\r\n --ffmpeg-location \"c:/apps/ffmpeg\" ^\r\n --external-downloader ffmpeg ^\r\n --external-downloader-args \"-ss 00:00:00 -to 00:00:30\" ^\r\n --format \"(bestvideo[height<=1080][ext=mp4])+(bestaudio[asr=48000][ext=webm])\" ^\r\n \"https://www.youtube.com/watch?v=1JWEb2uKZ28\" ^\r\n --merge-output-format mp4 -o test.mp4\r\n[debug] System config: []\r\n[debug] User config: []\r\n[debug] Custom config: []\r\n[debug] Command-line args: ['--verbose', '--external-downloader', 'ffmpeg', '--ffmpeg-location', 'c:/apps/ffmpeg', '--external-downloader-args', '-ss 00:00:00 -to 00:00:30', '--format', '(bestvideo[height<=1080][ext=mp4])+(bestaudio[asr=48000][ext=webm])', 'https://www.youtube.com/watch?v=1JWEb2uKZ28', '--merge-output-format', 'mp4', '-o', 'test.mp4']\r\n[debug] Encodings: locale cp1252, fs mbcs, out cp437, pref cp1252\r\n[debug] youtube-dl version 2024.02.23 [40bd5c181] (single file build)\r\n[debug] ** This version was built from the latest master code at https://github.com/ytdl-org/youtube-dl.\r\n[debug] Python 3.4.4 (CPython AMD64 32bit) - Windows-10-10.0.19041 - OpenSSL 1.0.2d 9 Jul 2015\r\n[debug] exe versions: ffmpeg N-113115-gf01f31d39a, ffprobe N-113115-gf01f31d39a\r\n[debug] Proxy map: {}\r\nWARNING: Requested external downloader cannot be used: ignoring --external-downloader-args.\r\n[debug] Invoking downloader on 'https://rr2---sn-xap5-uh2el.googlevideo.com/videoplayback....'\r\n[dashsegments] Total fragments: 115\r\n[download] Destination: test.f137.mp4\r\n[download]   2.1% of ~1.12GiB at  7.70MiB/s ETA 02:26\r\n```\r\n\r\nI took a `git clone` sources to a local folder to debug what happens but am unable to run youtubeDL directly from the sources folder, how do I do it with a windows python3.exe?\n`cd` into the cloned `youtube-dl` directory (that contains `AUTHORS`). Then use `python3 -m youtube_dl` as the yt-dl command.\r\n\r\nThe `--ffmpeg-location` is a valid pathname, or there would be a `ffmpeg-location ... does not exist ...` warning.\r\n\r\nThe `ffmpeg` in that location is found, or there would be a `Cannot identify executable ...` warning.\r\n\r\nThere is this fragment in the _ffmpeg_ downloader class:\r\n```py\r\n    @classmethod\r\n    def available(cls, path=None):\r\n        # TODO: Fix path for ffmpeg\r\n        # Fixme: This may be wrong when --ffmpeg-location is used\r\n        return FFmpegPostProcessor().available\r\n```\r\nProbably the comment author had in mind that the `available` is an instance method that is being called on the class, and so doesn't take account of initialisation that occurs at instance creation. You'd think there'd be a _flake8_ diagnostic for that. However the wrongness would be in the wrong direction: the static `available` value would be `True` but after trying to find the executable at instance creation the instance value might be `False`. This could happen if the program is invalid (but we know it's not) or if its version output can't be parsed. \nThanks, had to edit an adhoc python syslib folder, without it an error `module not found` in my environment. I can now debug this behaviour. For now I just downloaded few youtube files using `set PATH` fix.\r\n\r\n```\r\nClone sources\r\n  cd C:\\apps\\youtube-dl\r\n  git clone --depth 1 https://github.com/ytdl-org/youtube-dl.git\r\n  cd C:\\apps\\youtube-dl\\youtube-dl\r\n  c:\\apps\\python-3.10.7\\python.exe -m site\r\nEdit python syslib _pth text file:\r\n  c:\\apps\\python-3.10.7\\python310._pth\r\nAdd path to an adhoc project module.\r\n  C:/apps/youtube-dl/youtube-dl\r\nList new syslib values and run app directly from the git sources folder\r\n  c:\\apps\\python-3.10.7\\python.exe -m site\r\n  \"c:\\apps\\python-3.10.7\\python.exe\" -m youtube_dl --help\r\n```\r\n\nI guess the .pth setting is an alternative to setting the current directory.\n@dirkf Indeed is as you said a problem with method static-class-instance scope. Hardcoding `def available(cls): return True` \"fixed\" this problem with an external ffmpeg downloader, original code returns false which is not expected.\r\n\r\nhttps://github.com/ytdl-org/youtube-dl/blob/40bd5c18153afe765caa6726302ee1dd8a9a2ce6/youtube_dl/downloader/external.py#L363\r\n\r\nI made a quick fix possibly not breaking the internal logic, use a class attribute to remember ffmpeg variable and check for instance+class variable in `available()` function.\r\n```\r\nFile ffmpeg.py:\r\nclass FFmpegPostProcessor(PostProcessor):\r\n    cls_basename = None ## see _determine_executables() and available()\r\n   ...clip...\r\n\t\r\n   def _determine_executables(self):\r\n        ...clip...at the end of function do this hack\r\n        if FFmpegPostProcessor.cls_basename is None: \r\n            FFmpegPostProcessor.cls_basename = self.basename\r\n\r\n    @property\r\n    def available(self):\r\n        return self.basename is not None or FFmpegPostProcessor.cls_basename is not None\r\n\r\n```\r\n\r\nRun from the sources folder\r\n```\r\ncd \"C:\\apps\\youtube-dl\\youtube-dl\"\r\n\"c:\\apps\\python-3.10.7\\python.exe\" -m youtube_dl ^\r\n --verbose ^\r\n --ffmpeg-location \"c:/apps/ffmpeg/\" ^\r\n --external-downloader \"ffmpeg\" ^\r\n --external-downloader-args \"-ss 00:00:00 -to 00:00:10\" ^\r\n --format \"(bestvideo[height<=1080][ext=mp4])+(bestaudio[asr=48000][ext=webm])\" ^\r\n \"https://www.youtube.com/watch?v=1JWEb2uKZ28\" ^\r\n --merge-output-format mp4 -o \"../test.mp4\"\r\n ```\r\n\nI think I understand the issue properly now.\r\n\r\nYour case is failing with `ed.can_download(info_dict)` being falsy, which leads to the \"Requested external downloader cannot be used: ...\" warning. Then `FFmpegFD` is being selected as downloader because `protocol.startswith('m3u8') and info_dict.get('is_live')` is truthy (ie, regardless of the `--extermnal-downloader`) (all in `downloader/__init__.py`).\r\n\r\n`ed.can_download(info_dict)` is falsy because its first subcondition `ed.available()` is falsy, which in turn is because the 'basename` of a `FFmpegPostProcessor` created without reference to the yt-dl instance is `None` (since the yt-dl instance is how it finds out about `--ffmpeg-location`).\r\n\r\nPassing `downloader=self.ydl` to the constructor in `FFmpegFD._call_downloader()` instead of `downloader=self` (that is an actual bug, I think, caused by misunderstanding what `downloader` is supposed to mean, and still present in _yt-dlp_) should result in finding the executable but it's too late to recover the `--external-downloader-args`.\r\n\r\nI think that combining this with your hack to hard-code the result of `FFmpegFD.available()` could give an adequate complete solution.", "created_at": "2024-03-11T00:29:14Z"}
{"repo": "ytdl-org/youtube-dl", "pull_number": 32725, "instance_id": "ytdl-org__youtube-dl-32725", "issue_numbers": ["32716", "32452"], "base_commit": "f0812d784836d18fd25ea32f9b5a0c9c6e92425b", "patch": "diff --git a/youtube_dl/extractor/common.py b/youtube_dl/extractor/common.py\nindex 0e5dfd8fa99..7fae9e57bab 100644\n--- a/youtube_dl/extractor/common.py\n+++ b/youtube_dl/extractor/common.py\n@@ -25,6 +25,7 @@\n     compat_getpass,\n     compat_integer_types,\n     compat_http_client,\n+    compat_kwargs,\n     compat_map as map,\n     compat_open as open,\n     compat_os_name,\n@@ -1102,6 +1103,60 @@ def _search_regex(self, pattern, string, name, default=NO_DEFAULT, fatal=True, f\n             self._downloader.report_warning('unable to extract %s' % _name + bug_reports_message())\n             return None\n \n+    def _search_json(self, start_pattern, string, name, video_id, **kwargs):\n+        \"\"\"Searches string for the JSON object specified by start_pattern\"\"\"\n+\n+        # self, start_pattern, string, name, video_id, *, end_pattern='',\n+        # contains_pattern=r'{(?s:.+)}', fatal=True, default=NO_DEFAULT\n+        # NB: end_pattern is only used to reduce the size of the initial match\n+        end_pattern = kwargs.pop('end_pattern', '')\n+        # (?:[\\s\\S]) simulates (?(s):.) (eg)\n+        contains_pattern = kwargs.pop('contains_pattern', r'{[\\s\\S]+}')\n+        fatal = kwargs.pop('fatal', True)\n+        default = kwargs.pop('default', NO_DEFAULT)\n+\n+        if default is NO_DEFAULT:\n+            default, has_default = {}, False\n+        else:\n+            fatal, has_default = False, True\n+\n+        json_string = self._search_regex(\n+            r'(?:{0})\\s*(?P<json>{1})\\s*(?:{2})'.format(\n+                start_pattern, contains_pattern, end_pattern),\n+            string, name, group='json', fatal=fatal, default=None if has_default else NO_DEFAULT)\n+        if not json_string:\n+            return default\n+\n+        # yt-dlp has a special JSON parser that allows trailing text.\n+        # Until that arrives here, the diagnostic from the exception\n+        # raised by json.loads() is used to extract the wanted text.\n+        # Either way, it's a problem if a transform_source() can't\n+        # handle the trailing text.\n+\n+        # force an exception\n+        kwargs['fatal'] = True\n+\n+        # self._downloader._format_err(name, self._downloader.Styles.EMPHASIS)\n+        for _ in range(2):\n+            try:\n+                # return self._parse_json(json_string, video_id, ignore_extra=True, **kwargs)\n+                transform_source = kwargs.pop('transform_source', None)\n+                if transform_source:\n+                    json_string = transform_source(json_string)\n+                return self._parse_json(json_string, video_id, **compat_kwargs(kwargs))\n+            except ExtractorError as e:\n+                end = int_or_none(self._search_regex(r'\\(char\\s+(\\d+)', error_to_compat_str(e), 'end', default=None))\n+                if end is not None:\n+                    json_string = json_string[:end]\n+                    continue\n+                msg = 'Unable to extract {0} - Failed to parse JSON'.format(name)\n+                if fatal:\n+                    raise ExtractorError(msg, cause=e.cause, video_id=video_id)\n+                elif not has_default:\n+                    self.report_warning(\n+                        '{0}: {1}'.format(msg, error_to_compat_str(e)), video_id=video_id)\n+            return default\n+\n     def _html_search_regex(self, pattern, string, name, default=NO_DEFAULT, fatal=True, flags=0, group=None):\n         \"\"\"\n         Like _search_regex, but strips HTML tags and unescapes entities.\n@@ -2966,25 +3021,22 @@ def manifest_url(manifest):\n         return formats\n \n     def _find_jwplayer_data(self, webpage, video_id=None, transform_source=js_to_json):\n-        mobj = re.search(\n-            r'''(?s)jwplayer\\s*\\(\\s*(?P<q>'|\")(?!(?P=q)).+(?P=q)\\s*\\)(?!</script>).*?\\.\\s*setup\\s*\\(\\s*(?P<options>(?:\\([^)]*\\)|[^)])+)\\s*\\)''',\n-            webpage)\n-        if mobj:\n-            try:\n-                jwplayer_data = self._parse_json(mobj.group('options'),\n-                                                 video_id=video_id,\n-                                                 transform_source=transform_source)\n-            except ExtractorError:\n-                pass\n-            else:\n-                if isinstance(jwplayer_data, dict):\n-                    return jwplayer_data\n+        return self._search_json(\n+            r'''(?<!-)\\bjwplayer\\s*\\(\\s*(?P<q>'|\")(?!(?P=q)).+(?P=q)\\s*\\)(?:(?!</script>).)*?\\.\\s*(?:setup\\s*\\(|(?P<load>load)\\s*\\(\\s*\\[)''',\n+            webpage, 'JWPlayer data', video_id,\n+            # must be a {...} or sequence, ending\n+            contains_pattern=r'\\{[\\s\\S]*}(?(load)(?:\\s*,\\s*\\{[\\s\\S]*})*)', end_pattern=r'(?(load)\\]|\\))',\n+            transform_source=transform_source, default=None)\n \n     def _extract_jwplayer_data(self, webpage, video_id, *args, **kwargs):\n-        jwplayer_data = self._find_jwplayer_data(\n-            webpage, video_id, transform_source=js_to_json)\n-        return self._parse_jwplayer_data(\n-            jwplayer_data, video_id, *args, **kwargs)\n+\n+        # allow passing `transform_source` through to _find_jwplayer_data()\n+        transform_source = kwargs.pop('transform_source', None)\n+        kwfind = compat_kwargs({'transform_source': transform_source}) if transform_source else {}\n+\n+        jwplayer_data = self._find_jwplayer_data(webpage, video_id, **kwfind)\n+\n+        return self._parse_jwplayer_data(jwplayer_data, video_id, *args, **kwargs)\n \n     def _parse_jwplayer_data(self, jwplayer_data, video_id=None, require_title=True,\n                              m3u8_id=None, mpd_id=None, rtmp_params=None, base_url=None):\n@@ -3018,22 +3070,14 @@ def _parse_jwplayer_data(self, jwplayer_data, video_id=None, require_title=True,\n                 mpd_id=mpd_id, rtmp_params=rtmp_params, base_url=base_url)\n \n             subtitles = {}\n-            tracks = video_data.get('tracks')\n-            if tracks and isinstance(tracks, list):\n-                for track in tracks:\n-                    if not isinstance(track, dict):\n-                        continue\n-                    track_kind = track.get('kind')\n-                    if not track_kind or not isinstance(track_kind, compat_str):\n-                        continue\n-                    if track_kind.lower() not in ('captions', 'subtitles'):\n-                        continue\n-                    track_url = urljoin(base_url, track.get('file'))\n-                    if not track_url:\n-                        continue\n-                    subtitles.setdefault(track.get('label') or 'en', []).append({\n-                        'url': self._proto_relative_url(track_url)\n-                    })\n+            for track in traverse_obj(video_data, (\n+                    'tracks', lambda _, t: t.get('kind').lower() in ('captions', 'subtitles'))):\n+                track_url = urljoin(base_url, track.get('file'))\n+                if not track_url:\n+                    continue\n+                subtitles.setdefault(track.get('label') or 'en', []).append({\n+                    'url': self._proto_relative_url(track_url)\n+                })\n \n             entry = {\n                 'id': this_video_id,\ndiff --git a/youtube_dl/extractor/extractors.py b/youtube_dl/extractor/extractors.py\nindex 82221445fc2..285f3dd5f3c 100644\n--- a/youtube_dl/extractor/extractors.py\n+++ b/youtube_dl/extractor/extractors.py\n@@ -382,7 +382,6 @@\n     FC2EmbedIE,\n )\n from .fczenit import FczenitIE\n-from .filemoon import FileMoonIE\n from .fifa import FifaIE\n from .filmon import (\n     FilmOnIE,\ndiff --git a/youtube_dl/extractor/filemoon.py b/youtube_dl/extractor/filemoon.py\ndeleted file mode 100644\nindex 654df9b6915..00000000000\n--- a/youtube_dl/extractor/filemoon.py\n+++ /dev/null\n@@ -1,43 +0,0 @@\n-# coding: utf-8\n-from __future__ import unicode_literals\n-\n-import re\n-\n-from .common import InfoExtractor\n-from ..utils import (\n-    decode_packed_codes,\n-    js_to_json,\n-)\n-\n-\n-class FileMoonIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?filemoon\\.sx/./(?P<id>\\w+)'\n-    _TEST = {\n-        'url': 'https://filemoon.sx/e/dw40rxrzruqz',\n-        'md5': '5a713742f57ac4aef29b74733e8dda01',\n-        'info_dict': {\n-            'id': 'dw40rxrzruqz',\n-            'title': 'dw40rxrzruqz',\n-            'ext': 'mp4'\n-        }\n-    }\n-\n-    def _real_extract(self, url):\n-        video_id = self._match_id(url)\n-\n-        webpage = self._download_webpage(url, video_id)\n-        matches = re.findall(r'(?s)(eval.*?)</script>', webpage)\n-        packed = matches[-1]\n-        unpacked = decode_packed_codes(packed)\n-        jwplayer_sources = self._parse_json(\n-            self._search_regex(\n-                r'(?s)player\\s*\\.\\s*setup\\s*\\(\\s*\\{\\s*sources\\s*:\\s*(.*?])', unpacked, 'jwplayer sources'),\n-            video_id, transform_source=js_to_json)\n-\n-        formats = self._parse_jwplayer_formats(jwplayer_sources, video_id)\n-\n-        return {\n-            'id': video_id,\n-            'title': self._generic_title(url) or video_id,\n-            'formats': formats\n-        }\ndiff --git a/youtube_dl/extractor/xfileshare.py b/youtube_dl/extractor/xfileshare.py\nindex df9efa9faed..4dc3032e7e0 100644\n--- a/youtube_dl/extractor/xfileshare.py\n+++ b/youtube_dl/extractor/xfileshare.py\n@@ -4,20 +4,28 @@\n import re\n \n from .common import InfoExtractor\n-from ..compat import compat_chr\n+from ..compat import (\n+    compat_chr,\n+    compat_zip as zip,\n+)\n from ..utils import (\n+    clean_html,\n     decode_packed_codes,\n     determine_ext,\n     ExtractorError,\n+    get_element_by_id,\n     int_or_none,\n-    js_to_json,\n+    merge_dicts,\n+    T,\n+    traverse_obj,\n+    url_or_none,\n     urlencode_postdata,\n )\n \n \n # based on openload_decode from 2bfeee69b976fe049761dd3012e30b637ee05a58\n def aa_decode(aa_code):\n-    symbol_table = [\n+    symbol_table = (\n         ('7', '((\uff9f\uff70\uff9f) + (o^_^o))'),\n         ('6', '((o^_^o) +(o^_^o))'),\n         ('5', '((\uff9f\uff70\uff9f) + (\uff9f\u0398\uff9f))'),\n@@ -26,84 +34,180 @@ def aa_decode(aa_code):\n         ('3', '(o^_^o)'),\n         ('1', '(\uff9f\u0398\uff9f)'),\n         ('0', '(c^_^o)'),\n-    ]\n+        ('+', ''),\n+    )\n     delim = '(\uff9f\u0414\uff9f)[\uff9f\u03b5\uff9f]+'\n-    ret = ''\n-    for aa_char in aa_code.split(delim):\n+\n+    def chr_from_code(c):\n         for val, pat in symbol_table:\n-            aa_char = aa_char.replace(pat, val)\n-        aa_char = aa_char.replace('+ ', '')\n-        m = re.match(r'^\\d+', aa_char)\n-        if m:\n-            ret += compat_chr(int(m.group(0), 8))\n+            c = c.replace(pat, val)\n+        if c.startswith(('u', 'U')):\n+            base = 16\n+            c = c[1:]\n         else:\n-            m = re.match(r'^u([\\da-f]+)', aa_char)\n-            if m:\n-                ret += compat_chr(int(m.group(1), 16))\n-    return ret\n+            base = 10\n+        c = int_or_none(c, base=base)\n+        return '' if c is None else compat_chr(c)\n+\n+    return ''.join(\n+        chr_from_code(aa_char)\n+        for aa_char in aa_code.split(delim))\n \n \n class XFileShareIE(InfoExtractor):\n     _SITES = (\n-        (r'aparat\\.cam', 'Aparat'),\n-        (r'clipwatching\\.com', 'ClipWatching'),\n-        (r'gounlimited\\.to', 'GoUnlimited'),\n-        (r'govid\\.me', 'GoVid'),\n-        (r'holavid\\.com', 'HolaVid'),\n-        (r'streamty\\.com', 'Streamty'),\n-        (r'thevideobee\\.to', 'TheVideoBee'),\n-        (r'uqload\\.com', 'Uqload'),\n-        (r'vidbom\\.com', 'VidBom'),\n-        (r'vidlo\\.us', 'vidlo'),\n-        (r'vidlocker\\.xyz', 'VidLocker'),\n-        (r'vidshare\\.tv', 'VidShare'),\n-        (r'vup\\.to', 'VUp'),\n+        # status check 2024-02: site availability, G site: search\n+        (r'aparat\\.cam', 'Aparat'),  # Cloudflare says host error 522, apparently changed to wolfstreeam.tv\n+        (r'filemoon\\.sx/.', 'FileMoon'),\n+        (r'gounlimited\\.to', 'GoUnlimited'),  # no media pages listed\n+        (r'govid\\.me', 'GoVid'),  # no media pages listed\n+        (r'highstream\\.tv', 'HighStream'),  # clipwatching.com redirects here\n+        (r'holavid\\.com', 'HolaVid'),  # Cloudflare says host error 522\n+        # (r'streamty\\.com', 'Streamty'),  # no media pages listed, connection timeout\n+        # (r'thevideobee\\.to', 'TheVideoBee'),  # no pages listed, refuses connection\n+        (r'uqload\\.to', 'Uqload'),  # .com, .co redirect here\n+        (r'(?:vedbam\\.xyz|vadbam.net)', 'V?dB?m'),  # vidbom.com redirects here, but no valid media pages listed\n+        (r'vidlo\\.us', 'vidlo'),  # no valid media pages listed\n+        (r'vidlocker\\.xyz', 'VidLocker'),  # no media pages listed\n+        (r'(?:w\\d\\.)?viidshar\\.com', 'VidShare'),  # vidshare.tv redirects here\n+        # (r'vup\\.to', 'VUp'),  # domain not found\n         (r'wolfstream\\.tv', 'WolfStream'),\n-        (r'xvideosharing\\.com', 'XVideoSharing'),\n+        (r'xvideosharing\\.com', 'XVideoSharing'),  # just started showing 'maintenance mode'\n     )\n \n-    IE_DESC = 'XFileShare based sites: %s' % ', '.join(list(zip(*_SITES))[1])\n+    IE_DESC = 'XFileShare-based sites: %s' % ', '.join(list(zip(*_SITES))[1])\n     _VALID_URL = (r'https?://(?:www\\.)?(?P<host>%s)/(?:embed-)?(?P<id>[0-9a-zA-Z]+)'\n                   % '|'.join(site for site in list(zip(*_SITES))[0]))\n+    _EMBED_REGEX = [r'<iframe\\b[^>]+\\bsrc=([\"\\'])(?P<url>(?:https?:)?//(?:%s)/embed-[0-9a-zA-Z]+.*?)\\1' % '|'.join(site for site in list(zip(*_SITES))[0])]\n \n     _FILE_NOT_FOUND_REGEXES = (\n         r'>(?:404 - )?File Not Found<',\n         r'>The file was removed by administrator<',\n     )\n+    _TITLE_REGEXES = (\n+        r'style=\"z-index: [0-9]+;\">([^<]+)</span>',\n+        r'<td nowrap>([^<]+)</td>',\n+        r'h4-fine[^>]*>([^<]+)<',\n+        r'>Watch (.+)[ <]',\n+        r'<h2 class=\"video-page-head\">([^<]+)</h2>',\n+        r'<h2 style=\"[^\"]*color:#403f3d[^\"]*\"[^>]*>([^<]+)<',  # streamin.to (dead)\n+        r'title\\s*:\\s*\"([^\"]+)\"',  # govid.me\n+    )\n+    _SOURCE_URL_REGEXES = (\n+        r'(?:file|src)\\s*:\\s*([\"\\'])(?P<url>http(?:(?!\\1).)+\\.(?:m3u8|mp4|flv)(?:(?!\\1).)*)\\1',\n+        r'file_link\\s*=\\s*([\"\\'])(?P<url>http(?:(?!\\1).)+)\\1',\n+        r'addVariable\\((\\\\?[\"\\'])file\\1\\s*,\\s*(\\\\?[\"\\'])(?P<url>http(?:(?!\\2).)+)\\2\\)',\n+        r'<embed[^>]+src=([\"\\'])(?P<url>http(?:(?!\\1).)+\\.(?:m3u8|mp4|flv)(?:(?!\\1).)*)\\1',\n+    )\n+    _THUMBNAIL_REGEXES = (\n+        r'<video[^>]+poster=\"([^\"]+)\"',\n+        r'(?:image|poster)\\s*:\\s*[\"\\'](http[^\"\\']+)[\"\\'],',\n+    )\n \n     _TESTS = [{\n-        'url': 'http://xvideosharing.com/fq65f94nd2ve',\n-        'md5': '4181f63957e8fe90ac836fa58dc3c8a6',\n+        'note': 'link in `sources`',\n+        'url': 'https://uqload.to/dcsu06gdb45o',\n+        'md5': '7f8db187b254379440bf4fcad094ae86',\n         'info_dict': {\n-            'id': 'fq65f94nd2ve',\n+            'id': 'dcsu06gdb45o',\n             'ext': 'mp4',\n-            'title': 'sample',\n-            'thumbnail': r're:http://.*\\.jpg',\n+            'title': 'f2e31015957e74c8c8427982e161c3fc mp4',\n+            'thumbnail': r're:https://.*\\.jpg'\n+        },\n+        'params': {\n+            'nocheckcertificate': True,\n+        },\n+        'expected_warnings': ['Unable to extract JWPlayer data'],\n+    }, {\n+        'note': 'link in decoded `sources`',\n+        'url': 'https://xvideosharing.com/1tlg6agrrdgc',\n+        'md5': '2608ce41932c1657ae56258a64e647d9',\n+        'info_dict': {\n+            'id': '1tlg6agrrdgc',\n+            'ext': 'mp4',\n+            'title': '0121',\n+            'thumbnail': r're:https?://.*\\.jpg',\n+        },\n+        'skip': 'This server is in maintenance mode.',\n+    }, {\n+        'note': 'JWPlayer link in un-p,a,c,k,e,d JS',\n+        'url': 'https://filemoon.sx/e/dw40rxrzruqz',\n+        'md5': '5a713742f57ac4aef29b74733e8dda01',\n+        'info_dict': {\n+            'id': 'dw40rxrzruqz',\n+            'title': 'dw40rxrzruqz',\n+            'ext': 'mp4'\n+        },\n+    }, {\n+        'note': 'JWPlayer link in un-p,a,c,k,e,d JS',\n+        'url': 'https://vadbam.net/6lnbkci96wly.html',\n+        'md5': 'a1616800076177e2ac769203957c54bc',\n+        'info_dict': {\n+            'id': '6lnbkci96wly',\n+            'title': 'Heart Crime S01 E03 weciima autos',\n+            'ext': 'mp4'\n+        },\n+    }, {\n+        'note': 'JWPlayer link in clear',\n+        'url': 'https://w1.viidshar.com/nnibe0xf0h79.html',\n+        'md5': 'f0a580ce9df06cc61b4a5c979d672367',\n+        'info_dict': {\n+            'id': 'nnibe0xf0h79',\n+            'title': 'JaGa 68ar',\n+            'ext': 'mp4'\n+        },\n+        'params': {\n+            'skip_download': 'ffmpeg',\n+        },\n+        'expected_warnings': ['hlsnative has detected features it does not support'],\n+    }, {\n+        'note': 'JWPlayer link in clear',\n+        'url': 'https://wolfstream.tv/a3drtehyrg52.html',\n+        'md5': '1901d86a79c5e0c6a51bdc9a4cfd3769',\n+        'info_dict': {\n+            'id': 'a3drtehyrg52',\n+            'title': 'NFL 2023 W04 DET@GB',\n+            'ext': 'mp4'\n         },\n     }, {\n         'url': 'https://aparat.cam/n4d6dh0wvlpr',\n         'only_matching': True,\n     }, {\n-        'url': 'https://wolfstream.tv/nthme29v9u2x',\n+        'url': 'https://uqload.to/ug5somm0ctnk.html',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://highstream.tv/2owiyz3sjoux',\n+        'only_matching': True,\n+    }, {\n+        'url': 'https://vedbam.xyz/6lnbkci96wly.html',\n         'only_matching': True,\n     }]\n \n-    @staticmethod\n-    def _extract_urls(webpage):\n-        return [\n-            mobj.group('url')\n-            for mobj in re.finditer(\n-                r'<iframe\\b[^>]+\\bsrc=([\"\\'])(?P<url>(?:https?:)?//(?:%s)/embed-[0-9a-zA-Z]+.*?)\\1'\n-                % '|'.join(site for site in list(zip(*XFileShareIE._SITES))[0]),\n-                webpage)]\n+    @classmethod\n+    def _extract_urls(cls, webpage):\n+\n+        def yield_urls():\n+            for regex in cls._EMBED_REGEX:\n+                for mobj in re.finditer(regex, webpage):\n+                    yield mobj.group('url')\n+\n+        return list(yield_urls())\n \n     def _real_extract(self, url):\n-        host, video_id = re.match(self._VALID_URL, url).groups()\n+        host, video_id = self._match_valid_url(url).group('host', 'id')\n \n-        url = 'https://%s/' % host + ('embed-%s.html' % video_id if host in ('govid.me', 'vidlo.us') else video_id)\n+        url = 'https://%s/%s' % (\n+            host,\n+            'embed-%s.html' % video_id if host in ('govid.me', 'vidlo.us') else video_id)\n         webpage = self._download_webpage(url, video_id)\n-\n-        if any(re.search(p, webpage) for p in self._FILE_NOT_FOUND_REGEXES):\n+        container_div = get_element_by_id('container', webpage) or webpage\n+        if self._search_regex(\n+                r'>This server is in maintenance mode\\.', container_div,\n+                'maint error', group=0, default=None):\n+            raise ExtractorError(clean_html(container_div), expected=True)\n+        if self._search_regex(\n+                self._FILE_NOT_FOUND_REGEXES, container_div,\n+                'missing video error', group=0, default=None):\n             raise ExtractorError('Video %s does not exist' % video_id, expected=True)\n \n         fields = self._hidden_inputs(webpage)\n@@ -122,59 +226,43 @@ def _real_extract(self, url):\n                     'Content-type': 'application/x-www-form-urlencoded',\n                 })\n \n-        title = (self._search_regex(\n-            (r'style=\"z-index: [0-9]+;\">([^<]+)</span>',\n-             r'<td nowrap>([^<]+)</td>',\n-             r'h4-fine[^>]*>([^<]+)<',\n-             r'>Watch (.+)[ <]',\n-             r'<h2 class=\"video-page-head\">([^<]+)</h2>',\n-             r'<h2 style=\"[^\"]*color:#403f3d[^\"]*\"[^>]*>([^<]+)<',  # streamin.to\n-             r'title\\s*:\\s*\"([^\"]+)\"'),  # govid.me\n-            webpage, 'title', default=None) or self._og_search_title(\n-            webpage, default=None) or video_id).strip()\n-\n-        for regex, func in (\n-                (r'(eval\\(function\\(p,a,c,k,e,d\\){.+)', decode_packed_codes),\n-                (r'(\uff9f.+)', aa_decode)):\n-            obf_code = self._search_regex(regex, webpage, 'obfuscated code', default=None)\n-            if obf_code:\n-                webpage = webpage.replace(obf_code, func(obf_code))\n-\n-        formats = []\n-\n-        jwplayer_data = self._search_regex(\n-            [\n-                r'jwplayer\\(\"[^\"]+\"\\)\\.load\\(\\[({.+?})\\]\\);',\n-                r'jwplayer\\(\"[^\"]+\"\\)\\.setup\\(({.+?})\\);',\n-            ], webpage,\n-            'jwplayer data', default=None)\n-        if jwplayer_data:\n-            jwplayer_data = self._parse_json(\n-                jwplayer_data.replace(r\"\\'\", \"'\"), video_id, js_to_json)\n+        title = (\n+            self._search_regex(self._TITLE_REGEXES, webpage, 'title', default=None)\n+            or self._og_search_title(webpage, default=None)\n+            or video_id).strip()\n+\n+        obf_code = True\n+        while obf_code:\n+            for regex, func in (\n+                    (r'(?s)(?<!-)\\b(eval\\(function\\(p,a,c,k,e,d\\)\\{(?:(?!</script>).)+\\)\\))',\n+                     decode_packed_codes),\n+                    (r'(\uff9f.+)', aa_decode)):\n+                obf_code = self._search_regex(regex, webpage, 'obfuscated code', default=None)\n+                if obf_code:\n+                    webpage = webpage.replace(obf_code, func(obf_code))\n+                    break\n+\n+        jwplayer_data = self._find_jwplayer_data(\n+            webpage.replace(r'\\'', '\\''), video_id)\n+        result = self._parse_jwplayer_data(\n+            jwplayer_data, video_id, require_title=False,\n+            m3u8_id='hls', mpd_id='dash')\n+\n+        if not traverse_obj(result, 'formats'):\n             if jwplayer_data:\n-                formats = self._parse_jwplayer_data(\n-                    jwplayer_data, video_id, False,\n-                    m3u8_id='hls', mpd_id='dash')['formats']\n-\n-        if not formats:\n-            urls = []\n-            for regex in (\n-                    r'(?:file|src)\\s*:\\s*([\"\\'])(?P<url>http(?:(?!\\1).)+\\.(?:m3u8|mp4|flv)(?:(?!\\1).)*)\\1',\n-                    r'file_link\\s*=\\s*([\"\\'])(?P<url>http(?:(?!\\1).)+)\\1',\n-                    r'addVariable\\((\\\\?[\"\\'])file\\1\\s*,\\s*(\\\\?[\"\\'])(?P<url>http(?:(?!\\2).)+)\\2\\)',\n-                    r'<embed[^>]+src=([\"\\'])(?P<url>http(?:(?!\\1).)+\\.(?:m3u8|mp4|flv)(?:(?!\\1).)*)\\1'):\n+                self.report_warning(\n+                    'Failed to extract JWPlayer formats', video_id=video_id)\n+            urls = set()\n+            for regex in self._SOURCE_URL_REGEXES:\n                 for mobj in re.finditer(regex, webpage):\n-                    video_url = mobj.group('url')\n-                    if video_url not in urls:\n-                        urls.append(video_url)\n+                    urls.add(mobj.group('url'))\n \n             sources = self._search_regex(\n                 r'sources\\s*:\\s*(\\[(?!{)[^\\]]+\\])', webpage, 'sources', default=None)\n-            if sources:\n-                urls.extend(self._parse_json(sources, video_id))\n+            urls.update(traverse_obj(sources, (T(lambda s: self._parse_json(s, video_id)), Ellipsis)))\n \n             formats = []\n-            for video_url in urls:\n+            for video_url in traverse_obj(urls, (Ellipsis, T(url_or_none))):\n                 if determine_ext(video_url) == 'm3u8':\n                     formats.extend(self._extract_m3u8_formats(\n                         video_url, video_id, 'mp4',\n@@ -185,17 +273,19 @@ def _real_extract(self, url):\n                         'url': video_url,\n                         'format_id': 'sd',\n                     })\n-        self._sort_formats(formats)\n+            result = {'formats': formats}\n+\n+        self._sort_formats(result['formats'])\n \n         thumbnail = self._search_regex(\n-            [\n-                r'<video[^>]+poster=\"([^\"]+)\"',\n-                r'(?:image|poster)\\s*:\\s*[\"\\'](http[^\"\\']+)[\"\\'],',\n-            ], webpage, 'thumbnail', default=None)\n+            self._THUMBNAIL_REGEXES, webpage, 'thumbnail', default=None)\n+\n+        if not (title or result.get('title')):\n+            title = self._generic_title(url) or video_id\n \n-        return {\n+        return merge_dicts(result, {\n             'id': video_id,\n-            'title': title,\n+            'title': title or None,\n             'thumbnail': thumbnail,\n-            'formats': formats,\n-        }\n+            'http_headers': {'Referer': url}\n+        })\ndiff --git a/youtube_dl/utils.py b/youtube_dl/utils.py\nindex 61b94d84c44..042c9daa50e 100644\n--- a/youtube_dl/utils.py\n+++ b/youtube_dl/utils.py\n@@ -3832,14 +3832,15 @@ def get_method(self):\n         return 'PUT'\n \n \n-def int_or_none(v, scale=1, default=None, get_attr=None, invscale=1):\n+def int_or_none(v, scale=1, default=None, get_attr=None, invscale=1, base=None):\n     if get_attr:\n         if v is not None:\n             v = getattr(v, get_attr, None)\n     if v in (None, ''):\n         return default\n     try:\n-        return int(v) * invscale // scale\n+        # like int, raise if base is specified and v is not a string\n+        return (int(v) if base is None else int(v, base=base)) * invscale // scale\n     except (ValueError, TypeError, OverflowError):\n         return default\n \n", "test_patch": "diff --git a/test/test_utils.py b/test/test_utils.py\nindex 102420fcb88..d1d9ca1b24e 100644\n--- a/test/test_utils.py\n+++ b/test/test_utils.py\n@@ -512,11 +512,14 @@ def test_float_or_none(self):\n         self.assertEqual(float_or_none(set()), None)\n \n     def test_int_or_none(self):\n+        self.assertEqual(int_or_none(42), 42)\n         self.assertEqual(int_or_none('42'), 42)\n         self.assertEqual(int_or_none(''), None)\n         self.assertEqual(int_or_none(None), None)\n         self.assertEqual(int_or_none([]), None)\n         self.assertEqual(int_or_none(set()), None)\n+        self.assertEqual(int_or_none('42', base=8), 34)\n+        self.assertRaises(TypeError, int_or_none(42, base=8))\n \n     def test_str_to_int(self):\n         self.assertEqual(str_to_int('123,456'), 123456)\n", "problem_statement": "filemoon.sx jwplayer error\n<!--\r\n\r\n######################################################################\r\n  WARNING!\r\n  IGNORING THE FOLLOWING TEMPLATE WILL RESULT IN ISSUE CLOSED AS INCOMPLETE\r\n######################################################################\r\n\r\n-->\r\n\r\n\r\n## Checklist\r\n\r\n<!--\r\nCarefully read and work through this check list in order to prevent the most common mistakes and misuse of youtube-dl:\r\n- First of, make sure you are using the latest version of youtube-dl. Run `youtube-dl --version` and ensure your version is 2021.12.17. If it's not, see https://yt-dl.org/update on how to update. Issues with outdated version will be REJECTED.\r\n- Make sure that all provided video/audio/playlist URLs (if any) are alive and playable in a browser.\r\n- Make sure that all URLs and arguments with special characters are properly quoted or escaped as explained in http://yt-dl.org/escape.\r\n- Search the bugtracker for similar issues: http://yt-dl.org/search-issues. DO NOT post duplicates.\r\n- Finally, put x into all relevant boxes (like this [x])\r\n-->\r\n\r\n- [x] I'm reporting a broken site support\r\n- [x] I'm running youtube-dl 2024.02.03 [4416f82c8]\r\n- [x] I've checked that all provided URLs are alive and playable in a browser\r\n- [x] I've checked that all URLs and arguments with special characters are properly quoted or escaped\r\n- [x] I've searched the bugtracker for similar issues including closed ones\r\n\r\n\r\n## Verbose log\r\n\r\n<!--\r\nProvide the complete verbose output of youtube-dl that clearly demonstrates the problem.\r\nAdd the `-v` flag to your command line you run youtube-dl with (`youtube-dl -v <your command line>`), copy the WHOLE output and insert it below. It should look similar to this:\r\n [debug] System config: []\r\n [debug] User config: []\r\n [debug] Command-line args: [u'-v', u'http://www.youtube.com/watch?v=BaW_jenozKcj']\r\n [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251\r\n [debug] youtube-dl version 2021.12.17\r\n [debug] Python version 2.7.11 - Windows-2003Server-5.2.3790-SP2\r\n [debug] exe versions: ffmpeg N-75573-g1d0487f, ffprobe N-75573-g1d0487f, rtmpdump 2.4\r\n [debug] Proxy map: {}\r\n <more lines>\r\n-->\r\n\r\n```\r\nDownloads>youtube-dl --verbose https://filemoon.sx/d/dylsgj1mbwn5/video_2024-01-28_02-44-36.mp4\r\n[debug] System config: []\r\n[debug] User config: []\r\n[debug] Custom config: []\r\n[debug] Command-line args: ['--verbose', 'https://filemoon.sx/d/dylsgj1mbwn5/video_2024-01-28_02-44-36.mp4']\r\n[debug] Encodings: locale cp1252, fs mbcs, out cp437, pref cp1252\r\n[debug] youtube-dl version 2024.02.03 [4416f82c8] (single file build)\r\n[debug] ** This version was built from the latest master code at https://github.com/ytdl-org/youtube-dl.\r\n[debug] ** For support, visit the main site.\r\n[debug] Python 3.4.4 (CPython AMD64 32bit) - Windows-10-10.0.22621 - OpenSSL 1.0.2d 9 Jul 2015\r\n[debug] exe versions: ffmpeg 6.0-full_build-www.gyan.dev, ffprobe 6.0-full_build-www.gyan.dev\r\n[debug] Proxy map: {}\r\n[FileMoon] dylsgj1mbwn5: Downloading webpage\r\nERROR: Unable to extract jwplayer sources; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.\r\nTraceback (most recent call last):\r\n  File \"D:\\a\\ytdl-nightly\\ytdl-nightly\\youtube_dl\\YoutubeDL.py\", line 863, in wrapper\r\n  File \"D:\\a\\ytdl-nightly\\ytdl-nightly\\youtube_dl\\YoutubeDL.py\", line 959, in __extract_info\r\n  File \"D:\\a\\ytdl-nightly\\ytdl-nightly\\youtube_dl\\extractor\\common.py\", line 570, in extract\r\n  File \"D:\\a\\ytdl-nightly\\ytdl-nightly\\youtube_dl\\extractor\\filemoon.py\", line 34, in _real_extract\r\n  File \"D:\\a\\ytdl-nightly\\ytdl-nightly\\youtube_dl\\extractor\\common.py\", line 1100, in _search_regex\r\nyoutube_dl.utils.RegexNotFoundError: Unable to extract jwplayer sources; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.\r\n```\r\n\r\n\r\n## Description\r\n\r\n<!--\r\nProvide an explanation of your issue in an arbitrary form. Provide any additional information, suggested solution and as much context and examples as possible.\r\nIf work on your issue requires account credentials please provide them or explain how one can obtain them.\r\n-->\r\n\r\nSite not working with error in verbose log. Please let me know if any other information is needed, thank you!\r\n\n[xfileshare] Add uqload.co support\n## Please follow the guide below\r\n\r\n- You will be asked some questions, please read them **carefully** and answer honestly\r\n- Put an `x` into all the boxes [ ] relevant to your *pull request* (like that [x])\r\n- Use *Preview* tab to see how your *pull request* will actually look like\r\n\r\n---\r\n\r\n### Before submitting a *pull request* make sure you have:\r\n- [x] [Searched](https://github.com/ytdl-org/youtube-dl/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests\r\n- [x] Read [adding new extractor tutorial](https://github.com/ytdl-org/youtube-dl#adding-support-for-a-new-site)\r\n- [x] Read [youtube-dl coding conventions](https://github.com/ytdl-org/youtube-dl#youtube-dl-coding-conventions) and adjusted the code to meet them\r\n- [x] Covered the code with tests (note that PRs without tests will be REJECTED)\r\n- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8)\r\n\r\n### In order to be accepted and merged into youtube-dl each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check one of the following options:\r\n- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)\r\n- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)\r\n\r\n### What is the purpose of your *pull request*?\r\n- [ ] Bug fix\r\n- [x] Improvement\r\n- [ ] New extractor\r\n- [ ] New feature\r\n\r\n---\r\n\r\n### Description of your *pull request* and other information\r\n\r\nuqload.com now redirects to uqload.co\r\n\n", "hints_text": "The variable name in the obfuscated JS has changed:\r\n```diff\r\n--- old/youtube_dl/extractor/filemoon.py\r\n+++ new/youtube_dl/extractor/filemoon.py\r\n@@ -31,7 +31,7 @@\r\n         unpacked = decode_packed_codes(packed)\r\n         jwplayer_sources = self._parse_json(\r\n             self._search_regex(\r\n-                r'(?s)player\\s*\\.\\s*setup\\s*\\(\\s*\\{\\s*sources\\s*:\\s*(.*?])', unpacked, 'jwplayer sources'),\r\n+                r'(?s)(?:videop|player)\\s*\\.\\s*setup\\s*\\(\\s*\\{\\s*sources\\s*:\\s*(.*?])', unpacked, 'jwplayer sources'),\r\n             video_id, transform_source=js_to_json)\r\n \r\n         formats = self._parse_jwplayer_formats(jwplayer_sources, video_id)\r\n```\nWorks as advertised. \ud83d\ude00\ufe0f\r\n\r\n```\r\nyoutube-dl -v --ignore-config -o '~/Desktop/%(title)s.%(ext)s' https://filemoon.sx/d/dylsgj1mbwn5/video_2024-01-28_02-44-36.mp4\r\n[debug] System config: []\r\n[debug] User config: []\r\n[debug] Custom config: []\r\n[debug] Command-line args: ['-v', '--ignore-config', '-o', '~/Desktop/%(title)s.%(ext)s', 'https://filemoon.sx/d/dylsgj1mbwn5/video_2024-01-28_02-44-36.mp4']\r\n[debug] Encodings: locale UTF-8, fs utf-8, out utf-8, pref UTF-8\r\n[debug] youtube-dl version 2024.02.02\r\n[debug] Lazy loading extractors enabled\r\n[debug] Single file build\r\n[debug] Python 3.8.10 (CPython x86_64 64bit) - Linux-5.4.0-170-generic-x86_64-with-glibc2.29 - OpenSSL 1.1.1f  31 Mar 2020 - glibc 2.31\r\n[debug] exe versions: ffmpeg N-113412-g0b8e51b584-20240124, ffprobe N-113412-g0b8e51b584-20240124, phantomjs 2.1.1, rtmpdump 2.4\r\n[debug] Proxy map: {}\r\n[FileMoon] dylsgj1mbwn5: Downloading webpage\r\n[FileMoon] dylsgj1mbwn5: Downloading m3u8 information\r\n[debug] Default format spec: bestvideo+bestaudio/best\r\n[debug] Invoking downloader on 'https://be4242.rcr52.ams03.cdn112.com/hls2/01/05449/dylsgj1mbwn5_o/index-v1-a1.m3u8?t=XLKyHPQJUWtLmX3QHA5-nAUz1OBYhirC-IiISfsgZZA&s=1707073640&e=43200&f=27249932&srv=25&asn=33915&sp=2000'\r\n[hlsnative] Downloading m3u8 manifest\r\n[hlsnative] Total fragments: 1\r\n[download] Destination: /home/user/Desktop/video_2024-01-28_02-44-36.mp4\r\n[download] 100% of 787.62KiB in 00:00\r\n[debug] ffmpeg command line: ffprobe -show_streams file:/home/user/Desktop/video_2024-01-28_02-44-36.mp4\r\n[ffmpeg] Fixing malformed AAC bitstream in \"/home/user/Desktop/video_2024-01-28_02-44-36.mp4\"\r\n[debug] ffmpeg command line: ffmpeg -y -loglevel repeat+info -i file:/home/user/Desktop/video_2024-01-28_02-44-36.mp4 -c copy -f mp4 -bsf:a aac_adtstoasc file:/home/user/Desktop/video_2024-01-28_02-44-36.temp.mp4\r\n```\r\n\r\nHowever yt-dlp gives this:\r\n\r\n```\r\nERROR: [Piracy] This website is no longer supported since it has been determined to be primarily used for piracy.\r\n       DO NOT open issues for it\r\n\r\n```\nIf your head hurts when you hit it with a hammer, stop hitting it ...\r\n\r\nThe site's [copyright policy](https://filemoon.sx/copyright) does not support the categorisation as a \"Piracy\" site, though the site admins may be too busy with a major European war to enforce it.\r\n\r\nOur policy is that the initial support request should agree that\r\n>- [x] I've checked that none of provided URLs violate any copyrights\r\n\r\nPersonally I can't see that that is a useful or usually truthful (or grammatically correct) statement. Normally I like to see a DMCA policy since that is the sort of stuff that pleases people who care about copyright. This site doesn't mention that but it does say that it will remove content on receipt of a complaint supported by evidence of infringement of third-party rights. \r\n\r\nWhat we want is to make yt-dl no more liable for copyright infringement than Firefox, Chrome, Safari, or Lynx; in particular:\r\n1. to avoid anything that makes yt-dl seem like a sort of Napster, solely dedicated to processing content whose providers have no right to distribute it\r\n2. to avoid including URLs of such content in test cases (such as those that had to be removed in 2021).\nThanks.\r\n>[x] Covered the code with tests (note that PRs without tests will be REJECTED)\r\n\r\nNot so much, really. At a minimum there should be an `only_matching` test; ideally an actual test video that can be downloaded (its first 10kB, anyway) and whose metadata does not indicate \"pirated\" material.\r\n\r\nIf there are tests I can enable the CI test ...\r\n\nOK, Thanks a lot for the clearly explication. I add the \"only_matching\" test.\nSuperseded by #32725.", "created_at": "2024-02-21T12:27:15Z"}
{"repo": "ytdl-org/youtube-dl", "pull_number": 32710, "instance_id": "ytdl-org__youtube-dl-32710", "issue_numbers": ["26218", "32701"], "base_commit": "dc512e3a8a26a8e3fc7f1f67e5ee5e7699db8659", "patch": "diff --git a/youtube_dl/downloader/dash.py b/youtube_dl/downloader/dash.py\nindex 2800d42609e..f3c058879fb 100644\n--- a/youtube_dl/downloader/dash.py\n+++ b/youtube_dl/downloader/dash.py\n@@ -35,6 +35,7 @@ def real_download(self, filename, info_dict):\n         for frag_index, fragment in enumerate(fragments, 1):\n             if frag_index <= ctx['fragment_index']:\n                 continue\n+            success = False\n             # In DASH, the first segment contains necessary headers to\n             # generate a valid MP4 file, so always abort for the first segment\n             fatal = frag_index == 1 or not skip_unavailable_fragments\n@@ -42,10 +43,14 @@ def real_download(self, filename, info_dict):\n             if not fragment_url:\n                 assert fragment_base_url\n                 fragment_url = urljoin(fragment_base_url, fragment['path'])\n-            success = False\n+            headers = info_dict.get('http_headers')\n+            fragment_range = fragment.get('range')\n+            if fragment_range:\n+                headers = headers.copy() if headers else {}\n+                headers['Range'] = 'bytes=%s' % (fragment_range,)\n             for count in itertools.count():\n                 try:\n-                    success, frag_content = self._download_fragment(ctx, fragment_url, info_dict)\n+                    success, frag_content = self._download_fragment(ctx, fragment_url, info_dict, headers)\n                     if not success:\n                         return False\n                     self._append_fragment(ctx, frag_content)\ndiff --git a/youtube_dl/extractor/common.py b/youtube_dl/extractor/common.py\nindex d33557135b4..0e5dfd8fa99 100644\n--- a/youtube_dl/extractor/common.py\n+++ b/youtube_dl/extractor/common.py\n@@ -2,6 +2,7 @@\n from __future__ import unicode_literals\n \n import base64\n+import collections\n import datetime\n import functools\n import hashlib\n@@ -58,6 +59,7 @@\n     GeoRestrictedError,\n     GeoUtils,\n     int_or_none,\n+    join_nonempty,\n     js_to_json,\n     JSON_LD_RE,\n     mimetype2ext,\n@@ -74,6 +76,7 @@\n     str_or_none,\n     str_to_int,\n     strip_or_none,\n+    T,\n     traverse_obj,\n     try_get,\n     unescapeHTML,\n@@ -180,6 +183,8 @@ class InfoExtractor(object):\n                                             fragment_base_url\n                                  * \"duration\" (optional, int or float)\n                                  * \"filesize\" (optional, int)\n+                                 * \"range\" (optional, str of the form \"start-end\"\n+                                            to use in HTTP Range header)\n                     * preference Order number of this format. If this field is\n                                  present and not None, the formats get sorted\n                                  by this field, regardless of all other values.\n@@ -1751,6 +1756,12 @@ def _m3u8_meta_format(self, m3u8_url, ext=None, preference=None, m3u8_id=None):\n             'format_note': 'Quality selection URL',\n         }\n \n+    def _report_ignoring_subs(self, name):\n+        self.report_warning(bug_reports_message(\n+            'Ignoring subtitle tracks found in the {0} manifest; '\n+            'if any subtitle tracks are missing,'.format(name)\n+        ), only_once=True)\n+\n     def _extract_m3u8_formats(self, m3u8_url, video_id, ext=None,\n                               entry_protocol='m3u8', preference=None,\n                               m3u8_id=None, note=None, errnote=None,\n@@ -2191,23 +2202,46 @@ def _parse_xspf(self, xspf_doc, playlist_id, xspf_url=None, xspf_base_url=None):\n             })\n         return entries\n \n-    def _extract_mpd_formats(self, mpd_url, video_id, mpd_id=None, note=None, errnote=None, fatal=True, data=None, headers={}, query={}):\n+    def _extract_mpd_formats(self, *args, **kwargs):\n+        fmts, subs = self._extract_mpd_formats_and_subtitles(*args, **kwargs)\n+        if subs:\n+            self._report_ignoring_subs('DASH')\n+        return fmts\n+\n+    def _extract_mpd_formats_and_subtitles(\n+            self, mpd_url, video_id, mpd_id=None, note=None, errnote=None,\n+            fatal=True, data=None, headers=None, query=None):\n+\n+        # TODO: or not? param not yet implemented\n+        if self.get_param('ignore_no_formats_error'):\n+            fatal = False\n+\n         res = self._download_xml_handle(\n             mpd_url, video_id,\n-            note=note or 'Downloading MPD manifest',\n-            errnote=errnote or 'Failed to download MPD manifest',\n-            fatal=fatal, data=data, headers=headers, query=query)\n+            note='Downloading MPD manifest' if note is None else note,\n+            errnote='Failed to download MPD manifest' if errnote is None else errnote,\n+            fatal=fatal, data=data, headers=headers or {}, query=query or {})\n         if res is False:\n-            return []\n+            return [], {}\n         mpd_doc, urlh = res\n         if mpd_doc is None:\n-            return []\n-        mpd_base_url = base_url(urlh.geturl())\n+            return [], {}\n+\n+        # We could have been redirected to a new url when we retrieved our mpd file.\n+        mpd_url = urlh.geturl()\n+        mpd_base_url = base_url(mpd_url)\n \n-        return self._parse_mpd_formats(\n+        return self._parse_mpd_formats_and_subtitles(\n             mpd_doc, mpd_id, mpd_base_url, mpd_url)\n \n-    def _parse_mpd_formats(self, mpd_doc, mpd_id=None, mpd_base_url='', mpd_url=None):\n+    def _parse_mpd_formats(self, *args, **kwargs):\n+        fmts, subs = self._parse_mpd_formats_and_subtitles(*args, **kwargs)\n+        if subs:\n+            self._report_ignoring_subs('DASH')\n+        return fmts\n+\n+    def _parse_mpd_formats_and_subtitles(\n+            self, mpd_doc, mpd_id=None, mpd_base_url='', mpd_url=None):\n         \"\"\"\n         Parse formats from MPD manifest.\n         References:\n@@ -2215,8 +2249,10 @@ def _parse_mpd_formats(self, mpd_doc, mpd_id=None, mpd_base_url='', mpd_url=None\n             http://standards.iso.org/ittf/PubliclyAvailableStandards/c065274_ISO_IEC_23009-1_2014.zip\n          2. https://en.wikipedia.org/wiki/Dynamic_Adaptive_Streaming_over_HTTP\n         \"\"\"\n-        if mpd_doc.get('type') == 'dynamic':\n-            return []\n+        # TODO: param not yet implemented: default like previous yt-dl logic\n+        if not self.get_param('dynamic_mpd', False):\n+            if mpd_doc.get('type') == 'dynamic':\n+                return [], {}\n \n         namespace = self._search_regex(r'(?i)^{([^}]+)?}MPD$', mpd_doc.tag, 'namespace', default=None)\n \n@@ -2226,8 +2262,24 @@ def _add_ns(path):\n         def is_drm_protected(element):\n             return element.find(_add_ns('ContentProtection')) is not None\n \n+        from ..utils import YoutubeDLHandler\n+        fix_path = YoutubeDLHandler._fix_path\n+\n+        def resolve_base_url(element, parent_base_url=None):\n+            # TODO: use native XML traversal when ready\n+            b_url = traverse_obj(element, (\n+                T(lambda e: e.find(_add_ns('BaseURL')).text)))\n+            if parent_base_url and b_url:\n+                if not parent_base_url[-1] in ('/', ':'):\n+                    parent_base_url += '/'\n+                b_url = compat_urlparse.urljoin(parent_base_url, b_url)\n+            if b_url:\n+                b_url = fix_path(b_url)\n+            return b_url or parent_base_url\n+\n         def extract_multisegment_info(element, ms_parent_info):\n             ms_info = ms_parent_info.copy()\n+            base_url = ms_info['base_url'] = resolve_base_url(element, ms_info.get('base_url'))\n \n             # As per [1, 5.3.9.2.2] SegmentList and SegmentTemplate share some\n             # common attributes and elements.  We will only extract relevant\n@@ -2261,15 +2313,27 @@ def extract_common(source):\n             def extract_Initialization(source):\n                 initialization = source.find(_add_ns('Initialization'))\n                 if initialization is not None:\n-                    ms_info['initialization_url'] = initialization.attrib['sourceURL']\n+                    ms_info['initialization_url'] = initialization.get('sourceURL') or base_url\n+                    initialization_url_range = initialization.get('range')\n+                    if initialization_url_range:\n+                        ms_info['initialization_url_range'] = initialization_url_range\n \n             segment_list = element.find(_add_ns('SegmentList'))\n             if segment_list is not None:\n                 extract_common(segment_list)\n                 extract_Initialization(segment_list)\n                 segment_urls_e = segment_list.findall(_add_ns('SegmentURL'))\n-                if segment_urls_e:\n-                    ms_info['segment_urls'] = [segment.attrib['media'] for segment in segment_urls_e]\n+                segment_urls = traverse_obj(segment_urls_e, (\n+                    Ellipsis, T(lambda e: e.attrib), 'media'))\n+                if segment_urls:\n+                    ms_info['segment_urls'] = segment_urls\n+                segment_urls_range = traverse_obj(segment_urls_e, (\n+                    Ellipsis, T(lambda e: e.attrib), 'mediaRange',\n+                    T(lambda r: re.findall(r'^\\d+-\\d+$', r)), 0))\n+                if segment_urls_range:\n+                    ms_info['segment_urls_range'] = segment_urls_range\n+                    if not segment_urls:\n+                        ms_info['segment_urls'] = [base_url for _ in segment_urls_range]\n             else:\n                 segment_template = element.find(_add_ns('SegmentTemplate'))\n                 if segment_template is not None:\n@@ -2285,17 +2349,20 @@ def extract_Initialization(source):\n             return ms_info\n \n         mpd_duration = parse_duration(mpd_doc.get('mediaPresentationDuration'))\n-        formats = []\n+        formats, subtitles = [], {}\n+        stream_numbers = collections.defaultdict(int)\n+        mpd_base_url = resolve_base_url(mpd_doc, mpd_base_url or mpd_url)\n         for period in mpd_doc.findall(_add_ns('Period')):\n             period_duration = parse_duration(period.get('duration')) or mpd_duration\n             period_ms_info = extract_multisegment_info(period, {\n                 'start_number': 1,\n                 'timescale': 1,\n+                'base_url': mpd_base_url,\n             })\n             for adaptation_set in period.findall(_add_ns('AdaptationSet')):\n                 if is_drm_protected(adaptation_set):\n                     continue\n-                adaption_set_ms_info = extract_multisegment_info(adaptation_set, period_ms_info)\n+                adaptation_set_ms_info = extract_multisegment_info(adaptation_set, period_ms_info)\n                 for representation in adaptation_set.findall(_add_ns('Representation')):\n                     if is_drm_protected(representation):\n                         continue\n@@ -2303,27 +2370,35 @@ def extract_Initialization(source):\n                     representation_attrib.update(representation.attrib)\n                     # According to [1, 5.3.7.2, Table 9, page 41], @mimeType is mandatory\n                     mime_type = representation_attrib['mimeType']\n-                    content_type = mime_type.split('/')[0]\n-                    if content_type == 'text':\n-                        # TODO implement WebVTT downloading\n-                        pass\n-                    elif content_type in ('video', 'audio'):\n-                        base_url = ''\n-                        for element in (representation, adaptation_set, period, mpd_doc):\n-                            base_url_e = element.find(_add_ns('BaseURL'))\n-                            if base_url_e is not None:\n-                                base_url = base_url_e.text + base_url\n-                                if re.match(r'^https?://', base_url):\n-                                    break\n-                        if mpd_base_url and not re.match(r'^https?://', base_url):\n-                            if not mpd_base_url.endswith('/') and not base_url.startswith('/'):\n-                                mpd_base_url += '/'\n-                            base_url = mpd_base_url + base_url\n-                        representation_id = representation_attrib.get('id')\n-                        lang = representation_attrib.get('lang')\n-                        url_el = representation.find(_add_ns('BaseURL'))\n-                        filesize = int_or_none(url_el.attrib.get('{http://youtube.com/yt/2012/10/10}contentLength') if url_el is not None else None)\n-                        bandwidth = int_or_none(representation_attrib.get('bandwidth'))\n+                    content_type = representation_attrib.get('contentType') or mime_type.split('/')[0]\n+                    codec_str = representation_attrib.get('codecs', '')\n+                    # Some kind of binary subtitle found in some youtube livestreams\n+                    if mime_type == 'application/x-rawcc':\n+                        codecs = {'scodec': codec_str}\n+                    else:\n+                        codecs = parse_codecs(codec_str)\n+                    if content_type not in ('video', 'audio', 'text'):\n+                        if mime_type == 'image/jpeg':\n+                            content_type = mime_type\n+                        elif codecs.get('vcodec', 'none') != 'none':\n+                            content_type = 'video'\n+                        elif codecs.get('acodec', 'none') != 'none':\n+                            content_type = 'audio'\n+                        elif codecs.get('scodec', 'none') != 'none':\n+                            content_type = 'text'\n+                        elif mimetype2ext(mime_type) in ('tt', 'dfxp', 'ttml', 'xml', 'json'):\n+                            content_type = 'text'\n+                        else:\n+                            self.report_warning('Unknown MIME type %s in DASH manifest' % mime_type)\n+                            continue\n+\n+                    representation_id = representation_attrib.get('id')\n+                    lang = representation_attrib.get('lang')\n+                    url_el = representation.find(_add_ns('BaseURL'))\n+                    filesize = int_or_none(url_el.get('{http://youtube.com/yt/2012/10/10}contentLength') if url_el is not None else None)\n+                    bandwidth = int_or_none(representation_attrib.get('bandwidth'))\n+                    format_id = join_nonempty(representation_id or content_type, mpd_id)\n+                    if content_type in ('video', 'audio'):\n                         f = {\n                             'format_id': '%s-%s' % (mpd_id, representation_id) if mpd_id else representation_id,\n                             'manifest_url': mpd_url,\n@@ -2338,104 +2413,130 @@ def extract_Initialization(source):\n                             'filesize': filesize,\n                             'container': mimetype2ext(mime_type) + '_dash',\n                         }\n-                        f.update(parse_codecs(representation_attrib.get('codecs')))\n-                        representation_ms_info = extract_multisegment_info(representation, adaption_set_ms_info)\n-\n-                        def prepare_template(template_name, identifiers):\n-                            tmpl = representation_ms_info[template_name]\n-                            # First of, % characters outside $...$ templates\n-                            # must be escaped by doubling for proper processing\n-                            # by % operator string formatting used further (see\n-                            # https://github.com/ytdl-org/youtube-dl/issues/16867).\n-                            t = ''\n-                            in_template = False\n-                            for c in tmpl:\n+                        f.update(codecs)\n+                    elif content_type == 'text':\n+                        f = {\n+                            'ext': mimetype2ext(mime_type),\n+                            'manifest_url': mpd_url,\n+                            'filesize': filesize,\n+                        }\n+                    elif content_type == 'image/jpeg':\n+                        # See test case in VikiIE\n+                        # https://www.viki.com/videos/1175236v-choosing-spouse-by-lottery-episode-1\n+                        f = {\n+                            'format_id': format_id,\n+                            'ext': 'mhtml',\n+                            'manifest_url': mpd_url,\n+                            'format_note': 'DASH storyboards (jpeg)',\n+                            'acodec': 'none',\n+                            'vcodec': 'none',\n+                        }\n+                    if is_drm_protected(adaptation_set) or is_drm_protected(representation):\n+                        f['has_drm'] = True\n+                    representation_ms_info = extract_multisegment_info(representation, adaptation_set_ms_info)\n+\n+                    def prepare_template(template_name, identifiers):\n+                        tmpl = representation_ms_info[template_name]\n+                        # First of, % characters outside $...$ templates\n+                        # must be escaped by doubling for proper processing\n+                        # by % operator string formatting used further (see\n+                        # https://github.com/ytdl-org/youtube-dl/issues/16867).\n+                        t = ''\n+                        in_template = False\n+                        for c in tmpl:\n+                            t += c\n+                            if c == '$':\n+                                in_template = not in_template\n+                            elif c == '%' and not in_template:\n                                 t += c\n-                                if c == '$':\n-                                    in_template = not in_template\n-                                elif c == '%' and not in_template:\n-                                    t += c\n-                            # Next, $...$ templates are translated to their\n-                            # %(...) counterparts to be used with % operator\n-                            t = t.replace('$RepresentationID$', representation_id)\n-                            t = re.sub(r'\\$(%s)\\$' % '|'.join(identifiers), r'%(\\1)d', t)\n-                            t = re.sub(r'\\$(%s)%%([^$]+)\\$' % '|'.join(identifiers), r'%(\\1)\\2', t)\n-                            t.replace('$$', '$')\n-                            return t\n-\n-                        # @initialization is a regular template like @media one\n-                        # so it should be handled just the same way (see\n-                        # https://github.com/ytdl-org/youtube-dl/issues/11605)\n-                        if 'initialization' in representation_ms_info:\n-                            initialization_template = prepare_template(\n-                                'initialization',\n-                                # As per [1, 5.3.9.4.2, Table 15, page 54] $Number$ and\n-                                # $Time$ shall not be included for @initialization thus\n-                                # only $Bandwidth$ remains\n-                                ('Bandwidth', ))\n-                            representation_ms_info['initialization_url'] = initialization_template % {\n-                                'Bandwidth': bandwidth,\n-                            }\n-\n-                        def location_key(location):\n-                            return 'url' if re.match(r'^https?://', location) else 'path'\n-\n-                        if 'segment_urls' not in representation_ms_info and 'media' in representation_ms_info:\n-\n-                            media_template = prepare_template('media', ('Number', 'Bandwidth', 'Time'))\n-                            media_location_key = location_key(media_template)\n-\n-                            # As per [1, 5.3.9.4.4, Table 16, page 55] $Number$ and $Time$\n-                            # can't be used at the same time\n-                            if '%(Number' in media_template and 's' not in representation_ms_info:\n-                                segment_duration = None\n-                                if 'total_number' not in representation_ms_info and 'segment_duration' in representation_ms_info:\n-                                    segment_duration = float_or_none(representation_ms_info['segment_duration'], representation_ms_info['timescale'])\n-                                    representation_ms_info['total_number'] = int(math.ceil(float(period_duration) / segment_duration))\n-                                representation_ms_info['fragments'] = [{\n-                                    media_location_key: media_template % {\n-                                        'Number': segment_number,\n-                                        'Bandwidth': bandwidth,\n-                                    },\n-                                    'duration': segment_duration,\n-                                } for segment_number in range(\n-                                    representation_ms_info['start_number'],\n-                                    representation_ms_info['total_number'] + representation_ms_info['start_number'])]\n-                            else:\n-                                # $Number*$ or $Time$ in media template with S list available\n-                                # Example $Number*$: http://www.svtplay.se/klipp/9023742/stopptid-om-bjorn-borg\n-                                # Example $Time$: https://play.arkena.com/embed/avp/v2/player/media/b41dda37-d8e7-4d3f-b1b5-9a9db578bdfe/1/129411\n-                                representation_ms_info['fragments'] = []\n-                                segment_time = 0\n-                                segment_d = None\n-                                segment_number = representation_ms_info['start_number']\n-\n-                                def add_segment_url():\n-                                    segment_url = media_template % {\n-                                        'Time': segment_time,\n-                                        'Bandwidth': bandwidth,\n-                                        'Number': segment_number,\n-                                    }\n-                                    representation_ms_info['fragments'].append({\n-                                        media_location_key: segment_url,\n-                                        'duration': float_or_none(segment_d, representation_ms_info['timescale']),\n-                                    })\n+                        # Next, $...$ templates are translated to their\n+                        # %(...) counterparts to be used with % operator\n+                        t = t.replace('$RepresentationID$', representation_id)\n+                        t = re.sub(r'\\$(%s)\\$' % '|'.join(identifiers), r'%(\\1)d', t)\n+                        t = re.sub(r'\\$(%s)%%([^$]+)\\$' % '|'.join(identifiers), r'%(\\1)\\2', t)\n+                        t.replace('$$', '$')\n+                        return t\n+\n+                    # @initialization is a regular template like @media one\n+                    # so it should be handled just the same way (see\n+                    # https://github.com/ytdl-org/youtube-dl/issues/11605)\n+                    if 'initialization' in representation_ms_info:\n+                        initialization_template = prepare_template(\n+                            'initialization',\n+                            # As per [1, 5.3.9.4.2, Table 15, page 54] $Number$ and\n+                            # $Time$ shall not be included for @initialization thus\n+                            # only $Bandwidth$ remains\n+                            ('Bandwidth', ))\n+                        representation_ms_info['initialization_url'] = initialization_template % {\n+                            'Bandwidth': bandwidth,\n+                        }\n \n-                                for num, s in enumerate(representation_ms_info['s']):\n-                                    segment_time = s.get('t') or segment_time\n-                                    segment_d = s['d']\n+                    def location_key(location):\n+                        return 'url' if re.match(r'^https?://', location) else 'path'\n+\n+                    def calc_segment_duration():\n+                        return float_or_none(\n+                            representation_ms_info['segment_duration'],\n+                            representation_ms_info['timescale']) if 'segment_duration' in representation_ms_info else None\n+\n+                    if 'segment_urls' not in representation_ms_info and 'media' in representation_ms_info:\n+\n+                        media_template = prepare_template('media', ('Number', 'Bandwidth', 'Time'))\n+                        media_location_key = location_key(media_template)\n+\n+                        # As per [1, 5.3.9.4.4, Table 16, page 55] $Number$ and $Time$\n+                        # can't be used at the same time\n+                        if '%(Number' in media_template and 's' not in representation_ms_info:\n+                            segment_duration = None\n+                            if 'total_number' not in representation_ms_info and 'segment_duration' in representation_ms_info:\n+                                segment_duration = float_or_none(representation_ms_info['segment_duration'], representation_ms_info['timescale'])\n+                                representation_ms_info['total_number'] = int(math.ceil(\n+                                    float_or_none(period_duration, segment_duration, default=0)))\n+                            representation_ms_info['fragments'] = [{\n+                                media_location_key: media_template % {\n+                                    'Number': segment_number,\n+                                    'Bandwidth': bandwidth,\n+                                },\n+                                'duration': segment_duration,\n+                            } for segment_number in range(\n+                                representation_ms_info['start_number'],\n+                                representation_ms_info['total_number'] + representation_ms_info['start_number'])]\n+                        else:\n+                            # $Number*$ or $Time$ in media template with S list available\n+                            # Example $Number*$: http://www.svtplay.se/klipp/9023742/stopptid-om-bjorn-borg\n+                            # Example $Time$: https://play.arkena.com/embed/avp/v2/player/media/b41dda37-d8e7-4d3f-b1b5-9a9db578bdfe/1/129411\n+                            representation_ms_info['fragments'] = []\n+                            segment_time = 0\n+                            segment_d = None\n+                            segment_number = representation_ms_info['start_number']\n+\n+                            def add_segment_url():\n+                                segment_url = media_template % {\n+                                    'Time': segment_time,\n+                                    'Bandwidth': bandwidth,\n+                                    'Number': segment_number,\n+                                }\n+                                representation_ms_info['fragments'].append({\n+                                    media_location_key: segment_url,\n+                                    'duration': float_or_none(segment_d, representation_ms_info['timescale']),\n+                                })\n+\n+                            for num, s in enumerate(representation_ms_info['s']):\n+                                segment_time = s.get('t') or segment_time\n+                                segment_d = s['d']\n+                                add_segment_url()\n+                                segment_number += 1\n+                                for r in range(s.get('r', 0)):\n+                                    segment_time += segment_d\n                                     add_segment_url()\n                                     segment_number += 1\n-                                    for r in range(s.get('r', 0)):\n-                                        segment_time += segment_d\n-                                        add_segment_url()\n-                                        segment_number += 1\n-                                    segment_time += segment_d\n-                        elif 'segment_urls' in representation_ms_info and 's' in representation_ms_info:\n+                                segment_time += segment_d\n+                    elif 'segment_urls' in representation_ms_info:\n+                        fragments = []\n+                        if 's' in representation_ms_info:\n                             # No media template\n                             # Example: https://www.youtube.com/watch?v=iXZV5uAYMJI\n                             # or any YouTube dashsegments video\n-                            fragments = []\n                             segment_index = 0\n                             timescale = representation_ms_info['timescale']\n                             for s in representation_ms_info['s']:\n@@ -2447,48 +2548,78 @@ def add_segment_url():\n                                         'duration': duration,\n                                     })\n                                     segment_index += 1\n-                            representation_ms_info['fragments'] = fragments\n-                        elif 'segment_urls' in representation_ms_info:\n+                        elif 'segment_urls_range' in representation_ms_info:\n+                            # Segment URLs with mediaRange\n+                            # Example: https://kinescope.io/200615537/master.mpd\n+                            # https://github.com/ytdl-org/youtube-dl/issues/30235\n+                            # or any mpd generated with Bento4 `mp4dash --no-split --use-segment-list`\n+                            segment_duration = calc_segment_duration()\n+                            for segment_url, segment_url_range in zip(\n+                                    representation_ms_info['segment_urls'], representation_ms_info['segment_urls_range']):\n+                                fragments.append({\n+                                    location_key(segment_url): segment_url,\n+                                    'range': segment_url_range,\n+                                    'duration': segment_duration,\n+                                })\n+                        else:\n                             # Segment URLs with no SegmentTimeline\n                             # Example: https://www.seznam.cz/zpravy/clanek/cesko-zasahne-vitr-o-sile-vichrice-muze-byt-i-zivotu-nebezpecny-39091\n                             # https://github.com/ytdl-org/youtube-dl/pull/14844\n-                            fragments = []\n-                            segment_duration = float_or_none(\n-                                representation_ms_info['segment_duration'],\n-                                representation_ms_info['timescale']) if 'segment_duration' in representation_ms_info else None\n+                            segment_duration = calc_segment_duration()\n                             for segment_url in representation_ms_info['segment_urls']:\n-                                fragment = {\n+                                fragments.append({\n                                     location_key(segment_url): segment_url,\n-                                }\n-                                if segment_duration:\n-                                    fragment['duration'] = segment_duration\n-                                fragments.append(fragment)\n-                            representation_ms_info['fragments'] = fragments\n-                        # If there is a fragments key available then we correctly recognized fragmented media.\n-                        # Otherwise we will assume unfragmented media with direct access. Technically, such\n-                        # assumption is not necessarily correct since we may simply have no support for\n-                        # some forms of fragmented media renditions yet, but for now we'll use this fallback.\n-                        if 'fragments' in representation_ms_info:\n-                            f.update({\n-                                # NB: mpd_url may be empty when MPD manifest is parsed from a string\n-                                'url': mpd_url or base_url,\n-                                'fragment_base_url': base_url,\n-                                'fragments': [],\n-                                'protocol': 'http_dash_segments',\n+                                    'duration': segment_duration,\n+                                })\n+                        representation_ms_info['fragments'] = fragments\n+\n+                    # If there is a fragments key available then we correctly recognized fragmented media.\n+                    # Otherwise we will assume unfragmented media with direct access. Technically, such\n+                    # assumption is not necessarily correct since we may simply have no support for\n+                    # some forms of fragmented media renditions yet, but for now we'll use this fallback.\n+                    if 'fragments' in representation_ms_info:\n+                        base_url = representation_ms_info['base_url']\n+                        f.update({\n+                            # NB: mpd_url may be empty when MPD manifest is parsed from a string\n+                            'url': mpd_url or base_url,\n+                            'fragment_base_url': base_url,\n+                            'fragments': [],\n+                            'protocol': 'http_dash_segments',\n+                        })\n+                        if 'initialization_url' in representation_ms_info and 'initialization_url_range' in representation_ms_info:\n+                            # Initialization URL with range (accompanied by Segment URLs with mediaRange above)\n+                            # https://github.com/ytdl-org/youtube-dl/issues/30235\n+                            initialization_url = representation_ms_info['initialization_url']\n+                            f['fragments'].append({\n+                                location_key(initialization_url): initialization_url,\n+                                'range': representation_ms_info['initialization_url_range'],\n                             })\n-                            if 'initialization_url' in representation_ms_info:\n-                                initialization_url = representation_ms_info['initialization_url']\n-                                if not f.get('url'):\n-                                    f['url'] = initialization_url\n-                                f['fragments'].append({location_key(initialization_url): initialization_url})\n-                            f['fragments'].extend(representation_ms_info['fragments'])\n-                        else:\n-                            # Assuming direct URL to unfragmented media.\n-                            f['url'] = base_url\n-                        formats.append(f)\n+                        elif 'initialization_url' in representation_ms_info:\n+                            initialization_url = representation_ms_info['initialization_url']\n+                            if not f.get('url'):\n+                                f['url'] = initialization_url\n+                            f['fragments'].append({location_key(initialization_url): initialization_url})\n+                        elif 'initialization_url_range' in representation_ms_info:\n+                            # no Initialization URL but range (accompanied by no Segment URLs but mediaRange above)\n+                            # https://github.com/ytdl-org/youtube-dl/issues/27575\n+                            f['fragments'].append({\n+                                location_key(base_url): base_url,\n+                                'range': representation_ms_info['initialization_url_range'],\n+                            })\n+                        f['fragments'].extend(representation_ms_info['fragments'])\n+                        if not period_duration:\n+                            period_duration = sum(traverse_obj(representation_ms_info, (\n+                                'fragments', Ellipsis, 'duration', T(float_or_none))))\n                     else:\n-                        self.report_warning('Unknown MIME type %s in DASH manifest' % mime_type)\n-        return formats\n+                        # Assuming direct URL to unfragmented media.\n+                        f['url'] = representation_ms_info['base_url']\n+                    if content_type in ('video', 'audio', 'image/jpeg'):\n+                        f['manifest_stream_number'] = stream_numbers[f['url']]\n+                        stream_numbers[f['url']] += 1\n+                        formats.append(f)\n+                    elif content_type == 'text':\n+                        subtitles.setdefault(lang or 'und', []).append(f)\n+        return formats, subtitles\n \n     def _extract_ism_formats(self, ism_url, video_id, ism_id=None, note=None, errnote=None, fatal=True, data=None, headers={}, query={}):\n         res = self._download_xml_handle(\ndiff --git a/youtube_dl/extractor/vbox7.py b/youtube_dl/extractor/vbox7.py\nindex 8152acefd09..d114ecb0740 100644\n--- a/youtube_dl/extractor/vbox7.py\n+++ b/youtube_dl/extractor/vbox7.py\n@@ -2,9 +2,20 @@\n from __future__ import unicode_literals\n \n import re\n+import time\n \n from .common import InfoExtractor\n-from ..utils import ExtractorError\n+from ..compat import compat_kwargs\n+from ..utils import (\n+    determine_ext,\n+    ExtractorError,\n+    float_or_none,\n+    merge_dicts,\n+    T,\n+    traverse_obj,\n+    txt_or_none,\n+    url_or_none,\n+)\n \n \n class Vbox7IE(InfoExtractor):\n@@ -20,10 +31,12 @@ class Vbox7IE(InfoExtractor):\n                         )\n                         (?P<id>[\\da-fA-F]+)\n                     '''\n+    _EMBED_REGEX = [r'<iframe[^>]+src=(?P<q>[\"\\'])(?P<url>(?:https?:)?//vbox7\\.com/emb/external\\.php.+?)(?P=q)']\n     _GEO_COUNTRIES = ['BG']\n+    _GEO_BYPASS = False\n     _TESTS = [{\n-        'url': 'http://vbox7.com/play:0946fff23c',\n-        'md5': 'a60f9ab3a3a2f013ef9a967d5f7be5bf',\n+        'url': 'https://vbox7.com/play:0946fff23c',\n+        'md5': '50ca1f78345a9c15391af47d8062d074',\n         'info_dict': {\n             'id': '0946fff23c',\n             'ext': 'mp4',\n@@ -34,18 +47,21 @@ class Vbox7IE(InfoExtractor):\n             'upload_date': '20160812',\n             'uploader': 'zdraveibulgaria',\n         },\n-        'params': {\n-            'proxy': '127.0.0.1:8118',\n-        },\n+        'expected_warnings': [\n+            'Unable to download webpage',\n+        ],\n     }, {\n         'url': 'http://vbox7.com/play:249bb972c2',\n-        'md5': '99f65c0c9ef9b682b97313e052734c3f',\n+        'md5': 'aaf19465e37ec0b30b918df83ec32c50',\n         'info_dict': {\n             'id': '249bb972c2',\n             'ext': 'mp4',\n             'title': '\u0421\u043c\u044f\u0445! \u0427\u0443\u0434\u043e - \u0447\u0438\u0441\u0442 \u0437\u0430 \u0441\u0435\u043a\u0443\u043d\u0434\u0438 - \u0421\u043a\u0440\u0438\u0442\u0430 \u043a\u0430\u043c\u0435\u0440\u0430',\n+            'description': '\u0421\u043c\u044f\u0445! \u0427\u0443\u0434\u043e - \u0447\u0438\u0441\u0442 \u0437\u0430 \u0441\u0435\u043a\u0443\u043d\u0434\u0438 - \u0421\u043a\u0440\u0438\u0442\u0430 \u043a\u0430\u043c\u0435\u0440\u0430',\n+            'timestamp': 1360215023,\n+            'upload_date': '20130207',\n+            'uploader': 'svideteliat_ot_varshava',\n         },\n-        'skip': 'georestricted',\n     }, {\n         'url': 'http://vbox7.com/emb/external.php?vid=a240d20f9c&autoplay=1',\n         'only_matching': True,\n@@ -54,52 +70,109 @@ class Vbox7IE(InfoExtractor):\n         'only_matching': True,\n     }]\n \n-    @staticmethod\n-    def _extract_url(webpage):\n-        mobj = re.search(\n-            r'<iframe[^>]+src=(?P<q>[\"\\'])(?P<url>(?:https?:)?//vbox7\\.com/emb/external\\.php.+?)(?P=q)',\n-            webpage)\n+    @classmethod\n+    def _extract_url(cls, webpage):\n+        mobj = re.search(cls._EMBED_REGEX[0], webpage)\n         if mobj:\n             return mobj.group('url')\n \n+    # transform_source=None, fatal=True\n+    def _parse_json(self, json_string, video_id, *args, **kwargs):\n+        if '\"@context\"' in json_string[:30]:\n+            # this is ld+json, or that's the way to bet\n+            transform_source = args[0] if len(args) > 0 else kwargs.get('transform_source')\n+            if not transform_source:\n+\n+                def fix_chars(src):\n+                    # fix malformed ld+json: replace raw CRLFs with escaped LFs\n+                    return re.sub(\n+                        r'\"[^\"]+\"', lambda m: re.sub(r'\\r?\\n', r'\\\\n', m.group(0)), src)\n+\n+                if len(args) > 0:\n+                    args = (fix_chars,) + args[1:]\n+                else:\n+                    kwargs['transform_source'] = fix_chars\n+                    kwargs = compat_kwargs(kwargs)\n+\n+        return super(Vbox7IE, self)._parse_json(\n+            json_string, video_id, *args, **kwargs)\n+\n     def _real_extract(self, url):\n         video_id = self._match_id(url)\n+        url = 'https://vbox7.com/play:%s' % (video_id,)\n \n+        now = time.time()\n         response = self._download_json(\n-            'https://www.vbox7.com/ajax/video/nextvideo.php?vid=%s' % video_id,\n-            video_id)\n+            'https://www.vbox7.com/aj/player/item/options?vid=%s' % (video_id,),\n+            video_id, headers={'Referer': url})\n+        # estimate time to which possible `ago` member is relative\n+        now = now + 0.5 * (time.time() - now)\n \n         if 'error' in response:\n             raise ExtractorError(\n                 '%s said: %s' % (self.IE_NAME, response['error']), expected=True)\n \n-        video = response['options']\n-\n-        title = video['title']\n-        video_url = video['src']\n+        video_url = traverse_obj(response, ('options', 'src', T(url_or_none)))\n \n-        if '/na.mp4' in video_url:\n+        if '/na.mp4' in video_url or '':\n             self.raise_geo_restricted(countries=self._GEO_COUNTRIES)\n \n-        uploader = video.get('uploader')\n-\n-        webpage = self._download_webpage(\n-            'http://vbox7.com/play:%s' % video_id, video_id, fatal=None)\n-\n-        info = {}\n-\n-        if webpage:\n-            info = self._search_json_ld(\n-                webpage.replace('\"/*@context\"', '\"@context\"'), video_id,\n-                fatal=False)\n-\n-        info.update({\n+        ext = determine_ext(video_url)\n+        if ext == 'mpd':\n+            # In case MPD cannot be parsed, or anyway, get mp4 combined\n+            # formats usually provided to Safari, iOS, and old Windows\n+            try:\n+                formats, subtitles = self._extract_mpd_formats_and_subtitles(\n+                    video_url, video_id, 'dash', fatal=False)\n+            except KeyError:\n+                self.report_warning('Failed to parse MPD manifest')\n+                formats, subtitles = [], {}\n+\n+            video = response['options']\n+            resolutions = (1080, 720, 480, 240, 144)\n+            highest_res = traverse_obj(video, ('highestRes', T(int))) or resolutions[0]\n+            for res in traverse_obj(video, ('resolutions', lambda _, r: int(r) > 0)) or resolutions:\n+                if res > highest_res:\n+                    continue\n+                formats.append({\n+                    'url': video_url.replace('.mpd', '_%d.mp4' % res),\n+                    'format_id': '%dp' % res,\n+                    'height': res,\n+                })\n+            # if above formats are flaky, enable the line below\n+            # self._check_formats(formats, video_id)\n+        else:\n+            formats = [{\n+                'url': video_url,\n+            }]\n+            subtitles = {}\n+        self._sort_formats(formats)\n+\n+        webpage = self._download_webpage(url, video_id, fatal=False) or ''\n+\n+        info = self._search_json_ld(\n+            webpage.replace('\"/*@context\"', '\"@context\"'), video_id,\n+            fatal=False) if webpage else {}\n+\n+        if not info.get('title'):\n+            info['title'] = traverse_obj(response, (\n+                'options', 'title', T(txt_or_none))) or self._og_search_title(webpage)\n+\n+        def if_missing(k):\n+            return lambda x: None if k in info else x\n+\n+        info = merge_dicts(info, {\n             'id': video_id,\n-            'title': title,\n-            'url': video_url,\n-            'uploader': uploader,\n-            'thumbnail': self._proto_relative_url(\n+            'formats': formats,\n+            'subtitles': subtitles or None,\n+        }, info, traverse_obj(response, ('options', {\n+            'uploader': ('uploader', T(txt_or_none)),\n+            'timestamp': ('ago', T(if_missing('timestamp')), T(lambda t: int(round((now - t) / 60.0)) * 60)),\n+            'duration': ('duration', T(if_missing('duration')), T(float_or_none)),\n+        })))\n+        if 'thumbnail' not in info:\n+            info['thumbnail'] = self._proto_relative_url(\n                 info.get('thumbnail') or self._og_search_thumbnail(webpage),\n-                'http:'),\n-        })\n+                'https:'),\n+\n         return info\n", "test_patch": "diff --git a/test/test_InfoExtractor.py b/test/test_InfoExtractor.py\nindex 3f96645de02..d55d6ad5428 100644\n--- a/test/test_InfoExtractor.py\n+++ b/test/test_InfoExtractor.py\n@@ -993,7 +993,8 @@ def test_parse_mpd_formats(self):\n                     'tbr': 5997.485,\n                     'width': 1920,\n                     'height': 1080,\n-                }]\n+                }],\n+                {},\n             ), (\n                 # https://github.com/ytdl-org/youtube-dl/pull/14844\n                 'urls_only',\n@@ -1076,7 +1077,8 @@ def test_parse_mpd_formats(self):\n                     'tbr': 4400,\n                     'width': 1920,\n                     'height': 1080,\n-                }]\n+                }],\n+                {},\n             ), (\n                 # https://github.com/ytdl-org/youtube-dl/issues/20346\n                 # Media considered unfragmented even though it contains\n@@ -1122,18 +1124,185 @@ def test_parse_mpd_formats(self):\n                     'width': 360,\n                     'height': 360,\n                     'fps': 30,\n-                }]\n+                }],\n+                {},\n+            ), (\n+                # https://github.com/ytdl-org/youtube-dl/issues/30235\n+                # Bento4 generated test mpd\n+                # mp4dash --mpd-name=manifest.mpd --no-split --use-segment-list mediafiles\n+                'url_and_range',\n+                'http://unknown/manifest.mpd',  # mpd_url\n+                'http://unknown/',  # mpd_base_url\n+                [{\n+                    'manifest_url': 'http://unknown/manifest.mpd',\n+                    'fragment_base_url': 'http://unknown/',\n+                    'ext': 'm4a',\n+                    'format_id': 'audio-und-mp4a.40.2',\n+                    'format_note': 'DASH audio',\n+                    'container': 'm4a_dash',\n+                    'protocol': 'http_dash_segments',\n+                    'acodec': 'mp4a.40.2',\n+                    'vcodec': 'none',\n+                    'tbr': 98.808,\n+                }, {\n+                    'manifest_url': 'http://unknown/manifest.mpd',\n+                    'fragment_base_url': 'http://unknown/',\n+                    'ext': 'mp4',\n+                    'format_id': 'video-avc1',\n+                    'format_note': 'DASH video',\n+                    'container': 'mp4_dash',\n+                    'protocol': 'http_dash_segments',\n+                    'acodec': 'none',\n+                    'vcodec': 'avc1.4D401E',\n+                    'tbr': 699.597,\n+                    'width': 768,\n+                    'height': 432\n+                }],\n+                {},\n+            ), (\n+                # https://github.com/ytdl-org/youtube-dl/issues/27575\n+                # GPAC generated test mpd\n+                # MP4Box -dash 10000 -single-file -out manifest.mpd mediafiles\n+                'range_only',\n+                'http://unknown/manifest.mpd',  # mpd_url\n+                'http://unknown/',  # mpd_base_url\n+                [{\n+                    'manifest_url': 'http://unknown/manifest.mpd',\n+                    'fragment_base_url': 'http://unknown/audio_dashinit.mp4',\n+                    'ext': 'm4a',\n+                    'format_id': '2',\n+                    'format_note': 'DASH audio',\n+                    'container': 'm4a_dash',\n+                    'protocol': 'http_dash_segments',\n+                    'acodec': 'mp4a.40.2',\n+                    'vcodec': 'none',\n+                    'tbr': 98.096,\n+                }, {\n+                    'manifest_url': 'http://unknown/manifest.mpd',\n+                    'fragment_base_url': 'http://unknown/video_dashinit.mp4',\n+                    'ext': 'mp4',\n+                    'format_id': '1',\n+                    'format_note': 'DASH video',\n+                    'container': 'mp4_dash',\n+                    'protocol': 'http_dash_segments',\n+                    'acodec': 'none',\n+                    'vcodec': 'avc1.4D401E',\n+                    'tbr': 526.987,\n+                    'width': 768,\n+                    'height': 432\n+                }],\n+                {},\n+            ), (\n+                'subtitles',\n+                'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n+                'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/',\n+                [{\n+                    'format_id': 'audio=128001',\n+                    'manifest_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n+                    'ext': 'm4a',\n+                    'tbr': 128.001,\n+                    'asr': 48000,\n+                    'format_note': 'DASH audio',\n+                    'container': 'm4a_dash',\n+                    'vcodec': 'none',\n+                    'acodec': 'mp4a.40.2',\n+                    'url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n+                    'fragment_base_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/dash/',\n+                    'protocol': 'http_dash_segments',\n+                }, {\n+                    'format_id': 'video=100000',\n+                    'manifest_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n+                    'ext': 'mp4',\n+                    'width': 336,\n+                    'height': 144,\n+                    'tbr': 100,\n+                    'format_note': 'DASH video',\n+                    'container': 'mp4_dash',\n+                    'vcodec': 'avc1.4D401F',\n+                    'acodec': 'none',\n+                    'url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n+                    'fragment_base_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/dash/',\n+                    'protocol': 'http_dash_segments',\n+                }, {\n+                    'format_id': 'video=326000',\n+                    'manifest_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n+                    'ext': 'mp4',\n+                    'width': 562,\n+                    'height': 240,\n+                    'tbr': 326,\n+                    'format_note': 'DASH video',\n+                    'container': 'mp4_dash',\n+                    'vcodec': 'avc1.4D401F',\n+                    'acodec': 'none',\n+                    'url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n+                    'fragment_base_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/dash/',\n+                    'protocol': 'http_dash_segments',\n+                }, {\n+                    'format_id': 'video=698000',\n+                    'manifest_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n+                    'ext': 'mp4',\n+                    'width': 844,\n+                    'height': 360,\n+                    'tbr': 698,\n+                    'format_note': 'DASH video',\n+                    'container': 'mp4_dash',\n+                    'vcodec': 'avc1.4D401F',\n+                    'acodec': 'none',\n+                    'url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n+                    'fragment_base_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/dash/',\n+                    'protocol': 'http_dash_segments',\n+                }, {\n+                    'format_id': 'video=1493000',\n+                    'manifest_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n+                    'ext': 'mp4',\n+                    'width': 1126,\n+                    'height': 480,\n+                    'tbr': 1493,\n+                    'format_note': 'DASH video',\n+                    'container': 'mp4_dash',\n+                    'vcodec': 'avc1.4D401F',\n+                    'acodec': 'none',\n+                    'url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n+                    'fragment_base_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/dash/',\n+                    'protocol': 'http_dash_segments',\n+                }, {\n+                    'format_id': 'video=4482000',\n+                    'manifest_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n+                    'ext': 'mp4',\n+                    'width': 1688,\n+                    'height': 720,\n+                    'tbr': 4482,\n+                    'format_note': 'DASH video',\n+                    'container': 'mp4_dash',\n+                    'vcodec': 'avc1.4D401F',\n+                    'acodec': 'none',\n+                    'url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n+                    'fragment_base_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/dash/',\n+                    'protocol': 'http_dash_segments',\n+                }],\n+                {\n+                    'en': [\n+                        {\n+                            'ext': 'mp4',\n+                            'manifest_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n+                            'url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/manifest.mpd',\n+                            'fragment_base_url': 'https://sdn-global-streaming-cache-3qsdn.akamaized.net/stream/3144/files/17/07/672975/3144-kZT4LWMQw6Rh7Kpd.ism/dash/',\n+                            'protocol': 'http_dash_segments',\n+                        }\n+                    ]\n+                },\n             )\n         ]\n \n-        for mpd_file, mpd_url, mpd_base_url, expected_formats in _TEST_CASES:\n+        for mpd_file, mpd_url, mpd_base_url, expected_formats, expected_subtitles in _TEST_CASES:\n             with open('./test/testdata/mpd/%s.mpd' % mpd_file,\n                       mode='r', encoding='utf-8') as f:\n-                formats = self.ie._parse_mpd_formats(\n+                formats, subtitles = self.ie._parse_mpd_formats_and_subtitles(\n                     compat_etree_fromstring(f.read().encode('utf-8')),\n                     mpd_base_url=mpd_base_url, mpd_url=mpd_url)\n                 self.ie._sort_formats(formats)\n                 expect_value(self, formats, expected_formats, None)\n+                expect_value(self, subtitles, expected_subtitles, None)\n \n     def test_parse_f4m_formats(self):\n         _TEST_CASES = [\ndiff --git a/test/testdata/mpd/range_only.mpd b/test/testdata/mpd/range_only.mpd\nnew file mode 100644\nindex 00000000000..e0c2152d1a5\n--- /dev/null\n+++ b/test/testdata/mpd/range_only.mpd\n@@ -0,0 +1,35 @@\n+<?xml version=\"1.0\"?>\n+<!-- MPD file Generated with GPAC version 1.0.1-revrelease at 2021-11-27T20:53:11.690Z -->\n+<MPD xmlns=\"urn:mpeg:dash:schema:mpd:2011\" minBufferTime=\"PT1.500S\" type=\"static\" mediaPresentationDuration=\"PT0H0M30.196S\" maxSegmentDuration=\"PT0H0M10.027S\" profiles=\"urn:mpeg:dash:profile:full:2011\">\n+ <ProgramInformation moreInformationURL=\"http://gpac.io\">\n+  <Title>manifest.mpd generated by GPAC</Title>\n+ </ProgramInformation>\n+\n+ <Period duration=\"PT0H0M30.196S\">\n+  <AdaptationSet segmentAlignment=\"true\" maxWidth=\"768\" maxHeight=\"432\" maxFrameRate=\"30000/1001\" par=\"16:9\" lang=\"und\" startWithSAP=\"1\">\n+   <Representation id=\"1\" mimeType=\"video/mp4\" codecs=\"avc1.4D401E\" width=\"768\" height=\"432\" frameRate=\"30000/1001\" sar=\"1:1\" bandwidth=\"526987\">\n+    <BaseURL>video_dashinit.mp4</BaseURL>\n+    <SegmentList timescale=\"90000\" duration=\"900000\">\n+     <Initialization range=\"0-881\"/>\n+     <SegmentURL mediaRange=\"882-876094\" indexRange=\"882-925\"/>\n+     <SegmentURL mediaRange=\"876095-1466732\" indexRange=\"876095-876138\"/>\n+     <SegmentURL mediaRange=\"1466733-1953615\" indexRange=\"1466733-1466776\"/>\n+     <SegmentURL mediaRange=\"1953616-1994211\" indexRange=\"1953616-1953659\"/>\n+    </SegmentList>\n+   </Representation>\n+  </AdaptationSet>\n+  <AdaptationSet segmentAlignment=\"true\" lang=\"und\" startWithSAP=\"1\">\n+   <Representation id=\"2\" mimeType=\"audio/mp4\" codecs=\"mp4a.40.2\" audioSamplingRate=\"48000\" bandwidth=\"98096\">\n+    <AudioChannelConfiguration schemeIdUri=\"urn:mpeg:dash:23003:3:audio_channel_configuration:2011\" value=\"2\"/>\n+    <BaseURL>audio_dashinit.mp4</BaseURL>\n+    <SegmentList timescale=\"48000\" duration=\"480000\">\n+     <Initialization range=\"0-752\"/>\n+     <SegmentURL mediaRange=\"753-124129\" indexRange=\"753-796\"/>\n+     <SegmentURL mediaRange=\"124130-250544\" indexRange=\"124130-124173\"/>\n+     <SegmentURL mediaRange=\"250545-374929\" indexRange=\"250545-250588\"/>\n+    </SegmentList>\n+   </Representation>\n+  </AdaptationSet>\n+ </Period>\n+</MPD>\n+\ndiff --git a/test/testdata/mpd/subtitles.mpd b/test/testdata/mpd/subtitles.mpd\nnew file mode 100644\nindex 00000000000..6f948adba92\n--- /dev/null\n+++ b/test/testdata/mpd/subtitles.mpd\n@@ -0,0 +1,351 @@\n+<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+<!-- Created with Unified Streaming Platform (version=1.10.18-20255) -->\n+<MPD\n+  xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n+  xmlns=\"urn:mpeg:dash:schema:mpd:2011\"\n+  xsi:schemaLocation=\"urn:mpeg:dash:schema:mpd:2011 http://standards.iso.org/ittf/PubliclyAvailableStandards/MPEG-DASH_schema_files/DASH-MPD.xsd\"\n+  type=\"static\"\n+  mediaPresentationDuration=\"PT14M48S\"\n+  maxSegmentDuration=\"PT1M\"\n+  minBufferTime=\"PT10S\"\n+  profiles=\"urn:mpeg:dash:profile:isoff-live:2011\">\n+  <Period\n+    id=\"1\"\n+    duration=\"PT14M48S\">\n+    <BaseURL>dash/</BaseURL>\n+    <AdaptationSet\n+      id=\"1\"\n+      group=\"1\"\n+      contentType=\"audio\"\n+      segmentAlignment=\"true\"\n+      audioSamplingRate=\"48000\"\n+      mimeType=\"audio/mp4\"\n+      codecs=\"mp4a.40.2\"\n+      startWithSAP=\"1\">\n+      <AudioChannelConfiguration\n+        schemeIdUri=\"urn:mpeg:dash:23003:3:audio_channel_configuration:2011\"\n+        value=\"2\" />\n+      <Role schemeIdUri=\"urn:mpeg:dash:role:2011\" value=\"main\" />\n+      <SegmentTemplate\n+        timescale=\"48000\"\n+        initialization=\"3144-kZT4LWMQw6Rh7Kpd-$RepresentationID$.dash\"\n+        media=\"3144-kZT4LWMQw6Rh7Kpd-$RepresentationID$-$Time$.dash\">\n+        <SegmentTimeline>\n+          <S t=\"0\" d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"96256\" r=\"2\" />\n+          <S d=\"95232\" />\n+          <S d=\"3584\" />\n+        </SegmentTimeline>\n+      </SegmentTemplate>\n+      <Representation\n+        id=\"audio=128001\"\n+        bandwidth=\"128001\">\n+      </Representation>\n+    </AdaptationSet>\n+    <AdaptationSet\n+      id=\"2\"\n+      group=\"3\"\n+      contentType=\"text\"\n+      lang=\"en\"\n+      mimeType=\"application/mp4\"\n+      codecs=\"stpp\"\n+      startWithSAP=\"1\">\n+      <Role schemeIdUri=\"urn:mpeg:dash:role:2011\" value=\"subtitle\" />\n+      <SegmentTemplate\n+        timescale=\"1000\"\n+        initialization=\"3144-kZT4LWMQw6Rh7Kpd-$RepresentationID$.dash\"\n+        media=\"3144-kZT4LWMQw6Rh7Kpd-$RepresentationID$-$Time$.dash\">\n+        <SegmentTimeline>\n+          <S t=\"0\" d=\"60000\" r=\"9\" />\n+          <S d=\"24000\" />\n+        </SegmentTimeline>\n+      </SegmentTemplate>\n+      <Representation\n+        id=\"textstream_eng=1000\"\n+        bandwidth=\"1000\">\n+      </Representation>\n+    </AdaptationSet>\n+    <AdaptationSet\n+      id=\"3\"\n+      group=\"2\"\n+      contentType=\"video\"\n+      par=\"960:409\"\n+      minBandwidth=\"100000\"\n+      maxBandwidth=\"4482000\"\n+      maxWidth=\"1689\"\n+      maxHeight=\"720\"\n+      segmentAlignment=\"true\"\n+      mimeType=\"video/mp4\"\n+      codecs=\"avc1.4D401F\"\n+      startWithSAP=\"1\">\n+      <Role schemeIdUri=\"urn:mpeg:dash:role:2011\" value=\"main\" />\n+      <SegmentTemplate\n+        timescale=\"12288\"\n+        initialization=\"3144-kZT4LWMQw6Rh7Kpd-$RepresentationID$.dash\"\n+        media=\"3144-kZT4LWMQw6Rh7Kpd-$RepresentationID$-$Time$.dash\">\n+        <SegmentTimeline>\n+          <S t=\"0\" d=\"24576\" r=\"443\" />\n+        </SegmentTimeline>\n+      </SegmentTemplate>\n+      <Representation\n+        id=\"video=100000\"\n+        bandwidth=\"100000\"\n+        width=\"336\"\n+        height=\"144\"\n+        sar=\"2880:2863\"\n+        scanType=\"progressive\">\n+      </Representation>\n+      <Representation\n+        id=\"video=326000\"\n+        bandwidth=\"326000\"\n+        width=\"562\"\n+        height=\"240\"\n+        sar=\"115200:114929\"\n+        scanType=\"progressive\">\n+      </Representation>\n+      <Representation\n+        id=\"video=698000\"\n+        bandwidth=\"698000\"\n+        width=\"844\"\n+        height=\"360\"\n+        sar=\"86400:86299\"\n+        scanType=\"progressive\">\n+      </Representation>\n+      <Representation\n+        id=\"video=1493000\"\n+        bandwidth=\"1493000\"\n+        width=\"1126\"\n+        height=\"480\"\n+        sar=\"230400:230267\"\n+        scanType=\"progressive\">\n+      </Representation>\n+      <Representation\n+        id=\"video=4482000\"\n+        bandwidth=\"4482000\"\n+        width=\"1688\"\n+        height=\"720\"\n+        sar=\"86400:86299\"\n+        scanType=\"progressive\">\n+      </Representation>\n+    </AdaptationSet>\n+  </Period>\n+</MPD>\ndiff --git a/test/testdata/mpd/url_and_range.mpd b/test/testdata/mpd/url_and_range.mpd\nnew file mode 100644\nindex 00000000000..b8c68aad2e6\n--- /dev/null\n+++ b/test/testdata/mpd/url_and_range.mpd\n@@ -0,0 +1,32 @@\n+<?xml version=\"1.0\" ?>\n+<MPD xmlns=\"urn:mpeg:dash:schema:mpd:2011\" profiles=\"urn:mpeg:dash:profile:isoff-live:2011\" minBufferTime=\"PT10.01S\" mediaPresentationDuration=\"PT30.097S\" type=\"static\">\n+  <!-- Created with Bento4 mp4-dash.py, VERSION=2.0.0-639 -->\n+  <Period>\n+    <!-- Video -->\n+    <AdaptationSet mimeType=\"video/mp4\" segmentAlignment=\"true\" startWithSAP=\"1\" maxWidth=\"768\" maxHeight=\"432\">\n+      <Representation id=\"video-avc1\" codecs=\"avc1.4D401E\" width=\"768\" height=\"432\" scanType=\"progressive\" frameRate=\"30000/1001\" bandwidth=\"699597\">\n+        <SegmentList timescale=\"1000\" duration=\"10010\">\n+          <Initialization sourceURL=\"video-frag.mp4\" range=\"36-746\"/>\n+          <SegmentURL media=\"video-frag.mp4\" mediaRange=\"747-876117\"/>\n+          <SegmentURL media=\"video-frag.mp4\" mediaRange=\"876118-1466913\"/>\n+          <SegmentURL media=\"video-frag.mp4\" mediaRange=\"1466914-1953954\"/>\n+          <SegmentURL media=\"video-frag.mp4\" mediaRange=\"1953955-1994652\"/>\n+        </SegmentList>\n+      </Representation>\n+    </AdaptationSet>\n+    <!-- Audio -->\n+    <AdaptationSet mimeType=\"audio/mp4\" startWithSAP=\"1\" segmentAlignment=\"true\">\n+      <Representation id=\"audio-und-mp4a.40.2\" codecs=\"mp4a.40.2\" bandwidth=\"98808\" audioSamplingRate=\"48000\">\n+        <AudioChannelConfiguration schemeIdUri=\"urn:mpeg:mpegB:cicp:ChannelConfiguration\" value=\"2\"/>\n+        <SegmentList timescale=\"1000\" duration=\"10010\">\n+          <Initialization sourceURL=\"audio-frag.mp4\" range=\"32-623\"/>\n+          <SegmentURL media=\"audio-frag.mp4\" mediaRange=\"624-124199\"/>\n+          <SegmentURL media=\"audio-frag.mp4\" mediaRange=\"124200-250303\"/>\n+          <SegmentURL media=\"audio-frag.mp4\" mediaRange=\"250304-374365\"/>\n+          <SegmentURL media=\"audio-frag.mp4\" mediaRange=\"374366-374836\"/>\n+        </SegmentList>\n+      </Representation>\n+    </AdaptationSet>\n+  </Period>\n+</MPD>\n+\n", "problem_statement": "vbox7 extract url problem\n<!--\r\n\r\n######################################################################\r\n  WARNING!\r\n  IGNORING THE FOLLOWING TEMPLATE WILL RESULT IN ISSUE CLOSED AS INCOMPLETE\r\n######################################################################\r\n\r\n-->\r\n\r\n\r\n## Checklist\r\n\r\n<!--\r\nCarefully read and work through this check list in order to prevent the most common mistakes and misuse of youtube-dl:\r\n- First of, make sure you are using the latest version of youtube-dl. Run `youtube-dl --version` and ensure your version is 2020.07.28. If it's not, see https://yt-dl.org/update on how to update. Issues with outdated version will be REJECTED.\r\n- Make sure that all provided video/audio/playlist URLs (if any) are alive and playable in a browser.\r\n- Make sure that all URLs and arguments with special characters are properly quoted or escaped as explained in http://yt-dl.org/escape.\r\n- Search the bugtracker for similar issues: http://yt-dl.org/search-issues. DO NOT post duplicates.\r\n- Read bugs section in FAQ: http://yt-dl.org/reporting\r\n- Finally, put x into all relevant boxes (like this [x])\r\n-->\r\n\r\n- [x] I'm reporting a broken site support issue\r\n- [x] I've verified that I'm running youtube-dl version **2020.07.28**\r\n- [x] I've checked that all provided URLs are alive and playable in a browser\r\n- [x] I've checked that all URLs and arguments with special characters are properly quoted or escaped\r\n- [x] I've searched the bugtracker for similar bug reports including closed ones\r\n- [x] I've read bugs section in FAQ\r\n\r\n\r\n## Verbose log\r\n\r\n<!--\r\nProvide the complete verbose output of youtube-dl that clearly demonstrates the problem.\r\nAdd the `-v` flag to your command line you run youtube-dl with (`youtube-dl -v <your command line>`), copy the WHOLE output and insert it below. It should look similar to this:\r\n [debug] System config: []\r\n [debug] User config: []\r\n [debug] Command-line args: [u'-v', u'http://www.youtube.com/watch?v=BaW_jenozKcj']\r\n [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251\r\n [debug] youtube-dl version 2020.07.28\r\n [debug] Python version 2.7.11 - Windows-2003Server-5.2.3790-SP2\r\n [debug] exe versions: ffmpeg N-75573-g1d0487f, ffprobe N-75573-g1d0487f, rtmpdump 2.4\r\n [debug] Proxy map: {}\r\n <more lines>\r\n-->\r\n\r\n```\r\nC:\\Users\\PanteliX\\Desktop>C:\\Users\\PanteliX\\Desktop\\youtube-dl.exe --verbose -g\r\nhttps://www.vbox7.com/play:e96500c7cd >>sdasdasdsd.txt\r\n[debug] System config: []\r\n[debug] User config: []\r\n[debug] Custom config: []\r\n[debug] Command-line args: ['--verbose', '-g', 'https://www.vbox7.com/play:e9650\r\n0c7cd']\r\n[debug] Encodings: locale cp1251, fs mbcs, out cp1251, pref cp1251\r\n[debug] youtube-dl version 2020.07.28\r\n[debug] Python version 3.4.4 (CPython) - Windows-7-6.1.7601-SP1\r\n[debug] exe versions: none\r\n[debug] Proxy map: {}\r\nWARNING: [Vbox7] e96500c7cd: Failed to parse JSON Invalid control character at:\r\nline 6 column 173 (char 286)\r\nWARNING: unable to extract JSON-LD ; please report this issue on https://yt-dl.o\r\nrg/bug . Make sure you are using the latest version; type  youtube-dl -U  to upd\r\nate. Be sure to call youtube-dl with the --verbose flag and include its complete\r\n output.\r\n```\r\n\r\n\r\n## Description\r\n\r\n<!--\r\nProvide an explanation of your issue in an arbitrary form. Please make sure the description is worded well enough to be understood, see https://github.com/ytdl-org/youtube-dl#is-the-description-of-the-issue-itself-sufficient. Provide any additional information, suggested solution and as much context and examples as possible.\r\nIf work on your issue requires account credentials please provide them or explain how one can obtain them.\r\n-->\r\n\r\nWhen I try to download a video it gives me the following error\r\n\nUrgent - Vbox7 support needs to be fixed, so that archivists can save nearly all its' videos before they're permanently lost on Feb 22\n<!--\r\n\r\n######################################################################\r\n  WARNING!\r\n  IGNORING THE FOLLOWING TEMPLATE WILL RESULT IN ISSUE CLOSED AS INCOMPLETE\r\n######################################################################\r\n\r\n-->\r\n\r\n\r\n## Checklist\r\n\r\n<!--\r\nCarefully read and work through this check list in order to prevent the most common mistakes and misuse of youtube-dl:\r\n- First of, make sure you are using the latest version of youtube-dl. Run `youtube-dl --version` and ensure your version is 2021.12.17. If it's not, see https://yt-dl.org/update on how to update. Issues with outdated version will be REJECTED.\r\n- Make sure that all provided video/audio/playlist URLs (if any) are alive and playable in a browser.\r\n- Make sure that all URLs and arguments with special characters are properly quoted or escaped as explained in http://yt-dl.org/escape.\r\n- Search the bugtracker for similar issues: http://yt-dl.org/search-issues. DO NOT post duplicates.\r\n- Finally, put x into all relevant boxes (like this [x])\r\n-->\r\n\r\n- [x] I'm reporting a broken site support\r\n- [x] I've verified that I'm running youtube-dl version **2021.12.17**\r\n- [x] I've checked that all provided URLs are alive and playable in a browser\r\n- [x] I've checked that all URLs and arguments with special characters are properly quoted or escaped\r\n- [x] I've searched the bugtracker for similar issues including closed ones\r\n\r\n\r\n## Verbose log\r\n\r\n<!--\r\nProvide the complete verbose output of youtube-dl that clearly demonstrates the problem.\r\nAdd the `-v` flag to your command line you run youtube-dl with (`youtube-dl -v <your command line>`), copy the WHOLE output and insert it below. It should look similar to this:\r\n [debug] System config: []\r\n [debug] User config: []\r\n [debug] Command-line args: [u'-v', u'http://www.youtube.com/watch?v=BaW_jenozKcj']\r\n [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251\r\n [debug] youtube-dl version 2021.12.17\r\n [debug] Python version 2.7.11 - Windows-2003Server-5.2.3790-SP2\r\n [debug] exe versions: ffmpeg N-75573-g1d0487f, ffprobe N-75573-g1d0487f, rtmpdump 2.4\r\n [debug] Proxy map: {}\r\n <more lines>\r\n-->\r\n\r\n```\r\nPASTE VERBOSE LOG HERE\r\n```\r\n\r\n\r\n## Description\r\n\r\n<!--\r\nProvide an explanation of your issue in an arbitrary form. Provide any additional information, suggested solution and as much context and examples as possible.\r\nIf work on your issue requires account credentials please provide them or explain how one can obtain them.\r\n-->\r\n\r\nOn February 22nd Vbox7 will be privating nearly all their videos. Archivists and regular users need an easy way to back up videos/channels, whether individually or en masse.\r\nhttps://www.reddit.com/r/Archiveteam/comments/19d6ou7/video_platform_vbox7_is_about_to_restrict_access/\r\n\r\nRelated issues:\r\nhttps://github.com/ytdl-org/youtube-dl/pull/29680\r\nhttps://github.com/yt-dlp/yt-dlp/pull/8711\r\nhttps://github.com/yt-dlp/yt-dlp/pull/5661\r\nhttps://github.com/yt-dlp/yt-dlp/issues/1098\r\n\n", "hints_text": "\n", "created_at": "2024-01-27T18:35:18Z"}
{"repo": "ytdl-org/youtube-dl", "pull_number": 32695, "instance_id": "ytdl-org__youtube-dl-32695", "issue_numbers": ["32692"], "base_commit": "be008e657d79832642e2158557c899249c9e31cd", "patch": "diff --git a/youtube_dl/compat.py b/youtube_dl/compat.py\nindex 3c526a78dc5..818ccebd0a6 100644\n--- a/youtube_dl/compat.py\n+++ b/youtube_dl/compat.py\n@@ -58,19 +58,26 @@\n \n # Also fix up lack of method arg in old Pythons\n try:\n-    _req = compat_urllib_request.Request\n-    _req('http://127.0.0.1', method='GET')\n+    type(compat_urllib_request.Request('http://127.0.0.1', method='GET'))\n except TypeError:\n-    class _request(object):\n-        def __new__(cls, url, *args, **kwargs):\n-            method = kwargs.pop('method', None)\n-            r = _req(url, *args, **kwargs)\n-            if method:\n-                r.get_method = types.MethodType(lambda _: method, r)\n-            return r\n+    def _add_init_method_arg(cls):\n \n-    compat_urllib_request.Request = _request\n+        init = cls.__init__\n \n+        def wrapped_init(self, *args, **kwargs):\n+            method = kwargs.pop('method', 'GET')\n+            init(self, *args, **kwargs)\n+            if any(callable(x.__dict__.get('get_method')) for x in (self.__class__, self) if x != cls):\n+                # allow instance or its subclass to override get_method()\n+                return\n+            if self.has_data() and method == 'GET':\n+                method = 'POST'\n+            self.get_method = types.MethodType(lambda _: method, self)\n+\n+        cls.__init__ = wrapped_init\n+\n+    _add_init_method_arg(compat_urllib_request.Request)\n+    del _add_init_method_arg\n \n try:\n     import urllib.error as compat_urllib_error\ndiff --git a/youtube_dl/extractor/common.py b/youtube_dl/extractor/common.py\nindex 0eca9f84490..d33557135b4 100644\n--- a/youtube_dl/extractor/common.py\n+++ b/youtube_dl/extractor/common.py\n@@ -596,6 +596,14 @@ def set_downloader(self, downloader):\n         \"\"\"Sets the downloader for this IE.\"\"\"\n         self._downloader = downloader\n \n+    @property\n+    def cache(self):\n+        return self._downloader.cache\n+\n+    @property\n+    def cookiejar(self):\n+        return self._downloader.cookiejar\n+\n     def _real_initialize(self):\n         \"\"\"Real initialization process. Redefine in subclasses.\"\"\"\n         pass\n@@ -942,14 +950,47 @@ def _parse_json(self, json_string, video_id, transform_source=None, fatal=True):\n             else:\n                 self.report_warning(errmsg + str(ve))\n \n-    def report_warning(self, msg, video_id=None):\n+    def __ie_msg(self, *msg):\n+        return '[{0}] {1}'.format(self.IE_NAME, ''.join(msg))\n+\n+    # msg, video_id=None, *args, only_once=False, **kwargs\n+    def report_warning(self, msg, *args, **kwargs):\n+        if len(args) > 0:\n+            video_id = args[0]\n+            args = args[1:]\n+        else:\n+            video_id = kwargs.pop('video_id', None)\n         idstr = '' if video_id is None else '%s: ' % video_id\n         self._downloader.report_warning(\n-            '[%s] %s%s' % (self.IE_NAME, idstr, msg))\n+            self.__ie_msg(idstr, msg), *args, **kwargs)\n \n     def to_screen(self, msg):\n         \"\"\"Print msg to screen, prefixing it with '[ie_name]'\"\"\"\n-        self._downloader.to_screen('[%s] %s' % (self.IE_NAME, msg))\n+        self._downloader.to_screen(self.__ie_msg(msg))\n+\n+    def write_debug(self, msg, only_once=False, _cache=[]):\n+        '''Log debug message or Print message to stderr'''\n+        if not self.get_param('verbose', False):\n+            return\n+        message = '[debug] ' + self.__ie_msg(msg)\n+        logger = self.get_param('logger')\n+        if logger:\n+            logger.debug(message)\n+        else:\n+            if only_once and hash(message) in _cache:\n+                return\n+            self._downloader.to_stderr(message)\n+            _cache.append(hash(message))\n+\n+    # name, default=None, *args, **kwargs\n+    def get_param(self, name, *args, **kwargs):\n+        default, args = (args[0], args[1:]) if len(args) > 0 else (kwargs.pop('default', None), args)\n+        if self._downloader:\n+            return self._downloader.params.get(name, default, *args, **kwargs)\n+        return default\n+\n+    def report_drm(self, video_id):\n+        self.raise_no_formats('This video is DRM protected', expected=True, video_id=video_id)\n \n     def report_extraction(self, id_or_name):\n         \"\"\"Report information extraction.\"\"\"\n@@ -977,6 +1018,15 @@ def raise_login_required(msg='This video is only available for registered users'\n     def raise_geo_restricted(msg='This video is not available from your location due to geo restriction', countries=None):\n         raise GeoRestrictedError(msg, countries=countries)\n \n+    def raise_no_formats(self, msg, expected=False, video_id=None):\n+        if expected and (\n+                self.get_param('ignore_no_formats_error') or self.get_param('wait_for_video')):\n+            self.report_warning(msg, video_id)\n+        elif isinstance(msg, ExtractorError):\n+            raise msg\n+        else:\n+            raise ExtractorError(msg, expected=expected, video_id=video_id)\n+\n     # Methods for following #608\n     @staticmethod\n     def url_result(url, ie=None, video_id=None, video_title=None):\ndiff --git a/youtube_dl/extractor/youtube.py b/youtube_dl/extractor/youtube.py\nindex 3bf483c1c8a..db840fc4556 100644\n--- a/youtube_dl/extractor/youtube.py\n+++ b/youtube_dl/extractor/youtube.py\n@@ -2,6 +2,7 @@\n \n from __future__ import unicode_literals\n \n+import collections\n import itertools\n import json\n import os.path\n@@ -23,10 +24,10 @@\n )\n from ..jsinterp import JSInterpreter\n from ..utils import (\n-    ExtractorError,\n     clean_html,\n     dict_get,\n     error_to_compat_str,\n+    ExtractorError,\n     float_or_none,\n     extract_attributes,\n     get_element_by_attribute,\n@@ -36,7 +37,9 @@\n     LazyList,\n     merge_dicts,\n     mimetype2ext,\n+    NO_DEFAULT,\n     parse_codecs,\n+    parse_count,\n     parse_duration,\n     parse_qs,\n     qualities,\n@@ -44,7 +47,9 @@\n     smuggle_url,\n     str_or_none,\n     str_to_int,\n+    T,\n     traverse_obj,\n+    try_call,\n     try_get,\n     txt_or_none,\n     unescapeHTML,\n@@ -1247,7 +1252,7 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'title': 'IMG 3456',\n                 'description': '',\n                 'upload_date': '20170613',\n-                'uploader': 'ElevageOrVert',\n+                'uploader': \"l'Or Vert asbl\",\n                 'uploader_id': '@ElevageOrVert',\n             },\n             'params': {\n@@ -1460,6 +1465,30 @@ def __init__(self, *args, **kwargs):\n         self._code_cache = {}\n         self._player_cache = {}\n \n+    # *ytcfgs, webpage=None\n+    def _extract_player_url(self, *ytcfgs, **kw_webpage):\n+        if ytcfgs and not isinstance(ytcfgs[0], dict):\n+            webpage = kw_webpage.get('webpage') or ytcfgs[0]\n+        if webpage:\n+            player_url = self._search_regex(\n+                r'\"(?:PLAYER_JS_URL|jsUrl)\"\\s*:\\s*\"([^\"]+)\"',\n+                webpage or '', 'player URL', fatal=False)\n+            if player_url:\n+                ytcfgs = ytcfgs + ({'PLAYER_JS_URL': player_url},)\n+        return traverse_obj(\n+            ytcfgs, (Ellipsis, 'PLAYER_JS_URL'), (Ellipsis, 'WEB_PLAYER_CONTEXT_CONFIGS', Ellipsis, 'jsUrl'),\n+            get_all=False, expected_type=lambda u: urljoin('https://www.youtube.com', u))\n+\n+    def _download_player_url(self, video_id, fatal=False):\n+        res = self._download_webpage(\n+            'https://www.youtube.com/iframe_api',\n+            note='Downloading iframe API JS', video_id=video_id, fatal=fatal)\n+        player_version = self._search_regex(\n+            r'player\\\\?/([0-9a-fA-F]{8})\\\\?/', res or '', 'player version', fatal=fatal,\n+            default=NO_DEFAULT if res else None)\n+        if player_version:\n+            return 'https://www.youtube.com/s/player/{0}/player_ias.vflset/en_US/base.js'.format(player_version)\n+\n     def _signature_cache_id(self, example_sig):\n         \"\"\" Return a string representation of a signature \"\"\"\n         return '.'.join(compat_str(len(part)) for part in example_sig.split('.'))\n@@ -1474,46 +1503,49 @@ def _extract_player_info(cls, player_url):\n             raise ExtractorError('Cannot identify player %r' % player_url)\n         return id_m.group('id')\n \n-    def _get_player_code(self, video_id, player_url, player_id=None):\n+    def _load_player(self, video_id, player_url, fatal=True, player_id=None):\n         if not player_id:\n             player_id = self._extract_player_info(player_url)\n-\n         if player_id not in self._code_cache:\n-            self._code_cache[player_id] = self._download_webpage(\n-                player_url, video_id,\n+            code = self._download_webpage(\n+                player_url, video_id, fatal=fatal,\n                 note='Downloading player ' + player_id,\n                 errnote='Download of %s failed' % player_url)\n-        return self._code_cache[player_id]\n+            if code:\n+                self._code_cache[player_id] = code\n+        return self._code_cache[player_id] if fatal else self._code_cache.get(player_id)\n \n     def _extract_signature_function(self, video_id, player_url, example_sig):\n         player_id = self._extract_player_info(player_url)\n \n         # Read from filesystem cache\n-        func_id = 'js_%s_%s' % (\n+        func_id = 'js_{0}_{1}'.format(\n             player_id, self._signature_cache_id(example_sig))\n         assert os.path.basename(func_id) == func_id\n \n-        cache_spec = self._downloader.cache.load('youtube-sigfuncs', func_id)\n-        if cache_spec is not None:\n-            return lambda s: ''.join(s[i] for i in cache_spec)\n+        self.write_debug('Extracting signature function {0}'.format(func_id))\n+        cache_spec, code = self.cache.load('youtube-sigfuncs', func_id), None\n \n-        code = self._get_player_code(video_id, player_url, player_id)\n-        res = self._parse_sig_js(code)\n+        if not cache_spec:\n+            code = self._load_player(video_id, player_url, player_id)\n+        if code:\n+            res = self._parse_sig_js(code)\n+            test_string = ''.join(map(compat_chr, range(len(example_sig))))\n+            cache_spec = [ord(c) for c in res(test_string)]\n+            self.cache.store('youtube-sigfuncs', func_id, cache_spec)\n \n-        test_string = ''.join(map(compat_chr, range(len(example_sig))))\n-        cache_res = res(test_string)\n-        cache_spec = [ord(c) for c in cache_res]\n-\n-        self._downloader.cache.store('youtube-sigfuncs', func_id, cache_spec)\n-        return res\n+        return lambda s: ''.join(s[i] for i in cache_spec)\n \n     def _print_sig_code(self, func, example_sig):\n+        if not self.get_param('youtube_print_sig_code'):\n+            return\n+\n         def gen_sig_code(idxs):\n             def _genslice(start, end, step):\n                 starts = '' if start == 0 else str(start)\n                 ends = (':%d' % (end + step)) if end + step >= 0 else ':'\n                 steps = '' if step == 1 else (':%d' % step)\n-                return 's[%s%s%s]' % (starts, ends, steps)\n+                return 's[{0}{1}{2}]'.format(starts, ends, steps)\n \n             step = None\n             # Quelch pyflakes warnings - start will be set when step is set\n@@ -1564,143 +1596,137 @@ def _parse_sig_js(self, jscode):\n             jscode, 'Initial JS player signature function name', group='sig')\n \n         jsi = JSInterpreter(jscode)\n-\n         initial_function = jsi.extract_function(funcname)\n-\n         return lambda s: initial_function([s])\n \n-    def _decrypt_signature(self, s, video_id, player_url):\n-        \"\"\"Turn the encrypted s field into a working signature\"\"\"\n+    def _cached(self, func, *cache_id):\n+        def inner(*args, **kwargs):\n+            if cache_id not in self._player_cache:\n+                try:\n+                    self._player_cache[cache_id] = func(*args, **kwargs)\n+                except ExtractorError as e:\n+                    self._player_cache[cache_id] = e\n+                except Exception as e:\n+                    self._player_cache[cache_id] = ExtractorError(traceback.format_exc(), cause=e)\n \n-        if player_url is None:\n-            raise ExtractorError('Cannot decrypt signature without player_url')\n+            ret = self._player_cache[cache_id]\n+            if isinstance(ret, Exception):\n+                raise ret\n+            return ret\n+        return inner\n \n-        try:\n-            player_id = (player_url, self._signature_cache_id(s))\n-            if player_id not in self._player_cache:\n-                func = self._extract_signature_function(\n-                    video_id, player_url, s\n-                )\n-                self._player_cache[player_id] = func\n-            func = self._player_cache[player_id]\n-            if self._downloader.params.get('youtube_print_sig_code'):\n-                self._print_sig_code(func, s)\n-            return func(s)\n-        except Exception as e:\n-            tb = traceback.format_exc()\n-            raise ExtractorError(\n-                'Signature extraction failed: ' + tb, cause=e)\n-\n-    def _extract_player_url(self, webpage):\n-        player_url = self._search_regex(\n-            r'\"(?:PLAYER_JS_URL|jsUrl)\"\\s*:\\s*\"([^\"]+)\"',\n-            webpage or '', 'player URL', fatal=False)\n-        if not player_url:\n-            return\n-        if player_url.startswith('//'):\n-            player_url = 'https:' + player_url\n-        elif not re.match(r'https?://', player_url):\n-            player_url = compat_urllib_parse.urljoin(\n-                'https://www.youtube.com', player_url)\n-        return player_url\n+    def _decrypt_signature(self, s, video_id, player_url):\n+        \"\"\"Turn the encrypted s field into a working signature\"\"\"\n+        extract_sig = self._cached(\n+            self._extract_signature_function, 'sig', player_url, self._signature_cache_id(s))\n+        func = extract_sig(video_id, player_url, s)\n+        self._print_sig_code(func, s)\n+        return func(s)\n \n     # from yt-dlp\n     # See also:\n     # 1. https://github.com/ytdl-org/youtube-dl/issues/29326#issuecomment-894619419\n     # 2. https://code.videolan.org/videolan/vlc/-/blob/4fb284e5af69aa9ac2100ccbdd3b88debec9987f/share/lua/playlist/youtube.lua#L116\n     # 3. https://github.com/ytdl-org/youtube-dl/issues/30097#issuecomment-950157377\n+    def _decrypt_nsig(self, n, video_id, player_url):\n+        \"\"\"Turn the encrypted n field into a working signature\"\"\"\n+        if player_url is None:\n+            raise ExtractorError('Cannot decrypt nsig without player_url')\n+\n+        try:\n+            jsi, player_id, func_code = self._extract_n_function_code(video_id, player_url)\n+        except ExtractorError as e:\n+            raise ExtractorError('Unable to extract nsig jsi, player_id, func_codefunction code', cause=e)\n+        if self.get_param('youtube_print_sig_code'):\n+            self.to_screen('Extracted nsig function from {0}:\\n{1}\\n'.format(\n+                player_id, func_code[1]))\n+\n+        try:\n+            extract_nsig = self._cached(self._extract_n_function_from_code, 'nsig func', player_url)\n+            ret = extract_nsig(jsi, func_code)(n)\n+        except JSInterpreter.Exception as e:\n+            self.report_warning(\n+                '%s (%s %s)' % (\n+                    self.__ie_msg(\n+                        'Unable to decode n-parameter: download likely to be throttled'),\n+                    error_to_compat_str(e),\n+                    traceback.format_exc()))\n+            return\n+\n+        self.write_debug('Decrypted nsig {0} => {1}'.format(n, ret))\n+        return ret\n+\n     def _extract_n_function_name(self, jscode):\n-        target = r'(?P<nfunc>[a-zA-Z_$][\\w$]*)(?:\\[(?P<idx>\\d+)\\])?'\n-        nfunc_and_idx = self._search_regex(\n-            r'\\.get\\(\"n\"\\)\\)&&\\(b=(%s)\\([\\w$]+\\)' % (target, ),\n-            jscode, 'Initial JS player n function name')\n-        nfunc, idx = re.match(target, nfunc_and_idx).group('nfunc', 'idx')\n+        func_name, idx = self._search_regex(\n+            r'\\.get\\(\"n\"\\)\\)&&\\(b=(?P<nfunc>[a-zA-Z_$][\\w$]*)(?:\\[(?P<idx>\\d+)\\])?\\([\\w$]+\\)',\n+            jscode, 'Initial JS player n function name', group=('nfunc', 'idx'))\n         if not idx:\n-            return nfunc\n+            return func_name\n \n-        VAR_RE_TMPL = r'var\\s+%s\\s*=\\s*(?P<name>\\[(?P<alias>%s)\\])[;,]'\n-        note = 'Initial JS player n function {0} (%s[%s])' % (nfunc, idx)\n+        return self._parse_json(self._search_regex(\n+            r'var {0}\\s*=\\s*(\\[.+?\\])\\s*[,;]'.format(re.escape(func_name)), jscode,\n+            'Initial JS player n function list ({0}.{1})'.format(func_name, idx)),\n+            func_name, transform_source=js_to_json)[int(idx)]\n \n-        def search_function_code(needle, group):\n-            return self._search_regex(\n-                VAR_RE_TMPL % (re.escape(nfunc), needle), jscode,\n-                note.format(group), group=group)\n+    def _extract_n_function_code(self, video_id, player_url):\n+        player_id = self._extract_player_info(player_url)\n+        func_code = self.cache.load('youtube-nsig', player_id)\n+        jscode = func_code or self._load_player(video_id, player_url)\n+        jsi = JSInterpreter(jscode)\n \n-        if int_or_none(idx) == 0:\n-            real_nfunc = search_function_code(r'[a-zA-Z_$][\\w$]*', group='alias')\n-            if real_nfunc:\n-                return real_nfunc\n-        return self._parse_json(\n-            search_function_code('.+?', group='name'),\n-            nfunc, transform_source=js_to_json)[int(idx)]\n+        if func_code:\n+            return jsi, player_id, func_code\n \n-    def _extract_n_function(self, video_id, player_url):\n-        player_id = self._extract_player_info(player_url)\n-        func_code = self._downloader.cache.load('youtube-nsig', player_id)\n+        func_name = self._extract_n_function_name(jscode)\n \n+        # For redundancy\n+        func_code = self._search_regex(\n+            r'''(?xs)%s\\s*=\\s*function\\s*\\((?P<var>[\\w$]+)\\)\\s*\n+                     # NB: The end of the regex is intentionally kept strict\n+                     {(?P<code>.+?}\\s*return\\ [\\w$]+.join\\(\"\"\\))};''' % func_name,\n+            jscode, 'nsig function', group=('var', 'code'), default=None)\n         if func_code:\n-            jsi = JSInterpreter(func_code)\n+            func_code = ([func_code[0]], func_code[1])\n         else:\n-            jscode = self._get_player_code(video_id, player_url, player_id)\n-            funcname = self._extract_n_function_name(jscode)\n-            jsi = JSInterpreter(jscode)\n-            func_code = jsi.extract_function_code(funcname)\n-            self._downloader.cache.store('youtube-nsig', player_id, func_code)\n-\n-        if self._downloader.params.get('youtube_print_sig_code'):\n-            self.to_screen('Extracted nsig function from {0}:\\n{1}\\n'.format(player_id, func_code[1]))\n-\n-        return lambda s: jsi.extract_function_from_code(*func_code)([s])\n-\n-    def _n_descramble(self, n_param, player_url, video_id):\n-        \"\"\"Compute the response to YT's \"n\" parameter challenge,\n-           or None\n-\n-        Args:\n-        n_param     -- challenge string that is the value of the\n-                       URL's \"n\" query parameter\n-        player_url  -- URL of YT player JS\n-        video_id\n-        \"\"\"\n+            self.write_debug('Extracting nsig function with jsinterp')\n+            func_code = jsi.extract_function_code(func_name)\n \n-        sig_id = ('nsig_value', n_param)\n-        if sig_id in self._player_cache:\n-            return self._player_cache[sig_id]\n+        self.cache.store('youtube-nsig', player_id, func_code)\n+        return jsi, player_id, func_code\n+\n+    def _extract_n_function_from_code(self, jsi, func_code):\n+        func = jsi.extract_function_from_code(*func_code)\n+\n+        def extract_nsig(s):\n+            try:\n+                ret = func([s])\n+            except JSInterpreter.Exception:\n+                raise\n+            except Exception as e:\n+                raise JSInterpreter.Exception(traceback.format_exc(), cause=e)\n \n-        try:\n-            player_id = ('nsig', player_url)\n-            if player_id not in self._player_cache:\n-                self._player_cache[player_id] = self._extract_n_function(video_id, player_url)\n-            func = self._player_cache[player_id]\n-            ret = func(n_param)\n             if ret.startswith('enhanced_except_'):\n-                raise ExtractorError('Unhandled exception in decode')\n-            self._player_cache[sig_id] = ret\n-            if self._downloader.params.get('verbose', False):\n-                self._downloader.to_screen('[debug] [%s] %s' % (self.IE_NAME, 'Decrypted nsig {0} => {1}'.format(n_param, self._player_cache[sig_id])))\n-            return self._player_cache[sig_id]\n-        except Exception as e:\n-            self._downloader.report_warning(\n-                '[%s] %s (%s %s)' % (\n-                    self.IE_NAME,\n-                    'Unable to decode n-parameter: download likely to be throttled',\n-                    error_to_compat_str(e),\n-                    traceback.format_exc()))\n+                raise JSInterpreter.Exception('Signature function returned an exception')\n+            return ret\n+\n+        return extract_nsig\n+\n+    def _unthrottle_format_urls(self, video_id, player_url, *formats):\n+\n+        def decrypt_nsig(n):\n+            return self._cached(self._decrypt_nsig, 'nsig', n, player_url)\n \n-    def _unthrottle_format_urls(self, video_id, player_url, formats):\n         for fmt in formats:\n             parsed_fmt_url = compat_urllib_parse.urlparse(fmt['url'])\n             n_param = compat_parse_qs(parsed_fmt_url.query).get('n')\n             if not n_param:\n                 continue\n             n_param = n_param[-1]\n-            n_response = self._n_descramble(n_param, player_url, video_id)\n+            n_response = decrypt_nsig(n_param)(n_param, video_id, player_url)\n             if n_response is None:\n                 # give up if descrambling failed\n                 break\n-            for fmt_dct in traverse_obj(fmt, (None, (None, ('fragments', Ellipsis))), expected_type=dict):\n-                fmt_dct['url'] = update_url(\n-                    fmt_dct['url'], query_update={'n': [n_response]})\n+            fmt['url'] = update_url_query(fmt['url'], {'n': n_response})\n \n     # from yt-dlp, with tweaks\n     def _extract_signature_timestamp(self, video_id, player_url, ytcfg=None, fatal=False):\n@@ -1708,16 +1734,16 @@ def _extract_signature_timestamp(self, video_id, player_url, ytcfg=None, fatal=F\n         Extract signatureTimestamp (sts)\n         Required to tell API what sig/player version is in use.\n         \"\"\"\n-        sts = int_or_none(ytcfg.get('STS')) if isinstance(ytcfg, dict) else None\n+        sts = traverse_obj(ytcfg, 'STS', expected_type=int)\n         if not sts:\n             # Attempt to extract from player\n             if player_url is None:\n                 error_msg = 'Cannot extract signature timestamp without player_url.'\n                 if fatal:\n                     raise ExtractorError(error_msg)\n-                self._downloader.report_warning(error_msg)\n+                self.report_warning(error_msg)\n                 return\n-            code = self._get_player_code(video_id, player_url)\n+            code = self._load_player(video_id, player_url, fatal=fatal)\n             sts = int_or_none(self._search_regex(\n                 r'(?:signatureTimestamp|sts)\\s*:\\s*(?P<sts>[0-9]{5})', code or '',\n                 'JS player signature timestamp', group='sts', fatal=fatal))\n@@ -1733,12 +1759,18 @@ def _mark_watched(self, video_id, player_response):\n         # cpn generation algorithm is reverse engineered from base.js.\n         # In fact it works even with dummy cpn.\n         CPN_ALPHABET = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-_'\n-        cpn = ''.join((CPN_ALPHABET[random.randint(0, 256) & 63] for _ in range(0, 16)))\n-\n-        playback_url = update_url(\n-            playback_url, query_update={\n-                'ver': ['2'],\n-                'cpn': [cpn],\n+        cpn = ''.join(CPN_ALPHABET[random.randint(0, 256) & 63] for _ in range(0, 16))\n+\n+        # more consistent results setting it to right before the end\n+        qs = parse_qs(playback_url)\n+        video_length = '{0}'.format(float((qs.get('len') or ['1.5'])[0]) - 1)\n+\n+        playback_url = update_url_query(\n+            playback_url, {\n+                'ver': '2',\n+                'cpn': cpn,\n+                'cmt': video_length,\n+                'el': 'detailpage',  # otherwise defaults to \"shorts\"\n             })\n \n         self._download_webpage(\n@@ -1986,8 +2018,11 @@ def feed_entry(name):\n             else:\n                 self.to_screen('Downloading just video %s because of --no-playlist' % video_id)\n \n+        if not player_url:\n+            player_url = self._extract_player_url(webpage)\n+\n         formats = []\n-        itags = []\n+        itags = collections.defaultdict(set)\n         itag_qualities = {}\n         q = qualities(['tiny', 'small', 'medium', 'large', 'hd720', 'hd1080', 'hd1440', 'hd2160', 'hd2880', 'highres'])\n         CHUNK_SIZE = 10 << 20\n@@ -2003,58 +2038,92 @@ def build_fragments(f):\n                 })\n             } for range_start in range(0, f['filesize'], CHUNK_SIZE))\n \n+        lower = lambda s: s.lower()\n+\n         for fmt in streaming_formats:\n-            if fmt.get('targetDurationSec') or fmt.get('drmFamilies'):\n+            if fmt.get('targetDurationSec'):\n                 continue\n \n             itag = str_or_none(fmt.get('itag'))\n-            quality = fmt.get('quality')\n-            if itag and quality:\n+            audio_track = traverse_obj(fmt, ('audioTrack', T(dict))) or {}\n+\n+            quality = traverse_obj(fmt, ((\n+                # The 3gp format (17) in android client has a quality of \"small\",\n+                # but is actually worse than other formats\n+                T(lambda _: 'tiny' if itag == 17 else None),\n+                ('quality', T(lambda q: q if q and q != 'tiny' else None)),\n+                ('audioQuality', T(lower)),\n+                'quality'), T(txt_or_none)), get_all=False)\n+            if quality and itag:\n                 itag_qualities[itag] = quality\n             # FORMAT_STREAM_TYPE_OTF(otf=1) requires downloading the init fragment\n             # (adding `&sq=0` to the URL) and parsing emsg box to determine the\n-            # number of fragment that would subsequently requested with (`&sq=N`)\n+            # number of fragments that would subsequently be requested with (`&sq=N`)\n             if fmt.get('type') == 'FORMAT_STREAM_TYPE_OTF':\n                 continue\n \n             fmt_url = fmt.get('url')\n             if not fmt_url:\n                 sc = compat_parse_qs(fmt.get('signatureCipher'))\n-                fmt_url = url_or_none(try_get(sc, lambda x: x['url'][0]))\n-                encrypted_sig = try_get(sc, lambda x: x['s'][0])\n-                if not (sc and fmt_url and encrypted_sig):\n+                fmt_url = traverse_obj(sc, ('url', -1, T(url_or_none)))\n+                encrypted_sig = traverse_obj(sc, ('s', -1))\n+                if not (fmt_url and encrypted_sig):\n                     continue\n+                player_url = player_url or self._extract_player_url(webpage)\n                 if not player_url:\n-                    player_url = self._extract_player_url(webpage)\n-                if not player_url:\n                     continue\n-                signature = self._decrypt_signature(sc['s'][0], video_id, player_url)\n-                sp = try_get(sc, lambda x: x['sp'][0]) or 'signature'\n-                fmt_url += '&' + sp + '=' + signature\n+                try:\n+                    fmt_url = update_url_query(fmt_url, {\n+                        traverse_obj(sc, ('sp', -1)) or 'signature':\n+                            [self._decrypt_signature(encrypted_sig, video_id, player_url)],\n+                    })\n+                except ExtractorError as e:\n+                    self.report_warning('Signature extraction failed: Some formats may be missing',\n+                                        video_id=video_id, only_once=True)\n+                    self.write_debug(error_to_compat_str(e), only_once=True)\n+                    continue\n \n-            if itag:\n-                itags.append(itag)\n-            tbr = float_or_none(\n-                fmt.get('averageBitrate') or fmt.get('bitrate'), 1000)\n+            language_preference = (\n+                10 if audio_track.get('audioIsDefault')\n+                else -10 if 'descriptive' in (traverse_obj(audio_track, ('displayName', T(lower))) or '')\n+                else -1)\n+            name = (\n+                traverse_obj(fmt, ('qualityLabel', T(txt_or_none)))\n+                or quality.replace('audio_quality_', ''))\n             dct = {\n-                'asr': int_or_none(fmt.get('audioSampleRate')),\n-                'filesize': int_or_none(fmt.get('contentLength')),\n-                'format_id': itag,\n-                'format_note': fmt.get('qualityLabel') or quality,\n-                'fps': int_or_none(fmt.get('fps')),\n-                'height': int_or_none(fmt.get('height')),\n-                'quality': q(quality),\n-                'tbr': tbr,\n+                'format_id': join_nonempty(itag, fmt.get('isDrc') and 'drc'),\n                 'url': fmt_url,\n-                'width': fmt.get('width'),\n+                # Format 22 is likely to be damaged: see https://github.com/yt-dlp/yt-dlp/issues/3372\n+                'source_preference': ((-5 if itag == '22' else -1)\n+                                      + (100 if 'Premium' in name else 0)),\n+                'quality': q(quality),\n+                'language': join_nonempty(audio_track.get('id', '').split('.')[0],\n+                                          'desc' if language_preference < -1 else '') or None,\n+                'language_preference': language_preference,\n+                # Strictly de-prioritize 3gp formats\n+                'preference': -2 if itag == '17' else None,\n             }\n-            mimetype = fmt.get('mimeType')\n-            if mimetype:\n-                mobj = re.match(\n-                    r'((?:[^/]+)/(?:[^;]+))(?:;\\s*codecs=\"([^\"]+)\")?', mimetype)\n-                if mobj:\n-                    dct['ext'] = mimetype2ext(mobj.group(1))\n-                    dct.update(parse_codecs(mobj.group(2)))\n+            if itag:\n+                itags[itag].add(('https', dct.get('language')))\n+            self._unthrottle_format_urls(video_id, player_url, dct)\n+            dct.update(traverse_obj(fmt, {\n+                'asr': ('audioSampleRate', T(int_or_none)),\n+                'filesize': ('contentLength', T(int_or_none)),\n+                'format_note': ('qualityLabel', T(lambda x: x or quality)),\n+                # for some formats, fps is wrongly returned as 1\n+                'fps': ('fps', T(int_or_none), T(lambda f: f if f > 1 else None)),\n+                'audio_channels': ('audioChannels', T(int_or_none)),\n+                'height': ('height', T(int_or_none)),\n+                'has_drm': ('drmFamilies', T(bool)),\n+                'tbr': (('averageBitrate', 'bitrate'), T(lambda t: float_or_none(t, 1000))),\n+                'width': ('width', T(int_or_none)),\n+                '_duration_ms': ('approxDurationMs', T(int_or_none)),\n+            }, get_all=False))\n+            mime_mobj = re.match(\n+                r'((?:[^/]+)/(?:[^;]+))(?:;\\s*codecs=\"([^\"]+)\")?', fmt.get('mimeType') or '')\n+            if mime_mobj:\n+                dct['ext'] = mimetype2ext(mime_mobj.group(1))\n+                dct.update(parse_codecs(mime_mobj.group(2)))\n             single_stream = 'none' in (dct.get(c) for c in ('acodec', 'vcodec'))\n             if single_stream and dct.get('ext'):\n                 dct['container'] = dct['ext'] + '_dash'\n@@ -2069,32 +2138,62 @@ def build_fragments(f):\n \n             formats.append(dct)\n \n+        def process_manifest_format(f, proto, client_name, itag, all_formats=False):\n+            key = (proto, f.get('language'))\n+            if not all_formats and key in itags[itag]:\n+                return False\n+            itags[itag].add(key)\n+\n+            if itag:\n+                f['format_id'] = (\n+                    '{0}-{1}'.format(itag, proto)\n+                    if all_formats or any(p != proto for p, _ in itags[itag])\n+                    else itag)\n+\n+            if f.get('source_preference') is None:\n+                f['source_preference'] = -1\n+\n+            if itag in ('616', '235'):\n+                f['format_note'] = join_nonempty(f.get('format_note'), 'Premium', delim=' ')\n+                f['source_preference'] += 100\n+\n+            f['quality'] = q(traverse_obj(f, (\n+                'format_id', T(lambda s: itag_qualities[s.split('-')[0]])), default=-1))\n+            if try_call(lambda: f['fps'] <= 1):\n+                del f['fps']\n+\n+            if proto == 'hls' and f.get('has_drm'):\n+                f['has_drm'] = 'maybe'\n+                f['source_preference'] -= 5\n+            return True\n+\n         hls_manifest_url = streaming_data.get('hlsManifestUrl')\n         if hls_manifest_url:\n             for f in self._extract_m3u8_formats(\n                     hls_manifest_url, video_id, 'mp4', fatal=False):\n-                itag = self._search_regex(\n-                    r'/itag/(\\d+)', f['url'], 'itag', default=None)\n-                if itag:\n-                    f['format_id'] = itag\n-                formats.append(f)\n+                if process_manifest_format(\n+                        f, 'hls', None, self._search_regex(\n+                            r'/itag/(\\d+)', f['url'], 'itag', default=None)):\n+                    formats.append(f)\n \n         if self._downloader.params.get('youtube_include_dash_manifest', True):\n             dash_manifest_url = streaming_data.get('dashManifestUrl')\n             if dash_manifest_url:\n                 for f in self._extract_mpd_formats(\n                         dash_manifest_url, video_id, fatal=False):\n-                    itag = f['format_id']\n-                    if itag in itags:\n-                        continue\n-                    if itag in itag_qualities:\n-                        f['quality'] = q(itag_qualities[itag])\n-                    filesize = int_or_none(self._search_regex(\n-                        r'/clen/(\\d+)', f.get('fragment_base_url')\n-                        or f['url'], 'file size', default=None))\n-                    if filesize:\n-                        f['filesize'] = filesize\n-                    formats.append(f)\n+                    if process_manifest_format(\n+                            f, 'dash', None, f['format_id']):\n+                        f['filesize'] = traverse_obj(f, (\n+                            ('fragment_base_url', 'url'), T(lambda u: self._search_regex(\n+                                r'/clen/(\\d+)', u, 'file size', default=None)),\n+                            T(int_or_none)), get_all=False)\n+                        formats.append(f)\n+\n+        playable_formats = [f for f in formats if not f.get('has_drm')]\n+        if formats and not playable_formats:\n+            # If there are no formats that definitely don't have DRM, all have DRM\n+            self.report_drm(video_id)\n+        formats[:] = playable_formats\n \n         if not formats:\n             if streaming_data.get('licenseInfos'):\n@@ -2166,6 +2265,17 @@ def build_fragments(f):\n             video_details.get('lengthSeconds')\n             or microformat.get('lengthSeconds')) \\\n             or parse_duration(search_meta('duration'))\n+\n+        for f in formats:\n+            # Some formats may have much smaller duration than others (possibly damaged during encoding)\n+            # but avoid false positives with small duration differences.\n+            # Ref: https://github.com/yt-dlp/yt-dlp/issues/2823\n+            if try_call(lambda x: float(x.pop('_duration_ms')) / duration < 500, args=(f,)):\n+                self.report_warning(\n+                    '{0}: Some possibly damaged formats will be deprioritized'.format(video_id), only_once=True)\n+                # Strictly de-prioritize damaged formats\n+                f['preference'] = -10\n+\n         is_live = video_details.get('isLive')\n \n         owner_profile_url = self._yt_urljoin(self._extract_author_var(\n@@ -2174,10 +2284,6 @@ def build_fragments(f):\n         uploader = self._extract_author_var(\n             webpage, 'name', videodetails=video_details, metadata=microformat)\n \n-        if not player_url:\n-            player_url = self._extract_player_url(webpage)\n-        self._unthrottle_format_urls(video_id, player_url, formats)\n-\n         info = {\n             'id': video_id,\n             'title': self._live_title(video_title) if is_live else video_title,\n@@ -2370,6 +2476,14 @@ def chapter_time(mmlir):\n                             'like_count': str_to_int(like_count),\n                             'dislike_count': str_to_int(dislike_count),\n                         })\n+                    else:\n+                        info['like_count'] = traverse_obj(vpir, (\n+                            'videoActions', 'menuRenderer', 'topLevelButtons', Ellipsis,\n+                            'segmentedLikeDislikeButtonViewModel', 'likeButtonViewModel', 'likeButtonViewModel',\n+                            'toggleButtonViewModel', 'toggleButtonViewModel', 'defaultButtonViewModel',\n+                            'buttonViewModel', (('title', ('accessibilityText', T(lambda s: s.split()), Ellipsis))), T(parse_count)),\n+                            get_all=False)\n+\n                 vsir = content.get('videoSecondaryInfoRenderer')\n                 if vsir:\n                     rows = try_get(\n@@ -2484,7 +2598,7 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n         'playlist_mincount': 94,\n         'info_dict': {\n             'id': 'UCqj7Cz7revf5maW9g5pgNcg',\n-            'title': 'Igor Kleiner - Playlists',\n+            'title': r're:Igor Kleiner(?: Ph\\.D\\.)? - Playlists',\n             'description': 'md5:be97ee0f14ee314f1f002cf187166ee2',\n             'uploader': 'Igor Kleiner',\n             'uploader_id': '@IgorDataScience',\n@@ -2495,7 +2609,7 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n         'playlist_mincount': 94,\n         'info_dict': {\n             'id': 'UCqj7Cz7revf5maW9g5pgNcg',\n-            'title': 'Igor Kleiner - Playlists',\n+            'title': r're:Igor Kleiner(?: Ph\\.D\\.)? - Playlists',\n             'description': 'md5:be97ee0f14ee314f1f002cf187166ee2',\n             'uploader': 'Igor Kleiner',\n             'uploader_id': '@IgorDataScience',\n@@ -2607,7 +2721,7 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n         'url': 'https://www.youtube.com/channel/UCKfVa3S1e4PHvxWcwyMMg8w/channels',\n         'info_dict': {\n             'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n-            'title': 'lex will - Channels',\n+            'title': r're:lex will - (?:Home|Channels)',\n             'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n             'uploader': 'lex will',\n             'uploader_id': '@lexwill718',\n", "test_patch": "diff --git a/test/test_compat.py b/test/test_compat.py\nindex e233b1ae1b7..b83c8cb4100 100644\n--- a/test/test_compat.py\n+++ b/test/test_compat.py\n@@ -23,6 +23,7 @@\n     compat_urllib_parse_unquote,\n     compat_urllib_parse_unquote_plus,\n     compat_urllib_parse_urlencode,\n+    compat_urllib_request,\n )\n \n \n@@ -135,6 +136,19 @@ def test_compat_casefold(self):\n         self.assertEqual(compat_casefold('\\u03a3'), '\\u03c3')\n         self.assertEqual(compat_casefold('A\\u0345\\u03a3'), 'a\\u03b9\\u03c3')\n \n+    def test_compat_urllib_request_Request(self):\n+        self.assertEqual(\n+            compat_urllib_request.Request('http://127.0.0.1', method='PUT').get_method(),\n+            'PUT')\n+\n+        class PUTrequest(compat_urllib_request.Request):\n+            def get_method(self):\n+                return 'PUT'\n+\n+        self.assertEqual(\n+            PUTrequest('http://127.0.0.1').get_method(),\n+            'PUT')\n+\n \n if __name__ == '__main__':\n     unittest.main()\ndiff --git a/test/test_subtitles.py b/test/test_subtitles.py\nindex 1197721ff09..e005c78fc2a 100644\n--- a/test/test_subtitles.py\n+++ b/test/test_subtitles.py\n@@ -295,6 +295,7 @@ class TestNRKSubtitles(BaseTestSubtitles):\n     def test_allsubtitles(self):\n         self.DL.params['writesubtitles'] = True\n         self.DL.params['allsubtitles'] = True\n+        self.DL.params['format'] = 'best/bestvideo'\n         subtitles = self.getSubtitles()\n         self.assertEqual(set(subtitles.keys()), set(['nb-ttv']))\n         self.assertEqual(md5(subtitles['nb-ttv']), '67e06ff02d0deaf975e68f6cb8f6a149')\n", "problem_statement": "certain lengthy youtube videos take a long time to initialise\n<!--\r\n\r\n######################################################################\r\n  WARNING!\r\n  IGNORING THE FOLLOWING TEMPLATE WILL RESULT IN ISSUE CLOSED AS INCOMPLETE\r\n######################################################################\r\n\r\n-->\r\n\r\n\r\n## Checklist\r\n\r\n<!--\r\nCarefully read and work through this check list in order to prevent the most common mistakes and misuse of youtube-dl:\r\n- First of, make sure you are using the latest version of youtube-dl. Run `youtube-dl --version` and ensure your version is 2021.12.17. If it's not, see https://yt-dl.org/update on how to update. Issues with outdated version will be REJECTED.\r\n- Make sure that all provided video/audio/playlist URLs (if any) are alive and playable in a browser.\r\n- Make sure that all URLs and arguments with special characters are properly quoted or escaped as explained in http://yt-dl.org/escape.\r\n- Search the bugtracker for similar issues: http://yt-dl.org/search-issues. DO NOT post duplicates.\r\n- Finally, put x into all relevant boxes (like this [x])\r\n-->\r\n\r\n- [ ] I'm reporting a broken site support\r\n- [X] I've verified that I'm running youtube-dl version **2021.12.17**\r\n- [X] I've checked that all provided URLs are alive and playable in a browser\r\n- [X] I've checked that all URLs and arguments with special characters are properly quoted or escaped\r\n- [ ] I've searched the bugtracker for similar issues including closed ones\r\n\r\n\r\n## Verbose log\r\n\r\n<!--\r\nProvide the complete verbose output of youtube-dl that clearly demonstrates the problem.\r\nAdd the `-v` flag to your command line you run youtube-dl with (`youtube-dl -v <your command line>`), copy the WHOLE output and insert it below. It should look similar to this:\r\n [debug] System config: []\r\n [debug] User config: []\r\n [debug] Command-line args: [u'-v', u'http://www.youtube.com/watch?v=BaW_jenozKcj']\r\n [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251\r\n [debug] youtube-dl version 2021.12.17\r\n [debug] Python version 2.7.11 - Windows-2003Server-5.2.3790-SP2\r\n [debug] exe versions: ffmpeg N-75573-g1d0487f, ffprobe N-75573-g1d0487f, rtmpdump 2.4\r\n [debug] Proxy map: {}\r\n <more lines>\r\n-->\r\n\r\n```\r\n[debug] System config: []\r\n[debug] User config: []\r\n[debug] Custom config: []\r\n[debug] Command-line args: ['-v', '--simulate', '8SIiGo3TVKE']\r\n[debug] Encodings: locale UTF-8, fs utf-8, out utf-8, pref UTF-8\r\n[debug] youtube-dl version 2021.12.17\r\n[debug] Python 3.10.13 (PyPy version 7.3.14 AMD64 64bit) - Windows-8.1-6.3.9600-\r\nSP0 - OpenSSL 1.1.1t  7 Feb 2023\r\n[debug] exe versions: none\r\n[debug] Proxy map: {}\r\n[youtube] 8SIiGo3TVKE: Downloading webpage\r\n[debug] [youtube] Decrypted nsig 45tbpgX-M8RD3WRrq => JiFeLd6kfppZFg\r\n[debug] [youtube] Decrypted nsig t9SpsqJ7CqdW7NVV0 => hVl3_KSCH26XkA\r\n[debug] Default format spec: bestvideo+bestaudio/best\r\n```\r\n\r\n\r\n## Description\r\n\r\n<!--\r\nProvide an explanation of your issue in an arbitrary form. Provide any additional information, suggested solution and as much context and examples as possible.\r\nIf work on your issue requires account credentials please provide them or explain how one can obtain them.\r\n-->\r\n\r\nMost youtube videos take about 13 seconds of runtime for me, with --simulate. Some videos take 20 or 28 seconds. Other videos take 80 seconds. This depends on the video and is reproducible.\r\n\r\nSample video with such behaviour: 8SIiGo3TVKE\r\n\r\nyoutube-dl pauses for a looong time after:\r\n`[debug] [youtube] Decrypted nsig 45tbpgX-M8RD3WRrq => JiFeLd6kfppZFg`\r\n\r\nCPU is pegged during the pause and the total runtime is 80 seconds (!). Memory use is high as well: it rises slowly to ~218 MB (as opposed to ~50 MB normally)\r\n\r\nThe download works, of course, but this time it spends on initialisation is excessive. As far as I can tell, it is not caused by the n-sig decryption, but something that happens in between.\r\n\r\nThe only thing I've found in common with these videos is that they're long and have HD video (1080p or more).\r\n\r\nAbove verbose output was taken with PyPy 3.10, and it also happens with PyPy 2.7 as well as CPython 2.7.\r\n\r\nSo if somebody feels like profiling some code, there's something to profile here...\n", "hints_text": "With this low-spec 2-CPU notebook, the extraction for `8SIiGo3TVKE` takes ~210s with one CPU close to maxed and `VSZ` up to ~250MB. With no n-sig processing _yt-dl_ (Py2.7) is faster than _yt-dlp_ (Py 3.11) at < 11s.\r\n\r\nA trivial trace of the n-sig processing seems to be showing an increasing delay between applying the cached n-sig to each format.\r\n\r\nWe should unthrottle the DASH formats **before** `build_fragments()`: then 210s becomes 30s. \"**Bad programmer, BAD!**\"\nWith PR #32695, incorporating the above commit:\r\nyt-dl/Py2.7 ~5.5s\r\nyt-dl/Py3.11 <4s\r\nyt-dlp/Py3.11 ~7s\r\n\nThanks for the [test](https://github.com/ytdl-org/youtube-dl/issues/32687#issuecomment-1894432273). Yes indeed, it's much faster when it doesn't actually calculate the n-sig. Now I'm getting something like these figures:\r\nyt-dlp --extractor-args 'youtube:player-client=web' -> ~8s\r\nyt-dl/Py2.7 -> ~22s (still faster than above)\r\nyt-dl/Py3.11 -> ~13s.\r\n\r\nI guess there is some shim/shims making 2.7 so much slower.", "created_at": "2024-01-16T00:23:41Z"}
{"repo": "ytdl-org/youtube-dl", "pull_number": 32138, "instance_id": "ytdl-org__youtube-dl-32138", "issue_numbers": ["31688", "32456"], "base_commit": "47214e46d852e9d7ddf81d69a8e70806e2396c6c", "patch": "diff --git a/devscripts/make_lazy_extractors.py b/devscripts/make_lazy_extractors.py\nindex a8b6ff1b9a2..1a841a08b69 100644\n--- a/devscripts/make_lazy_extractors.py\n+++ b/devscripts/make_lazy_extractors.py\n@@ -4,6 +4,7 @@\n import io\n import os\n from os.path import dirname as dirn\n+import re\n import sys\n \n print('WARNING: Lazy loading extractors is an experimental feature that may not always work', file=sys.stderr)\n@@ -29,11 +30,18 @@\n with open('devscripts/lazy_load_template.py', 'rt') as f:\n     module_template = f.read()\n \n+\n+def get_source(m):\n+    return re.sub(r'(?m)^\\s*#.*\\n', '', getsource(m))\n+\n+\n module_contents = [\n-    module_template + '\\n' + getsource(InfoExtractor.suitable) + '\\n',\n+    module_template,\n+    get_source(InfoExtractor.suitable),\n+    get_source(InfoExtractor._match_valid_url) + '\\n',\n     'class LazyLoadSearchExtractor(LazyLoadExtractor):\\n    pass\\n',\n     # needed for suitable() methods of Youtube extractor (see #28780)\n-    'from youtube_dl.utils import parse_qs\\n',\n+    'from youtube_dl.utils import parse_qs, variadic\\n',\n ]\n \n ie_template = '''\n@@ -66,7 +74,7 @@ def build_lazy_ie(ie, name):\n         valid_url=valid_url,\n         module=ie.__module__)\n     if ie.suitable.__func__ is not InfoExtractor.suitable.__func__:\n-        s += '\\n' + getsource(ie.suitable)\n+        s += '\\n' + get_source(ie.suitable)\n     if hasattr(ie, '_make_valid_url'):\n         # search extractors\n         s += make_valid_template.format(valid_url=ie._make_valid_url())\ndiff --git a/youtube_dl/extractor/clipchamp.py b/youtube_dl/extractor/clipchamp.py\nnew file mode 100644\nindex 00000000000..3b485eaab0f\n--- /dev/null\n+++ b/youtube_dl/extractor/clipchamp.py\n@@ -0,0 +1,69 @@\n+# coding: utf-8\n+from __future__ import unicode_literals\n+\n+from .common import InfoExtractor\n+from ..compat import compat_str\n+from ..utils import (\n+    ExtractorError,\n+    merge_dicts,\n+    T,\n+    traverse_obj,\n+    unified_timestamp,\n+    url_or_none,\n+)\n+\n+\n+class ClipchampIE(InfoExtractor):\n+    _VALID_URL = r'https?://(?:www\\.)?clipchamp\\.com/watch/(?P<id>[\\w-]+)'\n+    _TESTS = [{\n+        'url': 'https://clipchamp.com/watch/gRXZ4ZhdDaU',\n+        'info_dict': {\n+            'id': 'gRXZ4ZhdDaU',\n+            'ext': 'mp4',\n+            'title': 'Untitled video',\n+            'uploader': 'Alexander Schwartz',\n+            'timestamp': 1680805580,\n+            'upload_date': '20230406',\n+            'thumbnail': r're:^https?://.+\\.jpg',\n+        },\n+        'params': {\n+            'skip_download': 'm3u8',\n+            'format': 'bestvideo',\n+        },\n+    }]\n+\n+    _STREAM_URL_TMPL = 'https://%s.cloudflarestream.com/%s/manifest/video.%s'\n+    _STREAM_URL_QUERY = {'parentOrigin': 'https://clipchamp.com'}\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+        webpage = self._download_webpage(url, video_id)\n+        data = self._search_nextjs_data(webpage, video_id)['props']['pageProps']['video']\n+\n+        storage_location = data.get('storage_location')\n+        if storage_location != 'cf_stream':\n+            raise ExtractorError('Unsupported clip storage location \"%s\"' % (storage_location,))\n+\n+        path = data['download_url']\n+        iframe = self._download_webpage(\n+            'https://iframe.cloudflarestream.com/' + path, video_id, 'Downloading player iframe')\n+        subdomain = self._search_regex(\n+            r'''\\bcustomer-domain-prefix\\s*=\\s*(\"|')(?P<sd>[\\w-]+)\\1''', iframe,\n+            'subdomain', group='sd', fatal=False) or 'customer-2ut9yn3y6fta1yxe'\n+\n+        formats = self._extract_mpd_formats(\n+            self._STREAM_URL_TMPL % (subdomain, path, 'mpd'), video_id,\n+            query=self._STREAM_URL_QUERY, fatal=False, mpd_id='dash')\n+        formats.extend(self._extract_m3u8_formats(\n+            self._STREAM_URL_TMPL % (subdomain, path, 'm3u8'), video_id, 'mp4',\n+            query=self._STREAM_URL_QUERY, fatal=False, m3u8_id='hls'))\n+\n+        return merge_dicts({\n+            'id': video_id,\n+            'formats': formats,\n+            'uploader': ' '.join(traverse_obj(data, ('creator', ('first_name', 'last_name'), T(compat_str)))) or None,\n+        }, traverse_obj(data, {\n+            'title': ('project', 'project_name', T(compat_str)),\n+            'timestamp': ('created_at', T(unified_timestamp)),\n+            'thumbnail': ('thumbnail_url', T(url_or_none)),\n+        }), rev=True)\ndiff --git a/youtube_dl/extractor/common.py b/youtube_dl/extractor/common.py\nindex 7244e5df64b..7f416d31241 100644\n--- a/youtube_dl/extractor/common.py\n+++ b/youtube_dl/extractor/common.py\n@@ -3,6 +3,7 @@\n \n import base64\n import datetime\n+import functools\n import hashlib\n import json\n import netrc\n@@ -23,6 +24,7 @@\n     compat_getpass,\n     compat_integer_types,\n     compat_http_client,\n+    compat_map as map,\n     compat_os_name,\n     compat_str,\n     compat_urllib_error,\n@@ -31,6 +33,7 @@\n     compat_urllib_request,\n     compat_urlparse,\n     compat_xml_parse_error,\n+    compat_zip as zip,\n )\n from ..downloader.f4m import (\n     get_base_url,\n@@ -70,6 +73,7 @@\n     str_or_none,\n     str_to_int,\n     strip_or_none,\n+    traverse_obj,\n     try_get,\n     unescapeHTML,\n     unified_strdate,\n@@ -79,6 +83,7 @@\n     urljoin,\n     url_basename,\n     url_or_none,\n+    variadic,\n     xpath_element,\n     xpath_text,\n     xpath_with_ns,\n@@ -367,9 +372,22 @@ class InfoExtractor(object):\n     title, description etc.\n \n \n-    Subclasses of this one should re-define the _real_initialize() and\n-    _real_extract() methods and define a _VALID_URL regexp.\n-    Probably, they should also be added to the list of extractors.\n+    A subclass of InfoExtractor must be defined to handle each specific site (or\n+    several sites). Such a concrete subclass should be added to the list of\n+    extractors. It should also:\n+    * define its _VALID_URL attribute as a regexp, or a Sequence of alternative\n+      regexps (but see below)\n+    * re-define the _real_extract() method\n+    * optionally re-define the _real_initialize() method.\n+\n+    An extractor subclass may also override suitable() if necessary, but the\n+    function signature must be preserved and the function must import everything\n+    it needs (except other extractors), so that lazy_extractors works correctly.\n+    If the subclass's suitable() and _real_extract() functions avoid using\n+    _VALID_URL, the subclass need not set that class attribute.\n+\n+    An abstract subclass of InfoExtractor may be used to simplify implementation\n+    within an extractor module; it should not be added to the list of extractors.\n \n     _GEO_BYPASS attribute may be set to False in order to disable\n     geo restriction bypass mechanisms for a particular extractor.\n@@ -405,21 +423,32 @@ def __init__(self, downloader=None):\n         self.set_downloader(downloader)\n \n     @classmethod\n-    def suitable(cls, url):\n-        \"\"\"Receives a URL and returns True if suitable for this IE.\"\"\"\n-\n+    def __match_valid_url(cls, url):\n         # This does not use has/getattr intentionally - we want to know whether\n-        # we have cached the regexp for *this* class, whereas getattr would also\n-        # match the superclass\n+        # we have cached the regexp for cls, whereas getattr would also\n+        # match its superclass\n         if '_VALID_URL_RE' not in cls.__dict__:\n-            cls._VALID_URL_RE = re.compile(cls._VALID_URL)\n-        return cls._VALID_URL_RE.match(url) is not None\n+            # _VALID_URL can now be a list/tuple of patterns\n+            cls._VALID_URL_RE = tuple(map(re.compile, variadic(cls._VALID_URL)))\n+        # 20% faster than next(filter(None, (p.match(url) for p in cls._VALID_URL_RE)), None) in 2.7\n+        for p in cls._VALID_URL_RE:\n+            p = p.match(url)\n+            if p:\n+                return p\n+\n+    # The public alias can safely be overridden, as in some back-ports\n+    _match_valid_url = __match_valid_url\n+\n+    @classmethod\n+    def suitable(cls, url):\n+        \"\"\"Receives a URL and returns True if suitable for this IE.\"\"\"\n+        # This function must import everything it needs (except other extractors),\n+        # so that lazy_extractors works correctly\n+        return cls.__match_valid_url(url) is not None\n \n     @classmethod\n     def _match_id(cls, url):\n-        if '_VALID_URL_RE' not in cls.__dict__:\n-            cls._VALID_URL_RE = re.compile(cls._VALID_URL)\n-        m = cls._VALID_URL_RE.match(url)\n+        m = cls.__match_valid_url(url)\n         assert m\n         return compat_str(m.group('id'))\n \n@@ -1005,6 +1034,8 @@ def _search_regex(self, pattern, string, name, default=NO_DEFAULT, fatal=True, f\n             if group is None:\n                 # return the first matching group\n                 return next(g for g in mobj.groups() if g is not None)\n+            elif isinstance(group, (list, tuple)):\n+                return tuple(mobj.group(g) for g in group)\n             else:\n                 return mobj.group(group)\n         elif default is not NO_DEFAULT:\n@@ -1020,10 +1051,9 @@ def _html_search_regex(self, pattern, string, name, default=NO_DEFAULT, fatal=Tr\n         Like _search_regex, but strips HTML tags and unescapes entities.\n         \"\"\"\n         res = self._search_regex(pattern, string, name, default, fatal, flags, group)\n-        if res:\n-            return clean_html(res).strip()\n-        else:\n-            return res\n+        if isinstance(res, tuple):\n+            return tuple(map(clean_html, res))\n+        return clean_html(res)\n \n     def _get_netrc_login_info(self, netrc_machine=None):\n         username = None\n@@ -1348,6 +1378,44 @@ def extract_video_object(e):\n                     break\n         return dict((k, v) for k, v in info.items() if v is not None)\n \n+    def _search_nextjs_data(self, webpage, video_id, **kw):\n+        nkw = dict((k, v) for k, v in kw.items() if k in ('transform_source', 'fatal'))\n+        kw.pop('transform_source', None)\n+        next_data = self._search_regex(\n+            r'''<script[^>]+\\bid\\s*=\\s*('|\")__NEXT_DATA__\\1[^>]*>(?P<nd>[^<]+)</script>''',\n+            webpage, 'next.js data', group='nd', **kw)\n+        if not next_data:\n+            return {}\n+        return self._parse_json(next_data, video_id, **nkw)\n+\n+    def _search_nuxt_data(self, webpage, video_id, *args, **kwargs):\n+        \"\"\"Parses Nuxt.js metadata. This works as long as the function __NUXT__ invokes is a pure function\"\"\"\n+\n+        # self, webpage, video_id, context_name='__NUXT__', *, fatal=True, traverse=('data', 0)\n+        context_name = args[0] if len(args) > 0 else kwargs.get('context_name', '__NUXT__')\n+        fatal = kwargs.get('fatal', True)\n+        traverse = kwargs.get('traverse', ('data', 0))\n+\n+        re_ctx = re.escape(context_name)\n+\n+        FUNCTION_RE = (r'\\(\\s*function\\s*\\((?P<arg_keys>[\\s\\S]*?)\\)\\s*\\{\\s*'\n+                       r'return\\s+(?P<js>\\{[\\s\\S]*?})\\s*;?\\s*}\\s*\\((?P<arg_vals>[\\s\\S]*?)\\)')\n+\n+        js, arg_keys, arg_vals = self._search_regex(\n+            (p.format(re_ctx, FUNCTION_RE) for p in\n+             (r'<script>\\s*window\\s*\\.\\s*{0}\\s*=\\s*{1}\\s*\\)\\s*;?\\s*</script>',\n+              r'{0}\\s*\\([\\s\\S]*?{1}')),\n+            webpage, context_name, group=('js', 'arg_keys', 'arg_vals'),\n+            default=NO_DEFAULT if fatal else (None, None, None))\n+        if js is None:\n+            return {}\n+\n+        args = dict(zip(arg_keys.split(','), map(json.dumps, self._parse_json(\n+            '[{0}]'.format(arg_vals), video_id, transform_source=js_to_json, fatal=fatal) or ())))\n+\n+        ret = self._parse_json(js, video_id, transform_source=functools.partial(js_to_json, vars=args), fatal=fatal)\n+        return traverse_obj(ret, traverse) or {}\n+\n     @staticmethod\n     def _hidden_inputs(html):\n         html = re.sub(r'<!--(?:(?!<!--).)*-->', '', html)\n@@ -2495,7 +2563,8 @@ def parse_content_type(content_type):\n                 return f\n             return {}\n \n-        def _media_formats(src, cur_media_type, type_info={}):\n+        def _media_formats(src, cur_media_type, type_info=None):\n+            type_info = type_info or {}\n             full_url = absolute_url(src)\n             ext = type_info.get('ext') or determine_ext(full_url)\n             if ext == 'm3u8':\n@@ -2513,6 +2582,7 @@ def _media_formats(src, cur_media_type, type_info={}):\n                 formats = [{\n                     'url': full_url,\n                     'vcodec': 'none' if cur_media_type == 'audio' else None,\n+                    'ext': ext,\n                 }]\n             return is_plain_url, formats\n \n@@ -2521,7 +2591,7 @@ def _media_formats(src, cur_media_type, type_info={}):\n         # so we wll include them right here (see\n         # https://www.ampproject.org/docs/reference/components/amp-video)\n         # For dl8-* tags see https://delight-vr.com/documentation/dl8-video/\n-        _MEDIA_TAG_NAME_RE = r'(?:(?:amp|dl8(?:-live)?)-)?(video|audio)'\n+        _MEDIA_TAG_NAME_RE = r'(?:(?:amp|dl8(?:-live)?)-)?(video(?:-js)?|audio)'\n         media_tags = [(media_tag, media_tag_name, media_type, '')\n                       for media_tag, media_tag_name, media_type\n                       in re.findall(r'(?s)(<(%s)[^>]*/>)' % _MEDIA_TAG_NAME_RE, webpage)]\n@@ -2539,7 +2609,8 @@ def _media_formats(src, cur_media_type, type_info={}):\n             media_attributes = extract_attributes(media_tag)\n             src = strip_or_none(media_attributes.get('src'))\n             if src:\n-                _, formats = _media_formats(src, media_type)\n+                f = parse_content_type(media_attributes.get('type'))\n+                _, formats = _media_formats(src, media_type, f)\n                 media_info['formats'].extend(formats)\n             media_info['thumbnail'] = absolute_url(media_attributes.get('poster'))\n             if media_content:\ndiff --git a/youtube_dl/extractor/dlf.py b/youtube_dl/extractor/dlf.py\nnew file mode 100644\nindex 00000000000..cc3de45826d\n--- /dev/null\n+++ b/youtube_dl/extractor/dlf.py\n@@ -0,0 +1,204 @@\n+# coding: utf-8\n+from __future__ import unicode_literals\n+\n+import re\n+\n+from .common import InfoExtractor\n+from ..compat import (\n+    compat_str,\n+)\n+from ..utils import (\n+    determine_ext,\n+    extract_attributes,\n+    int_or_none,\n+    merge_dicts,\n+    traverse_obj,\n+    url_or_none,\n+    variadic,\n+)\n+\n+\n+class DLFBaseIE(InfoExtractor):\n+    _VALID_URL_BASE = r'https?://(?:www\\.)?deutschlandfunk\\.de/'\n+    _BUTTON_REGEX = r'(<button[^>]+alt=\"Anh\u00f6ren\"[^>]+data-audio-diraid[^>]*>)'\n+\n+    def _parse_button_attrs(self, button, audio_id=None):\n+        attrs = extract_attributes(button)\n+        audio_id = audio_id or attrs['data-audio-diraid']\n+\n+        url = traverse_obj(\n+            attrs, 'data-audio-download-src', 'data-audio', 'data-audioreference',\n+            'data-audio-src', expected_type=url_or_none)\n+        ext = determine_ext(url)\n+        formats = (self._extract_m3u8_formats(url, audio_id, fatal=False)\n+                   if ext == 'm3u8' else [{'url': url, 'ext': ext, 'vcodec': 'none'}])\n+        self._sort_formats(formats)\n+\n+        def traverse_attrs(path):\n+            path = list(variadic(path))\n+            t = path.pop() if callable(path[-1]) else None\n+            return traverse_obj(attrs, path, expected_type=t, get_all=False)\n+\n+        def txt_or_none(v, default=None):\n+            return default if v is None else (compat_str(v).strip() or default)\n+\n+        return merge_dicts(*reversed([{\n+            'id': audio_id,\n+            # 'extractor_key': DLFIE.ie_key(),\n+            # 'extractor': DLFIE.IE_NAME,\n+            'formats': formats,\n+        }, dict((k, traverse_attrs(v)) for k, v in {\n+            'title': (('data-audiotitle', 'data-audio-title', 'data-audio-download-tracking-title'), txt_or_none),\n+            'duration': (('data-audioduration', 'data-audio-duration'), int_or_none),\n+            'thumbnail': ('data-audioimage', url_or_none),\n+            'uploader': 'data-audio-producer',\n+            'series': 'data-audio-series',\n+            'channel': 'data-audio-origin-site-name',\n+            'webpage_url': ('data-audio-download-tracking-path', url_or_none),\n+        }.items())]))\n+\n+\n+class DLFIE(DLFBaseIE):\n+    IE_NAME = 'dlf'\n+    _VALID_URL = DLFBaseIE._VALID_URL_BASE + r'[\\w-]+-dlf-(?P<id>[\\da-f]{8})-100\\.html'\n+    _TESTS = [\n+        # Audio as an HLS stream\n+        {\n+            'url': 'https://www.deutschlandfunk.de/tanz-der-saiteninstrumente-das-wild-strings-trio-aus-slowenien-dlf-03a3eb19-100.html',\n+            'info_dict': {\n+                'id': '03a3eb19',\n+                'title': r're:Tanz der Saiteninstrumente [-/] Das Wild Strings Trio aus Slowenien',\n+                'ext': 'm4a',\n+                'duration': 3298,\n+                'thumbnail': 'https://assets.deutschlandfunk.de/FALLBACK-IMAGE-AUDIO/512x512.png?t=1603714364673',\n+                'uploader': 'Deutschlandfunk',\n+                'series': 'On Stage',\n+                'channel': 'deutschlandfunk'\n+            },\n+            'params': {\n+                'skip_download': 'm3u8'\n+            },\n+            'skip': 'This webpage no longer exists'\n+        }, {\n+            'url': 'https://www.deutschlandfunk.de/russische-athleten-kehren-zurueck-auf-die-sportbuehne-ein-gefaehrlicher-tueroeffner-dlf-d9cc1856-100.html',\n+            'info_dict': {\n+                'id': 'd9cc1856',\n+                'title': 'Russische Athleten kehren zur\u00fcck auf die Sportb\u00fchne: Ein gef\u00e4hrlicher T\u00fcr\u00f6ffner',\n+                'ext': 'mp3',\n+                'duration': 291,\n+                'thumbnail': 'https://assets.deutschlandfunk.de/FALLBACK-IMAGE-AUDIO/512x512.png?t=1603714364673',\n+                'uploader': 'Deutschlandfunk',\n+                'series': 'Kommentare und Themen der Woche',\n+                'channel': 'deutschlandfunk'\n+            }\n+        },\n+    ]\n+\n+    def _real_extract(self, url):\n+        audio_id = self._match_id(url)\n+        webpage = self._download_webpage(url, audio_id)\n+\n+        return self._parse_button_attrs(\n+            self._search_regex(self._BUTTON_REGEX, webpage, 'button'), audio_id)\n+\n+\n+class DLFCorpusIE(DLFBaseIE):\n+    IE_NAME = 'dlf:corpus'\n+    IE_DESC = 'DLF Multi-feed Archives'\n+    _VALID_URL = DLFBaseIE._VALID_URL_BASE + r'(?P<id>(?![\\w-]+-dlf-[\\da-f]{8})[\\w-]+-\\d+)\\.html'\n+    _TESTS = [\n+        # Recorded news broadcast with referrals to related broadcasts\n+        {\n+            'url': 'https://www.deutschlandfunk.de/fechten-russland-belarus-ukraine-protest-100.html',\n+            'info_dict': {\n+                'id': 'fechten-russland-belarus-ukraine-protest-100',\n+                'title': r're:Wiederzulassung als neutrale Athleten [-/] Was die R\u00fcckkehr russischer und belarussischer Sportler beim Fechten bedeutet',\n+                'description': 'md5:91340aab29c71aa7518ad5be13d1e8ad'\n+            },\n+            'playlist_mincount': 5,\n+            'playlist': [{\n+                'info_dict': {\n+                    'id': '1fc5d64a',\n+                    'title': r're:Wiederzulassung als neutrale Athleten [-/] Was die R\u00fcckkehr russischer und belarussischer Sportler beim Fechten bedeutet',\n+                    'ext': 'mp3',\n+                    'duration': 252,\n+                    'thumbnail': 'https://assets.deutschlandfunk.de/aad16241-6b76-4a09-958b-96d0ee1d6f57/512x512.jpg?t=1679480020313',\n+                    'uploader': 'Deutschlandfunk',\n+                    'series': 'Sport',\n+                    'channel': 'deutschlandfunk'\n+                }\n+            }, {\n+                'info_dict': {\n+                    'id': '2ada145f',\n+                    'title': r're:(?:Sportpolitik / )?Fechtverband votiert f\u00fcr R\u00fcckkehr russischer Athleten',\n+                    'ext': 'mp3',\n+                    'duration': 336,\n+                    'thumbnail': 'https://assets.deutschlandfunk.de/FILE_93982766f7317df30409b8a184ac044a/512x512.jpg?t=1678547581005',\n+                    'uploader': 'Deutschlandfunk',\n+                    'series': 'Deutschlandfunk Nova',\n+                    'channel': 'deutschlandfunk-nova'\n+                }\n+            }, {\n+                'info_dict': {\n+                    'id': '5e55e8c9',\n+                    'title': r're:Wiederzulassung von Russland und Belarus [-/] \"Herumlavieren\" des Fechter-Bundes sorgt f\u00fcr Unverst\u00e4ndnis',\n+                    'ext': 'mp3',\n+                    'duration': 187,\n+                    'thumbnail': 'https://assets.deutschlandfunk.de/a595989d-1ed1-4a2e-8370-b64d7f11d757/512x512.jpg?t=1679173825412',\n+                    'uploader': 'Deutschlandfunk',\n+                    'series': 'Sport am Samstag',\n+                    'channel': 'deutschlandfunk'\n+                }\n+            }, {\n+                'info_dict': {\n+                    'id': '47e1a096',\n+                    'title': r're:R\u00fcckkehr Russlands im Fechten [-/] \"Fassungslos, dass es einfach so passiert ist\"',\n+                    'ext': 'mp3',\n+                    'duration': 602,\n+                    'thumbnail': 'https://assets.deutschlandfunk.de/da4c494a-21cc-48b4-9cc7-40e09fd442c2/512x512.jpg?t=1678562155770',\n+                    'uploader': 'Deutschlandfunk',\n+                    'series': 'Sport am Samstag',\n+                    'channel': 'deutschlandfunk'\n+                }\n+            }, {\n+                'info_dict': {\n+                    'id': '5e55e8c9',\n+                    'title': r're:Wiederzulassung von Russland und Belarus [-/] \"Herumlavieren\" des Fechter-Bundes sorgt f\u00fcr Unverst\u00e4ndnis',\n+                    'ext': 'mp3',\n+                    'duration': 187,\n+                    'thumbnail': 'https://assets.deutschlandfunk.de/a595989d-1ed1-4a2e-8370-b64d7f11d757/512x512.jpg?t=1679173825412',\n+                    'uploader': 'Deutschlandfunk',\n+                    'series': 'Sport am Samstag',\n+                    'channel': 'deutschlandfunk'\n+                }\n+            }]\n+        },\n+        # Podcast feed with tag buttons, playlist count fluctuates\n+        {\n+            'url': 'https://www.deutschlandfunk.de/kommentare-und-themen-der-woche-100.html',\n+            'info_dict': {\n+                'id': 'kommentare-und-themen-der-woche-100',\n+                'title': 'Meinung - Kommentare und Themen der Woche',\n+                'description': 'md5:2901bbd65cd2d45e116d399a099ce5d5',\n+            },\n+            'playlist_mincount': 10,\n+        },\n+        # Podcast feed with no description\n+        {\n+            'url': 'https://www.deutschlandfunk.de/podcast-tolle-idee-100.html',\n+            'info_dict': {\n+                'id': 'podcast-tolle-idee-100',\n+                'title': 'Wissenschaftspodcast - Tolle Idee! - Was wurde daraus?',\n+            },\n+            'playlist_mincount': 11,\n+        },\n+    ]\n+\n+    def _real_extract(self, url):\n+        playlist_id = self._match_id(url)\n+        webpage = self._download_webpage(url, playlist_id)\n+\n+        return self.playlist_result(\n+            map(self._parse_button_attrs, re.findall(self._BUTTON_REGEX, webpage)),\n+            playlist_id, self._html_search_meta(['og:title', 'twitter:title'], webpage, default=None),\n+            self._html_search_meta(['description', 'og:description', 'twitter:description'], webpage, default=None))\ndiff --git a/youtube_dl/extractor/extractors.py b/youtube_dl/extractor/extractors.py\nindex 3a87f9e3380..42b009ef5a9 100644\n--- a/youtube_dl/extractor/extractors.py\n+++ b/youtube_dl/extractor/extractors.py\n@@ -226,6 +226,7 @@\n     CiscoLiveSearchIE,\n )\n from .cjsw import CJSWIE\n+from .clipchamp import ClipchampIE\n from .cliphunter import CliphunterIE\n from .clippit import ClippitIE\n from .cliprs import ClipRsIE\n@@ -295,6 +296,10 @@\n from .dctp import DctpTvIE\n from .deezer import DeezerPlaylistIE\n from .democracynow import DemocracynowIE\n+from .dlf import (\n+    DLFCorpusIE,\n+    DLFIE,\n+)\n from .dfb import DFBIE\n from .dhm import DHMIE\n from .digg import DiggIE\n@@ -444,6 +449,13 @@\n from .giantbomb import GiantBombIE\n from .giga import GigaIE\n from .glide import GlideIE\n+from .globalplayer import (\n+    GlobalPlayerLiveIE,\n+    GlobalPlayerLivePlaylistIE,\n+    GlobalPlayerAudioIE,\n+    GlobalPlayerAudioEpisodeIE,\n+    GlobalPlayerVideoIE\n+)\n from .globo import (\n     GloboIE,\n     GloboArticleIE,\n@@ -975,6 +987,10 @@\n from .pornotube import PornotubeIE\n from .pornovoisines import PornoVoisinesIE\n from .pornoxo import PornoXOIE\n+from .pr0gramm import (\n+    Pr0grammIE,\n+    Pr0grammStaticIE,\n+)\n from .puhutv import (\n     PuhuTVIE,\n     PuhuTVSerieIE,\n@@ -1565,6 +1581,7 @@\n     WeiboMobileIE\n )\n from .weiqitv import WeiqiTVIE\n+from .whyp import WhypIE\n from .wistia import (\n     WistiaIE,\n     WistiaPlaylistIE,\n@@ -1678,7 +1695,3 @@\n )\n from .zoom import ZoomIE\n from .zype import ZypeIE\n-from .pr0gramm import (\n-    Pr0grammIE,\n-    Pr0grammStaticIE,\n-)\ndiff --git a/youtube_dl/extractor/globalplayer.py b/youtube_dl/extractor/globalplayer.py\nnew file mode 100644\nindex 00000000000..ae75dcabf72\n--- /dev/null\n+++ b/youtube_dl/extractor/globalplayer.py\n@@ -0,0 +1,273 @@\n+# coding: utf-8\n+from __future__ import unicode_literals\n+\n+from .common import InfoExtractor\n+from ..utils import (\n+    clean_html,\n+    join_nonempty,\n+    merge_dicts,\n+    parse_duration,\n+    str_or_none,\n+    T,\n+    traverse_obj,\n+    unified_strdate,\n+    unified_timestamp,\n+    urlhandle_detect_ext,\n+)\n+\n+\n+class GlobalPlayerBaseIE(InfoExtractor):\n+\n+    def _get_page_props(self, url, video_id):\n+        webpage = self._download_webpage(url, video_id)\n+        return self._search_nextjs_data(webpage, video_id)['props']['pageProps']\n+\n+    def _request_ext(self, url, video_id):\n+        return urlhandle_detect_ext(self._request_webpage(  # Server rejects HEAD requests\n+            url, video_id, note='Determining source extension'))\n+\n+    @staticmethod\n+    def _clean_desc(x):\n+        x = clean_html(x)\n+        if x:\n+            x = x.replace('\\xa0', ' ')\n+        return x\n+\n+    def _extract_audio(self, episode, series):\n+\n+        return merge_dicts({\n+            'vcodec': 'none',\n+        }, traverse_obj(series, {\n+            'series': 'title',\n+            'series_id': 'id',\n+            'thumbnail': 'imageUrl',\n+            'uploader': 'itunesAuthor',  # podcasts only\n+        }), traverse_obj(episode, {\n+            'id': 'id',\n+            'description': ('description', T(self._clean_desc)),\n+            'duration': ('duration', T(parse_duration)),\n+            'thumbnail': 'imageUrl',\n+            'url': 'streamUrl',\n+            'timestamp': (('pubDate', 'startDate'), T(unified_timestamp)),\n+            'title': 'title',\n+        }, get_all=False), rev=True)\n+\n+\n+class GlobalPlayerLiveIE(GlobalPlayerBaseIE):\n+    _VALID_URL = r'https?://www\\.globalplayer\\.com/live/(?P<id>\\w+)/\\w+'\n+    _TESTS = [{\n+        'url': 'https://www.globalplayer.com/live/smoothchill/uk/',\n+        'info_dict': {\n+            'id': '2mx1E',\n+            'ext': 'aac',\n+            'display_id': 'smoothchill-uk',\n+            'title': 're:^Smooth Chill.+$',\n+            'thumbnail': 'https://herald.musicradio.com/media/f296ade8-50c9-4f60-911f-924e96873620.png',\n+            'description': 'Music To Chill To',\n+            # 'live_status': 'is_live',\n+            'is_live': True,\n+        },\n+    }, {\n+        # national station\n+        'url': 'https://www.globalplayer.com/live/heart/uk/',\n+        'info_dict': {\n+            'id': '2mwx4',\n+            'ext': 'aac',\n+            'description': 'turn up the feel good!',\n+            'thumbnail': 'https://herald.musicradio.com/media/49b9e8cb-15bf-4bf2-8c28-a4850cc6b0f3.png',\n+            # 'live_status': 'is_live',\n+            'is_live': True,\n+            'title': 're:^Heart UK.+$',\n+            'display_id': 'heart-uk',\n+        },\n+    }, {\n+        # regional variation\n+        'url': 'https://www.globalplayer.com/live/heart/london/',\n+        'info_dict': {\n+            'id': 'AMqg',\n+            'ext': 'aac',\n+            'thumbnail': 'https://herald.musicradio.com/media/49b9e8cb-15bf-4bf2-8c28-a4850cc6b0f3.png',\n+            'title': 're:^Heart London.+$',\n+            # 'live_status': 'is_live',\n+            'is_live': True,\n+            'display_id': 'heart-london',\n+            'description': 'turn up the feel good!',\n+        },\n+    }]\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+        station = self._get_page_props(url, video_id)['station']\n+        stream_url = station['streamUrl']\n+\n+        return merge_dicts({\n+            'id': station['id'],\n+            'display_id': (\n+                join_nonempty('brandSlug', 'slug', from_dict=station)\n+                or station.get('legacyStationPrefix')),\n+            'url': stream_url,\n+            'ext': self._request_ext(stream_url, video_id),\n+            'vcodec': 'none',\n+            'is_live': True,\n+        }, {\n+            'title': self._live_title(traverse_obj(\n+                station, (('name', 'brandName'), T(str_or_none)),\n+                get_all=False)),\n+        }, traverse_obj(station, {\n+            'description': 'tagline',\n+            'thumbnail': 'brandLogo',\n+        }), rev=True)\n+\n+\n+class GlobalPlayerLivePlaylistIE(GlobalPlayerBaseIE):\n+    _VALID_URL = r'https?://www\\.globalplayer\\.com/playlists/(?P<id>\\w+)'\n+    _TESTS = [{\n+        # \"live playlist\"\n+        'url': 'https://www.globalplayer.com/playlists/8bLk/',\n+        'info_dict': {\n+            'id': '8bLk',\n+            'ext': 'aac',\n+            # 'live_status': 'is_live',\n+            'is_live': True,\n+            'description': r're:(?s).+\\bclassical\\b.+\\bClassic FM Hall [oO]f Fame\\b',\n+            'thumbnail': 'https://images.globalplayer.com/images/551379?width=450&signature=oMLPZIoi5_dBSHnTMREW0Xg76mA=',\n+            'title': 're:Classic FM Hall of Fame.+$'\n+        },\n+    }]\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+        station = self._get_page_props(url, video_id)['playlistData']\n+        stream_url = station['streamUrl']\n+\n+        return merge_dicts({\n+            'id': video_id,\n+            'url': stream_url,\n+            'ext': self._request_ext(stream_url, video_id),\n+            'vcodec': 'none',\n+            'is_live': True,\n+        }, traverse_obj(station, {\n+            'title': 'title',\n+            'description': ('description', T(self._clean_desc)),\n+            'thumbnail': 'image',\n+        }), rev=True)\n+\n+\n+class GlobalPlayerAudioIE(GlobalPlayerBaseIE):\n+    _VALID_URL = r'https?://www\\.globalplayer\\.com/(?:(?P<podcast>podcasts)/|catchup/\\w+/\\w+/)(?P<id>\\w+)/?(?:$|[?#])'\n+    _TESTS = [{\n+        # podcast\n+        'url': 'https://www.globalplayer.com/podcasts/42KuaM/',\n+        'playlist_mincount': 5,\n+        'info_dict': {\n+            'id': '42KuaM',\n+            'title': 'Filthy Ritual',\n+            'thumbnail': 'md5:60286e7d12d795bd1bbc9efc6cee643e',\n+            'categories': ['Society & Culture', 'True Crime'],\n+            'uploader': 'Global',\n+            'description': r're:(?s).+\\bscam\\b.+?\\bseries available now\\b',\n+        },\n+    }, {\n+        # radio catchup\n+        'url': 'https://www.globalplayer.com/catchup/lbc/uk/46vyD7z/',\n+        'playlist_mincount': 2,\n+        'info_dict': {\n+            'id': '46vyD7z',\n+            'description': 'Nick Ferrari At Breakfast is Leading Britain\\'s Conversation.',\n+            'title': 'Nick Ferrari',\n+            'thumbnail': 'md5:4df24d8a226f5b2508efbcc6ae874ebf',\n+        },\n+    }]\n+\n+    def _real_extract(self, url):\n+        video_id, podcast = self._match_valid_url(url).group('id', 'podcast')\n+        props = self._get_page_props(url, video_id)\n+        series = props['podcastInfo'] if podcast else props['catchupInfo']\n+\n+        return merge_dicts({\n+            '_type': 'playlist',\n+            'id': video_id,\n+            'entries': [self._extract_audio(ep, series) for ep in traverse_obj(\n+                        series, ('episodes', lambda _, v: v['id'] and v['streamUrl']))],\n+            'categories': traverse_obj(series, ('categories', Ellipsis, 'name')) or None,\n+        }, traverse_obj(series, {\n+            'description': ('description', T(self._clean_desc)),\n+            'thumbnail': 'imageUrl',\n+            'title': 'title',\n+            'uploader': 'itunesAuthor',  # podcasts only\n+        }), rev=True)\n+\n+\n+class GlobalPlayerAudioEpisodeIE(GlobalPlayerBaseIE):\n+    _VALID_URL = r'https?://www\\.globalplayer\\.com/(?:(?P<podcast>podcasts)|catchup/\\w+/\\w+)/episodes/(?P<id>\\w+)/?(?:$|[?#])'\n+    _TESTS = [{\n+        # podcast\n+        'url': 'https://www.globalplayer.com/podcasts/episodes/7DrfNnE/',\n+        'info_dict': {\n+            'id': '7DrfNnE',\n+            'ext': 'mp3',\n+            'title': 'Filthy Ritual - Trailer',\n+            'description': 'md5:1f1562fd0f01b4773b590984f94223e0',\n+            'thumbnail': 'md5:60286e7d12d795bd1bbc9efc6cee643e',\n+            'duration': 225.0,\n+            'timestamp': 1681254900,\n+            'series': 'Filthy Ritual',\n+            'series_id': '42KuaM',\n+            'upload_date': '20230411',\n+            'uploader': 'Global',\n+        },\n+    }, {\n+        # radio catchup\n+        'url': 'https://www.globalplayer.com/catchup/lbc/uk/episodes/2zGq26Vcv1fCWhddC4JAwETXWe/',\n+        'only_matching': True,\n+        # expired: refresh the details with a current show for a full test\n+        'info_dict': {\n+            'id': '2zGq26Vcv1fCWhddC4JAwETXWe',\n+            'ext': 'm4a',\n+            'timestamp': 1682056800,\n+            'series': 'Nick Ferrari',\n+            'thumbnail': 'md5:4df24d8a226f5b2508efbcc6ae874ebf',\n+            'upload_date': '20230421',\n+            'series_id': '46vyD7z',\n+            'description': 'Nick Ferrari At Breakfast is Leading Britain\\'s Conversation.',\n+            'title': 'Nick Ferrari',\n+            'duration': 10800.0,\n+        },\n+    }]\n+\n+    def _real_extract(self, url):\n+        video_id, podcast = self._match_valid_url(url).group('id', 'podcast')\n+        props = self._get_page_props(url, video_id)\n+        episode = props['podcastEpisode'] if podcast else props['catchupEpisode']\n+\n+        return self._extract_audio(\n+            episode, traverse_obj(episode, 'podcast', 'show', expected_type=dict) or {})\n+\n+\n+class GlobalPlayerVideoIE(GlobalPlayerBaseIE):\n+    _VALID_URL = r'https?://www\\.globalplayer\\.com/videos/(?P<id>\\w+)'\n+    _TESTS = [{\n+        'url': 'https://www.globalplayer.com/videos/2JsSZ7Gm2uP/',\n+        'info_dict': {\n+            'id': '2JsSZ7Gm2uP',\n+            'ext': 'mp4',\n+            'description': 'md5:6a9f063c67c42f218e42eee7d0298bfd',\n+            'thumbnail': 'md5:d4498af48e15aae4839ce77b97d39550',\n+            'upload_date': '20230420',\n+            'title': 'Treble Malakai Bayoh sings a sublime Handel aria at Classic FM Live',\n+        },\n+    }]\n+\n+    def _real_extract(self, url):\n+        video_id = self._match_id(url)\n+        meta = self._get_page_props(url, video_id)['videoData']\n+\n+        return merge_dicts({\n+            'id': video_id,\n+        }, traverse_obj(meta, {\n+            'url': 'url',\n+            'thumbnail': ('image', 'url'),\n+            'title': 'title',\n+            'upload_date': ('publish_date', T(unified_strdate)),\n+            'description': 'description',\n+        }), rev=True)\ndiff --git a/youtube_dl/extractor/whyp.py b/youtube_dl/extractor/whyp.py\nnew file mode 100644\nindex 00000000000..644eb4617eb\n--- /dev/null\n+++ b/youtube_dl/extractor/whyp.py\n@@ -0,0 +1,55 @@\n+# coding: utf-8\n+from __future__ import unicode_literals\n+\n+from .common import InfoExtractor\n+from ..utils import (\n+    float_or_none,\n+    merge_dicts,\n+    str_or_none,\n+    T,\n+    traverse_obj,\n+    url_or_none,\n+)\n+\n+\n+class WhypIE(InfoExtractor):\n+    _VALID_URL = r'https?://(?:www\\.)?whyp\\.it/tracks/(?P<id>\\d+)'\n+    _TESTS = [{\n+        'url': 'https://www.whyp.it/tracks/18337/home-page-example-track-b4kq7',\n+        'md5': 'c1187b42ebf8605284e3dc92aeb33d16',\n+        'info_dict': {\n+            'url': 'https://cdn.whyp.it/50eb17cc-e9ff-4e18-b89b-dc9206a95cb1.mp3',\n+            'id': '18337',\n+            'title': 'Home Page Example Track',\n+            'description': r're:(?s).+\\bexample track\\b',\n+            'ext': 'mp3',\n+            'duration': 52.82,\n+            'uploader': 'Brad',\n+            'uploader_id': '1',\n+            'thumbnail': 'https://cdn.whyp.it/a537bb36-3373-4c61-96c8-27fc1b2f427a.jpg',\n+        },\n+    }, {\n+        'url': 'https://www.whyp.it/tracks/18337',\n+        'only_matching': True,\n+    }]\n+\n+    def _real_extract(self, url):\n+        unique_id = self._match_id(url)\n+        webpage = self._download_webpage(url, unique_id)\n+        data = self._search_nuxt_data(webpage, unique_id)['rawTrack']\n+\n+        return merge_dicts({\n+            'url': data['audio_url'],\n+            'id': unique_id,\n+        }, traverse_obj(data, {\n+            'title': 'title',\n+            'description': 'description',\n+            'duration': ('duration', T(float_or_none)),\n+            'uploader': ('user', 'username'),\n+            'uploader_id': ('user', 'id', T(str_or_none)),\n+            'thumbnail': ('artwork_url', T(url_or_none)),\n+        }), {\n+            'ext': 'mp3',\n+            'vcodec': 'none',\n+            'http_headers': {'Referer': 'https://whyp.it/'},\n+        }, rev=True)\ndiff --git a/youtube_dl/utils.py b/youtube_dl/utils.py\nindex 494f8341b48..d52fa7a28ba 100644\n--- a/youtube_dl/utils.py\n+++ b/youtube_dl/utils.py\n@@ -4268,13 +4268,9 @@ def variadic(x, allowed_types=NO_DEFAULT):\n \n \n def dict_get(d, key_or_keys, default=None, skip_false_values=True):\n-    if isinstance(key_or_keys, (list, tuple)):\n-        for key in key_or_keys:\n-            if key not in d or d[key] is None or skip_false_values and not d[key]:\n-                continue\n-            return d[key]\n-        return default\n-    return d.get(key_or_keys, default)\n+    exp = (lambda x: x or None) if skip_false_values else IDENTITY\n+    return traverse_obj(d, *variadic(key_or_keys), expected_type=exp,\n+                        default=default, get_all=False)\n \n \n def try_call(*funcs, **kwargs):\n@@ -4307,16 +4303,38 @@ def try_get(src, getter, expected_type=None):\n                 return v\n \n \n-def merge_dicts(*dicts):\n+def merge_dicts(*dicts, **kwargs):\n+    \"\"\"\n+        Merge the `dict`s in `dicts` using the first valid value for each key.\n+        Normally valid: not None and not an empty string\n+\n+        Keyword-only args:\n+        unblank:    allow empty string if False (default True)\n+        rev:        merge dicts in reverse order (default False)\n+\n+        merge_dicts(dct1, dct2, ..., unblank=False, rev=True)\n+        matches {**dct1, **dct2, ...}\n+\n+        However, merge_dicts(dct1, dct2, ..., rev=True) may often be better.\n+    \"\"\"\n+\n+    unblank = kwargs.get('unblank', True)\n+    rev = kwargs.get('rev', False)\n+\n+    if unblank:\n+        def can_merge_str(k, v, to_dict):\n+            return (isinstance(v, compat_str) and v\n+                    and isinstance(to_dict[k], compat_str)\n+                    and not to_dict[k])\n+    else:\n+        can_merge_str = lambda k, v, to_dict: False\n+\n     merged = {}\n-    for a_dict in dicts:\n+    for a_dict in reversed(dicts) if rev else dicts:\n         for k, v in a_dict.items():\n             if v is None:\n                 continue\n-            if (k not in merged\n-                    or (isinstance(v, compat_str) and v\n-                        and isinstance(merged[k], compat_str)\n-                        and not merged[k])):\n+            if (k not in merged) or can_merge_str(k, v, merged):\n                 merged[k] = v\n     return merged\n \n@@ -4370,46 +4388,108 @@ def strip_jsonp(code):\n         r'\\g<callback_data>', code)\n \n \n-def js_to_json(code):\n-    COMMENT_RE = r'/\\*(?:(?!\\*/).)*?\\*/|//[^\\n]*'\n+def js_to_json(code, *args, **kwargs):\n+\n+    # vars is a dict of (var, val) pairs to substitute\n+    vars = args[0] if len(args) > 0 else kwargs.get('vars', {})\n+    strict = kwargs.get('strict', False)\n+\n+    STRING_QUOTES = '\\'\"`'\n+    STRING_RE = '|'.join(r'{0}(?:\\\\.|[^\\\\{0}])*{0}'.format(q) for q in STRING_QUOTES)\n+    COMMENT_RE = r'/\\*(?:(?!\\*/).)*?\\*/|//[^\\n]*\\n'\n     SKIP_RE = r'\\s*(?:{comment})?\\s*'.format(comment=COMMENT_RE)\n     INTEGER_TABLE = (\n         (r'(?s)^(0[xX][0-9a-fA-F]+){skip}:?$'.format(skip=SKIP_RE), 16),\n         (r'(?s)^(0+[0-7]+){skip}:?$'.format(skip=SKIP_RE), 8),\n+        (r'(?s)^(\\d+){skip}:?$'.format(skip=SKIP_RE), 10),\n     )\n+    # compat candidate\n+    JSONDecodeError = json.JSONDecodeError if 'JSONDecodeError' in dir(json) else ValueError\n+\n+    def process_escape(match):\n+        JSON_PASSTHROUGH_ESCAPES = r'\"\\bfnrtu'\n+        escape = match.group(1) or match.group(2)\n+\n+        return ('\\\\' + escape if escape in JSON_PASSTHROUGH_ESCAPES\n+                else '\\\\u00' if escape == 'x'\n+                else '' if escape == '\\n'\n+                else escape)\n+\n+    def template_substitute(match):\n+        evaluated = js_to_json(match.group(1), vars, strict=strict)\n+        if evaluated[0] == '\"':\n+            return json.loads(evaluated)\n+        return evaluated\n \n     def fix_kv(m):\n         v = m.group(0)\n         if v in ('true', 'false', 'null'):\n             return v\n-        elif v.startswith('/*') or v.startswith('//') or v.startswith('!') or v == ',':\n-            return \"\"\n-\n-        if v[0] in (\"'\", '\"'):\n-            v = re.sub(r'(?s)\\\\.|\"', lambda m: {\n-                '\"': '\\\\\"',\n-                \"\\\\'\": \"'\",\n-                '\\\\\\n': '',\n-                '\\\\x': '\\\\u00',\n-            }.get(m.group(0), m.group(0)), v[1:-1])\n-        else:\n-            for regex, base in INTEGER_TABLE:\n-                im = re.match(regex, v)\n-                if im:\n-                    i = int(im.group(1), base)\n-                    return '\"%d\":' % i if v.endswith(':') else '%d' % i\n+        elif v in ('undefined', 'void 0'):\n+            return 'null'\n+        elif v.startswith('/*') or v.startswith('//') or v == ',':\n+            return ''\n \n-        return '\"%s\"' % v\n+        if v[0] in STRING_QUOTES:\n+            v = re.sub(r'(?s)\\${([^}]+)}', template_substitute, v[1:-1]) if v[0] == '`' else v[1:-1]\n+            escaped = re.sub(r'(?s)(\")|\\\\(.)', process_escape, v)\n+            return '\"{0}\"'.format(escaped)\n+\n+        inv = IDENTITY\n+        im = re.split(r'^!+', v)\n+        if len(im) > 1 and not im[-1].endswith(':'):\n+            if (len(v) - len(im[1])) % 2 == 1:\n+                inv = lambda x: 'true' if x == 0 else 'false'\n+            else:\n+                inv = lambda x: 'false' if x == 0 else 'true'\n+        if not any(x for x in im):\n+            return\n+        v = im[-1]\n+\n+        for regex, base in INTEGER_TABLE:\n+            im = re.match(regex, v)\n+            if im:\n+                i = int(im.group(1), base)\n+                return ('\"%s\":' if v.endswith(':') else '%s') % inv(i)\n+\n+        if v in vars:\n+            try:\n+                if not strict:\n+                    json.loads(vars[v])\n+            except JSONDecodeError:\n+                return inv(json.dumps(vars[v]))\n+            else:\n+                return inv(vars[v])\n+\n+        if not strict:\n+            v = try_call(inv, args=(v,), default=v)\n+            if v in ('true', 'false'):\n+                return v\n+            return '\"{0}\"'.format(v)\n+\n+        raise ValueError('Unknown value: ' + v)\n+\n+    def create_map(mobj):\n+        return json.dumps(dict(json.loads(js_to_json(mobj.group(1) or '[]', vars=vars))))\n+\n+    code = re.sub(r'new Map\\((\\[.*?\\])?\\)', create_map, code)\n+    if not strict:\n+        code = re.sub(r'new Date\\((\".+\")\\)', r'\\g<1>', code)\n+        code = re.sub(r'new \\w+\\((.*?)\\)', lambda m: json.dumps(m.group(0)), code)\n+        code = re.sub(r'parseInt\\([^\\d]+(\\d+)[^\\d]+\\)', r'\\1', code)\n+        code = re.sub(r'\\(function\\([^)]*\\)\\s*\\{[^}]*\\}\\s*\\)\\s*\\(\\s*([\"\\'][^)]*[\"\\'])\\s*\\)', r'\\1', code)\n \n     return re.sub(r'''(?sx)\n-        \"(?:[^\"\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^\"\\\\]*\"|\n-        '(?:[^'\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^'\\\\]*'|\n-        {comment}|,(?={skip}[\\]}}])|\n-        (?:(?<![0-9])[eE]|[a-df-zA-DF-Z_])[.a-zA-Z_0-9]*|\n-        \\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:{skip}:)?|\n-        [0-9]+(?={skip}:)|\n+        {str_}|\n+        {comment}|\n+        ,(?={skip}[\\]}}])|\n+        void\\s0|\n+        !*(?:(?<!\\d)[eE]|[a-df-zA-DF-Z_$])[.a-zA-Z_$0-9]*|\n+        (?:\\b|!+)0(?:[xX][\\da-fA-F]+|[0-7]+)(?:{skip}:)?|\n+        !+\\d+(?:\\.\\d*)?(?:{skip}:)?|\n+        [0-9]+(?:{skip}:)|\n         !+\n-        '''.format(comment=COMMENT_RE, skip=SKIP_RE), fix_kv, code)\n+        '''.format(comment=COMMENT_RE, skip=SKIP_RE, str_=STRING_RE), fix_kv, code)\n \n \n def qualities(quality_ids):\n@@ -6029,6 +6109,37 @@ def clean_podcast_url(url):\n         )/''', '', url)\n \n \n+if __debug__:\n+    # Raise TypeError if args can't be bound\n+    # needs compat owing to unstable inspect API, thanks PSF :-(\n+    try:\n+        inspect.signature\n+\n+        def _try_bind_args(fn, *args, **kwargs):\n+            inspect.signature(fn).bind(*args, **kwargs)\n+    except AttributeError:\n+        # Py < 3.3\n+        def _try_bind_args(fn, *args, **kwargs):\n+            fn_args = inspect.getargspec(fn)\n+            # Py2: ArgInfo(args, varargs, keywords, defaults)\n+            # Py3: ArgSpec(args, varargs, keywords, defaults)\n+            if not fn_args.keywords:\n+                for k in kwargs:\n+                    if k not in (fn_args.args or []):\n+                        raise TypeError(\"got an unexpected keyword argument: '{0}'\".format(k))\n+            if not fn_args.varargs:\n+                args_to_bind = len(args)\n+                bindable = len(fn_args.args or [])\n+                if args_to_bind > bindable:\n+                    raise TypeError('too many positional arguments')\n+                bindable -= len(fn_args.defaults or [])\n+                if args_to_bind < bindable:\n+                    if kwargs:\n+                        bindable -= len(set(fn_args.args or []) & set(kwargs))\n+                    if bindable > args_to_bind:\n+                        raise TypeError(\"missing a required argument: '{0}'\".format(fn_args.args[args_to_bind]))\n+\n+\n def traverse_obj(obj, *paths, **kwargs):\n     \"\"\"\n     Safely traverse nested `dict`s and `Iterable`s\n@@ -6247,10 +6358,7 @@ def apply_path(start_obj, path, test_type):\n \n             if __debug__ and callable(key):\n                 # Verify function signature\n-                args = inspect.getargspec(key)\n-                if len(args.args) != 2:\n-                    # crash differently in 2.6 !\n-                    inspect.getcallargs(key, None, None)\n+                _try_bind_args(key, None, None)\n \n             new_objs = []\n             for obj in objs:\n", "test_patch": "diff --git a/test/test_InfoExtractor.py b/test/test_InfoExtractor.py\nindex 6d25441db14..34773fbd071 100644\n--- a/test/test_InfoExtractor.py\n+++ b/test/test_InfoExtractor.py\n@@ -7,15 +7,33 @@\n import os\n import sys\n import unittest\n+\n sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n \n-from test.helper import FakeYDL, expect_dict, expect_value, http_server_port\n-from youtube_dl.compat import compat_etree_fromstring, compat_http_server\n-from youtube_dl.extractor.common import InfoExtractor\n-from youtube_dl.extractor import YoutubeIE, get_info_extractor\n-from youtube_dl.utils import encode_data_uri, strip_jsonp, ExtractorError, RegexNotFoundError\n import threading\n \n+from test.helper import (\n+    expect_dict,\n+    expect_value,\n+    FakeYDL,\n+    http_server_port,\n+)\n+from youtube_dl.compat import (\n+    compat_etree_fromstring,\n+    compat_http_server,\n+)\n+from youtube_dl.extractor.common import InfoExtractor\n+from youtube_dl.extractor import (\n+    get_info_extractor,\n+    YoutubeIE,\n+)\n+from youtube_dl.utils import (\n+    encode_data_uri,\n+    ExtractorError,\n+    RegexNotFoundError,\n+    strip_jsonp,\n+)\n+\n \n TEAPOT_RESPONSE_STATUS = 418\n TEAPOT_RESPONSE_BODY = \"<h1>418 I'm a teapot</h1>\"\n@@ -100,6 +118,71 @@ def test_html_search_meta(self):\n         self.assertRaises(RegexNotFoundError, ie._html_search_meta, 'z', html, None, fatal=True)\n         self.assertRaises(RegexNotFoundError, ie._html_search_meta, ('z', 'x'), html, None, fatal=True)\n \n+    def test_search_nextjs_data(self):\n+        html = '''\n+<!DOCTYPE html>\n+<html>\n+<head>\n+  <meta http-equiv=\"content-type\" content=\n+  \"text/html; charset=utf-8\">\n+  <meta name=\"viewport\" content=\"width=device-width\">\n+  <title>Test _search_nextjs_data()</title>\n+</head>\n+<body>\n+  <div id=\"__next\">\n+    <div style=\"background-color:#17171E\" class=\"FU\" dir=\"ltr\">\n+      <div class=\"sc-93de261d-0 dyzzYE\">\n+        <div>\n+          <header class=\"HD\"></header>\n+          <main class=\"MN\">\n+            <div style=\"height:0\" class=\"HT0\">\n+              <div style=\"width:NaN%\" data-testid=\n+              \"stream-container\" class=\"WDN\"></div>\n+            </div>\n+          </main>\n+        </div>\n+        <footer class=\"sc-6e5faf91-0 dEGaHS\"></footer>\n+      </div>\n+    </div>\n+  </div>\n+  <script id=\"__NEXT_DATA__\" type=\"application/json\">\n+  {\"props\":{\"pageProps\":{\"video\":{\"id\":\"testid\"}}}}\n+  </script>\n+</body>\n+</html>\n+'''\n+        search = self.ie._search_nextjs_data(html, 'testID')\n+        self.assertEqual(search['props']['pageProps']['video']['id'], 'testid')\n+\n+    def test_search_nuxt_data(self):\n+        html = '''\n+<!DOCTYPE html>\n+<html>\n+<head>\n+  <meta http-equiv=\"content-type\" content=\n+  \"text/html; charset=utf-8\">\n+  <title>Nuxt.js Test Page</title>\n+  <meta name=\"viewport\" content=\n+  \"width=device-width, initial-scale=1\">\n+  <meta data-hid=\"robots\" name=\"robots\" content=\"all\">\n+</head>\n+<body class=\"BD\">\n+  <div id=\"__layout\">\n+    <h1 class=\"H1\">Example heading</h1>\n+    <div class=\"IN\">\n+      <p>Decoy text</p>\n+    </div>\n+  </div>\n+  <script>\n+  window.__NUXT__=(function(a,b,c,d,e,f,g,h){return {decoy:\" default\",data:[{track:{id:f,title:g}}]}}(null,null,\"c\",null,null,\"testid\",\"Nuxt.js title\",null));\n+  </script>\n+  <script src=\"/_nuxt/a12345b.js\" defer=\"defer\"></script>\n+</body>\n+</html>\n+'''\n+        search = self.ie._search_nuxt_data(html, 'testID')\n+        self.assertEqual(search['track']['id'], 'testid')\n+\n     def test_search_json_ld_realworld(self):\n         # https://github.com/ytdl-org/youtube-dl/issues/23306\n         expect_dict(\n@@ -348,6 +431,24 @@ def test_parse_html5_media_entries(self):\n                 }],\n             })\n \n+        # from https://0000.studio/\n+        # with type attribute but without extension in URL\n+        expect_dict(\n+            self,\n+            self.ie._parse_html5_media_entries(\n+                'https://0000.studio',\n+                r'''\n+                <video src=\"https://d1ggyt9m8pwf3g.cloudfront.net/protected/ap-northeast-1:1864af40-28d5-492b-b739-b32314b1a527/archive/clip/838db6a7-8973-4cd6-840d-8517e4093c92\"\n+                    controls=\"controls\" type=\"video/mp4\" preload=\"metadata\" autoplay=\"autoplay\" playsinline class=\"object-contain\">\n+                </video>\n+                ''', None)[0],\n+            {\n+                'formats': [{\n+                    'url': 'https://d1ggyt9m8pwf3g.cloudfront.net/protected/ap-northeast-1:1864af40-28d5-492b-b739-b32314b1a527/archive/clip/838db6a7-8973-4cd6-840d-8517e4093c92',\n+                    'ext': 'mp4',\n+                }],\n+            })\n+\n     def test_extract_jwplayer_data_realworld(self):\n         # from http://www.suffolk.edu/sjc/\n         expect_dict(\ndiff --git a/test/test_utils.py b/test/test_utils.py\nindex 2ee727caf2b..e83977f298f 100644\n--- a/test/test_utils.py\n+++ b/test/test_utils.py\n@@ -20,7 +20,7 @@\n from youtube_dl.utils import (\n     age_restricted,\n     args_to_str,\n-    encode_base_n,\n+    base_url,\n     caesar,\n     clean_html,\n     clean_podcast_url,\n@@ -29,10 +29,12 @@\n     detect_exe_version,\n     determine_ext,\n     dict_get,\n+    encode_base_n,\n     encode_compat_str,\n     encodeFilename,\n     escape_rfc3986,\n     escape_url,\n+    expand_path,\n     extract_attributes,\n     ExtractorError,\n     find_xpath_attr,\n@@ -51,6 +53,7 @@\n     js_to_json,\n     LazyList,\n     limit_length,\n+    lowercase_escape,\n     merge_dicts,\n     mimetype2ext,\n     month_by_name,\n@@ -66,17 +69,16 @@\n     parse_resolution,\n     parse_bitrate,\n     pkcs1pad,\n-    read_batch_urls,\n-    sanitize_filename,\n-    sanitize_path,\n-    sanitize_url,\n-    expand_path,\n     prepend_extension,\n-    replace_extension,\n+    read_batch_urls,\n     remove_start,\n     remove_end,\n     remove_quotes,\n+    replace_extension,\n     rot47,\n+    sanitize_filename,\n+    sanitize_path,\n+    sanitize_url,\n     shell_quote,\n     smuggle_url,\n     str_or_none,\n@@ -93,10 +95,8 @@\n     unified_timestamp,\n     unsmuggle_url,\n     uppercase_escape,\n-    lowercase_escape,\n     url_basename,\n     url_or_none,\n-    base_url,\n     urljoin,\n     urlencode_postdata,\n     urshift,\n@@ -905,6 +905,85 @@ def test_escape_url(self):\n         )\n         self.assertEqual(escape_url('http://vimeo.com/56015672#at=0'), 'http://vimeo.com/56015672#at=0')\n \n+    def test_js_to_json_vars_strings(self):\n+        self.assertDictEqual(\n+            json.loads(js_to_json(\n+                '''{\n+                    'null': a,\n+                    'nullStr': b,\n+                    'true': c,\n+                    'trueStr': d,\n+                    'false': e,\n+                    'falseStr': f,\n+                    'unresolvedVar': g,\n+                }''',\n+                {\n+                    'a': 'null',\n+                    'b': '\"null\"',\n+                    'c': 'true',\n+                    'd': '\"true\"',\n+                    'e': 'false',\n+                    'f': '\"false\"',\n+                    'g': 'var',\n+                }\n+            )),\n+            {\n+                'null': None,\n+                'nullStr': 'null',\n+                'true': True,\n+                'trueStr': 'true',\n+                'false': False,\n+                'falseStr': 'false',\n+                'unresolvedVar': 'var'\n+            }\n+        )\n+\n+        self.assertDictEqual(\n+            json.loads(js_to_json(\n+                '''{\n+                    'int': a,\n+                    'intStr': b,\n+                    'float': c,\n+                    'floatStr': d,\n+                }''',\n+                {\n+                    'a': '123',\n+                    'b': '\"123\"',\n+                    'c': '1.23',\n+                    'd': '\"1.23\"',\n+                }\n+            )),\n+            {\n+                'int': 123,\n+                'intStr': '123',\n+                'float': 1.23,\n+                'floatStr': '1.23',\n+            }\n+        )\n+\n+        self.assertDictEqual(\n+            json.loads(js_to_json(\n+                '''{\n+                    'object': a,\n+                    'objectStr': b,\n+                    'array': c,\n+                    'arrayStr': d,\n+                }''',\n+                {\n+                    'a': '{}',\n+                    'b': '\"{}\"',\n+                    'c': '[]',\n+                    'd': '\"[]\"',\n+                }\n+            )),\n+            {\n+                'object': {},\n+                'objectStr': '{}',\n+                'array': [],\n+                'arrayStr': '[]',\n+            }\n+        )\n+\n     def test_js_to_json_realworld(self):\n         inp = '''{\n             'clip':{'provider':'pseudo'}\n@@ -975,10 +1054,10 @@ def test_js_to_json_edgecases(self):\n             !42: 42\n         }''')\n         self.assertEqual(json.loads(on), {\n-            'a': 0,\n-            'b': 1,\n-            'c': 0,\n-            'd': 42.42,\n+            'a': True,\n+            'b': False,\n+            'c': False,\n+            'd': True,\n             'e': [],\n             'f': \"abc\",\n             'g': \"\",\n@@ -1048,10 +1127,26 @@ def test_js_to_json_edgecases(self):\n         on = js_to_json('{ \"040\": \"040\" }')\n         self.assertEqual(json.loads(on), {'040': '040'})\n \n+        on = js_to_json('[1,//{},\\n2]')\n+        self.assertEqual(json.loads(on), [1, 2])\n+\n+        on = js_to_json(r'\"\\^\\$\\#\"')\n+        self.assertEqual(json.loads(on), R'^$#', msg='Unnecessary escapes should be stripped')\n+\n+        on = js_to_json('\\'\"\\\\\"\"\\'')\n+        self.assertEqual(json.loads(on), '\"\"\"', msg='Unnecessary quote escape should be escaped')\n+\n     def test_js_to_json_malformed(self):\n         self.assertEqual(js_to_json('42a1'), '42\"a1\"')\n         self.assertEqual(js_to_json('42a-1'), '42\"a\"-1')\n \n+    def test_js_to_json_template_literal(self):\n+        self.assertEqual(js_to_json('`Hello ${name}`', {'name': '\"world\"'}), '\"Hello world\"')\n+        self.assertEqual(js_to_json('`${name}${name}`', {'name': '\"X\"'}), '\"XX\"')\n+        self.assertEqual(js_to_json('`${name}${name}`', {'name': '5'}), '\"55\"')\n+        self.assertEqual(js_to_json('`${name}\"${name}\"`', {'name': '5'}), '\"5\\\\\"5\\\\\"\"')\n+        self.assertEqual(js_to_json('`${name}`', {}), '\"name\"')\n+\n     def test_extract_attributes(self):\n         self.assertEqual(extract_attributes('<e x=\"y\">'), {'x': 'y'})\n         self.assertEqual(extract_attributes(\"<e x='y'>\"), {'x': 'y'})\n@@ -1586,6 +1681,11 @@ def test_traverse_obj(self):\n             'dict': {},\n         }\n \n+        # define a pukka Iterable\n+        def iter_range(stop):\n+            for from_ in range(stop):\n+                yield from_\n+\n         # Test base functionality\n         self.assertEqual(traverse_obj(_TEST_DATA, ('str',)), 'str',\n                          msg='allow tuple path')\n@@ -1602,13 +1702,13 @@ def test_traverse_obj(self):\n         # Test Ellipsis behavior\n         self.assertCountEqual(traverse_obj(_TEST_DATA, Ellipsis),\n                               (item for item in _TEST_DATA.values() if item not in (None, {})),\n-                              msg='`...` should give all non discarded values')\n+                              msg='`...` should give all non-discarded values')\n         self.assertCountEqual(traverse_obj(_TEST_DATA, ('urls', 0, Ellipsis)), _TEST_DATA['urls'][0].values(),\n                               msg='`...` selection for dicts should select all values')\n         self.assertEqual(traverse_obj(_TEST_DATA, (Ellipsis, Ellipsis, 'url')),\n                          ['https://www.example.com/0', 'https://www.example.com/1'],\n                          msg='nested `...` queries should work')\n-        self.assertCountEqual(traverse_obj(_TEST_DATA, (Ellipsis, Ellipsis, 'index')), range(4),\n+        self.assertCountEqual(traverse_obj(_TEST_DATA, (Ellipsis, Ellipsis, 'index')), iter_range(4),\n                               msg='`...` query result should be flattened')\n         self.assertEqual(traverse_obj(iter(range(4)), Ellipsis), list(range(4)),\n                          msg='`...` should accept iterables')\n@@ -1618,7 +1718,7 @@ def test_traverse_obj(self):\n                          [_TEST_DATA['urls']],\n                          msg='function as query key should perform a filter based on (key, value)')\n         self.assertCountEqual(traverse_obj(_TEST_DATA, lambda _, x: isinstance(x[0], str)), set(('str',)),\n-                              msg='exceptions in the query function should be catched')\n+                              msg='exceptions in the query function should be caught')\n         self.assertEqual(traverse_obj(iter(range(4)), lambda _, x: x % 2 == 0), [0, 2],\n                          msg='function key should accept iterables')\n         if __debug__:\n@@ -1706,7 +1806,7 @@ def test_traverse_obj(self):\n         self.assertEqual(traverse_obj(_TEST_DATA, {0: 'dict'}), {},\n                          msg='remove empty values when dict key')\n         self.assertEqual(traverse_obj(_TEST_DATA, {0: 'dict'}, default=Ellipsis), {0: Ellipsis},\n-                         msg='use `default` when dict key and `default`')\n+                         msg='use `default` when dict key and a default')\n         self.assertEqual(traverse_obj(_TEST_DATA, {0: {0: 'fail'}}), {},\n                          msg='remove empty values when nested dict key fails')\n         self.assertEqual(traverse_obj(None, {0: 'fail'}), {},\n@@ -1768,7 +1868,7 @@ def test_traverse_obj(self):\n         self.assertEqual(traverse_obj(_EXPECTED_TYPE_DATA, 'str', expected_type=str),\n                          'str', msg='accept matching `expected_type` type')\n         self.assertEqual(traverse_obj(_EXPECTED_TYPE_DATA, 'str', expected_type=int),\n-                         None, msg='reject non matching `expected_type` type')\n+                         None, msg='reject non-matching `expected_type` type')\n         self.assertEqual(traverse_obj(_EXPECTED_TYPE_DATA, 'int', expected_type=lambda x: str(x)),\n                          '0', msg='transform type using type function')\n         self.assertEqual(traverse_obj(_EXPECTED_TYPE_DATA, 'str', expected_type=lambda _: 1 / 0),\n@@ -1780,7 +1880,7 @@ def test_traverse_obj(self):\n         self.assertEqual(traverse_obj(_TEST_DATA, {0: 100, 1: 1.2, 2: 'None'}, expected_type=str_or_none),\n                          {0: '100', 1: '1.2'}, msg='function as expected_type should transform dict values')\n         self.assertEqual(traverse_obj(_TEST_DATA, ({0: 1.2}, 0, set((int_or_none,))), expected_type=int),\n-                         1, msg='expected_type should not filter non final dict values')\n+                         1, msg='expected_type should not filter non-final dict values')\n         self.assertEqual(traverse_obj(_TEST_DATA, {0: {0: 100, 1: 'str'}}, expected_type=int),\n                          {0: {0: 100}}, msg='expected_type should transform deep dict values')\n         self.assertEqual(traverse_obj(_TEST_DATA, [({0: '...'}, {0: '...'})], expected_type=type(Ellipsis)),\n@@ -1838,7 +1938,7 @@ def test_traverse_obj(self):\n         self.assertEqual(traverse_obj(_TRAVERSE_STRING_DATA, ('str', slice(0, None, 2)),\n                                       _traverse_string=True), 'sr',\n                          msg='`slice` should result in string if `traverse_string`')\n-        self.assertEqual(traverse_obj(_TRAVERSE_STRING_DATA, ('str', lambda i, v: i or v == \"s\"),\n+        self.assertEqual(traverse_obj(_TRAVERSE_STRING_DATA, ('str', lambda i, v: i or v == 's'),\n                                       _traverse_string=True), 'str',\n                          msg='function should result in string if `traverse_string`')\n         self.assertEqual(traverse_obj(_TRAVERSE_STRING_DATA, ('str', (0, 2)),\n", "problem_statement": "GlobalPlayer\n<!--\r\n\r\n######################################################################\r\n  WARNING!\r\n  IGNORING THE FOLLOWING TEMPLATE WILL RESULT IN ISSUE CLOSED AS INCOMPLETE\r\n######################################################################\r\n\r\n-->\r\n\r\n\r\n## Checklist\r\n\r\n<!--\r\nCarefully read and work through this check list in order to prevent the most common mistakes and misuse of youtube-dl:\r\n- First of, make sure you are using the latest version of youtube-dl. Run `youtube-dl --version` and ensure your version is 2021.12.17. If it's not, see https://yt-dl.org/update on how to update. Issues with outdated version will be REJECTED.\r\n- Make sure that all provided video/audio/playlist URLs (if any) are alive and playable in a browser.\r\n- Make sure that site you are requesting is not dedicated to copyright infringement, see https://yt-dl.org/copyright-infringement. youtube-dl does not support such sites. In order for site support request to be accepted all provided example URLs should not violate any copyrights.\r\n- Search the bugtracker for similar site support requests: http://yt-dl.org/search-issues. DO NOT post duplicates.\r\n- Finally, put x into all relevant boxes (like this [x])\r\n-->\r\n\r\n- [x] I'm reporting a new site support request\r\n- [x] I've verified that I'm running youtube-dl version **2021.12.17**\r\n- [x] I've checked that all provided URLs are alive and playable in a browser\r\n- [x] I've checked that none of provided URLs violate any copyrights\r\n- [x] I've searched the bugtracker for similar site support requests including closed ones\r\n\r\n\r\n## Example URLs\r\n\r\n<!--\r\nProvide all kinds of example URLs support for which should be included. Replace following example URLs by yours.\r\n-->\r\n\r\n- Single video: \r\n- Single video: https://www.globalplayer.com/podcasts/episodes/7Drf561/\r\n- Playlist: https://www.globalplayer.com/podcasts/42KnZn/\r\n\r\n\r\n## Description\r\n\r\n<!--\r\nProvide any additional information.\r\nIf work on your issue requires account credentials please provide them or explain how one can obtain them.\r\n-->\r\n\r\n\r\nPaywall: NO\r\nRegion Blocked: UNKNOWN\r\nRegion: GREAT BRITAIN \r\nCredentials Required: YES\r\nUsername: no-reply00456temp@treadspear.com\r\nPassword: no-reply00456temp@treadspear.com\r\nRegistration: https://www.globalplayer.com/register/\r\n\r\n\r\nI am a new to the youtube-dl project and I am very impressed so far. I would like to make a script to archive some podcasts on the Global Player. Does anyone have some recommendations I can try to do this?\r\n\r\nThank you,\ninspect.getargspec() is removed in Python 3.11\n<!--\r\n\r\n######################################################################\r\n  WARNING!\r\n  IGNORING THE FOLLOWING TEMPLATE WILL RESULT IN ISSUE CLOSED AS INCOMPLETE\r\n######################################################################\r\n\r\n-->\r\n\r\n\r\n## Checklist\r\n\r\n<!--\r\nCarefully read and work through this check list in order to prevent the most common mistakes and misuse of youtube-dl:\r\n- First of, make sure you are using the latest version of youtube-dl. Run `youtube-dl --version` and ensure your version is 2021.12.17. If it's not, see https://yt-dl.org/update on how to update. Issues with outdated version will be REJECTED.\r\n- Make sure that all provided video/audio/playlist URLs (if any) are alive and playable in a browser.\r\n- Make sure that all URLs and arguments with special characters are properly quoted or escaped as explained in http://yt-dl.org/escape.\r\n- Search the bugtracker for similar issues: http://yt-dl.org/search-issues. DO NOT post duplicates.\r\n- Read bugs section in FAQ: http://yt-dl.org/reporting\r\n- Finally, put x into all relevant boxes (like this [x])\r\n-->\r\n\r\n- [ ] I'm reporting a broken site support issue\r\n- [x] I've verified that I'm running youtube-dl version **2021.12.17**\r\n- [x] I've checked that all provided URLs are alive and playable in a browser\r\n- [x] I've checked that all URLs and arguments with special characters are properly quoted or escaped\r\n- [x] I've searched the bugtracker for similar bug reports including closed ones\r\n- [x] I've read bugs section in FAQ\r\n\r\n\r\n\r\n## Description\r\n\r\n<!--\r\nProvide an explanation of your issue in an arbitrary form. Please make sure the description is worded well enough to be understood, see https://github.com/ytdl-org/youtube-dl#is-the-description-of-the-issue-itself-sufficient. Provide any additional information, suggested solution and as much context and examples as possible.\r\nIf work on your issue requires account credentials please provide them or explain how one can obtain them.\r\n-->\r\n\r\n\r\n\r\nhttps://github.com/ytdl-org/youtube-dl/blob/47214e46d852e9d7ddf81d69a8e70806e2396c6c/youtube_dl/utils.py#L6250-L6253\r\ncauses\r\n```\r\nERROR: module 'inspect' has no attribute 'getargspec'\r\n```\r\nbecause `inspect.getargspec()` is removed in Python 3.11 (see https://docs.python.org/3/whatsnew/3.11.html)\r\n> Removed from the [inspect](https://docs.python.org/3/library/inspect.html#module-inspect) module:\r\n  >  The getargspec() function, deprecated since Python 3.0; use [inspect.signature()](https://docs.python.org/3/library/inspect.html#inspect.signature) or [inspect.getfullargspec()](https://docs.python.org/3/library/inspect.html#inspect.getfullargspec) instead.\n", "hints_text": "Their site requires **an account/log-in** for audio and/or video content to start playing... That doesn't mean the same is true for `youtube-dl` \ud83d\ude09 ...\r\nSince this is a **UK-based service**, some content might be geo-fenced  from overseas...\r\n\r\nI find that the **generic** extractor can handle the \"single video\" (actually, it's **audio-only**) URI the OP provided: \r\n```cmd\r\nyt-dl -v -c --no-part --write-description --write-thumbnail --add-metadata --embed-thumbnail \"https://www.globalplayer.com/podcasts/episodes/7Drf561/\" => \r\n\r\n[debug] System config: []\r\n[debug] User config: []\r\n[debug] Custom config: []\r\n[debug] Command-line args: ['--ffmpeg-location', '.\\\\FFmpeg', '--external-downloader-args', '-v 8 -stats', '-v', '-c', '--no-part', '--write-description', '--write-thumbnail', '--add-metadata', '--embed-thumbnail', 'https://www.globalplayer.com/podcasts/episodes/7Drf561/']\r\n[debug] Encodings: locale cp1253, fs mbcs, out cp737, pref cp1253\r\n[debug] youtube-dl version 2023.02.24.43044\r\n[debug] Python version 3.4.4 (CPython) - Windows-Vista-6.0.6003-SP2\r\n[debug] exe versions: ffmpeg n5.2-dev-2245-N-109649-gab8cde6, ffprobe n5.2-dev-2245-N-109649-gab8cde6, phantomjs 2.1.1, rtmpdump 2.4\r\n[debug] Proxy map: {}\r\n[generic] 7Drf561: Requesting header\r\nWARNING: Falling back on generic information extractor.\r\n[generic] 7Drf561: Downloading webpage\r\n[generic] 7Drf561: Extracting information\r\n[debug] Default format spec: bestvideo+bestaudio/best\r\nWARNING: There's no description to write.\r\n[debug] Invoking downloader on 'https://dax.captivate.fm/637d3b2b-41eb-440d-ab2c-76d95c427ee2/LBC-UK-08022023-035818-08022023-043316.mp3?aw_0_1st.showid=4fb1b093-d258-4b00-ab19-4ffa3debb6cc\\\\u0026aw_0_1st.episodeid=185ae231-a952-41e9-9d29-eafe15192903'\r\n[download] Destination: Steve Allen - The Whole Show - Podcast _ Global Player-u0026aw_0_1st.mp3\r\n[download] 100% of 126.09MiB in 02:35\r\n[ffmpeg] Adding metadata to 'Steve Allen - The Whole Show - Podcast _ Global Player-u0026aw_0_1st.mp3'\r\n[debug] ffmpeg command line: \".\\FFmpeg\\ffmpeg\" -y -loglevel \"repeat+info\" -i \"file:Steve Allen - The Whole Show - Podcast _ Global Player-u0026aw_0_1st.mp3\" -ccopy -metadata \"purl=https://www.globalplayer.com/podcasts/episodes/7Drf561/\" -metadata \"title=Steve Allen - The Whole Show - Podcast | Global Player\" -metadata \"artist=www.globalplayer.com\" \"file:Steve Allen - The Whole Show - Podcast _ Global Player-u0026aw_0_1st.temp.mp3\"\r\n[embedthumbnail] There aren't any thumbnails to embed\r\n```\r\nOverseas location, thus not being geo-fenced for that particular item... \r\nFWIW, a proper IE has to be authored if one wants:\r\na) more meaningful/correct filenames (e.g. including radio station name, date of first broadcast, etc.)\r\nb) complete description/file metadata\r\nc) cover/thumbnail...\nThe page tested above has:\r\n1. ld+json that we ought to be extracting once the common code recognises a `PodcastEpisode` (also a `RadioEpisode`) as a sort of `Episode`\r\n2. almost the same metadata in `og:...` `<meta>` tags\r\n2. Next.js hydration JSON that could be extracted by a dedicated extractor.\r\n\r\nCurrently, I expect (without actually testing) that this catch-all in the generic extractor is finding the media link in the hydration JSON:\r\n```py\r\n            found = filter_video(re.findall(\r\n                r'[^A-Za-z0-9]?(?:file|video_url)[\"\\']?:\\s*[\"\\'](http(?![^\\'\"]+\\.[0-9]+[\\'\"])[^\\'\"]+)[\"\\']', webpage))\r\n```\r\n\n... It's not only MP3 **podcasts**  that are offered via `GlobalPlayer`, it's a full-fledged **catch-up** service for (most) radio programmes broadcast over the [Global Group](https://en.wikipedia.org/wiki/List_of_Global_Group_stations) (commercial) radio network in the UK \ud83d\ude09 ... \r\n\r\nAOD links, e.g. for **CapitalFM**, are like below: \r\n\r\nhttps://www.globalplayer.com/catchup/capital/uk/episodes/2zGrsPHALvZrD1F7DB837dwSFp/\r\n\r\nYou'll notice the different structure of the URI compared to the \"podcast one\" found in OP; `catch-up` has a limited availability time frame of only 7 days after first broadcast, audio is served as a lowly `HE-AACv1@48kbps` encode, inside the MP4 container (with Apple's `.m4a` file extension for audio): \r\n```shell\r\nyt-dl \"https://www.globalplayer.com/catchup/capital/uk/episodes/2zGrsPHALvZrD1F7DB837dwSFp/\" => \r\n\r\n[generic] 2zGrsPHALvZrD1F7DB837dwSFp: Requesting header\r\nWARNING: Falling back on generic information extractor.\r\n[generic] 2zGrsPHALvZrD1F7DB837dwSFp: Downloading webpage\r\n[generic] 2zGrsPHALvZrD1F7DB837dwSFp: Extracting information\r\n[download] Destination: The Sky VIP Official Big Top 40 _ Global Player-u0026aw_0_1st.m4a\r\n[download] 100% of 61.13MiB in 02:02\r\n```\r\nHopefully, support for **non-podcast** AOD can be added, too... \ud83d\ude04 \nThe AOD examples lack the ld+json but the page structure is otherwise similar.\n> FWIW, a proper IE has to be authored if one wants:\r\na) more meaningful/correct filenames (e.g. including radio station name, date of first broadcast, etc.)\r\nb) complete description/file metadata\r\nc) cover/thumbnail...\r\n\r\nRecent \"downstream\" implementation: \r\n\r\nhttps://github.com/yt-dlp/yt-dlp/pull/6903\r\n\r\nmerged as https://github.com/yt-dlp/yt-dlp/commit/30647668a92a0ca5cd108776804baac0996bd9f7 \ud83d\ude09 ...\nNot a tricky back-port, but needs some core back-ports to be merged first:\r\n```console\r\n$ pytest -k GlobalPlayer============================= test session starts ==============================\r\nplatform linux2 -- Python 2.7.18, pytest-4.6.11, py-1.11.0, pluggy-0.13.1\r\nrootdir: /home/df/Documents/src/youtube-dl\r\ncollected 2621 items / 2613 deselected / 8 selected                            \r\n\r\ntest/test_download.py ........                                           [100%]\r\n\r\n================== 8 passed, 2613 deselected in 6.22 seconds ===================\r\n$\r\n```\n", "created_at": "2023-05-03T14:11:49Z"}
{"repo": "ytdl-org/youtube-dl", "pull_number": 31675, "instance_id": "ytdl-org__youtube-dl-31675", "issue_numbers": ["31568", "31530"], "base_commit": "1d3751c3fe50b203d3e2bff71d866c8c500f8288", "patch": "diff --git a/youtube_dl/extractor/youtube.py b/youtube_dl/extractor/youtube.py\nindex 66b0257df46..4246d84f9e5 100644\n--- a/youtube_dl/extractor/youtube.py\n+++ b/youtube_dl/extractor/youtube.py\n@@ -31,6 +31,7 @@\n     get_element_by_attribute,\n     int_or_none,\n     js_to_json,\n+    merge_dicts,\n     mimetype2ext,\n     parse_codecs,\n     parse_duration,\n@@ -400,6 +401,62 @@ def _search_results(self, query, params):\n                 break\n             data['continuation'] = token\n \n+    @staticmethod\n+    def _owner_endpoints_path():\n+        return [\n+            Ellipsis,\n+            lambda k, _: k.endswith('SecondaryInfoRenderer'),\n+            ('owner', 'videoOwner'), 'videoOwnerRenderer', 'title',\n+            'runs', Ellipsis]\n+\n+    def _extract_channel_id(self, webpage, videodetails={}, metadata={}, renderers=[]):\n+        channel_id = None\n+        if any((videodetails, metadata, renderers)):\n+            channel_id = (\n+                traverse_obj(videodetails, 'channelId')\n+                or traverse_obj(metadata, 'externalChannelId', 'externalId')\n+                or traverse_obj(renderers,\n+                                self._owner_endpoints_path() + [\n+                                    'navigationEndpoint', 'browseEndpoint', 'browseId'],\n+                                get_all=False)\n+            )\n+        return channel_id or self._html_search_meta(\n+            'channelId', webpage, 'channel id', default=None)\n+\n+    def _extract_author_var(self, webpage, var_name,\n+                            videodetails={}, metadata={}, renderers=[]):\n+        result = None\n+        paths = {\n+            #       (HTML, videodetails, metadata, renderers)\n+            'name': ('content', 'author', (('ownerChannelName', None), 'title'), ['text']),\n+            'url': ('href', 'ownerProfileUrl', 'vanityChannelUrl',\n+                    ['navigationEndpoint', 'browseEndpoint', 'canonicalBaseUrl'])\n+        }\n+        if any((videodetails, metadata, renderers)):\n+            result = (\n+                traverse_obj(videodetails, paths[var_name][1], get_all=False)\n+                or traverse_obj(metadata, paths[var_name][2], get_all=False)\n+                or traverse_obj(renderers,\n+                                self._owner_endpoints_path() + paths[var_name][3],\n+                                get_all=False)\n+            )\n+        return result or traverse_obj(\n+            extract_attributes(self._search_regex(\n+                r'''(?s)(<link\\b[^>]+\\bitemprop\\s*=\\s*(\"|')%s\\2[^>]*>)'''\n+                % re.escape(var_name),\n+                get_element_by_attribute('itemprop', 'author', webpage) or '',\n+                'author link', default='')),\n+            paths[var_name][0])\n+\n+    @staticmethod\n+    def _yt_urljoin(url_or_path):\n+        return urljoin('https://www.youtube.com', url_or_path)\n+\n+    def _extract_uploader_id(self, uploader_url):\n+        return self._search_regex(\n+            r'/(?:(?:channel|user)/|(?=@))([^/?&#]+)', uploader_url or '',\n+            'uploader id', default=None)\n+\n \n class YoutubeIE(YoutubeBaseInfoExtractor):\n     IE_DESC = 'YouTube.com'\n@@ -516,8 +573,8 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'ext': 'mp4',\n                 'title': 'youtube-dl test video \"\\'/\\\\\u00e4\u21ad\ud835\udd50',\n                 'uploader': 'Philipp Hagemeister',\n-                'uploader_id': 'phihag',\n-                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/user/phihag',\n+                'uploader_id': '@PhilippHagemeister',\n+                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@PhilippHagemeister',\n                 'channel': 'Philipp Hagemeister',\n                 'channel_id': 'UCLqxVugv74EIW3VWh2NOa3Q',\n                 'channel_url': r're:https?://(?:www\\.)?youtube\\.com/channel/UCLqxVugv74EIW3VWh2NOa3Q',\n@@ -557,8 +614,8 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'ext': 'mp4',\n                 'title': 'youtube-dl test video \"\\'/\\\\\u00e4\u21ad\ud835\udd50',\n                 'uploader': 'Philipp Hagemeister',\n-                'uploader_id': 'phihag',\n-                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/user/phihag',\n+                'uploader_id': '@PhilippHagemeister',\n+                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@PhilippHagemeister',\n                 'upload_date': '20121002',\n                 'description': 'test chars:  \"\\'/\\\\\u00e4\u21ad\ud835\udd50\\ntest URL: https://github.com/rg3/youtube-dl/issues/1892\\n\\nThis is a test video for youtube-dl.\\n\\nFor more information, contact phihag@phihag.de .',\n                 'categories': ['Science & Technology'],\n@@ -588,7 +645,7 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'youtube_include_dash_manifest': True,\n                 'format': '141',\n             },\n-            'skip': 'format 141 not served anymore',\n+            'skip': 'format 141 not served any more',\n         },\n         # DASH manifest with encrypted signature\n         {\n@@ -600,7 +657,7 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'description': 'md5:8f5e2b82460520b619ccac1f509d43bf',\n                 'duration': 244,\n                 'uploader': 'AfrojackVEVO',\n-                'uploader_id': 'AfrojackVEVO',\n+                'uploader_id': '@AfrojackVEVO',\n                 'upload_date': '20131011',\n                 'abr': 129.495,\n             },\n@@ -618,8 +675,8 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'duration': 219,\n                 'upload_date': '20100909',\n                 'uploader': 'Amazing Atheist',\n-                'uploader_id': 'TheAmazingAtheist',\n-                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/user/TheAmazingAtheist',\n+                'uploader_id': '@theamazingatheist',\n+                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@theamazingatheist',\n                 'title': 'Burning Everyone\\'s Koran',\n                 'description': 'SUBSCRIBE: http://www.youtube.com/saturninefilms \\r\\n\\r\\nEven Obama has taken a stand against freedom on this issue: http://www.huffingtonpost.com/2010/09/09/obama-gma-interview-quran_n_710282.html',\n             }\n@@ -635,8 +692,8 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'description': r're:(?s).{100,}About the Game\\n.*?The Witcher 3: Wild Hunt.{100,}',\n                 'duration': 142,\n                 'uploader': 'The Witcher',\n-                'uploader_id': 'WitcherGame',\n-                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/user/WitcherGame',\n+                'uploader_id': '@thewitcher',\n+                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@thewitcher',\n                 'upload_date': '20140605',\n                 'thumbnail': 'https://i.ytimg.com/vi/HtVdAasjOgU/maxresdefault.jpg',\n                 'age_limit': 18,\n@@ -659,7 +716,7 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'description': 'md5:bf77e03fcae5529475e500129b05668a',\n                 'duration': 177,\n                 'uploader': 'FlyingKitty',\n-                'uploader_id': 'FlyingKitty900',\n+                'uploader_id': '@FlyingKitty900',\n                 'upload_date': '20200408',\n                 'thumbnail': 'https://i.ytimg.com/vi/HsUATh_Nc2U/maxresdefault.jpg',\n                 'age_limit': 18,\n@@ -682,7 +739,7 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'description': 'md5:17eccca93a786d51bc67646756894066',\n                 'duration': 106,\n                 'uploader': 'Projekt Melody',\n-                'uploader_id': 'UC1yoRdFoFJaCY-AGfD9W0wQ',\n+                'uploader_id': '@ProjektMelody',\n                 'upload_date': '20191227',\n                 'age_limit': 18,\n                 'thumbnail': 'https://i.ytimg.com/vi/Tq92D6wQ1mg/sddefault.jpg',\n@@ -704,10 +761,10 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'title': 'OOMPH! - Such Mich Find Mich (Lyrics)',\n                 'description': 'Fan Video. Music & Lyrics by OOMPH!.',\n                 'duration': 210,\n-                'uploader': 'Herr Lurik',\n-                'uploader_id': 'st3in234',\n                 'upload_date': '20130730',\n-                'uploader_url': 'http://www.youtube.com/user/st3in234',\n+                'uploader': 'Herr Lurik',\n+                'uploader_id': '@HerrLurik',\n+                'uploader_url': 'http://www.youtube.com/@HerrLurik',\n                 'age_limit': 0,\n                 'thumbnail': 'https://i.ytimg.com/vi/MeJVWBSsPAY/hqdefault.jpg',\n                 'tags': ['oomph', 'such mich find mich', 'lyrics', 'german industrial', 'musica industrial'],\n@@ -740,8 +797,8 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'ext': 'mp4',\n                 'duration': 266,\n                 'upload_date': '20100430',\n-                'uploader_id': 'deadmau5',\n-                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/user/deadmau5',\n+                'uploader_id': '@deadmau5',\n+                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@deadmau5',\n                 'creator': 'deadmau5',\n                 'description': 'md5:6cbcd3a92ce1bc676fc4d6ab4ace2336',\n                 'uploader': 'deadmau5',\n@@ -762,8 +819,8 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'description': r're:(?s)(?:.+\\s)?HO09  - Women -  GER-AUS - Hockey - 31 July 2012 - London 2012 Olympic Games\\s*',\n                 'duration': 6085,\n                 'upload_date': '20150827',\n-                'uploader_id': 'olympic',\n-                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/user/olympic',\n+                'uploader_id': '@Olympics',\n+                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@Olympics',\n                 'uploader': r're:Olympics?',\n                 'age_limit': 0,\n                 'thumbnail': 'https://i.ytimg.com/vi/lqQg6PlCWgI/maxresdefault.jpg',\n@@ -785,8 +842,8 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'stretched_ratio': 16 / 9.,\n                 'duration': 85,\n                 'upload_date': '20110310',\n-                'uploader_id': 'AllenMeow',\n-                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/user/AllenMeow',\n+                'uploader_id': '@AllenMeow',\n+                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@AllenMeow',\n                 'description': 'made by Wacom from Korea | \u5b57\u5e55&\u52a0\u6cb9\u6dfb\u918b by TY\\'s Allen | \u611f\u8b1dheylisa00cavey1001\u540c\u5b78\u71b1\u60c5\u63d0\u4f9b\u6897\u53ca\u7ffb\u8b6f',\n                 'uploader': '\u5b6b\u110b\u1105',\n                 'title': '[A-made] \u8b8a\u614b\u598d\u5b57\u5e55\u7248 \u592a\u598d \u6211\u5c31\u662f\u9019\u6a23\u7684\u4eba',\n@@ -824,7 +881,7 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'uploader': 'dorappi2000',\n                 'formats': 'mincount:31',\n             },\n-            'skip': 'not actual anymore',\n+            'skip': 'not actual any more',\n         },\n         # DASH manifest with segment_list\n         {\n@@ -905,6 +962,7 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n             'params': {\n                 'skip_download': True,\n             },\n+            'skip': 'Not multifeed any more',\n         },\n         {\n             # Multifeed video with comma in title (see https://github.com/ytdl-org/youtube-dl/issues/8536)\n@@ -914,7 +972,7 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'title': 'DevConf.cz 2016 Day 2 Workshops 1 14:00 - 15:30',\n             },\n             'playlist_count': 2,\n-            'skip': 'Not multifeed anymore',\n+            'skip': 'Not multifeed any more',\n         },\n         {\n             'url': 'https://vid.plus/FlRa-iH7PGw',\n@@ -938,8 +996,8 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'description': 'md5:8085699c11dc3f597ce0410b0dcbb34a',\n                 'duration': 133,\n                 'upload_date': '20151119',\n-                'uploader_id': 'IronSoulElf',\n-                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/user/IronSoulElf',\n+                'uploader_id': '@IronSoulElf',\n+                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@IronSoulElf',\n                 'uploader': 'IronSoulElf',\n                 'creator': r're:Todd Haberman[;,]\\s+Daniel Law Heath and Aaron Kaplan',\n                 'track': 'Dark Walk',\n@@ -987,8 +1045,8 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'description': 'md5:a677553cf0840649b731a3024aeff4cc',\n                 'duration': 721,\n                 'upload_date': '20150127',\n-                'uploader_id': 'BerkmanCenter',\n-                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/user/BerkmanCenter',\n+                'uploader_id': '@BKCHarvard',\n+                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@BKCHarvard',\n                 'uploader': 'The Berkman Klein Center for Internet & Society',\n                 'license': 'Creative Commons Attribution license (reuse allowed)',\n             },\n@@ -1007,8 +1065,8 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'duration': 4060,\n                 'upload_date': '20151119',\n                 'uploader': 'Bernie Sanders',\n-                'uploader_id': 'UCH1dpzjCEiGAt8CXkryhkZg',\n-                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/channel/UCH1dpzjCEiGAt8CXkryhkZg',\n+                'uploader_id': '@BernieSanders',\n+                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@BernieSanders',\n                 'license': 'Creative Commons Attribution license (reuse allowed)',\n             },\n             'params': {\n@@ -1054,8 +1112,8 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'duration': 2085,\n                 'upload_date': '20170118',\n                 'uploader': 'Vsauce',\n-                'uploader_id': 'Vsauce',\n-                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/user/Vsauce',\n+                'uploader_id': '@Vsauce',\n+                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@Vsauce',\n                 'series': 'Mind Field',\n                 'season_number': 1,\n                 'episode_number': 1,\n@@ -1134,7 +1192,7 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'skip_download': True,\n                 'youtube_include_dash_manifest': False,\n             },\n-            'skip': 'not actual anymore',\n+            'skip': 'not actual any more',\n         },\n         {\n             # Youtube Music Auto-generated description\n@@ -1191,8 +1249,8 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'title': 'IMG 3456',\n                 'description': '',\n                 'upload_date': '20170613',\n-                'uploader_id': 'ElevageOrVert',\n                 'uploader': 'ElevageOrVert',\n+                'uploader_id': '@ElevageOrVert',\n             },\n             'params': {\n                 'skip_download': True,\n@@ -1210,8 +1268,8 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'title': 'Part 77   Sort a list of simple types in c#',\n                 'description': 'md5:b8746fa52e10cdbf47997903f13b20dc',\n                 'upload_date': '20130831',\n-                'uploader_id': 'kudvenkat',\n                 'uploader': 'kudvenkat',\n+                'uploader_id': '@Csharp-video-tutorialsBlogspot',\n             },\n             'params': {\n                 'skip_download': True,\n@@ -1263,8 +1321,8 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'description': 'md5:ea770e474b7cd6722b4c95b833c03630',\n                 'upload_date': '20201120',\n                 'uploader': 'Walk around Japan',\n-                'uploader_id': 'UC3o_t8PzBmXf5S9b7GLx1Mw',\n-                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/channel/UC3o_t8PzBmXf5S9b7GLx1Mw',\n+                'uploader_id': '@walkaroundjapan7124',\n+                'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@walkaroundjapan7124',\n             },\n             'params': {\n                 'skip_download': True,\n@@ -1276,11 +1334,11 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n             'info_dict': {\n                 'id': '4L2J27mJ3Dc',\n                 'ext': 'mp4',\n+                'title': 'Midwest Squid Game #Shorts',\n+                'description': 'md5:976512b8a29269b93bbd8a61edc45a6d',\n                 'upload_date': '20211025',\n                 'uploader': 'Charlie Berens',\n-                'description': 'md5:976512b8a29269b93bbd8a61edc45a6d',\n-                'uploader_id': 'fivedlrmilkshake',\n-                'title': 'Midwest Squid Game #Shorts',\n+                'uploader_id': '@CharlieBerens',\n             },\n             'params': {\n                 'skip_download': True,\n@@ -2088,25 +2146,19 @@ def feed_entry(name):\n                 thumbnails = [{'url': thumbnail}]\n \n         category = microformat.get('category') or search_meta('genre')\n-        channel_id = video_details.get('channelId') \\\n-            or microformat.get('externalChannelId') \\\n-            or search_meta('channelId')\n+        channel_id = self._extract_channel_id(\n+            webpage, videodetails=video_details, metadata=microformat)\n         duration = int_or_none(\n             video_details.get('lengthSeconds')\n             or microformat.get('lengthSeconds')) \\\n             or parse_duration(search_meta('duration'))\n         is_live = video_details.get('isLive')\n \n-        def gen_owner_profile_url():\n-            yield microformat.get('ownerProfileUrl')\n-            yield extract_attributes(self._search_regex(\n-                r'''(?s)(<link\\b[^>]+\\bitemprop\\s*=\\s*(\"|')url\\2[^>]*>)''',\n-                get_element_by_attribute('itemprop', 'author', webpage),\n-                'owner_profile_url', default='')).get('href')\n+        owner_profile_url = self._yt_urljoin(self._extract_author_var(\n+            webpage, 'url', videodetails=video_details, metadata=microformat))\n \n-        owner_profile_url = next(\n-            (x for x in map(url_or_none, gen_owner_profile_url()) if x),\n-            None)\n+        uploader = self._extract_author_var(\n+            webpage, 'name', videodetails=video_details, metadata=microformat)\n \n         if not player_url:\n             player_url = self._extract_player_url(webpage)\n@@ -2121,13 +2173,8 @@ def gen_owner_profile_url():\n             'upload_date': unified_strdate(\n                 microformat.get('uploadDate')\n                 or search_meta('uploadDate')),\n-            'uploader': video_details['author'],\n-            'uploader_id': self._search_regex(\n-                r'/(?:channel|user)/([^/?&#]+)', owner_profile_url,\n-                'uploader id', fatal=False) if owner_profile_url else None,\n-            'uploader_url': owner_profile_url,\n+            'uploader': uploader,\n             'channel_id': channel_id,\n-            'channel_url': 'https://www.youtube.com/channel/' + channel_id if channel_id else None,\n             'duration': duration,\n             'view_count': int_or_none(\n                 video_details.get('viewCount')\n@@ -2257,6 +2304,13 @@ def chapter_time(mmlir):\n                 initial_data,\n                 lambda x: x['contents']['twoColumnWatchNextResults']['results']['results']['contents'],\n                 list) or []\n+            if not info['channel_id']:\n+                channel_id = self._extract_channel_id('', renderers=contents)\n+            if not info['uploader']:\n+                info['uploader'] = self._extract_author_var('', 'name', renderers=contents)\n+            if not owner_profile_url:\n+                owner_profile_url = self._yt_urljoin(self._extract_author_var('', 'url', renderers=contents))\n+\n             for content in contents:\n                 vpir = content.get('videoPrimaryInfoRenderer')\n                 if vpir:\n@@ -2304,10 +2358,6 @@ def chapter_time(mmlir):\n                         })\n                 vsir = content.get('videoSecondaryInfoRenderer')\n                 if vsir:\n-                    info['channel'] = get_text(try_get(\n-                        vsir,\n-                        lambda x: x['owner']['videoOwnerRenderer']['title'],\n-                        dict))\n                     rows = try_get(\n                         vsir,\n                         lambda x: x['metadataRowContainer']['metadataRowContainerRenderer']['rows'],\n@@ -2365,7 +2415,14 @@ def chapter_time(mmlir):\n \n         self.mark_watched(video_id, player_response)\n \n-        return info\n+        return merge_dicts(\n+            info, {\n+                'uploader_id': self._extract_uploader_id(owner_profile_url),\n+                'uploader_url': owner_profile_url,\n+                'channel_id': channel_id,\n+                'channel_url': channel_id and self._yt_urljoin('/channel/' + channel_id),\n+                'channel': info['uploader'],\n+            })\n \n \n class YoutubeTabIE(YoutubeBaseInfoExtractor):\n@@ -2394,6 +2451,8 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n             'description': 'Short clips from Super Cooper Sundays!',\n             'id': 'UCKMA8kHZ8bPYpnMNaUSxfEQ',\n             'title': 'Super Cooper Shorts - Shorts',\n+            'uploader': 'Super Cooper Shorts',\n+            'uploader_id': '@SuperCooperShorts',\n         }\n     }, {\n         # Channel that does not have a Shorts tab. Test should just download videos on Home tab instead\n@@ -2404,14 +2463,17 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n             'title': 'Emergency Awesome - Home',\n         },\n         'playlist_mincount': 5,\n+        'skip': 'new test page needed to replace `Emergency Awesome - Shorts`',\n     }, {\n         # playlists, multipage\n         'url': 'https://www.youtube.com/c/\u0418\u0433\u043e\u0440\u044c\u041a\u043b\u0435\u0439\u043d\u0435\u0440/playlists?view=1&flow=grid',\n         'playlist_mincount': 94,\n         'info_dict': {\n             'id': 'UCqj7Cz7revf5maW9g5pgNcg',\n-            'title': '\u0418\u0433\u043e\u0440\u044c \u041a\u043b\u0435\u0439\u043d\u0435\u0440 - Playlists',\n+            'title': 'Igor Kleiner - Playlists',\n             'description': 'md5:be97ee0f14ee314f1f002cf187166ee2',\n+            'uploader': 'Igor Kleiner',\n+            'uploader_id': '@IgorDataScience',\n         },\n     }, {\n         # playlists, multipage, different order\n@@ -2419,8 +2481,10 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n         'playlist_mincount': 94,\n         'info_dict': {\n             'id': 'UCqj7Cz7revf5maW9g5pgNcg',\n-            'title': '\u0418\u0433\u043e\u0440\u044c \u041a\u043b\u0435\u0439\u043d\u0435\u0440 - Playlists',\n+            'title': 'Igor Kleiner - Playlists',\n             'description': 'md5:be97ee0f14ee314f1f002cf187166ee2',\n+            'uploader': 'Igor Kleiner',\n+            'uploader_id': '@IgorDataScience',\n         },\n     }, {\n         # playlists, series\n@@ -2430,6 +2494,8 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n             'id': 'UCYO_jab_esuFRV4b17AJtAw',\n             'title': '3Blue1Brown - Playlists',\n             'description': 'md5:e1384e8a133307dd10edee76e875d62f',\n+            'uploader': '3Blue1Brown',\n+            'uploader_id': '@3blue1brown',\n         },\n     }, {\n         # playlists, singlepage\n@@ -2439,6 +2505,8 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n             'id': 'UCAEtajcuhQ6an9WEzY9LEMQ',\n             'title': 'ThirstForScience - Playlists',\n             'description': 'md5:609399d937ea957b0f53cbffb747a14c',\n+            'uploader': 'ThirstForScience',\n+            'uploader_id': '@ThirstForScience',\n         }\n     }, {\n         'url': 'https://www.youtube.com/c/ChristophLaimer/playlists',\n@@ -2447,20 +2515,22 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n         # basic, single video playlist\n         'url': 'https://www.youtube.com/playlist?list=PL4lCao7KL_QFVb7Iudeipvc2BCavECqzc',\n         'info_dict': {\n-            'uploader_id': 'UCmlqkdCBesrv2Lak1mF_MxA',\n-            'uploader': 'Sergey M.',\n             'id': 'PL4lCao7KL_QFVb7Iudeipvc2BCavECqzc',\n             'title': 'youtube-dl public playlist',\n+            'uploader': 'Sergey M.',\n+            'uploader_id': '@sergeym.6173',\n+            'channel_id': 'UCmlqkdCBesrv2Lak1mF_MxA',\n         },\n         'playlist_count': 1,\n     }, {\n         # empty playlist\n         'url': 'https://www.youtube.com/playlist?list=PL4lCao7KL_QFodcLWhDpGCYnngnHtQ-Xf',\n         'info_dict': {\n-            'uploader_id': 'UCmlqkdCBesrv2Lak1mF_MxA',\n-            'uploader': 'Sergey M.',\n             'id': 'PL4lCao7KL_QFodcLWhDpGCYnngnHtQ-Xf',\n             'title': 'youtube-dl empty playlist',\n+            'uploader': 'Sergey M.',\n+            'uploader_id': '@sergeym.6173',\n+            'channel_id': 'UCmlqkdCBesrv2Lak1mF_MxA',\n         },\n         'playlist_count': 0,\n     }, {\n@@ -2470,6 +2540,8 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n             'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n             'title': 'lex will - Home',\n             'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n         },\n         'playlist_mincount': 2,\n     }, {\n@@ -2479,6 +2551,8 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n             'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n             'title': 'lex will - Videos',\n             'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n         },\n         'playlist_mincount': 975,\n     }, {\n@@ -2488,6 +2562,8 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n             'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n             'title': 'lex will - Videos',\n             'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n         },\n         'playlist_mincount': 199,\n     }, {\n@@ -2497,6 +2573,8 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n             'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n             'title': 'lex will - Playlists',\n             'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n         },\n         'playlist_mincount': 17,\n     }, {\n@@ -2506,6 +2584,8 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n             'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n             'title': 'lex will - Community',\n             'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n         },\n         'playlist_mincount': 18,\n     }, {\n@@ -2515,8 +2595,10 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n             'id': 'UCKfVa3S1e4PHvxWcwyMMg8w',\n             'title': 'lex will - Channels',\n             'description': 'md5:2163c5d0ff54ed5f598d6a7e6211e488',\n+            'uploader': 'lex will',\n+            'uploader_id': '@lexwill718',\n         },\n-        'playlist_mincount': 138,\n+        'playlist_mincount': 75,\n     }, {\n         'url': 'https://invidio.us/channel/UCmlqkdCBesrv2Lak1mF_MxA',\n         'only_matching': True,\n@@ -2533,7 +2615,8 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n             'title': '29C3: Not my department',\n             'id': 'PLwP_SiAcdui0KVebT0mU9Apz359a4ubsC',\n             'uploader': 'Christiaan008',\n-            'uploader_id': 'UCEPzS1rYsrkqzSLNp76nrcg',\n+            'uploader_id': '@ChRiStIaAn008',\n+            'channel_id': 'UCEPzS1rYsrkqzSLNp76nrcg',\n         },\n         'playlist_count': 96,\n     }, {\n@@ -2543,7 +2626,8 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n             'title': 'Uploads from Cauchemar',\n             'id': 'UUBABnxM4Ar9ten8Mdjj1j0Q',\n             'uploader': 'Cauchemar',\n-            'uploader_id': 'UCBABnxM4Ar9ten8Mdjj1j0Q',\n+            'uploader_id': '@Cauchemar89',\n+            'channel_id': 'UCBABnxM4Ar9ten8Mdjj1j0Q',\n         },\n         'playlist_mincount': 1123,\n     }, {\n@@ -2557,7 +2641,8 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n             'title': 'Uploads from Interstellar Movie',\n             'id': 'UUXw-G3eDE9trcvY2sBMM_aA',\n             'uploader': 'Interstellar Movie',\n-            'uploader_id': 'UCXw-G3eDE9trcvY2sBMM_aA',\n+            'uploader_id': '@InterstellarMovie',\n+            'channel_id': 'UCXw-G3eDE9trcvY2sBMM_aA',\n         },\n         'playlist_mincount': 21,\n     }, {\n@@ -2566,8 +2651,9 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n         'info_dict': {\n             'title': 'Data Analysis with Dr Mike Pound',\n             'id': 'PLzH6n4zXuckpfMu_4Ff8E7Z1behQks5ba',\n-            'uploader_id': 'UC9-y-6csu5WGm29I7JiwpnA',\n             'uploader': 'Computerphile',\n+            'uploader_id': '@Computerphile',\n+            'channel_id': 'UC9-y-6csu5WGm29I7JiwpnA',\n         },\n         'playlist_mincount': 11,\n     }, {\n@@ -2605,14 +2691,14 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n     }, {\n         'url': 'https://www.youtube.com/channel/UCoMdktPbSTixAyNGwb-UYkQ/live',\n         'info_dict': {\n-            'id': '9Auq9mYxFEE',\n+            'id': r're:[\\da-zA-Z_-]{8,}',\n             'ext': 'mp4',\n-            'title': 'Watch Sky News live',\n+            'title': r're:(?s)[A-Z].{20,}',\n             'uploader': 'Sky News',\n-            'uploader_id': 'skynews',\n-            'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/user/skynews',\n-            'upload_date': '20191102',\n-            'description': 'md5:78de4e1c2359d0ea3ed829678e38b662',\n+            'uploader_id': '@SkyNews',\n+            'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@SkyNews',\n+            'upload_date': r're:\\d{8}',\n+            'description': r're:(?s)(?:.*\\n)+SUBSCRIBE to our YouTube channel for more videos: http://www\\.youtube\\.com/skynews *\\n.*',\n             'categories': ['News & Politics'],\n             'tags': list,\n             'like_count': int,\n@@ -2701,34 +2787,22 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n     }, {\n         'note': 'Search tab',\n         'url': 'https://www.youtube.com/c/3blue1brown/search?query=linear%20algebra',\n-        'playlist_mincount': 40,\n+        'playlist_mincount': 20,\n         'info_dict': {\n             'id': 'UCYO_jab_esuFRV4b17AJtAw',\n             'title': '3Blue1Brown - Search - linear algebra',\n             'description': 'md5:e1384e8a133307dd10edee76e875d62f',\n             'uploader': '3Blue1Brown',\n-            'uploader_id': 'UCYO_jab_esuFRV4b17AJtAw',\n+            'uploader_id': '@3blue1brown',\n+            'channel_id': 'UCYO_jab_esuFRV4b17AJtAw',\n         }\n     }]\n \n     @classmethod\n     def suitable(cls, url):\n-        return False if YoutubeIE.suitable(url) else super(\n+        return not YoutubeIE.suitable(url) and super(\n             YoutubeTabIE, cls).suitable(url)\n \n-    def _extract_channel_id(self, webpage):\n-        channel_id = self._html_search_meta(\n-            'channelId', webpage, 'channel id', default=None)\n-        if channel_id:\n-            return channel_id\n-        channel_url = self._html_search_meta(\n-            ('og:url', 'al:ios:url', 'al:android:url', 'al:web:url',\n-             'twitter:url', 'twitter:app:url:iphone', 'twitter:app:url:ipad',\n-             'twitter:app:url:googleplay'), webpage, 'channel url')\n-        return self._search_regex(\n-            r'https?://(?:www\\.)?youtube\\.com/channel/([^/?#&])+',\n-            channel_url, 'channel id')\n-\n     @staticmethod\n     def _extract_grid_item_renderer(item):\n         assert isinstance(item, dict)\n@@ -3116,27 +3190,18 @@ def _extract_selected_tab(tabs):\n         else:\n             raise ExtractorError('Unable to find selected tab')\n \n-    @staticmethod\n-    def _extract_uploader(data):\n+    def _extract_uploader(self, metadata, data):\n         uploader = {}\n-        sidebar_renderer = try_get(\n-            data, lambda x: x['sidebar']['playlistSidebarRenderer']['items'], list)\n-        if sidebar_renderer:\n-            for item in sidebar_renderer:\n-                if not isinstance(item, dict):\n-                    continue\n-                renderer = item.get('playlistSidebarSecondaryInfoRenderer')\n-                if not isinstance(renderer, dict):\n-                    continue\n-                owner = try_get(\n-                    renderer, lambda x: x['videoOwner']['videoOwnerRenderer']['title']['runs'][0], dict)\n-                if owner:\n-                    uploader['uploader'] = owner.get('text')\n-                    uploader['uploader_id'] = try_get(\n-                        owner, lambda x: x['navigationEndpoint']['browseEndpoint']['browseId'], compat_str)\n-                    uploader['uploader_url'] = urljoin(\n-                        'https://www.youtube.com/',\n-                        try_get(owner, lambda x: x['navigationEndpoint']['browseEndpoint']['canonicalBaseUrl'], compat_str))\n+        renderers = traverse_obj(data,\n+                                 ('sidebar', 'playlistSidebarRenderer', 'items'))\n+        uploader['channel_id'] = self._extract_channel_id('', metadata=metadata, renderers=renderers)\n+        uploader['uploader'] = (\n+            self._extract_author_var('', 'name', renderers=renderers)\n+            or self._extract_author_var('', 'name', metadata=metadata))\n+        uploader['uploader_url'] = self._yt_urljoin(\n+            self._extract_author_var('', 'url', metadata=metadata, renderers=renderers))\n+        uploader['uploader_id'] = self._extract_uploader_id(uploader['uploader_url'])\n+        uploader['channel'] = uploader['uploader']\n         return uploader\n \n     @staticmethod\n@@ -3187,8 +3252,7 @@ def _extract_from_tabs(self, item_id, webpage, data, tabs):\n             self._entries(selected_tab, item_id, webpage),\n             playlist_id=playlist_id, playlist_title=title,\n             playlist_description=description)\n-        playlist.update(self._extract_uploader(data))\n-        return playlist\n+        return merge_dicts(playlist, self._extract_uploader(renderer, data))\n \n     def _extract_from_playlist(self, item_id, url, data, playlist):\n         title = playlist.get('title') or try_get(\n@@ -3275,8 +3339,9 @@ class YoutubePlaylistIE(InfoExtractor):\n         'info_dict': {\n             'title': '[OLD]Team Fortress 2 (Class-based LP)',\n             'id': 'PLBB231211A4F62143',\n-            'uploader': 'Wickydoo',\n-            'uploader_id': 'UCKSpbfbl5kRQpTdL7kMc-1Q',\n+            'uploader': 'Wickman',\n+            'uploader_id': '@WickmanVT',\n+            'channel_id': 'UCKSpbfbl5kRQpTdL7kMc-1Q',\n         },\n         'playlist_mincount': 29,\n     }, {\n@@ -3290,21 +3355,25 @@ class YoutubePlaylistIE(InfoExtractor):\n     }, {\n         'note': 'embedded',\n         'url': 'https://www.youtube.com/embed/videoseries?list=PL6IaIsEjSbf96XFRuNccS_RuEXwNdsoEu',\n-        'playlist_count': 4,\n+        # TODO: full playlist requires _reload_with_unavailable_videos()\n+        # 'playlist_count': 4,\n+        'playlist_mincount': 1,\n         'info_dict': {\n             'title': 'JODA15',\n             'id': 'PL6IaIsEjSbf96XFRuNccS_RuEXwNdsoEu',\n             'uploader': 'milan',\n-            'uploader_id': 'UCEI1-PVPcYXjB73Hfelbmaw',\n+            'uploader_id': '@milan5503',\n+            'channel_id': 'UCEI1-PVPcYXjB73Hfelbmaw',\n         }\n     }, {\n         'url': 'http://www.youtube.com/embed/_xDOZElKyNU?list=PLsyOSbh5bs16vubvKePAQ1x3PhKavfBIl',\n-        'playlist_mincount': 982,\n+        'playlist_mincount': 455,\n         'info_dict': {\n             'title': '2018 Chinese New Singles (11/6 updated)',\n             'id': 'PLsyOSbh5bs16vubvKePAQ1x3PhKavfBIl',\n             'uploader': 'LBK',\n-            'uploader_id': 'UC21nz3_MesPLqtDqwdvnoxA',\n+            'uploader_id': '@music_king',\n+            'channel_id': 'UC21nz3_MesPLqtDqwdvnoxA',\n         }\n     }, {\n         'url': 'TLGGrESM50VT6acwMjAyMjAxNw',\n@@ -3342,8 +3411,8 @@ class YoutubeYtBeIE(InfoExtractor):\n             'ext': 'mp4',\n             'title': 'Small Scale Baler and Braiding Rugs',\n             'uploader': 'Backus-Page House Museum',\n-            'uploader_id': 'backuspagemuseum',\n-            'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/user/backuspagemuseum',\n+            'uploader_id': '@backuspagemuseum',\n+            'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/@backuspagemuseum',\n             'upload_date': '20161008',\n             'description': 'md5:800c0c78d5eb128500bffd4f0b4f2e8a',\n             'categories': ['Nonprofits & Activism'],\n", "test_patch": "diff --git a/test/test_download.py b/test/test_download.py\nindex 19936969fa0..d500083079e 100644\n--- a/test/test_download.py\n+++ b/test/test_download.py\n@@ -148,6 +148,7 @@ def try_rm_tcs_files(tcs=None):\n                 try_rm(tc_filename)\n                 try_rm(tc_filename + '.part')\n                 try_rm(os.path.splitext(tc_filename)[0] + '.info.json')\n+\n         try_rm_tcs_files()\n         try:\n             try_num = 1\n@@ -213,7 +214,15 @@ def try_rm_tcs_files(tcs=None):\n                 # First, check test cases' data against extracted data alone\n                 expect_info_dict(self, tc_res_dict, tc.get('info_dict', {}))\n                 # Now, check downloaded file consistency\n+                # support test-case with volatile ID, signalled by regexp value\n+                if tc.get('info_dict', {}).get('id', '').startswith('re:'):\n+                    test_id = tc['info_dict']['id']\n+                    tc['info_dict']['id'] = tc_res_dict['id']\n+                else:\n+                    test_id = None\n                 tc_filename = get_tc_filename(tc)\n+                if test_id:\n+                    tc['info_dict']['id'] = test_id\n                 if not test_case.get('params', {}).get('skip_download', False):\n                     self.assertTrue(os.path.exists(tc_filename), msg='Missing file ' + tc_filename)\n                     self.assertTrue(tc_filename in finished_hook_called)\n", "problem_statement": "youtube channel urls can also be @channelname\n## Please follow the guide below\r\n\r\n- You will be asked some questions, please read them **carefully** and answer honestly\r\n- Put an `x` into all the boxes [ ] relevant to your *pull request* (like that [x])\r\n- Use *Preview* tab to see how your *pull request* will actually look like\r\n\r\n---\r\n\r\n### Before submitting a *pull request* make sure you have:\r\n- [x] [Searched](https://github.com/ytdl-org/youtube-dl/search?q=is%3Apr&type=Issues) the bugtracker for similar pull requests\r\n- [ ] Read [adding new extractor tutorial](https://github.com/ytdl-org/youtube-dl#adding-support-for-a-new-site)\r\n- [x] Read [youtube-dl coding conventions](https://github.com/ytdl-org/youtube-dl#youtube-dl-coding-conventions) and adjusted the code to meet them\r\n- [ ] Covered the code with tests (note that PRs without tests will be REJECTED)\r\n- [ ] Checked the code with [flake8](https://pypi.python.org/pypi/flake8)\r\n\r\n### In order to be accepted and merged into youtube-dl each piece of code must be in public domain or released under [Unlicense](http://unlicense.org/). Check one of the following options:\r\n- [x] I am the original author of this code and I am willing to release it under [Unlicense](http://unlicense.org/)\r\n- [ ] I am not the original author of this code but it is in public domain or released under [Unlicense](http://unlicense.org/) (provide reliable evidence)\r\n\r\n### What is the purpose of your *pull request*?\r\n- [x] Bug fix\r\n- [ ] Improvement\r\n- [ ] New extractor\r\n- [ ] New feature\r\n\r\n---\r\n\r\n### Description of your *pull request* and other information\r\n\r\nExplanation of your *pull request* in arbitrary form goes here. Please make sure the description explains the purpose and effect of your *pull request* and is worded well enough to be understood. Provide as much context and examples as possible.\r\n\r\nan example this https://www.youtube.com/channel/UCKjaXKTMQMgpyac7WsnNrKw and https://www.youtube.com/@alangomezok are the same, it also crashed because it didn't match, the new url template, but it looks like that is already fixed\r\n\r\nhope this helps, my first proposed change :)\n[YouTube] Unable to extract uploader id\n**This issue is solved: read the [Description](#Description) below**\r\nIf you still want to comment after reading that, **think again**: first read [this post below](https://github.com/ytdl-org/youtube-dl/issues/31530#issuecomment-1520185289) to avoid your comment being deleted or marked as spam.\r\n<!--\r\n\r\n######################################################################\r\n  WARNING!\r\n  IGNORING THE FOLLOWING TEMPLATE WILL RESULT IN ISSUE CLOSED AS INCOMPLETE\r\n######################################################################\r\n\r\n-->\r\n\r\n## Checklist\r\n\r\n<!--\r\nCarefully read and work through this check list in order to prevent the most common mistakes and misuse of youtube-dl:\r\n- First of, make sure you are using the latest version of youtube-dl. Run `youtube-dl --version` and ensure your version is 2021.12.17. If it's not, see https://yt-dl.org/update on how to update. Issues with outdated version will be REJECTED.\r\n- Make sure that all provided video/audio/playlist URLs (if any) are alive and playable in a browser.\r\n- Make sure that all URLs and arguments with special characters are properly quoted or escaped as explained in http://yt-dl.org/escape.\r\n- Search the bugtracker for similar issues: http://yt-dl.org/search-issues. DO NOT post duplicates.\r\n- Finally, put x into all relevant boxes (like this [x])\r\n-->\r\n\r\n- [x] I'm reporting a broken site support\r\n- [x] I've verified that I'm running youtube-dl version **2021.12.17**\r\n- [x] I've checked that all provided URLs are alive and playable in a browser\r\n- [x] I've checked that all URLs and arguments with special characters are properly quoted or escaped\r\n- [x] I've searched the bugtracker for similar issues including closed ones\r\n\r\n\r\n## Verbose log\r\n\r\n<!--\r\nProvide the complete verbose output of youtube-dl that clearly demonstrates the problem.\r\nAdd the `-v` flag to your command line you run youtube-dl with (`youtube-dl -v <your command line>`), copy the WHOLE output and insert it below. It should look similar to this:\r\n [debug] System config: []\r\n [debug] User config: []\r\n [debug] Command-line args: [u'-v', u'http://www.youtube.com/watch?v=BaW_jenozKcj']\r\n [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251\r\n [debug] youtube-dl version 2021.12.17\r\n [debug] Python version 2.7.11 - Windows-2003Server-5.2.3790-SP2\r\n [debug] exe versions: ffmpeg N-75573-g1d0487f, ffprobe N-75573-g1d0487f, rtmpdump 2.4\r\n [debug] Proxy map: {}\r\n <more lines>\r\n-->\r\n<details><summary>[YouTube] Unable to extract uploader id</summary>\r\n\r\n```\r\nyt-dlp.exe\" -x -4 --download-archive \"C:\\Youtube-dlp\\download_history.txt\" --external-downloader aria2c --restrict-filenames --match-filter \"description*='ESTRENO'\" --audio-format mp3 -o \"E:\\FOLDER\\RENAME\\%%(title)s.%%(ext)s\" \"https://www.youtube.com/watch?v=lcusH7ksCNw&list=PLuXU-g7mjkKPvWSFq7xI_N4Xj7A3iSOU3&index=67\" -v\r\n[debug] Command-line config: ['-x', '-4', '--download-archive', 'C:\\\\Youtube-dlp\\\\download_history.txt', '--external-downloader', 'aria2c', '--restrict-filenames', '--match-filter', \"description*='ESTRENO'\", '--audio-format', 'mp3', '-o', 'E:\\\\A.PROCESAR\\\\RENAME\\\\%%(title)s.%%(ext)s', 'https://www.youtube.com/watch?v=lcusH7ksCNw&list=PLuXU-g7mjkKPvWSFq7xI_N4Xj7A3iSOU3&index=67', '-v']\r\n[debug] Encodings: locale cp1252, fs utf-8, pref cp1252, out utf-8, error utf-8, screen utf-8\r\n[debug] yt-dlp version 2023.01.06 [6becd25] (win_exe)\r\n[debug] Python 3.8.10 (CPython AMD64 64bit) - Windows-10-10.0.19044-SP0 (OpenSSL 1.1.1k  25 Mar 2021)\r\n[debug] exe versions: ffmpeg N-109541-g94aa70d757-20230109 (setts), ffprobe N-109541-g94aa70d757-20230109, phantomjs 2.1.1\r\n[debug] Optional libraries: Cryptodome-3.16.0, brotli-1.0.9, certifi-2022.12.07, mutagen-1.46.0, sqlite3-2.6.0, websockets-10.4\r\n[debug] Proxy map: {}\r\n[debug] Loaded 1760 extractors\r\n[debug] Loading archive file 'C:\\\\Youtube-dlp\\\\ download_history.txt'\r\n[youtube:tab] Extracting URL: https://www.youtube.com/watch?v=lcusH7ksCNw&list=PLuXU-g7mjkKPvWSFq7xI_N4Xj7A3iSOU3&index=67\r\n[youtube:tab] Downloading playlist PLuXU-g7mjkKPvWSFq7xI_N4Xj7A3iSOU3 - add --no-playlist to download just the video lcusH7ksCNw\r\n[youtube:tab] PLuXU-g7mjkKPvWSFq7xI_N4Xj7A3iSOU3: Downloading webpage\r\n[youtube:tab] Extracting URL: https://www.youtube.com/playlist?list=PLuXU-g7mjkKPvWSFq7xI_N4Xj7A3iSOU3\r\n[youtube:tab] PLuXU-g7mjkKPvWSFq7xI_N4Xj7A3iSOU3: Downloading webpage\r\n[youtube:tab] PLuXU-g7mjkKPvWSFq7xI_N4Xj7A3iSOU3: Redownloading playlist API JSON with unavailable videos\r\n[download] Downloading playlist: LA VENGANZA SER\u00c1 TERRIBLE (Programas)\r\n[youtube:tab] Playlist LA VENGANZA SER\u00c1 TERRIBLE (Programas): Downloading 67 items of 67\r\n[download] Downloading item 1 of 67\r\n[youtube] Extracting URL: https://www.youtube.com/watch?v=3h5-QPbKoVg\r\n[youtube] 3h5-QPbKoVg: Downloading webpage\r\n[youtube] 3h5-QPbKoVg: Downloading android player API JSON\r\n[debug] Loading youtube-nsig.1cbba2b4 from cache\r\nWARNING: [youtube] 3h5-QPbKoVg: Native nsig extraction failed: Trying with PhantomJS\r\n         n = GDZ_5inyZ3r6QSP ; player = https://www.youtube.com/s/player/1cbba2b4/player_ias.vflset/en_US/base.js\r\n[debug] [youtube] Signature function returned an exception; please report this issue on  https://github.com/yt-dlp/yt-dlp/issues?q= , filling out the appropriate issue template. Confirm you are on the latest version using  yt-dlp -U\r\n[youtube] 3h5-QPbKoVg: Executing signature code\r\n[debug] [youtube] PhantomJS command line: phantomjs \"--ssl-protocol=any\" \"C:\\Users\\USER\\AppData\\Local\\Temp\\tmp88untizd\"\r\n[debug] [youtube] Decrypted nsig GDZ_5inyZ3r6QSP => ECvndyupC0ex4A\r\nERROR: [youtube] 3h5-QPbKoVg: Unable to extract uploader id; please report this issue on  https://github.com/yt-dlp/yt-dlp/issues?q= , filling out the appropriate issue template. Confirm you are on the latest version using  yt-dlp -U\r\n  File \"yt_dlp\\extractor\\common.py\", line 680, in extract\r\n  File \"yt_dlp\\extractor\\youtube.py\", line 4049, in _real_extract\r\n  File \"yt_dlp\\extractor\\common.py\", line 1228, in _search_regex\r\n```\r\n</details>\r\n\r\n<a id=Description>\r\n\r\n## Description\r\n\r\n</a>\r\n\r\n<!--\r\nProvide an explanation of your issue in an arbitrary form. Provide any additional information, suggested solution and as much context and examples as possible.\r\nIf work on your issue requires account credentials please provide them or explain how one can obtain them.\r\n-->\r\n\r\n[Revised by @dirkf]\r\nOP actually reported the bug for _yt-dlp_.\r\n\r\nThe problem, which was revealed by a change in the metadata served by YouTube, applied to both [_youtube-dl_](https://github.com/ytdl-org/youtube-dl/issues/31530#issuecomment-1433230848) and [_yt-dlp_](https://github.com/yt-dlp/yt-dlp/issues/6247) and has been fixed in both programs.\r\n\r\nThere is a release of [_yt-dlp_](https://github.com/yt-dlp/yt-dlp) that includes the fix. See #31535 for discussion of whether this would be a good choice for you.\r\n\r\n**See [below](https://github.com/ytdl-org/youtube-dl/issues/31530#issuecomment-1435975611) for ways to update to a fixed version of _youtube-dl_**. Using `-U` doesn't (yet) work. [A new release may be created, including the final fix, some time later than now](https://github.com/ytdl-org/youtube-dl/issues/31585). However, there is now a **nightly build** from the latest code [here](https://github.com/ytdl-org/ytdl-nightly/releases).\r\n\r\nThere's **no point posting your log that shows the error** now: feel free to thumb-up [this post](https://github.com/ytdl-org/youtube-dl/issues/31530#issuecomment-1433230848). Also, be aware that the initial patch still flagged the original issue [as a WARNING](https://github.com/ytdl-org/youtube-dl/issues/31530#issuecomment-1436130288); the [fully fixed yt-dl](https://github.com/ytdl-org/youtube-dl/issues/31530#issuecomment-1443615453) doesn't. [Consider raising a separate issue](https://github.com/ytdl-org/youtube-dl/issues/30839) if your fixed yt-dl shows a different error.\n", "hints_text": "Please review https://github.com/ytdl-org/youtube-dl/issues/31530#issuecomment-1435734719, if you didn't. I think there are more places to hit than just the change you've suggested.\nSome videos are still showing the old scheme, eg test video `MgNrAu2pzNs`:\r\n```py\r\n{\r\n    'uploader_id': 'UC-pWHpBjdGG69N9mM2auIAA',\r\n    'uploader_url': 'http://www.youtube.com_channel/UC-pWHpBjdGG69N9mM2auIAA',\r\n}\r\n```\nSame here;  a more simplified sample command line and result to reproduce the issue is as follows:\r\nI have a batch (.cmd in Windows), downloading multiple playlists' updates.\r\nWhen the issue first occured, I have added .yt-dlp.exe -U as the first command, which did not help.\r\nI just have created a single video command to ease reproduction.\r\n\r\nAdditional info: youtube-dl shows the same error without -v option; a subsequent attempt with -v added suddenly downloaded the video. So it might be hard to reproduce.\r\nSuggestion: Can that be due to manfest.googlevideos.com temporarily being unreachable?\r\n(I guess that from youtube-dl's verbose output when it worked)\r\n\r\nMy error sample:\r\n\r\n...> .\\yt-dlp.exe -v https://youtube.com/watch?v=hrpVKlj2QI0\r\n[debug] Command-line config: ['-v', 'https://youtube.com/watch?v=hrpVKlj2QI0']\r\n[debug] Encodings: locale cp1252, fs utf-8, pref cp1252, out utf-8, error utf-8, screen utf-8\r\n[debug] yt-dlp version 2023.01.06 [6becd25] (win_exe)\r\n[debug] Python 3.8.10 (CPython AMD64 64bit) - Windows-10-10.0.22621-SP0 (OpenSSL 1.1.1k  25 Mar 2021)\r\n[debug] exe versions: ffmpeg N-109820-g2c4dcbd66b-20230210 (setts), ffprobe N-109820-g2c4dcbd66b-20230210, phantomjs 2.1.1\r\n[debug] Optional libraries: Cryptodome-3.16.0, brotli-1.0.9, certifi-2022.12.07, mutagen-1.46.0, sqlite3-2.6.0, websockets-10.4\r\n[debug] Proxy map: {}\r\n[debug] Loaded 1760 extractors\r\n[youtube] Extracting URL: https://youtube.com/watch?v=hrpVKlj2QI0\r\n[youtube] hrpVKlj2QI0: Downloading webpage\r\n[youtube] hrpVKlj2QI0: Downloading android player API JSON\r\n[youtube] hrpVKlj2QI0: Downloading MPD manifest\r\nERROR: [youtube] hrpVKlj2QI0: Unable to extract uploader id; please report this issue on  https://github.com/yt-dlp/yt-dlp/issues?q= , filling out the appropriate issue template. Confirm you are on the latest version using  yt-dlp -U\r\n\r\n  File \"yt_dlp\\extractor\\common.py\", line 680, in extract\r\n  File \"yt_dlp\\extractor\\youtube.py\", line 4049, in _real_extract\r\n  File \"yt_dlp\\extractor\\common.py\", line 1228, in _search_regex\r\n", "created_at": "2023-02-24T05:00:30Z"}
{"repo": "ytdl-org/youtube-dl", "pull_number": 31235, "instance_id": "ytdl-org__youtube-dl-31235", "issue_numbers": ["18051"], "base_commit": "7009bb9f3182449ae8cc05cc28b768b63030a485", "patch": "diff --git a/youtube_dl/aes.py b/youtube_dl/aes.py\nindex d0de2d93f39..a94a410798b 100644\n--- a/youtube_dl/aes.py\n+++ b/youtube_dl/aes.py\n@@ -8,6 +8,18 @@\n BLOCK_SIZE_BYTES = 16\n \n \n+def pkcs7_padding(data):\n+    \"\"\"\n+    PKCS#7 padding\n+\n+    @param {int[]} data        cleartext\n+    @returns {int[]}           padding data\n+    \"\"\"\n+\n+    remaining_length = BLOCK_SIZE_BYTES - len(data) % BLOCK_SIZE_BYTES\n+    return data + [remaining_length] * remaining_length\n+\n+\n def aes_ctr_decrypt(data, key, counter):\n     \"\"\"\n     Decrypt with aes in counter mode\n@@ -76,8 +88,7 @@ def aes_cbc_encrypt(data, key, iv):\n     previous_cipher_block = iv\n     for i in range(block_count):\n         block = data[i * BLOCK_SIZE_BYTES: (i + 1) * BLOCK_SIZE_BYTES]\n-        remaining_length = BLOCK_SIZE_BYTES - len(block)\n-        block += [remaining_length] * remaining_length\n+        block = pkcs7_padding(block)\n         mixed_block = xor(block, previous_cipher_block)\n \n         encrypted_block = aes_encrypt(mixed_block, expanded_key)\n@@ -88,6 +99,28 @@ def aes_cbc_encrypt(data, key, iv):\n     return encrypted_data\n \n \n+def aes_ecb_encrypt(data, key):\n+    \"\"\"\n+    Encrypt with aes in ECB mode. Using PKCS#7 padding\n+\n+    @param {int[]} data        cleartext\n+    @param {int[]} key         16/24/32-Byte cipher key\n+    @returns {int[]}           encrypted data\n+    \"\"\"\n+    expanded_key = key_expansion(key)\n+    block_count = int(ceil(float(len(data)) / BLOCK_SIZE_BYTES))\n+\n+    encrypted_data = []\n+    for i in range(block_count):\n+        block = data[i * BLOCK_SIZE_BYTES: (i + 1) * BLOCK_SIZE_BYTES]\n+        block = pkcs7_padding(block)\n+\n+        encrypted_block = aes_encrypt(block, expanded_key)\n+        encrypted_data += encrypted_block\n+\n+    return encrypted_data\n+\n+\n def key_expansion(data):\n     \"\"\"\n     Generate key schedule\ndiff --git a/youtube_dl/extractor/neteasemusic.py b/youtube_dl/extractor/neteasemusic.py\nindex 978a05841ce..fad22a2cd0b 100644\n--- a/youtube_dl/extractor/neteasemusic.py\n+++ b/youtube_dl/extractor/neteasemusic.py\n@@ -1,20 +1,31 @@\n # coding: utf-8\n from __future__ import unicode_literals\n \n-from hashlib import md5\n from base64 import b64encode\n+from binascii import hexlify\n from datetime import datetime\n+from hashlib import md5\n+from random import randint\n+import json\n import re\n+import time\n \n from .common import InfoExtractor\n+from ..aes import aes_ecb_encrypt, pkcs7_padding\n from ..compat import (\n     compat_urllib_parse_urlencode,\n     compat_str,\n     compat_itertools_count,\n )\n from ..utils import (\n-    sanitized_Request,\n+    ExtractorError,\n+    bytes_to_intlist,\n     float_or_none,\n+    int_or_none,\n+    intlist_to_bytes,\n+    sanitized_Request,\n+    std_headers,\n+    try_get,\n )\n \n \n@@ -35,32 +46,85 @@ def _encrypt(cls, dfsid):\n         result = b64encode(m.digest()).decode('ascii')\n         return result.replace('/', '_').replace('+', '-')\n \n+    @classmethod\n+    def make_player_api_request_data_and_headers(cls, song_id, bitrate):\n+        KEY = b'e82ckenh8dichen8'\n+        URL = '/api/song/enhance/player/url'\n+        now = int(time.time() * 1000)\n+        rand = randint(0, 1000)\n+        cookie = {\n+            'osver': None,\n+            'deviceId': None,\n+            'appver': '8.0.0',\n+            'versioncode': '140',\n+            'mobilename': None,\n+            'buildver': '1623435496',\n+            'resolution': '1920x1080',\n+            '__csrf': '',\n+            'os': 'pc',\n+            'channel': None,\n+            'requestId': '{0}_{1:04}'.format(now, rand),\n+        }\n+        request_text = json.dumps(\n+            {'ids': '[{0}]'.format(song_id), 'br': bitrate, 'header': cookie},\n+            separators=(',', ':'))\n+        message = 'nobody{0}use{1}md5forencrypt'.format(\n+            URL, request_text).encode('latin1')\n+        msg_digest = md5(message).hexdigest()\n+\n+        data = '{0}-36cd479b6b5-{1}-36cd479b6b5-{2}'.format(\n+            URL, request_text, msg_digest)\n+        data = pkcs7_padding(bytes_to_intlist(data))\n+        encrypted = intlist_to_bytes(aes_ecb_encrypt(data, bytes_to_intlist(KEY)))\n+        encrypted_params = hexlify(encrypted).decode('ascii').upper()\n+\n+        cookie = '; '.join(\n+            ['{0}={1}'.format(k, v if v is not None else 'undefined')\n+             for [k, v] in cookie.items()])\n+\n+        headers = {\n+            'User-Agent': std_headers['User-Agent'],\n+            'Content-Type': 'application/x-www-form-urlencoded',\n+            'Referer': 'https://music.163.com',\n+            'Cookie': cookie,\n+        }\n+        return ('params={0}'.format(encrypted_params), headers)\n+\n+    def _call_player_api(self, song_id, bitrate):\n+        url = 'https://interface3.music.163.com/eapi/song/enhance/player/url'\n+        data, headers = self.make_player_api_request_data_and_headers(song_id, bitrate)\n+        try:\n+            return self._download_json(\n+                url, song_id, data=data.encode('ascii'), headers=headers)\n+        except ExtractorError as e:\n+            if type(e.cause) in (ValueError, TypeError):\n+                # JSON load failure\n+                raise\n+        except Exception:\n+            pass\n+        return {}\n+\n     def extract_formats(self, info):\n         formats = []\n+        song_id = info['id']\n         for song_format in self._FORMATS:\n             details = info.get(song_format)\n             if not details:\n                 continue\n-            song_file_path = '/%s/%s.%s' % (\n-                self._encrypt(details['dfsId']), details['dfsId'], details['extension'])\n-\n-            # 203.130.59.9, 124.40.233.182, 115.231.74.139, etc is a reverse proxy-like feature\n-            # from NetEase's CDN provider that can be used if m5.music.126.net does not\n-            # work, especially for users outside of Mainland China\n-            # via: https://github.com/JixunMoe/unblock-163/issues/3#issuecomment-163115880\n-            for host in ('http://m5.music.126.net', 'http://115.231.74.139/m1.music.126.net',\n-                         'http://124.40.233.182/m1.music.126.net', 'http://203.130.59.9/m1.music.126.net'):\n-                song_url = host + song_file_path\n+\n+            bitrate = int_or_none(details.get('bitrate')) or 999000\n+            data = self._call_player_api(song_id, bitrate)\n+            for song in try_get(data, lambda x: x['data'], list) or []:\n+                song_url = try_get(song, lambda x: x['url'])\n                 if self._is_valid_url(song_url, info['id'], 'song'):\n                     formats.append({\n                         'url': song_url,\n                         'ext': details.get('extension'),\n-                        'abr': float_or_none(details.get('bitrate'), scale=1000),\n+                        'abr': float_or_none(song.get('br'), scale=1000),\n                         'format_id': song_format,\n-                        'filesize': details.get('size'),\n-                        'asr': details.get('sr')\n+                        'filesize': int_or_none(song.get('size')),\n+                        'asr': int_or_none(details.get('sr')),\n                     })\n-                    break\n         return formats\n \n     @classmethod\n@@ -79,30 +143,16 @@ class NetEaseMusicIE(NetEaseMusicBaseIE):\n     _VALID_URL = r'https?://music\\.163\\.com/(#/)?song\\?id=(?P<id>[0-9]+)'\n     _TESTS = [{\n         'url': 'http://music.163.com/#/song?id=32102397',\n-        'md5': 'f2e97280e6345c74ba9d5677dd5dcb45',\n+        'md5': '3e909614ce09b1ccef4a3eb205441190',\n         'info_dict': {\n             'id': '32102397',\n             'ext': 'mp3',\n-            'title': 'Bad Blood (feat. Kendrick Lamar)',\n+            'title': 'Bad Blood',\n             'creator': 'Taylor Swift / Kendrick Lamar',\n-            'upload_date': '20150517',\n-            'timestamp': 1431878400,\n-            'description': 'md5:a10a54589c2860300d02e1de821eb2ef',\n+            'upload_date': '20150516',\n+            'timestamp': 1431792000,\n+            'description': 'md5:25fc5f27e47aad975aa6d36382c7833c',\n         },\n-        'skip': 'Blocked outside Mainland China',\n-    }, {\n-        'note': 'No lyrics translation.',\n-        'url': 'http://music.163.com/#/song?id=29822014',\n-        'info_dict': {\n-            'id': '29822014',\n-            'ext': 'mp3',\n-            'title': '\u542c\u89c1\u4e0b\u96e8\u7684\u58f0\u97f3',\n-            'creator': '\u5468\u6770\u4f26',\n-            'upload_date': '20141225',\n-            'timestamp': 1419523200,\n-            'description': 'md5:a4d8d89f44656af206b7b2555c0bce6c',\n-        },\n-        'skip': 'Blocked outside Mainland China',\n     }, {\n         'note': 'No lyrics.',\n         'url': 'http://music.163.com/song?id=17241424',\n@@ -112,9 +162,9 @@ class NetEaseMusicIE(NetEaseMusicBaseIE):\n             'title': 'Opus 28',\n             'creator': 'Dustin O\\'Halloran',\n             'upload_date': '20080211',\n+            'description': 'md5:f12945b0f6e0365e3b73c5032e1b0ff4',\n             'timestamp': 1202745600,\n         },\n-        'skip': 'Blocked outside Mainland China',\n     }, {\n         'note': 'Has translated name.',\n         'url': 'http://music.163.com/#/song?id=22735043',\n@@ -128,7 +178,6 @@ class NetEaseMusicIE(NetEaseMusicBaseIE):\n             'timestamp': 1264608000,\n             'alt_title': '\u8bf4\u51fa\u613f\u671b\u5427(Genie)',\n         },\n-        'skip': 'Blocked outside Mainland China',\n     }]\n \n     def _process_lyrics(self, lyrics_info):\n", "test_patch": "diff --git a/test/test_aes.py b/test/test_aes.py\nindex cc89fb6ab27..0f181466bcf 100644\n--- a/test/test_aes.py\n+++ b/test/test_aes.py\n@@ -8,7 +8,7 @@\n import unittest\n sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n \n-from youtube_dl.aes import aes_decrypt, aes_encrypt, aes_cbc_decrypt, aes_cbc_encrypt, aes_decrypt_text\n+from youtube_dl.aes import aes_decrypt, aes_encrypt, aes_cbc_decrypt, aes_cbc_encrypt, aes_decrypt_text, aes_ecb_encrypt\n from youtube_dl.utils import bytes_to_intlist, intlist_to_bytes\n import base64\n \n@@ -58,6 +58,13 @@ def test_decrypt_text(self):\n         decrypted = (aes_decrypt_text(encrypted, password, 32))\n         self.assertEqual(decrypted, self.secret_msg)\n \n+    def test_ecb_encrypt(self):\n+        data = bytes_to_intlist(self.secret_msg)\n+        encrypted = intlist_to_bytes(aes_ecb_encrypt(data, self.key))\n+        self.assertEqual(\n+            encrypted,\n+            b'\\xaa\\x86]\\x81\\x97>\\x02\\x92\\x9d\\x1bR[[L/u\\xd3&\\xd1(h\\xde{\\x81\\x94\\xba\\x02\\xae\\xbd\\xa6\\xd0:')\n+\n \n if __name__ == '__main__':\n     unittest.main()\n", "problem_statement": "[dl fail] Is netease module still being maintained?\n### Make sure you are using the *latest* version: run `youtube-dl --version` and ensure your version is *2018.10.29*. If it's not, read [this FAQ entry](https://github.com/rg3/youtube-dl/blob/master/README.md#how-do-i-update-youtube-dl) and update. Issues with outdated version will be rejected.\r\n- [x] I've **verified** and **I assure** that I'm running youtube-dl **2018.10.29**\r\n\r\n### Before submitting an *issue* make sure you have:\r\n- [x] At least skimmed through the [README](https://github.com/rg3/youtube-dl/blob/master/README.md), **most notably** the [FAQ](https://github.com/rg3/youtube-dl#faq) and [BUGS](https://github.com/rg3/youtube-dl#bugs) sections\r\n- [x] [Searched](https://github.com/rg3/youtube-dl/search?type=Issues) the bugtracker for similar issues including closed ones\r\n- [x] Checked that provided video/audio/playlist URLs (if any) are alive and playable in a browser\r\n\r\n### What is the purpose of your *issue*?\r\n- [x] Bug report (encountered problems with youtube-dl)\r\n- [ ] Site support request (request for adding support for a new site)\r\n- [ ] Feature request (request for a new functionality)\r\n- [ ] Question\r\n- [ ] Other\r\n\r\n---\r\n\r\nFull command and output:\r\n\r\n```\r\nC:\\Users\\inkux\\Desktop>youtube-dl https://music.163.com/#/song?id=33166366 --verbose --proxy \"\"\r\n[debug] System config: []\r\n[debug] User config: ['--proxy', 'socks5://[censored]/']\r\n[debug] Custom config: []\r\n[debug] Command-line args: ['https://music.163.com/#/song?id=33166366', '--verbose', '--proxy', '']\r\n[debug] Encodings: locale cp936, fs mbcs, out cp936, pref cp936\r\n[debug] youtube-dl version 2018.10.29\r\n[debug] Python version 3.4.4 (CPython) - Windows-10-10.0.17134\r\n[debug] exe versions: ffmpeg N-90414-gabf35afb6f, ffprobe N-90414-gabf35afb6f\r\n[debug] Proxy map: {}\r\n[netease:song] 33166366: Downloading song info\r\n[netease:song] 33166366: Checking song URL\r\n[netease:song] 33166366: song URL is invalid, skipping\r\n[netease:song] 33166366: Checking song URL\r\n[netease:song] 33166366: song URL is invalid, skipping\r\n[netease:song] 33166366: Checking song URL\r\n[netease:song] 33166366: song URL is invalid, skipping\r\n[netease:song] 33166366: Checking song URL\r\n[netease:song] 33166366: song URL is invalid, skipping\r\n[netease:song] 33166366: Checking song URL\r\n[netease:song] 33166366: song URL is invalid, skipping\r\n[netease:song] 33166366: Checking song URL\r\n[netease:song] 33166366: song URL is invalid, skipping\r\n[netease:song] 33166366: Checking song URL\r\n[netease:song] 33166366: song URL is invalid, skipping\r\n[netease:song] 33166366: Checking song URL\r\n[netease:song] 33166366: song URL is invalid, skipping\r\nERROR: No video formats found; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\dst\\AppData\\Roaming\\Build archive\\youtube-dl\\rg3\\tmpadzwnijc\\build\\youtube_dl\\YoutubeDL.py\", line 792, in extract_info\r\n  File \"C:\\Users\\dst\\AppData\\Roaming\\Build archive\\youtube-dl\\rg3\\tmpadzwnijc\\build\\youtube_dl\\extractor\\common.py\", line 508, in extract\r\n  File \"C:\\Users\\dst\\AppData\\Roaming\\Build archive\\youtube-dl\\rg3\\tmpadzwnijc\\build\\youtube_dl\\extractor\\neteasemusic.py\", line 164, in _real_extract\r\n  File \"C:\\Users\\dst\\AppData\\Roaming\\Build archive\\youtube-dl\\rg3\\tmpadzwnijc\\build\\youtube_dl\\extractor\\common.py\", line 1287, in _sort_formats\r\nyoutube_dl.utils.ExtractorError: No video formats found; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.\r\n```\r\n------\r\nI've noticed from the issue page that netease module had been down for quite a while in 2016, but since I got an instruction to report this and those issues are pretty aged, I decided to report it anyway.\r\n\r\nI was downloading this song which is totally playable in my browser(Google Chrome), and is also downloadable as a mp3 file using netease client (PC, Android), of course using a Chinese IP address.\r\n\r\nAs you can see youtube-dl correctly recognized the ID of the song from its URL but has been unable to obtain any format.\r\n\r\nAnd if this module is never going to be maintained for a period of time, I think it's a good idea to disable the module if it is believed it will never work again, so no one gets a bug report request when they see netease music in your support sites list and then fails to download using yt-dl, given that netease is a pretty popular music site in China.\r\n\r\n------\n", "hints_text": "Well there I am. Pretty late but I randomly came across this extractor.\r\n\r\nThere are changes on netease (music.163.com). They changed endpoints and have more than one now. Also their response is different.\r\n\r\nSong (data as in m4a) related api is\r\n`https://music.163.com/weapi/song/enhance/player/url/v1?csrf_token=`\r\nsong details is now\r\n`https://music.163.com/weapi/v3/song/detail?csrf_token=`\r\nand so on. if there is enough interest I can update the extractor.\r\n\n@blackjack4494 I for one would be interested. Tried to use it just now and came across this report.\nHi, I think this is still not solved, because the traceback still remains the same, how can we start fixing this? There is also a geolocation restriction in this case.\n[This API](https://github.com/Binaryify/NeteaseCloudMusicApi) written in Node.js is able to get the real audio file URL with a given ID and quality. For example, https://api.moeblog.vip/163/ is a deployment of this API, and the real URL of the song with ID 29848621 can be got by `https://api.moeblog.vip/163/?type=url&id=29848621&br=128`. It worked.\r\n\r\nThus, examining the [Node.js API](https://github.com/Binaryify/NeteaseCloudMusicApi) will be helpful.\r\n\r\nGeolocation restriction, like paid contents, seems to be unable to bypass, at least by this API.\r\n\r\nI'll come back in a week after I finish my exams and try to fix this issue if no one else is going to do it.\nI've translated relevant parts of the Node.js API to python, and the following script is able to get the real music file URL with a given id (needs [pycrypto](https://pypi.org/project/pycrypto/)):\r\n\r\n```python\r\nimport requests\r\nimport json\r\nfrom hashlib import md5\r\nfrom Crypto.Cipher import AES\r\nimport Crypto\r\nimport time\r\nfrom random import randint\r\n\r\nHEADERS = {\r\n    \"User-Agent\": \"Mozilla/5.0 (Linux; U; Android 9; zh-cn; Redmi Note 8 Build/PKQ1.190616.001) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/71.0.3578.141 Mobile Safari/537.36 XiaoMi/MiuiBrowser/12.5.22\",\r\n    \"Content-Type\": \"application/x-www-form-urlencoded\",\r\n    \"Referer\": \"https://music.163.com\",\r\n}\r\n\r\n\r\nKEY = \"e82ckenh8dichen8\"\r\n\r\n\r\ndef pad(data):\r\n    # https://stackoverflow.com/a/10550004/12425329\r\n    pad = 16 - len(data) % 16\r\n    return data + pad * chr(pad)\r\n\r\n\r\ndef make_data_and_headers(song_id):\r\n    KEY = \"e82ckenh8dichen8\"\r\n    URL = \"/api/song/enhance/player/url\"\r\n    cookie = {\r\n        \"osver\": None,\r\n        \"deviceId\": None,\r\n        \"appver\": \"8.0.0\",\r\n        \"versioncode\": \"140\",\r\n        \"mobilename\": None,\r\n        \"buildver\": \"1623435496\",\r\n        \"resolution\": \"1920x1080\",\r\n        \"__csrf\": \"\",\r\n        \"os\": \"pc\",\r\n        \"channel\": None,\r\n        \"requestId\": f\"{int(time.time()*1000)}_{randint(0, 1000):04}\",\r\n    }\r\n    text = json.dumps(\r\n        {\"ids\": f\"[{song_id}]\", \"br\": 999000, \"header\": cookie},\r\n        separators=(\",\", \":\"),\r\n    )\r\n    message = f\"nobody{URL}use{text}md5forencrypt\"\r\n    m = md5()\r\n    m.update(message.encode(\"latin1\"))\r\n    digest = m.hexdigest()\r\n\r\n    data = f\"{URL}-36cd479b6b5-{text}-36cd479b6b5-{digest}\"\r\n\r\n    data = '/api/song/enhance/player/url-36cd479b6b5-{\"ids\":\"[33894312]\",\"br\":999000,\"header\":{\"appver\":\"8.0.0\",\"versioncode\":\"140\",\"buildver\":\"1623455100\",\"resolution\":\"1920x1080\",\"__csrf\":\"\",\"os\":\"pc\",\"requestId\":\"1623455100782_0489\"}}-36cd479b6b5-a036727d6cb4f68dc27d0e1962f56eb8'\r\n\r\n    data = pad(data)\r\n\r\n    cipher = Crypto.Cipher.AES.new(KEY, AES.MODE_ECB)\r\n    encrypted = cipher.encrypt(data.encode(\"latin1\"))\r\n\r\n    headers = HEADERS\r\n    process_v = lambda v: v if v is not None else \"undefined\"\r\n    headers.update(\r\n        {\"Cookie\": \"; \".join([f\"{k}={process_v(v)}\" for [k, v] in cookie.items()])}\r\n    )\r\n    return (f\"params={encrypted.hex().upper()}\", headers)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    song_id = input(\"song_id? (default to 491233178)\")\r\n    if not song_id:\r\n        song_id = 491233178\r\n    data, headers = make_data_and_headers(491233178)\r\n    # print(data)\r\n    # print(headers)\r\n    r = requests.post(\r\n        \"https://interface3.music.163.com/eapi/song/enhance/player/url\",\r\n        data=data,  # json.dumps(data, separators=(\",\", \":\")),\r\n        headers=HEADERS,\r\n    )\r\n    print(r.json())\r\n```\r\n\r\nThe next challenge is to adapt it into youtube-dl", "created_at": "2022-09-14T04:31:39Z"}
{"repo": "ytdl-org/youtube-dl", "pull_number": 31182, "instance_id": "ytdl-org__youtube-dl-31182", "issue_numbers": ["31173"], "base_commit": "b0a60ce2032172aeaaf27fe3866ab72768f10cb2", "patch": "diff --git a/youtube_dl/jsinterp.py b/youtube_dl/jsinterp.py\nindex 8e119d08a3b..48c27a1c04b 100644\n--- a/youtube_dl/jsinterp.py\n+++ b/youtube_dl/jsinterp.py\n@@ -7,6 +7,7 @@\n import re\n \n from .utils import (\n+    error_to_compat_str,\n     ExtractorError,\n     js_to_json,\n     remove_quotes,\n@@ -130,7 +131,7 @@ def wrapped(a, b):\n _OPERATOR_RE = '|'.join(map(lambda x: re.escape(x[0]), _OPERATORS + _LOG_OPERATORS))\n \n _MATCHING_PARENS = dict(zip(*zip('()', '{}', '[]')))\n-_QUOTES = '\\'\"'\n+_QUOTES = '\\'\"/'\n \n \n def _ternary(cndn, if_true=True, if_false=False):\n@@ -155,6 +156,12 @@ def __init__(self):\n         ExtractorError.__init__(self, 'Invalid continue')\n \n \n+class JS_Throw(ExtractorError):\n+    def __init__(self, e):\n+        self.error = e\n+        ExtractorError.__init__(self, 'Uncaught exception ' + error_to_compat_str(e))\n+\n+\n class LocalNameSpace(ChainMap):\n     def __getitem__(self, key):\n         try:\n@@ -172,6 +179,17 @@ def __setitem__(self, key, value):\n     def __delitem__(self, key):\n         raise NotImplementedError('Deleting is not supported')\n \n+    # except\n+    def pop(self, key, *args):\n+        try:\n+            off = self.__getitem__(key)\n+            super(LocalNameSpace, self).__delitem__(key)\n+            return off\n+        except KeyError:\n+            if len(args) > 0:\n+                return args[0]\n+            raise\n+\n     def __contains__(self, key):\n         try:\n             super(LocalNameSpace, self).__getitem__(key)\n@@ -188,9 +206,29 @@ class JSInterpreter(object):\n \n     undefined = _UNDEFINED\n \n+    RE_FLAGS = {\n+        # special knowledge: Python's re flags are bitmask values, current max 128\n+        # invent new bitmask values well above that for literal parsing\n+        # TODO: new pattern class to execute matches with these flags\n+        'd': 1024,  # Generate indices for substring matches\n+        'g': 2048,  # Global search\n+        'i': re.I,  # Case-insensitive search\n+        'm': re.M,  # Multi-line search\n+        's': re.S,  # Allows . to match newline characters\n+        'u': re.U,  # Treat a pattern as a sequence of unicode code points\n+        'y': 4096,  # Perform a \"sticky\" search that matches starting at the current position in the target string\n+    }\n+\n+    _EXC_NAME = '__youtube_dl_exception__'\n+    _OBJ_NAME = '__youtube_dl_jsinterp_obj'\n+\n+    OP_CHARS = None\n+\n     def __init__(self, code, objects=None):\n         self.code, self._functions = code, {}\n         self._objects = {} if objects is None else objects\n+        if type(self).OP_CHARS is None:\n+            type(self).OP_CHARS = self.OP_CHARS = self.__op_chars()\n \n     class Exception(ExtractorError):\n         def __init__(self, msg, *args, **kwargs):\n@@ -199,32 +237,64 @@ def __init__(self, msg, *args, **kwargs):\n                 msg = '{0} in: {1!r}'.format(msg.rstrip(), expr[:100])\n             super(JSInterpreter.Exception, self).__init__(msg, *args, **kwargs)\n \n+    @classmethod\n+    def __op_chars(cls):\n+        op_chars = set(';,')\n+        for op in cls._all_operators():\n+            for c in op[0]:\n+                op_chars.add(c)\n+        return op_chars\n+\n     def _named_object(self, namespace, obj):\n         self.__named_object_counter += 1\n-        name = '__youtube_dl_jsinterp_obj%d' % (self.__named_object_counter, )\n+        name = '%s%d' % (self._OBJ_NAME, self.__named_object_counter)\n         namespace[name] = obj\n         return name\n \n-    @staticmethod\n-    def _separate(expr, delim=',', max_split=None, skip_delims=None):\n+    @classmethod\n+    def _regex_flags(cls, expr):\n+        flags = 0\n+        if not expr:\n+            return flags, expr\n+        for idx, ch in enumerate(expr):\n+            if ch not in cls.RE_FLAGS:\n+                break\n+            flags |= cls.RE_FLAGS[ch]\n+        return flags, expr[idx:] if idx > 0 else expr\n+\n+    @classmethod\n+    def _separate(cls, expr, delim=',', max_split=None, skip_delims=None):\n         if not expr:\n             return\n         counters = {k: 0 for k in _MATCHING_PARENS.values()}\n-        start, splits, pos, skipping, delim_len = 0, 0, 0, 0, len(delim) - 1\n-        in_quote, escaping = None, False\n+        start, splits, pos, delim_len = 0, 0, 0, len(delim) - 1\n+        in_quote, escaping, skipping = None, False, 0\n+        after_op, in_regex_char_group, skip_re = True, False, 0\n+\n         for idx, char in enumerate(expr):\n+            if skip_re > 0:\n+                skip_re -= 1\n+                continue\n             if not in_quote:\n                 if char in _MATCHING_PARENS:\n                     counters[_MATCHING_PARENS[char]] += 1\n                 elif char in counters:\n                     counters[char] -= 1\n-            if not escaping:\n-                if char in _QUOTES and in_quote in (char, None):\n-                    in_quote = None if in_quote else char\n-                else:\n-                    escaping = in_quote and char == '\\\\'\n-            else:\n-                escaping = False\n+            if not escaping and char in _QUOTES and in_quote in (char, None):\n+                if in_quote or after_op or char != '/':\n+                    in_quote = None if in_quote and not in_regex_char_group else char\n+                    if in_quote is None and char == '/' and delim != '/':\n+                        # regexp flags\n+                        n_idx = idx + 1\n+                        while n_idx < len(expr) and expr[n_idx] in cls.RE_FLAGS:\n+                            n_idx += 1\n+                        skip_re = n_idx - idx - 1\n+                        if skip_re > 0:\n+                            continue\n+            elif in_quote == '/' and char in '[]':\n+                in_regex_char_group = char == '['\n+            escaping = not escaping and in_quote and char == '\\\\'\n+            after_op = not in_quote and char in cls.OP_CHARS or (char == ' ' and after_op)\n \n             if char != delim[pos] or any(counters.values()) or in_quote:\n                 pos = skipping = 0\n@@ -313,16 +383,23 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n             if should_return:\n                 return ret, should_return\n \n-        m = re.match(r'(?P<var>(?:var|const|let)\\s)|return(?:\\s+|$)', stmt)\n+        m = re.match(r'(?P<var>(?:var|const|let)\\s)|return(?:\\s+|(?=[\"\\'])|$)|(?P<throw>throw\\s+)', stmt)\n         if m:\n             expr = stmt[len(m.group(0)):].strip()\n+            if m.group('throw'):\n+                raise JS_Throw(self.interpret_expression(expr, local_vars, allow_recursion))\n             should_return = not m.group('var')\n         if not expr:\n             return None, should_return\n \n         if expr[0] in _QUOTES:\n             inner, outer = self._separate(expr, expr[0], 1)\n-            inner = json.loads(js_to_json(inner + expr[0]))  # , strict=True))\n+            if expr[0] == '/':\n+                flags, _ = self._regex_flags(outer)\n+                inner, outer = inner.replace('\"', r'\\\"'), ''\n+                inner = re.compile(js_to_json(inner + expr[0]), flags=flags)  # , strict=True))\n+            else:\n+                inner = json.loads(js_to_json(inner + expr[0]))  # , strict=True))\n             if not outer:\n                 return inner, should_return\n             expr = self._named_object(local_vars, inner) + outer\n@@ -374,22 +451,37 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n                 for item in self._separate(inner)])\n             expr = name + outer\n \n-        m = re.match(r'(?P<try>try|finally)\\s*|(?:(?P<catch>catch)|(?P<for>for)|(?P<switch>switch))\\s*\\(', expr)\n+        m = re.match(r'''(?x)\n+            (?P<try>try|finally)\\s*|\n+            (?P<catch>catch\\s*(?P<err>\\(\\s*{_NAME_RE}\\s*\\)))|\n+            (?P<switch>switch)\\s*\\(|\n+            (?P<for>for)\\s*\\(|'''.format(**globals()), expr)\n         md = m.groupdict() if m else {}\n         if md.get('try'):\n             if expr[m.end()] == '{':\n                 try_expr, expr = self._separate_at_paren(expr[m.end():], '}')\n             else:\n                 try_expr, expr = expr[m.end() - 1:], ''\n-            ret, should_abort = self.interpret_statement(try_expr, local_vars, allow_recursion)\n-            if should_abort:\n-                return ret, True\n+            try:\n+                ret, should_abort = self.interpret_statement(try_expr, local_vars, allow_recursion)\n+                if should_abort:\n+                    return ret, True\n+            except JS_Throw as e:\n+                local_vars[self._EXC_NAME] = e.error\n+            except Exception as e:\n+                # XXX: This works for now, but makes debugging future issues very hard\n+                local_vars[self._EXC_NAME] = e\n             ret, should_abort = self.interpret_statement(expr, local_vars, allow_recursion)\n             return ret, should_abort or should_return\n \n         elif md.get('catch'):\n-            # We ignore the catch block\n-            _, expr = self._separate_at_paren(expr, '}')\n+            catch_expr, expr = self._separate_at_paren(expr[m.end():], '}')\n+            if self._EXC_NAME in local_vars:\n+                catch_vars = local_vars.new_child({m.group('err'): local_vars.pop(self._EXC_NAME)})\n+                ret, should_abort = self.interpret_statement(catch_expr, catch_vars, allow_recursion)\n+                if should_abort:\n+                    return ret, True\n+\n             ret, should_abort = self.interpret_statement(expr, local_vars, allow_recursion)\n             return ret, should_abort or should_return\n \n@@ -503,7 +595,7 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n                 raise self.Exception('List index %s must be integer' % (idx, ), expr=expr)\n             idx = int(idx)\n             left_val[idx] = self._operator(\n-                m.group('op'), left_val[idx], m.group('expr'), expr, local_vars, allow_recursion)\n+                m.group('op'), self._index(left_val, idx), m.group('expr'), expr, local_vars, allow_recursion)\n             return left_val[idx], should_return\n \n         elif expr.isdigit():\n", "test_patch": "diff --git a/test/test_jsinterp.py b/test/test_jsinterp.py\nindex 328941e09cc..faddf00d5a6 100644\n--- a/test/test_jsinterp.py\n+++ b/test/test_jsinterp.py\n@@ -9,6 +9,7 @@\n sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n \n import math\n+import re\n \n from youtube_dl.jsinterp import JSInterpreter\n undefined = JSInterpreter.undefined\n@@ -316,19 +317,39 @@ def test_object(self):\n         function x() { return {}; }\n         ''')\n         self.assertEqual(jsi.call_function('x'), {})\n+\n         jsi = JSInterpreter('''\n         function x() { let a = {m1: 42, m2: 0 }; return [a[\"m1\"], a.m2]; }\n         ''')\n         self.assertEqual(jsi.call_function('x'), [42, 0])\n+\n         jsi = JSInterpreter('''\n         function x() { let a; return a?.qq; }\n         ''')\n         self.assertIs(jsi.call_function('x'), undefined)\n+\n         jsi = JSInterpreter('''\n         function x() { let a = {m1: 42, m2: 0 }; return a?.qq; }\n         ''')\n         self.assertIs(jsi.call_function('x'), undefined)\n \n+    def test_regex(self):\n+        jsi = JSInterpreter('''\n+        function x() { let a=/,,[/,913,/](,)}/; }\n+        ''')\n+        self.assertIs(jsi.call_function('x'), None)\n+\n+        jsi = JSInterpreter('''\n+        function x() { let a=/,,[/,913,/](,)}/; return a; }\n+        ''')\n+        # Pythons disagree on the type of a pattern\n+        self.assertTrue(isinstance(jsi.call_function('x'), type(re.compile(''))))\n+\n+        jsi = JSInterpreter('''\n+        function x() { let a=/,,[/,913,/](,)}/i; return a; }\n+        ''')\n+        self.assertEqual(jsi.call_function('x').flags & re.I, re.I)\n+\n \n if __name__ == '__main__':\n     unittest.main()\ndiff --git a/test/test_youtube_signature.py b/test/test_youtube_signature.py\nindex 4d756dad308..43e22388d0b 100644\n--- a/test/test_youtube_signature.py\n+++ b/test/test_youtube_signature.py\n@@ -106,6 +106,10 @@\n         'https://www.youtube.com/s/player/c81bbb4a/player_ias.vflset/en_US/base.js',\n         'gre3EcLurNY2vqp94', 'Z9DfGxWP115WTg',\n     ),\n+    (\n+        'https://www.youtube.com/s/player/1f7d5369/player_ias.vflset/en_US/base.js',\n+        'batNX7sYqIJdkJ', 'IhOkL_zxbkOZBw',\n+    ),\n ]\n \n \n", "problem_statement": "[YouTube] TypeError: '>' not supported between instances of 'int' and 'NoneType' \n## Checklist\r\n\r\n- [x] I'm reporting a broken site support\r\n- [x] I've verified that I'm running youtube-dl version **2021.12.17**\r\n- [x] I've checked that all provided URLs are alive and playable in a browser\r\n- [x] I've checked that all URLs and arguments with special characters are properly quoted or escaped\r\n- [x] I've searched the bugtracker for similar issues including closed ones\r\n\r\n\r\n## Verbose log\r\n\r\n```\r\n[debug] System config: []\r\n[debug] User config: []\r\n[debug] Custom config: []\r\n[debug] Command-line args: [\u2026]\r\n[debug] Encodings: locale UTF-8, fs utf-8, out utf-8, pref UTF-8\r\n[debug] youtube-dl version 2021.12.17\r\n[debug] Git HEAD: e52e8b811\r\n[debug] Python version 3.8.10 (CPython) - Linux-5.8.0-44-lowlatency-x86_64-with-glibc2.29\r\n[debug] exe versions: ffmpeg 4.2.7, ffprobe 4.2.7\r\n[debug] Proxy map: {\u2026}\r\n[youtube] t-hR-TZJT2U: Downloading webpage\r\n[youtube] t-hR-TZJT2U: Downloading MPD manifest\r\nWARNING: [youtube] Unable to decode n-parameter: download likely to be throttled (Failed to evaluate 0 > None (caused by TypeError(\"'>' not supported between instances of 'int' and 'NoneType'\")); please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see  https://yt-dl.org/update  on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output. Traceback (most recent call last):\r\n  File \"/mnt/\u2026/youtube_dl/jsinterp.py\", line 203, in _operator\r\n    return opfunc(left_val, right_val)\r\nTypeError: '>' not supported between instances of 'int' and 'NoneType'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n\u2026\r\n  File \"/mnt/\u2026/youtube_dl/jsinterp.py\", line 590, in interpret_expression\r\n    ret, should_return = self.interpret_statement(expr, local_vars, allow_recursion)\r\n  File \"/mnt/\u2026/youtube_dl/jsinterp.py\", line 454, in interpret_statement\r\n    return self._operator(op, 0 if left_val is None else left_val,\r\n  File \"/mnt/\u2026/youtube_dl/jsinterp.py\", line 205, in _operator\r\n    raise self.Exception('Failed to evaluate {left_val!r} {op} {right_val!r}'.format(**locals()), expr, cause=e)\r\nyoutube_dl.jsinterp.JSInterpreter.Exception: Failed to evaluate 0 > None (caused by TypeError(\"'>' not supported between instances of 'int' and 'NoneType'\")); please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see  https://yt-dl.org/update  on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.\r\n)\r\n```\r\n\r\n\r\n## Description\r\nDownload seems to work nonetheless.\n", "hints_text": "Apparently you just need to reinstall Python 2.7!\r\n\r\nPython 3.5 has this message:\r\n```\r\n...\r\n  File \"/home/df/Documents/src/youtube-dl/youtube_dl/jsinterp.py\", line 205, in _operator\r\n    raise self.Exception('Failed to evaluate {left_val!r} {op} {right_val!r}'.format(**locals()), expr, cause=e)\r\nyoutube_dl.jsinterp.JSInterpreter.Exception: Failed to evaluate 0 > None (caused by TypeError('unorderable types: int() > NoneType()',));  ...\r\n```\r\nHowever today's dev version fixes whatever problem there was:\r\n```shellsession\r\n$ python3.9 -m youtube_dl -F -v 'https://www.youtube.com/watch?v=t-hR-TZJT2U'\r\n[debug] System config: ['--prefer-ffmpeg']\r\n[debug] User config: []\r\n[debug] Custom config: []\r\n[debug] Command-line args: ['-F', '-v', 'https://www.youtube.com/watch?v=t-hR-TZJT2U']\r\n[debug] Encodings: locale UTF-8, fs utf-8, out utf-8, pref UTF-8\r\n[debug] youtube-dl version 2021.12.17\r\n[debug] Git HEAD: e52e8b811\r\n[debug] Python version 3.9.13 (CPython) - Linux-4.4.0-210-generic-i686-with-glibc2.23\r\n[debug] exe versions: avconv 4.3, avprobe 4.3, ffmpeg 4.3, ffprobe 4.3\r\n[debug] Proxy map: {}\r\n[youtube] t-hR-TZJT2U: Downloading webpage\r\n[youtube] t-hR-TZJT2U: Downloading MPD manifest\r\n[debug] [youtube] Decrypted nsig BnLAR5umBf9R48MhQ => y5mzqXT4vmPGow\r\n[debug] [youtube] Decrypted nsig oxC0xXr4o8rSUj1L9 => mfQBGpVcJ9-_fQ\r\n[info] Available formats for t-hR-TZJT2U:\r\nformat code  extension  resolution note\r\n139          m4a        audio only DASH audio   49k , m4a_dash container, mp4a.40.5 (22050Hz), 513.68KiB\r\n140          m4a        audio only tiny  129k , m4a_dash container, mp4a.40.2@129k (44100Hz), 1.33MiB\r\n251          webm       audio only tiny  141k , webm_dash container, opus @141k (48000Hz), 1.45MiB\r\n278          webm       256x144    DASH video   95k , webm_dash container, vp9, 30fps, video only\r\n160          mp4        256x144    DASH video  108k , mp4_dash container, avc1.4d400b, 30fps, video only\r\n242          webm       426x240    DASH video  220k , webm_dash container, vp9, 30fps, video only\r\n133          mp4        426x240    DASH video  242k , mp4_dash container, avc1.4d400c, 30fps, video only\r\n134          mp4        640x360    360p  315k , mp4_dash container, avc1.4d401e@ 315k, 30fps, video only, 3.23MiB\r\n243          webm       640x360    DASH video  405k , webm_dash container, vp9, 30fps, video only\r\n244          webm       854x480    DASH video  752k , webm_dash container, vp9, 30fps, video only\r\n135          mp4        854x480    DASH video 1155k , mp4_dash container, avc1.4d4014, 30fps, video only\r\n136          mp4        1280x720   720p 1092k , mp4_dash container, avc1.64001f@1092k, 30fps, video only, 11.20MiB\r\n247          webm       1280x720   DASH video 1505k , webm_dash container, vp9, 30fps, video only\r\n302          webm       1280x720   DASH video 2646k , webm_dash container, vp9, 60fps, video only\r\n298          mp4        1280x720   DASH video 3465k , mp4_dash container, avc1.4d400d, 60fps, video only\r\n299          mp4        1920x1080  1080p60 3021k , mp4_dash container, avc1.64002a@3021k, 60fps, video only, 30.98MiB\r\n303          webm       1920x1080  DASH video 4410k , webm_dash container, vp9, 60fps, video only\r\n18           mp4        640x360    360p  443k , avc1.42001E, 30fps, mp4a.40.2 (44100Hz)\r\n22           mp4        1280x720   720p 1220k , avc1.64001F, 30fps, mp4a.40.2 (44100Hz) (best)\r\n$\r\n``` \r\nComing in  this version:\r\n* operator `??`\r\n* operator `?.`\r\n* operator `**`\r\n* accurate operator functions ([Douglas Crockford special](https://devclass.com/2022/08/04/retire_javascript_says-json-creator-douglas-crockford/): `undefined ** 0 === 1`)\r\n* `undefined` handling\r\n* object literals `{a: 1, \"b\": expr}`\r\n\nThe problem is with this line 264 of the pretty-printed nsig code:\r\n```\r\n      6 >= c[104] ? (0, c[22]) ((0, c[1]) ((0, c[5]) (c[13], c[44]), c[76], c[12], c[75]), c[2], (0, c[0]) (((0, c[82]) (c[12]), (0, c[98]) (c[72], c[12])), c[106], (0, c[31]) (c[63], c[35]) > (0, c[102]) (c[ - 120 * Math.pow(7, 2) + 5969], c[82]), c[12], c[10]), c[29], c[21])  : (0, c[81]) ((0, c[40]) (c[10], c[11], (0, c[20]) ()), c[53], (0, c[new Date('31 December 1969 13:15:53 -1045') / 1000]) ((0, c[53]) ((0, c[12]) (c[26]), c[35], c[10], c[275 % Math.pow(new Date('1969-12-31T17:00:02.000-07:00') / 1000, 5) + 9]), c[40], c[67], c[74], (0, c[21]) ()), (0, c[53]) ((0, c[35]) (c[42], c[104]), c[35], c[67], c[48]), (0, c[86]) (c[Math.pow(5, 5) + 17176 + - 20291], c[51]), c[49], c[64], c[42]),\r\n```\r\nyt-dl is trying to evaluate this part of the line, and the RHS of the '>' is evaluating to `None`: \r\n```\r\n(0,c[0])(((0,c[82])(c[12]),(0,c[98])(c[72],c[12])),c[106],(0,c[31])(c[63],c[35])>(0,c[102])(c[-120*Math.pow(7,2)+5969],c[82]),c[12],c[10])\r\n```\r\nHere `c[102]` is a number, so something went wrong way back, something that behaves differently in Py2.7 (no error, though I didn't check the download speed), Py3.5, Py 3.8+.\n> Apparently you just need to reinstall Python 2.7!\r\n\r\nI tried it, but the error persists. Maybe Ubuntu mirrors didn't get the update yet.\nSorry, that wasn't a serious suggestion. You'd have to go through more hoops to get Python 2.7 running your yt-dl, anyway.\r\n\r\nPlease wait for the new version.\n> You'd have to go through more hoops to get Python 2.7 running your yt-dl, anyway.\r\n\r\nIndeed, I had to edit the shebang line of my wrapper script. I didn't remember I had upgraded it to use python3 yet. Now the warning seems gone and download speed seems to be back to normal. So thanks for reminding me of that extra step!", "created_at": "2022-08-18T19:33:14Z"}
{"repo": "ytdl-org/youtube-dl", "pull_number": 31175, "instance_id": "ytdl-org__youtube-dl-31175", "issue_numbers": ["31173"], "base_commit": "e52e8b8111cf7ca27daef184bacd926865e951b1", "patch": "diff --git a/youtube_dl/jsinterp.py b/youtube_dl/jsinterp.py\nindex c60a9b3c234..8e119d08a3b 100644\n--- a/youtube_dl/jsinterp.py\n+++ b/youtube_dl/jsinterp.py\n@@ -7,7 +7,6 @@\n import re\n \n from .utils import (\n-    NO_DEFAULT,\n     ExtractorError,\n     js_to_json,\n     remove_quotes,\n@@ -21,6 +20,70 @@\n \n _NAME_RE = r'[a-zA-Z_$][\\w$]*'\n \n+_UNDEFINED = object()\n+\n+\n+def _js_bit_op(op):\n+\n+    def wrapped(a, b):\n+        def zeroise(x):\n+            return 0 if x in (None, _UNDEFINED) else x\n+        return op(zeroise(a), zeroise(b))\n+\n+    return wrapped\n+\n+\n+def _js_arith_op(op):\n+\n+    def wrapped(a, b):\n+        if _UNDEFINED in (a, b):\n+            return float('nan')\n+        return op(a or 0, b or 0)\n+\n+    return wrapped\n+\n+\n+def _js_div(a, b):\n+    if _UNDEFINED in (a, b) or not (a and b):\n+        return float('nan')\n+    return float('inf') if not b else operator.truediv(a or 0, b)\n+\n+\n+def _js_mod(a, b):\n+    if _UNDEFINED in (a, b) or not b:\n+        return float('nan')\n+    return (a or 0) % b\n+\n+\n+def _js_exp(a, b):\n+    if not b:\n+        # even 0 ** 0 !!\n+        return 1\n+    if _UNDEFINED in (a, b):\n+        return float('nan')\n+    return (a or 0) ** b\n+\n+\n+def _js_eq_op(op):\n+\n+    def wrapped(a, b):\n+        if set((a, b)) <= set((None, _UNDEFINED)):\n+            return op(a, a)\n+        return op(a, b)\n+\n+    return wrapped\n+\n+\n+def _js_comp_op(op):\n+\n+    def wrapped(a, b):\n+        if _UNDEFINED in (a, b):\n+            return False\n+        return op(a or 0, b or 0)\n+\n+    return wrapped\n+\n+\n # (op, definition) in order of binding priority, tightest first\n # avoid dict to maintain order\n # definition None => Defined in JSInterpreter._operator\n@@ -30,40 +93,38 @@\n )\n \n _OPERATORS = (\n-    ('|', operator.or_),\n-    ('^', operator.xor),\n-    ('&', operator.and_),\n-    ('>>', operator.rshift),\n-    ('<<', operator.lshift),\n-    ('+', operator.add),\n-    ('-', operator.sub),\n-    ('*', operator.mul),\n-    ('/', operator.truediv),\n-    ('%', operator.mod),\n+    ('>>', _js_bit_op(operator.rshift)),\n+    ('<<', _js_bit_op(operator.lshift)),\n+    ('+', _js_arith_op(operator.add)),\n+    ('-', _js_arith_op(operator.sub)),\n+    ('*', _js_arith_op(operator.mul)),\n+    ('/', _js_div),\n+    ('%', _js_mod),\n+    ('**', _js_exp),\n )\n \n _COMP_OPERATORS = (\n     ('===', operator.is_),\n-    ('==', operator.eq),\n+    ('==', _js_eq_op(operator.eq)),\n     ('!==', operator.is_not),\n-    ('!=', operator.ne),\n-    ('<=', operator.le),\n-    ('>=', operator.ge),\n-    ('<', operator.lt),\n-    ('>', operator.gt),\n+    ('!=', _js_eq_op(operator.ne)),\n+    ('<=', _js_comp_op(operator.le)),\n+    ('>=', _js_comp_op(operator.ge)),\n+    ('<', _js_comp_op(operator.lt)),\n+    ('>', _js_comp_op(operator.gt)),\n )\n \n _LOG_OPERATORS = (\n-    ('&', operator.and_),\n-    ('|', operator.or_),\n-    ('^', operator.xor),\n+    ('|', _js_bit_op(operator.or_)),\n+    ('^', _js_bit_op(operator.xor)),\n+    ('&', _js_bit_op(operator.and_)),\n )\n \n _SC_OPERATORS = (\n     ('?', None),\n+    ('??', None),\n     ('||', None),\n     ('&&', None),\n-    # TODO: ('??', None),\n )\n \n _OPERATOR_RE = '|'.join(map(lambda x: re.escape(x[0]), _OPERATORS + _LOG_OPERATORS))\n@@ -74,7 +135,7 @@\n \n def _ternary(cndn, if_true=True, if_false=False):\n     \"\"\"Simulate JS's ternary operator (cndn?if_true:if_false)\"\"\"\n-    if cndn in (False, None, 0, ''):\n+    if cndn in (False, None, 0, '', _UNDEFINED):\n         return if_false\n     try:\n         if math.isnan(cndn):  # NB: NaN cannot be checked by membership\n@@ -95,6 +156,12 @@ def __init__(self):\n \n \n class LocalNameSpace(ChainMap):\n+    def __getitem__(self, key):\n+        try:\n+            return super(LocalNameSpace, self).__getitem__(key)\n+        except KeyError:\n+            return _UNDEFINED\n+\n     def __setitem__(self, key, value):\n         for scope in self.maps:\n             if key in scope:\n@@ -105,6 +172,13 @@ def __setitem__(self, key, value):\n     def __delitem__(self, key):\n         raise NotImplementedError('Deleting is not supported')\n \n+    def __contains__(self, key):\n+        try:\n+            super(LocalNameSpace, self).__getitem__(key)\n+            return True\n+        except KeyError:\n+            return False\n+\n     def __repr__(self):\n         return 'LocalNameSpace%s' % (self.maps, )\n \n@@ -112,6 +186,8 @@ def __repr__(self):\n class JSInterpreter(object):\n     __named_object_counter = 0\n \n+    undefined = _UNDEFINED\n+\n     def __init__(self, code, objects=None):\n         self.code, self._functions = code, {}\n         self._objects = {} if objects is None else objects\n@@ -185,12 +261,16 @@ def _separate_at_paren(cls, expr, delim):\n     @staticmethod\n     def _all_operators():\n         return itertools.chain(\n+            # Ref: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Operator_Precedence\n             _SC_OPERATORS, _LOG_OPERATORS, _COMP_OPERATORS, _OPERATORS)\n \n     def _operator(self, op, left_val, right_expr, expr, local_vars, allow_recursion):\n         if op in ('||', '&&'):\n             if (op == '&&') ^ _ternary(left_val):\n                 return left_val  # short circuiting\n+        elif op == '??':\n+            if left_val not in (None, self.undefined):\n+                return left_val\n         elif op == '?':\n             right_expr = _ternary(left_val, *self._separate(right_expr, ':', 1))\n \n@@ -204,12 +284,14 @@ def _operator(self, op, left_val, right_expr, expr, local_vars, allow_recursion)\n         except Exception as e:\n             raise self.Exception('Failed to evaluate {left_val!r} {op} {right_val!r}'.format(**locals()), expr, cause=e)\n \n-    def _index(self, obj, idx):\n+    def _index(self, obj, idx, allow_undefined=False):\n         if idx == 'length':\n             return len(obj)\n         try:\n             return obj[int(idx)] if isinstance(obj, list) else obj[idx]\n         except Exception as e:\n+            if allow_undefined:\n+                return self.undefined\n             raise self.Exception('Cannot get index {idx}'.format(**locals()), expr=repr(obj), cause=e)\n \n     def _dump(self, obj, namespace):\n@@ -249,8 +331,8 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n             obj = expr[4:]\n             if obj.startswith('Date('):\n                 left, right = self._separate_at_paren(obj[4:], ')')\n-                left = self.interpret_expression(left, local_vars, allow_recursion)\n-                expr = unified_timestamp(left, False)\n+                expr = unified_timestamp(\n+                    self.interpret_expression(left, local_vars, allow_recursion), False)\n                 if not expr:\n                     raise self.Exception('Failed to parse date {left!r}'.format(**locals()), expr=expr)\n                 expr = self._dump(int(expr * 1000), local_vars) + right\n@@ -263,6 +345,14 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n \n         if expr.startswith('{'):\n             inner, outer = self._separate_at_paren(expr, '}')\n+            # try for object expression\n+            sub_expressions = [list(self._separate(sub_expr.strip(), ':', 1)) for sub_expr in self._separate(inner)]\n+            if all(len(sub_expr) == 2 for sub_expr in sub_expressions):\n+                return dict(\n+                    (key_expr if re.match(_NAME_RE, key_expr) else key_expr,\n+                     self.interpret_expression(val_expr, local_vars, allow_recursion))\n+                    for key_expr, val_expr in sub_expressions), should_return\n+            # or statement list\n             inner, should_abort = self.interpret_statement(inner, local_vars, allow_recursion)\n             if not outer or should_abort:\n                 return inner, should_abort or should_return\n@@ -387,13 +477,13 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n             (?P<assign>\n                 (?P<out>{_NAME_RE})(?:\\[(?P<index>[^\\]]+?)\\])?\\s*\n                 (?P<op>{_OPERATOR_RE})?\n-                =(?P<expr>.*)$\n+                =(?!=)(?P<expr>.*)$\n             )|(?P<return>\n                 (?!if|return|true|false|null|undefined)(?P<name>{_NAME_RE})$\n             )|(?P<indexing>\n                 (?P<in>{_NAME_RE})\\[(?P<idx>.+)\\]$\n             )|(?P<attribute>\n-                (?P<var>{_NAME_RE})(?:\\.(?P<member>[^(]+)|\\[(?P<member2>[^\\]]+)\\])\\s*\n+                (?P<var>{_NAME_RE})(?:(?P<nullish>\\?)?\\.(?P<member>[^(]+)|\\[(?P<member2>[^\\]]+)\\])\\s*\n             )|(?P<function>\n                 (?P<fname>{_NAME_RE})\\((?P<args>.*)\\)$\n             )'''.format(**globals()), expr)\n@@ -405,7 +495,7 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n                 local_vars[m.group('out')] = self._operator(\n                     m.group('op'), left_val, m.group('expr'), expr, local_vars, allow_recursion)\n                 return local_vars[m.group('out')], should_return\n-            elif left_val is None:\n+            elif left_val in (None, self.undefined):\n                 raise self.Exception('Cannot index undefined variable ' + m.group('out'), expr=expr)\n \n             idx = self.interpret_expression(m.group('index'), local_vars, allow_recursion)\n@@ -424,6 +514,9 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n         elif expr == 'continue':\n             raise JS_Continue()\n \n+        elif expr == 'undefined':\n+            return self.undefined, should_return\n+\n         elif md.get('return'):\n             return local_vars[m.group('name')], should_return\n \n@@ -441,7 +534,9 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n \n         for op, _ in self._all_operators():\n             # hackety: </> have higher priority than <</>>, but don't confuse them\n-            skip_delim = (op + op) if op in ('<', '>') else None\n+            skip_delim = (op + op) if op in '<>*?' else None\n+            if op == '?':\n+                skip_delim = (skip_delim, '?.')\n             separated = list(self._separate(expr, op, skip_delims=skip_delim))\n             if len(separated) < 2:\n                 continue\n@@ -451,12 +546,10 @@ def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n                 right_expr = '-' + right_expr\n                 separated.pop()\n             left_val = self.interpret_expression(op.join(separated), local_vars, allow_recursion)\n-            return self._operator(op, 0 if left_val is None else left_val,\n-                                  right_expr, expr, local_vars, allow_recursion), should_return\n+            return self._operator(op, left_val, right_expr, expr, local_vars, allow_recursion), should_return\n \n         if md.get('attribute'):\n-            variable = m.group('var')\n-            member = m.group('member')\n+            variable, member, nullish = m.group('var', 'member', 'nullish')\n             if not member:\n                 member = self.interpret_expression(m.group('member2'), local_vars, allow_recursion)\n             arg_str = expr[m.end():]\n@@ -477,15 +570,24 @@ def eval_method():\n                     'String': compat_str,\n                     'Math': float,\n                 }\n-                obj = local_vars.get(variable, types.get(variable, NO_DEFAULT))\n-                if obj is NO_DEFAULT:\n-                    if variable not in self._objects:\n-                        self._objects[variable] = self.extract_object(variable)\n-                    obj = self._objects[variable]\n+                obj = local_vars.get(variable)\n+                if obj in (self.undefined, None):\n+                    obj = types.get(variable, self.undefined)\n+                if obj is self.undefined:\n+                    try:\n+                        if variable not in self._objects:\n+                            self._objects[variable] = self.extract_object(variable)\n+                        obj = self._objects[variable]\n+                    except self.Exception:\n+                        if not nullish:\n+                            raise\n+\n+                if nullish and obj is self.undefined:\n+                    return self.undefined\n \n                 # Member access\n                 if arg_str is None:\n-                    return self._index(obj, member)\n+                    return self._index(obj, member, nullish)\n \n                 # Function call\n                 argvals = [\n@@ -660,7 +762,14 @@ def call_function(self, funcname, *args):\n     def build_arglist(cls, arg_text):\n         if not arg_text:\n             return []\n-        return list(filter(None, (x.strip() or None for x in cls._separate(arg_text))))\n+\n+        def valid_arg(y):\n+            y = y.strip()\n+            if not y:\n+                raise cls.Exception('Missing arg in \"%s\"' % (arg_text, ))\n+            return y\n+\n+        return [valid_arg(x) for x in cls._separate(arg_text)]\n \n     def build_function(self, argnames, code, *global_stack):\n         global_stack = list(global_stack) or [{}]\n", "test_patch": "diff --git a/test/test_jsinterp.py b/test/test_jsinterp.py\nindex c6c93174353..328941e09cc 100644\n--- a/test/test_jsinterp.py\n+++ b/test/test_jsinterp.py\n@@ -8,7 +8,10 @@\n import unittest\n sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n \n+import math\n+\n from youtube_dl.jsinterp import JSInterpreter\n+undefined = JSInterpreter.undefined\n \n \n class TestJSInterpreter(unittest.TestCase):\n@@ -48,6 +51,9 @@ def test_operators(self):\n         jsi = JSInterpreter('function f(){return 1 << 5;}')\n         self.assertEqual(jsi.call_function('f'), 32)\n \n+        jsi = JSInterpreter('function f(){return 2 ** 5}')\n+        self.assertEqual(jsi.call_function('f'), 32)\n+\n         jsi = JSInterpreter('function f(){return 19 & 21;}')\n         self.assertEqual(jsi.call_function('f'), 17)\n \n@@ -57,6 +63,15 @@ def test_operators(self):\n         jsi = JSInterpreter('function f(){return []? 2+3: 4;}')\n         self.assertEqual(jsi.call_function('f'), 5)\n \n+        jsi = JSInterpreter('function f(){return 1 == 2}')\n+        self.assertEqual(jsi.call_function('f'), False)\n+\n+        jsi = JSInterpreter('function f(){return 0 && 1 || 2;}')\n+        self.assertEqual(jsi.call_function('f'), 2)\n+\n+        jsi = JSInterpreter('function f(){return 0 ?? 42;}')\n+        self.assertEqual(jsi.call_function('f'), 0)\n+\n     def test_array_access(self):\n         jsi = JSInterpreter('function f(){var x = [1,2,3]; x[0] = 4; x[0] = 5; x[2.0] = 7; return x;}')\n         self.assertEqual(jsi.call_function('f'), [5, 2, 7])\n@@ -203,6 +218,11 @@ def test_comma(self):\n         ''')\n         self.assertEqual(jsi.call_function('x'), 7)\n \n+        jsi = JSInterpreter('''\n+        function x() { return (l=[0,1,2,3], function(a, b){return a+b})((l[1], l[2]), l[3]) }\n+        ''')\n+        self.assertEqual(jsi.call_function('x'), 5)\n+\n     def test_void(self):\n         jsi = JSInterpreter('''\n         function x() { return void 42; }\n@@ -215,6 +235,100 @@ def test_return_function(self):\n         ''')\n         self.assertEqual(jsi.call_function('x')([]), 1)\n \n+    def test_null(self):\n+        jsi = JSInterpreter('''\n+        function x() { return null; }\n+        ''')\n+        self.assertIs(jsi.call_function('x'), None)\n+\n+        jsi = JSInterpreter('''\n+        function x() { return [null > 0, null < 0, null == 0, null === 0]; }\n+        ''')\n+        self.assertEqual(jsi.call_function('x'), [False, False, False, False])\n+\n+        jsi = JSInterpreter('''\n+        function x() { return [null >= 0, null <= 0]; }\n+        ''')\n+        self.assertEqual(jsi.call_function('x'), [True, True])\n+\n+    def test_undefined(self):\n+        jsi = JSInterpreter('''\n+        function x() { return undefined === undefined; }\n+        ''')\n+        self.assertTrue(jsi.call_function('x'))\n+\n+        jsi = JSInterpreter('''\n+        function x() { return undefined; }\n+        ''')\n+        self.assertIs(jsi.call_function('x'), undefined)\n+\n+        jsi = JSInterpreter('''\n+        function x() { let v; return v; }\n+        ''')\n+        self.assertIs(jsi.call_function('x'), undefined)\n+\n+        jsi = JSInterpreter('''\n+        function x() { return [undefined === undefined, undefined == undefined, undefined < undefined, undefined > undefined]; }\n+        ''')\n+        self.assertEqual(jsi.call_function('x'), [True, True, False, False])\n+\n+        jsi = JSInterpreter('''\n+        function x() { return [undefined === 0, undefined == 0, undefined < 0, undefined > 0]; }\n+        ''')\n+        self.assertEqual(jsi.call_function('x'), [False, False, False, False])\n+\n+        jsi = JSInterpreter('''\n+        function x() { return [undefined >= 0, undefined <= 0]; }\n+        ''')\n+        self.assertEqual(jsi.call_function('x'), [False, False])\n+\n+        jsi = JSInterpreter('''\n+        function x() { return [undefined > null, undefined < null, undefined == null, undefined === null]; }\n+        ''')\n+        self.assertEqual(jsi.call_function('x'), [False, False, True, False])\n+\n+        jsi = JSInterpreter('''\n+        function x() { return [undefined === null, undefined == null, undefined < null, undefined > null]; }\n+        ''')\n+        self.assertEqual(jsi.call_function('x'), [False, True, False, False])\n+\n+        jsi = JSInterpreter('''\n+        function x() { let v; return [42+v, v+42, v**42, 42**v, 0**v]; }\n+        ''')\n+        for y in jsi.call_function('x'):\n+            self.assertTrue(math.isnan(y))\n+\n+        jsi = JSInterpreter('''\n+        function x() { let v; return v**0; }\n+        ''')\n+        self.assertEqual(jsi.call_function('x'), 1)\n+\n+        jsi = JSInterpreter('''\n+        function x() { let v; return [v>42, v<=42, v&&42, 42&&v]; }\n+        ''')\n+        self.assertEqual(jsi.call_function('x'), [False, False, undefined, undefined])\n+\n+        jsi = JSInterpreter('function x(){return undefined ?? 42; }')\n+        self.assertEqual(jsi.call_function('x'), 42)\n+\n+    def test_object(self):\n+        jsi = JSInterpreter('''\n+        function x() { return {}; }\n+        ''')\n+        self.assertEqual(jsi.call_function('x'), {})\n+        jsi = JSInterpreter('''\n+        function x() { let a = {m1: 42, m2: 0 }; return [a[\"m1\"], a.m2]; }\n+        ''')\n+        self.assertEqual(jsi.call_function('x'), [42, 0])\n+        jsi = JSInterpreter('''\n+        function x() { let a; return a?.qq; }\n+        ''')\n+        self.assertIs(jsi.call_function('x'), undefined)\n+        jsi = JSInterpreter('''\n+        function x() { let a = {m1: 42, m2: 0 }; return a?.qq; }\n+        ''')\n+        self.assertIs(jsi.call_function('x'), undefined)\n+\n \n if __name__ == '__main__':\n     unittest.main()\ndiff --git a/test/test_youtube_signature.py b/test/test_youtube_signature.py\nindex 6e955e0f019..4d756dad308 100644\n--- a/test/test_youtube_signature.py\n+++ b/test/test_youtube_signature.py\n@@ -102,6 +102,10 @@\n         'https://www.youtube.com/s/player/4c3f79c5/player_ias.vflset/en_US/base.js',\n         'TDCstCG66tEAO5pR9o', 'dbxNtZ14c-yWyw',\n     ),\n+    (\n+        'https://www.youtube.com/s/player/c81bbb4a/player_ias.vflset/en_US/base.js',\n+        'gre3EcLurNY2vqp94', 'Z9DfGxWP115WTg',\n+    ),\n ]\n \n \n", "problem_statement": "[YouTube] TypeError: '>' not supported between instances of 'int' and 'NoneType' \n## Checklist\r\n\r\n- [x] I'm reporting a broken site support\r\n- [x] I've verified that I'm running youtube-dl version **2021.12.17**\r\n- [x] I've checked that all provided URLs are alive and playable in a browser\r\n- [x] I've checked that all URLs and arguments with special characters are properly quoted or escaped\r\n- [x] I've searched the bugtracker for similar issues including closed ones\r\n\r\n\r\n## Verbose log\r\n\r\n```\r\n[debug] System config: []\r\n[debug] User config: []\r\n[debug] Custom config: []\r\n[debug] Command-line args: [\u2026]\r\n[debug] Encodings: locale UTF-8, fs utf-8, out utf-8, pref UTF-8\r\n[debug] youtube-dl version 2021.12.17\r\n[debug] Git HEAD: e52e8b811\r\n[debug] Python version 3.8.10 (CPython) - Linux-5.8.0-44-lowlatency-x86_64-with-glibc2.29\r\n[debug] exe versions: ffmpeg 4.2.7, ffprobe 4.2.7\r\n[debug] Proxy map: {\u2026}\r\n[youtube] t-hR-TZJT2U: Downloading webpage\r\n[youtube] t-hR-TZJT2U: Downloading MPD manifest\r\nWARNING: [youtube] Unable to decode n-parameter: download likely to be throttled (Failed to evaluate 0 > None (caused by TypeError(\"'>' not supported between instances of 'int' and 'NoneType'\")); please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see  https://yt-dl.org/update  on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output. Traceback (most recent call last):\r\n  File \"/mnt/\u2026/youtube_dl/jsinterp.py\", line 203, in _operator\r\n    return opfunc(left_val, right_val)\r\nTypeError: '>' not supported between instances of 'int' and 'NoneType'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n\u2026\r\n  File \"/mnt/\u2026/youtube_dl/jsinterp.py\", line 590, in interpret_expression\r\n    ret, should_return = self.interpret_statement(expr, local_vars, allow_recursion)\r\n  File \"/mnt/\u2026/youtube_dl/jsinterp.py\", line 454, in interpret_statement\r\n    return self._operator(op, 0 if left_val is None else left_val,\r\n  File \"/mnt/\u2026/youtube_dl/jsinterp.py\", line 205, in _operator\r\n    raise self.Exception('Failed to evaluate {left_val!r} {op} {right_val!r}'.format(**locals()), expr, cause=e)\r\nyoutube_dl.jsinterp.JSInterpreter.Exception: Failed to evaluate 0 > None (caused by TypeError(\"'>' not supported between instances of 'int' and 'NoneType'\")); please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see  https://yt-dl.org/update  on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.\r\n)\r\n```\r\n\r\n\r\n## Description\r\nDownload seems to work nonetheless.\n", "hints_text": "Apparently you just need to reinstall Python 2.7!\r\n\r\nPython 3.5 has this message:\r\n```\r\n...\r\n  File \"/home/df/Documents/src/youtube-dl/youtube_dl/jsinterp.py\", line 205, in _operator\r\n    raise self.Exception('Failed to evaluate {left_val!r} {op} {right_val!r}'.format(**locals()), expr, cause=e)\r\nyoutube_dl.jsinterp.JSInterpreter.Exception: Failed to evaluate 0 > None (caused by TypeError('unorderable types: int() > NoneType()',));  ...\r\n```\r\nHowever today's dev version fixes whatever problem there was:\r\n```shellsession\r\n$ python3.9 -m youtube_dl -F -v 'https://www.youtube.com/watch?v=t-hR-TZJT2U'\r\n[debug] System config: ['--prefer-ffmpeg']\r\n[debug] User config: []\r\n[debug] Custom config: []\r\n[debug] Command-line args: ['-F', '-v', 'https://www.youtube.com/watch?v=t-hR-TZJT2U']\r\n[debug] Encodings: locale UTF-8, fs utf-8, out utf-8, pref UTF-8\r\n[debug] youtube-dl version 2021.12.17\r\n[debug] Git HEAD: e52e8b811\r\n[debug] Python version 3.9.13 (CPython) - Linux-4.4.0-210-generic-i686-with-glibc2.23\r\n[debug] exe versions: avconv 4.3, avprobe 4.3, ffmpeg 4.3, ffprobe 4.3\r\n[debug] Proxy map: {}\r\n[youtube] t-hR-TZJT2U: Downloading webpage\r\n[youtube] t-hR-TZJT2U: Downloading MPD manifest\r\n[debug] [youtube] Decrypted nsig BnLAR5umBf9R48MhQ => y5mzqXT4vmPGow\r\n[debug] [youtube] Decrypted nsig oxC0xXr4o8rSUj1L9 => mfQBGpVcJ9-_fQ\r\n[info] Available formats for t-hR-TZJT2U:\r\nformat code  extension  resolution note\r\n139          m4a        audio only DASH audio   49k , m4a_dash container, mp4a.40.5 (22050Hz), 513.68KiB\r\n140          m4a        audio only tiny  129k , m4a_dash container, mp4a.40.2@129k (44100Hz), 1.33MiB\r\n251          webm       audio only tiny  141k , webm_dash container, opus @141k (48000Hz), 1.45MiB\r\n278          webm       256x144    DASH video   95k , webm_dash container, vp9, 30fps, video only\r\n160          mp4        256x144    DASH video  108k , mp4_dash container, avc1.4d400b, 30fps, video only\r\n242          webm       426x240    DASH video  220k , webm_dash container, vp9, 30fps, video only\r\n133          mp4        426x240    DASH video  242k , mp4_dash container, avc1.4d400c, 30fps, video only\r\n134          mp4        640x360    360p  315k , mp4_dash container, avc1.4d401e@ 315k, 30fps, video only, 3.23MiB\r\n243          webm       640x360    DASH video  405k , webm_dash container, vp9, 30fps, video only\r\n244          webm       854x480    DASH video  752k , webm_dash container, vp9, 30fps, video only\r\n135          mp4        854x480    DASH video 1155k , mp4_dash container, avc1.4d4014, 30fps, video only\r\n136          mp4        1280x720   720p 1092k , mp4_dash container, avc1.64001f@1092k, 30fps, video only, 11.20MiB\r\n247          webm       1280x720   DASH video 1505k , webm_dash container, vp9, 30fps, video only\r\n302          webm       1280x720   DASH video 2646k , webm_dash container, vp9, 60fps, video only\r\n298          mp4        1280x720   DASH video 3465k , mp4_dash container, avc1.4d400d, 60fps, video only\r\n299          mp4        1920x1080  1080p60 3021k , mp4_dash container, avc1.64002a@3021k, 60fps, video only, 30.98MiB\r\n303          webm       1920x1080  DASH video 4410k , webm_dash container, vp9, 60fps, video only\r\n18           mp4        640x360    360p  443k , avc1.42001E, 30fps, mp4a.40.2 (44100Hz)\r\n22           mp4        1280x720   720p 1220k , avc1.64001F, 30fps, mp4a.40.2 (44100Hz) (best)\r\n$\r\n``` \r\nComing in  this version:\r\n* operator `??`\r\n* operator `?.`\r\n* operator `**`\r\n* accurate operator functions ([Douglas Crockford special](https://devclass.com/2022/08/04/retire_javascript_says-json-creator-douglas-crockford/): `undefined ** 0 === 1`)\r\n* `undefined` handling\r\n* object literals `{a: 1, \"b\": expr}`\r\n\nThe problem is with this line 264 of the pretty-printed nsig code:\r\n```\r\n      6 >= c[104] ? (0, c[22]) ((0, c[1]) ((0, c[5]) (c[13], c[44]), c[76], c[12], c[75]), c[2], (0, c[0]) (((0, c[82]) (c[12]), (0, c[98]) (c[72], c[12])), c[106], (0, c[31]) (c[63], c[35]) > (0, c[102]) (c[ - 120 * Math.pow(7, 2) + 5969], c[82]), c[12], c[10]), c[29], c[21])  : (0, c[81]) ((0, c[40]) (c[10], c[11], (0, c[20]) ()), c[53], (0, c[new Date('31 December 1969 13:15:53 -1045') / 1000]) ((0, c[53]) ((0, c[12]) (c[26]), c[35], c[10], c[275 % Math.pow(new Date('1969-12-31T17:00:02.000-07:00') / 1000, 5) + 9]), c[40], c[67], c[74], (0, c[21]) ()), (0, c[53]) ((0, c[35]) (c[42], c[104]), c[35], c[67], c[48]), (0, c[86]) (c[Math.pow(5, 5) + 17176 + - 20291], c[51]), c[49], c[64], c[42]),\r\n```\r\nyt-dl is trying to evaluate this part of the line, and the RHS of the '>' is evaluating to `None`: \r\n```\r\n(0,c[0])(((0,c[82])(c[12]),(0,c[98])(c[72],c[12])),c[106],(0,c[31])(c[63],c[35])>(0,c[102])(c[-120*Math.pow(7,2)+5969],c[82]),c[12],c[10])\r\n```\r\nHere `c[102]` is a number, so something went wrong way back, something that behaves differently in Py2.7 (no error, though I didn't check the download speed), Py3.5, Py 3.8+.\n> Apparently you just need to reinstall Python 2.7!\r\n\r\nI tried it, but the error persists. Maybe Ubuntu mirrors didn't get the update yet.\nSorry, that wasn't a serious suggestion. You'd have to go through more hoops to get Python 2.7 running your yt-dl, anyway.\r\n\r\nPlease wait for the new version.\n> You'd have to go through more hoops to get Python 2.7 running your yt-dl, anyway.\r\n\r\nIndeed, I had to edit the shebang line of my wrapper script. I didn't remember I had upgraded it to use python3 yet. Now the warning seems gone and download speed seems to be back to normal. So thanks for reminding me of that extra step!", "created_at": "2022-08-17T04:57:07Z"}
{"repo": "ytdl-org/youtube-dl", "pull_number": 31152, "instance_id": "ytdl-org__youtube-dl-31152", "issue_numbers": ["31149"], "base_commit": "0f6422590e44e99e9b81cf2367666efe89fae3aa", "patch": "diff --git a/youtube_dl/extractor/zdf.py b/youtube_dl/extractor/zdf.py\nindex 3d39bb33aec..fcc63ef52ca 100644\n--- a/youtube_dl/extractor/zdf.py\n+++ b/youtube_dl/extractor/zdf.py\n@@ -8,13 +8,14 @@\n from ..utils import (\n     determine_ext,\n     ExtractorError,\n+    extract_attributes,\n     float_or_none,\n     int_or_none,\n     merge_dicts,\n     NO_DEFAULT,\n-    orderedSet,\n     parse_codecs,\n     qualities,\n+    str_or_none,\n     try_get,\n     unified_timestamp,\n     update_url_query,\n@@ -57,28 +58,39 @@ def _extract_format(self, video_id, formats, format_urls, meta):\n         format_urls.add(format_url)\n         mime_type = meta.get('mimeType')\n         ext = determine_ext(format_url)\n+\n+        join_nonempty = lambda s, l: s.join(filter(None, l))\n+        meta_map = lambda t: map(lambda x: str_or_none(meta.get(x)), t)\n+\n         if mime_type == 'application/x-mpegURL' or ext == 'm3u8':\n-            formats.extend(self._extract_m3u8_formats(\n+            new_formats = self._extract_m3u8_formats(\n                 format_url, video_id, 'mp4', m3u8_id='hls',\n-                entry_protocol='m3u8_native', fatal=False))\n+                entry_protocol='m3u8_native', fatal=False)\n         elif mime_type == 'application/f4m+xml' or ext == 'f4m':\n-            formats.extend(self._extract_f4m_formats(\n-                update_url_query(format_url, {'hdcore': '3.7.0'}), video_id, f4m_id='hds', fatal=False))\n+            new_formats = self._extract_f4m_formats(\n+                update_url_query(format_url, {'hdcore': '3.7.0'}), video_id, f4m_id='hds', fatal=False)\n         else:\n             f = parse_codecs(meta.get('mimeCodec'))\n+            if not f:\n+                data = meta.get('type', '').split('_')\n+                if try_get(data, lambda x: x[2]) == ext:\n+                    f = dict(zip(('vcodec', 'acodec'), data[1]))\n+\n             format_id = ['http']\n-            for p in (meta.get('type'), meta.get('quality')):\n-                if p and isinstance(p, compat_str):\n-                    format_id.append(p)\n+            format_id.extend(join_nonempty('-', meta_map(('type', 'quality'))))\n             f.update({\n                 'url': format_url,\n                 'format_id': '-'.join(format_id),\n-                'format_note': meta.get('quality'),\n-                'language': meta.get('language'),\n-                'quality': qualities(self._QUALITIES)(meta.get('quality')),\n-                'preference': -10,\n+                'tbr': int_or_none(self._search_regex(r'_(\\d+)k_', format_url, 'tbr', default=None))\n             })\n-            formats.append(f)\n+            new_formats = [f]\n+\n+        formats.extend(merge_dicts(f, {\n+            'format_note': join_nonempty(',', meta_map(('quality', 'class'))),\n+            'language': meta.get('language'),\n+            'language_preference': 10 if meta.get('class') == 'main' else -10 if meta.get('class') == 'ad' else -1,\n+            'quality': qualities(self._QUALITIES)(meta.get('quality')),\n+        }) for f in new_formats)\n \n     def _extract_ptmd(self, ptmd_url, video_id, api_token, referrer):\n         ptmd = self._call_api(\n@@ -107,6 +119,7 @@ def _extract_ptmd(self, ptmd_url, video_id, api_token, referrer):\n                                 'type': f.get('type'),\n                                 'mimeType': f.get('mimeType'),\n                                 'quality': quality.get('quality'),\n+                                'class': track.get('class'),\n                                 'language': track.get('language'),\n                             })\n         self._sort_formats(formats)\n@@ -171,6 +184,20 @@ class ZDFIE(ZDFBaseIE):\n             'duration': 2615,\n             'timestamp': 1465021200,\n             'upload_date': '20160604',\n+            'thumbnail': 'https://www.zdf.de/assets/mauve-im-labor-100~768x432?cb=1464909117806',\n+        },\n+    }, {\n+        'url': 'https://www.zdf.de/funk/druck-11790/funk-alles-ist-verzaubert-102.html',\n+        'md5': '1b93bdec7d02fc0b703c5e7687461628',\n+        'info_dict': {\n+            'ext': 'mp4',\n+            'id': 'video_funk_1770473',\n+            'duration': 1278,\n+            'description': 'Die Neue an der Schule verdreht Ismail den Kopf.',\n+            'title': 'Alles ist verzaubert',\n+            'timestamp': 1635520560,\n+            'upload_date': '20211029',\n+            'thumbnail': 'https://www.zdf.de/assets/teaser-funk-alles-ist-verzaubert-100~1920x1080?cb=1636466431799',\n         },\n     }, {\n         # Same as https://www.phoenix.de/sendungen/dokumentationen/gesten-der-maechtigen-i-a-89468.html?ref=suche\n@@ -204,6 +231,19 @@ class ZDFIE(ZDFBaseIE):\n             'timestamp': 1641355200,\n             'upload_date': '20220105',\n         },\n+        'skip': 'No longer available \"Diese Seite wurde leider nicht gefunden\"'\n+    }, {\n+        'url': 'https://www.zdf.de/serien/soko-stuttgart/das-geld-anderer-leute-100.html',\n+        'info_dict': {\n+            'id': '191205_1800_sendung_sok8',\n+            'ext': 'mp4',\n+            'title': 'Das Geld anderer Leute',\n+            'description': 'md5:cb6f660850dc5eb7d1ab776ea094959d',\n+            'duration': 2581.0,\n+            'timestamp': 1654790700,\n+            'upload_date': '20220609',\n+            'thumbnail': 'https://epg-image.zdf.de/fotobase-webdelivery/images/e2d7e55a-09f0-424e-ac73-6cac4dd65f35?layout=2400x1350',\n+        },\n     }]\n \n     def _extract_entry(self, url, player, content, video_id):\n@@ -265,15 +305,16 @@ def _extract_mobile(self, video_id):\n             'https://zdf-cdn.live.cellular.de/mediathekV2/document/%s' % video_id,\n             video_id)\n \n-        document = video['document']\n-\n-        title = document['titel']\n-        content_id = document['basename']\n-\n         formats = []\n-        format_urls = set()\n-        for f in document['formitaeten']:\n-            self._extract_format(content_id, formats, format_urls, f)\n+        formitaeten = try_get(video, lambda x: x['document']['formitaeten'], list)\n+        document = formitaeten and video['document']\n+        if formitaeten:\n+            title = document['titel']\n+            content_id = document['basename']\n+\n+            format_urls = set()\n+            for f in formitaeten or []:\n+                self._extract_format(content_id, formats, format_urls, f)\n         self._sort_formats(formats)\n \n         thumbnails = []\n@@ -320,9 +361,9 @@ class ZDFChannelIE(ZDFBaseIE):\n         'url': 'https://www.zdf.de/sport/das-aktuelle-sportstudio',\n         'info_dict': {\n             'id': 'das-aktuelle-sportstudio',\n-            'title': 'das aktuelle sportstudio | ZDF',\n+            'title': 'das aktuelle sportstudio',\n         },\n-        'playlist_mincount': 23,\n+        'playlist_mincount': 18,\n     }, {\n         'url': 'https://www.zdf.de/dokumentation/planet-e',\n         'info_dict': {\n@@ -330,6 +371,14 @@ class ZDFChannelIE(ZDFBaseIE):\n             'title': 'planet e.',\n         },\n         'playlist_mincount': 50,\n+    }, {\n+        'url': 'https://www.zdf.de/gesellschaft/aktenzeichen-xy-ungeloest',\n+        'info_dict': {\n+            'id': 'aktenzeichen-xy-ungeloest',\n+            'title': 'Aktenzeichen XY... ungel\u00f6st',\n+            'entries': \"lambda x: not any('xy580-fall1-kindermoerder-gesucht-100' in e['url'] for e in x)\",\n+        },\n+        'playlist_mincount': 2,\n     }, {\n         'url': 'https://www.zdf.de/filme/taunuskrimi/',\n         'only_matching': True,\n@@ -339,60 +388,36 @@ class ZDFChannelIE(ZDFBaseIE):\n     def suitable(cls, url):\n         return False if ZDFIE.suitable(url) else super(ZDFChannelIE, cls).suitable(url)\n \n+    def _og_search_title(self, webpage, fatal=False):\n+        title = super(ZDFChannelIE, self)._og_search_title(webpage, fatal=fatal)\n+        return re.split(r'\\s+[-|]\\s+ZDF(?:mediathek)?$', title or '')[0] or None\n+\n     def _real_extract(self, url):\n         channel_id = self._match_id(url)\n \n         webpage = self._download_webpage(url, channel_id)\n \n-        entries = [\n-            self.url_result(item_url, ie=ZDFIE.ie_key())\n-            for item_url in orderedSet(re.findall(\n-                r'data-plusbar-url=[\"\\'](http.+?\\.html)', webpage))]\n-\n-        return self.playlist_result(\n-            entries, channel_id, self._og_search_title(webpage, fatal=False))\n-\n-        r\"\"\"\n-        player = self._extract_player(webpage, channel_id)\n-\n-        channel_id = self._search_regex(\n-            r'docId\\s*:\\s*([\"\\'])(?P<id>(?!\\1).+?)\\1', webpage,\n-            'channel id', group='id')\n-\n-        channel = self._call_api(\n-            'https://api.zdf.de/content/documents/%s.json' % channel_id,\n-            player, url, channel_id)\n-\n-        items = []\n-        for module in channel['module']:\n-            for teaser in try_get(module, lambda x: x['teaser'], list) or []:\n-                t = try_get(\n-                    teaser, lambda x: x['http://zdf.de/rels/target'], dict)\n-                if not t:\n-                    continue\n-                items.extend(try_get(\n-                    t,\n-                    lambda x: x['resultsWithVideo']['http://zdf.de/rels/search/results'],\n-                    list) or [])\n-            items.extend(try_get(\n-                module,\n-                lambda x: x['filterRef']['resultsWithVideo']['http://zdf.de/rels/search/results'],\n-                list) or [])\n-\n-        entries = []\n-        entry_urls = set()\n-        for item in items:\n-            t = try_get(item, lambda x: x['http://zdf.de/rels/target'], dict)\n-            if not t:\n-                continue\n-            sharing_url = t.get('http://zdf.de/rels/sharing-url')\n-            if not sharing_url or not isinstance(sharing_url, compat_str):\n-                continue\n-            if sharing_url in entry_urls:\n-                continue\n-            entry_urls.add(sharing_url)\n-            entries.append(self.url_result(\n-                sharing_url, ie=ZDFIE.ie_key(), video_id=t.get('id')))\n-\n-        return self.playlist_result(entries, channel_id, channel.get('title'))\n-        \"\"\"\n+        matches = re.finditer(\n+            r'''<div\\b[^>]*?\\sdata-plusbar-id\\s*=\\s*([\"'])(?P<p_id>[\\w-]+)\\1[^>]*?\\sdata-plusbar-url=\\1(?P<url>%s)\\1''' % ZDFIE._VALID_URL,\n+            webpage)\n+\n+        if self._downloader.params.get('noplaylist', False):\n+            entry = next(\n+                (self.url_result(m.group('url'), ie=ZDFIE.ie_key()) for m in matches),\n+                None)\n+            self.to_screen('Downloading just the main video because of --no-playlist')\n+            if entry:\n+                return entry\n+        else:\n+            self.to_screen('Downloading playlist %s - add --no-playlist to download just the main video' % (channel_id, ))\n+\n+        def check_video(m):\n+            v_ref = self._search_regex(\n+                r'''(<a\\b[^>]*?\\shref\\s*=[^>]+?\\sdata-target-id\\s*=\\s*([\"'])%s\\2[^>]*>)''' % (m.group('p_id'), ),\n+                webpage, 'check id', default='')\n+            v_ref = extract_attributes(v_ref)\n+            return v_ref.get('data-target-video-type') != 'novideo'\n+\n+        return self.playlist_from_matches(\n+            (m.group('url') for m in matches if check_video(m)),\n+            channel_id, self._og_search_title(webpage, fatal=False))\n", "test_patch": "diff --git a/test/helper.py b/test/helper.py\nindex e62aab11e77..c6a2f06670d 100644\n--- a/test/helper.py\n+++ b/test/helper.py\n@@ -128,6 +128,12 @@ def expect_value(self, got, expected, field):\n         self.assertTrue(\n             contains_str in got,\n             'field %s (value: %r) should contain %r' % (field, got, contains_str))\n+    elif isinstance(expected, compat_str) and re.match(r'^lambda \\w+:', expected):\n+        fn = eval(expected)\n+        suite = expected.split(':', 1)[1].strip()\n+        self.assertTrue(\n+            fn(got),\n+            'Expected field %s to meet condition %s, but value %r failed ' % (field, suite, got))\n     elif isinstance(expected, type):\n         self.assertTrue(\n             isinstance(got, expected),\n@@ -137,7 +143,7 @@ def expect_value(self, got, expected, field):\n     elif isinstance(expected, list) and isinstance(got, list):\n         self.assertEqual(\n             len(expected), len(got),\n-            'Expect a list of length %d, but got a list of length %d for field %s' % (\n+            'Expected a list of length %d, but got a list of length %d for field %s' % (\n                 len(expected), len(got), field))\n         for index, (item_got, item_expected) in enumerate(zip(got, expected)):\n             type_got = type(item_got)\n", "problem_statement": "[zdf] ytdl fails with KeyError: 'basename'\n<!--\r\n\r\n######################################################################\r\n  WARNING!\r\n  IGNORING THE FOLLOWING TEMPLATE WILL RESULT IN ISSUE CLOSED AS INCOMPLETE\r\n######################################################################\r\n\r\n-->\r\n\r\n\r\n## Checklist\r\n\r\n<!--\r\nCarefully read and work through this check list in order to prevent the most common mistakes and misuse of youtube-dl:\r\n- First of, make sure you are using the latest version of youtube-dl. Run `youtube-dl --version` and ensure your version is 2021.12.17. If it's not, see https://yt-dl.org/update on how to update. Issues with outdated version will be REJECTED.\r\n- Make sure that all provided video/audio/playlist URLs (if any) are alive and playable in a browser.\r\n- Make sure that all URLs and arguments with special characters are properly quoted or escaped as explained in http://yt-dl.org/escape.\r\n- Search the bugtracker for similar issues: http://yt-dl.org/search-issues. DO NOT post duplicates.\r\n- Read bugs section in FAQ: http://yt-dl.org/reporting\r\n- Finally, put x into all relevant boxes (like this [x])\r\n-->\r\n\r\n- [X] I'm reporting a broken site support issue\r\n- [X] I've verified that I'm running youtube-dl version **2021.12.17**\r\n- [X] I've checked that all provided URLs are alive and playable in a browser\r\n- [X] I've checked that all URLs and arguments with special characters are properly quoted or escaped\r\n- [X] I've searched the bugtracker for similar bug reports including closed ones\r\n- [X] I've read bugs section in FAQ\r\n\r\n\r\n## Verbose log\r\n\r\n<!--\r\nProvide the complete verbose output of youtube-dl that clearly demonstrates the problem.\r\nAdd the `-v` flag to your command line you run youtube-dl with (`youtube-dl -v <your command line>`), copy the WHOLE output and insert it below. It should look similar to this:\r\n [debug] System config: []\r\n [debug] User config: []\r\n [debug] Command-line args: [u'-v', u'http://www.youtube.com/watch?v=BaW_jenozKcj']\r\n [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251\r\n [debug] youtube-dl version 2021.12.17\r\n [debug] Python version 2.7.11 - Windows-2003Server-5.2.3790-SP2\r\n [debug] exe versions: ffmpeg N-75573-g1d0487f, ffprobe N-75573-g1d0487f, rtmpdump 2.4\r\n [debug] Proxy map: {}\r\n <more lines>\r\n-->\r\n\r\n```\r\n\u279c  ~ youtube-dl --version\r\n2021.12.17\r\n\u279c  ~ youtube-dl --verbose https://www.zdf.de/gesellschaft/aktenzeichen-xy-ungeloest\r\n[debug] System config: []\r\n[debug] User config: []\r\n[debug] Custom config: []\r\n[debug] Command-line args: ['--verbose', 'https://www.zdf.de/gesellschaft/aktenzeichen-xy-ungeloest']\r\n[debug] Encodings: locale UTF-8, fs utf-8, out utf-8, pref UTF-8\r\n[debug] youtube-dl version 2021.12.17\r\n[debug] Python version 3.10.5 (CPython) - macOS-12.5-x86_64-i386-64bit\r\n[debug] exe versions: ffmpeg 5.1, ffprobe 5.1, rtmpdump 2.4\r\n[debug] Proxy map: {}\r\n[debug] Using fake IP 53.60.232.50 (DE) as X-Forwarded-For.\r\n[ZDFChannel] aktenzeichen-xy-ungeloest: Downloading webpage\r\n[download] Downloading playlist: Aktenzeichen XY... ungel\u00f6st\r\n[ZDFChannel] playlist Aktenzeichen XY... ungel\u00f6st: Collected 44 video ids (downloading 44 of them)\r\n[download] Downloading video 1 of 44\r\n[debug] Using fake IP 53.88.203.47 (DE) as X-Forwarded-For.\r\n[ZDF] aktenzeichen-xy-ungeloest-vom-3-august-2022-100: Downloading webpage\r\n[ZDF] aktenzeichen-xy-ungeloest-vom-3-august-2022-100: Downloading JSON content\r\n[ZDF] aktenzeichen-xy-ungeloest-vom-3-august-2022-100: Downloading JSON metadata\r\n[ZDF] 220803_sendung_axy: Downloading m3u8 information\r\n[ZDF] 220803_sendung_axy: Downloading m3u8 information\r\n[debug] Default format spec: bestvideo+bestaudio/best\r\n[debug] Invoking downloader on 'https://zdfvod-rwrtr.akamaized.net/i/,/mp4/none/zdf/22/08/220803_sendung_axy/6/220803_sendung_axy,_508k_p9,_808k_p11,_1628k_p13,_3328k_p15,v15.mp4.csmil/index-f4-v1-a1.m3u8'\r\n[download] Aktenzeichen XY... ungel\u00f6st vom 3. August 2022-220803_sendung_axy.mp4 has already been downloaded\r\n[download] 100% of 1.66GiB\r\n[debug] ffmpeg command line: ffprobe -show_streams 'file:Aktenzeichen XY... ungel\u00f6st vom 3. August 2022-220803_sendung_axy.mp4'\r\n[ffmpeg] Fixing malformed AAC bitstream in \"Aktenzeichen XY... ungel\u00f6st vom 3. August 2022-220803_sendung_axy.mp4\"\r\n[debug] ffmpeg command line: ffmpeg -y -loglevel repeat+info -i 'file:Aktenzeichen XY... ungel\u00f6st vom 3. August 2022-220803_sendung_axy.mp4' -c copy -f mp4 -bsf:a aac_adtstoasc 'file:Aktenzeichen XY... ungel\u00f6st vom 3. August 2022-220803_sendung_axy.temp.mp4'\r\n[download] Downloading video 2 of 44\r\n[ZDF] xy580-fall1-kindermoerder-gesucht-100: Downloading webpage\r\n[ZDF] xy580-fall1-kindermoerder-gesucht-100: Downloading JSON metadata\r\nERROR: An extractor error has occurred. (caused by KeyError('basename')); please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see  https://yt-dl.org/update  on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/Cellar/youtube-dl/2021.12.17/libexec/lib/python3.10/site-packages/youtube_dl/extractor/common.py\", line 534, in extract\r\n    ie_result = self._real_extract(url)\r\n  File \"/usr/local/Cellar/youtube-dl/2021.12.17/libexec/lib/python3.10/site-packages/youtube_dl/extractor/zdf.py\", line 294, in _real_extract\r\n    return self._extract_mobile(video_id)\r\n  File \"/usr/local/Cellar/youtube-dl/2021.12.17/libexec/lib/python3.10/site-packages/youtube_dl/extractor/zdf.py\", line 251, in _extract_mobile\r\n    content_id = document['basename']\r\nKeyError: 'basename'\r\nTraceback (most recent call last):\r\n  File \"/usr/local/Cellar/youtube-dl/2021.12.17/libexec/lib/python3.10/site-packages/youtube_dl/extractor/common.py\", line 534, in extract\r\n    ie_result = self._real_extract(url)\r\n  File \"/usr/local/Cellar/youtube-dl/2021.12.17/libexec/lib/python3.10/site-packages/youtube_dl/extractor/zdf.py\", line 294, in _real_extract\r\n    return self._extract_mobile(video_id)\r\n  File \"/usr/local/Cellar/youtube-dl/2021.12.17/libexec/lib/python3.10/site-packages/youtube_dl/extractor/zdf.py\", line 251, in _extract_mobile\r\n    content_id = document['basename']\r\nKeyError: 'basename'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/Cellar/youtube-dl/2021.12.17/libexec/lib/python3.10/site-packages/youtube_dl/YoutubeDL.py\", line 815, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"/usr/local/Cellar/youtube-dl/2021.12.17/libexec/lib/python3.10/site-packages/youtube_dl/YoutubeDL.py\", line 836, in __extract_info\r\n    ie_result = ie.extract(url)\r\n  File \"/usr/local/Cellar/youtube-dl/2021.12.17/libexec/lib/python3.10/site-packages/youtube_dl/extractor/common.py\", line 547, in extract\r\n    raise ExtractorError('An extractor error has occurred.', cause=e)\r\nyoutube_dl.utils.ExtractorError: An extractor error has occurred. (caused by KeyError('basename')); please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see  https://yt-dl.org/update  on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.\r\n```\r\n\r\n\r\n## Description\r\n\r\n<!--\r\nProvide an explanation of your issue in an arbitrary form. Please make sure the description is worded well enough to be understood, see https://github.com/ytdl-org/youtube-dl#is-the-description-of-the-issue-itself-sufficient. Provide any additional information, suggested solution and as much context and examples as possible.\r\nIf work on your issue requires account credentials please provide them or explain how one can obtain them.\r\n-->\r\n\r\nI was trying to download the current episode of Aktenzeichen XY from [https://www.zdf.de/gesellschaft/aktenzeichen-xy-ungeloest](https://www.zdf.de/gesellschaft/aktenzeichen-xy-ungeloest).\r\n\n", "hints_text": "What did you expect this page to be, a single video or a playlist?\r\n\r\nyt-dl thinks it's a playlist (`ZDFChannelIE`) with 44 items, corresponding to the `data-plusbar-url` values in the page source. The first item is downloaded (3 August). Then, as item 2 has no video, it fails to get player data for it and crashes when falling back to the corresponding mobile page. If you use `--ignore-errors` this happens for items 3-7 too, until 8-10 which do have videos, and so on.\r\n\r\nUse `--playlist-items 1` to avoid this and just get the first video (though I'm not sure that item 1 is guaranteed to be the newest).\r\n\r\nSingle video pages (`ZDFIE`) have a `.html` extension in the URL.\r\n\r\nThere doesn't seem to be any metadata in the `data-plusbar...` blocks that would indicate when what appears to be a linked single video page actually has a video. We can fix the actual crash in the log but then your original command will just break on playlist item 2 with `ERROR: No video formats found` instead.\r\n\r\nUsing the _jq_ program, you can list the URLs of the playlist items and perhaps pick the one you want:\r\n```\r\nyoutube-dl --flat-playlist -J 'https://www.zdf.de/gesellschaft/aktenzeichen-xy-ungeloest' | jq '.entries | .[] | .url'\r\n```\r\n\nActually, the page does show which items are videos. The `<div ...>` element that contains the `data-plusbar-url` attribute is enclosed in a link that contains a `data-target-video-type` attribute and a `data-target-id` that matches the `data-plusbar-id`. The video type `'novideo'` shows the items to skip.", "created_at": "2022-08-09T19:38:59Z"}
{"repo": "ytdl-org/youtube-dl", "pull_number": 31043, "instance_id": "ytdl-org__youtube-dl-31043", "issue_numbers": ["31034"], "base_commit": "384f632e8a9b61e864a26678d85b2b39933b9bae", "patch": "diff --git a/devscripts/make_lazy_extractors.py b/devscripts/make_lazy_extractors.py\nindex 878ae72b132..edc19183da5 100644\n--- a/devscripts/make_lazy_extractors.py\n+++ b/devscripts/make_lazy_extractors.py\n@@ -13,6 +13,11 @@\n lazy_extractors_filename = sys.argv[1]\n if os.path.exists(lazy_extractors_filename):\n     os.remove(lazy_extractors_filename)\n+# Py2: may be confused by leftover lazy_extractors.pyc\n+try:\n+    os.remove(lazy_extractors_filename + 'c')\n+except OSError:\n+    pass\n \n from youtube_dl.extractor import _ALL_CLASSES\n from youtube_dl.extractor.common import InfoExtractor, SearchInfoExtractor\n@@ -22,7 +27,10 @@\n \n module_contents = [\n     module_template + '\\n' + getsource(InfoExtractor.suitable) + '\\n',\n-    'class LazyLoadSearchExtractor(LazyLoadExtractor):\\n    pass\\n']\n+    'class LazyLoadSearchExtractor(LazyLoadExtractor):\\n    pass\\n',\n+    # needed for suitable() methods of Youtube extractor (see #28780)\n+    'from youtube_dl.utils import parse_qs\\n',\n+]\n \n ie_template = '''\n class {name}({bases}):\ndiff --git a/youtube_dl/compat.py b/youtube_dl/compat.py\nindex 28942a8c1ce..39551f81066 100644\n--- a/youtube_dl/compat.py\n+++ b/youtube_dl/compat.py\n@@ -21,6 +21,10 @@\n import sys\n import xml.etree.ElementTree\n \n+# naming convention\n+# 'compat_' + Python3_name.replace('.', '_')\n+# other aliases exist for convenience and/or legacy\n+\n # deal with critical unicode/str things first\n try:\n     # Python 2\n@@ -28,6 +32,7 @@\n         unicode, basestring, unichr\n     )\n     from .casefold import casefold as compat_casefold\n+\n except NameError:\n     compat_str, compat_basestring, compat_chr = (\n         str, str, chr\n@@ -53,16 +58,15 @@\n     import urllib.parse as compat_urllib_parse\n except ImportError:  # Python 2\n     import urllib as compat_urllib_parse\n+    import urlparse as _urlparse\n+    for a in dir(_urlparse):\n+        if not hasattr(compat_urllib_parse, a):\n+            setattr(compat_urllib_parse, a, getattr(_urlparse, a))\n+    del _urlparse\n \n-try:\n-    from urllib.parse import urlparse as compat_urllib_parse_urlparse\n-except ImportError:  # Python 2\n-    from urlparse import urlparse as compat_urllib_parse_urlparse\n-\n-try:\n-    import urllib.parse as compat_urlparse\n-except ImportError:  # Python 2\n-    import urlparse as compat_urlparse\n+# unfavoured aliases\n+compat_urlparse = compat_urllib_parse\n+compat_urllib_parse_urlparse = compat_urllib_parse.urlparse\n \n try:\n     import urllib.response as compat_urllib_response\n@@ -73,6 +77,7 @@\n     import http.cookiejar as compat_cookiejar\n except ImportError:  # Python 2\n     import cookielib as compat_cookiejar\n+compat_http_cookiejar = compat_cookiejar\n \n if sys.version_info[0] == 2:\n     class compat_cookiejar_Cookie(compat_cookiejar.Cookie):\n@@ -84,11 +89,13 @@ def __init__(self, version, name, value, *args, **kwargs):\n             compat_cookiejar.Cookie.__init__(self, version, name, value, *args, **kwargs)\n else:\n     compat_cookiejar_Cookie = compat_cookiejar.Cookie\n+compat_http_cookiejar_Cookie = compat_cookiejar_Cookie\n \n try:\n     import http.cookies as compat_cookies\n except ImportError:  # Python 2\n     import Cookie as compat_cookies\n+compat_http_cookies = compat_cookies\n \n if sys.version_info[0] == 2:\n     class compat_cookies_SimpleCookie(compat_cookies.SimpleCookie):\n@@ -98,6 +105,7 @@ def load(self, rawdata):\n             return super(compat_cookies_SimpleCookie, self).load(rawdata)\n else:\n     compat_cookies_SimpleCookie = compat_cookies.SimpleCookie\n+compat_http_cookies_SimpleCookie = compat_cookies_SimpleCookie\n \n try:\n     import html.entities as compat_html_entities\n@@ -2351,16 +2359,19 @@ def load(self, rawdata):\n     from urllib.error import HTTPError as compat_HTTPError\n except ImportError:  # Python 2\n     from urllib2 import HTTPError as compat_HTTPError\n+compat_urllib_HTTPError = compat_HTTPError\n \n try:\n     from urllib.request import urlretrieve as compat_urlretrieve\n except ImportError:  # Python 2\n     from urllib import urlretrieve as compat_urlretrieve\n+compat_urllib_request_urlretrieve = compat_urlretrieve\n \n try:\n     from html.parser import HTMLParser as compat_HTMLParser\n except ImportError:  # Python 2\n     from HTMLParser import HTMLParser as compat_HTMLParser\n+compat_html_parser_HTMLParser = compat_HTMLParser\n \n try:  # Python 2\n     from HTMLParser import HTMLParseError as compat_HTMLParseError\n@@ -2374,6 +2385,7 @@ def load(self, rawdata):\n         # and uniform cross-version exception handling\n         class compat_HTMLParseError(Exception):\n             pass\n+compat_html_parser_HTMLParseError = compat_HTMLParseError\n \n try:\n     from subprocess import DEVNULL\n@@ -2390,6 +2402,8 @@ class compat_HTMLParseError(Exception):\n     from urllib.parse import unquote_to_bytes as compat_urllib_parse_unquote_to_bytes\n     from urllib.parse import unquote as compat_urllib_parse_unquote\n     from urllib.parse import unquote_plus as compat_urllib_parse_unquote_plus\n+    from urllib.parse import urlencode as compat_urllib_parse_urlencode\n+    from urllib.parse import parse_qs as compat_parse_qs\n except ImportError:  # Python 2\n     _asciire = (compat_urllib_parse._asciire if hasattr(compat_urllib_parse, '_asciire')\n                 else re.compile(r'([\\x00-\\x7f]+)'))\n@@ -2456,9 +2470,6 @@ def compat_urllib_parse_unquote_plus(string, encoding='utf-8', errors='replace')\n         string = string.replace('+', ' ')\n         return compat_urllib_parse_unquote(string, encoding, errors)\n \n-try:\n-    from urllib.parse import urlencode as compat_urllib_parse_urlencode\n-except ImportError:  # Python 2\n     # Python 2 will choke in urlencode on mixture of byte and unicode strings.\n     # Possible solutions are to either port it from python 3 with all\n     # the friends or manually ensure input query contains only byte strings.\n@@ -2480,7 +2491,62 @@ def encode_dict(d):\n         def encode_list(l):\n             return [encode_elem(e) for e in l]\n \n-        return compat_urllib_parse.urlencode(encode_elem(query), doseq=doseq)\n+        return compat_urllib_parse._urlencode(encode_elem(query), doseq=doseq)\n+\n+    # HACK: The following is the correct parse_qs implementation from cpython 3's stdlib.\n+    # Python 2's version is apparently totally broken\n+    def _parse_qsl(qs, keep_blank_values=False, strict_parsing=False,\n+                   encoding='utf-8', errors='replace'):\n+        qs, _coerce_result = qs, compat_str\n+        pairs = [s2 for s1 in qs.split('&') for s2 in s1.split(';')]\n+        r = []\n+        for name_value in pairs:\n+            if not name_value and not strict_parsing:\n+                continue\n+            nv = name_value.split('=', 1)\n+            if len(nv) != 2:\n+                if strict_parsing:\n+                    raise ValueError('bad query field: %r' % (name_value,))\n+                # Handle case of a control-name with no equal sign\n+                if keep_blank_values:\n+                    nv.append('')\n+                else:\n+                    continue\n+            if len(nv[1]) or keep_blank_values:\n+                name = nv[0].replace('+', ' ')\n+                name = compat_urllib_parse_unquote(\n+                    name, encoding=encoding, errors=errors)\n+                name = _coerce_result(name)\n+                value = nv[1].replace('+', ' ')\n+                value = compat_urllib_parse_unquote(\n+                    value, encoding=encoding, errors=errors)\n+                value = _coerce_result(value)\n+                r.append((name, value))\n+        return r\n+\n+    def compat_parse_qs(qs, keep_blank_values=False, strict_parsing=False,\n+                        encoding='utf-8', errors='replace'):\n+        parsed_result = {}\n+        pairs = _parse_qsl(qs, keep_blank_values, strict_parsing,\n+                           encoding=encoding, errors=errors)\n+        for name, value in pairs:\n+            if name in parsed_result:\n+                parsed_result[name].append(value)\n+            else:\n+                parsed_result[name] = [value]\n+        return parsed_result\n+\n+    setattr(compat_urllib_parse, '_urlencode',\n+            getattr(compat_urllib_parse, 'urlencode'))\n+    for name, fix in (\n+            ('unquote_to_bytes', compat_urllib_parse_unquote_to_bytes),\n+            ('parse_unquote', compat_urllib_parse_unquote),\n+            ('unquote_plus', compat_urllib_parse_unquote_plus),\n+            ('urlencode', compat_urllib_parse_urlencode),\n+            ('parse_qs', compat_parse_qs)):\n+        setattr(compat_urllib_parse, name, fix)\n+\n+compat_urllib_parse_parse_qs = compat_parse_qs\n \n try:\n     from urllib.request import DataHandler as compat_urllib_request_DataHandler\n@@ -2520,6 +2586,7 @@ def data_open(self, req):\n     from xml.etree.ElementTree import ParseError as compat_xml_parse_error\n except ImportError:  # Python 2.6\n     from xml.parsers.expat import ExpatError as compat_xml_parse_error\n+compat_xml_etree_ElementTree_ParseError = compat_xml_parse_error\n \n etree = xml.etree.ElementTree\n \n@@ -2533,10 +2600,11 @@ def doctype(self, name, pubid, system):\n     # xml.etree.ElementTree.Element is a method in Python <=2.6 and\n     # the following will crash with:\n     #  TypeError: isinstance() arg 2 must be a class, type, or tuple of classes and types\n-    isinstance(None, xml.etree.ElementTree.Element)\n+    isinstance(None, etree.Element)\n     from xml.etree.ElementTree import Element as compat_etree_Element\n except TypeError:  # Python <=2.6\n     from xml.etree.ElementTree import _ElementInterface as compat_etree_Element\n+compat_xml_etree_ElementTree_Element = compat_etree_Element\n \n if sys.version_info[0] >= 3:\n     def compat_etree_fromstring(text):\n@@ -2592,6 +2660,7 @@ def compat_etree_register_namespace(prefix, uri):\n             if k == uri or v == prefix:\n                 del etree._namespace_map[k]\n         etree._namespace_map[uri] = prefix\n+compat_xml_etree_register_namespace = compat_etree_register_namespace\n \n if sys.version_info < (2, 7):\n     # Here comes the crazy part: In 2.6, if the xpath is a unicode,\n@@ -2603,53 +2672,6 @@ def compat_xpath(xpath):\n else:\n     compat_xpath = lambda xpath: xpath\n \n-try:\n-    from urllib.parse import parse_qs as compat_parse_qs\n-except ImportError:  # Python 2\n-    # HACK: The following is the correct parse_qs implementation from cpython 3's stdlib.\n-    # Python 2's version is apparently totally broken\n-\n-    def _parse_qsl(qs, keep_blank_values=False, strict_parsing=False,\n-                   encoding='utf-8', errors='replace'):\n-        qs, _coerce_result = qs, compat_str\n-        pairs = [s2 for s1 in qs.split('&') for s2 in s1.split(';')]\n-        r = []\n-        for name_value in pairs:\n-            if not name_value and not strict_parsing:\n-                continue\n-            nv = name_value.split('=', 1)\n-            if len(nv) != 2:\n-                if strict_parsing:\n-                    raise ValueError('bad query field: %r' % (name_value,))\n-                # Handle case of a control-name with no equal sign\n-                if keep_blank_values:\n-                    nv.append('')\n-                else:\n-                    continue\n-            if len(nv[1]) or keep_blank_values:\n-                name = nv[0].replace('+', ' ')\n-                name = compat_urllib_parse_unquote(\n-                    name, encoding=encoding, errors=errors)\n-                name = _coerce_result(name)\n-                value = nv[1].replace('+', ' ')\n-                value = compat_urllib_parse_unquote(\n-                    value, encoding=encoding, errors=errors)\n-                value = _coerce_result(value)\n-                r.append((name, value))\n-        return r\n-\n-    def compat_parse_qs(qs, keep_blank_values=False, strict_parsing=False,\n-                        encoding='utf-8', errors='replace'):\n-        parsed_result = {}\n-        pairs = _parse_qsl(qs, keep_blank_values, strict_parsing,\n-                           encoding=encoding, errors=errors)\n-        for name, value in pairs:\n-            if name in parsed_result:\n-                parsed_result[name].append(value)\n-            else:\n-                parsed_result[name] = [value]\n-        return parsed_result\n-\n \n compat_os_name = os._name if os.name == 'java' else os.name\n \n@@ -2774,6 +2796,8 @@ def compat_expanduser(path):\n     else:\n         compat_expanduser = os.path.expanduser\n \n+compat_os_path_expanduser = compat_expanduser\n+\n \n if compat_os_name == 'nt' and sys.version_info < (3, 8):\n     # os.path.realpath on Windows does not follow symbolic links\n@@ -2785,6 +2809,8 @@ def compat_realpath(path):\n else:\n     compat_realpath = os.path.realpath\n \n+compat_os_path_realpath = compat_realpath\n+\n \n if sys.version_info < (3, 0):\n     def compat_print(s):\n@@ -2805,11 +2831,15 @@ def compat_getpass(prompt, *args, **kwargs):\n else:\n     compat_getpass = getpass.getpass\n \n+compat_getpass_getpass = compat_getpass\n+\n+\n try:\n     compat_input = raw_input\n except NameError:  # Python 3\n     compat_input = input\n \n+\n # Python < 2.6.5 require kwargs to be bytes\n try:\n     def _testfunc(x):\n@@ -2915,15 +2945,16 @@ def compat_get_terminal_size(fallback=(80, 24)):\n                 lines = _lines\n         return _terminal_size(columns, lines)\n \n+\n try:\n     itertools.count(start=0, step=1)\n     compat_itertools_count = itertools.count\n except TypeError:  # Python 2.6\n     def compat_itertools_count(start=0, step=1):\n-        n = start\n         while True:\n-            yield n\n-            n += step\n+            yield start\n+            start += step\n+\n \n if sys.version_info >= (3, 0):\n     from tokenize import tokenize as compat_tokenize_tokenize\n@@ -3075,6 +3106,8 @@ def compat_b64decode(s, *args, **kwargs):\n else:\n     compat_b64decode = base64.b64decode\n \n+compat_base64_b64decode = compat_b64decode\n+\n \n if platform.python_implementation() == 'PyPy' and sys.pypy_version_info < (5, 4, 0):\n     # PyPy2 prior to version 5.4.0 expects byte strings as Windows function\n@@ -3094,30 +3127,53 @@ def compat_ctypes_WINFUNCTYPE(*args, **kwargs):\n         return ctypes.WINFUNCTYPE(*args, **kwargs)\n \n \n-__all__ = [\n+legacy = [\n     'compat_HTMLParseError',\n     'compat_HTMLParser',\n     'compat_HTTPError',\n-    'compat_Struct',\n     'compat_b64decode',\n-    'compat_basestring',\n-    'compat_casefold',\n-    'compat_chr',\n-    'compat_collections_abc',\n-    'compat_collections_chain_map',\n     'compat_cookiejar',\n     'compat_cookiejar_Cookie',\n     'compat_cookies',\n     'compat_cookies_SimpleCookie',\n-    'compat_ctypes_WINFUNCTYPE',\n     'compat_etree_Element',\n-    'compat_etree_fromstring',\n     'compat_etree_register_namespace',\n     'compat_expanduser',\n+    'compat_getpass',\n+    'compat_parse_qs',\n+    'compat_realpath',\n+    'compat_urllib_parse_parse_qs',\n+    'compat_urllib_parse_unquote',\n+    'compat_urllib_parse_unquote_plus',\n+    'compat_urllib_parse_unquote_to_bytes',\n+    'compat_urllib_parse_urlencode',\n+    'compat_urllib_parse_urlparse',\n+    'compat_urlparse',\n+    'compat_urlretrieve',\n+    'compat_xml_parse_error',\n+]\n+\n+\n+__all__ = [\n+    'compat_html_parser_HTMLParseError',\n+    'compat_html_parser_HTMLParser',\n+    'compat_Struct',\n+    'compat_base64_b64decode',\n+    'compat_basestring',\n+    'compat_casefold',\n+    'compat_chr',\n+    'compat_collections_abc',\n+    'compat_collections_chain_map',\n+    'compat_http_cookiejar',\n+    'compat_http_cookiejar_Cookie',\n+    'compat_http_cookies',\n+    'compat_http_cookies_SimpleCookie',\n+    'compat_ctypes_WINFUNCTYPE',\n+    'compat_etree_fromstring',\n     'compat_filter',\n     'compat_get_terminal_size',\n     'compat_getenv',\n-    'compat_getpass',\n+    'compat_getpass_getpass',\n     'compat_html_entities',\n     'compat_html_entities_html5',\n     'compat_http_client',\n@@ -3131,11 +3187,11 @@ def compat_ctypes_WINFUNCTYPE(*args, **kwargs):\n     'compat_numeric_types',\n     'compat_ord',\n     'compat_os_name',\n-    'compat_parse_qs',\n+    'compat_os_path_expanduser',\n+    'compat_os_path_realpath',\n     'compat_print',\n     'compat_re_Match',\n     'compat_re_Pattern',\n-    'compat_realpath',\n     'compat_setenv',\n     'compat_shlex_quote',\n     'compat_shlex_split',\n@@ -3147,17 +3203,14 @@ def compat_ctypes_WINFUNCTYPE(*args, **kwargs):\n     'compat_tokenize_tokenize',\n     'compat_urllib_error',\n     'compat_urllib_parse',\n-    'compat_urllib_parse_unquote',\n-    'compat_urllib_parse_unquote_plus',\n-    'compat_urllib_parse_unquote_to_bytes',\n-    'compat_urllib_parse_urlencode',\n-    'compat_urllib_parse_urlparse',\n     'compat_urllib_request',\n     'compat_urllib_request_DataHandler',\n     'compat_urllib_response',\n-    'compat_urlparse',\n-    'compat_urlretrieve',\n-    'compat_xml_parse_error',\n+    'compat_urllib_request_urlretrieve',\n+    'compat_urllib_HTTPError',\n+    'compat_xml_etree_ElementTree_Element',\n+    'compat_xml_etree_ElementTree_ParseError',\n+    'compat_xml_etree_register_namespace',\n     'compat_xpath',\n     'compat_zip',\n     'workaround_optparse_bug9161',\ndiff --git a/youtube_dl/extractor/youtube.py b/youtube_dl/extractor/youtube.py\nindex 3d12e2e4a68..1b20e1d08d3 100644\n--- a/youtube_dl/extractor/youtube.py\n+++ b/youtube_dl/extractor/youtube.py\n@@ -14,12 +14,11 @@\n     compat_chr,\n     compat_HTTPError,\n     compat_map as map,\n-    compat_parse_qs,\n     compat_str,\n+    compat_urllib_parse,\n+    compat_urllib_parse_parse_qs as compat_parse_qs,\n     compat_urllib_parse_unquote_plus,\n-    compat_urllib_parse_urlencode,\n     compat_urllib_parse_urlparse,\n-    compat_urlparse,\n )\n from ..jsinterp import JSInterpreter\n from ..utils import (\n@@ -28,20 +27,25 @@\n     dict_get,\n     error_to_compat_str,\n     float_or_none,\n+    extract_attributes,\n+    get_element_by_attribute,\n     int_or_none,\n     js_to_json,\n     mimetype2ext,\n     parse_codecs,\n     parse_duration,\n+    parse_qs,\n     qualities,\n     remove_start,\n     smuggle_url,\n     str_or_none,\n     str_to_int,\n+    traverse_obj,\n     try_get,\n     unescapeHTML,\n     unified_strdate,\n     unsmuggle_url,\n+    update_url,\n     update_url_query,\n     url_or_none,\n     urlencode_postdata,\n@@ -49,10 +53,6 @@\n )\n \n \n-def parse_qs(url):\n-    return compat_urlparse.parse_qs(compat_urlparse.urlparse(url).query)\n-\n-\n class YoutubeBaseInfoExtractor(InfoExtractor):\n     \"\"\"Provide base functions for Youtube extractors\"\"\"\n     _LOGIN_URL = 'https://accounts.google.com/ServiceLogin'\n@@ -286,15 +286,18 @@ def _real_initialize(self):\n     _YT_INITIAL_PLAYER_RESPONSE_RE = r'ytInitialPlayerResponse\\s*=\\s*({.+?})\\s*;'\n     _YT_INITIAL_BOUNDARY_RE = r'(?:var\\s+meta|</script|\\n)'\n \n-    def _call_api(self, ep, query, video_id, fatal=True):\n+    def _call_api(self, ep, query, video_id, fatal=True, headers=None):\n         data = self._DEFAULT_API_DATA.copy()\n         data.update(query)\n+        real_headers = {'content-type': 'application/json'}\n+        if headers:\n+            real_headers.update(headers)\n \n         return self._download_json(\n             'https://www.youtube.com/youtubei/v1/%s' % ep, video_id=video_id,\n             note='Downloading API JSON', errnote='Unable to download API page',\n             data=json.dumps(data).encode('utf8'), fatal=fatal,\n-            headers={'content-type': 'application/json'},\n+            headers=real_headers,\n             query={'key': 'AIzaSyAO_FJ2SlqU8Q4STEHLGCilw_Y9_11qcW8'})\n \n     def _extract_yt_initial_data(self, video_id, webpage):\n@@ -315,7 +318,8 @@ def _extract_video(self, renderer):\n         title = try_get(\n             renderer,\n             (lambda x: x['title']['runs'][0]['text'],\n-             lambda x: x['title']['simpleText']), compat_str)\n+             lambda x: x['title']['simpleText'],\n+             lambda x: x['headline']['simpleText']), compat_str)\n         description = try_get(\n             renderer, lambda x: x['descriptionSnippet']['runs'][0]['text'],\n             compat_str)\n@@ -514,6 +518,7 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'uploader': 'Philipp Hagemeister',\n                 'uploader_id': 'phihag',\n                 'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/user/phihag',\n+                'channel': 'Philipp Hagemeister',\n                 'channel_id': 'UCLqxVugv74EIW3VWh2NOa3Q',\n                 'channel_url': r're:https?://(?:www\\.)?youtube\\.com/channel/UCLqxVugv74EIW3VWh2NOa3Q',\n                 'upload_date': '20121002',\n@@ -523,10 +528,10 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'duration': 10,\n                 'view_count': int,\n                 'like_count': int,\n-                'dislike_count': int,\n+                'thumbnail': 'https://i.ytimg.com/vi/BaW_jenozKc/maxresdefault.jpg',\n                 'start_time': 1,\n                 'end_time': 9,\n-            }\n+            },\n         },\n         {\n             'url': '//www.YouTube.com/watch?v=yZIXLfi8CZQ',\n@@ -561,7 +566,6 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'duration': 10,\n                 'view_count': int,\n                 'like_count': int,\n-                'dislike_count': int,\n             },\n             'params': {\n                 'skip_download': True,\n@@ -620,8 +624,9 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'description': 'SUBSCRIBE: http://www.youtube.com/saturninefilms \\r\\n\\r\\nEven Obama has taken a stand against freedom on this issue: http://www.huffingtonpost.com/2010/09/09/obama-gma-interview-quran_n_710282.html',\n             }\n         },\n-        # Normal age-gate video (No vevo, embed allowed), available via embed page\n+        # Age-gated videos\n         {\n+            'note': 'Age-gated video (No vevo, embed allowed)',\n             'url': 'https://youtube.com/watch?v=HtVdAasjOgU',\n             'info_dict': {\n                 'id': 'HtVdAasjOgU',\n@@ -633,14 +638,98 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'uploader_id': 'WitcherGame',\n                 'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/user/WitcherGame',\n                 'upload_date': '20140605',\n+                'thumbnail': 'https://i.ytimg.com/vi/HtVdAasjOgU/maxresdefault.jpg',\n+                'age_limit': 18,\n+                'categories': ['Gaming'],\n+                'tags': 'count:17',\n+                'channel': 'The Witcher',\n+                'channel_url': 'https://www.youtube.com/channel/UCzybXLxv08IApdjdN0mJhEg',\n+                'channel_id': 'UCzybXLxv08IApdjdN0mJhEg',\n+                'view_count': int,\n+                'like_count': int,\n+            },\n+        },\n+        {\n+            'note': 'Age-gated video with embed allowed in public site',\n+            'url': 'https://youtube.com/watch?v=HsUATh_Nc2U',\n+            'info_dict': {\n+                'id': 'HsUATh_Nc2U',\n+                'ext': 'mp4',\n+                'title': 'Godzilla 2 (Official Video)',\n+                'description': 'md5:bf77e03fcae5529475e500129b05668a',\n+                'duration': 177,\n+                'uploader': 'FlyingKitty',\n+                'uploader_id': 'FlyingKitty900',\n+                'upload_date': '20200408',\n+                'thumbnail': 'https://i.ytimg.com/vi/HsUATh_Nc2U/maxresdefault.jpg',\n                 'age_limit': 18,\n+                'categories': ['Entertainment'],\n+                'tags': ['Flyingkitty', 'godzilla 2'],\n+                'channel': 'FlyingKitty',\n+                'channel_url': 'https://www.youtube.com/channel/UCYQT13AtrJC0gsM1far_zJg',\n+                'channel_id': 'UCYQT13AtrJC0gsM1far_zJg',\n+                'view_count': int,\n+                'like_count': int,\n             },\n         },\n         {\n-            # Age-gated video only available with authentication (unavailable\n-            # via embed page workaround)\n+            'note': 'Age-gated video embeddable only with clientScreen=EMBED',\n+            'url': 'https://youtube.com/watch?v=Tq92D6wQ1mg',\n+            'info_dict': {\n+                'id': 'Tq92D6wQ1mg',\n+                'ext': 'mp4',\n+                'title': '[MMD] Adios - EVERGLOW [+Motion DL]',\n+                'description': 'md5:17eccca93a786d51bc67646756894066',\n+                'duration': 106,\n+                'uploader': 'Projekt Melody',\n+                'uploader_id': 'UC1yoRdFoFJaCY-AGfD9W0wQ',\n+                'upload_date': '20191227',\n+                'age_limit': 18,\n+                'thumbnail': 'https://i.ytimg.com/vi/Tq92D6wQ1mg/sddefault.jpg',\n+                'tags': ['mmd', 'dance', 'mikumikudance', 'kpop', 'vtuber'],\n+                'categories': ['Entertainment'],\n+                'channel': 'Projekt Melody',\n+                'channel_url': 'https://www.youtube.com/channel/UC1yoRdFoFJaCY-AGfD9W0wQ',\n+                'channel_id': 'UC1yoRdFoFJaCY-AGfD9W0wQ',\n+                'view_count': int,\n+                'like_count': int,\n+            },\n+        },\n+        {\n+            'note': 'Non-Age-gated non-embeddable video',\n+            'url': 'https://youtube.com/watch?v=MeJVWBSsPAY',\n+            'info_dict': {\n+                'id': 'MeJVWBSsPAY',\n+                'ext': 'mp4',\n+                'title': 'OOMPH! - Such Mich Find Mich (Lyrics)',\n+                'description': 'Fan Video. Music & Lyrics by OOMPH!.',\n+                'duration': 210,\n+                'uploader': 'Herr Lurik',\n+                'uploader_id': 'st3in234',\n+                'upload_date': '20130730',\n+                'uploader_url': 'http://www.youtube.com/user/st3in234',\n+                'age_limit': 0,\n+                'thumbnail': 'https://i.ytimg.com/vi/MeJVWBSsPAY/hqdefault.jpg',\n+                'tags': ['oomph', 'such mich find mich', 'lyrics', 'german industrial', 'musica industrial'],\n+                'categories': ['Music'],\n+                'channel': 'Herr Lurik',\n+                'channel_url': 'https://www.youtube.com/channel/UCdR3RSDPqub28LjZx0v9-aA',\n+                'channel_id': 'UCdR3RSDPqub28LjZx0v9-aA',\n+                'artist': 'OOMPH!',\n+                'view_count': int,\n+                'like_count': int,\n+            },\n+        },\n+        {\n+            'note': 'Non-bypassable age-gated video',\n+            'url': 'https://youtube.com/watch?v=Cr381pDsSsA',\n+            'only_matching': True,\n+        },\n+        {\n+            'note': 'Age-gated video only available with authentication (not via embed workaround)',\n             'url': 'XgnwCQzjau8',\n             'only_matching': True,\n+            'skip': '''This video has been removed for violating YouTube's Community Guidelines''',\n         },\n         # video_info is None (https://github.com/ytdl-org/youtube-dl/issues/4421)\n         # YouTube Red ad is not captured for creator\n@@ -669,17 +758,23 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n             'info_dict': {\n                 'id': 'lqQg6PlCWgI',\n                 'ext': 'mp4',\n+                'title': 'Hockey - Women -  GER-AUS - London 2012 Olympic Games',\n+                'description': r're:(?s)(?:.+\\s)?HO09  - Women -  GER-AUS - Hockey - 31 July 2012 - London 2012 Olympic Games\\s*',\n                 'duration': 6085,\n                 'upload_date': '20150827',\n                 'uploader_id': 'olympic',\n                 'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/user/olympic',\n-                'description': 'HO09  - Women -  GER-AUS - Hockey - 31 July 2012 - London 2012 Olympic Games',\n-                'uploader': 'Olympic',\n-                'title': 'Hockey - Women -  GER-AUS - London 2012 Olympic Games',\n+                'uploader': r're:Olympics?',\n+                'age_limit': 0,\n+                'thumbnail': 'https://i.ytimg.com/vi/lqQg6PlCWgI/maxresdefault.jpg',\n+                'categories': ['Sports'],\n+                'tags': ['Hockey', '2012-07-31', '31 July 2012', 'Riverbank Arena', 'Session', 'Olympics', 'Olympic Games', 'London 2012', '2012 Summer Olympics', 'Summer Games'],\n+                'channel': 'Olympics',\n+                'channel_url': 'https://www.youtube.com/channel/UCTl3QQTvqHFjurroKxexy2Q',\n+                'channel_id': 'UCTl3QQTvqHFjurroKxexy2Q',\n+                'view_count': int,\n+                'like_count': int,\n             },\n-            'params': {\n-                'skip_download': 'requires avconv',\n-            }\n         },\n         # Non-square pixels\n         {\n@@ -839,16 +934,16 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n                 'id': 'lsguqyKfVQg',\n                 'ext': 'mp4',\n                 'title': '{dark walk}; Loki/AC/Dishonored; collab w/Elflover21',\n-                'alt_title': 'Dark Walk - Position Music',\n+                'alt_title': 'Dark Walk',\n                 'description': 'md5:8085699c11dc3f597ce0410b0dcbb34a',\n                 'duration': 133,\n                 'upload_date': '20151119',\n                 'uploader_id': 'IronSoulElf',\n                 'uploader_url': r're:https?://(?:www\\.)?youtube\\.com/user/IronSoulElf',\n                 'uploader': 'IronSoulElf',\n-                'creator': 'Todd Haberman,  Daniel Law Heath and Aaron Kaplan',\n-                'track': 'Dark Walk - Position Music',\n-                'artist': 'Todd Haberman,  Daniel Law Heath and Aaron Kaplan',\n+                'creator': r're:Todd Haberman[;,]\\s+Daniel Law Heath and Aaron Kaplan',\n+                'track': 'Dark Walk',\n+                'artist': r're:Todd Haberman[;,]\\s+Daniel Law Heath and Aaron Kaplan',\n                 'album': 'Position Music - Production Music Vol. 143 - Dark Walk',\n             },\n             'params': {\n@@ -1300,11 +1395,7 @@ class YoutubeIE(YoutubeBaseInfoExtractor):\n \n     @classmethod\n     def suitable(cls, url):\n-        # Hack for lazy extractors until more generic solution is implemented\n-        # (see #28780)\n-        from .youtube import parse_qs\n-        qs = parse_qs(url)\n-        if qs.get('list', [None])[0]:\n+        if parse_qs(url).get('list', [None])[0]:\n             return False\n         return super(YoutubeIE, cls).suitable(url)\n \n@@ -1454,7 +1545,7 @@ def _extract_player_url(self, webpage):\n         if player_url.startswith('//'):\n             player_url = 'https:' + player_url\n         elif not re.match(r'https?://', player_url):\n-            player_url = compat_urlparse.urljoin(\n+            player_url = compat_urllib_parse.urljoin(\n                 'https://www.youtube.com', player_url)\n         return player_url\n \n@@ -1536,9 +1627,8 @@ def _n_descramble(self, n_param, player_url, video_id):\n \n     def _unthrottle_format_urls(self, video_id, player_url, formats):\n         for fmt in formats:\n-            parsed_fmt_url = compat_urlparse.urlparse(fmt['url'])\n-            qs = compat_urlparse.parse_qs(parsed_fmt_url.query)\n-            n_param = qs.get('n')\n+            parsed_fmt_url = compat_urllib_parse.urlparse(fmt['url'])\n+            n_param = compat_parse_qs(parsed_fmt_url.query).get('n')\n             if not n_param:\n                 continue\n             n_param = n_param[-1]\n@@ -1546,9 +1636,29 @@ def _unthrottle_format_urls(self, video_id, player_url, formats):\n             if n_response is None:\n                 # give up if descrambling failed\n                 break\n-            qs['n'] = [n_response]\n-            fmt['url'] = compat_urlparse.urlunparse(\n-                parsed_fmt_url._replace(query=compat_urllib_parse_urlencode(qs, True)))\n+            fmt['url'] = update_url(\n+                parsed_fmt_url, query_update={'n': [n_response]})\n+\n+    # from yt-dlp, with tweaks\n+    def _extract_signature_timestamp(self, video_id, player_url, ytcfg=None, fatal=False):\n+        \"\"\"\n+        Extract signatureTimestamp (sts)\n+        Required to tell API what sig/player version is in use.\n+        \"\"\"\n+        sts = int_or_none(ytcfg.get('STS')) if isinstance(ytcfg, dict) else None\n+        if not sts:\n+            # Attempt to extract from player\n+            if player_url is None:\n+                error_msg = 'Cannot extract signature timestamp without player_url.'\n+                if fatal:\n+                    raise ExtractorError(error_msg)\n+                self._downloader.report_warning(error_msg)\n+                return\n+            code = self._get_player_code(video_id, player_url)\n+            sts = int_or_none(self._search_regex(\n+                r'(?:signatureTimestamp|sts)\\s*:\\s*(?P<sts>[0-9]{5})', code or '',\n+                'JS player signature timestamp', group='sts', fatal=fatal))\n+        return sts\n \n     def _mark_watched(self, video_id, player_response):\n         playback_url = url_or_none(try_get(\n@@ -1556,20 +1666,17 @@ def _mark_watched(self, video_id, player_response):\n             lambda x: x['playbackTracking']['videostatsPlaybackUrl']['baseUrl']))\n         if not playback_url:\n             return\n-        parsed_playback_url = compat_urlparse.urlparse(playback_url)\n-        qs = compat_urlparse.parse_qs(parsed_playback_url.query)\n \n         # cpn generation algorithm is reverse engineered from base.js.\n         # In fact it works even with dummy cpn.\n         CPN_ALPHABET = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-_'\n         cpn = ''.join((CPN_ALPHABET[random.randint(0, 256) & 63] for _ in range(0, 16)))\n \n-        qs.update({\n-            'ver': ['2'],\n-            'cpn': [cpn],\n-        })\n-        playback_url = compat_urlparse.urlunparse(\n-            parsed_playback_url._replace(query=compat_urllib_parse_urlencode(qs, True)))\n+        playback_url = update_url(\n+            playback_url, query_update={\n+                'ver': ['2'],\n+                'cpn': [cpn],\n+            })\n \n         self._download_webpage(\n             playback_url, video_id, 'Marking watched',\n@@ -1674,6 +1781,7 @@ def _real_extract(self, url):\n             webpage_url + '&bpctr=9999999999&has_verified=1', video_id, fatal=False)\n \n         player_response = None\n+        player_url = None\n         if webpage:\n             player_response = self._extract_yt_initial_variable(\n                 webpage, self._YT_INITIAL_PLAYER_RESPONSE_RE,\n@@ -1682,27 +1790,61 @@ def _real_extract(self, url):\n             player_response = self._call_api(\n                 'player', {'videoId': video_id}, video_id)\n \n-        playability_status = player_response.get('playabilityStatus') or {}\n-        if playability_status.get('reason') == 'Sign in to confirm your age':\n-            video_info = self._download_webpage(\n-                base_url + 'get_video_info', video_id,\n-                'Refetching age-gated info webpage',\n-                'unable to download video info webpage', query={\n-                    'video_id': video_id,\n-                    'eurl': 'https://youtube.googleapis.com/v/' + video_id,\n-                    'html5': 1,\n-                    # See https://github.com/ytdl-org/youtube-dl/issues/29333#issuecomment-864049544\n-                    'c': 'TVHTML5',\n-                    'cver': '6.20180913',\n-                }, fatal=False)\n-            if video_info:\n-                pr = self._parse_json(\n-                    try_get(\n-                        compat_parse_qs(video_info),\n-                        lambda x: x['player_response'][0], compat_str) or '{}',\n-                    video_id, fatal=False)\n-                if pr and isinstance(pr, dict):\n-                    player_response = pr\n+        def is_agegated(playability):\n+            if not isinstance(playability, dict):\n+                return\n+\n+            if playability.get('desktopLegacyAgeGateReason'):\n+                return True\n+\n+            reasons = filter(None, (playability.get(r) for r in ('status', 'reason')))\n+            AGE_GATE_REASONS = (\n+                'confirm your age', 'age-restricted', 'inappropriate',  # reason\n+                'age_verification_required', 'age_check_required',  # status\n+            )\n+            return any(expected in reason for expected in AGE_GATE_REASONS for reason in reasons)\n+\n+        def get_playability_status(response):\n+            return try_get(response, lambda x: x['playabilityStatus'], dict) or {}\n+\n+        playability_status = get_playability_status(player_response)\n+        if (is_agegated(playability_status)\n+                and int_or_none(self._downloader.params.get('age_limit'), default=18) >= 18):\n+\n+            self.report_age_confirmation()\n+\n+            # Thanks: https://github.com/yt-dlp/yt-dlp/pull/3233\n+            pb_context = {'html5Preference': 'HTML5_PREF_WANTS'}\n+\n+            # Use signatureTimestamp if available\n+            # Thanks https://github.com/ytdl-org/youtube-dl/issues/31034#issuecomment-1160718026\n+            player_url = self._extract_player_url(webpage)\n+            ytcfg = self._extract_ytcfg(video_id, webpage)\n+            sts = self._extract_signature_timestamp(video_id, player_url, ytcfg)\n+            if sts:\n+                pb_context['signatureTimestamp'] = sts\n+\n+            query = {\n+                'playbackContext': {'contentPlaybackContext': pb_context},\n+                'contentCheckOk': True,\n+                'racyCheckOk': True,\n+                'context': {\n+                    'client': {'clientName': 'TVHTML5_SIMPLY_EMBEDDED_PLAYER', 'clientVersion': '2.0', 'hl': 'en', 'clientScreen': 'EMBED'},\n+                    'thirdParty': {'embedUrl': 'https://google.com'},\n+                },\n+                'videoId': video_id,\n+            }\n+            headers = {\n+                'X-YouTube-Client-Name': '85',\n+                'X-YouTube-Client-Version': '2.0',\n+                'Origin': 'https://www.youtube.com'\n+            }\n+\n+            video_info = self._call_api('player', query, video_id, fatal=False, headers=headers)\n+            age_gate_status = get_playability_status(video_info)\n+            if age_gate_status.get('status') == 'OK':\n+                player_response = video_info\n+                playability_status = age_gate_status\n \n         trailer_video_id = try_get(\n             playability_status,\n@@ -1784,7 +1926,6 @@ def feed_entry(name):\n         formats = []\n         itags = []\n         itag_qualities = {}\n-        player_url = None\n         q = qualities(['tiny', 'small', 'medium', 'large', 'hd720', 'hd1080', 'hd1440', 'hd2160', 'hd2880', 'highres'])\n         streaming_data = player_response.get('streamingData') or {}\n         streaming_formats = streaming_data.get('formats') or []\n@@ -1928,15 +2069,15 @@ def feed_entry(name):\n \n         thumbnails = []\n         for container in (video_details, microformat):\n-            for thumbnail in (try_get(\n+            for thumbnail in try_get(\n                     container,\n-                    lambda x: x['thumbnail']['thumbnails'], list) or []):\n-                thumbnail_url = thumbnail.get('url')\n+                    lambda x: x['thumbnail']['thumbnails'], list) or []:\n+                thumbnail_url = url_or_none(thumbnail.get('url'))\n                 if not thumbnail_url:\n                     continue\n                 thumbnails.append({\n                     'height': int_or_none(thumbnail.get('height')),\n-                    'url': thumbnail_url,\n+                    'url': update_url(thumbnail_url, query=None, fragment=None),\n                     'width': int_or_none(thumbnail.get('width')),\n                 })\n             if thumbnails:\n@@ -1955,7 +2096,17 @@ def feed_entry(name):\n             or microformat.get('lengthSeconds')) \\\n             or parse_duration(search_meta('duration'))\n         is_live = video_details.get('isLive')\n-        owner_profile_url = microformat.get('ownerProfileUrl')\n+\n+        def gen_owner_profile_url():\n+            yield microformat.get('ownerProfileUrl')\n+            yield extract_attributes(self._search_regex(\n+                r'''(?s)(<link\\b[^>]+\\bitemprop\\s*=\\s*(\"|')url\\2[^>]*>)''',\n+                get_element_by_attribute('itemprop', 'author', webpage),\n+                'owner_profile_url', default='')).get('href')\n+\n+        owner_profile_url = next(\n+            (x for x in map(url_or_none, gen_owner_profile_url()) if x),\n+            None)\n \n         if not player_url:\n             player_url = self._extract_player_url(webpage)\n@@ -2040,6 +2191,7 @@ def process_language(container, base_url, lang_code, query):\n                         info[d_k] = parse_duration(query[k][0])\n \n         if video_description:\n+            # Youtube Music Auto-generated description\n             mobj = re.search(r'(?s)(?P<track>[^\u00b7\\n]+)\u00b7(?P<artist>[^\\n]+)\\n+(?P<album>[^\\n]+)(?:.+?\u2117\\s*(?P<release_year>\\d{4})(?!\\d))?(?:.+?Released on\\s*:\\s*(?P<release_date>\\d{4}-\\d{2}-\\d{2}))?(.+?\\nArtist\\s*:\\s*(?P<clean_artist>[^\\n]+))?.+\\nAuto-generated by YouTube\\.\\s*$', video_description)\n             if mobj:\n                 release_year = mobj.group('release_year')\n@@ -2114,7 +2266,8 @@ def chapter_time(mmlir):\n                                 lambda x: x['superTitleIcon']['iconType']) == 'LOCATION_PIN':\n                             info['location'] = stl\n                         else:\n-                            mobj = re.search(r'(.+?)\\s*S(\\d+)\\s*\u2022\\s*E(\\d+)', stl)\n+                            # \u2022? doesn't match, but [\u2022]? does; \\xa0 = non-breaking space\n+                            mobj = re.search(r'([^\\xa0\\s].*?)[\\xa0\\s]*S(\\d+)[\\xa0\\s]*[\u2022]?[\\xa0\\s]*E(\\d+)', stl)\n                             if mobj:\n                                 info.update({\n                                     'series': mobj.group(1),\n@@ -2125,7 +2278,7 @@ def chapter_time(mmlir):\n                             vpir,\n                             lambda x: x['videoActions']['menuRenderer']['topLevelButtons'],\n                             list) or []):\n-                        tbr = tlb.get('toggleButtonRenderer') or {}\n+                        tbr = traverse_obj(tlb, ('segmentedLikeDislikeButtonRenderer', 'likeButton', 'toggleButtonRenderer'), 'toggleButtonRenderer') or {}\n                         for getter, regex in [(\n                                 lambda x: x['defaultText']['accessibility']['accessibilityData'],\n                                 r'(?P<count>[\\d,]+)\\s*(?P<type>(?:dis)?like)'), ([\n@@ -2141,6 +2294,7 @@ def chapter_time(mmlir):\n                     sbr_tooltip = try_get(\n                         vpir, lambda x: x['sentimentBar']['sentimentBarRenderer']['tooltip'])\n                     if sbr_tooltip:\n+                        # however dislike_count was hidden by YT, as if there could ever be dislikable content on YT\n                         like_count, dislike_count = sbr_tooltip.split(' / ')\n                         info.update({\n                             'like_count': str_to_int(like_count),\n@@ -2178,6 +2332,30 @@ def chapter_time(mmlir):\n                             elif mrr_title == 'Song':\n                                 info['track'] = mrr_contents_text\n \n+            # this is not extraction but spelunking!\n+            carousel_lockups = traverse_obj(\n+                initial_data,\n+                ('engagementPanels', Ellipsis, 'engagementPanelSectionListRenderer',\n+                 'content', 'structuredDescriptionContentRenderer', 'items', Ellipsis,\n+                 'videoDescriptionMusicSectionRenderer', 'carouselLockups', Ellipsis),\n+                expected_type=dict) or []\n+            # try to reproduce logic from metadataRowContainerRenderer above (if it still is)\n+            fields = (('ALBUM', 'album'), ('ARTIST', 'artist'), ('SONG', 'track'), ('LICENSES', 'license'))\n+            # multiple_songs ?\n+            if len(carousel_lockups) > 1:\n+                fields = fields[-1:]\n+            for info_row in traverse_obj(\n+                    carousel_lockups,\n+                    (0, 'carouselLockupRenderer', 'infoRows', Ellipsis, 'infoRowRenderer'),\n+                    expected_type=dict):\n+                row_title = traverse_obj(info_row, ('title', 'simpleText'))\n+                row_text = traverse_obj(info_row, 'defaultMetadata', 'expandedMetadata', expected_type=get_text)\n+                if not row_text:\n+                    continue\n+                for name, field in fields:\n+                    if name == row_title and not info.get(field):\n+                        info[field] = row_text\n+\n         for s_k, d_k in [('artist', 'creator'), ('track', 'alt_title')]:\n             v = info.get(s_k)\n             if v:\n@@ -2392,7 +2570,6 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n             'tags': list,\n             'view_count': int,\n             'like_count': int,\n-            'dislike_count': int,\n         },\n         'params': {\n             'skip_download': True,\n@@ -2419,7 +2596,6 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n             'categories': ['News & Politics'],\n             'tags': list,\n             'like_count': int,\n-            'dislike_count': int,\n         },\n         'params': {\n             'skip_download': True,\n@@ -2439,7 +2615,6 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):\n             'categories': ['News & Politics'],\n             'tags': ['Cenk Uygur (TV Program Creator)', 'The Young Turks (Award-Winning Work)', 'Talk Show (TV Genre)'],\n             'like_count': int,\n-            'dislike_count': int,\n         },\n         'params': {\n             'skip_download': True,\n@@ -3020,8 +3195,7 @@ def _extract_identity_token(self, ytcfg, webpage):\n \n     def _real_extract(self, url):\n         item_id = self._match_id(url)\n-        url = compat_urlparse.urlunparse(\n-            compat_urlparse.urlparse(url)._replace(netloc='www.youtube.com'))\n+        url = update_url(url, netloc='www.youtube.com')\n         # Handle both video/playlist URLs\n         qs = parse_qs(url)\n         video_id = qs.get('v', [None])[0]\n@@ -3121,11 +3295,7 @@ class YoutubePlaylistIE(InfoExtractor):\n     def suitable(cls, url):\n         if YoutubeTabIE.suitable(url):\n             return False\n-        # Hack for lazy extractors until more generic solution is implemented\n-        # (see #28780)\n-        from .youtube import parse_qs\n-        qs = parse_qs(url)\n-        if qs.get('v', [None])[0]:\n+        if parse_qs(url).get('v', [None])[0]:\n             return False\n         return super(YoutubePlaylistIE, cls).suitable(url)\n \n@@ -3155,7 +3325,6 @@ class YoutubeYtBeIE(InfoExtractor):\n             'categories': ['Nonprofits & Activism'],\n             'tags': list,\n             'like_count': int,\n-            'dislike_count': int,\n         },\n         'params': {\n             'noplaylist': True,\n@@ -3265,9 +3434,9 @@ class YoutubeSearchURLIE(YoutubeBaseInfoExtractor):\n     }]\n \n     def _real_extract(self, url):\n-        qs = compat_parse_qs(compat_urllib_parse_urlparse(url).query)\n-        query = (qs.get('search_query') or qs.get('q'))[0]\n-        params = qs.get('sp', ('',))[0]\n+        qs = parse_qs(url)\n+        query = (qs.get('search_query') or qs.get('q'))[-1]\n+        params = qs.get('sp', ('',))[-1]\n         return self.playlist_result(self._search_results(query, params), query, query)\n \n \ndiff --git a/youtube_dl/utils.py b/youtube_dl/utils.py\nindex e3c3ccff904..4edbfa27ba6 100644\n--- a/youtube_dl/utils.py\n+++ b/youtube_dl/utils.py\n@@ -42,6 +42,7 @@\n     compat_HTMLParser,\n     compat_HTTPError,\n     compat_basestring,\n+    compat_casefold,\n     compat_chr,\n     compat_collections_abc,\n     compat_cookiejar,\n@@ -54,18 +55,18 @@\n     compat_integer_types,\n     compat_kwargs,\n     compat_os_name,\n-    compat_parse_qs,\n+    compat_re_Match,\n     compat_shlex_quote,\n     compat_str,\n     compat_struct_pack,\n     compat_struct_unpack,\n     compat_urllib_error,\n     compat_urllib_parse,\n+    compat_urllib_parse_parse_qs as compat_parse_qs,\n     compat_urllib_parse_urlencode,\n     compat_urllib_parse_urlparse,\n     compat_urllib_parse_unquote_plus,\n     compat_urllib_request,\n-    compat_urlparse,\n     compat_xpath,\n )\n \n@@ -80,12 +81,12 @@ def register_socks_protocols():\n     # In Python < 2.6.5, urlsplit() suffers from bug https://bugs.python.org/issue7904\n     # URLs with protocols not in urlparse.uses_netloc are not handled correctly\n     for scheme in ('socks', 'socks4', 'socks4a', 'socks5'):\n-        if scheme not in compat_urlparse.uses_netloc:\n-            compat_urlparse.uses_netloc.append(scheme)\n+        if scheme not in compat_urllib_parse.uses_netloc:\n+            compat_urllib_parse.uses_netloc.append(scheme)\n \n \n-# This is not clearly defined otherwise\n-compiled_regex_type = type(re.compile(''))\n+# Unfavoured alias\n+compiled_regex_type = compat_re_Match\n \n \n def random_user_agent():\n@@ -2725,7 +2726,7 @@ def make_socks_conn_class(base_class, socks_proxy):\n     assert issubclass(base_class, (\n         compat_http_client.HTTPConnection, compat_http_client.HTTPSConnection))\n \n-    url_components = compat_urlparse.urlparse(socks_proxy)\n+    url_components = compat_urllib_parse.urlparse(socks_proxy)\n     if url_components.scheme.lower() == 'socks5':\n         socks_type = ProxyType.SOCKS5\n     elif url_components.scheme.lower() in ('socks', 'socks4'):\n@@ -3673,7 +3674,7 @@ def remove_quotes(s):\n \n \n def url_basename(url):\n-    path = compat_urlparse.urlparse(url).path\n+    path = compat_urllib_parse.urlparse(url).path\n     return path.strip('/').split('/')[-1]\n \n \n@@ -3693,7 +3694,7 @@ def urljoin(base, path):\n     if not isinstance(base, compat_str) or not re.match(\n             r'^(?:https?:)?//', base):\n         return None\n-    return compat_urlparse.urljoin(base, path)\n+    return compat_urllib_parse.urljoin(base, path)\n \n \n class HEADRequest(compat_urllib_request.Request):\n@@ -4091,6 +4092,10 @@ def escape_url(url):\n     ).geturl()\n \n \n+def parse_qs(url):\n+    return compat_parse_qs(compat_urllib_parse.urlparse(url).query)\n+\n+\n def read_batch_urls(batch_fd):\n     def fixup(url):\n         if not isinstance(url, compat_str):\n@@ -4111,14 +4116,28 @@ def urlencode_postdata(*args, **kargs):\n     return compat_urllib_parse_urlencode(*args, **kargs).encode('ascii')\n \n \n+def update_url(url, **kwargs):\n+    \"\"\"Replace URL components specified by kwargs\n+       url: compat_str or parsed URL tuple\n+       if query_update is in kwargs, update query with\n+       its value instead of replacing (overrides any `query`)\n+       returns: compat_str\n+    \"\"\"\n+    if not kwargs:\n+        return compat_urllib_parse.urlunparse(url) if isinstance(url, tuple) else url\n+    if not isinstance(url, tuple):\n+        url = compat_urllib_parse.urlparse(url)\n+    query = kwargs.pop('query_update', None)\n+    if query:\n+        qs = compat_parse_qs(url.query)\n+        qs.update(query)\n+        kwargs['query'] = compat_urllib_parse_urlencode(qs, True)\n+        kwargs = compat_kwargs(kwargs)\n+    return compat_urllib_parse.urlunparse(url._replace(**kwargs))\n+\n+\n def update_url_query(url, query):\n-    if not query:\n-        return url\n-    parsed_url = compat_urlparse.urlparse(url)\n-    qs = compat_parse_qs(parsed_url.query)\n-    qs.update(query)\n-    return compat_urlparse.urlunparse(parsed_url._replace(\n-        query=compat_urllib_parse_urlencode(qs, True)))\n+    return update_url(url, query_update=query)\n \n \n def update_Request(req, url=None, data=None, headers={}, query={}):\n@@ -5586,7 +5605,7 @@ def proxy_open(self, req, proxy, type):\n \n         if proxy == '__noproxy__':\n             return None  # No Proxy\n-        if compat_urlparse.urlparse(proxy).scheme.lower() in ('socks', 'socks4', 'socks4a', 'socks5'):\n+        if compat_urllib_parse.urlparse(proxy).scheme.lower() in ('socks', 'socks4', 'socks4a', 'socks5'):\n             req.add_header('Ytdl-socks-proxy', proxy)\n             # youtube-dl's http/https handlers do wrapping the socket with socks\n             return None\n@@ -6024,14 +6043,6 @@ def traverse_obj(obj, *paths, **kwargs):\n     str = compat_str\n \n     is_sequence = lambda x: isinstance(x, compat_collections_abc.Sequence) and not isinstance(x, (str, bytes))\n-    # stand-in until compat_re_Match is added\n-    compat_re_Match = type(re.match('a', 'a'))\n-    # stand-in until casefold.py is added\n-    try:\n-        ''.casefold()\n-        compat_casefold = lambda s: s.casefold()\n-    except AttributeError:\n-        compat_casefold = lambda s: s.lower()\n     casefold = lambda k: compat_casefold(k) if isinstance(k, str) else k\n \n     if isinstance(expected_type, type):\n", "test_patch": "diff --git a/test/test_age_restriction.py b/test/test_age_restriction.py\nindex 6f5513faa2c..db98494ab85 100644\n--- a/test/test_age_restriction.py\n+++ b/test/test_age_restriction.py\n@@ -11,6 +11,7 @@\n \n \n from youtube_dl import YoutubeDL\n+from youtube_dl.utils import DownloadError\n \n \n def _download_restricted(url, filename, age):\n@@ -26,7 +27,10 @@ def _download_restricted(url, filename, age):\n     ydl.add_default_info_extractors()\n     json_filename = os.path.splitext(filename)[0] + '.info.json'\n     try_rm(json_filename)\n-    ydl.download([url])\n+    try:\n+        ydl.download([url])\n+    except DownloadError:\n+        try_rm(json_filename)\n     res = os.path.exists(json_filename)\n     try_rm(json_filename)\n     return res\n@@ -38,12 +42,12 @@ def _assert_restricted(self, url, filename, age, old_age=None):\n         self.assertFalse(_download_restricted(url, filename, age))\n \n     def test_youtube(self):\n-        self._assert_restricted('07FYdnEawAQ', '07FYdnEawAQ.mp4', 10)\n+        self._assert_restricted('HtVdAasjOgU', 'HtVdAasjOgU.mp4', 10)\n \n     def test_youporn(self):\n         self._assert_restricted(\n-            'http://www.youporn.com/watch/505835/sex-ed-is-it-safe-to-masturbate-daily/',\n-            '505835.mp4', 2, old_age=25)\n+            'https://www.youporn.com/watch/16715086/sex-ed-in-detention-18-asmr/',\n+            '16715086.mp4', 2, old_age=25)\n \n \n if __name__ == '__main__':\ndiff --git a/test/test_compat.py b/test/test_compat.py\nindex 0986cff3705..e233b1ae1b7 100644\n--- a/test/test_compat.py\n+++ b/test/test_compat.py\n@@ -48,10 +48,11 @@ def test_compat_expanduser(self):\n \n     def test_all_present(self):\n         import youtube_dl.compat\n-        all_names = youtube_dl.compat.__all__\n-        present_names = set(filter(\n+        all_names = sorted(\n+            youtube_dl.compat.__all__ + youtube_dl.compat.legacy)\n+        present_names = set(map(compat_str, filter(\n             lambda c: '_' in c and not c.startswith('_'),\n-            dir(youtube_dl.compat))) - set(['unicode_literals'])\n+            dir(youtube_dl.compat)))) - set(['unicode_literals'])\n         self.assertEqual(all_names, sorted(present_names))\n \n     def test_compat_urllib_parse_unquote(self):\ndiff --git a/test/test_execution.py b/test/test_execution.py\nindex 32948d93e25..704e1461232 100644\n--- a/test/test_execution.py\n+++ b/test/test_execution.py\n@@ -40,14 +40,16 @@ def test_cmdline_umlauts(self):\n         self.assertFalse(stderr)\n \n     def test_lazy_extractors(self):\n+        lazy_extractors = 'youtube_dl/extractor/lazy_extractors.py'\n         try:\n-            subprocess.check_call([sys.executable, 'devscripts/make_lazy_extractors.py', 'youtube_dl/extractor/lazy_extractors.py'], cwd=rootDir, stdout=_DEV_NULL)\n+            subprocess.check_call([sys.executable, 'devscripts/make_lazy_extractors.py', lazy_extractors], cwd=rootDir, stdout=_DEV_NULL)\n             subprocess.check_call([sys.executable, 'test/test_all_urls.py'], cwd=rootDir, stdout=_DEV_NULL)\n         finally:\n-            try:\n-                os.remove('youtube_dl/extractor/lazy_extractors.py')\n-            except (IOError, OSError):\n-                pass\n+            for x in ['', 'c'] if sys.version_info[0] < 3 else ['']:\n+                try:\n+                    os.remove(lazy_extractors + x)\n+                except (IOError, OSError):\n+                    pass\n \n \n if __name__ == '__main__':\n", "problem_statement": "Login fails/age-restriction\nI have followed the checklist, and really need some help, I can't figure this out myself. :((((\r\n\r\nHello, I have looked and tried all apparent workarounds to age-restricted videos, and haven't gotten any to work.  I'm using Win10 command prompt.  I'd like to download a playlist of my own age-restricted favorites that I've pulled out as time has gone by, but even singe videos, it doesn't work.  I just can't log in, getting a 400 error no matter what.  I've also tried to just enter the username, and then password when prompted, but that method, for some reason I can't type anything at all and have to close CMD altogether.\r\n\n", "hints_text": "", "created_at": "2022-06-21T15:21:58Z"}
{"repo": "ytdl-org/youtube-dl", "pull_number": 30582, "instance_id": "ytdl-org__youtube-dl-30582", "issue_numbers": ["30752"], "base_commit": "af9e72507ea38e5ab3fa2751ed09ec88021260cb", "patch": "diff --git a/youtube_dl/extractor/youtube.py b/youtube_dl/extractor/youtube.py\nindex 63918924df8..7943b94f9d5 100644\n--- a/youtube_dl/extractor/youtube.py\n+++ b/youtube_dl/extractor/youtube.py\n@@ -28,6 +28,7 @@\n     dict_get,\n     float_or_none,\n     int_or_none,\n+    js_to_json,\n     mimetype2ext,\n     parse_codecs,\n     parse_duration,\n@@ -1391,9 +1392,16 @@ def _extract_player_url(self, webpage):\n     # 2. https://code.videolan.org/videolan/vlc/-/blob/4fb284e5af69aa9ac2100ccbdd3b88debec9987f/share/lua/playlist/youtube.lua#L116\n     # 3. https://github.com/ytdl-org/youtube-dl/issues/30097#issuecomment-950157377\n     def _extract_n_function_name(self, jscode):\n-        return self._search_regex(\n-            (r'\\.get\\(\"n\"\\)\\)&&\\(b=(?P<nfunc>[a-zA-Z0-9$]{3})\\([a-zA-Z0-9]\\)',),\n-            jscode, 'Initial JS player n function name', group='nfunc')\n+        target = r'(?P<nfunc>[a-zA-Z0-9$]{3})(?:\\[(?P<idx>\\d+)\\])?'\n+        nfunc_and_idx = self._search_regex(\n+            r'\\.get\\(\"n\"\\)\\)&&\\(b=(%s)\\([a-zA-Z0-9]\\)' % (target, ),\n+            jscode, 'Initial JS player n function name')\n+        nfunc, idx = re.match(target, nfunc_and_idx).group('nfunc', 'idx')\n+        if not idx:\n+            return nfunc\n+        return self._parse_json(self._search_regex(\n+            r'var %s\\s*=\\s*(\\[.+?\\]);' % (nfunc, ), jscode,\n+            'Initial JS player n function list ({nfunc}[{idx}])'.format(**locals())), nfunc, transform_source=js_to_json)[int(idx)]\n \n     def _extract_n_function(self, video_id, player_url):\n         player_id = self._extract_player_info(player_url)\n", "test_patch": "diff --git a/test/test_youtube_signature.py b/test/test_youtube_signature.py\nindex c8e85b5005a..fc5e9828e2c 100644\n--- a/test/test_youtube_signature.py\n+++ b/test/test_youtube_signature.py\n@@ -82,6 +82,14 @@\n         'https://www.youtube.com/s/player/f1ca6900/player_ias.vflset/en_US/base.js',\n         'cu3wyu6LQn2hse', 'jvxetvmlI9AN9Q',\n     ),\n+    (\n+        'https://www.youtube.com/s/player/8040e515/player_ias.vflset/en_US/base.js',\n+        'wvOFaY-yjgDuIEg5', 'HkfBFDHmgw4rsw',\n+    ),\n+    (\n+        'https://www.youtube.com/s/player/e06dea74/player_ias.vflset/en_US/base.js',\n+        'AiuodmaDDYw8d3y4bf', 'ankd8eza2T6Qmw',\n+    ),\n ]\n \n \n@@ -110,10 +118,17 @@ def test_youtube_extract_player_info(self):\n class TestSignature(unittest.TestCase):\n     def setUp(self):\n         TEST_DIR = os.path.dirname(os.path.abspath(__file__))\n-        self.TESTDATA_DIR = os.path.join(TEST_DIR, 'testdata')\n+        self.TESTDATA_DIR = os.path.join(TEST_DIR, 'testdata/sigs')\n         if not os.path.exists(self.TESTDATA_DIR):\n             os.mkdir(self.TESTDATA_DIR)\n \n+    def tearDown(self):\n+        try:\n+            for f in os.listdir(self.TESTDATA_DIR):\n+                os.remove(f)\n+        except OSError:\n+            pass\n+\n \n def t_factory(name, sig_func, url_pattern):\n     def make_tfunc(url, sig_input, expected_sig):\n@@ -145,12 +160,7 @@ def signature(jscode, sig_input):\n \n \n def n_sig(jscode, sig_input):\n-    # Pending implementation of _extract_n_function_name() or similar in\n-    # youtube.py, hard-code here\n-    # funcname = YoutubeIE(FakeYDL())._extract_n_function_name(jscode)\n-    import re\n-    funcname = re.search(r'[=(,&|](\\w+)\\(\\w+\\),\\w+\\.set\\(\"n\",', jscode)\n-    funcname = funcname and funcname.group(1)\n+    funcname = YoutubeIE(FakeYDL())._extract_n_function_name(jscode)\n     return JSInterpreter(jscode).call_function(funcname, sig_input)\n \n \n", "problem_statement": "Virtually Every Utube video i download d/ls at approx 50kb/s \nbut when i try a mainstream supported utube video i get full speed.  this has been the same after every update since early 2020.     what is going on?  am i doing something incorrect?  d/l full quality better than 480p is a thing of my past.  \n", "hints_text": "", "created_at": "2022-02-01T14:17:50Z"}
{"repo": "ytdl-org/youtube-dl", "pull_number": 30329, "instance_id": "ytdl-org__youtube-dl-30329", "issue_numbers": ["24093"], "base_commit": "af9e72507ea38e5ab3fa2751ed09ec88021260cb", "patch": "diff --git a/youtube_dl/downloader/__init__.py b/youtube_dl/downloader/__init__.py\nindex 2e485df9dac..d8f2fa34226 100644\n--- a/youtube_dl/downloader/__init__.py\n+++ b/youtube_dl/downloader/__init__.py\n@@ -1,22 +1,31 @@\n from __future__ import unicode_literals\n \n+from ..utils import (\n+    determine_protocol,\n+)\n+\n+\n+def get_suitable_downloader(info_dict, params={}):\n+    info_dict['protocol'] = determine_protocol(info_dict)\n+    info_copy = info_dict.copy()\n+    return _get_suitable_downloader(info_copy, params)\n+\n+\n+# Some of these require get_suitable_downloader\n from .common import FileDownloader\n+from .dash import DashSegmentsFD\n from .f4m import F4mFD\n from .hls import HlsFD\n from .http import HttpFD\n from .rtmp import RtmpFD\n-from .dash import DashSegmentsFD\n from .rtsp import RtspFD\n from .ism import IsmFD\n+from .niconico import NiconicoDmcFD\n from .external import (\n     get_external_downloader,\n     FFmpegFD,\n )\n \n-from ..utils import (\n-    determine_protocol,\n-)\n-\n PROTOCOL_MAP = {\n     'rtmp': RtmpFD,\n     'm3u8_native': HlsFD,\n@@ -26,13 +35,12 @@\n     'f4m': F4mFD,\n     'http_dash_segments': DashSegmentsFD,\n     'ism': IsmFD,\n+    'niconico_dmc': NiconicoDmcFD,\n }\n \n \n-def get_suitable_downloader(info_dict, params={}):\n+def _get_suitable_downloader(info_dict, params={}):\n     \"\"\"Get the downloader class that can handle the info dict.\"\"\"\n-    protocol = determine_protocol(info_dict)\n-    info_dict['protocol'] = protocol\n \n     # if (info_dict.get('start_time') or info_dict.get('end_time')) and not info_dict.get('requested_formats') and FFmpegFD.can_download(info_dict):\n     #     return FFmpegFD\n@@ -43,6 +51,7 @@ def get_suitable_downloader(info_dict, params={}):\n         if ed.can_download(info_dict):\n             return ed\n \n+    protocol = info_dict['protocol']\n     if protocol.startswith('m3u8') and info_dict.get('is_live'):\n         return FFmpegFD\n \ndiff --git a/youtube_dl/downloader/niconico.py b/youtube_dl/downloader/niconico.py\nnew file mode 100644\nindex 00000000000..6392c998994\n--- /dev/null\n+++ b/youtube_dl/downloader/niconico.py\n@@ -0,0 +1,66 @@\n+# coding: utf-8\n+from __future__ import unicode_literals\n+\n+try:\n+    import threading\n+except ImportError:\n+    threading = None\n+\n+from .common import FileDownloader\n+from ..downloader import get_suitable_downloader\n+from ..extractor.niconico import NiconicoIE\n+from ..utils import sanitized_Request\n+\n+\n+class NiconicoDmcFD(FileDownloader):\n+    \"\"\" Downloading niconico douga from DMC with heartbeat \"\"\"\n+\n+    FD_NAME = 'niconico_dmc'\n+\n+    def real_download(self, filename, info_dict):\n+        self.to_screen('[%s] Downloading from DMC' % self.FD_NAME)\n+\n+        ie = NiconicoIE(self.ydl)\n+        info_dict, heartbeat_info_dict = ie._get_heartbeat_info(info_dict)\n+\n+        fd = get_suitable_downloader(info_dict, params=self.params)(self.ydl, self.params)\n+        for ph in self._progress_hooks:\n+            fd.add_progress_hook(ph)\n+\n+        if not threading:\n+            self.to_screen('[%s] Threading for Heartbeat not available' % self.FD_NAME)\n+            return fd.real_download(filename, info_dict)\n+\n+        success = download_complete = False\n+        timer = [None]\n+        heartbeat_lock = threading.Lock()\n+        heartbeat_url = heartbeat_info_dict['url']\n+        heartbeat_data = heartbeat_info_dict['data'].encode()\n+        heartbeat_interval = heartbeat_info_dict.get('interval', 30)\n+\n+        request = sanitized_Request(heartbeat_url, heartbeat_data)\n+\n+        def heartbeat():\n+            try:\n+                self.ydl.urlopen(request).read()\n+            except Exception:\n+                self.to_screen('[%s] Heartbeat failed' % self.FD_NAME)\n+\n+            with heartbeat_lock:\n+                if not download_complete:\n+                    timer[0] = threading.Timer(heartbeat_interval, heartbeat)\n+                    timer[0].start()\n+\n+        heartbeat_info_dict['ping']()\n+        self.to_screen('[%s] Heartbeat with %d second interval ...' % (self.FD_NAME, heartbeat_interval))\n+        try:\n+            heartbeat()\n+            if type(fd).__name__ == 'HlsFD':\n+                info_dict.update(ie._extract_m3u8_formats(info_dict['url'], info_dict['id'])[0])\n+            success = fd.real_download(filename, info_dict)\n+        finally:\n+            if heartbeat_lock:\n+                with heartbeat_lock:\n+                    timer[0].cancel()\n+                    download_complete = True\n+            return success\ndiff --git a/youtube_dl/extractor/extractors.py b/youtube_dl/extractor/extractors.py\nindex 6e8fc3961b7..ece9dedaa4a 100644\n--- a/youtube_dl/extractor/extractors.py\n+++ b/youtube_dl/extractor/extractors.py\n@@ -789,7 +789,14 @@\n     NickNightIE,\n     NickRuIE,\n )\n-from .niconico import NiconicoIE, NiconicoPlaylistIE\n+from .niconico import (\n+    NiconicoIE,\n+    NiconicoPlaylistIE,\n+    NiconicoUserIE,\n+    NicovideoSearchIE,\n+    NicovideoSearchDateIE,\n+    NicovideoSearchURLIE,\n+)\n from .ninecninemedia import NineCNineMediaIE\n from .ninegag import NineGagIE\n from .ninenow import NineNowIE\ndiff --git a/youtube_dl/extractor/niconico.py b/youtube_dl/extractor/niconico.py\nindex a85fc3d5c9d..93f81396897 100644\n--- a/youtube_dl/extractor/niconico.py\n+++ b/youtube_dl/extractor/niconico.py\n@@ -2,25 +2,28 @@\n from __future__ import unicode_literals\n \n import datetime\n-import functools\n+import itertools\n import json\n-import math\n+import re\n \n-from .common import InfoExtractor\n+from .common import InfoExtractor, SearchInfoExtractor\n+from ..postprocessor.ffmpeg import FFmpegPostProcessor\n from ..compat import (\n     compat_parse_qs,\n+    compat_str,\n     compat_urllib_parse_urlparse,\n )\n from ..utils import (\n-    determine_ext,\n-    dict_get,\n     ExtractorError,\n+    dict_get,\n     float_or_none,\n-    InAdvancePagedList,\n     int_or_none,\n+    OnDemandPagedList,\n     parse_duration,\n     parse_iso8601,\n+    PostProcessingError,\n     remove_start,\n+    str_or_none,\n     try_get,\n     unified_timestamp,\n     urlencode_postdata,\n@@ -34,7 +37,7 @@ class NiconicoIE(InfoExtractor):\n \n     _TESTS = [{\n         'url': 'http://www.nicovideo.jp/watch/sm22312215',\n-        'md5': 'd1a75c0823e2f629128c43e1212760f9',\n+        'md5': 'a5bad06f1347452102953f323c69da34s',\n         'info_dict': {\n             'id': 'sm22312215',\n             'ext': 'mp4',\n@@ -157,11 +160,34 @@ class NiconicoIE(InfoExtractor):\n     }, {\n         'url': 'http://sp.nicovideo.jp/watch/sm28964488?ss_pos=1&cp_in=wt_tg',\n         'only_matching': True,\n+    }, {\n+        # DMC video with heartbeat\n+        'url': 'https://www.nicovideo.jp/watch/sm34815188',\n+        'md5': '9360c6e1f1519d7759e2fe8e1326ae83',\n+        'info_dict': {\n+            'id': 'sm34815188',\n+            'ext': 'mp4',\n+            'title': 'md5:aee93e9f3366db72f902f6cd5d389cb7',\n+            'description': 'md5:7b9149fc7a00ab053cafaf5c19662704',\n+            'thumbnail': r're:https?://.*',\n+            'uploader': 'md5:2762e18fa74dbb40aa1ad27c6291ee32',\n+            'uploader_id': '67449889',\n+            'upload_date': '20190322',\n+            'timestamp': int,  # timestamp is unstable\n+            'duration': 1082.0,\n+            'view_count': int,\n+            'comment_count': int,\n+        },\n     }]\n \n     _VALID_URL = r'https?://(?:www\\.|secure\\.|sp\\.)?nicovideo\\.jp/watch/(?P<id>(?:[a-z]{2})?[0-9]+)'\n     _NETRC_MACHINE = 'niconico'\n \n+    _API_HEADERS = {\n+        'X-Frontend-ID': '6',\n+        'X-Frontend-Version': '0'\n+    }\n+\n     def _real_initialize(self):\n         self._login()\n \n@@ -191,37 +217,89 @@ def _login(self):\n             self._downloader.report_warning('unable to log in: bad username or password')\n         return login_ok\n \n-    def _extract_format_for_quality(self, api_data, video_id, audio_quality, video_quality):\n-        def yesno(boolean):\n-            return 'yes' if boolean else 'no'\n-\n-        session_api_data = api_data['video']['dmcInfo']['session_api']\n-        session_api_endpoint = session_api_data['urls'][0]\n-\n-        format_id = '-'.join(map(lambda s: remove_start(s['id'], 'archive_'), [video_quality, audio_quality]))\n+    def _get_heartbeat_info(self, info_dict):\n+\n+        video_id, video_src_id, audio_src_id = info_dict['url'].split(':')[1].split('/')\n+\n+        api_data = (\n+            info_dict.get('_api_data')\n+            or self._parse_json(\n+                self._html_search_regex(\n+                    'data-api-data=\"([^\"]+)\"',\n+                    self._download_webpage('http://www.nicovideo.jp/watch/' + video_id, video_id),\n+                    'API data', default='{}'),\n+                video_id))\n+\n+        session_api_data = try_get(api_data, lambda x: x['media']['delivery']['movie']['session'])\n+        session_api_endpoint = try_get(session_api_data, lambda x: x['urls'][0])\n+\n+        def ping():\n+            status = try_get(\n+                self._download_json(\n+                    'https://nvapi.nicovideo.jp/v1/2ab0cbaa/watch', video_id,\n+                    query={'t': try_get(api_data, lambda x: x['media']['delivery']['trackingId'])},\n+                    note='Acquiring permission for downloading video',\n+                    headers=self._API_HEADERS),\n+                lambda x: x['meta']['status'])\n+            if status != 200:\n+                self.report_warning('Failed to acquire permission for playing video. The video may not download.')\n+\n+        yesno = lambda x: 'yes' if x else 'no'\n+\n+        # m3u8 (encryption)\n+        if try_get(api_data, lambda x: x['media']['delivery']['encryption']) is not None:\n+            protocol = 'm3u8'\n+            encryption = self._parse_json(session_api_data['token'], video_id)['hls_encryption']\n+            session_api_http_parameters = {\n+                'parameters': {\n+                    'hls_parameters': {\n+                        'encryption': {\n+                            encryption: {\n+                                'encrypted_key': try_get(api_data, lambda x: x['media']['delivery']['encryption']['encryptedKey']),\n+                                'key_uri': try_get(api_data, lambda x: x['media']['delivery']['encryption']['keyUri'])\n+                            }\n+                        },\n+                        'transfer_preset': '',\n+                        'use_ssl': yesno(session_api_endpoint['isSsl']),\n+                        'use_well_known_port': yesno(session_api_endpoint['isWellKnownPort']),\n+                        'segment_duration': 6000,\n+                    }\n+                }\n+            }\n+        # http\n+        else:\n+            protocol = 'http'\n+            session_api_http_parameters = {\n+                'parameters': {\n+                    'http_output_download_parameters': {\n+                        'use_ssl': yesno(session_api_endpoint['isSsl']),\n+                        'use_well_known_port': yesno(session_api_endpoint['isWellKnownPort']),\n+                    }\n+                }\n+            }\n \n         session_response = self._download_json(\n             session_api_endpoint['url'], video_id,\n             query={'_format': 'json'},\n             headers={'Content-Type': 'application/json'},\n-            note='Downloading JSON metadata for %s' % format_id,\n+            note='Downloading JSON metadata for %s' % info_dict['format_id'],\n             data=json.dumps({\n                 'session': {\n                     'client_info': {\n-                        'player_id': session_api_data['player_id'],\n+                        'player_id': session_api_data.get('playerId'),\n                     },\n                     'content_auth': {\n-                        'auth_type': session_api_data['auth_types'][session_api_data['protocols'][0]],\n-                        'content_key_timeout': session_api_data['content_key_timeout'],\n+                        'auth_type': try_get(session_api_data, lambda x: x['authTypes'][session_api_data['protocols'][0]]),\n+                        'content_key_timeout': session_api_data.get('contentKeyTimeout'),\n                         'service_id': 'nicovideo',\n-                        'service_user_id': session_api_data['service_user_id']\n+                        'service_user_id': session_api_data.get('serviceUserId')\n                     },\n-                    'content_id': session_api_data['content_id'],\n+                    'content_id': session_api_data.get('contentId'),\n                     'content_src_id_sets': [{\n                         'content_src_ids': [{\n                             'src_id_to_mux': {\n-                                'audio_src_ids': [audio_quality['id']],\n-                                'video_src_ids': [video_quality['id']],\n+                                'audio_src_ids': [audio_src_id],\n+                                'video_src_ids': [video_src_id],\n                             }\n                         }]\n                     }],\n@@ -229,52 +307,81 @@ def yesno(boolean):\n                     'content_uri': '',\n                     'keep_method': {\n                         'heartbeat': {\n-                            'lifetime': session_api_data['heartbeat_lifetime']\n+                            'lifetime': session_api_data.get('heartbeatLifetime')\n                         }\n                     },\n-                    'priority': session_api_data['priority'],\n+                    'priority': session_api_data.get('priority'),\n                     'protocol': {\n                         'name': 'http',\n                         'parameters': {\n-                            'http_parameters': {\n-                                'parameters': {\n-                                    'http_output_download_parameters': {\n-                                        'use_ssl': yesno(session_api_endpoint['is_ssl']),\n-                                        'use_well_known_port': yesno(session_api_endpoint['is_well_known_port']),\n-                                    }\n-                                }\n-                            }\n+                            'http_parameters': session_api_http_parameters\n                         }\n                     },\n-                    'recipe_id': session_api_data['recipe_id'],\n+                    'recipe_id': session_api_data.get('recipeId'),\n                     'session_operation_auth': {\n                         'session_operation_auth_by_signature': {\n-                            'signature': session_api_data['signature'],\n-                            'token': session_api_data['token'],\n+                            'signature': session_api_data.get('signature'),\n+                            'token': session_api_data.get('token'),\n                         }\n                     },\n                     'timing_constraint': 'unlimited'\n                 }\n             }).encode())\n \n-        resolution = video_quality.get('resolution', {})\n+        info_dict['url'] = session_response['data']['session']['content_uri']\n+        info_dict['protocol'] = protocol\n+\n+        # get heartbeat info\n+        heartbeat_info_dict = {\n+            'url': session_api_endpoint['url'] + '/' + session_response['data']['session']['id'] + '?_format=json&_method=PUT',\n+            'data': json.dumps(session_response['data']),\n+            # interval, convert milliseconds to seconds, then halve to make a buffer.\n+            'interval': float_or_none(session_api_data.get('heartbeatLifetime'), scale=3000),\n+            'ping': ping\n+        }\n+\n+        return info_dict, heartbeat_info_dict\n+\n+    def _extract_format_for_quality(self, api_data, video_id, audio_quality, video_quality):\n+        def parse_format_id(id_code):\n+            mobj = re.match(r'''(?x)\n+                    (?:archive_)?\n+                    (?:(?P<codec>[^_]+)_)?\n+                    (?:(?P<br>[\\d]+)kbps_)?\n+                    (?:(?P<res>[\\d+]+)p_)?\n+                ''', '%s_' % id_code)\n+            return mobj.groupdict() if mobj else {}\n+\n+        protocol = 'niconico_dmc'\n+        format_id = '-'.join(map(lambda s: remove_start(s['id'], 'archive_'), [video_quality, audio_quality]))\n+        vdict = parse_format_id(video_quality['id'])\n+        adict = parse_format_id(audio_quality['id'])\n+        resolution = try_get(video_quality, lambda x: x['metadata']['resolution'], dict) or {'height': vdict.get('res')}\n+        vbr = try_get(video_quality, lambda x: x['metadata']['bitrate'], float)\n \n         return {\n-            'url': session_response['data']['session']['content_uri'],\n+            'url': '%s:%s/%s/%s' % (protocol, video_id, video_quality['id'], audio_quality['id']),\n             'format_id': format_id,\n+            'format_note': 'DMC %s' % try_get(video_quality, lambda x: x['metadata']['label'], compat_str),\n             'ext': 'mp4',  # Session API are used in HTML5, which always serves mp4\n-            'abr': float_or_none(audio_quality.get('bitrate'), 1000),\n-            'vbr': float_or_none(video_quality.get('bitrate'), 1000),\n-            'height': resolution.get('height'),\n-            'width': resolution.get('width'),\n+            'vcodec': vdict.get('codec'),\n+            'acodec': adict.get('codec'),\n+            'vbr': float_or_none(vbr, 1000) or float_or_none(vdict.get('br')),\n+            'abr': float_or_none(audio_quality.get('bitrate'), 1000) or float_or_none(adict.get('br')),\n+            'height': int_or_none(resolution.get('height', vdict.get('res'))),\n+            'width': int_or_none(resolution.get('width')),\n+            'quality': -2 if 'low' in format_id else -1,  # Default quality value is -1\n+            'protocol': protocol,\n+            'http_headers': {\n+                'Origin': 'https://www.nicovideo.jp',\n+                'Referer': 'https://www.nicovideo.jp/watch/' + video_id,\n+            }\n         }\n \n     def _real_extract(self, url):\n         video_id = self._match_id(url)\n \n-        # Get video webpage. We are not actually interested in it for normal\n-        # cases, but need the cookies in order to be able to download the\n-        # info webpage\n+        # Get video webpage for API data.\n         webpage, handle = self._download_webpage_handle(\n             'http://www.nicovideo.jp/watch/' + video_id, video_id)\n         if video_id.startswith('so'):\n@@ -284,86 +391,136 @@ def _real_extract(self, url):\n             'data-api-data=\"([^\"]+)\"', webpage,\n             'API data', default='{}'), video_id)\n \n-        def _format_id_from_url(video_url):\n-            return 'economy' if video_real_url.endswith('low') else 'normal'\n-\n-        try:\n-            video_real_url = api_data['video']['smileInfo']['url']\n-        except KeyError:  # Flash videos\n-            # Get flv info\n-            flv_info_webpage = self._download_webpage(\n-                'http://flapi.nicovideo.jp/api/getflv/' + video_id + '?as3=1',\n-                video_id, 'Downloading flv info')\n-\n-            flv_info = compat_parse_qs(flv_info_webpage)\n-            if 'url' not in flv_info:\n-                if 'deleted' in flv_info:\n-                    raise ExtractorError('The video has been deleted.',\n-                                         expected=True)\n-                elif 'closed' in flv_info:\n-                    raise ExtractorError('Niconico videos now require logging in',\n-                                         expected=True)\n-                elif 'error' in flv_info:\n-                    raise ExtractorError('%s reports error: %s' % (\n-                        self.IE_NAME, flv_info['error'][0]), expected=True)\n-                else:\n-                    raise ExtractorError('Unable to find video URL')\n-\n-            video_info_xml = self._download_xml(\n-                'http://ext.nicovideo.jp/api/getthumbinfo/' + video_id,\n-                video_id, note='Downloading video info page')\n-\n-            def get_video_info(items):\n-                if not isinstance(items, list):\n-                    items = [items]\n-                for item in items:\n-                    ret = xpath_text(video_info_xml, './/' + item)\n-                    if ret:\n-                        return ret\n-\n-            video_real_url = flv_info['url'][0]\n-\n-            extension = get_video_info('movie_type')\n-            if not extension:\n-                extension = determine_ext(video_real_url)\n-\n-            formats = [{\n-                'url': video_real_url,\n-                'ext': extension,\n-                'format_id': _format_id_from_url(video_real_url),\n-            }]\n-        else:\n-            formats = []\n-\n-            dmc_info = api_data['video'].get('dmcInfo')\n-            if dmc_info:  # \"New\" HTML5 videos\n-                quality_info = dmc_info['quality']\n-                for audio_quality in quality_info['audios']:\n-                    for video_quality in quality_info['videos']:\n-                        if not audio_quality['available'] or not video_quality['available']:\n-                            continue\n-                        formats.append(self._extract_format_for_quality(\n-                            api_data, video_id, audio_quality, video_quality))\n-\n-                self._sort_formats(formats)\n-            else:  # \"Old\" HTML5 videos\n-                formats = [{\n+        def get_video_info_web(items):\n+            return dict_get(api_data['video'], items)\n+\n+        # Get video info\n+        video_info_xml = self._download_xml(\n+            'http://ext.nicovideo.jp/api/getthumbinfo/' + video_id,\n+            video_id, note='Downloading video info page')\n+\n+        def get_video_info_xml(items):\n+            if not isinstance(items, list):\n+                items = [items]\n+            for item in items:\n+                ret = xpath_text(video_info_xml, './/' + item)\n+                if ret:\n+                    return ret\n+\n+        if get_video_info_xml('error'):\n+            error_code = get_video_info_xml('code')\n+\n+            if error_code == 'DELETED':\n+                raise ExtractorError('The video has been deleted.',\n+                                     expected=True)\n+            elif error_code == 'NOT_FOUND':\n+                raise ExtractorError('The video is not found.',\n+                                     expected=True)\n+            elif error_code == 'COMMUNITY':\n+                self.to_screen('%s: The video is community members only.' % video_id)\n+            else:\n+                raise ExtractorError('%s reports error: %s' % (self.IE_NAME, error_code))\n+\n+        # Start extracting video formats\n+        formats = []\n+\n+        # Get HTML5 videos info\n+        quality_info = try_get(api_data, lambda x: x['media']['delivery']['movie'])\n+        if not quality_info:\n+            raise ExtractorError('The video can\\'t be downloaded', expected=True)\n+\n+        for audio_quality in quality_info.get('audios') or {}:\n+            for video_quality in quality_info.get('videos') or {}:\n+                if not audio_quality.get('isAvailable') or not video_quality.get('isAvailable'):\n+                    continue\n+                formats.append(self._extract_format_for_quality(\n+                    api_data, video_id, audio_quality, video_quality))\n+\n+        # Get flv/swf info\n+        timestamp = None\n+        video_real_url = try_get(api_data, lambda x: x['video']['smileInfo']['url'])\n+        if video_real_url:\n+            is_economy = video_real_url.endswith('low')\n+\n+            if is_economy:\n+                self.report_warning('Site is currently in economy mode! You will only have access to lower quality streams')\n+\n+            # Invoking ffprobe to determine resolution\n+            pp = FFmpegPostProcessor(self._downloader)\n+            cookies = self._get_cookies('https://nicovideo.jp').output(header='', sep='; path=/; domain=nicovideo.jp;\\n')\n+\n+            self.to_screen('%s: %s' % (video_id, 'Checking smile format with ffprobe'))\n+\n+            try:\n+                metadata = pp.get_metadata_object(video_real_url, ['-cookies', cookies])\n+            except PostProcessingError as err:\n+                raise ExtractorError(err.msg, expected=True)\n+\n+            v_stream = a_stream = {}\n+\n+            # Some complex swf files doesn't have video stream (e.g. nm4809023)\n+            for stream in metadata['streams']:\n+                if stream['codec_type'] == 'video':\n+                    v_stream = stream\n+                elif stream['codec_type'] == 'audio':\n+                    a_stream = stream\n+\n+            # Community restricted videos seem to have issues with the thumb API not returning anything at all\n+            filesize = int(\n+                (get_video_info_xml('size_high') if not is_economy else get_video_info_xml('size_low'))\n+                or metadata['format']['size']\n+            )\n+            extension = (\n+                get_video_info_xml('movie_type')\n+                or 'mp4' if 'mp4' in metadata['format']['format_name'] else metadata['format']['format_name']\n+            )\n+\n+            # 'creation_time' tag on video stream of re-encoded SMILEVIDEO mp4 files are '1970-01-01T00:00:00.000000Z'.\n+            timestamp = (\n+                parse_iso8601(get_video_info_web('first_retrieve'))\n+                or unified_timestamp(get_video_info_web('postedDateTime'))\n+            )\n+            metadata_timestamp = (\n+                parse_iso8601(try_get(v_stream, lambda x: x['tags']['creation_time']))\n+                or timestamp if extension != 'mp4' else 0\n+            )\n+\n+            # According to compconf, smile videos from pre-2017 are always better quality than their DMC counterparts\n+            smile_threshold_timestamp = parse_iso8601('2016-12-08T00:00:00+09:00')\n+\n+            is_source = timestamp < smile_threshold_timestamp or metadata_timestamp > 0\n+\n+            # If movie file size is unstable, old server movie is not source movie.\n+            if filesize > 1:\n+                formats.append({\n                     'url': video_real_url,\n-                    'ext': 'mp4',\n-                    'format_id': _format_id_from_url(video_real_url),\n-                }]\n-\n-            def get_video_info(items):\n-                return dict_get(api_data['video'], items)\n+                    'format_id': 'smile' if not is_economy else 'smile_low',\n+                    'format_note': 'SMILEVIDEO source' if not is_economy else 'SMILEVIDEO low quality',\n+                    'ext': extension,\n+                    'container': extension,\n+                    'vcodec': v_stream.get('codec_name'),\n+                    'acodec': a_stream.get('codec_name'),\n+                    # Some complex swf files doesn't have total bit rate metadata (e.g. nm6049209)\n+                    'tbr': int_or_none(metadata['format'].get('bit_rate'), scale=1000),\n+                    'vbr': int_or_none(v_stream.get('bit_rate'), scale=1000),\n+                    'abr': int_or_none(a_stream.get('bit_rate'), scale=1000),\n+                    'height': int_or_none(v_stream.get('height')),\n+                    'width': int_or_none(v_stream.get('width')),\n+                    'source_preference': 5 if not is_economy else -2,\n+                    'quality': 5 if is_source and not is_economy else None,\n+                    'filesize': filesize\n+                })\n+\n+        self._sort_formats(formats)\n \n         # Start extracting information\n-        title = get_video_info('title')\n-        if not title:\n-            title = self._og_search_title(webpage, default=None)\n-        if not title:\n-            title = self._html_search_regex(\n+        title = (\n+            get_video_info_xml('title')  # prefer to get the untranslated original title\n+            or get_video_info_web(['originalTitle', 'title'])\n+            or self._og_search_title(webpage, default=None)\n+            or self._html_search_regex(\n                 r'<span[^>]+class=\"videoHeaderTitle\"[^>]*>([^<]+)</span>',\n-                webpage, 'video title')\n+                webpage, 'video title'))\n \n         watch_api_data_string = self._html_search_regex(\n             r'<div[^>]+id=\"watchAPIDataContainer\"[^>]+>([^<]+)</div>',\n@@ -372,14 +529,15 @@ def get_video_info(items):\n         video_detail = watch_api_data.get('videoDetail', {})\n \n         thumbnail = (\n-            get_video_info(['thumbnail_url', 'thumbnailURL'])\n+            self._html_search_regex(r'<meta property=\"og:image\" content=\"([^\"]+)\">', webpage, 'thumbnail data', default=None)\n+            or dict_get(  # choose highest from 720p to 240p\n+                get_video_info_web('thumbnail'),\n+                ['ogp', 'player', 'largeUrl', 'middleUrl', 'url'])\n             or self._html_search_meta('image', webpage, 'thumbnail', default=None)\n             or video_detail.get('thumbnail'))\n \n-        description = get_video_info('description')\n+        description = get_video_info_web('description')\n \n-        timestamp = (parse_iso8601(get_video_info('first_retrieve'))\n-                     or unified_timestamp(get_video_info('postedDateTime')))\n         if not timestamp:\n             match = self._html_search_meta('datePublished', webpage, 'date published', default=None)\n             if match:\n@@ -388,19 +546,25 @@ def get_video_info(items):\n             timestamp = parse_iso8601(\n                 video_detail['postedAt'].replace('/', '-'),\n                 delimiter=' ', timezone=datetime.timedelta(hours=9))\n+        timestamp = timestamp or try_get(api_data, lambda x: parse_iso8601(x['video']['registeredAt']))\n \n-        view_count = int_or_none(get_video_info(['view_counter', 'viewCount']))\n+        view_count = int_or_none(get_video_info_web(['view_counter', 'viewCount']))\n         if not view_count:\n             match = self._html_search_regex(\n                 r'>Views: <strong[^>]*>([^<]+)</strong>',\n                 webpage, 'view count', default=None)\n             if match:\n                 view_count = int_or_none(match.replace(',', ''))\n-        view_count = view_count or video_detail.get('viewCount')\n+        view_count = (\n+            view_count\n+            or video_detail.get('viewCount')\n+            or try_get(api_data, lambda x: x['video']['count']['view']))\n+\n+        comment_count = (\n+            int_or_none(get_video_info_web('comment_num'))\n+            or video_detail.get('commentCount')\n+            or try_get(api_data, lambda x: x['video']['count']['comment']))\n \n-        comment_count = (int_or_none(get_video_info('comment_num'))\n-                         or video_detail.get('commentCount')\n-                         or try_get(api_data, lambda x: x['thread']['commentCount']))\n         if not comment_count:\n             match = self._html_search_regex(\n                 r'>Comments: <strong[^>]*>([^<]+)</strong>',\n@@ -409,22 +573,41 @@ def get_video_info(items):\n                 comment_count = int_or_none(match.replace(',', ''))\n \n         duration = (parse_duration(\n-            get_video_info('length')\n+            get_video_info_web('length')\n             or self._html_search_meta(\n                 'video:duration', webpage, 'video duration', default=None))\n             or video_detail.get('length')\n-            or get_video_info('duration'))\n+            or get_video_info_web('duration'))\n+\n+        webpage_url = get_video_info_web('watch_url') or url\n \n-        webpage_url = get_video_info('watch_url') or url\n+        # for channel movie and community movie\n+        channel_id = try_get(\n+            api_data,\n+            (lambda x: x['channel']['globalId'],\n+             lambda x: x['community']['globalId']))\n+        channel = try_get(\n+            api_data,\n+            (lambda x: x['channel']['name'],\n+             lambda x: x['community']['name']))\n \n         # Note: cannot use api_data.get('owner', {}) because owner may be set to \"null\"\n         # in the JSON, which will cause None to be returned instead of {}.\n         owner = try_get(api_data, lambda x: x.get('owner'), dict) or {}\n-        uploader_id = get_video_info(['ch_id', 'user_id']) or owner.get('id')\n-        uploader = get_video_info(['ch_name', 'user_nickname']) or owner.get('nickname')\n+        uploader_id = str_or_none(\n+            get_video_info_web(['ch_id', 'user_id'])\n+            or owner.get('id')\n+            or channel_id\n+        )\n+        uploader = (\n+            get_video_info_web(['ch_name', 'user_nickname'])\n+            or owner.get('nickname')\n+            or channel\n+        )\n \n         return {\n             'id': video_id,\n+            '_api_data': api_data,\n             'title': title,\n             'formats': formats,\n             'thumbnail': thumbnail,\n@@ -432,6 +615,8 @@ def get_video_info(items):\n             'uploader': uploader,\n             'timestamp': timestamp,\n             'uploader_id': uploader_id,\n+            'channel': channel,\n+            'channel_id': channel_id,\n             'view_count': view_count,\n             'comment_count': comment_count,\n             'duration': duration,\n@@ -440,7 +625,7 @@ def get_video_info(items):\n \n \n class NiconicoPlaylistIE(InfoExtractor):\n-    _VALID_URL = r'https?://(?:www\\.)?nicovideo\\.jp/(?:user/\\d+/)?mylist/(?P<id>\\d+)'\n+    _VALID_URL = r'https?://(?:www\\.)?nicovideo\\.jp/(?:user/\\d+/|my/)?mylist/(?P<id>\\d+)'\n \n     _TESTS = [{\n         'url': 'http://www.nicovideo.jp/mylist/27411728',\n@@ -456,60 +641,185 @@ class NiconicoPlaylistIE(InfoExtractor):\n         'url': 'https://www.nicovideo.jp/user/805442/mylist/27411728',\n         'only_matching': True,\n     }]\n-    _PAGE_SIZE = 100\n \n-    def _call_api(self, list_id, resource, query):\n-        return self._download_json(\n-            'https://nvapi.nicovideo.jp/v2/mylists/' + list_id, list_id,\n-            'Downloading %s JSON metatdata' % resource, query=query,\n-            headers={'X-Frontend-Id': 6})['data']['mylist']\n-\n-    def _parse_owner(self, item):\n-        owner = item.get('owner') or {}\n-        if owner:\n-            return {\n-                'uploader': owner.get('name'),\n-                'uploader_id': owner.get('id'),\n-            }\n-        return {}\n-\n-    def _fetch_page(self, list_id, page):\n-        page += 1\n-        items = self._call_api(list_id, 'page %d' % page, {\n-            'page': page,\n-            'pageSize': self._PAGE_SIZE,\n-        })['items']\n-        for item in items:\n-            video = item.get('video') or {}\n-            video_id = video.get('id')\n-            if not video_id:\n-                continue\n-            count = video.get('count') or {}\n-            get_count = lambda x: int_or_none(count.get(x))\n-            info = {\n+    _API_HEADERS = {\n+        'X-Frontend-ID': '6',\n+        'X-Frontend-Version': '0'\n+    }\n+\n+    def _real_extract(self, url):\n+        list_id = self._match_id(url)\n+\n+        def get_page_data(pagenum, pagesize):\n+            return self._download_json(\n+                'http://nvapi.nicovideo.jp/v2/mylists/' + list_id, list_id,\n+                query={'page': 1 + pagenum, 'pageSize': pagesize},\n+                headers=self._API_HEADERS).get('data').get('mylist')\n+\n+        data = get_page_data(0, 1)\n+        title = data.get('name')\n+        description = data.get('description')\n+        uploader = data.get('owner').get('name')\n+        uploader_id = data.get('owner').get('id')\n+\n+        def pagefunc(pagenum):\n+            data = get_page_data(pagenum, 25)\n+            return ({\n                 '_type': 'url',\n-                'id': video_id,\n-                'title': video.get('title'),\n-                'url': 'https://www.nicovideo.jp/watch/' + video_id,\n-                'description': video.get('shortDescription'),\n-                'duration': int_or_none(video.get('duration')),\n-                'view_count': get_count('view'),\n-                'comment_count': get_count('comment'),\n-                'ie_key': NiconicoIE.ie_key(),\n-            }\n-            info.update(self._parse_owner(video))\n-            yield info\n+                'url': 'http://www.nicovideo.jp/watch/' + item.get('watchId'),\n+            } for item in data.get('items'))\n+\n+        return {\n+            '_type': 'playlist',\n+            'id': list_id,\n+            'title': title,\n+            'description': description,\n+            'uploader': uploader,\n+            'uploader_id': uploader_id,\n+            'entries': OnDemandPagedList(pagefunc, 25),\n+        }\n+\n+\n+class NicovideoSearchBaseIE(InfoExtractor):\n+    _MAX_RESULTS = float('inf')\n+\n+    def _entries(self, url, item_id, query=None, note='Downloading page %(page)s'):\n+        query = query or {}\n+        pages = [query['page']] if 'page' in query else itertools.count(1)\n+        for page_num in pages:\n+            query['page'] = str(page_num)\n+            webpage = self._download_webpage(url, item_id, query=query, note=note % {'page': page_num})\n+            results = re.findall(r'(?<=data-video-id=)[\"\\']?(?P<videoid>.+?)(?=[\"\\'])', webpage)\n+            for item in results:\n+                yield self.url_result('http://www.nicovideo.jp/watch/%s' % item, 'Niconico', item)\n+            if not results:\n+                break\n+\n+    def _get_n_results(self, query, n):\n+        entries = self._entries(self._proto_relative_url('//www.nicovideo.jp/search/%s' % query), query)\n+        if n < self._MAX_RESULTS:\n+            entries = itertools.islice(entries, 0, n)\n+        return self.playlist_result(entries, query, query)\n+\n+\n+class NicovideoSearchIE(NicovideoSearchBaseIE, SearchInfoExtractor):\n+    IE_DESC = 'Nico video search'\n+    IE_NAME = 'nicovideo:search'\n+    _SEARCH_KEY = 'nicosearch'\n+\n+    def _search_results(self, query):\n+        return self._entries(\n+            self._proto_relative_url('//www.nicovideo.jp/search/%s' % query), query)\n+\n+\n+class NicovideoSearchURLIE(NicovideoSearchBaseIE):\n+    IE_NAME = '%s_url' % NicovideoSearchIE.IE_NAME\n+    IE_DESC = 'Nico video search URLs'\n+    _VALID_URL = r'https?://(?:www\\.)?nicovideo\\.jp/search/(?P<id>[^?#&]+)?'\n+    _TESTS = [{\n+        'url': 'http://www.nicovideo.jp/search/sm9',\n+        'info_dict': {\n+            'id': 'sm9',\n+            'title': 'sm9'\n+        },\n+        'playlist_mincount': 40,\n+    }, {\n+        'url': 'https://www.nicovideo.jp/search/sm9?sort=h&order=d&end=2020-12-31&start=2020-01-01',\n+        'info_dict': {\n+            'id': 'sm9',\n+            'title': 'sm9'\n+        },\n+        'playlist_count': 31,\n+    }]\n+\n+    def _real_extract(self, url):\n+        query = self._match_id(url)\n+        return self.playlist_result(self._entries(url, query), query, query)\n+\n+\n+class NicovideoSearchDateIE(NicovideoSearchBaseIE, SearchInfoExtractor):\n+    IE_DESC = 'Nico video search, newest first'\n+    IE_NAME = '%s:date' % NicovideoSearchIE.IE_NAME\n+    _SEARCH_KEY = 'nicosearchdate'\n+\n+    _TESTS = [{\n+        'url': 'nicosearchdateall:a',\n+        'info_dict': {\n+            'id': 'a',\n+            'title': 'a'\n+        },\n+        'playlist_mincount': 1610,\n+    }]\n+\n+    _START_DATE = datetime.date(2007, 1, 1)\n+    _RESULTS_PER_PAGE = 32\n+    _MAX_PAGES = 50\n+\n+    def _entries(self, url, item_id, start_date=None, end_date=None):\n+        start_date, end_date = start_date or self._START_DATE, end_date or datetime.datetime.now().date()\n+\n+        # If the last page has a full page of videos, we need to break down the query interval further\n+        last_page_len = len(list(self._get_entries_for_date(\n+            url, item_id, start_date, end_date, self._MAX_PAGES,\n+            note='Checking number of videos from {0} to {1}'.format(start_date, end_date))))\n+        if (last_page_len == self._RESULTS_PER_PAGE and start_date != end_date):\n+            midpoint = start_date + ((end_date - start_date) // 2)\n+            for entry in itertools.chain(\n+                    iter(self._entries(url, item_id, midpoint, end_date)),\n+                    iter(self._entries(url, item_id, start_date, midpoint))):\n+                yield entry\n+        else:\n+            self.to_screen('{0}: Downloading results from {1} to {2}'.format(item_id, start_date, end_date))\n+            for entry in iter(self._get_entries_for_date(\n+                    url, item_id, start_date, end_date, note='    Downloading page %(page)s')):\n+                yield entry\n+\n+    def _get_entries_for_date(self, url, item_id, start_date, end_date=None, page_num=None, note=None):\n+        query = {\n+            'start': compat_str(start_date),\n+            'end': compat_str(end_date or start_date),\n+            'sort': 'f',\n+            'order': 'd',\n+        }\n+        if page_num:\n+            query['page'] = compat_str(page_num)\n+\n+        for entry in iter(super(NicovideoSearchDateIE, self)._entries(url, item_id, query=query, note=note)):\n+            yield entry\n+\n+\n+class NiconicoUserIE(InfoExtractor):\n+    _VALID_URL = r'https?://(?:www\\.)?nicovideo\\.jp/user/(?P<id>\\d+)/?(?:$|[#?])'\n+    _TEST = {\n+        'url': 'https://www.nicovideo.jp/user/419948',\n+        'info_dict': {\n+            'id': '419948',\n+        },\n+        'playlist_mincount': 101,\n+    }\n+    _API_URL = \"https://nvapi.nicovideo.jp/v1/users/%s/videos?sortKey=registeredAt&sortOrder=desc&pageSize=%s&page=%s\"\n+    _PAGE_SIZE = 100\n+\n+    _API_HEADERS = {\n+        'X-Frontend-ID': '6',\n+        'X-Frontend-Version': '0'\n+    }\n+\n+    def _entries(self, list_id):\n+        total_count = 1\n+        count = page_num = 0\n+        while count < total_count:\n+            json_parsed = self._download_json(\n+                self._API_URL % (list_id, self._PAGE_SIZE, page_num + 1), list_id,\n+                headers=self._API_HEADERS,\n+                note='Downloading JSON metadata%s' % (' page %d' % page_num if page_num else ''))\n+            if not page_num:\n+                total_count = int_or_none(json_parsed['data'].get('totalCount'))\n+            for entry in json_parsed[\"data\"][\"items\"]:\n+                count += 1\n+                yield self.url_result('https://www.nicovideo.jp/watch/%s' % entry['id'])\n+            page_num += 1\n \n     def _real_extract(self, url):\n         list_id = self._match_id(url)\n-        mylist = self._call_api(list_id, 'list', {\n-            'pageSize': 1,\n-        })\n-        entries = InAdvancePagedList(\n-            functools.partial(self._fetch_page, list_id),\n-            math.ceil(mylist['totalItemCount'] / self._PAGE_SIZE),\n-            self._PAGE_SIZE)\n-        result = self.playlist_result(\n-            entries, list_id, mylist.get('name'), mylist.get('description'))\n-        result.update(self._parse_owner(mylist))\n-        return result\n+        return self.playlist_result(self._entries(list_id), list_id)\n", "test_patch": "diff --git a/test/parameters.json b/test/parameters.json\nindex 65fd5442860..864c9d13044 100644\n--- a/test/parameters.json\n+++ b/test/parameters.json\n@@ -18,7 +18,6 @@\n     \"noprogress\": false, \n     \"outtmpl\": \"%(id)s.%(ext)s\", \n     \"password\": null, \n-    \"playlistend\": -1, \n     \"playliststart\": 1, \n     \"prefer_free_formats\": false, \n     \"quiet\": false, \ndiff --git a/test/test_download.py b/test/test_download.py\nindex ebe820dfc19..8e43cfa1270 100644\n--- a/test/test_download.py\n+++ b/test/test_download.py\n@@ -121,6 +121,7 @@ def print_skipping(reason):\n         params['outtmpl'] = tname + '_' + params['outtmpl']\n         if is_playlist and 'playlist' not in test_case:\n             params.setdefault('extract_flat', 'in_playlist')\n+            params.setdefault('playlistend', test_case.get('playlist_mincount'))\n             params.setdefault('skip_download', True)\n \n         ydl = YoutubeDL(params, auto_init=False)\n", "problem_statement": "[niconico] Support encrypted official videos\n<!--\r\n\r\n######################################################################\r\n  WARNING!\r\n  IGNORING THE FOLLOWING TEMPLATE WILL RESULT IN ISSUE CLOSED AS INCOMPLETE\r\n######################################################################\r\n\r\n-->\r\n\r\n\r\n## Checklist\r\n\r\n<!--\r\nCarefully read and work through this check list in order to prevent the most common mistakes and misuse of youtube-dl:\r\n- First of, make sure you are using the latest version of youtube-dl. Run `youtube-dl --version` and ensure your version is 2020.02.16. If it's not, see https://yt-dl.org/update on how to update. Issues with outdated version will be REJECTED.\r\n- Search the bugtracker for similar site feature requests: http://yt-dl.org/search-issues. DO NOT post duplicates.\r\n- Finally, put x into all relevant boxes (like this [x])\r\n-->\r\n\r\n- [x] I'm reporting a site feature request\r\n- [x] I've verified that I'm running youtube-dl version **2020.02.16**\r\n- [x] I've searched the bugtracker for similar site feature requests including closed ones\r\n\r\n\r\n## Description\r\n\r\n<!--\r\nProvide an explanation of your site feature request in an arbitrary form. Please make sure the description is worded well enough to be understood, see https://github.com/ytdl-org/youtube-dl#is-the-description-of-the-issue-itself-sufficient. Provide any additional information, suggested solution and as much context and examples as possible.\r\n-->\r\n\r\nI did a pull requests (https://github.com/ytdl-org/youtube-dl/pull/23824) before, but I'll post it to the issue just in case.\r\n\r\nniconico (Nico Nico Douga) is not known outside of Japan, but is the largest video sharing site in Japan.\r\n\r\n### Problem\r\n\r\nniconico began using HLS to encrypt the distribution of some official videos, such as anime, on March 5, 2019.\r\nAs a result, some official videos can no longer be downloaded with the previously used Extractor.\r\n(Example of video that can be downloaded as before: https://www.nicovideo.jp/watch/sm32868846)\r\n(Example of video that cannot be downloaded due to HLS encryption: https://www.nicovideo.jp/watch/so36189921)\r\n\r\n### Correction and resolution\r\n\r\nThe pull requests I mentioned earlier include an improvement that allows you to download niconico's encrypted HLS videos.\r\nSpecifically, it has been modified to make a request to the (most recently published) Ping API (otherwise the download will fail).\r\nIn addition, the encrypted HLS video has been modified to send different parameters to the API for acquiring the viewing session than the HTTP video.\r\n\r\nOn niconico, if you do not send a \"heartbeat\" every 60 seconds, the download session will be closed and a 403 error will occur.\r\nShort videos can complete the download before the session closes, but long videos do not.\r\nYou can \"resume\" videos that support HTTP, but you can't resume videos that are HLS only.\r\nTherefore, it is necessary to implement a \"heartbeat\".\r\n\r\nThe pull requests also include a change that allows you to send a \"heartbeat\" once every 60 seconds during the download, and to download long videos to the end.\r\nI understand that although some pull requests implementing \"heartbeat\" have been sent in the past, such as https://github.com/ytdl-org/youtube-dl/issues/14582, they have not been merged even now doing.\r\nThe code for \"heartbeat\" is based on https://github.com/archiif/youtube-dl/pull/1/commits/84995347fccc12f588efe59540418b9070a3c177.\r\nI tried it on various video sites, but adding this feature will not hinder other sites from downloading.\r\nAlso, since February 2018, it has been improved to play videos without logging in, so you do not need an account to download.\r\n\r\n### lastly\r\n\r\nAs a side change, we made changes to get high quality thumbnails and original titles.\r\nIn addition, support for \"heartbeat\" has made it easier to download normal niconico HTTP videos (especially long videos) as a side effect, eliminating the need to \"resume\".\r\n\r\nI strongly hope that this change will be incorporated into the project.\r\nCan you merge this (https://github.com/ytdl-org/youtube-dl/pull/23824) Pull requests?\r\n\r\nThank you.\r\n\r\n(Sentence may be a bit strange because I am using translation)\n", "hints_text": "Would like to see this merged as well. Perhaps worth mentioning that archiif@8499534 is actually also a PR here as #18230\n> Would like to see this merged as well. Perhaps worth mentioning that [archiif/youtube-dl@8499534](https://github.com/archiif/youtube-dl/commit/8499534) is actually also a PR here as #18230\r\n\r\nIt is OK to Merge # 18230 before my pull request. However, it has been left for over a year, I don't think it will be merged into the project in the future ...\r\nI thought # 18230 would never be merged anymore, so I incorporated the content of the # 18230 pull request.\r\nI'm not very used to pull requests, but if I get in the way when merging, should I close this pull request once?", "created_at": "2021-12-07T01:53:28Z"}
{"repo": "ytdl-org/youtube-dl", "pull_number": 29698, "instance_id": "ytdl-org__youtube-dl-29698", "issue_numbers": ["29690"], "base_commit": "af9e72507ea38e5ab3fa2751ed09ec88021260cb", "patch": "diff --git a/youtube_dl/YoutubeDL.py b/youtube_dl/YoutubeDL.py\nindex fe30758ef9c..69736acffa6 100755\n--- a/youtube_dl/YoutubeDL.py\n+++ b/youtube_dl/YoutubeDL.py\n@@ -1529,7 +1529,7 @@ def sanitize_numeric_fields(info):\n                 # see http://bugs.python.org/issue1646728)\n                 try:\n                     upload_date = datetime.datetime.utcfromtimestamp(info_dict[ts_key])\n-                    info_dict[date_key] = upload_date.strftime('%Y%m%d')\n+                    info_dict[date_key] = compat_str(upload_date.strftime('%Y%m%d'))\n                 except (ValueError, OverflowError, OSError):\n                     pass\n \ndiff --git a/youtube_dl/extractor/vimeo.py b/youtube_dl/extractor/vimeo.py\nindex 0b386f450b7..a66912502e8 100644\n--- a/youtube_dl/extractor/vimeo.py\n+++ b/youtube_dl/extractor/vimeo.py\n@@ -271,7 +271,7 @@ class VimeoIE(VimeoBaseInfoExtractor):\n                         )?\n                         vimeo(?:pro)?\\.com/\n                         (?!(?:channels|album|showcase)/[^/?#]+/?(?:$|[?#])|[^/]+/review/|ondemand/)\n-                        (?:.*?/)?\n+                        (?:.*?/)??\n                         (?:\n                             (?:\n                                 play_redirect_hls|\n@@ -517,14 +517,28 @@ class VimeoIE(VimeoBaseInfoExtractor):\n             'url': 'https://vimeo.com/7809605',\n             'only_matching': True,\n         },\n-        {\n-            'url': 'https://vimeo.com/160743502/abd0e13fb4',\n-            'only_matching': True,\n-        },\n         {\n             # requires passing unlisted_hash(a52724358e) to load_download_config request\n             'url': 'https://vimeo.com/392479337/a52724358e',\n             'only_matching': True,\n+        },\n+        {\n+            # similar, but all numeric: ID must be 581039021, not 9603038895\n+            # issue #29690\n+            'url': 'https://vimeo.com/581039021/9603038895',\n+            'info_dict': {\n+                'id': '581039021',\n+                # these have to be provided but we don't care\n+                'ext': 'mp4',\n+                'timestamp': 1627621014,\n+                'title': 're:.+',\n+                'uploader_id': 're:.+',\n+                'uploader': 're:.+',\n+                'upload_date': r're:\\d+',\n+            },\n+            'params': {\n+                'skip_download': True,\n+            },\n         }\n         # https://gettingthingsdone.com/workflowmap/\n         # vimeo embed with check-password page protected by Referer header\n", "test_patch": "diff --git a/test/test_YoutubeDL.py b/test/test_YoutubeDL.py\nindex a35effe0e4a..f8c8e619cf9 100644\n--- a/test/test_YoutubeDL.py\n+++ b/test/test_YoutubeDL.py\n@@ -997,6 +997,25 @@ def _real_extract(self, url):\n         self.assertEqual(downloaded['extractor'], 'Video')\n         self.assertEqual(downloaded['extractor_key'], 'Video')\n \n+    def test_default_times(self):\n+        \"\"\"Test addition of missing upload/release/_date from /release_/timestamp\"\"\"\n+        info = {\n+            'id': '1234',\n+            'url': TEST_URL,\n+            'title': 'Title',\n+            'ext': 'mp4',\n+            'timestamp': 1631352900,\n+            'release_timestamp': 1632995931,\n+        }\n+\n+        params = {'simulate': True, }\n+        ydl = FakeYDL(params)\n+        out_info = ydl.process_ie_result(info)\n+        self.assertTrue(isinstance(out_info['upload_date'], compat_str))\n+        self.assertEqual(out_info['upload_date'], '20210911')\n+        self.assertTrue(isinstance(out_info['release_date'], compat_str))\n+        self.assertEqual(out_info['release_date'], '20210930')\n+\n \n if __name__ == '__main__':\n     unittest.main()\n", "problem_statement": "[Vimeo] ERROR: Unable to download JSON metadata: HTTP Error 404\nsince the tool asked me to report im putting this here and hope its as intended \r\ni just put in the output with the error \r\ni used the latest version and also updated ffmpeg to check if it has something to do with it\r\n\r\n-------------------\r\n\r\nyoutube-dl -f bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best https://vimeo.com/581039021/9603038895 -o B:\\yt/%(title)s.%(ext)s--verbose --merge-output-format mp4\r\n[debug] System config: []\r\n[debug] User config: []\r\n[debug] Custom config: []\r\n[debug] Command-line args: ['-f', 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best', 'https://vimeo.com/581039021/9603038895', '-o', 'B:\\\\yt/%(title)s.%(ext)s', '--verbose', '--merge-output-format', 'mp4']\r\n[debug] Encodings: locale cp1252, fs mbcs, out cp850, pref cp1252\r\n[debug] youtube-dl version 2021.06.06\r\n[debug] Python version 3.4.4 (CPython) - Windows-7-6.1.7601-SP1\r\n[debug] exe versions: ffmpeg 4.2.2\r\n[debug] Proxy map: {}\r\n[vimeo] 9603038895: Downloading webpage\r\n[vimeo] 9603038895: Downloading JSON metadata\r\n[vimeo] 9603038895: Downloading JSON metadata\r\nERROR: Unable to download JSON metadata: HTTP Error 404: Not Found (caused by HTTPError()); please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.\r\n  File \"C:\\Users\\dst\\AppData\\Roaming\\Build archive\\youtube-dl\\ytdl-org\\tmpkqxnwl31\\build\\youtube_dl\\extractor\\common.py\", line 634, in _request_webpage\r\n  File \"C:\\Users\\dst\\AppData\\Roaming\\Build archive\\youtube-dl\\ytdl-org\\tmpkqxnwl31\\build\\youtube_dl\\YoutubeDL.py\", line 2288, in urlopen\r\n  File \"C:\\Python\\Python34\\lib\\urllib\\request.py\", line 470, in open\r\n  File \"C:\\Python\\Python34\\lib\\urllib\\request.py\", line 580, in http_response\r\n  File \"C:\\Python\\Python34\\lib\\urllib\\request.py\", line 508, in error\r\n  File \"C:\\Python\\Python34\\lib\\urllib\\request.py\", line 442, in _call_chain\r\n  File \"C:\\Python\\Python34\\lib\\urllib\\request.py\", line 588, in http_error_default\r\n\r\n-------------------\r\n\r\nive checked with a older video to see if there is some vimeo related issue but the older url works perfectly -- gonna post the \"**working**\" example now\r\n\r\n------------------\r\n\r\n\r\nyoutube-dl -f bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best https://vimeo.com/580888053/a127b5ccd0 -o B:\\yt/%(title)s.%(ext)s--verbose --merge-output-format mp4\r\n[debug] System config: []\r\n[debug] User config: []\r\n[debug] Custom config: []\r\n[debug] Command-line args: ['-f', 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best', 'https://vimeo.com/580888053/a127b5ccd0', '-o', 'B:\\\\yt/%(title)s.%(ext)s', '--verbose', '--merge-output-format', 'mp4']\r\n[debug] Encodings: locale cp1252, fs mbcs, out cp850, pref cp1252\r\n[debug] youtube-dl version 2021.06.06\r\n[debug] Python version 3.4.4 (CPython) - Windows-7-6.1.7601-SP1\r\n[debug] exe versions: ffmpeg 2021-07-27-git-0068b3d0f0-full_build-www.gyan.dev\r\n[debug] Proxy map: {}\r\n[vimeo] 580888053: Downloading JSON metadata\r\n[vimeo] 580888053: Downloading JSON metadata\r\n[vimeo] 580888053: Downloading JSON metadata\r\n[vimeo] 580888053: Downloading fastly_skyfire m3u8 information\r\n[vimeo] 580888053: Downloading fastly_skyfire m3u8 information\r\n[vimeo] 580888053: Downloading akfire_interconnect_quic m3u8 information\r\n[vimeo] 580888053: Downloading akfire_interconnect_quic m3u8 information\r\n[vimeo] 580888053: Downloading fastly_skyfire MPD information\r\n[vimeo] 580888053: Downloading fastly_skyfire MPD information\r\n[vimeo] 580888053: Downloading akfire_interconnect_quic MPD information\r\n[vimeo] 580888053: Downloading akfire_interconnect_quic MPD information\r\n[debug] Invoking downloader on 'https://skyfire.vimeocdn.com/1627634832-0x16a4d5563ea72c9611c90b1709e96e32fa1843c6/7062d513-389f-4cc6-adb9-a9c2cf1a35bf/sep/video/b72d696c,360af58c,c69af52d,fa60c372,ebaf0f3a/master.mpd?base64_init=1'\r\n[dashsegments] Total fragments: 156\r\n[download] Destination: B:\\yt\\Thursday, July 29, 2021 BMA Mid-Day Video Update.fdash-fastly_skyfire_sep-video-fa60c372.mp4\r\n[download] 100% of 118.54MiB in 00:11\r\n[debug] Invoking downloader on 'https://skyfire.vimeocdn.com/1627634832-0x16a4d5563ea72c9611c90b1709e96e32fa1843c6/7062d513-389f-4cc6-adb9-a9c2cf1a35bf/sep/video/b72d696c,360af58c,c69af52d,fa60c372,ebaf0f3a/master.mpd?base64_init=1'\r\n[dashsegments] Total fragments: 156\r\n[download] Destination: B:\\yt\\Thursday, July 29, 2021 BMA Mid-Day Video Update.fdash-fastly_skyfire_sep-audio-b72d696c.m4a\r\n[download] 100% of 28.62MiB in 00:07\r\n[ffmpeg] Merging formats into \"B:\\yt\\Thursday, July 29, 2021 BMA Mid-Day Video Update.mp4\"\r\n[debug] ffmpeg command line: ffmpeg -y -loglevel \"repeat+info\" -i \"file:B:\\yt\\Thursday, July 29, 2021 BMA Mid-Day Video Update.fdash-fastly_skyfire_sep-video-fa60c372.mp4\" -i \"file:B:\\yt\\Thursday, July 29, 2021 BMA Mid-Day Video Update.fdash-fastly_skyfire_sep-audio-b72d696c.m4a\" -c copy -map \"0:v:0\" -map \"1:a:0\" \"file:B:\\yt\\Thursday, July 29, 2021 BMA Mid-Day Video Update.temp.mp4\"\r\nDeleting original file B:\\yt\\Thursday, July 29, 2021 BMA Mid-Day Video Update.fdash-fastly_skyfire_sep-video-fa60c372.mp4 (pass -k to keep)\r\nDeleting original file B:\\yt\\Thursday, July 29, 2021 BMA Mid-Day Video Update.fdash-fastly_skyfire_sep-audio-b72d696c.m4a (pass -k to keep)\r\n\r\n------------------\r\n\r\nthe working video was uploaded a few hours befor the \"not working\" one\r\nworking: https://vimeo.com/580888053/a127b5ccd0 (upload around 12 hours agoaround 3 PM CST Thursday July 29 2021)\r\nnot working: https://vimeo.com/581039021/9603038895 (upload around 3 hours ago around 11 PM CST Thursday July 29 2021)\r\n\r\nin case more information are required let me know im hoping ive done everything as wanted \r\ndayta\r\n\r\nside note: im using win7 thus using the latest version of phthon that was available and working tryed different ones without any change in result.. the newest version of python does not work on win 7 but that shouldnt be part of the problem itself since the error only occours with this one url out of alot of others which are still working perfectly\n", "hints_text": "The problem comes when fetching the video details.\r\n\r\nThe working video is correctly identified as having the 'ID/hash' (`580888053/a127b5ccd0`) pattern and the extractor goes on to fetch the video details using `api.vimeo.com` without loading the original page itself. From the debug log we can see that it is being handled with ID 580888053. The similar URL from the extractor's tests also works.\r\n\r\nThe failing video is identified with ID 9603038895 instead of 581039021 and unsurprisingly this means that the extractor fails to find the video details.\r\n\r\nThe problem occurs when a Vimeo URL ends with '/ID/hash' and both ID and hash are numeric. The pattern match skips over the ID part and assigns the ID from the hash part. One component of the URL pattern needs to be constrained (non-greedy `(?:.*?/)??` instead of `(?:.*?/)?`), and then we get:\r\n```\r\n# youtube-dl -F -v 'https://vimeo.com/581039021/9603038895'\r\n[debug] System config: [u'--restrict-filenames', u'--prefer-ffmpeg', u'-f', u'best[height<=?1080][fps<=?60]', u'-o', u'/media/drive1/Video/%(title)s.%(ext)s']\r\n[debug] User config: [u'-f', u'(best/bestvideo+bestaudio)[height<=?1080][fps<=?60][tbr<=?1900]']\r\n[debug] Custom config: []\r\n[debug] Command-line args: [u'-F', u'-v', u'https://vimeo.com/581039021/9603038895']\r\n[debug] Encodings: locale ASCII, fs ASCII, out ASCII, pref ASCII\r\n[debug] youtube-dl version 2021.06.06.1\r\n[debug] Python version 2.7.1 (CPython) - Linux-2.6.18-7.1-7405b0-smp-with-libc0\r\n[debug] exe versions: ffmpeg 4.1, ffprobe 4.1\r\n[debug] Proxy map: {}\r\n[vimeo] 581039021: Downloading JSON metadata\r\n[vimeo] 581039021: Downloading JSON metadata\r\n[vimeo] 581039021: Downloading JSON metadata\r\n[vimeo] 581039021: Downloading akfire_interconnect_quic m3u8 information\r\n[vimeo] 581039021: Downloading akfire_interconnect_quic m3u8 information\r\n[vimeo] 581039021: Downloading fastly_skyfire m3u8 information\r\n[vimeo] 581039021: Downloading fastly_skyfire m3u8 information\r\n[vimeo] 581039021: Downloading akfire_interconnect_quic MPD information\r\n[vimeo] 581039021: Downloading akfire_interconnect_quic MPD information\r\n[vimeo] 581039021: Downloading fastly_skyfire MPD information\r\n[vimeo] 581039021: Downloading fastly_skyfire MPD information\r\n[info] Available formats for 581039021:\r\nformat code                                                                                        extension  resolution note\r\nhls-fastly_skyfire-1501                                                                            mp4        1920x1080  1501k , avc1.640028, 30.0fps, mp4a.40.2\r\nhls-akfire_interconnect_quic-1501                                                                  mp4        1920x1080  1501k , avc1.640028, 30.0fps, mp4a.40.2\r\nhttp-1080p                                                                                         mp4        1920x1080  30fps\r\nhls-fastly_skyfire-916                                                                             mp4        1280x720    916k , avc1.640020, 30.0fps, mp4a.40.2\r\nhls-akfire_interconnect_quic-916                                                                   mp4        1280x720    916k , avc1.640020, 30.0fps, mp4a.40.2\r\nhttp-720p                                                                                          mp4        1280x720   30fps\r\ndash-fastly_skyfire-video-7248713b                                                                 mp4        960x540    DASH video 1115k , mp4_dash container, avc1.64001F, 30fps, mp4a.40.2 (48000Hz)\r\ndash-akfire_interconnect_quic-video-7248713b                                                       mp4        960x540    DASH video 1115k , mp4_dash container, avc1.64001F, 30fps, mp4a.40.2 (48000Hz)\r\nhls-fastly_skyfire-661                                                                             mp4        960x540     661k , avc1.64001F, 30.0fps, mp4a.40.2\r\nhls-akfire_interconnect_quic-661                                                                   mp4        960x540     661k , avc1.64001F, 30.0fps, mp4a.40.2\r\nhttp-540p                                                                                          mp4        960x540    30fps\r\ndash-fastly_skyfire-video-ded7a2c1                                                                 mp4        640x360    DASH video  440k , mp4_dash container, avc1.64001E, 30fps, mp4a.40.2 (48000Hz)\r\ndash-akfire_interconnect_quic-video-ded7a2c1                                                       mp4        640x360    DASH video  440k , mp4_dash container, avc1.64001E, 30fps, mp4a.40.2 (48000Hz)\r\nhls-fastly_skyfire-337                                                                             mp4        640x360     337k , avc1.64001E, 30.0fps, mp4a.40.2\r\nhls-akfire_interconnect_quic-337                                                                   mp4        640x360     337k , avc1.64001E, 30.0fps, mp4a.40.2\r\nhttp-360p                                                                                          mp4        640x360    30fps\r\nhttp-240p                                                                                          mp4        426x240    30fps\r\nhls-fastly_skyfire_sep-1501+dash-fastly_skyfire_sep-audio-7248713b                                 mp4        1920x1080  avc1.640028, 30.0fps, mp4a.40.2\r\nhls-akfire_interconnect_quic_sep-1501+dash-akfire_interconnect_quic_sep-audio-7248713b             mp4        1920x1080  avc1.640028, 30.0fps, mp4a.40.2\r\ndash-fastly_skyfire_sep-video-36b397cb+dash-fastly_skyfire_sep-audio-ded7a2c1                      mp4        1280x720   avc1.640020, 30fps, mp4a.40.2\r\ndash-akfire_interconnect_quic_sep-video-36b397cb+dash-akfire_interconnect_quic_sep-audio-ded7a2c1  mp4        1280x720   avc1.640020, 30fps, mp4a.40.2\r\nhls-fastly_skyfire_sep-916+dash-fastly_skyfire_sep-audio-06e865e1                                  mp4        1280x720   avc1.640020, 30.0fps, opus \r\nhls-akfire_interconnect_quic_sep-916+dash-akfire_interconnect_quic_sep-audio-06e865e1              mp4        1280x720   avc1.640020, 30.0fps, opus \r\ndash-fastly_skyfire_sep-video-7248713b+dash-fastly_skyfire_sep-audio-8f1b4276                      mp4        960x540    avc1.64001F, 30fps, opus \r\ndash-akfire_interconnect_quic_sep-video-7248713b+dash-akfire_interconnect_quic_sep-audio-8f1b4276  mp4        960x540    avc1.64001F, 30fps, opus \r\nhls-fastly_skyfire_sep-533+hls-fastly_skyfire_sep-audio-medium-audio                               mp4        960x540    avc1.64001F, 30.0fps\r\nhls-akfire_interconnect_quic_sep-533+hls-fastly_skyfire_sep-audio-high-audio                       mp4        960x540    avc1.64001F, 30.0fps\r\nhls-fastly_skyfire_sep-336+hls-akfire_interconnect_quic_sep-audio-medium-audio                     mp4        640x360    avc1.64001E, 30.0fps\r\nhls-akfire_interconnect_quic_sep-336+hls-akfire_interconnect_quic_sep-audio-high-audio             mp4        640x360    avc1.64001E, 30.0fps (best)\r\n#\r\n``` \r\n", "created_at": "2021-07-31T12:24:13Z"}
{"repo": "ytdl-org/youtube-dl", "pull_number": 28801, "instance_id": "ytdl-org__youtube-dl-28801", "issue_numbers": ["26211"], "base_commit": "40bd5c18153afe765caa6726302ee1dd8a9a2ce6", "patch": "diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py\nindex 61b94d84c44..c249e71681d 100644\n--- a/youtube_dl/utils.py\n+++ b/youtube_dl/utils.py\n@@ -2182,8 +2182,28 @@ def sanitize_url(url):\n     return url\n \n \n+def extract_basic_auth(url):\n+    parts = compat_urllib_parse.urlsplit(url)\n+    if parts.username is None:\n+        return url, None\n+    url = compat_urllib_parse.urlunsplit(parts._replace(netloc=(\n+        parts.hostname if parts.port is None\n+        else '%s:%d' % (parts.hostname, parts.port))))\n+    auth_payload = base64.b64encode(\n+        ('%s:%s' % (parts.username, parts.password or '')).encode('utf-8'))\n+    return url, 'Basic {0}'.format(auth_payload.decode('ascii'))\n+\n+\n def sanitized_Request(url, *args, **kwargs):\n-    return compat_urllib_request.Request(escape_url(sanitize_url(url)), *args, **kwargs)\n+    url, auth_header = extract_basic_auth(escape_url(sanitize_url(url)))\n+    if auth_header is not None:\n+        headers = args[1] if len(args) > 1 else kwargs.get('headers')\n+        headers = headers or {}\n+        headers['Authorization'] = auth_header\n+        if len(args) <= 1 and kwargs.get('headers') is None:\n+            kwargs['headers'] = headers\n+            kwargs = compat_kwargs(kwargs)\n+    return compat_urllib_request.Request(url, *args, **kwargs)\n \n \n def expand_path(s):\n", "test_patch": "diff --git a/test/test_utils.py b/test/test_utils.py\nindex 102420fcb88..90d64b5811e 100644\n--- a/test/test_utils.py\n+++ b/test/test_utils.py\n@@ -81,6 +81,7 @@\n     sanitize_filename,\n     sanitize_path,\n     sanitize_url,\n+    sanitized_Request,\n     shell_quote,\n     smuggle_url,\n     str_or_none,\n@@ -255,6 +256,18 @@ def test_sanitize_url(self):\n         self.assertEqual(sanitize_url('https://foo.bar'), 'https://foo.bar')\n         self.assertEqual(sanitize_url('foo bar'), 'foo bar')\n \n+    def test_sanitized_Request(self):\n+        self.assertFalse(sanitized_Request('http://foo.bar').has_header('Authorization'))\n+        self.assertFalse(sanitized_Request('http://:foo.bar').has_header('Authorization'))\n+        self.assertEqual(sanitized_Request('http://@foo.bar').get_header('Authorization'),\n+                         'Basic Og==')\n+        self.assertEqual(sanitized_Request('http://:pass@foo.bar').get_header('Authorization'),\n+                         'Basic OnBhc3M=')\n+        self.assertEqual(sanitized_Request('http://user:@foo.bar').get_header('Authorization'),\n+                         'Basic dXNlcjo=')\n+        self.assertEqual(sanitized_Request('http://user:pass@foo.bar').get_header('Authorization'),\n+                         'Basic dXNlcjpwYXNz')\n+\n     def test_expand_path(self):\n         def env(var):\n             return '%{0}%'.format(var) if sys.platform == 'win32' else '${0}'.format(var)\n", "problem_statement": "error when entering username and password\n<!--\r\n\r\n######################################################################\r\n  WARNING!\r\n  IGNORING THE FOLLOWING TEMPLATE WILL RESULT IN ISSUE CLOSED AS INCOMPLETE\r\n######################################################################\r\n\r\n-->\r\n\r\n\r\n## Checklist\r\n\r\n<!--\r\nCarefully read and work through this check list in order to prevent the most common mistakes and misuse of youtube-dl:\r\n- First of, make sure you are using the latest version of youtube-dl. Run `youtube-dl --version` and ensure your version is 2020.07.28. If it's not, see https://yt-dl.org/update on how to update. Issues with outdated version will be REJECTED.\r\n- Make sure that all provided video/audio/playlist URLs (if any) are alive and playable in a browser.\r\n- Make sure that all URLs and arguments with special characters are properly quoted or escaped as explained in http://yt-dl.org/escape.\r\n- Search the bugtracker for similar issues: http://yt-dl.org/search-issues. DO NOT post duplicates.\r\n- Read bugs section in FAQ: http://yt-dl.org/reporting\r\n- Finally, put x into all relevant boxes (like this [x])\r\n-->\r\n\r\n- [x] I'm reporting a broken site support issue\r\n- [x ] I've verified that I'm running youtube-dl version **2020.07.28**\r\n- [x ] I've checked that all provided URLs are alive and playable in a browser\r\n- [ x] I've checked that all URLs and arguments with special characters are properly quoted or escaped\r\n- [x ] I've searched the bugtracker for similar bug reports including closed ones\r\n- [x ] I've read bugs section in FAQ\r\n\r\n\r\n## Verbose log\r\n\r\n<!--\r\nProvide the complete verbose output of youtube-dl that clearly demonstrates the problem.\r\nAdd the `-v` flag to your command line you run youtube-dl with (`youtube-dl -v <your command line>`), copy the WHOLE output and insert it below. It should look similar to this:\r\n [debug] System config: []\r\n [debug] User config: []\r\n [debug] Command-line args: [u'-v', u'http://www.youtube.com/watch?v=BaW_jenozKcj']\r\n [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251\r\n [debug] youtube-dl version 2020.07.28\r\n [debug] Python version 2.7.11 - Windows-2003Server-5.2.3790-SP2\r\n [debug] exe versions: ffmpeg N-75573-g1d0487f, ffprobe N-75573-g1d0487f, rtmpdump 2.4\r\n [debug] Proxy map: {}\r\n <more lines>\r\n-->\r\n\r\n```\r\nPASTE VERBOSE LOG HERE\r\n```\r\n[tim@cabra ~]$ youtube-dl --version\r\n2020.07.28\r\n[tim@cabra ~]$ youtube-dl -o '%(title)s.%(ext)s'   'http://mooo.peelwiki.com/dl/BillfromNorthWales/01%20John%20Peel/1978-12-26%20John%20Peel%20BBC%20Radio%201.mp3' --username 'peel' --password 'group' --verbose\r\n[debug] System config: []\r\n[debug] User config: []\r\n[debug] Custom config: []\r\n[debug] Command-line args: [u'-o', u'%(title)s.%(ext)s', u'http://mooo.peelwiki.com/dl/BillfromNorthWales/01%20John%20Peel/1978-12-26%20John%20Peel%20BBC%20Radio%201.mp3', u'--username', u'PRIVATE', u'--password', u'PRIVATE', u'--verbose']\r\n[debug] Encodings: locale UTF-8, fs UTF-8, out UTF-8, pref UTF-8\r\n[debug] youtube-dl version 2020.07.28\r\n[debug] Python version 2.7.17 (CPython) - Linux-5.3.11-100.fc29.x86_64-x86_64-with-fedora-29-Twenty_Nine\r\n[debug] exe versions: ffmpeg 4.0.5, ffprobe 4.0.5\r\n[debug] Proxy map: {}\r\n[generic] 1978-12-26 John Peel BBC Radio 1: Requesting header\r\nWARNING: Could not send HEAD request to http://mooo.peelwiki.com/dl/BillfromNorthWales/01%20John%20Peel/1978-12-26%20John%20Peel%20BBC%20Radio%201.mp3: HTTP Error 401: Authorization Required\r\n[generic] 1978-12-26 John Peel BBC Radio 1: Downloading webpage\r\nERROR: Unable to download webpage: HTTP Error 401: Authorization Required (caused by HTTPError()); please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.\r\n  File \"/usr/local/bin/youtube-dl/youtube_dl/extractor/common.py\", line 627, in _request_webpage\r\n    return self._downloader.urlopen(url_or_request)\r\n  File \"/usr/local/bin/youtube-dl/youtube_dl/YoutubeDL.py\", line 2238, in urlopen\r\n    return self._opener.open(req, timeout=self._socket_timeout)\r\n  File \"/usr/lib64/python2.7/urllib2.py\", line 435, in open\r\n    response = meth(req, response)\r\n  File \"/usr/lib64/python2.7/urllib2.py\", line 548, in http_response\r\n    'http', request, response, code, msg, hdrs)\r\n  File \"/usr/lib64/python2.7/urllib2.py\", line 473, in error\r\n    return self._call_chain(*args)\r\n  File \"/usr/lib64/python2.7/urllib2.py\", line 407, in _call_chain\r\n    result = func(*args)\r\n  File \"/usr/lib64/python2.7/urllib2.py\", line 556, in http_error_default\r\n    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)\r\n\r\n[tim@cabra ~]$ \r\n\r\n\r\n\r\n\r\n\r\n## Description\r\n\r\n<!--\r\nProvide an explanation of your issue in an arbitrary form. Please make sure the description is worded well enough to be understood, see https://github.com/ytdl-org/youtube-dl#is-the-description-of-the-issue-itself-sufficient. Provide any additional information, suggested solution and as much context and examples as possible.\r\nIf work on your issue requires account credentials please provide them or explain how one can obtain them.\r\n-->\r\n\r\nWRITE DESCRIPTION HERE\r\n\r\n\r\nThis is the first time I have had to enter a username/password combo into youtube-dl. From the browser I enter this url followed by username/password and it opens up a media player within the browser. This is the next level url\r\n\r\nhttps://peel.fandom.com/wiki/1978\r\n\r\nsearch for 26 December 1978 and then head to the bottom of that page for the mooo links.\r\n\r\nI have tried a few different options with the same results - http and https. Username/password in single/double and no quotes with the same response\r\n\r\nRegards\n", "hints_text": "`--username` and `--password` only work for specific supported sites and not with the generic extractor.\r\n\r\nYou can download this file by adding the login credentials at the beginning of the URL and using `wget`, `curl`, your browser, etc\r\n\r\n``` bash\r\n$ wget 'http://peel:group@mooo.peelwiki.com/dl/BillfromNorthWales/01 John Peel/1978-12-26 John Peel BBC Radio 1.mp3'\r\n```\r\n\r\n`youtube-dl` also doesn't work with this kind of URL, by the way:\r\n``` bash\r\n$ youtube-dl -v 'http://peel:group@mooo.peelwiki.com/dl/BillfromNorthWales/01 John Peel/1978-12-26 John Peel BBC Radio 1.mp3'\r\n[debug] System config: []\r\n[debug] User config: []\r\n[debug] Custom config: []\r\n[debug] Command-line args: ['-v', 'http://peel:group@mooo.peelwiki.com/dl/BillfromNorthWales/01 John Peel/1978-12-26 John Peel BBC Radio 1.mp3']\r\n[debug] Encodings: locale UTF-8, fs utf-8, out utf-8, pref UTF-8\r\n[debug] youtube-dl version 2020.07.28\r\n[debug] Python version 3.8.5 (CPython) - Linux-5.4.50-1-lts-x86_64-with-glibc2.2.5\r\n[debug] exe versions: ffmpeg 4.2.3, ffprobe 4.2.3, rtmpdump 2.4\r\n[debug] Proxy map: {}\r\n[generic] 1978-12-26 John Peel BBC Radio 1: Requesting header\r\nWARNING: Could not send HEAD request to http://peel:group@mooo.peelwiki.com/dl/BillfromNorthWales/01 John Peel/1978-12-26 John Peel BBC Radio 1.mp3: nonnumeric port: 'group@mooo.peelwiki.com'\r\n[generic] 1978-12-26 John Peel BBC Radio 1: Downloading webpage\r\nERROR: Unable to download webpage: nonnumeric port: 'group@mooo.peelwiki.com' (caused by InvalidURL(\"nonnumeric port: 'group@mooo.peelwiki.com'\")); please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see  https://yt-dl.org/update  on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.\r\n  File \"/home/mike/.local/lib/python3.8/site-packages/youtube_dl/extractor/common.py\", line 627, in _request_webpage\r\n    return self._downloader.urlopen(url_or_request)\r\n  File \"/home/mike/.local/lib/python3.8/site-packages/youtube_dl/YoutubeDL.py\", line 2238, in urlopen\r\n    return self._opener.open(req, timeout=self._socket_timeout)\r\n  File \"/usr/lib/python3.8/urllib/request.py\", line 525, in open\r\n    response = self._open(req, data)\r\n  File \"/usr/lib/python3.8/urllib/request.py\", line 542, in _open\r\n    result = self._call_chain(self.handle_open, protocol, protocol +\r\n  File \"/usr/lib/python3.8/urllib/request.py\", line 502, in _call_chain\r\n    result = func(*args)\r\n  File \"/home/mike/.local/lib/python3.8/site-packages/youtube_dl/utils.py\", line 2578, in http_open\r\n    return self.do_open(functools.partial(\r\n  File \"/usr/lib/python3.8/urllib/request.py\", line 1319, in do_open\r\n    h = http_class(host, timeout=req.timeout, **http_conn_args)\r\n  File \"/home/mike/.local/lib/python3.8/site-packages/youtube_dl/utils.py\", line 2479, in _create_http_connection\r\n    hc = http_class(*args, **compat_kwargs(kwargs))\r\n  File \"/usr/lib/python3.8/http/client.py\", line 833, in __init__\r\n    (self.host, self.port) = self._get_hostport(host, port)\r\n  File \"/usr/lib/python3.8/http/client.py\", line 876, in _get_hostport\r\n    raise InvalidURL(\"nonnumeric port: '%s'\" % host[i+1:])\r\n```\nthanks for the educated response", "created_at": "2021-04-19T12:59:50Z"}
{"repo": "ytdl-org/youtube-dl", "pull_number": 23199, "instance_id": "ytdl-org__youtube-dl-23199", "issue_numbers": ["23197"], "base_commit": "0de9fd24dc8723c78a90cb546e4a05818304521e", "patch": "diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py\nindex aed988b884b..0d30075aa1d 100644\n--- a/youtube_dl/utils.py\n+++ b/youtube_dl/utils.py\n@@ -1718,13 +1718,16 @@ def random_user_agent():\n     '%B %d %Y',\n     '%B %dst %Y',\n     '%B %dnd %Y',\n+    '%B %drd %Y',\n     '%B %dth %Y',\n     '%b %d %Y',\n     '%b %dst %Y',\n     '%b %dnd %Y',\n+    '%b %drd %Y',\n     '%b %dth %Y',\n     '%b %dst %Y %I:%M',\n     '%b %dnd %Y %I:%M',\n+    '%b %drd %Y %I:%M',\n     '%b %dth %Y %I:%M',\n     '%Y %m %d',\n     '%Y-%m-%d',\n", "test_patch": "diff --git a/test/test_utils.py b/test/test_utils.py\nindex 3920542bb43..0db37d9d88e 100644\n--- a/test/test_utils.py\n+++ b/test/test_utils.py\n@@ -340,6 +340,8 @@ def test_unified_dates(self):\n         self.assertEqual(unified_strdate('July 15th, 2013'), '20130715')\n         self.assertEqual(unified_strdate('September 1st, 2013'), '20130901')\n         self.assertEqual(unified_strdate('Sep 2nd, 2013'), '20130902')\n+        self.assertEqual(unified_strdate('November 3rd, 2019'), '20191103')\n+        self.assertEqual(unified_strdate('October 23rd, 2005'), '20051023')\n \n     def test_unified_timestamps(self):\n         self.assertEqual(unified_timestamp('December 21, 2010'), 1292889600)\n", "problem_statement": "unified_strdate returns None on dates with \"3rd\" and \"23rd\"\n<!--\r\n\r\n######################################################################\r\n  WARNING!\r\n  IGNORING THE FOLLOWING TEMPLATE WILL RESULT IN ISSUE CLOSED AS INCOMPLETE\r\n######################################################################\r\n\r\n-->\r\n\r\n\r\n## Checklist\r\n\r\n<!--\r\nCarefully read and work through this check list in order to prevent the most common mistakes and misuse of youtube-dl:\r\n- First of, make sure you are using the latest version of youtube-dl. Run `youtube-dl --version` and ensure your version is 2019.11.22. If it's not, see https://yt-dl.org/update on how to update. Issues with outdated version will be REJECTED.\r\n- Make sure that all provided video/audio/playlist URLs (if any) are alive and playable in a browser.\r\n- Make sure that all URLs and arguments with special characters are properly quoted or escaped as explained in http://yt-dl.org/escape.\r\n- Search the bugtracker for similar issues: http://yt-dl.org/search-issues. DO NOT post duplicates.\r\n- Read bugs section in FAQ: http://yt-dl.org/reporting\r\n- Finally, put x into all relevant boxes (like this [x])\r\n-->\r\n\r\n- [ ] I'm reporting a broken site support issue\r\n- [x] I've verified that I'm running youtube-dl version **2019.11.22**\r\n- [x] I've checked that all provided URLs are alive and playable in a browser\r\n- [x] I've checked that all URLs and arguments with special characters are properly quoted or escaped\r\n- [x] I've searched the bugtracker for similar bug reports including closed ones\r\n- [x] I've read bugs section in FAQ\r\n\r\n\r\n## Verbose log\r\n\r\n<!--\r\nProvide the complete verbose output of youtube-dl that clearly demonstrates the problem.\r\nAdd the `-v` flag to your command line you run youtube-dl with (`youtube-dl -v <your command line>`), copy the WHOLE output and insert it below. It should look similar to this:\r\n [debug] System config: []\r\n [debug] User config: []\r\n [debug] Command-line args: [u'-v', u'http://www.youtube.com/watch?v=BaW_jenozKcj']\r\n [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251\r\n [debug] youtube-dl version 2019.11.22\r\n [debug] Python version 2.7.11 - Windows-2003Server-5.2.3790-SP2\r\n [debug] exe versions: ffmpeg N-75573-g1d0487f, ffprobe N-75573-g1d0487f, rtmpdump 2.4\r\n [debug] Proxy map: {}\r\n <more lines>\r\n-->\r\n\r\n[debug] System config: []\r\n[debug] User config: []\r\n[debug] Custom config: []\r\n[debug] Command-line args: ['https://www.bitchute.com/video/KDAtOH7nEUGe/', '--no-check-certificate', '--verbose']\r\n[debug] Encodings: locale UTF-8, fs utf-8, out UTF-8, pref UTF-8\r\n[debug] youtube-dl version 2019.11.22\r\n[debug] Git HEAD: 8267f2fa9\r\n[debug] Python version 3.7.4 (CPython) - Linux-5.3.8-gnu-x86_64-with-glibc2.2.5\r\n[debug] exe versions: ffmpeg 4.2.1, ffprobe 4.2.1\r\n[debug] Proxy map: {}\r\n[BitChute] KDAtOH7nEUGe: Downloading webpage\r\n[BitChute] KDAtOH7nEUGe: Checking video URL\r\n\r\n(Custom output snipped. See description for details.)\r\n\r\n\r\n\r\n\r\n## Description\r\n\r\n<!--\r\nProvide an explanation of your issue in an arbitrary form. Please make sure the description is worded well enough to be understood, see https://github.com/ytdl-org/youtube-dl#is-the-description-of-the-issue-itself-sufficient. Provide any additional information, suggested solution and as much context and examples as possible.\r\nIf work on your issue requires account credentials please provide them or explain how one can obtain them.\r\n-->\r\n\r\n\"unified_strdate\" from utils returns None instead of a date if the string sent contains \"3rd\" or \"23rd\".\r\n\r\nI initially discovered this issue when creating pull #23193. Other sites (such as youtube) don't seem to have prefixes such as \"*st\", \"*nd\", \"*rd\", etc, on the day of the month, and thus don't seem to run into this problem when getting dates.\r\n\r\nI hacked together a quick test patch that runs through all possible dates and displays them along with the converted version from unified_strdate. The full output and the test patch are attached. To run it, just apply the patch and tell youtube-dl to download any bitchute video (I used \"https://www.bitchute.com/video/KDAtOH7nEUGe/\" in this example, but any video will work).\r\n\r\n[date_test_patch.txt](https://github.com/ytdl-org/youtube-dl/files/3884233/date_test_patch.txt)\r\n[output.txt](https://github.com/ytdl-org/youtube-dl/files/3884234/output.txt)\r\n\r\nAs you can see from this snippet, all of the 3rd and 23rd days return \"None\" instead of a date. The full output also has \"None\" in days that do not exist (such as February 31st), since this was just a quick hack to check which dates worked and which didn't.\r\n\r\n```\r\nJanuary 1st, 2019:  20190101\r\nJanuary 2nd, 2019:  20190102\r\nJanuary 3rd, 2019:  None\r\nJanuary 4th, 2019:  20190104\r\n...\r\nJanuary 22nd, 2019:  20190122\r\nJanuary 23rd, 2019:  None\r\nJanuary 24th, 2019:  20190124\r\n...\r\nFebruary 2nd, 2019:  20190202\r\nFebruary 3rd, 2019:  None\r\nFebruary 4th, 2019:  20190204\r\n...\r\n```\n", "hints_text": "", "created_at": "2019-11-25T00:22:11Z"}
