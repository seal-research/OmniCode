{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data/codearena_instances.json\n",
    "with open('codearena_instances.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_tasks = [t for t in data if 'keras' in t['instance_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repo\n",
      "instance_id\n",
      "base_commit\n",
      "patch\n",
      "test_patch\n",
      "problem_statement\n",
      "hints_text\n",
      "created_at\n",
      "version\n",
      "FAIL_TO_PASS\n",
      "PASS_TO_PASS\n",
      "environment_setup_commit\n",
      "bad_patch\n",
      "bad_patch_author\n",
      "Review\n",
      "Review_Author\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(data[0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mwaskom/seaborn\n",
      "OpenInterpreter/open-interpreter\n",
      "pallets/flask\n",
      "keras-team/keras\n",
      "sympy/sympy\n",
      "django/django\n",
      "astropy/astropy\n",
      "ytdl-org/youtube-dl\n",
      "scikit-learn/scikit-learn\n",
      "psf/requests\n",
      "pytest-dev/pytest\n",
      "pylint-dev/pylint\n",
      "matplotlib/matplotlib\n",
      "pydata/xarray\n",
      "sphinx-doc/sphinx\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(set(data[i]['repo'] for i in range(len(data)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_instance_ids = [\n",
    "    \"keras-team__keras-18526\",\n",
    "    \"keras-team__keras-18553\",\n",
    "    \"keras-team__keras-18571\",\n",
    "    \"keras-team__keras-18585\",\n",
    "    \"keras-team__keras-18649\",\n",
    "    \"keras-team__keras-18659\",\n",
    "    \"keras-team__keras-18766\",\n",
    "    \"keras-team__keras-18852\",\n",
    "    \"keras-team__keras-18871\",\n",
    "    \"keras-team__keras-18902\",\n",
    "    \"keras-team__keras-18926\",\n",
    "    \"keras-team__keras-18975\",\n",
    "    \"keras-team__keras-18977\",\n",
    "    \"keras-team__keras-19088\",\n",
    "    \"keras-team__keras-19102\",\n",
    "    \"keras-team__keras-19143\",\n",
    "    \"keras-team__keras-19190\",\n",
    "    \"keras-team__keras-19201\",\n",
    "    \"keras-team__keras-19260\",\n",
    "    \"keras-team__keras-19284\",\n",
    "    \"keras-team__keras-19300\",\n",
    "    \"keras-team__keras-19331\",\n",
    "    \"keras-team__keras-19387\",\n",
    "    \"keras-team__keras-19459\",\n",
    "    \"keras-team__keras-19466\",\n",
    "    \"keras-team__keras-19484\",\n",
    "    \"keras-team__keras-19636\",\n",
    "    \"keras-team__keras-19641\",\n",
    "    \"keras-team__keras-19773\",\n",
    "    \"keras-team__keras-19775\",\n",
    "    \"keras-team__keras-19799\",\n",
    "    \"keras-team__keras-19826\",\n",
    "    \"keras-team__keras-19838\",\n",
    "    \"keras-team__keras-19844\",\n",
    "    \"keras-team__keras-19852\",\n",
    "    \"keras-team__keras-19863\",\n",
    "    \"keras-team__keras-19872\",\n",
    "    \"keras-team__keras-19903\",\n",
    "    \"keras-team__keras-19915\",\n",
    "    \"keras-team__keras-19924\",\n",
    "    \"keras-team__keras-19931\",\n",
    "    \"keras-team__keras-19937\",\n",
    "    \"keras-team__keras-19955\",\n",
    "    \"keras-team__keras-19973\",\n",
    "    \"keras-team__keras-19979\",\n",
    "    \"keras-team__keras-20002\",\n",
    "    \"keras-team__keras-20008\",\n",
    "    \"keras-team__keras-20023\",\n",
    "    \"keras-team__keras-20026\",\n",
    "    \"keras-team__keras-20033\",\n",
    "    \"keras-team__keras-20034\",\n",
    "    \"keras-team__keras-20040\",\n",
    "    \"keras-team__keras-20041\",\n",
    "    \"keras-team__keras-20076\",\n",
    "    \"keras-team__keras-20079\",\n",
    "    \"keras-team__keras-20122\",\n",
    "    \"keras-team__keras-20125\",\n",
    "    \"keras-team__keras-20197\",\n",
    "    \"keras-team__keras-20206\",\n",
    "    \"keras-team__keras-20296\",\n",
    "    \"keras-team__keras-20380\",\n",
    "    \"keras-team__keras-20389\",\n",
    "    \"keras-team__keras-20410\",\n",
    "    \"keras-team__keras-20443\",\n",
    "    \"keras-team__keras-20447\",\n",
    "    \"keras-team__keras-20456\",\n",
    "    \"keras-team__keras-20457\",\n",
    "    \"keras-team__keras-20484\",\n",
    "    \"keras-team__keras-20534\",\n",
    "    \"keras-team__keras-20537\",\n",
    "    \"keras-team__keras-20541\",\n",
    "    \"keras-team__keras-20550\",\n",
    "    \"keras-team__keras-20602\",\n",
    "    \"keras-team__keras-20609\",\n",
    "    \"keras-team__keras-20612\",\n",
    "    \"keras-team__keras-20626\",\n",
    "    \"keras-team__keras-20689\",\n",
    "    \"keras-team__keras-20703\",\n",
    "    \"keras-team__keras-20733\",\n",
    "    \"keras-team__keras-20745\",\n",
    "    \"keras-team__keras-20765\",\n",
    "    \"keras-team__keras-20768\",\n",
    "    \"keras-team__keras-20782\",\n",
    "    \"keras-team__keras-20791\",\n",
    "    \"keras-team__keras-20808\",\n",
    "    \"keras-team__keras-20815\",\n",
    "    \"keras-team__keras-20822\",\n",
    "    \"keras-team__keras-20829\",\n",
    "    \"keras-team__keras-20888\",\n",
    "    \"keras-team__keras-20901\",\n",
    "    \"keras-team__keras-20973\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task(instance_id):\n",
    "    # get task with instance_id keras-team__keras-19260\n",
    "    task = [t for t in data if t['instance_id'] == instance_id][0]\n",
    "    return task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = get_task('keras-team__keras-19143')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"repo\": \"keras-team/keras\",\n",
      "    \"pull_number\": 19143,\n",
      "    \"instance_id\": \"keras-team__keras-19143\",\n",
      "    \"issue_numbers\": [\n",
      "        \"19062\"\n",
      "    ],\n",
      "    \"base_commit\": \"ca179cf1dea6a8e5dfd862678484fb2af2b64f32\",\n",
      "    \"patch\": \"diff --git a/keras/backend/jax/__init__.py b/keras/backend/jax/__init__.py\\nindex 587fa47b7736..327bd95dc0de 100644\\n--- a/keras/backend/jax/__init__.py\\n+++ b/keras/backend/jax/__init__.py\\n@@ -1,6 +1,7 @@\\n from keras.backend.jax import core\\n from keras.backend.jax import distribution_lib\\n from keras.backend.jax import image\\n+from keras.backend.jax import linalg\\n from keras.backend.jax import math\\n from keras.backend.jax import nn\\n from keras.backend.jax import numpy\\ndiff --git a/keras/backend/jax/linalg.py b/keras/backend/jax/linalg.py\\nnew file mode 100644\\nindex 000000000000..73d548a2d91d\\n--- /dev/null\\n+++ b/keras/backend/jax/linalg.py\\n@@ -0,0 +1,72 @@\\n+import jax\\n+import jax.numpy as jnp\\n+import jax.scipy as jsp\\n+\\n+from keras.backend import config\\n+from keras.backend import standardize_dtype\\n+from keras.backend.common import dtypes\\n+from keras.backend.jax.core import cast\\n+from keras.backend.jax.core import convert_to_tensor\\n+\\n+\\n+def cholesky(a):\\n+    out = jnp.linalg.cholesky(a)\\n+    if jnp.any(jnp.isnan(out)):\\n+        raise ValueError(\\n+            \\\"Cholesky decomposition failed. \\\"\\n+            \\\"The input might not be a valid positive definite matrix.\\\"\\n+        )\\n+    return out\\n+\\n+\\n+def det(a):\\n+    return jnp.linalg.det(a)\\n+\\n+\\n+def eig(x):\\n+    return jnp.linalg.eig(x)\\n+\\n+\\n+def inv(a):\\n+    return jnp.linalg.inv(a)\\n+\\n+\\n+def lu_factor(x):\\n+    lu_factor_fn = jsp.linalg.lu_factor\\n+    if x.ndim > 2:\\n+        for i in range(x.ndim - 2):\\n+            lu_factor_fn = jax.vmap(lu_factor_fn)\\n+\\n+    return lu_factor_fn(x)\\n+\\n+\\n+def norm(x, ord=None, axis=None, keepdims=False):\\n+    x = convert_to_tensor(x)\\n+    if standardize_dtype(x.dtype) == \\\"int64\\\":\\n+        dtype = config.floatx()\\n+    else:\\n+        dtype = dtypes.result_type(x.dtype, float)\\n+    x = cast(x, dtype)\\n+    return jnp.linalg.norm(x, ord=ord, axis=axis, keepdims=keepdims)\\n+\\n+\\n+def qr(x, mode=\\\"reduced\\\"):\\n+    if mode not in {\\\"reduced\\\", \\\"complete\\\"}:\\n+        raise ValueError(\\n+            \\\"`mode` argument value not supported. \\\"\\n+            \\\"Expected one of {'reduced', 'complete'}. \\\"\\n+            f\\\"Received: mode={mode}\\\"\\n+        )\\n+    return jnp.linalg.qr(x, mode=mode)\\n+\\n+\\n+def solve(a, b):\\n+    return jnp.linalg.solve(a, b)\\n+\\n+\\n+def solve_triangular(a, b, lower=False):\\n+    return jsp.linalg.solve_triangular(a, b, lower=lower)\\n+\\n+\\n+def svd(x, full_matrices=True, compute_uv=True):\\n+    return jnp.linalg.svd(x, full_matrices=full_matrices, compute_uv=compute_uv)\\ndiff --git a/keras/backend/numpy/__init__.py b/keras/backend/numpy/__init__.py\\nindex ce0d20e5f8f4..f84ec2e32291 100644\\n--- a/keras/backend/numpy/__init__.py\\n+++ b/keras/backend/numpy/__init__.py\\n@@ -1,5 +1,6 @@\\n from keras.backend.numpy import core\\n from keras.backend.numpy import image\\n+from keras.backend.numpy import linalg\\n from keras.backend.numpy import math\\n from keras.backend.numpy import nn\\n from keras.backend.numpy import numpy\\ndiff --git a/keras/backend/numpy/linalg.py b/keras/backend/numpy/linalg.py\\nnew file mode 100644\\nindex 000000000000..fbe54200f417\\n--- /dev/null\\n+++ b/keras/backend/numpy/linalg.py\\n@@ -0,0 +1,78 @@\\n+import numpy as np\\n+import scipy.linalg as sl\\n+\\n+from keras.backend import standardize_dtype\\n+from keras.backend.common import dtypes\\n+from keras.backend.torch.core import convert_to_tensor\\n+\\n+\\n+def cholesky(a):\\n+    return np.linalg.cholesky(a)\\n+\\n+\\n+def det(a):\\n+    return np.linalg.det(a)\\n+\\n+\\n+def eig(a):\\n+    return np.linalg.eig(a)\\n+\\n+\\n+def inv(a):\\n+    return np.linalg.inv(a)\\n+\\n+\\n+def lu_factor(a):\\n+    if a.ndim == 2:\\n+        return sl.lu_factor(a)\\n+\\n+    m, n = a.shape[-2:]\\n+    signature = \\\"(m,n) -> (m,n), \\\"\\n+    signature += \\\"(m)\\\" if m <= n else \\\"(n)\\\"\\n+    _lu_factor_gufunc = np.vectorize(\\n+        sl.lu_factor,\\n+        signature=signature,\\n+    )\\n+    return _lu_factor_gufunc(a)\\n+\\n+\\n+def norm(x, ord=None, axis=None, keepdims=False):\\n+    x = convert_to_tensor(x)\\n+    dtype = standardize_dtype(x.dtype)\\n+    if \\\"int\\\" in dtype or dtype == \\\"bool\\\":\\n+        dtype = dtypes.result_type(x.dtype, \\\"float32\\\")\\n+    return np.linalg.norm(x, ord=ord, axis=axis, keepdims=keepdims).astype(\\n+        dtype\\n+    )\\n+\\n+\\n+def qr(x, mode=\\\"reduced\\\"):\\n+    if mode not in {\\\"reduced\\\", \\\"complete\\\"}:\\n+        raise ValueError(\\n+            \\\"`mode` argument value not supported. \\\"\\n+            \\\"Expected one of {'reduced', 'complete'}. \\\"\\n+            f\\\"Received: mode={mode}\\\"\\n+        )\\n+    return np.linalg.qr(x, mode=mode)\\n+\\n+\\n+def solve(a, b):\\n+    return np.linalg.solve(a, b)\\n+\\n+\\n+def solve_triangular(a, b, lower=False):\\n+    if a.ndim == 2:\\n+        return sl.solve_triangular(a, b, lower=lower)\\n+\\n+    _vectorized_solve_triangular = np.vectorize(\\n+        lambda a, b: sl.solve_triangular(a, b, lower=lower),\\n+        signature=\\\"(n,n),(n,m)->(n,m)\\\",\\n+    )\\n+    if b.ndim == a.ndim - 1:\\n+        b = np.expand_dims(b, axis=-1)\\n+        return _vectorized_solve_triangular(a, b).squeeze(axis=-1)\\n+    return _vectorized_solve_triangular(a, b)\\n+\\n+\\n+def svd(x, full_matrices=True, compute_uv=True):\\n+    return np.linalg.svd(x, full_matrices=full_matrices, compute_uv=compute_uv)\\ndiff --git a/keras/backend/tensorflow/__init__.py b/keras/backend/tensorflow/__init__.py\\nindex 7976bcf784b9..2791de505385 100644\\n--- a/keras/backend/tensorflow/__init__.py\\n+++ b/keras/backend/tensorflow/__init__.py\\n@@ -1,6 +1,7 @@\\n from keras.backend.tensorflow import core\\n from keras.backend.tensorflow import distribution_lib\\n from keras.backend.tensorflow import image\\n+from keras.backend.tensorflow import linalg\\n from keras.backend.tensorflow import math\\n from keras.backend.tensorflow import nn\\n from keras.backend.tensorflow import numpy\\ndiff --git a/keras/backend/tensorflow/linalg.py b/keras/backend/tensorflow/linalg.py\\nnew file mode 100644\\nindex 000000000000..cd6ae07db5a1\\n--- /dev/null\\n+++ b/keras/backend/tensorflow/linalg.py\\n@@ -0,0 +1,162 @@\\n+import tensorflow as tf\\n+from tensorflow.experimental import numpy as tfnp\\n+\\n+from keras.backend import config\\n+from keras.backend import standardize_dtype\\n+from keras.backend.common import dtypes\\n+from keras.backend.tensorflow.core import cast\\n+from keras.backend.tensorflow.core import convert_to_tensor\\n+\\n+\\n+def cholesky(a):\\n+    out = tf.linalg.cholesky(a)\\n+    # tf.linalg.cholesky simply returns NaNs for non-positive definite matrices\\n+    return tf.debugging.check_numerics(out, \\\"Cholesky\\\")\\n+\\n+\\n+def det(a):\\n+    return tf.linalg.det(a)\\n+\\n+\\n+def eig(a):\\n+    return tf.linalg.eig(a)\\n+\\n+\\n+def inv(a):\\n+    return tf.linalg.inv(a)\\n+\\n+\\n+def lu_factor(a):\\n+    lu, p = tf.linalg.lu(a)\\n+    return lu, tf.math.invert_permutation(p)\\n+\\n+\\n+def norm(x, ord=None, axis=None, keepdims=False):\\n+    x = convert_to_tensor(x)\\n+    x_shape = x.shape\\n+    ndim = x_shape.rank\\n+\\n+    if axis is None:\\n+        axis = tuple(range(ndim))\\n+    elif isinstance(axis, int):\\n+        axis = (axis,)\\n+\\n+    axis = axis[0] if len(axis) == 1 else axis\\n+    num_axes = 1 if isinstance(axis, int) else len(axis)\\n+\\n+    if num_axes == 1 and ord is None:\\n+        ord = \\\"euclidean\\\"\\n+    elif num_axes == 2 and ord is None:\\n+        ord = \\\"fro\\\"\\n+\\n+    if standardize_dtype(x.dtype) == \\\"int64\\\":\\n+        dtype = config.floatx()\\n+    else:\\n+        dtype = dtypes.result_type(x.dtype, float)\\n+    x = cast(x, dtype)\\n+\\n+    # Fast path to utilze `tf.linalg.norm`\\n+    if (num_axes == 1 and ord in (\\\"euclidean\\\", 1, 2, float(\\\"inf\\\"))) or (\\n+        num_axes == 2 and ord in (\\\"euclidean\\\", \\\"fro\\\", 1, 2, float(\\\"inf\\\"))\\n+    ):\\n+        return tf.linalg.norm(x, ord=ord, axis=axis, keepdims=keepdims)\\n+\\n+    # Ref: jax.numpy.linalg.norm\\n+    if num_axes == 1 and ord not in (\\\"fro\\\", \\\"nuc\\\"):\\n+        if ord == float(\\\"-inf\\\"):\\n+            return tf.math.reduce_min(\\n+                tf.math.abs(x), axis=axis, keepdims=keepdims\\n+            )\\n+        elif ord == 0:\\n+            return tf.math.reduce_sum(\\n+                tf.cast(tf.not_equal(x, 0), dtype=x.dtype),\\n+                axis=axis,\\n+                keepdims=keepdims,\\n+            )\\n+        else:\\n+            ord = convert_to_tensor(ord, dtype=x.dtype)\\n+            out = tf.math.reduce_sum(\\n+                tf.pow(tf.math.abs(x), ord), axis=axis, keepdims=keepdims\\n+            )\\n+            return tf.pow(out, 1.0 / ord)\\n+    elif num_axes == 2 and ord in (\\\"nuc\\\", float(\\\"-inf\\\"), -2, -1):\\n+        row_axis, col_axis = axis[0], axis[1]\\n+        row_axis = row_axis + ndim if row_axis < 0 else row_axis\\n+        col_axis = col_axis + ndim if col_axis < 0 else col_axis\\n+        if ord == float(\\\"-inf\\\"):\\n+            if not keepdims and row_axis > col_axis:\\n+                row_axis -= 1\\n+            x = tf.math.reduce_min(\\n+                tf.reduce_sum(tf.math.abs(x), axis=col_axis, keepdims=keepdims),\\n+                axis=row_axis,\\n+                keepdims=keepdims,\\n+            )\\n+        elif ord == -1:\\n+            if not keepdims and col_axis > row_axis:\\n+                col_axis -= 1\\n+            x = tf.math.reduce_min(\\n+                tf.reduce_sum(tf.math.abs(x), axis=row_axis, keepdims=keepdims),\\n+                axis=col_axis,\\n+                keepdims=keepdims,\\n+            )\\n+        else:\\n+            x = tfnp.moveaxis(x, axis, (-2, -1))\\n+            if ord == -2:\\n+                x = tf.math.reduce_min(\\n+                    tf.linalg.svd(x, compute_uv=False), axis=-1\\n+                )\\n+            else:\\n+                x = tf.math.reduce_sum(\\n+                    tf.linalg.svd(x, compute_uv=False), axis=-1\\n+                )\\n+            if keepdims:\\n+                x = tf.expand_dims(x, axis[0])\\n+                x = tf.expand_dims(x, axis[1])\\n+        return x\\n+\\n+    if num_axes == 1:\\n+        raise ValueError(\\n+            f\\\"Invalid `ord` argument for vector norm. Received: ord={ord}\\\"\\n+        )\\n+    elif num_axes == 2:\\n+        raise ValueError(\\n+            f\\\"Invalid `ord` argument for matrix norm. Received: ord={ord}\\\"\\n+        )\\n+    else:\\n+        raise ValueError(f\\\"Invalid axis values. Received: axis={axis}\\\")\\n+\\n+\\n+def qr(x, mode=\\\"reduced\\\"):\\n+    if mode not in {\\\"reduced\\\", \\\"complete\\\"}:\\n+        raise ValueError(\\n+            \\\"`mode` argument value not supported. \\\"\\n+            \\\"Expected one of {'reduced', 'complete'}. \\\"\\n+            f\\\"Received: mode={mode}\\\"\\n+        )\\n+    if mode == \\\"reduced\\\":\\n+        return tf.linalg.qr(x)\\n+    return tf.linalg.qr(x, full_matrices=True)\\n+\\n+\\n+def solve(a, b):\\n+    # tensorflow.linalg.solve only supports same rank inputs\\n+    if tf.rank(b) == tf.rank(a) - 1:\\n+        b = tf.expand_dims(b, axis=-1)\\n+        return tf.squeeze(tf.linalg.solve(a, b), axis=-1)\\n+    return tf.linalg.solve(a, b)\\n+\\n+\\n+def solve_triangular(a, b, lower=False):\\n+    if b.shape.ndims == a.shape.ndims - 1:\\n+        b = tf.expand_dims(b, axis=-1)\\n+        return tf.squeeze(\\n+            tf.linalg.triangular_solve(a, b, lower=lower), axis=-1\\n+        )\\n+    return tf.linalg.triangular_solve(a, b, lower=lower)\\n+\\n+\\n+def svd(x, full_matrices=True, compute_uv=True):\\n+    s, u, v = tf.linalg.svd(\\n+        x, full_matrices=full_matrices, compute_uv=compute_uv\\n+    )\\n+    return u, s, tf.linalg.adjoint(v)\\ndiff --git a/keras/backend/torch/__init__.py b/keras/backend/torch/__init__.py\\nindex a7669bbc9e6d..1358104f6f4b 100644\\n--- a/keras/backend/torch/__init__.py\\n+++ b/keras/backend/torch/__init__.py\\n@@ -16,6 +16,7 @@\\n \\n from keras.backend.torch import core\\n from keras.backend.torch import image\\n+from keras.backend.torch import linalg\\n from keras.backend.torch import math\\n from keras.backend.torch import nn\\n from keras.backend.torch import numpy\\ndiff --git a/keras/backend/torch/linalg.py b/keras/backend/torch/linalg.py\\nnew file mode 100644\\nindex 000000000000..3deeaefdbe79\\n--- /dev/null\\n+++ b/keras/backend/torch/linalg.py\\n@@ -0,0 +1,70 @@\\n+import torch\\n+\\n+from keras.backend import config\\n+from keras.backend import standardize_dtype\\n+from keras.backend.common import dtypes\\n+from keras.backend.torch.core import cast\\n+from keras.backend.torch.core import convert_to_tensor\\n+\\n+\\n+def cholesky(x):\\n+    return torch.cholesky(x)\\n+\\n+\\n+def det(x):\\n+    return torch.det(x)\\n+\\n+\\n+def eig(x):\\n+    return torch.linalg.eig(x)\\n+\\n+\\n+def inv(x):\\n+    return torch.linalg.inv(x)\\n+\\n+\\n+def lu_factor(x):\\n+    LU, pivots = torch.linalg.lu_factor(x)\\n+    # torch retuns pivots with 1-based indexing\\n+    return LU, pivots - 1\\n+\\n+\\n+def norm(x, ord=None, axis=None, keepdims=False):\\n+    x = convert_to_tensor(x)\\n+    if standardize_dtype(x.dtype) == \\\"int64\\\":\\n+        dtype = config.floatx()\\n+    else:\\n+        dtype = dtypes.result_type(x.dtype, float)\\n+    x = cast(x, dtype)\\n+    return torch.linalg.norm(x, ord=ord, dim=axis, keepdim=keepdims)\\n+\\n+\\n+def qr(x, mode=\\\"reduced\\\"):\\n+    if mode not in {\\\"reduced\\\", \\\"complete\\\"}:\\n+        raise ValueError(\\n+            \\\"`mode` argument value not supported. \\\"\\n+            \\\"Expected one of {'reduced', 'complete'}. \\\"\\n+            f\\\"Received: mode={mode}\\\"\\n+        )\\n+    return torch.linalg.qr(x, mode=mode)\\n+\\n+\\n+def solve(a, b):\\n+    return torch.linalg.solve(a, b)\\n+\\n+\\n+def solve_triangular(a, b, lower=False):\\n+    if b.ndim == a.ndim - 1:\\n+        b = torch.unsqueeze(b, axis=-1)\\n+        return torch.linalg.solve_triangular(a, b, upper=not lower).squeeze(\\n+            axis=-1\\n+        )\\n+    return torch.linalg.solve_triangular(a, b, upper=not lower)\\n+\\n+\\n+def svd(x, full_matrices=True, compute_uv=True):\\n+    if not compute_uv:\\n+        raise NotImplementedError(\\n+            \\\"`compute_uv=False` is not supported for torch backend.\\\"\\n+        )\\n+    return torch.linalg.svd(x, full_matrices=full_matrices)\\ndiff --git a/keras/ops/__init__.py b/keras/ops/__init__.py\\nindex 2e405791a048..3afb84a2b296 100644\\n--- a/keras/ops/__init__.py\\n+++ b/keras/ops/__init__.py\\n@@ -10,6 +10,7 @@\\n from keras.ops import image\\n from keras.ops import operation_utils\\n from keras.ops.core import *  # noqa: F403\\n+from keras.ops.linalg import *  # noqa: F403\\n from keras.ops.math import *  # noqa: F403\\n from keras.ops.nn import *  # noqa: F403\\n from keras.ops.numpy import *  # noqa: F403\\ndiff --git a/keras/ops/linalg.py b/keras/ops/linalg.py\\nnew file mode 100644\\nindex 000000000000..cbbc4b1d9c7c\\n--- /dev/null\\n+++ b/keras/ops/linalg.py\\n@@ -0,0 +1,598 @@\\n+from keras import backend\\n+from keras.api_export import keras_export\\n+from keras.backend import KerasTensor\\n+from keras.backend import any_symbolic_tensors\\n+from keras.ops.operation import Operation\\n+from keras.ops.operation_utils import reduce_shape\\n+\\n+\\n+class Cholesky(Operation):\\n+    def __init__(self):\\n+        super().__init__()\\n+\\n+    def call(self, x):\\n+        return _cholesky(x)\\n+\\n+    def compute_output_spec(self, x):\\n+        _assert_2d(x)\\n+        _assert_square(x)\\n+        return KerasTensor(x.shape, x.dtype)\\n+\\n+\\n+@keras_export([\\\"keras.ops.cholesky\\\", \\\"keras.ops.linalg.cholesky\\\"])\\n+def cholesky(x):\\n+    \\\"\\\"\\\"Computes the Cholesky decomposition of a positive semi-definite matrix.\\n+\\n+    Args:\\n+        x: Input tensor of shape `(..., M, M)`.\\n+\\n+    Returns:\\n+        A tensor of shape `(..., M, M)` representing the lower triangular\\n+        Cholesky factor of `x`.\\n+\\n+    \\\"\\\"\\\"\\n+    if any_symbolic_tensors((x,)):\\n+        return Cholesky().symbolic_call(x)\\n+    return _cholesky(x)\\n+\\n+\\n+def _cholesky(x):\\n+    x = backend.convert_to_tensor(x)\\n+    _assert_2d(x)\\n+    _assert_square(x)\\n+    try:\\n+        return backend.linalg.cholesky(x)\\n+    except Exception as e:\\n+        raise ValueError(f\\\"Cholesky decomposition failed: {e}\\\")\\n+\\n+\\n+class Det(Operation):\\n+\\n+    def __init__(self):\\n+        super().__init__()\\n+\\n+    def call(self, x):\\n+        return _det(x)\\n+\\n+    def compute_output_spec(self, x):\\n+        _assert_2d(x)\\n+        _assert_square(x)\\n+        return KerasTensor(x.shape[:-2], x.dtype)\\n+\\n+\\n+@keras_export([\\\"keras.ops.det\\\", \\\"keras.ops.linalg.det\\\"])\\n+def det(x):\\n+    \\\"\\\"\\\"Computes the determinant of a square tensor.\\n+\\n+    Args:\\n+        x: Input tensor of shape `(..., M, M)`.\\n+\\n+    Returns:\\n+        A tensor of shape `(...,)` represeting the determinant of `x`.\\n+\\n+    \\\"\\\"\\\"\\n+    if any_symbolic_tensors((x,)):\\n+        return Det().symbolic_call(x)\\n+    return _det(x)\\n+\\n+\\n+def _det(x):\\n+    x = backend.convert_to_tensor(x)\\n+    _assert_2d(x)\\n+    _assert_square(x)\\n+    return backend.linalg.det(x)\\n+\\n+\\n+class Eig(Operation):\\n+\\n+    def __init__(self):\\n+        super().__init__()\\n+\\n+    def call(self, x):\\n+        return _eig(x)\\n+\\n+    def compute_output_spec(self, x):\\n+        _assert_2d(x)\\n+        _assert_square(x)\\n+        return (\\n+            KerasTensor(x.shape[:-1], x.dtype),\\n+            KerasTensor(x.shape, x.dtype),\\n+        )\\n+\\n+\\n+@keras_export([\\\"keras.ops.eig\\\", \\\"keras.ops.linalg.eig\\\"])\\n+def eig(x):\\n+    \\\"\\\"\\\"Computes the eigenvalues and eigenvectors of a square matrix.\\n+\\n+    Args:\\n+        x: Input tensor of shape `(..., M, M)`.\\n+\\n+    Returns:\\n+        A tuple of two tensors: a tensor of shape `(..., M)` containing\\n+        eigenvalues and a tensor of shape `(..., M, M)` containing eigenvectors.\\n+\\n+    \\\"\\\"\\\"\\n+    if any_symbolic_tensors((x,)):\\n+        return Eig().symbolic_call(x)\\n+    return _eig(x)\\n+\\n+\\n+def _eig(x):\\n+    x = backend.convert_to_tensor(x)\\n+    _assert_2d(x)\\n+    _assert_square(x)\\n+    return backend.linalg.eig(x)\\n+\\n+\\n+class Inv(Operation):\\n+\\n+    def __init__(self):\\n+        super().__init__()\\n+\\n+    def call(self, x):\\n+        return _inv(x)\\n+\\n+    def compute_output_spec(self, x):\\n+        _assert_2d(x)\\n+        _assert_square(x)\\n+        return KerasTensor(x.shape, x.dtype)\\n+\\n+\\n+@keras_export([\\\"keras.ops.inv\\\", \\\"keras.ops.linalg.inv\\\"])\\n+def inv(x):\\n+    \\\"\\\"\\\"Computes the inverse of a square tensor.\\n+\\n+    Args:\\n+        x: Input tensor of shape `(..., M, M)`.\\n+\\n+    Returns:\\n+        A tensor of shape `(..., M, M)` representing the inverse of `x`.\\n+\\n+    \\\"\\\"\\\"\\n+    if any_symbolic_tensors((x,)):\\n+        return Inv().symbolic_call(x)\\n+    return _inv(x)\\n+\\n+\\n+def _inv(x):\\n+    x = backend.convert_to_tensor(x)\\n+    _assert_2d(x)\\n+    _assert_square(x)\\n+    return backend.linalg.inv(x)\\n+\\n+\\n+class LuFactor(Operation):\\n+\\n+    def __init__(self):\\n+        super().__init__()\\n+\\n+    def call(self, x):\\n+        return _lu_factor(x)\\n+\\n+    def compute_output_spec(self, x):\\n+        _assert_2d(x)\\n+        batch_shape = x.shape[:-2]\\n+        m, n = x.shape[-2:]\\n+        k = min(m, n)\\n+        return (\\n+            KerasTensor(batch_shape + (m, n), x.dtype),\\n+            KerasTensor(batch_shape + (k,), x.dtype),\\n+        )\\n+\\n+\\n+@keras_export([\\\"keras.ops.lu_factor\\\", \\\"keras.ops.linalg.lu_factor\\\"])\\n+def lu_factor(x):\\n+    \\\"\\\"\\\"Computes the lower-upper decomposition of a square matrix.\\n+\\n+    Args:\\n+        x: A tensor of shape `(..., M, M)`.\\n+\\n+    Returns:\\n+        A tuple of two tensors: a tensor of shape `(..., M, M)` containing the\\n+        lower and upper triangular matrices and a tensor of shape `(..., M)`\\n+        containing the pivots.\\n+\\n+    \\\"\\\"\\\"\\n+    if any_symbolic_tensors((x,)):\\n+        return LuFactor().symbolic_call(x)\\n+    return _lu_factor(x)\\n+\\n+\\n+def _lu_factor(x):\\n+    x = backend.convert_to_tensor(x)\\n+    _assert_2d(x)\\n+    if backend.backend() == \\\"tensorflow\\\":\\n+        try:\\n+            _assert_square(x)\\n+        except ValueError as e:\\n+            raise ValueError(\\n+                f\\\"LU decomposition failed: {e}. LU decomposition is only \\\"\\n+                \\\"supported for square matrices in Tensorflow.\\\"\\n+            )\\n+    return backend.linalg.lu_factor(x)\\n+\\n+\\n+class Norm(Operation):\\n+    def __init__(self, ord=None, axis=None, keepdims=False):\\n+        super().__init__()\\n+        if isinstance(ord, str):\\n+            if ord not in (\\\"fro\\\", \\\"nuc\\\"):\\n+                raise ValueError(\\n+                    \\\"Invalid `ord` argument. \\\"\\n+                    \\\"Expected one of {'fro', 'nuc'} when using string. \\\"\\n+                    f\\\"Received: ord={ord}\\\"\\n+                )\\n+        if isinstance(axis, int):\\n+            axis = [axis]\\n+        self.ord = ord\\n+        self.axis = axis\\n+        self.keepdims = keepdims\\n+\\n+    def compute_output_spec(self, x):\\n+        output_dtype = backend.standardize_dtype(x.dtype)\\n+        if \\\"int\\\" in output_dtype or output_dtype == \\\"bool\\\":\\n+            output_dtype = backend.floatx()\\n+        if self.axis is None:\\n+            axis = tuple(range(len(x.shape)))\\n+        else:\\n+            axis = self.axis\\n+        num_axes = len(axis)\\n+        if num_axes == 1 and isinstance(self.ord, str):\\n+            raise ValueError(\\n+                \\\"Invalid `ord` argument for vector norm. \\\"\\n+                f\\\"Received: ord={self.ord}\\\"\\n+            )\\n+        elif num_axes == 2 and self.ord not in (\\n+            None,\\n+            \\\"fro\\\",\\n+            \\\"nuc\\\",\\n+            float(\\\"inf\\\"),\\n+            float(\\\"-inf\\\"),\\n+            1,\\n+            -1,\\n+            2,\\n+            -2,\\n+        ):\\n+            raise ValueError(\\n+                \\\"Invalid `ord` argument for matrix norm. \\\"\\n+                f\\\"Received: ord={self.ord}\\\"\\n+            )\\n+        return KerasTensor(\\n+            reduce_shape(x.shape, axis=self.axis, keepdims=self.keepdims),\\n+            dtype=output_dtype,\\n+        )\\n+\\n+    def call(self, x):\\n+        x = backend.convert_to_tensor(x)\\n+        return backend.linalg.norm(\\n+            x, ord=self.ord, axis=self.axis, keepdims=self.keepdims\\n+        )\\n+\\n+\\n+@keras_export([\\\"keras.ops.norm\\\", \\\"keras.ops.linalg.norm\\\"])\\n+def norm(x, ord=None, axis=None, keepdims=False):\\n+    \\\"\\\"\\\"Matrix or vector norm.\\n+\\n+    This function is able to return one of eight different matrix norms, or one\\n+    of an infinite number of vector norms (described below), depending on the\\n+    value of the `ord` parameter.\\n+\\n+    Args:\\n+        x: Input tensor.\\n+        ord: Order of the norm (see table under Notes). The default is `None`.\\n+        axis: If `axis` is an integer, it specifies the axis of `x` along which\\n+            to compute the vector norms. If `axis` is a 2-tuple, it specifies\\n+            the axes that hold 2-D matrices, and the matrix norms of these\\n+            matrices are computed.\\n+        keepdims: If this is set to `True`, the axes which are reduced are left\\n+            in the result as dimensions with size one.\\n+\\n+    Note:\\n+        For values of `ord < 1`, the result is, strictly speaking, not a\\n+        mathematical 'norm', but it may still be useful for various numerical\\n+        purposes. The following norms can be calculated:\\n+        - For matrices:\\n+            - `ord=None`: Frobenius norm\\n+            - `ord=\\\"fro\\\"`: Frobenius norm\\n+            - `ord=nuc`: nuclear norm\\n+            - `ord=np.inf`: `max(sum(abs(x), axis=1))`\\n+            - `ord=-np.inf`: `min(sum(abs(x), axis=1))`\\n+            - `ord=0`: not supported\\n+            - `ord=1`: `max(sum(abs(x), axis=0))`\\n+            - `ord=-1`: `min(sum(abs(x), axis=0))`\\n+            - `ord=2`: 2-norm (largest sing. value)\\n+            - `ord=-2`: smallest singular value\\n+            - other: not supported\\n+        - For vectors:\\n+            - `ord=None`: 2-norm\\n+            - `ord=\\\"fro\\\"`: not supported\\n+            - `ord=nuc`: not supported\\n+            - `ord=np.inf`: `max(abs(x))`\\n+            - `ord=-np.inf`: `min(abs(x))`\\n+            - `ord=0`: `sum(x != 0)`\\n+            - `ord=1`: as below\\n+            - `ord=-1`: as below\\n+            - `ord=2`: as below\\n+            - `ord=-2`: as below\\n+            - other: `sum(abs(x)**ord)**(1./ord)`\\n+\\n+    Returns:\\n+        Norm of the matrix or vector(s).\\n+\\n+    Example:\\n+\\n+    >>> x = keras.ops.reshape(keras.ops.arange(9, dtype=\\\"float32\\\") - 4, (3, 3))\\n+    >>> keras.ops.linalg.norm(x)\\n+    7.7459664\\n+    \\\"\\\"\\\"\\n+    if any_symbolic_tensors((x,)):\\n+        return Norm(ord=ord, axis=axis, keepdims=keepdims).symbolic_call(x)\\n+    x = backend.convert_to_tensor(x)\\n+    return backend.linalg.norm(x, ord=ord, axis=axis, keepdims=keepdims)\\n+\\n+\\n+class Qr(Operation):\\n+    def __init__(self, mode=\\\"reduced\\\"):\\n+        super().__init__()\\n+        if mode not in {\\\"reduced\\\", \\\"complete\\\"}:\\n+            raise ValueError(\\n+                \\\"`mode` argument value not supported. \\\"\\n+                \\\"Expected one of {'reduced', 'complete'}. \\\"\\n+                f\\\"Received: mode={mode}\\\"\\n+            )\\n+        self.mode = mode\\n+\\n+    def compute_output_spec(self, x):\\n+        if len(x.shape) < 2:\\n+            raise ValueError(\\n+                \\\"Input should have rank >= 2. Received: \\\"\\n+                f\\\"input.shape = {x.shape}\\\"\\n+            )\\n+        m = x.shape[-2]\\n+        n = x.shape[-1]\\n+        if m is None or n is None:\\n+            raise ValueError(\\n+                \\\"Input should have its last 2 dimensions \\\"\\n+                \\\"fully-defined. Received: \\\"\\n+                f\\\"input.shape = {x.shape}\\\"\\n+            )\\n+        k = min(m, n)\\n+        base = tuple(x.shape[:-2])\\n+        if self.mode == \\\"reduced\\\":\\n+            return (\\n+                KerasTensor(shape=base + (m, k), dtype=x.dtype),\\n+                KerasTensor(shape=base + (k, n), dtype=x.dtype),\\n+            )\\n+        # 'complete' mode.\\n+        return (\\n+            KerasTensor(shape=base + (m, m), dtype=x.dtype),\\n+            KerasTensor(shape=base + (m, n), dtype=x.dtype),\\n+        )\\n+\\n+    def call(self, x):\\n+        x = backend.convert_to_tensor(x)\\n+        return backend.linalg.qr(x, mode=self.mode)\\n+\\n+\\n+@keras_export([\\\"keras.ops.qr\\\", \\\"keras.ops.linalg.qr\\\"])\\n+def qr(x, mode=\\\"reduced\\\"):\\n+    \\\"\\\"\\\"Computes the QR decomposition of a tensor.\\n+\\n+    Args:\\n+        x: Input tensor of shape `(..., M, N)`.\\n+        mode: A string specifying the mode of the QR decomposition.\\n+            - 'reduced': Returns the reduced QR decomposition. (default)\\n+            - 'complete': Returns the complete QR decomposition.\\n+\\n+    Returns:\\n+        A tuple containing two tensors. The first tensor of shape `(..., M, K)`\\n+        is the orthogonal matrix `q` and the second tensor of shape\\n+        `(..., K, N)` is the upper triangular matrix `r`, where `K = min(M, N)`.\\n+\\n+    Example:\\n+\\n+    >>> x = keras.ops.convert_to_tensor([[1., 2.], [3., 4.], [5., 6.]])\\n+    >>> q, r = qr(x)\\n+    >>> print(q)\\n+    array([[-0.16903079  0.897085]\\n+           [-0.5070925   0.2760267 ]\\n+           [-0.8451542  -0.34503305]], shape=(3, 2), dtype=float32)\\n+    \\\"\\\"\\\"\\n+    if any_symbolic_tensors((x,)):\\n+        return Qr(mode=mode).symbolic_call(x)\\n+    x = backend.convert_to_tensor(x)\\n+    return backend.linalg.qr(x, mode=mode)\\n+\\n+\\n+class Solve(Operation):\\n+\\n+    def __init__(self):\\n+        super().__init__()\\n+\\n+    def call(self, a, b):\\n+        return _solve(a, b)\\n+\\n+    def compute_output_spec(self, a, b):\\n+        _assert_2d(a)\\n+        _assert_square(a)\\n+        _assert_1d(b)\\n+        _assert_a_b_compat(a, b)\\n+        return KerasTensor(b.shape, b.dtype)\\n+\\n+\\n+@keras_export([\\\"keras.ops.solve\\\", \\\"keras.ops.linalg.solve\\\"])\\n+def solve(a, b):\\n+    \\\"\\\"\\\"Solves a linear system of equations given by `a x = b`.\\n+\\n+    Args:\\n+        a: A tensor of shape `(..., M, M)` representing the coefficients matrix.\\n+        b: A tensor of shape `(..., M)` or `(..., M, N)` represeting the\\n+        right-hand side or \\\"dependent variable\\\" matrix.\\n+\\n+    Returns:\\n+        A tensor of shape `(..., M)` or `(..., M, N)` representing the solution\\n+        of the linear system. Returned shape is identical to `b`.\\n+\\n+    \\\"\\\"\\\"\\n+    if any_symbolic_tensors((a, b)):\\n+        return Solve().symbolic_call(a, b)\\n+    return _solve(a, b)\\n+\\n+\\n+def _solve(a, b):\\n+    a = backend.convert_to_tensor(a)\\n+    b = backend.convert_to_tensor(b)\\n+    _assert_2d(a)\\n+    _assert_square(a)\\n+    _assert_1d(b)\\n+    _assert_a_b_compat(a, b)\\n+    return backend.linalg.solve(a, b)\\n+\\n+\\n+class SolveTriangular(Operation):\\n+\\n+    def __init__(self, lower=False):\\n+        super().__init__()\\n+        self.lower = lower\\n+\\n+    def call(self, a, b):\\n+        return _solve_triangular(a, b, self.lower)\\n+\\n+    def compute_output_spec(self, a, b):\\n+        _assert_2d(a)\\n+        _assert_square(a)\\n+        _assert_1d(b)\\n+        _assert_a_b_compat(a, b)\\n+        return KerasTensor(b.shape, b.dtype)\\n+\\n+\\n+@keras_export(\\n+    [\\\"keras.ops.solve_triangular\\\", \\\"keras.ops.linalg.solve_triangular\\\"]\\n+)\\n+def solve_triangular(a, b, lower=False):\\n+    \\\"\\\"\\\"Solves a linear system of equations given by `a x = b`.\\n+\\n+    Args:\\n+        a: A tensor of shape `(..., M, M)` representing the coefficients matrix.\\n+        b: A tensor of shape `(..., M)` or `(..., M, N)` represeting the\\n+        right-hand side or \\\"dependent variable\\\" matrix.\\n+\\n+    Returns:\\n+        A tensor of shape `(..., M)` or `(..., M, N)` representing the solution\\n+        of the linear system. Returned shape is identical to `b`.\\n+\\n+    \\\"\\\"\\\"\\n+    if any_symbolic_tensors((a, b)):\\n+        return SolveTriangular(lower).symbolic_call(a, b)\\n+    return _solve_triangular(a, b, lower)\\n+\\n+\\n+def _solve_triangular(a, b, lower=False):\\n+    a = backend.convert_to_tensor(a)\\n+    b = backend.convert_to_tensor(b)\\n+    _assert_2d(a)\\n+    _assert_square(a)\\n+    _assert_1d(b)\\n+    _assert_a_b_compat(a, b)\\n+    return backend.linalg.solve_triangular(a, b, lower)\\n+\\n+\\n+class SVD(Operation):\\n+\\n+    def __init__(self, full_matrices=True, compute_uv=True):\\n+        super().__init__()\\n+        self.full_matrices = full_matrices\\n+        self.compute_uv = compute_uv\\n+\\n+    def call(self, x):\\n+        return _svd(x, self.full_matrices, self.compute_uv)\\n+\\n+    def compute_output_spec(self, x):\\n+        _assert_2d(x)\\n+        rows, columns = x.shape[-2:]\\n+        batches = x.shape[:-2]\\n+        s_shape = batches + (min(rows, columns),)\\n+        if self.full_matrices:\\n+            u_shape = batches + (rows, rows)\\n+            v_shape = batches + (columns, columns)\\n+        else:\\n+            u_shape = batches + (rows, min(rows, columns))\\n+            v_shape = batches + (min(rows, columns), columns)\\n+\\n+        if self.compute_uv:\\n+            return (\\n+                KerasTensor(u_shape, x.dtype),\\n+                KerasTensor(s_shape, x.dtype),\\n+                KerasTensor(v_shape, x.dtype),\\n+            )\\n+        return KerasTensor(s_shape, x.dtype)\\n+\\n+\\n+@keras_export([\\\"keras.ops.svd\\\", \\\"keras.ops.linalg.svd\\\"])\\n+def svd(x, full_matrices=True, compute_uv=True):\\n+    \\\"\\\"\\\"Computes the singular value decomposition of a matrix.\\n+\\n+    Args:\\n+        x: Input tensor of shape `(..., M, N)`.\\n+\\n+    Returns:\\n+        A tuple of three tensors: a tensor of shape `(..., M, M)` containing the\\n+        left singular vectors, a tensor of shape `(..., M, N)` containing the\\n+        singular values and a tensor of shape `(..., N, N)` containing the\\n+        right singular vectors.\\n+\\n+    \\\"\\\"\\\"\\n+    if any_symbolic_tensors((x,)):\\n+        return SVD(full_matrices, compute_uv).symbolic_call(x)\\n+    return _svd(x, full_matrices, compute_uv)\\n+\\n+\\n+def _svd(x, full_matrices=True, compute_uv=True):\\n+    x = backend.convert_to_tensor(x)\\n+    _assert_2d(x)\\n+    return backend.linalg.svd(x, full_matrices, compute_uv)\\n+\\n+\\n+def _assert_1d(*arrays):\\n+    for a in arrays:\\n+        if a.ndim < 1:\\n+            raise ValueError(\\n+                \\\"Expected input to have rank >= 1. \\\"\\n+                \\\"Received scalar input {a}.\\\"\\n+            )\\n+\\n+\\n+def _assert_2d(*arrays):\\n+    for a in arrays:\\n+        if a.ndim < 2:\\n+            raise ValueError(\\n+                \\\"Expected input to have rank >= 2. \\\"\\n+                \\\"Received input with shape {a.shape}.\\\"\\n+            )\\n+\\n+\\n+def _assert_square(*arrays):\\n+    for a in arrays:\\n+        m, n = a.shape[-2:]\\n+        if m != n:\\n+            raise ValueError(\\n+                \\\"Expected a square matrix. \\\"\\n+                f\\\"Received non-square input with shape {a.shape}\\\"\\n+            )\\n+\\n+\\n+def _assert_a_b_compat(a, b):\\n+    if a.ndim == b.ndim:\\n+        if a.shape[-2] != b.shape[-2]:\\n+            raise ValueError(\\n+                \\\"Incompatible shapes between `a` and `b`. \\\"\\n+                \\\"Expected `a.shape[-2] == b.shape[-2]`. \\\"\\n+                f\\\"Received: a.shape={a.shape}, b.shape={b.shape}\\\"\\n+            )\\n+    elif a.ndim == b.ndim - 1:\\n+        if a.shape[-1] != b.shape[-1]:\\n+            raise ValueError(\\n+                \\\"Incompatible shapes between `a` and `b`. \\\"\\n+                \\\"Expected `a.shape[-1] == b.shape[-1]`. \\\"\\n+                f\\\"Received: a.shape={a.shape}, b.shape={b.shape}\\\"\\n+            )\\ndiff --git a/keras/ops/math.py b/keras/ops/math.py\\nindex 6218a203d8f7..5d81eed75770 100644\\n--- a/keras/ops/math.py\\n+++ b/keras/ops/math.py\\n@@ -244,78 +244,6 @@ def logsumexp(x, axis=None, keepdims=False):\\n     return backend.math.logsumexp(x, axis=axis, keepdims=keepdims)\\n \\n \\n-class Qr(Operation):\\n-    def __init__(self, mode=\\\"reduced\\\"):\\n-        super().__init__()\\n-        if mode not in {\\\"reduced\\\", \\\"complete\\\"}:\\n-            raise ValueError(\\n-                \\\"`mode` argument value not supported. \\\"\\n-                \\\"Expected one of {'reduced', 'complete'}. \\\"\\n-                f\\\"Received: mode={mode}\\\"\\n-            )\\n-        self.mode = mode\\n-\\n-    def compute_output_spec(self, x):\\n-        if len(x.shape) < 2:\\n-            raise ValueError(\\n-                \\\"Input should have rank >= 2. Received: \\\"\\n-                f\\\"input.shape = {x.shape}\\\"\\n-            )\\n-        m = x.shape[-2]\\n-        n = x.shape[-1]\\n-        if m is None or n is None:\\n-            raise ValueError(\\n-                \\\"Input should have its last 2 dimensions \\\"\\n-                \\\"fully-defined. Received: \\\"\\n-                f\\\"input.shape = {x.shape}\\\"\\n-            )\\n-        k = min(m, n)\\n-        base = tuple(x.shape[:-2])\\n-        if self.mode == \\\"reduced\\\":\\n-            return (\\n-                KerasTensor(shape=base + (m, k), dtype=x.dtype),\\n-                KerasTensor(shape=base + (k, n), dtype=x.dtype),\\n-            )\\n-        # 'complete' mode.\\n-        return (\\n-            KerasTensor(shape=base + (m, m), dtype=x.dtype),\\n-            KerasTensor(shape=base + (m, n), dtype=x.dtype),\\n-        )\\n-\\n-    def call(self, x):\\n-        return backend.math.qr(x, mode=self.mode)\\n-\\n-\\n-@keras_export(\\\"keras.ops.qr\\\")\\n-def qr(x, mode=\\\"reduced\\\"):\\n-    \\\"\\\"\\\"Computes the QR decomposition of a tensor.\\n-\\n-    Args:\\n-        x: Input tensor.\\n-        mode: A string specifying the mode of the QR decomposition.\\n-            - 'reduced': Returns the reduced QR decomposition. (default)\\n-            - 'complete': Returns the complete QR decomposition.\\n-\\n-    Returns:\\n-        A tuple containing two tensors. The first tensor represents the\\n-        orthogonal matrix Q, and the second tensor represents the upper\\n-        triangular matrix R.\\n-\\n-    Example:\\n-\\n-    >>> x = keras.ops.convert_to_tensor([[1., 2.], [3., 4.], [5., 6.]])\\n-    >>> q, r = qr(x)\\n-    >>> print(q)\\n-    array([[-0.16903079  0.897085]\\n-           [-0.5070925   0.2760267 ]\\n-           [-0.8451542  -0.34503305]], shape=(3, 2), dtype=float32)\\n-    \\\"\\\"\\\"\\n-\\n-    if any_symbolic_tensors((x,)):\\n-        return Qr(mode=mode).symbolic_call(x)\\n-    return backend.math.qr(x, mode=mode)\\n-\\n-\\n class ExtractSequences(Operation):\\n     def __init__(self, sequence_length, sequence_stride):\\n         super().__init__()\\n@@ -996,157 +924,3 @@ def erfinv(x):\\n         return Erfinv().symbolic_call(x)\\n     x = backend.convert_to_tensor(x)\\n     return backend.math.erfinv(x)\\n-\\n-\\n-class Solve(Operation):\\n-    def call(self, a, b):\\n-        a = backend.convert_to_tensor(a)\\n-        b = backend.convert_to_tensor(b)\\n-        return backend.math.solve(a, b)\\n-\\n-    def compute_output_spec(self, a, b):\\n-        return KerasTensor(shape=a.shape, dtype=a.dtype)\\n-\\n-\\n-@keras_export(\\\"keras.ops.solve\\\")\\n-def solve(a, b):\\n-    \\\"\\\"\\\"Solves for `x` in the equation `a * x = b`.\\n-\\n-    Args:\\n-        a: Input tensor.\\n-        b: Input tensor.\\n-\\n-    Returns:\\n-        A tensor with the same shape and dtype as `a`.\\n-\\n-    Example:\\n-\\n-    >>> a = np.array([[1, 2], [4, 5]], dtype=\\\"float32\\\")\\n-    >>> b = np.array([[2, 4], [8, 10]], dtype=\\\"float32\\\")\\n-    >>> keras.ops.solve(x1, x2)\\n-    array([[2, 0], [0, 2]], dtype=\\\"float32\\\")\\n-    \\\"\\\"\\\"\\n-    if any_symbolic_tensors((a, b)):\\n-        return Solve().symbolic_call(a, b)\\n-    a = backend.convert_to_tensor(a)\\n-    b = backend.convert_to_tensor(b)\\n-    return backend.math.solve(a, b)\\n-\\n-\\n-class Norm(Operation):\\n-    def __init__(self, ord=None, axis=None, keepdims=False):\\n-        super().__init__()\\n-        if isinstance(ord, str):\\n-            if ord not in (\\\"fro\\\", \\\"nuc\\\"):\\n-                raise ValueError(\\n-                    \\\"Invalid `ord` argument. \\\"\\n-                    \\\"Expected one of {'fro', 'nuc'} when using string. \\\"\\n-                    f\\\"Received: ord={ord}\\\"\\n-                )\\n-        if isinstance(axis, int):\\n-            axis = [axis]\\n-        self.ord = ord\\n-        self.axis = axis\\n-        self.keepdims = keepdims\\n-\\n-    def compute_output_spec(self, x):\\n-        output_dtype = backend.standardize_dtype(x.dtype)\\n-        if \\\"int\\\" in output_dtype or output_dtype == \\\"bool\\\":\\n-            output_dtype = backend.floatx()\\n-        if self.axis is None:\\n-            axis = tuple(range(len(x.shape)))\\n-        else:\\n-            axis = self.axis\\n-        num_axes = len(axis)\\n-        if num_axes == 1 and isinstance(self.ord, str):\\n-            raise ValueError(\\n-                \\\"Invalid `ord` argument for vector norm. \\\"\\n-                f\\\"Received: ord={self.ord}\\\"\\n-            )\\n-        elif num_axes == 2 and self.ord not in (\\n-            None,\\n-            \\\"fro\\\",\\n-            \\\"nuc\\\",\\n-            float(\\\"inf\\\"),\\n-            float(\\\"-inf\\\"),\\n-            1,\\n-            -1,\\n-            2,\\n-            -2,\\n-        ):\\n-            raise ValueError(\\n-                \\\"Invalid `ord` argument for matrix norm. \\\"\\n-                f\\\"Received: ord={self.ord}\\\"\\n-            )\\n-        return KerasTensor(\\n-            reduce_shape(x.shape, axis=self.axis, keepdims=self.keepdims),\\n-            dtype=output_dtype,\\n-        )\\n-\\n-    def call(self, x):\\n-        x = backend.convert_to_tensor(x)\\n-        return backend.math.norm(\\n-            x, ord=self.ord, axis=self.axis, keepdims=self.keepdims\\n-        )\\n-\\n-\\n-@keras_export(\\\"keras.ops.norm\\\")\\n-def norm(x, ord=None, axis=None, keepdims=False):\\n-    \\\"\\\"\\\"Matrix or vector norm.\\n-\\n-    This function is able to return one of eight different matrix norms, or one\\n-    of an infinite number of vector norms (described below), depending on the\\n-    value of the `ord` parameter.\\n-\\n-    Args:\\n-        x: Input tensor.\\n-        ord: Order of the norm (see table under Notes). The default is `None`.\\n-        axis: If `axis` is an integer, it specifies the axis of `x` along which\\n-            to compute the vector norms. If `axis` is a 2-tuple, it specifies\\n-            the axes that hold 2-D matrices, and the matrix norms of these\\n-            matrices are computed.\\n-        keepdims: If this is set to `True`, the axes which are reduced are left\\n-            in the result as dimensions with size one.\\n-\\n-    Note:\\n-        For values of `ord < 1`, the result is, strictly speaking, not a\\n-        mathematical 'norm', but it may still be useful for various numerical\\n-        purposes. The following norms can be calculated:\\n-        - For matrices:\\n-            - `ord=None`: Frobenius norm\\n-            - `ord=\\\"fro\\\"`: Frobenius norm\\n-            - `ord=nuc`: nuclear norm\\n-            - `ord=np.inf`: `max(sum(abs(x), axis=1))`\\n-            - `ord=-np.inf`: `min(sum(abs(x), axis=1))`\\n-            - `ord=0`: not supported\\n-            - `ord=1`: `max(sum(abs(x), axis=0))`\\n-            - `ord=-1`: `min(sum(abs(x), axis=0))`\\n-            - `ord=2`: 2-norm (largest sing. value)\\n-            - `ord=-2`: smallest singular value\\n-            - other: not supported\\n-        - For vectors:\\n-            - `ord=None`: 2-norm\\n-            - `ord=\\\"fro\\\"`: not supported\\n-            - `ord=nuc`: not supported\\n-            - `ord=np.inf`: `max(abs(x))`\\n-            - `ord=-np.inf`: `min(abs(x))`\\n-            - `ord=0`: `sum(x != 0)`\\n-            - `ord=1`: as below\\n-            - `ord=-1`: as below\\n-            - `ord=2`: as below\\n-            - `ord=-2`: as below\\n-            - other: `sum(abs(x)**ord)**(1./ord)`\\n-\\n-    Returns:\\n-        Norm of the matrix or vector(s).\\n-\\n-    Example:\\n-\\n-    >>> x = keras.ops.reshape(keras.ops.arange(9, dtype=\\\"float32\\\") - 4, (3, 3))\\n-    >>> keras.ops.norm(x)\\n-    7.7459664\\n-    \\\"\\\"\\\"\\n-    if any_symbolic_tensors((x,)):\\n-        return Norm(ord=ord, axis=axis, keepdims=keepdims).symbolic_call(x)\\n-    x = backend.convert_to_tensor(x)\\n-    return backend.math.norm(x, ord=ord, axis=axis, keepdims=keepdims)\\n\",\n",
      "    \"test_patch\": \"diff --git a/keras/ops/linalg_test.py b/keras/ops/linalg_test.py\\nnew file mode 100644\\nindex 000000000000..f0146d63f665\\n--- /dev/null\\n+++ b/keras/ops/linalg_test.py\\n@@ -0,0 +1,519 @@\\n+import numpy as np\\n+from absl.testing import parameterized\\n+\\n+from keras import backend\\n+from keras import ops\\n+from keras import testing\\n+from keras.backend.common.keras_tensor import KerasTensor\\n+from keras.ops import linalg\\n+from keras.testing.test_utils import named_product\\n+\\n+\\n+class LinalgOpsDynamicShapeTest(testing.TestCase):\\n+    def test_cholesky(self):\\n+        x = KerasTensor([None, 20, 20])\\n+        out = linalg.cholesky(x)\\n+        self.assertEqual(out.shape, (None, 20, 20))\\n+\\n+        x = KerasTensor([None, None, 20])\\n+        with self.assertRaises(ValueError):\\n+            linalg.cholesky(x)\\n+\\n+        x = KerasTensor([None, 20, 15])\\n+        with self.assertRaises(ValueError):\\n+            linalg.cholesky(x)\\n+\\n+    def test_det(self):\\n+        x = KerasTensor([None, 20, 20])\\n+        out = linalg.det(x)\\n+        self.assertEqual(out.shape, (None,))\\n+\\n+        x = KerasTensor([None, None, 20])\\n+        with self.assertRaises(ValueError):\\n+            linalg.det(x)\\n+\\n+        x = KerasTensor([None, 20, 15])\\n+        with self.assertRaises(ValueError):\\n+            linalg.det(x)\\n+\\n+    def test_eig(self):\\n+        x = KerasTensor([None, 20, 20])\\n+        w, v = linalg.eig(x)\\n+        self.assertEqual(w.shape, (None, 20))\\n+        self.assertEqual(v.shape, (None, 20, 20))\\n+\\n+        x = KerasTensor([None, None, 20])\\n+        with self.assertRaises(ValueError):\\n+            linalg.eig(x)\\n+\\n+        x = KerasTensor([None, 20, 15])\\n+        with self.assertRaises(ValueError):\\n+            linalg.eig(x)\\n+\\n+    def test_inv(self):\\n+        x = KerasTensor([None, 20, 20])\\n+        out = linalg.inv(x)\\n+        self.assertEqual(out.shape, (None, 20, 20))\\n+\\n+        x = KerasTensor([None, None, 20])\\n+        with self.assertRaises(ValueError):\\n+            linalg.inv(x)\\n+\\n+        x = KerasTensor([None, 20, 15])\\n+        with self.assertRaises(ValueError):\\n+            linalg.inv(x)\\n+\\n+    def test_lu_factor(self):\\n+        x = KerasTensor([None, 4, 3])\\n+        lu, p = linalg.lu_factor(x)\\n+        self.assertEqual(lu.shape, (None, 4, 3))\\n+        self.assertEqual(p.shape, (None, 3))\\n+\\n+        x = KerasTensor([None, 2, 3])\\n+        lu, p = linalg.lu_factor(x)\\n+        self.assertEqual(lu.shape, (None, 2, 3))\\n+        self.assertEqual(p.shape, (None, 2))\\n+\\n+    def test_norm(self):\\n+        x = KerasTensor((None, 3))\\n+        self.assertEqual(linalg.norm(x).shape, ())\\n+\\n+        x = KerasTensor((None, 3, 3))\\n+        self.assertEqual(linalg.norm(x, axis=1).shape, (None, 3))\\n+        self.assertEqual(\\n+            linalg.norm(x, axis=1, keepdims=True).shape, (None, 1, 3)\\n+        )\\n+\\n+    def test_qr(self):\\n+        x = KerasTensor((None, 4, 3), dtype=\\\"float32\\\")\\n+        q, r = linalg.qr(x, mode=\\\"reduced\\\")\\n+        qref, rref = np.linalg.qr(np.ones((2, 4, 3)), mode=\\\"reduced\\\")\\n+        qref_shape = (None,) + qref.shape[1:]\\n+        rref_shape = (None,) + rref.shape[1:]\\n+        self.assertEqual(q.shape, qref_shape)\\n+        self.assertEqual(r.shape, rref_shape)\\n+\\n+        q, r = linalg.qr(x, mode=\\\"complete\\\")\\n+        qref, rref = np.linalg.qr(np.ones((2, 4, 3)), mode=\\\"complete\\\")\\n+        qref_shape = (None,) + qref.shape[1:]\\n+        rref_shape = (None,) + rref.shape[1:]\\n+        self.assertEqual(q.shape, qref_shape)\\n+        self.assertEqual(r.shape, rref_shape)\\n+\\n+    def test_solve(self):\\n+        a = KerasTensor([None, 20, 20])\\n+        b = KerasTensor([None, 20, 5])\\n+        out = linalg.solve(a, b)\\n+        self.assertEqual(out.shape, (None, 20, 5))\\n+\\n+        a = KerasTensor([None, 20, 20])\\n+        b = KerasTensor([None, 20])\\n+        out = linalg.solve(a, b)\\n+        self.assertEqual(out.shape, (None, 20))\\n+\\n+        a = KerasTensor([None, None, 20])\\n+        b = KerasTensor([None, 20, 5])\\n+        with self.assertRaises(ValueError):\\n+            linalg.solve(a, b)\\n+\\n+        a = KerasTensor([None, 20, 15])\\n+        b = KerasTensor([None, 20, 5])\\n+        with self.assertRaises(ValueError):\\n+            linalg.solve(a, b)\\n+\\n+        a = KerasTensor([None, 20, 20])\\n+        b = KerasTensor([None, None, 5])\\n+        with self.assertRaises(ValueError):\\n+            linalg.solve(a, b)\\n+\\n+    def test_solve_triangular(self):\\n+        a = KerasTensor([None, 20, 20])\\n+        b = KerasTensor([None, 20, 5])\\n+        out = linalg.solve_triangular(a, b)\\n+        self.assertEqual(out.shape, (None, 20, 5))\\n+\\n+        a = KerasTensor([None, 20, 20])\\n+        b = KerasTensor([None, 20])\\n+        out = linalg.solve_triangular(a, b)\\n+        self.assertEqual(out.shape, (None, 20))\\n+\\n+        a = KerasTensor([None, 20, 20])\\n+        b = KerasTensor([None, 20, 5])\\n+        out = linalg.solve_triangular(a, b, lower=True)\\n+        self.assertEqual(out.shape, (None, 20, 5))\\n+\\n+        a = KerasTensor([None, 20, 20])\\n+        b = KerasTensor([None, 20])\\n+        out = linalg.solve_triangular(a, b, lower=True)\\n+        self.assertEqual(out.shape, (None, 20))\\n+\\n+        a = KerasTensor([None, 20, 15])\\n+        b = KerasTensor([None, 20, 5])\\n+        with self.assertRaises(ValueError):\\n+            linalg.solve_triangular(a, b)\\n+\\n+        a = KerasTensor([None, 20, 20])\\n+        b = KerasTensor([None, None, 5])\\n+        with self.assertRaises(ValueError):\\n+            linalg.solve_triangular(a, b)\\n+\\n+    def test_svd(self):\\n+        x = KerasTensor((None, 3, 2))\\n+        u, s, v = linalg.svd(x)\\n+        self.assertEqual(u.shape, (None, 3, 3))\\n+        self.assertEqual(s.shape, (None, 2))\\n+        self.assertEqual(v.shape, (None, 2, 2))\\n+\\n+        u, s, v = linalg.svd(x, full_matrices=False)\\n+        self.assertEqual(u.shape, (None, 3, 2))\\n+        self.assertEqual(s.shape, (None, 2))\\n+        self.assertEqual(v.shape, (None, 2, 2))\\n+\\n+        s = linalg.svd(x, compute_uv=False)\\n+        self.assertEqual(s.shape, (None, 2))\\n+\\n+\\n+class LinalgOpsStaticShapeTest(testing.TestCase):\\n+    def test_cholesky(self):\\n+        x = KerasTensor([4, 3, 3])\\n+        out = linalg.cholesky(x)\\n+        self.assertEqual(out.shape, (4, 3, 3))\\n+\\n+        x = KerasTensor([10, 20, 15])\\n+        with self.assertRaises(ValueError):\\n+            linalg.cholesky(x)\\n+\\n+    def test_det(self):\\n+        x = KerasTensor([4, 3, 3])\\n+        out = linalg.det(x)\\n+        self.assertEqual(out.shape, (4,))\\n+\\n+        x = KerasTensor([10, 20, 15])\\n+        with self.assertRaises(ValueError):\\n+            linalg.det(x)\\n+\\n+    def test_eig(self):\\n+        x = KerasTensor([4, 3, 3])\\n+        w, v = linalg.eig(x)\\n+        self.assertEqual(w.shape, (4, 3))\\n+        self.assertEqual(v.shape, (4, 3, 3))\\n+\\n+        x = KerasTensor([10, 20, 15])\\n+        with self.assertRaises(ValueError):\\n+            linalg.eig(x)\\n+\\n+    def test_inv(self):\\n+        x = KerasTensor([4, 3, 3])\\n+        out = linalg.inv(x)\\n+        self.assertEqual(out.shape, (4, 3, 3))\\n+\\n+        x = KerasTensor([10, 20, 15])\\n+        with self.assertRaises(ValueError):\\n+            linalg.inv(x)\\n+\\n+    def test_lu_factor(self):\\n+        x = KerasTensor([10, 4, 3])\\n+        lu, p = linalg.lu_factor(x)\\n+        self.assertEqual(lu.shape, (10, 4, 3))\\n+        self.assertEqual(p.shape, (10, 3))\\n+\\n+        x = KerasTensor([10, 2, 3])\\n+        lu, p = linalg.lu_factor(x)\\n+        self.assertEqual(lu.shape, (10, 2, 3))\\n+        self.assertEqual(p.shape, (10, 2))\\n+\\n+    def test_norm(self):\\n+        x = KerasTensor((10, 3))\\n+        self.assertEqual(linalg.norm(x).shape, ())\\n+\\n+        x = KerasTensor((10, 3, 3))\\n+        self.assertEqual(linalg.norm(x, axis=1).shape, (10, 3))\\n+        self.assertEqual(\\n+            linalg.norm(x, axis=1, keepdims=True).shape, (10, 1, 3)\\n+        )\\n+\\n+    def test_qr(self):\\n+        x = KerasTensor((4, 3), dtype=\\\"float32\\\")\\n+        q, r = linalg.qr(x, mode=\\\"reduced\\\")\\n+        qref, rref = np.linalg.qr(np.ones((4, 3)), mode=\\\"reduced\\\")\\n+        self.assertEqual(q.shape, qref.shape)\\n+        self.assertEqual(r.shape, rref.shape)\\n+\\n+        q, r = linalg.qr(x, mode=\\\"complete\\\")\\n+        qref, rref = np.linalg.qr(np.ones((4, 3)), mode=\\\"complete\\\")\\n+        self.assertEqual(q.shape, qref.shape)\\n+        self.assertEqual(r.shape, rref.shape)\\n+\\n+        with self.assertRaises(ValueError):\\n+            linalg.qr(x, mode=\\\"invalid\\\")\\n+\\n+    def test_solve(self):\\n+        a = KerasTensor([4, 3, 3])\\n+        b = KerasTensor([4, 3, 5])\\n+        out = linalg.solve(a, b)\\n+        self.assertEqual(out.shape, (4, 3, 5))\\n+\\n+        a = KerasTensor([4, 3, 3])\\n+        b = KerasTensor([4, 3])\\n+        out = linalg.solve(a, b)\\n+        self.assertEqual(out.shape, (4, 3))\\n+\\n+        a = KerasTensor([10, 20, 15])\\n+        b = KerasTensor([10, 20, 5])\\n+        with self.assertRaises(ValueError):\\n+            linalg.solve(a, b)\\n+\\n+        a = KerasTensor([20, 20])\\n+        b = KerasTensor([])\\n+        with self.assertRaises(ValueError):\\n+            linalg.solve(a, b)\\n+\\n+    def test_solve_triangular(self):\\n+        a = KerasTensor([4, 3, 3])\\n+        b = KerasTensor([4, 3, 5])\\n+        out = linalg.solve_triangular(a, b)\\n+        self.assertEqual(out.shape, (4, 3, 5))\\n+\\n+        a = KerasTensor([4, 3, 3])\\n+        b = KerasTensor([4, 3])\\n+        out = linalg.solve_triangular(a, b)\\n+        self.assertEqual(out.shape, (4, 3))\\n+\\n+        a = KerasTensor([10, 20, 15])\\n+        b = KerasTensor([10, 20, 5])\\n+        with self.assertRaises(ValueError):\\n+            linalg.solve_triangular(a, b)\\n+\\n+    def test_svd(self):\\n+        x = KerasTensor((10, 3, 2))\\n+        u, s, v = linalg.svd(x)\\n+        self.assertEqual(u.shape, (10, 3, 3))\\n+        self.assertEqual(s.shape, (10, 2))\\n+        self.assertEqual(v.shape, (10, 2, 2))\\n+\\n+        u, s, v = linalg.svd(x, full_matrices=False)\\n+        self.assertEqual(u.shape, (10, 3, 2))\\n+        self.assertEqual(s.shape, (10, 2))\\n+        self.assertEqual(v.shape, (10, 2, 2))\\n+\\n+        s = linalg.svd(x, compute_uv=False)\\n+        self.assertEqual(s.shape, (10, 2))\\n+\\n+\\n+class LinalgOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\\n+\\n+    def test_cholesky(self):\\n+        x = np.random.rand(4, 3, 3).astype(\\\"float32\\\")\\n+        with self.assertRaises(ValueError):\\n+            linalg.cholesky(x)\\n+        x_psd = x @ x.transpose((0, 2, 1)) + 1e-5 * np.eye(3)\\n+        out = linalg.cholesky(x_psd)\\n+        self.assertAllClose(out, np.linalg.cholesky(x_psd), atol=1e-4)\\n+\\n+    def test_det(self):\\n+        x = np.random.rand(4, 3, 3)\\n+        out = linalg.det(x)\\n+        self.assertAllClose(out, np.linalg.det(x), atol=1e-5)\\n+\\n+        with self.assertRaises(ValueError):\\n+            x = np.random.rand(4, 3, 4)\\n+            linalg.det(x)\\n+\\n+    def test_eig(self):\\n+        x = np.random.rand(2, 3, 3)\\n+        x = x @ x.transpose((0, 2, 1))\\n+        if backend.backend() == \\\"jax\\\":\\n+            import jax\\n+\\n+            if jax.default_backend() == \\\"gpu\\\":\\n+                # eig not implemented for jax on gpu backend\\n+                with self.assertRaises(NotImplementedError):\\n+                    linalg.eig(x)\\n+                return\\n+        w, v = map(ops.convert_to_numpy, linalg.eig(x))\\n+        x_reconstructed = (v * w[..., None, :]) @ v.transpose((0, 2, 1))\\n+        self.assertAllClose(x_reconstructed, x, atol=1e-4)\\n+\\n+    def test_inv(self):\\n+        x = np.random.rand(4, 3, 3)\\n+        x_inv = ops.convert_to_numpy(linalg.inv(x))\\n+        x_reconstructed = x @ x_inv\\n+        # high tolerance due to numerical instability\\n+        self.assertAllClose(\\n+            x_reconstructed, np.repeat(np.eye(3)[None], 4, 0), atol=1e-3\\n+        )\\n+\\n+    def test_lu_factor(self):\\n+        def _pivot_matrix(pivots, n):\\n+            p_matrix = np.eye(n)\\n+            for i, p in enumerate(pivots):\\n+                identity = np.eye(n, n)\\n+                q = identity[i, :].copy()\\n+                identity[i, :] = identity[p, :]\\n+                identity[p, :] = q\\n+                p_matrix = np.dot(p_matrix, identity)\\n+            return p_matrix\\n+\\n+        def _reconstruct(lu, pivots, m, n):\\n+            lower = np.tril(lu[:, : min(m, n)], -1) + np.eye(m, min(m, n))\\n+            upper = np.triu(lu[: min(m, n)])\\n+\\n+            # pivots are defined differently in tensorflow\\n+            # compared to the other backends\\n+            if backend.backend() == \\\"tensorflow\\\":\\n+                p_matrix = np.eye(m)[pivots]\\n+            else:\\n+                p_matrix = _pivot_matrix(pivots, m)\\n+            out = p_matrix @ lower @ upper\\n+            return out\\n+\\n+        m, n = 4, 4\\n+        x = np.random.rand(m, n)\\n+        lu, pivots = map(ops.convert_to_numpy, linalg.lu_factor(x))\\n+        x_reconstructed = _reconstruct(lu, pivots, m, n)\\n+        self.assertAllClose(x_reconstructed, x, atol=1e-5)\\n+\\n+        m, n = 4, 3\\n+        x = np.random.rand(m, n)\\n+        if backend.backend() == \\\"tensorflow\\\":\\n+            with self.assertRaises(ValueError):\\n+                linalg.lu_factor(x)\\n+        else:\\n+            lu, pivots = map(ops.convert_to_numpy, linalg.lu_factor(x))\\n+            x_reconstructed = _reconstruct(lu, pivots, m, n)\\n+            self.assertAllClose(x_reconstructed, x, atol=1e-5)\\n+\\n+        # batched case\\n+        m, n = 3, 4\\n+        x = np.random.rand(2, m, n)\\n+        if backend.backend() == \\\"tensorflow\\\":\\n+            with self.assertRaises(ValueError):\\n+                linalg.lu_factor(x)\\n+        else:\\n+            lu, pivots = map(ops.convert_to_numpy, linalg.lu_factor(x))\\n+            for i in range(2):\\n+                self.assertAllClose(\\n+                    _reconstruct(lu[i], pivots[i], m, n), x[i], atol=1e-5\\n+                )\\n+\\n+    @parameterized.named_parameters(\\n+        named_product(\\n+            ord=[None, \\\"fro\\\", \\\"nuc\\\", -np.inf, -2, -1, 0, 1, 2, np.inf, 3],\\n+            axis=[None, 1, -1],\\n+            keepdims=[False, True],\\n+        )\\n+    )\\n+    def test_norm_vectors(self, ord, axis, keepdims):\\n+        if axis is None:\\n+            x = np.random.random((5,))\\n+        else:\\n+            x = np.random.random((5, 6))\\n+        if ord in (\\\"fro\\\", \\\"nuc\\\"):\\n+            error = RuntimeError if backend.backend() == \\\"torch\\\" else ValueError\\n+            with self.assertRaises(error):\\n+                linalg.norm(x, ord=ord, axis=axis, keepdims=keepdims)\\n+            return\\n+        output = linalg.norm(x, ord=ord, axis=axis, keepdims=keepdims)\\n+        expected_result = np.linalg.norm(\\n+            x, ord=ord, axis=axis, keepdims=keepdims\\n+        )\\n+        self.assertAllClose(output, expected_result)\\n+\\n+    def test_qr(self):\\n+        x = np.random.random((4, 5))\\n+        q, r = linalg.qr(x, mode=\\\"reduced\\\")\\n+        qref, rref = np.linalg.qr(x, mode=\\\"reduced\\\")\\n+        self.assertAllClose(qref, q)\\n+        self.assertAllClose(rref, r)\\n+\\n+        q, r = linalg.qr(x, mode=\\\"complete\\\")\\n+        qref, rref = np.linalg.qr(x, mode=\\\"complete\\\")\\n+        self.assertAllClose(qref, q)\\n+        self.assertAllClose(rref, r)\\n+\\n+    def test_solve(self):\\n+        x1 = np.array([[1, 2], [4, 5]], dtype=\\\"float32\\\")\\n+        x2 = np.array([[2, 4], [8, 10]], dtype=\\\"float32\\\")\\n+        output = linalg.solve(x1, x2)\\n+        expected_result = np.array([[2, 0], [0, 2]], dtype=\\\"float32\\\")\\n+        self.assertAllClose(output, expected_result)\\n+\\n+    def test_solve_triangular(self):\\n+\\n+        # 2d-case\\n+        x1 = np.array([[1, 2], [0, 5]], dtype=\\\"float32\\\")\\n+        x2 = np.array([2, 10], dtype=\\\"float32\\\")\\n+        output = linalg.solve_triangular(x1, x2, lower=True)\\n+        expected_result = np.array([2, 2], dtype=\\\"float32\\\")\\n+        self.assertAllClose(output, expected_result)\\n+\\n+        output = linalg.solve_triangular(x1, x2, lower=False)\\n+        expected_result = np.array([-2, 2], dtype=\\\"float32\\\")\\n+        self.assertAllClose(output, expected_result)\\n+\\n+        # batched case\\n+        x1 = np.array([[[1, 2], [0, 5]], [[1, 2], [0, 5]]], dtype=\\\"float32\\\")\\n+        x2 = np.array([[2, 10], [2, 10]], dtype=\\\"float32\\\")\\n+        output = linalg.solve_triangular(x1, x2, lower=True)\\n+        expected_result = np.array([[2, 2], [2, 2]], dtype=\\\"float32\\\")\\n+        self.assertAllClose(output, expected_result)\\n+\\n+    def test_svd(self):\\n+        x = np.random.rand(4, 30, 20)\\n+        u, s, vh = linalg.svd(x)\\n+        x_reconstructed = (u[..., :, : s.shape[-1]] * s[..., None, :]) @ vh[\\n+            ..., : s.shape[-1], :\\n+        ]\\n+        self.assertAllClose(x_reconstructed, x, atol=1e-4)\\n+\\n+\\n+class QrOpTest(testing.TestCase):\\n+    def test_qr_init_mode_reduced(self):\\n+        qr_op = linalg.Qr(mode=\\\"reduced\\\")\\n+        self.assertIsNotNone(qr_op)\\n+\\n+    def test_qr_init_mode_complete(self):\\n+        qr_op = linalg.Qr(mode=\\\"complete\\\")\\n+        self.assertIsNotNone(qr_op)\\n+\\n+    def test_qr_init_invalid_mode(self):\\n+        invalid_mode = \\\"invalid_mode\\\"\\n+        expected_error = (\\n+            r\\\"`mode` argument value not supported. \\\"\\n+            r\\\"Expected one of \\\\{'reduced', 'complete'\\\\}. \\\"\\n+            f\\\"Received: mode={invalid_mode}\\\"\\n+        )\\n+        with self.assertRaisesRegex(ValueError, expected_error):\\n+            linalg.Qr(mode=invalid_mode)\\n+\\n+    def test_compute_output_spec_low_rank(self):\\n+        qr_op = linalg.Qr(mode=\\\"reduced\\\")\\n+        low_rank_input = np.random.rand(3)\\n+        with self.assertRaisesRegex(\\n+            ValueError, r\\\"Input should have rank >= 2. Received: .*\\\"\\n+        ):\\n+            qr_op.compute_output_spec(low_rank_input)\\n+\\n+    def test_compute_output_spec_undefined_dimensions(self):\\n+        qr_op = linalg.Qr(mode=\\\"reduced\\\")\\n+        undefined_dim_input = KerasTensor(shape=(None, 4), dtype=\\\"float32\\\")\\n+        with self.assertRaisesRegex(\\n+            ValueError,\\n+            r\\\"Input should have its last 2 dimensions \\\"\\n+            r\\\"fully-defined. Received: .*\\\",\\n+        ):\\n+            qr_op.compute_output_spec(undefined_dim_input)\\n+\\n+    def test_qr_call_mode_reduced(self):\\n+        qr_op = linalg.Qr(mode=\\\"reduced\\\")\\n+        test_input = np.random.rand(10, 10)\\n+        q, r = qr_op.call(test_input)\\n+        self.assertEqual(q.shape, (10, 10))\\n+        self.assertEqual(r.shape, (10, 10))\\n+\\n+    def test_qr_call_mode_complete(self):\\n+        qr_op = linalg.Qr(mode=\\\"complete\\\")\\n+        test_input = np.random.rand(10, 10)\\n+        q, r = qr_op.call(test_input)\\n+        self.assertEqual(q.shape, (10, 10))\\n+        self.assertEqual(r.shape, (10, 10))\\ndiff --git a/keras/ops/math_test.py b/keras/ops/math_test.py\\nindex f80d7a22d526..937b10095744 100644\\n--- a/keras/ops/math_test.py\\n+++ b/keras/ops/math_test.py\\n@@ -7,12 +7,9 @@\\n \\n from keras import backend\\n from keras import testing\\n-from keras.backend.common import standardize_dtype\\n from keras.backend.common.keras_tensor import KerasTensor\\n from keras.backend.common.variables import ALLOWED_DTYPES\\n from keras.ops import math as kmath\\n-from keras.ops import numpy as knp\\n-from keras.testing.test_utils import named_product\\n \\n \\n def _stft(\\n@@ -179,22 +176,6 @@ def test_logsumexp(self):\\n         result = kmath.logsumexp(x)\\n         self.assertEqual(result.shape, ())\\n \\n-    def test_qr(self):\\n-        x = KerasTensor((None, 4, 3), dtype=\\\"float32\\\")\\n-        q, r = kmath.qr(x, mode=\\\"reduced\\\")\\n-        qref, rref = np.linalg.qr(np.ones((2, 4, 3)), mode=\\\"reduced\\\")\\n-        qref_shape = (None,) + qref.shape[1:]\\n-        rref_shape = (None,) + rref.shape[1:]\\n-        self.assertEqual(q.shape, qref_shape)\\n-        self.assertEqual(r.shape, rref_shape)\\n-\\n-        q, r = kmath.qr(x, mode=\\\"complete\\\")\\n-        qref, rref = np.linalg.qr(np.ones((2, 4, 3)), mode=\\\"complete\\\")\\n-        qref_shape = (None,) + qref.shape[1:]\\n-        rref_shape = (None,) + rref.shape[1:]\\n-        self.assertEqual(q.shape, qref_shape)\\n-        self.assertEqual(r.shape, rref_shape)\\n-\\n     def test_extract_sequences(self):\\n         # Defined dimension\\n         x = KerasTensor((None, 32), dtype=\\\"float32\\\")\\n@@ -285,16 +266,6 @@ def test_rsqrt(self):\\n         x = KerasTensor([None, 3])\\n         self.assertEqual(kmath.rsqrt(x).shape, (None, 3))\\n \\n-    def test_norm(self):\\n-        x = KerasTensor((None, 3))\\n-        self.assertEqual(kmath.norm(x).shape, ())\\n-\\n-        x = KerasTensor((None, 3, 3))\\n-        self.assertEqual(kmath.norm(x, axis=1).shape, (None, 3))\\n-        self.assertEqual(\\n-            kmath.norm(x, axis=1, keepdims=True).shape, (None, 1, 3)\\n-        )\\n-\\n \\n class MathOpsStaticShapeTest(testing.TestCase):\\n     @pytest.mark.skipif(\\n@@ -345,18 +316,6 @@ def test_logsumexp(self):\\n         result = kmath.logsumexp(x)\\n         self.assertEqual(result.shape, ())\\n \\n-    def test_qr(self):\\n-        x = KerasTensor((4, 3), dtype=\\\"float32\\\")\\n-        q, r = kmath.qr(x, mode=\\\"reduced\\\")\\n-        qref, rref = np.linalg.qr(np.ones((4, 3)), mode=\\\"reduced\\\")\\n-        self.assertEqual(q.shape, qref.shape)\\n-        self.assertEqual(r.shape, rref.shape)\\n-\\n-        q, r = kmath.qr(x, mode=\\\"complete\\\")\\n-        qref, rref = np.linalg.qr(np.ones((4, 3)), mode=\\\"complete\\\")\\n-        self.assertEqual(q.shape, qref.shape)\\n-        self.assertEqual(r.shape, rref.shape)\\n-\\n     def test_extract_sequences(self):\\n         x = KerasTensor((10, 16), dtype=\\\"float32\\\")\\n         sequence_length = 3\\n@@ -433,16 +392,6 @@ def test_istft(self):\\n         )\\n         self.assertEqual(output.shape, ref.shape)\\n \\n-    def test_solve(self):\\n-        x1 = KerasTensor((2, 2), dtype=\\\"float32\\\")\\n-        x2 = KerasTensor((2, 2), dtype=\\\"float32\\\")\\n-        outputs = kmath.solve(x1, x2)\\n-        self.assertEqual(outputs.shape, (2, 2))\\n-\\n-    def test_norm(self):\\n-        x = KerasTensor((2, 3))\\n-        self.assertEqual(kmath.norm(x).shape, ())\\n-\\n \\n class MathOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\\n     @pytest.mark.skipif(\\n@@ -642,18 +591,6 @@ def test_logsumexp(self):\\n         expected = np.log(np.sum(np.exp(x), axis=1))\\n         self.assertAllClose(outputs, expected)\\n \\n-    def test_qr(self):\\n-        x = np.random.random((4, 5))\\n-        q, r = kmath.qr(x, mode=\\\"reduced\\\")\\n-        qref, rref = np.linalg.qr(x, mode=\\\"reduced\\\")\\n-        self.assertAllClose(qref, q)\\n-        self.assertAllClose(rref, r)\\n-\\n-        q, r = kmath.qr(x, mode=\\\"complete\\\")\\n-        qref, rref = np.linalg.qr(x, mode=\\\"complete\\\")\\n-        self.assertAllClose(qref, q)\\n-        self.assertAllClose(rref, r)\\n-\\n     def test_extract_sequences(self):\\n         # Test 1D case.\\n         x = np.random.random((10,))\\n@@ -924,59 +861,6 @@ def test_erfinv_operation_edge_cases(self):\\n             expected_output, output_from_edge_erfinv_op, atol=1e-4\\n         )\\n \\n-    def test_solve(self):\\n-        x1 = np.array([[1, 2], [4, 5]], dtype=\\\"float32\\\")\\n-        x2 = np.array([[2, 4], [8, 10]], dtype=\\\"float32\\\")\\n-        output = kmath.solve(x1, x2)\\n-        expected_result = np.array([[2, 0], [0, 2]], dtype=\\\"float32\\\")\\n-        self.assertAllClose(output, expected_result)\\n-\\n-    @parameterized.named_parameters(\\n-        named_product(\\n-            ord=[None, \\\"fro\\\", \\\"nuc\\\", -np.inf, -2, -1, 0, 1, 2, np.inf, 3],\\n-            axis=[None, (0, 1), (0, 2)],\\n-            keepdims=[False, True],\\n-        )\\n-    )\\n-    def test_norm_matrices(self, ord, axis, keepdims):\\n-        if axis is None:\\n-            x = np.random.random((6, 7))\\n-        else:\\n-            x = np.random.random((5, 6, 7))\\n-        if ord in (0, 3):\\n-            error = RuntimeError if backend.backend() == \\\"torch\\\" else ValueError\\n-            with self.assertRaises(error):\\n-                kmath.norm(x, ord=ord, axis=axis, keepdims=keepdims)\\n-            return\\n-        output = kmath.norm(x, ord=ord, axis=axis, keepdims=keepdims)\\n-        expected_result = np.linalg.norm(\\n-            x, ord=ord, axis=axis, keepdims=keepdims\\n-        )\\n-        self.assertAllClose(output, expected_result, atol=1e-5, rtol=1e-5)\\n-\\n-    @parameterized.named_parameters(\\n-        named_product(\\n-            ord=[None, \\\"fro\\\", \\\"nuc\\\", -np.inf, -2, -1, 0, 1, 2, np.inf, 3],\\n-            axis=[None, 1, -1],\\n-            keepdims=[False, True],\\n-        )\\n-    )\\n-    def test_norm_vectors(self, ord, axis, keepdims):\\n-        if axis is None:\\n-            x = np.random.random((5,))\\n-        else:\\n-            x = np.random.random((5, 6))\\n-        if ord in (\\\"fro\\\", \\\"nuc\\\"):\\n-            error = RuntimeError if backend.backend() == \\\"torch\\\" else ValueError\\n-            with self.assertRaises(error):\\n-                kmath.norm(x, ord=ord, axis=axis, keepdims=keepdims)\\n-            return\\n-        output = kmath.norm(x, ord=ord, axis=axis, keepdims=keepdims)\\n-        expected_result = np.linalg.norm(\\n-            x, ord=ord, axis=axis, keepdims=keepdims\\n-        )\\n-        self.assertAllClose(output, expected_result)\\n-\\n \\n class MathDtypeTest(testing.TestCase, parameterized.TestCase):\\n     \\\"\\\"\\\"Test the floating dtype to verify that the behavior matches JAX.\\\"\\\"\\\"\\n@@ -1010,71 +894,6 @@ def tearDown(self) -> None:\\n         self.jax_enable_x64.__exit__(None, None, None)\\n         return super().tearDown()\\n \\n-    @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\\n-    def test_norm(self, dtype):\\n-        import jax.numpy as jnp\\n-\\n-        x = knp.ones((1,), dtype=dtype)\\n-        x_jax = jnp.ones((1,), dtype=dtype)\\n-        expected_dtype = standardize_dtype(jnp.linalg.norm(x_jax).dtype)\\n-        if dtype == \\\"int64\\\":\\n-            expected_dtype = \\\"float32\\\"\\n-\\n-        self.assertEqual(standardize_dtype(kmath.norm(x).dtype), expected_dtype)\\n-        self.assertEqual(kmath.Norm().symbolic_call(x).dtype, expected_dtype)\\n-\\n-\\n-class QrOpTest(testing.TestCase):\\n-    def test_qr_init_mode_reduced(self):\\n-        qr_op = kmath.Qr(mode=\\\"reduced\\\")\\n-        self.assertIsNotNone(qr_op)\\n-\\n-    def test_qr_init_mode_complete(self):\\n-        qr_op = kmath.Qr(mode=\\\"complete\\\")\\n-        self.assertIsNotNone(qr_op)\\n-\\n-    def test_qr_init_invalid_mode(self):\\n-        invalid_mode = \\\"invalid_mode\\\"\\n-        expected_error = (\\n-            r\\\"`mode` argument value not supported. \\\"\\n-            r\\\"Expected one of \\\\{'reduced', 'complete'\\\\}. \\\"\\n-            f\\\"Received: mode={invalid_mode}\\\"\\n-        )\\n-        with self.assertRaisesRegex(ValueError, expected_error):\\n-            kmath.Qr(mode=invalid_mode)\\n-\\n-    def test_compute_output_spec_low_rank(self):\\n-        qr_op = kmath.Qr(mode=\\\"reduced\\\")\\n-        low_rank_input = np.random.rand(3)\\n-        with self.assertRaisesRegex(\\n-            ValueError, r\\\"Input should have rank >= 2. Received: .*\\\"\\n-        ):\\n-            qr_op.compute_output_spec(low_rank_input)\\n-\\n-    def test_compute_output_spec_undefined_dimensions(self):\\n-        qr_op = kmath.Qr(mode=\\\"reduced\\\")\\n-        undefined_dim_input = KerasTensor(shape=(None, 4), dtype=\\\"float32\\\")\\n-        with self.assertRaisesRegex(\\n-            ValueError,\\n-            r\\\"Input should have its last 2 dimensions \\\"\\n-            r\\\"fully-defined. Received: .*\\\",\\n-        ):\\n-            qr_op.compute_output_spec(undefined_dim_input)\\n-\\n-    def test_qr_call_mode_reduced(self):\\n-        qr_op = kmath.Qr(mode=\\\"reduced\\\")\\n-        test_input = np.random.rand(10, 10)\\n-        q, r = qr_op.call(test_input)\\n-        self.assertEqual(q.shape, (10, 10))\\n-        self.assertEqual(r.shape, (10, 10))\\n-\\n-    def test_qr_call_mode_complete(self):\\n-        qr_op = kmath.Qr(mode=\\\"complete\\\")\\n-        test_input = np.random.rand(10, 10)\\n-        q, r = qr_op.call(test_input)\\n-        self.assertEqual(q.shape, (10, 10))\\n-        self.assertEqual(r.shape, (10, 10))\\n-\\n \\n class ExtractSequencesOpTest(testing.TestCase):\\n     def test_extract_sequences_init_length_1_stride_1(self):\\n@@ -1236,16 +1055,6 @@ def test_fft_init_default_axis(self):\\n         self.assertEqual(fft_op.axis, -1, \\\"Default axis should be -1\\\")\\n \\n \\n-class SolveTest(testing.TestCase):\\n-    def test_solve_call(self):\\n-        solve_op = kmath.Solve()\\n-        a = np.array([[3, 2], [1, 2]], dtype=np.float32)\\n-        b = np.array([[9, 8], [5, 4]], dtype=np.float32)\\n-        output = solve_op.call(a, b)\\n-        expected_output = np.linalg.solve(a, b)\\n-        self.assertAllClose(output, expected_output, atol=1e-6, rtol=1e-6)\\n-\\n-\\n class FFT2Test(testing.TestCase):\\n     def test_fft2_correct_input(self):\\n         fft2_op = kmath.FFT2()\\n\",\n",
      "    \"problem_statement\": \"What are the keras3 operations to replace those in tf.linalg?\\nI am looking for a replacement to tf.linalg.eigh, etc.\\r\\nThere doesn't seem to be equivalent functionality in keras.ops or keras.utils. \\r\\n\\r\\n\\n\",\n",
      "    \"hints_text\": \"Hi @tsjain ,\\r\\n\\r\\nThis Op seems not yet implemented in Keras3. But if you are using tensorflow as backend you can still use Tensorflow API itself.\\r\\n\\nThe major promise of keras 3 was to seamlessly switch backends, so it would be good if this functionality was abstracted into the API as well.\\nLooks like a wide variety of tf.linalg.* operations are not supported, even though the migration guide https://keras.io/guides/migrating_to_keras_3/#transitioning-to-backendagnostic-keras-3 says they should be\\nHi @tsjain, this sounds like a good idea! Would you be interested in contributing a PR for making these linalg ops backend-agnostic for Keras 3?\\nIf I had the knowledge for this, I would. But I am a mostly an end-user of keras.\\nHi @nkovela1 I'd be interested in contributing on that, with some guidance. For instance, should we implement this as a separate `ops.linalg` namespace (some operations already implemented in math would also be moved there, like `qr`, `solve`, `norm`)? Would it be okay to start with a PR that implements only a small subset of the most common linear algebra operations (thinking about stuff like `cholesky`, `inv`, `det`, `lu`, `eig`, ...)?\",\n",
      "    \"created_at\": \"2024-02-03T16:59:29Z\",\n",
      "    \"version\": null,\n",
      "    \"PASS_TO_PASS\": [],\n",
      "    \"FAIL_TO_PASS\": [\n",
      "        \"keras/ops/math_test.py\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# pretty print the task dict\n",
    "print(json.dumps(task, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create csv with rows instance_id, pr link, problem statement, hints text. save as keras_tasks.csv\n",
    "# just do it for successful_instance_ids\n",
    "import csv\n",
    "\n",
    "with open('keras_tasks.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['instance_id', 'pr_link', 'problem_statement', 'hints_text'])\n",
    "    for instance_id in successful_instance_ids:\n",
    "        task = get_task(instance_id)\n",
    "        number_id = instance_id.split('-')[-1]\n",
    "        pr_link = f'https://github.com/keras-team/keras/pull/{number_id}'\n",
    "        writer.writerow([instance_id, pr_link, task['problem_statement'], task['hints_text']])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = 19852\n",
    "task = get_task(f'keras-team__keras-{number}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.keras format much slower to load\n",
      "Anyone experiencing unreasonably slow load times when loading a keras-format saved model? I have noticed this repeated when working in ipython, where simply instantiating a model via `Model.from_config` then calling `model.load_weights` is much (several factors) faster than loading a `model.keras` file.\n",
      "\n",
      "My understanding is the keras format is simply a zip file with the config.json file and weights h5 (iirc) but weirdly enough, there's something not right going on while loading.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the problem statement\n",
    "print(task['problem_statement'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could you please provide the comparison and the time difference in loading the model with .keras and and other format.\n",
      "\n",
      "For more details on the changes included with .keras format and why it is preferred over other format, refer https://keras.io/guides/serialization_and_saving/ \n",
      "I don't have an example at the moment but recently we updated our prod system from keras 2 to keras 3 so we converted all legacy saved models to the new keras 3 format which lead to our service to take over 12 minutes to load all models (>15 models loading in subprocesses in parallel). Moving to `from_config` + `load_weights` reduced the time to ~2 minutes (which is on par with what we had before).\n",
      "\n",
      "For what it's worth, before we did that migration, I was already working on GPT2Backbone models with keras-nlp and noticed the same issue were loading the .keras model was really slow (but didn't give it much thought at the time)\n",
      "What you're using is actually the same as what `load_model` is using except for the interaction with the zip file. So perhaps the zip file reading is the issue.\n",
      "100% which is why I find this very odd\n",
      "I encountered this issue before when trying to quantize Gemma\n",
      "\n",
      "I have created this script to demonstrate the issue (using GPT-2)\n",
      "\n",
      "`check_loading.py`\n",
      "```python\n",
      "import argparse\n",
      "import json\n",
      "\n",
      "import keras\n",
      "import keras_nlp\n",
      "\n",
      "\n",
      "def get_args():\n",
      "    parser = argparse.ArgumentParser()\n",
      "    parser.add_argument(\n",
      "        \"-m\",\n",
      "        \"--mode\",\n",
      "        default=\"save\",\n",
      "        choices=[\"save\", \"load\", \"load_weights\"],\n",
      "    )\n",
      "    args = parser.parse_args()\n",
      "    return args\n",
      "\n",
      "\n",
      "def main(args):\n",
      "    if args.mode == \"save\":\n",
      "        model = keras_nlp.models.GPT2CausalLM.from_preset(\"gpt2_base_en\")\n",
      "        # Save keras file\n",
      "        model.save(\"model.keras\")\n",
      "        # Save serialized config and weights\n",
      "        config = keras.saving.serialize_keras_object(model)\n",
      "        with open(\"model.json\", \"w\") as f:\n",
      "            json.dump(config, f)\n",
      "        model.save_weights(\"model.weights.h5\")\n",
      "    elif args.mode == \"load\":\n",
      "        model = keras.saving.load_model(\"model.keras\")\n",
      "    else:\n",
      "        with open(\"model.json\", \"r\") as f:\n",
      "            config = json.load(f)\n",
      "        model = keras.saving.deserialize_keras_object(config)\n",
      "        model.load_weights(\"model.weights.h5\")\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    keras.config.disable_traceback_filtering()\n",
      "    main(get_args())\n",
      "\n",
      "```\n",
      "\n",
      "Usage:\n",
      "```bash\n",
      "# 1. Save the model\n",
      "python check_loading.py -m save\n",
      "# 2. Profile `load_model`\n",
      "pyinstrument python check_loading.py -m load\n",
      "# 3. Profile `deserialize_keras_object` and `load_weights`\n",
      "pyinstrument python check_loading.py -m load_weights\n",
      "```\n",
      "\n",
      "The result:\n",
      "\n",
      "|Method|Cost Time|\n",
      "|-|-|\n",
      "|`load_model`|27.861s|\n",
      "|`deserialize_keras_object` + `load_weights`|3.166s|\n",
      "\n",
      "Logs:\n",
      "\n",
      "<details>\n",
      "\n",
      "```console\n",
      "  _     ._   __/__   _ _  _  _ _/_   Recorded: 10:05:02  Samples:  10954\n",
      " /_//_/// /_\\ / //_// / //_'/ //     Duration: 27.861    CPU time: 30.009\n",
      "/   _/                      v4.6.2\n",
      "\n",
      "Program: /home/hongyu/miniconda3/envs/kimm/bin/pyinstrument check_loading.py -m load\n",
      "\n",
      "27.861 <module>  check_loading.py:1\n",
      " 25.635 main  check_loading.py:20\n",
      "   25.635 load_model  keras/src/saving/saving_api.py:116\n",
      "      25.634 load_model  keras/src/saving/saving_lib.py:138\n",
      "         25.634 _load_model_from_fileobj  keras/src/saving/saving_lib.py:157\n",
      "            24.319 _load_state  keras/src/saving/saving_lib.py:395\n",
      "              23.507 _load_container_state  keras/src/saving/saving_lib.py:510\n",
      "                23.507 _load_state  keras/src/saving/saving_lib.py:395\n",
      "                   23.505 _load_container_state  keras/src/saving/saving_lib.py:510\n",
      "                      23.504 _load_state  keras/src/saving/saving_lib.py:395\n",
      "                         21.286 _load_state  keras/src/saving/saving_lib.py:395\n",
      "                           9.102 _load_state  keras/src/saving/saving_lib.py:395\n",
      "                             5.381 H5IOStore.get  keras/src/saving/saving_lib.py:632\n",
      "                               5.381 H5Entry.__init__  keras/src/saving/saving_lib.py:646\n",
      "                                  3.618 Group.__contains__  h5py/_hl/group.py:508\n",
      "                                      [6 frames hidden]  h5py, zipfile, <built-in>\n",
      "                                  1.763 File.__getitem__  h5py/_hl/group.py:348\n",
      "                                       [6 frames hidden]  h5py, zipfile, <built-in>\n",
      "                             3.717 EinsumDense.load_own_variables  keras/src/layers/core/einsum_dense.py:279\n",
      "                                3.579 H5Entry.__getitem__  keras/src/saving/saving_lib.py:702\n",
      "                                   3.577 Group.__getitem__  h5py/_hl/group.py:348\n",
      "                                        [6 frames hidden]  h5py, zipfile, <built-in>\n",
      "                           7.054 H5IOStore.get  keras/src/saving/saving_lib.py:632\n",
      "                             7.054 H5Entry.__init__  keras/src/saving/saving_lib.py:646\n",
      "                                4.377 Group.__contains__  h5py/_hl/group.py:508\n",
      "                                    [9 frames hidden]  h5py, zipfile, <built-in>\n",
      "                                2.677 Group.__getitem__  h5py/_hl/group.py:348\n",
      "                                     [6 frames hidden]  h5py, zipfile, <built-in>\n",
      "                           3.121 LayerNormalization.load_own_variables  keras/src/layers/layer.py:1187\n",
      "                             1.936 H5Entry.__getitem__  keras/src/saving/saving_lib.py:702\n",
      "                               1.935 Group.__getitem__  h5py/_hl/group.py:348\n",
      "                                    [6 frames hidden]  h5py, zipfile, <built-in>\n",
      "                             0.967 Variable.assign  keras/src/backend/common/variables.py:223\n",
      "                                0.962 Variable._convert_to_tensor  keras/src/backend/tensorflow/core.py:53\n",
      "                                   0.962 convert_to_tensor  keras/src/backend/tensorflow/core.py:102\n",
      "                                      0.961 error_handler  tensorflow/python/util/traceback_utils.py:138\n",
      "                                           [18 frames hidden]  tensorflow, h5py, zipfile, <built-in>\n",
      "                           1.978 Dense.load_own_variables  keras/src/layers/core/dense.py:224\n",
      "                              1.690 H5Entry.__getitem__  keras/src/saving/saving_lib.py:702\n",
      "                                 1.690 Group.__getitem__  h5py/_hl/group.py:348\n",
      "                                      [6 frames hidden]  h5py, zipfile, <built-in>\n",
      "                         1.576 H5IOStore.get  keras/src/saving/saving_lib.py:632\n",
      "                           1.576 H5Entry.__init__  keras/src/saving/saving_lib.py:646\n",
      "                              1.391 Group.__contains__  h5py/_hl/group.py:508\n",
      "                                   [6 frames hidden]  h5py, zipfile, <built-in>\n",
      "                         0.344 ReversibleEmbedding.load_own_variables  keras_nlp/src/layers/modeling/reversible_embedding.py:151\n",
      "                           0.344 ReversibleEmbedding.load_own_variables  keras/src/layers/core/embedding.py:214\n",
      "                              0.288 Variable.assign  keras/src/backend/common/variables.py:223\n",
      "                                 0.288 Variable._convert_to_tensor  keras/src/backend/tensorflow/core.py:53\n",
      "                                    0.288 convert_to_tensor  keras/src/backend/tensorflow/core.py:102\n",
      "                                       0.288 error_handler  tensorflow/python/util/traceback_utils.py:138\n",
      "                                            [11 frames hidden]  tensorflow\n",
      "                         0.298 TransformerDecoder.load_own_variables  keras/src/layers/layer.py:1187\n",
      "              0.809 _load_state  keras/src/saving/saving_lib.py:395\n",
      "                 0.586 _load_state  keras/src/saving/saving_lib.py:395\n",
      "                    0.467 GPT2Tokenizer.load_assets  keras_nlp/src/tokenizers/byte_pair_tokenizer.py:327\n",
      "                         [3 frames hidden]  keras_nlp\n",
      "            0.534 deserialize_keras_object  keras/src/saving/serialization_lib.py:393\n",
      "              0.534 GPT2CausalLM.from_config  keras_nlp/src/models/task.py:143\n",
      "                 0.522 deserialize  keras/src/layers/__init__.py:153\n",
      "                    0.522 deserialize_keras_object  keras/src/saving/serialization_lib.py:393\n",
      "                       0.516 GPT2Backbone.from_config  keras_nlp/src/models/backbone.py:139\n",
      "                            [3 frames hidden]  keras_nlp\n",
      "                               0.293 TransformerDecoder.__call__  keras_nlp/src/layers/modeling/transformer_decoder.py:253\n",
      "                                0.288 build_wrapper  keras/src/layers/layer.py:220\n",
      "                                   0.288 TransformerDecoder.build  keras_nlp/src/layers/modeling/transformer_decoder.py:134\n",
      "            0.458 DiskIOStore.__init__  keras/src/saving/saving_lib.py:556\n",
      "               0.458 ZipFile.extractall  zipfile.py:1666\n",
      "                    [4 frames hidden]  zipfile, shutil, <built-in>\n",
      " 2.121 <module>  keras/__init__.py:1\n",
      "    2.121 <module>  keras/api/__init__.py:1\n",
      "       2.120 <module>  keras/api/_tf_keras/__init__.py:1\n",
      "          2.120 <module>  keras/api/_tf_keras/keras/__init__.py:1\n",
      "             2.110 <module>  keras/api/activations/__init__.py:1\n",
      "                2.110 <module>  keras/src/__init__.py:1\n",
      "                   2.109 <module>  keras/src/activations/__init__.py:1\n",
      "                      1.921 <module>  keras/src/activations/activations.py:1\n",
      "                         1.917 <module>  keras/src/backend/__init__.py:1\n",
      "                            1.824 <module>  keras/src/backend/common/__init__.py:1\n",
      "                               1.823 <module>  keras/src/backend/common/dtypes.py:1\n",
      "                                  1.823 <module>  keras/src/backend/common/variables.py:1\n",
      "                                     1.758 <module>  keras/src/utils/__init__.py:1\n",
      "                                        1.721 <module>  keras/src/utils/model_visualization.py:1\n",
      "                                           1.705 <module>  keras/src/tree/__init__.py:1\n",
      "                                              1.705 <module>  keras/src/tree/tree_api.py:1\n",
      "                                                 1.699 <module>  keras/src/tree/optree_impl.py:1\n",
      "                                                    1.698 <module>  tensorflow/__init__.py:1\n",
      "                                                         [23 frames hidden]  tensorflow\n",
      "\n",
      "\n",
      "  _     ._   __/__   _ _  _  _ _/_   Recorded: 10:05:39  Samples:  2266\n",
      " /_//_/// /_\\ / //_// / //_'/ //     Duration: 3.166     CPU time: 5.276\n",
      "/   _/                      v4.6.2\n",
      "\n",
      "Program: /home/hongyu/miniconda3/envs/kimm/bin/pyinstrument check_loading.py -m load_weights\n",
      "\n",
      "3.165 <module>  check_loading.py:1\n",
      " 2.121 <module>  keras/__init__.py:1\n",
      "   2.121 <module>  keras/api/__init__.py:1\n",
      "      2.120 <module>  keras/api/_tf_keras/__init__.py:1\n",
      "         2.120 <module>  keras/api/_tf_keras/keras/__init__.py:1\n",
      "            2.110 <module>  keras/api/activations/__init__.py:1\n",
      "               2.110 <module>  keras/src/__init__.py:1\n",
      "                  2.109 <module>  keras/src/activations/__init__.py:1\n",
      "                     1.922 <module>  keras/src/activations/activations.py:1\n",
      "                       1.917 <module>  keras/src/backend/__init__.py:1\n",
      "                          1.825 <module>  keras/src/backend/common/__init__.py:1\n",
      "                            1.824 <module>  keras/src/backend/common/dtypes.py:1\n",
      "                               1.824 <module>  keras/src/backend/common/variables.py:1\n",
      "                                  1.760 <module>  keras/src/utils/__init__.py:1\n",
      "                                    1.722 <module>  keras/src/utils/model_visualization.py:1\n",
      "                                       1.707 <module>  keras/src/tree/__init__.py:1\n",
      "                                          1.707 <module>  keras/src/tree/tree_api.py:1\n",
      "                                             1.701 <module>  keras/src/tree/optree_impl.py:1\n",
      "                                                1.701 <module>  tensorflow/__init__.py:1\n",
      "                                                     [116 frames hidden]  tensorflow, <built-in>, inspect, requ...\n",
      "                                  0.063 <module>  numpy/__init__.py:1\n",
      "                          0.091 <module>  keras/src/backend/tensorflow/__init__.py:1\n",
      "                             0.089 <module>  keras/src/backend/tensorflow/numpy.py:1\n",
      "                                0.089 elementwise_unary  keras/src/backend/tensorflow/sparse.py:348\n",
      "                                   0.089 update_wrapper  functools.py:35\n",
      "                     0.186 <module>  keras/src/saving/__init__.py:1\n",
      "                        0.186 <module>  keras/src/saving/saving_api.py:1\n",
      "                           0.186 <module>  keras/src/legacy/saving/legacy_h5_format.py:1\n",
      "                              0.182 <module>  keras/src/legacy/saving/saving_utils.py:1\n",
      "                                 0.139 <module>  keras/src/models/__init__.py:1\n",
      "                                   0.138 <module>  keras/src/models/functional.py:1\n",
      "                                      0.138 <module>  keras/src/models/model.py:1\n",
      "                                         0.135 <module>  keras/src/trainers/trainer.py:1\n",
      "                                            0.135 <module>  keras/src/trainers/data_adapters/__init__.py:1\n",
      "                                               0.134 <module>  keras/src/trainers/data_adapters/array_data_adapter.py:1\n",
      "                                                  0.133 <module>  keras/src/trainers/data_adapters/array_slicing.py:1\n",
      "                                                     0.133 <module>  pandas/__init__.py:1\n",
      "                                                          [5 frames hidden]  pandas\n",
      "                                 0.043 <module>  keras/src/layers/__init__.py:1\n",
      " 0.940 main  check_loading.py:20\n",
      "   0.540 deserialize_keras_object  keras/src/saving/serialization_lib.py:393\n",
      "     0.539 GPT2CausalLM.from_config  keras_nlp/src/models/task.py:143\n",
      "        0.528 deserialize  keras/src/layers/__init__.py:153\n",
      "           0.528 deserialize_keras_object  keras/src/saving/serialization_lib.py:393\n",
      "              0.522 GPT2Backbone.from_config  keras_nlp/src/models/backbone.py:139\n",
      "                   [3 frames hidden]  keras_nlp\n",
      "                      0.522 GPT2Backbone.__init__  keras_nlp/src/models/gpt2/gpt2_backbone.py:92\n",
      "                       0.294 TransformerDecoder.__call__  keras_nlp/src/layers/modeling/transformer_decoder.py:253\n",
      "                         0.289 build_wrapper  keras/src/layers/layer.py:220\n",
      "                            0.289 TransformerDecoder.build  keras_nlp/src/layers/modeling/transformer_decoder.py:134\n",
      "                               0.223 build_wrapper  keras/src/layers/layer.py:220\n",
      "                                  0.138 CachedMultiHeadAttention.build  keras/src/layers/attention/multi_head_attention.py:199\n",
      "                                    0.091 build_wrapper  keras/src/layers/layer.py:220\n",
      "                                       0.088 EinsumDense.build  keras/src/layers/core/einsum_dense.py:147\n",
      "                                          0.082 EinsumDense.add_weight  keras/src/layers/layer.py:455\n",
      "                                             0.080 Variable.__init__  keras/src/backend/common/variables.py:80\n",
      "                                                0.046 Variable._initialize  keras/src/backend/tensorflow/core.py:30\n",
      "                                                   0.045 error_handler  tensorflow/python/util/traceback_utils.py:138\n",
      "                                                        [14 frames hidden]  tensorflow, <built-in>\n",
      "                                  0.046 Dense.build  keras/src/layers/core/dense.py:102\n",
      "                                    0.045 Dense.add_weight  keras/src/layers/layer.py:455\n",
      "                                       0.045 Variable.__init__  keras/src/backend/common/variables.py:80\n",
      "                                  0.033 LayerNormalization.build  keras/src/layers/normalization/layer_normalization.py:147\n",
      "                                     0.033 LayerNormalization.add_weight  keras/src/layers/layer.py:455\n",
      "                                        0.033 Variable.__init__  keras/src/backend/common/variables.py:80\n",
      "                       0.199 Dropout.__init__  keras/src/layers/regularization/dropout.py:41\n",
      "                          0.199 SeedGenerator.__init__  keras/src/random/seed_generator.py:48\n",
      "                             0.199 Variable.__init__  keras/src/backend/common/variables.py:80\n",
      "                                0.198 seed_initializer  keras/src/random/seed_generator.py:70\n",
      "                                   0.198 convert_to_tensor  keras/src/backend/tensorflow/core.py:102\n",
      "                                      0.198 error_handler  tensorflow/python/util/traceback_utils.py:138\n",
      "                                           [17 frames hidden]  tensorflow, <built-in>\n",
      "   0.399 error_handler  keras/src/utils/traceback_utils.py:110\n",
      "      0.399 GPT2CausalLM.load_weights  keras/src/models/model.py:321\n",
      "         0.399 load_weights  keras/src/saving/saving_api.py:226\n",
      "            0.399 load_weights_only  keras/src/saving/saving_lib.py:239\n",
      "               0.399 _load_state  keras/src/saving/saving_lib.py:395\n",
      "                  0.392 _load_container_state  keras/src/saving/saving_lib.py:510\n",
      "                     0.392 _load_state  keras/src/saving/saving_lib.py:395\n",
      "                        0.391 _load_container_state  keras/src/saving/saving_lib.py:510\n",
      "                           0.390 _load_state  keras/src/saving/saving_lib.py:395\n",
      "                              0.209 _load_state  keras/src/saving/saving_lib.py:395\n",
      "                                0.088 Dense.load_own_variables  keras/src/layers/core/dense.py:224\n",
      "                                  0.086 Variable.assign  keras/src/backend/common/variables.py:223\n",
      "                                     0.086 Variable._convert_to_tensor  keras/src/backend/tensorflow/core.py:53\n",
      "                                        0.086 convert_to_tensor  keras/src/backend/tensorflow/core.py:102\n",
      "                                           0.085 error_handler  tensorflow/python/util/traceback_utils.py:138\n",
      "                                                [14 frames hidden]  tensorflow, h5py\n",
      "                                0.077 _load_state  keras/src/saving/saving_lib.py:395\n",
      "                                   0.060 EinsumDense.load_own_variables  keras/src/layers/core/einsum_dense.py:279\n",
      "                                      0.059 Variable.assign  keras/src/backend/common/variables.py:223\n",
      "                                         0.046 Variable._convert_to_tensor  keras/src/backend/tensorflow/core.py:53\n",
      "                                            0.046 convert_to_tensor  keras/src/backend/tensorflow/core.py:102\n",
      "                                               0.046 error_handler  tensorflow/python/util/traceback_utils.py:138\n",
      "                                                    [11 frames hidden]  tensorflow\n",
      "                              0.172 ReversibleEmbedding.load_own_variables  keras_nlp/src/layers/modeling/reversible_embedding.py:151\n",
      "                                 0.172 ReversibleEmbedding.load_own_variables  keras/src/layers/core/embedding.py:214\n",
      "                                    0.164 Variable.assign  keras/src/backend/common/variables.py:223\n",
      "                                       0.164 Variable._convert_to_tensor  keras/src/backend/tensorflow/core.py:53\n",
      "                                          0.164 convert_to_tensor  keras/src/backend/tensorflow/core.py:102\n",
      "                                             0.164 error_handler  tensorflow/python/util/traceback_utils.py:138\n",
      "                                                  [14 frames hidden]  tensorflow, h5py\n",
      " 0.102 <module>  keras_nlp/__init__.py:1\n",
      "      [18 frames hidden]  keras_nlp, tensorflow_text\n",
      "```\n",
      "\n",
      "</details>\n",
      "By diving into the example provided by @james77777778 ,\n",
      "in the hidden frames, there's a call: `Group.__getitem__` -> `ZipExtFile.seek`\n",
      "This makes sense when we are using archive.\n",
      "\n",
      "in python stdlib `zipfile.ZipExtFile`: `seek` -> `read` -> `_read1` -> `_update_crc`\n",
      "The overhead caused by `_update_crc` during each `seek()` call is significant.\n",
      "reference:\n",
      "https://github.com/python/cpython/blob/f878d46e5614f08a9302fcb6fc611ef49e9acf2f/Lib/zipfile/__init__.py#L1133\n",
      "A simple way to deal with it, which will work fine:\n",
      "\n",
      "https://github.com/keras-team/keras/blob/a2df0f9ac595639aa2d3a0359122b030d934389e/keras/src/saving/saving_lib.py#L620-L627\n",
      "\n",
      "by changing line 624 to `self.io_file = io.BytesIO(self.archive.open(self.root_path, \"r\").read()) `\n",
      "That probably fixes the speed issue but would lead to unwanted extra memory usage which is undesirable\n",
      "> That probably fixes the speed issue but would lead to unwanted extra memory usage which is undesirable\n",
      "\n",
      "Is that a good tradeoff? Should we instead unzip on disk then load from the h5 file? What do you think @james77777778 @Grvzard ?\n",
      "> Is that a good tradeoff?\n",
      "\n",
      "Generally, It should be okay to load the entire h5 into memory before loading. This is the case when saving:\n",
      "\n",
      "1. write into memory first:\n",
      "https://github.com/keras-team/keras/blob/77ef1792201cd30b52146c71dd0380786512ac84/keras/src/saving/saving_lib.py#L620-L622\n",
      "2. write to disk when closing:\n",
      "https://github.com/keras-team/keras/blob/77ef1792201cd30b52146c71dd0380786512ac84/keras/src/saving/saving_lib.py#L635-L638\n",
      "\n",
      "We can also provide an option to let users decide whether to use a faster but more memory-intensive approach.\n",
      "\n",
      "> Should we instead unzip on disk then load from the h5 file? \n",
      "\n",
      "Actually, `h5py` doesn't recommend using file-like object. https://docs.h5py.org/en/stable/high/file.html#python-file-like-objects\n",
      "So, unzipping and then loading from the H5 file might be a better approach, IMO.\n",
      "\n",
      "> So, unzipping and then loading from the H5 file might be a better approach\n",
      "\n",
      "Same.\n"
     ]
    }
   ],
   "source": [
    "print(task['hints_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '''keras-team__keras-18526\tTRUE\n",
    "keras-team__keras-18553\tTRUE\n",
    "keras-team__keras-18571\tTRUE\n",
    "keras-team__keras-18585\tTRUE\n",
    "keras-team__keras-18649\tTRUE\n",
    "keras-team__keras-18659\tTRUE\n",
    "keras-team__keras-18766\tTRUE\n",
    "keras-team__keras-18852\tTRUE\n",
    "keras-team__keras-18871\tTRUE\n",
    "keras-team__keras-18902\tTRUE\n",
    "keras-team__keras-18926\tTRUE\n",
    "keras-team__keras-18975\tTRUE\n",
    "keras-team__keras-18977\tTRUE\n",
    "keras-team__keras-19088\tTRUE\n",
    "keras-team__keras-19102\tTRUE\n",
    "keras-team__keras-19143\tFALSE\n",
    "keras-team__keras-19190\tTRUE\n",
    "keras-team__keras-19201\tTRUE\n",
    "keras-team__keras-19260\tFALSE\n",
    "keras-team__keras-19284\tTRUE\n",
    "keras-team__keras-19300\tTRUE\n",
    "keras-team__keras-19331\tTRUE\n",
    "keras-team__keras-19387\tTRUE\n",
    "keras-team__keras-19459\tTRUE\n",
    "keras-team__keras-19466\tTRUE\n",
    "keras-team__keras-19484\tTRUE\n",
    "keras-team__keras-19636\tTRUE\n",
    "keras-team__keras-19641\tTRUE\n",
    "keras-team__keras-19773\tTRUE\n",
    "keras-team__keras-19775\tTRUE\n",
    "keras-team__keras-19799\tTRUE\n",
    "keras-team__keras-19826\tTRUE\n",
    "keras-team__keras-19838\tTRUE\n",
    "keras-team__keras-19844\tTRUE\n",
    "keras-team__keras-19852\tTRUE\n",
    "keras-team__keras-19863\tTRUE\n",
    "keras-team__keras-19872\tTRUE\n",
    "keras-team__keras-19903\tFALSE\n",
    "keras-team__keras-19915\tTRUE\n",
    "keras-team__keras-19924\tTRUE\n",
    "keras-team__keras-19931\tTRUE\n",
    "keras-team__keras-19937\tTRUE\n",
    "keras-team__keras-19955\tTRUE\n",
    "keras-team__keras-19973\tTRUE\n",
    "keras-team__keras-19979\tTRUE\n",
    "keras-team__keras-20002\tTRUE\n",
    "keras-team__keras-20008\tTRUE\n",
    "keras-team__keras-20023\tFALSE\n",
    "keras-team__keras-20026\tTRUE\n",
    "keras-team__keras-20033\tTRUE\n",
    "keras-team__keras-20034\tFALSE\n",
    "keras-team__keras-20040\tTRUE\n",
    "keras-team__keras-20041\tTRUE\n",
    "keras-team__keras-20076\tTRUE\n",
    "keras-team__keras-20079\tTRUE\n",
    "keras-team__keras-20122\tTRUE\n",
    "keras-team__keras-20125\tTRUE\n",
    "keras-team__keras-20197\tTRUE\n",
    "keras-team__keras-20206\tTRUE\n",
    "keras-team__keras-20296\tTRUE\n",
    "keras-team__keras-20380\tTRUE\n",
    "keras-team__keras-20389\tTRUE\n",
    "keras-team__keras-20410\tTRUE\n",
    "keras-team__keras-20443\tTRUE\n",
    "keras-team__keras-20447\tTRUE\n",
    "keras-team__keras-20456\tTRUE\n",
    "keras-team__keras-20457\tFALSE\n",
    "keras-team__keras-20484\tFALSE\n",
    "keras-team__keras-20534\tTRUE\n",
    "keras-team__keras-20537\tTRUE\n",
    "keras-team__keras-20541\tTRUE\n",
    "keras-team__keras-20550\tTRUE\n",
    "keras-team__keras-20602\tTRUE\n",
    "keras-team__keras-20609\tTRUE\n",
    "keras-team__keras-20612\tTRUE\n",
    "keras-team__keras-20626\tTRUE\n",
    "keras-team__keras-20689\tTRUE\n",
    "keras-team__keras-20703\tTRUE\n",
    "keras-team__keras-20733\tTRUE\n",
    "keras-team__keras-20745\tTRUE\n",
    "keras-team__keras-20765\tTRUE\n",
    "keras-team__keras-20768\tTRUE\n",
    "keras-team__keras-20782\tTRUE\n",
    "keras-team__keras-20791\tTRUE\n",
    "keras-team__keras-20808\tTRUE\n",
    "keras-team__keras-20815\tTRUE\n",
    "keras-team__keras-20822\tFALSE\n",
    "keras-team__keras-20829\tTRUE\n",
    "keras-team__keras-20888\tTRUE\n",
    "keras-team__keras-20901\tTRUE\n",
    "keras-team__keras-20973\tTRUE'''\n",
    "\n",
    "# parse to get instance_ids that are FALSE\n",
    "lines = s.split('\\n')\n",
    "false_instance_ids = [line.split('\\t')[0] for line in lines if line.split('\\t')[1] == 'FALSE']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['keras-team__keras-19143',\n",
       " 'keras-team__keras-19260',\n",
       " 'keras-team__keras-19903',\n",
       " 'keras-team__keras-20023',\n",
       " 'keras-team__keras-20034',\n",
       " 'keras-team__keras-20457',\n",
       " 'keras-team__keras-20484',\n",
       " 'keras-team__keras-20822']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_instance_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_instance_ids = list(set(successful_instance_ids) - set(false_instance_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_instance_ids = sorted(good_instance_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['keras-team__keras-18526',\n",
       " 'keras-team__keras-18553',\n",
       " 'keras-team__keras-18571',\n",
       " 'keras-team__keras-18585',\n",
       " 'keras-team__keras-18649',\n",
       " 'keras-team__keras-18659',\n",
       " 'keras-team__keras-18766',\n",
       " 'keras-team__keras-18852',\n",
       " 'keras-team__keras-18871',\n",
       " 'keras-team__keras-18902',\n",
       " 'keras-team__keras-18926',\n",
       " 'keras-team__keras-18975',\n",
       " 'keras-team__keras-18977',\n",
       " 'keras-team__keras-19088',\n",
       " 'keras-team__keras-19102',\n",
       " 'keras-team__keras-19190',\n",
       " 'keras-team__keras-19201',\n",
       " 'keras-team__keras-19284',\n",
       " 'keras-team__keras-19300',\n",
       " 'keras-team__keras-19331',\n",
       " 'keras-team__keras-19387',\n",
       " 'keras-team__keras-19459',\n",
       " 'keras-team__keras-19466',\n",
       " 'keras-team__keras-19484',\n",
       " 'keras-team__keras-19636',\n",
       " 'keras-team__keras-19641',\n",
       " 'keras-team__keras-19773',\n",
       " 'keras-team__keras-19775',\n",
       " 'keras-team__keras-19799',\n",
       " 'keras-team__keras-19826',\n",
       " 'keras-team__keras-19838',\n",
       " 'keras-team__keras-19844',\n",
       " 'keras-team__keras-19852',\n",
       " 'keras-team__keras-19863',\n",
       " 'keras-team__keras-19872',\n",
       " 'keras-team__keras-19915',\n",
       " 'keras-team__keras-19924',\n",
       " 'keras-team__keras-19931',\n",
       " 'keras-team__keras-19937',\n",
       " 'keras-team__keras-19955',\n",
       " 'keras-team__keras-19973',\n",
       " 'keras-team__keras-19979',\n",
       " 'keras-team__keras-20002',\n",
       " 'keras-team__keras-20008',\n",
       " 'keras-team__keras-20026',\n",
       " 'keras-team__keras-20033',\n",
       " 'keras-team__keras-20040',\n",
       " 'keras-team__keras-20041',\n",
       " 'keras-team__keras-20076',\n",
       " 'keras-team__keras-20079',\n",
       " 'keras-team__keras-20122',\n",
       " 'keras-team__keras-20125',\n",
       " 'keras-team__keras-20197',\n",
       " 'keras-team__keras-20206',\n",
       " 'keras-team__keras-20296',\n",
       " 'keras-team__keras-20380',\n",
       " 'keras-team__keras-20389',\n",
       " 'keras-team__keras-20410',\n",
       " 'keras-team__keras-20443',\n",
       " 'keras-team__keras-20447',\n",
       " 'keras-team__keras-20456',\n",
       " 'keras-team__keras-20534',\n",
       " 'keras-team__keras-20537',\n",
       " 'keras-team__keras-20541',\n",
       " 'keras-team__keras-20550',\n",
       " 'keras-team__keras-20602',\n",
       " 'keras-team__keras-20609',\n",
       " 'keras-team__keras-20612',\n",
       " 'keras-team__keras-20626',\n",
       " 'keras-team__keras-20689',\n",
       " 'keras-team__keras-20703',\n",
       " 'keras-team__keras-20733',\n",
       " 'keras-team__keras-20745',\n",
       " 'keras-team__keras-20765',\n",
       " 'keras-team__keras-20768',\n",
       " 'keras-team__keras-20782',\n",
       " 'keras-team__keras-20791',\n",
       " 'keras-team__keras-20808',\n",
       " 'keras-team__keras-20815',\n",
       " 'keras-team__keras-20829',\n",
       " 'keras-team__keras-20888',\n",
       " 'keras-team__keras-20901',\n",
       " 'keras-team__keras-20973']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_instance_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(good_instance_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove keras tasks that aren't in good_instance_ids\n",
    "new_data = [t for t in data if 'keras' not in t['repo'] or t['instance_id'] in good_instance_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data) - len(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keras_tasks) - len(good_instance_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save new data to codearena_instances.json\n",
    "with open('codearena_instances.json', 'w') as f:\n",
    "    json.dump(new_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codearena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
