<<<<<<< HEAD
{"repo": "scrapy/scrapy", "pull_number": 6606, "instance_id": "scrapy__scrapy-6606", "issue_numbers": ["6600"], "base_commit": "98ba61256deceba7b04b938a97005258f4ef5c66", "patch": "diff --git a/scrapy/cmdline.py b/scrapy/cmdline.py\nindex 065adccfb29..b08fd34095c 100644\n--- a/scrapy/cmdline.py\n+++ b/scrapy/cmdline.py\n@@ -96,10 +96,9 @@ def _get_project_only_cmds(settings: BaseSettings) -> set[str]:\n \n \n def _pop_command_name(argv: list[str]) -> str | None:\n-    for i, arg in enumerate(argv[1:]):\n-        if not arg.startswith(\"-\"):\n-            del argv[i]\n-            return arg\n+    for i in range(1, len(argv)):\n+        if not argv[i].startswith(\"-\"):\n+            return argv.pop(i)\n     return None\n \n \n", "test_patch": "diff --git a/tests/test_commands.py b/tests/test_commands.py\nindex 1aae3222e5c..50f09304333 100644\n--- a/tests/test_commands.py\n+++ b/tests/test_commands.py\n@@ -23,7 +23,7 @@\n from twisted.trial import unittest\n \n import scrapy\n-from scrapy.cmdline import _print_unknown_command_msg\n+from scrapy.cmdline import _pop_command_name, _print_unknown_command_msg\n from scrapy.commands import ScrapyCommand, ScrapyHelpFormatter, view\n from scrapy.commands.startproject import IGNORE\n from scrapy.settings import Settings\n@@ -1163,3 +1163,29 @@ def test_help_messages(self):\n         for command in self.commands:\n             _, out, _ = self.proc(command, \"-h\")\n             self.assertIn(\"Usage\", out)\n+\n+\n+class PopCommandNameTest(unittest.TestCase):\n+    def test_valid_command(self):\n+        argv = [\"scrapy\", \"crawl\", \"my_spider\"]\n+        command = _pop_command_name(argv)\n+        self.assertEqual(command, \"crawl\")\n+        self.assertEqual(argv, [\"scrapy\", \"my_spider\"])\n+\n+    def test_no_command(self):\n+        argv = [\"scrapy\"]\n+        command = _pop_command_name(argv)\n+        self.assertIsNone(command)\n+        self.assertEqual(argv, [\"scrapy\"])\n+\n+    def test_option_before_command(self):\n+        argv = [\"scrapy\", \"-h\", \"crawl\"]\n+        command = _pop_command_name(argv)\n+        self.assertEqual(command, \"crawl\")\n+        self.assertEqual(argv, [\"scrapy\", \"-h\"])\n+\n+    def test_option_after_command(self):\n+        argv = [\"scrapy\", \"crawl\", \"-h\"]\n+        command = _pop_command_name(argv)\n+        self.assertEqual(command, \"crawl\")\n+        self.assertEqual(argv, [\"scrapy\", \"-h\"])\n", "problem_statement": "Investigate off-by-1 in `scrapy.cmdline._pop_command_name()`\nIt looks like `del argv[i]` removes the wrong item in `scrapy.cmdline._pop_command_name()` but as we don't seem to see any problems because of this it's worth investigating what exactly happens here and either fixing or refactoring the code.\n", "hints_text": "Hi. I looked into `scrapy.cmdline._pop_command_name()` and the goal is to extract the command name from `argv `list. The `i` variable in the function is used to track the position in the original `argv `list and it's value is 0. However, since `argv[1:]` is a slice of `argv`, it starts at index 0 of the slice, not the original list, so when `del argv[i]` is executed, it deletes the wrong element because `i` does not align with the original list's index. I created a sample scrapy project [using this tutorial](https://docs.scrapy.org/en/master/intro/tutorial.html) and ran the command `scrapy crawl quotes`. I can see that the value of argv at the start is a list `['C:\\\\Users\\\\Hp\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\Scripts\\\\scrapy', 'crawl', 'quotes']`. It is expected that the `'crawl'` should be removed from `argv` and but after the for loop executes, `scrapy` is removed from the list and the value of `argv` is `['crawl', 'quotes']`. Screenshot below for reference.\r\n![image](https://github.com/user-attachments/assets/084f3f48-9d59-431b-bc90-1eaff5ec1fc9)\r\n\r\nAs the `for `loop iterates over `argv[1:]`, the value of `arg` is `'crawl' `and the `return arg` line returns `'crawl'`. So even though the wrong value was removed from `argv` due to incorrect index being used, the value returned by `_pop_command_name()` is the expected value.\r\n\r\nTo fix the issue, we need to ensure that the index `i` refers to the correct position in the original `argv` list. I can work on refactoring the code. However, I noticed that the function implementation for `scrapy.cmdline._pop_command_name()` in the master branch is different from what I see from the installed version of scrapy v2.12.0. I see there is a branch 2.12. For which branch should the PR be submitted?\nThanks for the analysis. As `argv` is modified in-place and used after `_pop_command_name()` is called (`parser.parse_known_args(args=argv[1:])`, I wonder why it works even though `argv` is incorrect. I guess `parse_known_args()` only cares about dash-prefixed argv entries, but it's possible that a dash-prefixed one will be deleted because of off-by-1 and then the behavior will be unexpected. It would be nice to find such cases. `scrapy -h crawl` doesn't work while `scrapy crawl -h` does, which may be the only way to break this, and `scrapy -h` says `scrapy <command> [options] [args]` which suggests the options can't go before the command name. If it's indeed the only way to break this, and the fix fixes such commands, it would still be actually useful.\r\n\r\n> For which branch should the PR be submitted?\r\n\r\nAlways for master.\n> it's possible that a dash-prefixed one will be deleted because of off-by-1 and then the behavior will be unexpected.\r\n\r\nThat's correct. If I use the command `scrapy crawl quote`, then the `parser.parse_known_args(args=argv[1:])` works even if `argv` is incorrect because the value passed to `parse_known_args` is `argv[1:]` which will always be `'quote'`, irrespective of whether `scrapy` or `crawl` is popped from the original arg list, and since `_pop_command_name` unintentionally returns the correct command, the program runs without an error. Same goes for `scrapy crawl -h`. However, if I try the command `scrapy -h crawl`, then `-h` gets deleted from the arg list and the value returned by `_pop_command_name` is `'crawl'`, which instructs the program to run the crawl command. As the line `parser.parse_known_args(args=argv[1:])` is executed, the value of `argv[1:]` also remains `'crawl'` and since there is no spider with the name 'crawl' in my scrapy project, the program exits with an error. Will work on fixing it and submit a PR today.\r\n\r\nI have a question unrelated to the problem. I would like to understand why the implementation of scrapy.cmdline._pop_command_name() from the scrapy package I installed from pip is different from the master branch. I thought that master branch contains the latest changes that should also be present in the latest package installed from pip.\n> I would like to understand why the implementation of scrapy.cmdline._pop_command_name() from the scrapy package I installed from pip is different from the master branch. \r\n\r\nBecause it was changed in master after 2.12.0 was released.\r\n\r\n> I thought that master branch contains the latest changes that should also be present in the latest package installed from pip.\r\n\r\nWell no, it contains changes that were not released yet.", "created_at": "2025-01-04T10:41:31Z"}
{"repo": "scrapy/scrapy", "pull_number": 6579, "instance_id": "scrapy__scrapy-6579", "issue_numbers": ["6578"], "base_commit": "c330a399dcc69f6d51fcfbe397fbc42b5a9ee323", "patch": "diff --git a/pyproject.toml b/pyproject.toml\nindex 8c985753fce..571a61f1c81 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -72,9 +72,9 @@ version = {file = \"./scrapy/VERSION\"}\n \n [tool.mypy]\n ignore_missing_imports = true\n+implicit_reexport = false\n \n # Interface classes are hard to support\n-\n [[tool.mypy.overrides]]\n module = \"twisted.internet.interfaces\"\n follow_imports = \"skip\"\n@@ -92,6 +92,14 @@ follow_imports = \"skip\"\n module = \"scrapy.settings.default_settings\"\n ignore_errors = true\n \n+[[tool.mypy.overrides]]\n+module = \"itemadapter\"\n+implicit_reexport = true\n+\n+[[tool.mypy.overrides]]\n+module = \"twisted\"\n+implicit_reexport = true\n+\n [tool.bumpversion]\n current_version = \"2.12.0\"\n commit = true\n@@ -359,13 +367,9 @@ ignore = [\n ]\n \n [tool.ruff.lint.per-file-ignores]\n-# Exclude files that are meant to provide top-level imports\n-\"scrapy/__init__.py\" = [\"E402\"]\n-\"scrapy/core/downloader/handlers/http.py\" = [\"F401\"]\n-\"scrapy/http/__init__.py\" = [\"F401\"]\n-\"scrapy/linkextractors/__init__.py\" = [\"E402\", \"F401\"]\n-\"scrapy/selector/__init__.py\" = [\"F401\"]\n-\"scrapy/spiders/__init__.py\" = [\"E402\", \"F401\"]\n+# Circular import workarounds\n+\"scrapy/linkextractors/__init__.py\" = [\"E402\"]\n+\"scrapy/spiders/__init__.py\" = [\"E402\"]\n \n # Skip bandit in tests\n \"tests/**\" = [\"S\"]\ndiff --git a/scrapy/core/downloader/handlers/http.py b/scrapy/core/downloader/handlers/http.py\nindex 52535bd8b58..93b96c779d1 100644\n--- a/scrapy/core/downloader/handlers/http.py\n+++ b/scrapy/core/downloader/handlers/http.py\n@@ -2,3 +2,8 @@\n from scrapy.core.downloader.handlers.http11 import (\n     HTTP11DownloadHandler as HTTPDownloadHandler,\n )\n+\n+__all__ = [\n+    \"HTTP10DownloadHandler\",\n+    \"HTTPDownloadHandler\",\n+]\ndiff --git a/scrapy/http/__init__.py b/scrapy/http/__init__.py\nindex d0b726bad90..0e5c2b53b05 100644\n--- a/scrapy/http/__init__.py\n+++ b/scrapy/http/__init__.py\n@@ -15,3 +15,16 @@\n from scrapy.http.response.json import JsonResponse\n from scrapy.http.response.text import TextResponse\n from scrapy.http.response.xml import XmlResponse\n+\n+__all__ = [\n+    \"FormRequest\",\n+    \"Headers\",\n+    \"HtmlResponse\",\n+    \"JsonRequest\",\n+    \"JsonResponse\",\n+    \"Request\",\n+    \"Response\",\n+    \"TextResponse\",\n+    \"XmlResponse\",\n+    \"XmlRpcRequest\",\n+]\ndiff --git a/scrapy/linkextractors/__init__.py b/scrapy/linkextractors/__init__.py\nindex 1c7e96ae0df..b39859f7b31 100644\n--- a/scrapy/linkextractors/__init__.py\n+++ b/scrapy/linkextractors/__init__.py\n@@ -126,3 +126,8 @@ def _is_valid_url(url: str) -> bool:\n \n # Top-level imports\n from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor as LinkExtractor\n+\n+__all__ = [\n+    \"IGNORED_EXTENSIONS\",\n+    \"LinkExtractor\",\n+]\ndiff --git a/scrapy/selector/__init__.py b/scrapy/selector/__init__.py\nindex 85c500d6665..7cfa3c36439 100644\n--- a/scrapy/selector/__init__.py\n+++ b/scrapy/selector/__init__.py\n@@ -4,3 +4,8 @@\n \n # top-level imports\n from scrapy.selector.unified import Selector, SelectorList\n+\n+__all__ = [\n+    \"Selector\",\n+    \"SelectorList\",\n+]\ndiff --git a/scrapy/spiders/__init__.py b/scrapy/spiders/__init__.py\nindex 6136dabc70a..e255e91cc1f 100644\n--- a/scrapy/spiders/__init__.py\n+++ b/scrapy/spiders/__init__.py\n@@ -117,3 +117,12 @@ def __repr__(self) -> str:\n from scrapy.spiders.crawl import CrawlSpider, Rule\n from scrapy.spiders.feed import CSVFeedSpider, XMLFeedSpider\n from scrapy.spiders.sitemap import SitemapSpider\n+\n+__all__ = [\n+    \"CSVFeedSpider\",\n+    \"CrawlSpider\",\n+    \"Rule\",\n+    \"SitemapSpider\",\n+    \"Spider\",\n+    \"XMLFeedSpider\",\n+]\n", "test_patch": "diff --git a/tests/test_item.py b/tests/test_item.py\nindex 5a8ee095e61..4804128417a 100644\n--- a/tests/test_item.py\n+++ b/tests/test_item.py\n@@ -1,7 +1,8 @@\n import unittest\n+from abc import ABCMeta\n from unittest import mock\n \n-from scrapy.item import ABCMeta, Field, Item, ItemMeta\n+from scrapy.item import Field, Item, ItemMeta\n \n \n class ItemTest(unittest.TestCase):\ndiff --git a/tests/test_utils_url.py b/tests/test_utils_url.py\nindex 94a59f8835e..314082742cf 100644\n--- a/tests/test_utils_url.py\n+++ b/tests/test_utils_url.py\n@@ -6,7 +6,7 @@\n from scrapy.linkextractors import IGNORED_EXTENSIONS\n from scrapy.spiders import Spider\n from scrapy.utils.misc import arg_to_iter\n-from scrapy.utils.url import (\n+from scrapy.utils.url import (  # type: ignore[attr-defined]\n     _is_filesystem_path,\n     _public_w3lib_objects,\n     add_http_if_no_scheme,\n", "problem_statement": "pyright reports scrapy imports as private\n<!--\r\n\r\nThanks for taking an interest in Scrapy!\r\n\r\nIf you have a question that starts with \"How to...\", please see the Scrapy Community page: https://scrapy.org/community/.\r\nThe GitHub issue tracker's purpose is to deal with bug reports and feature requests for the project itself.\r\n\r\nKeep in mind that by filing an issue, you are expected to comply with Scrapy's Code of Conduct, including treating everyone with respect: https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\r\n\r\nThe following is a suggested template to structure your issue, you can find more guidelines at https://doc.scrapy.org/en/latest/contributing.html#reporting-bugs\r\n\r\n-->\r\n\r\n### Description\r\n\r\nWhen importing various classes and methods from the documented import locations, pyright reports that the class/method isn't exported from that module with the `reportPrivateImportUsage` error. These classes include, but are not limited to:\r\n\r\n* `scrapy.http.Request`\r\n* `scrapy.http.Response`\r\n* `itemadapter.ItemAdapter`\r\n* `itemadapter.is_item`\r\n\r\n### Steps to Reproduce\r\n\r\n1. Create a new scrapy project with `scrapy startproject`\r\n2. Optionally, create a new spider or otherwise add an import `from scrapy.http import Response` somewhere\r\n3. Install pyright (`pip install pyright`) \r\n4. Run pyright \r\n\r\nI created a sample here: https://github.com/paulcwatts/scrapy-pyright-bug\r\n\r\n**Expected behavior:** \r\n\r\npyright to run without errors.\r\n\r\n**Actual behavior:** \r\n\r\npyright returns errors regarding scrapy imports:\r\n\r\n\r\n```\r\n/.../pyright_bug/middlewares.py\r\n  /.../pyright_bug/middlewares.py:9:25 - error: \"is_item\" is not exported from module \"itemadapter\"\r\n  \u00a0\u00a0Import from \"itemadapter.utils\" instead (reportPrivateImportUsage)\r\n  /.../pyright_bug/middlewares.py:9:34 - error: \"ItemAdapter\" is not exported from module \"itemadapter\"\r\n  \u00a0\u00a0Import from \"itemadapter.adapter\" instead (reportPrivateImportUsage)\r\n/.../pyright_bug/pipelines.py\r\n  /.../pyright_bug/pipelines.py:8:25 - error: \"ItemAdapter\" is not exported from module \"itemadapter\"\r\n  \u00a0\u00a0Import from \"itemadapter.adapter\" instead (reportPrivateImportUsage)\r\n/.../pyright_bug/spiders/my_spider.py\r\n  /.../pyright_bug/spiders/my_spider.py:2:25 - error: \"Response\" is not exported from module \"scrapy.http\"\r\n  \u00a0\u00a0Import from \"scrapy.http.response\" instead (reportPrivateImportUsage)\r\n4 errors, 0 warnings, 0 informations \r\n```\r\n\r\n**Reproduces how often:** \r\n\r\n100%\r\n\r\n### Versions\r\n\r\n```\r\nScrapy       : 2.12.0\r\nlxml         : 5.3.0.0\r\nlibxml2      : 2.12.9\r\ncssselect    : 1.2.0\r\nparsel       : 1.9.1\r\nw3lib        : 2.2.1\r\nTwisted      : 24.11.0\r\nPython       : 3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 10:07:17) [Clang 14.0.6 ]\r\npyOpenSSL    : 24.3.0 (OpenSSL 3.4.0 22 Oct 2024)\r\ncryptography : 44.0.0\r\nPlatform     : macOS-15.1.1-arm64-arm-64bit\r\n```\r\n\r\nIn addition, the result of `pyright --version`\r\n\r\n```\r\npyright 1.1.390\r\n```\r\n\r\n### Additional context\r\n\r\nThis is the meaning of the error according to the [pyright documentation](https://microsoft.github.io/pyright/#/configuration?id=main-configuration-options):\r\n\r\n> reportPrivateImportUsage [boolean or string, optional]: Generate or suppress diagnostics for use of a symbol from a \"py.typed\" > module that is not meant to be exported from that module. The default value for this setting is \"error\".\r\n\r\nI'm not certain of this, but I think this means is that any import that is meant to be considered public should appear in an `__all__` statement. \r\n\n", "hints_text": "We don't currently plan to make the codebase error-free with type checkers other than `mypy`. In this specific case I also wonder how popular is disabling that diagnostic, as such things are controversial.\r\n\r\nAlso note that `itemadapter` is a separate project.\nThat's fair, although it's not possible with pyright to disable this rule for a specific import across an entire project: you can either disable the rule for a specific line, for a list of files/directories, or for the entire project. This forces any scrapy project that uses pyright to disable that rule for their *entire project*, or to litter their imports with `pyright: ignore` statements, or to use workarounds.\r\n\r\nOne workaround I could imagine is to bundle all scrapy imports into a single project file, ignore the diagnostic in that file, and then use that file as the scrapy import path across the project, rather than the actual scrapy import path.\r\n\nI think we should mark re-exports explicitly, I just don't like the options we have for that :)", "created_at": "2024-12-11T10:27:07Z"}
{"repo": "scrapy/scrapy", "pull_number": 6547, "instance_id": "scrapy__scrapy-6547", "issue_numbers": ["6514", "6520"], "base_commit": "efb53aafdcaae058962c6189ddecb3dc62b02c31", "patch": "diff --git a/.bandit.yml b/.bandit.yml\ndeleted file mode 100644\nindex b7f1817e034..00000000000\n--- a/.bandit.yml\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-skips:\n-- B101  # assert_used, needed for mypy\n-- B321  # ftplib, https://github.com/scrapy/scrapy/issues/4180\n-- B402  # import_ftplib, https://github.com/scrapy/scrapy/issues/4180\n-- B411  # import_xmlrpclib, https://github.com/PyCQA/bandit/issues/1082\n-- B503  # ssl_with_bad_defaults\n-exclude_dirs: ['tests']\ndiff --git a/.bumpversion.cfg b/.bumpversion.cfg\ndeleted file mode 100644\nindex f83e3e890bf..00000000000\n--- a/.bumpversion.cfg\n+++ /dev/null\n@@ -1,11 +0,0 @@\n-[bumpversion]\n-current_version = 2.12.0\n-commit = True\n-tag = True\n-tag_name = {new_version}\n-\n-[bumpversion:file:scrapy/VERSION]\n-\n-[bumpversion:file:SECURITY.md]\n-parse = (?P<major>\\d+)\\.(?P<minor>\\d+)\\.x\n-serialize = {major}.{minor}.x\ndiff --git a/.coveragerc b/.coveragerc\ndeleted file mode 100644\nindex f9ad353d54f..00000000000\n--- a/.coveragerc\n+++ /dev/null\n@@ -1,12 +0,0 @@\n-[run]\n-branch = true\n-include = scrapy/*\n-omit =\n-  tests/*\n-disable_warnings = include-ignored\n-\n-[report]\n-# https://github.com/nedbat/coveragepy/issues/831#issuecomment-517778185\n-exclude_lines =\n-    pragma: no cover\n-    if TYPE_CHECKING:\ndiff --git a/.isort.cfg b/.isort.cfg\ndeleted file mode 100644\nindex f238bf7ea13..00000000000\n--- a/.isort.cfg\n+++ /dev/null\n@@ -1,2 +0,0 @@\n-[settings]\n-profile = black\ndiff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml\nindex fbd710f6f92..b411f492710 100644\n--- a/.pre-commit-config.yaml\n+++ b/.pre-commit-config.yaml\n@@ -3,7 +3,8 @@ repos:\n   rev: 1.7.9\n   hooks:\n   - id: bandit\n-    args: [-r, -c, .bandit.yml]\n+    args: [\"-c\", \"pyproject.toml\"]\n+    additional_dependencies: [\"bandit[toml]\"]\n - repo: https://github.com/PyCQA/flake8\n   rev: 7.1.0\n   hooks:\ndiff --git a/MANIFEST.in b/MANIFEST.in\nindex 06971e39c80..7700ae7bd81 100644\n--- a/MANIFEST.in\n+++ b/MANIFEST.in\n@@ -10,7 +10,6 @@ include scrapy/py.typed\n \n include codecov.yml\n include conftest.py\n-include pytest.ini\n include tox.ini\n \n recursive-include scrapy/templates *\ndiff --git a/pylintrc b/pylintrc\ndeleted file mode 100644\nindex e927b903c14..00000000000\n--- a/pylintrc\n+++ /dev/null\n@@ -1,73 +0,0 @@\n-[MASTER]\n-persistent=no\n-jobs=1  # >1 hides results\n-\n-[MESSAGES CONTROL]\n-disable=abstract-method,\n-        arguments-differ,\n-        arguments-renamed,\n-        attribute-defined-outside-init,\n-        bad-classmethod-argument,\n-        bare-except,\n-        broad-except,\n-        broad-exception-raised,\n-        c-extension-no-member,\n-        consider-using-with,\n-        cyclic-import,\n-        dangerous-default-value,\n-        disallowed-name,\n-        duplicate-code,  # https://github.com/PyCQA/pylint/issues/214\n-        eval-used,\n-        fixme,\n-        function-redefined,\n-        global-statement,\n-        implicit-str-concat,\n-        import-error,\n-        import-outside-toplevel,\n-        inherit-non-class,\n-        invalid-name,\n-        invalid-overridden-method,\n-        isinstance-second-argument-not-valid-type,\n-        keyword-arg-before-vararg,\n-        line-too-long,\n-        logging-format-interpolation,\n-        logging-fstring-interpolation,\n-        logging-not-lazy,\n-        lost-exception,\n-        missing-docstring,\n-        no-member,\n-        no-method-argument,\n-        no-name-in-module,\n-        no-self-argument,\n-        no-value-for-parameter,  # https://github.com/pylint-dev/pylint/issues/3268\n-        not-callable,\n-        pointless-statement,\n-        pointless-string-statement,\n-        protected-access,\n-        raise-missing-from,\n-        redefined-builtin,\n-        redefined-outer-name,\n-        reimported,\n-        signature-differs,\n-        too-few-public-methods,\n-        too-many-ancestors,\n-        too-many-arguments,\n-        too-many-branches,\n-        too-many-format-args,\n-        too-many-function-args,\n-        too-many-instance-attributes,\n-        too-many-lines,\n-        too-many-locals,\n-        too-many-public-methods,\n-        too-many-return-statements,\n-        unbalanced-tuple-unpacking,\n-        unnecessary-dunder-call,\n-        unnecessary-pass,\n-        unreachable,\n-        unused-argument,\n-        unused-import,\n-        unused-variable,\n-        used-before-assignment,\n-        useless-return,\n-        wildcard-import,\n-        wrong-import-position\ndiff --git a/pyproject.toml b/pyproject.toml\nnew file mode 100644\nindex 00000000000..f25715e76f9\n--- /dev/null\n+++ b/pyproject.toml\n@@ -0,0 +1,235 @@\n+[build-system]\n+requires = [\"setuptools >= 61.0\"]\n+build-backend = \"setuptools.build_meta\"\n+\n+[project]\n+name = \"Scrapy\"\n+dynamic = [\"version\"]\n+description = \"A high-level Web Crawling and Web Scraping framework\"\n+dependencies = [\n+    \"Twisted>=21.7.0\",\n+    \"cryptography>=37.0.0\",\n+    \"cssselect>=0.9.1\",\n+    \"itemloaders>=1.0.1\",\n+    \"parsel>=1.5.0\",\n+    \"pyOpenSSL>=22.0.0\",\n+    \"queuelib>=1.4.2\",\n+    \"service_identity>=18.1.0\",\n+    \"w3lib>=1.17.0\",\n+    \"zope.interface>=5.1.0\",\n+    \"protego>=0.1.15\",\n+    \"itemadapter>=0.1.0\",\n+    \"packaging\",\n+    \"tldextract\",\n+    \"lxml>=4.6.0\",\n+    \"defusedxml>=0.7.1\",\n+    # Platform-specific dependencies\n+    'PyDispatcher>=2.0.5; platform_python_implementation == \"CPython\"',\n+    'PyPyDispatcher>=2.1.0; platform_python_implementation == \"PyPy\"',\n+]\n+classifiers = [\n+    \"Framework :: Scrapy\",\n+    \"Development Status :: 5 - Production/Stable\",\n+    \"Environment :: Console\",\n+    \"Intended Audience :: Developers\",\n+    \"License :: OSI Approved :: BSD License\",\n+    \"Operating System :: OS Independent\",\n+    \"Programming Language :: Python\",\n+    \"Programming Language :: Python :: 3\",\n+    \"Programming Language :: Python :: 3.9\",\n+    \"Programming Language :: Python :: 3.10\",\n+    \"Programming Language :: Python :: 3.11\",\n+    \"Programming Language :: Python :: 3.12\",\n+    \"Programming Language :: Python :: 3.13\",\n+    \"Programming Language :: Python :: Implementation :: CPython\",\n+    \"Programming Language :: Python :: Implementation :: PyPy\",\n+    \"Topic :: Internet :: WWW/HTTP\",\n+    \"Topic :: Software Development :: Libraries :: Application Frameworks\",\n+    \"Topic :: Software Development :: Libraries :: Python Modules\",\n+]\n+readme = \"README.rst\"\n+requires-python = \">=3.9\"\n+authors = [{ name = \"Scrapy developers\", email = \"pablo@pablohoffman.com\" }]\n+maintainers = [{ name = \"Pablo Hoffman\", email = \"pablo@pablohoffman.com\" }]\n+\n+[project.urls]\n+Homepage = \"https://scrapy.org/\"\n+Documentation = \"https://docs.scrapy.org/\"\n+Source = \"https://github.com/scrapy/scrapy\"\n+Tracker = \"https://github.com/scrapy/scrapy/issues\"\n+Changelog = \"https://github.com/scrapy/scrapy/commits/master/\"\n+releasenotes = \"https://docs.scrapy.org/en/latest/news.html\"\n+\n+[project.scripts]\n+scrapy = \"scrapy.cmdline:execute\"\n+\n+[tool.setuptools.packages.find]\n+where = [\".\"]\n+include = [\"scrapy\", \"scrapy.*\",]\n+\n+[tool.setuptools.dynamic]\n+version = {file = \"./scrapy/VERSION\"}\n+\n+[tool.mypy]\n+ignore_missing_imports = true\n+\n+# Interface classes are hard to support\n+\n+[[tool.mypy.overrides]]\n+module = \"twisted.internet.interfaces\"\n+follow_imports = \"skip\"\n+\n+[[tool.mypy.overrides]]\n+module = \"scrapy.interfaces\"\n+ignore_errors = true\n+\n+[[tool.mypy.overrides]]\n+module = \"twisted.internet.reactor\"\n+follow_imports = \"skip\"\n+\n+# FIXME: remove the following section once the issues are solved\n+[[tool.mypy.overrides]]\n+module = \"scrapy.settings.default_settings\"\n+ignore_errors = true\n+\n+[tool.bandit]\n+skips = [\n+    \"B101\", # assert_used, needed for mypy\n+    \"B321\", # ftplib, https://github.com/scrapy/scrapy/issues/4180\n+    \"B402\", # import_ftplib, https://github.com/scrapy/scrapy/issues/4180\n+    \"B411\", # import_xmlrpclib, https://github.com/PyCQA/bandit/issues/1082\n+    \"B503\", # ssl_with_bad_defaults\n+]\n+exclude_dirs = [\"tests\"]\n+\n+[tool.bumpversion]\n+current_version = \"2.12.0\"\n+commit = true\n+tag = true\n+tag_name = \"{new_version}\"\n+\n+[[tool.bumpversion.files]]\n+filename = \"scrapy/VERSION\"\n+\n+[[tool.bumpversion.files]]\n+filename = \"SECURITY.md\"\n+parse = \"\"\"(?P<major>0|[1-9]\\\\d*)\\\\.(?P<minor>0|[1-9]\\\\d*)\"\"\"\n+serialize = [\"{major}.{minor}\"]\n+\n+[tool.coverage.run]\n+branch = true\n+include = [\"scrapy/*\"]\n+omit = [\"tests/*\"]\n+disable_warnings = [\"include-ignored\"]\n+\n+[tool.coverage.report]\n+# https://github.com/nedbat/coveragepy/issues/831#issuecomment-517778185\n+exclude_lines = [\"pragma: no cover\", \"if TYPE_CHECKING:\"]\n+\n+[tool.isort]\n+profile = \"black\"\n+\n+[tool.pylint.MASTER]\n+persistent = \"no\"\n+jobs = 1          # >1 hides results\n+\n+[tool.pylint.\"MESSAGES CONTROL\"]\n+disable = [\n+    \"abstract-method\",\n+    \"arguments-differ\",\n+    \"arguments-renamed\",\n+    \"attribute-defined-outside-init\",\n+    \"bad-classmethod-argument\",\n+    \"bare-except\",\n+    \"broad-except\",\n+    \"broad-exception-raised\",\n+    \"c-extension-no-member\",\n+    \"consider-using-with\",\n+    \"cyclic-import\",\n+    \"dangerous-default-value\",\n+    \"disallowed-name\",\n+    \"duplicate-code\",                            # https://github.com/PyCQA/pylint/issues/214\n+    \"eval-used\",\n+    \"fixme\",\n+    \"function-redefined\",\n+    \"global-statement\",\n+    \"implicit-str-concat\",\n+    \"import-error\",\n+    \"import-outside-toplevel\",\n+    \"inherit-non-class\",\n+    \"invalid-name\",\n+    \"invalid-overridden-method\",\n+    \"isinstance-second-argument-not-valid-type\",\n+    \"keyword-arg-before-vararg\",\n+    \"line-too-long\",\n+    \"logging-format-interpolation\",\n+    \"logging-fstring-interpolation\",\n+    \"logging-not-lazy\",\n+    \"lost-exception\",\n+    \"missing-docstring\",\n+    \"no-member\",\n+    \"no-method-argument\",\n+    \"no-name-in-module\",\n+    \"no-self-argument\",\n+    \"no-value-for-parameter\",                    # https://github.com/pylint-dev/pylint/issues/3268\n+    \"not-callable\",\n+    \"pointless-statement\",\n+    \"pointless-string-statement\",\n+    \"protected-access\",\n+    \"raise-missing-from\",\n+    \"redefined-builtin\",\n+    \"redefined-outer-name\",\n+    \"reimported\",\n+    \"signature-differs\",\n+    \"too-few-public-methods\",\n+    \"too-many-ancestors\",\n+    \"too-many-arguments\",\n+    \"too-many-branches\",\n+    \"too-many-format-args\",\n+    \"too-many-function-args\",\n+    \"too-many-instance-attributes\",\n+    \"too-many-lines\",\n+    \"too-many-locals\",\n+    \"too-many-public-methods\",\n+    \"too-many-return-statements\",\n+    \"unbalanced-tuple-unpacking\",\n+    \"unnecessary-dunder-call\",\n+    \"unnecessary-pass\",\n+    \"unreachable\",\n+    \"unused-argument\",\n+    \"unused-import\",\n+    \"unused-variable\",\n+    \"used-before-assignment\",\n+    \"useless-return\",\n+    \"wildcard-import\",\n+    \"wrong-import-position\",\n+]\n+\n+[tool.pytest.ini_options]\n+xfail_strict = true\n+usefixtures = \"chdir\"\n+python_files = [\"test_*.py\", \"__init__.py\"]\n+python_classes = []\n+addopts = [\n+    \"--assert=plain\",\n+    \"--ignore=docs/_ext\",\n+    \"--ignore=docs/conf.py\",\n+    \"--ignore=docs/news.rst\",\n+    \"--ignore=docs/topics/dynamic-content.rst\",\n+    \"--ignore=docs/topics/items.rst\",\n+    \"--ignore=docs/topics/leaks.rst\",\n+    \"--ignore=docs/topics/loaders.rst\",\n+    \"--ignore=docs/topics/selectors.rst\",\n+    \"--ignore=docs/topics/shell.rst\",\n+    \"--ignore=docs/topics/stats.rst\",\n+    \"--ignore=docs/topics/telnetconsole.rst\",\n+    \"--ignore=docs/utils\",\n+]\n+markers = [\n+    \"only_asyncio: marks tests as only enabled when --reactor=asyncio is passed\",\n+    \"only_not_asyncio: marks tests as only enabled when --reactor=asyncio is not passed\",\n+    \"requires_uvloop: marks tests as only enabled when uvloop is known to be working\",\n+    \"requires_botocore: marks tests that need botocore (but not boto3)\",\n+    \"requires_boto3: marks tests that need botocore and boto3\",\n+]\n+filterwarnings = []\n\\ No newline at end of file\ndiff --git a/setup.cfg b/setup.cfg\ndeleted file mode 100644\nindex 151e784c661..00000000000\n--- a/setup.cfg\n+++ /dev/null\n@@ -1,24 +0,0 @@\n-[bdist_rpm]\n-doc_files = docs AUTHORS INSTALL LICENSE README.rst\n-\n-[bdist_wheel]\n-universal=1\n-\n-[mypy]\n-ignore_missing_imports = true\n-\n-# Interface classes are hard to support\n-\n-[mypy-twisted.internet.interfaces]\n-follow_imports = skip\n-\n-[mypy-scrapy.interfaces]\n-ignore_errors = True\n-\n-[mypy-twisted.internet.reactor]\n-follow_imports = skip\n-\n-# FIXME: remove the following sections once the issues are solved\n-\n-[mypy-scrapy.settings.default_settings]\n-ignore_errors = True\ndiff --git a/setup.py b/setup.py\ndeleted file mode 100644\nindex 6cc1150a568..00000000000\n--- a/setup.py\n+++ /dev/null\n@@ -1,75 +0,0 @@\n-from pathlib import Path\n-\n-from setuptools import find_packages, setup\n-\n-version = (Path(__file__).parent / \"scrapy/VERSION\").read_text(\"ascii\").strip()\n-\n-\n-install_requires = [\n-    \"Twisted>=21.7.0\",\n-    \"cryptography>=37.0.0\",\n-    \"cssselect>=0.9.1\",\n-    \"itemloaders>=1.0.1\",\n-    \"parsel>=1.5.0\",\n-    \"pyOpenSSL>=22.0.0\",\n-    \"queuelib>=1.4.2\",\n-    \"service_identity>=18.1.0\",\n-    \"w3lib>=1.17.0\",\n-    \"zope.interface>=5.1.0\",\n-    \"protego>=0.1.15\",\n-    \"itemadapter>=0.1.0\",\n-    \"packaging\",\n-    \"tldextract\",\n-    \"lxml>=4.6.0\",\n-    \"defusedxml>=0.7.1\",\n-]\n-extras_require = {\n-    ':platform_python_implementation == \"CPython\"': [\"PyDispatcher>=2.0.5\"],\n-    ':platform_python_implementation == \"PyPy\"': [\"PyPyDispatcher>=2.1.0\"],\n-}\n-\n-\n-setup(\n-    name=\"Scrapy\",\n-    version=version,\n-    url=\"https://scrapy.org\",\n-    project_urls={\n-        \"Documentation\": \"https://docs.scrapy.org/\",\n-        \"Source\": \"https://github.com/scrapy/scrapy\",\n-        \"Tracker\": \"https://github.com/scrapy/scrapy/issues\",\n-    },\n-    description=\"A high-level Web Crawling and Web Scraping framework\",\n-    long_description=open(\"README.rst\", encoding=\"utf-8\").read(),\n-    author=\"Scrapy developers\",\n-    author_email=\"pablo@pablohoffman.com\",\n-    maintainer=\"Pablo Hoffman\",\n-    maintainer_email=\"pablo@pablohoffman.com\",\n-    license=\"BSD\",\n-    packages=find_packages(exclude=(\"tests\", \"tests.*\")),\n-    include_package_data=True,\n-    zip_safe=False,\n-    entry_points={\"console_scripts\": [\"scrapy = scrapy.cmdline:execute\"]},\n-    classifiers=[\n-        \"Framework :: Scrapy\",\n-        \"Development Status :: 5 - Production/Stable\",\n-        \"Environment :: Console\",\n-        \"Intended Audience :: Developers\",\n-        \"License :: OSI Approved :: BSD License\",\n-        \"Operating System :: OS Independent\",\n-        \"Programming Language :: Python\",\n-        \"Programming Language :: Python :: 3\",\n-        \"Programming Language :: Python :: 3.9\",\n-        \"Programming Language :: Python :: 3.10\",\n-        \"Programming Language :: Python :: 3.11\",\n-        \"Programming Language :: Python :: 3.12\",\n-        \"Programming Language :: Python :: 3.13\",\n-        \"Programming Language :: Python :: Implementation :: CPython\",\n-        \"Programming Language :: Python :: Implementation :: PyPy\",\n-        \"Topic :: Internet :: WWW/HTTP\",\n-        \"Topic :: Software Development :: Libraries :: Application Frameworks\",\n-        \"Topic :: Software Development :: Libraries :: Python Modules\",\n-    ],\n-    python_requires=\">=3.9\",\n-    install_requires=install_requires,\n-    extras_require=extras_require,\n-)\ndiff --git a/tox.ini b/tox.ini\nindex 5783a0e6172..4e1a99473f5 100644\n--- a/tox.ini\n+++ b/tox.ini\n@@ -79,7 +79,7 @@ deps =\n     {[testenv:extra-deps]deps}\n     pylint==3.2.5\n commands =\n-    pylint conftest.py docs extras scrapy setup.py tests\n+    pylint conftest.py docs extras scrapy tests\n \n [testenv:twinecheck]\n basepython = python3\n", "test_patch": "diff --git a/pytest.ini b/pytest.ini\ndeleted file mode 100644\nindex 824c0e9e91b..00000000000\n--- a/pytest.ini\n+++ /dev/null\n@@ -1,26 +0,0 @@\n-[pytest]\n-xfail_strict = true\n-usefixtures = chdir\n-python_files=test_*.py __init__.py\n-python_classes=\n-addopts =\n-    --assert=plain\n-    --ignore=docs/_ext\n-    --ignore=docs/conf.py\n-    --ignore=docs/news.rst\n-    --ignore=docs/topics/dynamic-content.rst\n-    --ignore=docs/topics/items.rst\n-    --ignore=docs/topics/leaks.rst\n-    --ignore=docs/topics/loaders.rst\n-    --ignore=docs/topics/selectors.rst\n-    --ignore=docs/topics/shell.rst\n-    --ignore=docs/topics/stats.rst\n-    --ignore=docs/topics/telnetconsole.rst\n-    --ignore=docs/utils\n-markers =\n-    only_asyncio: marks tests as only enabled when --reactor=asyncio is passed\n-    only_not_asyncio: marks tests as only enabled when --reactor=asyncio is not passed\n-    requires_uvloop: marks tests as only enabled when uvloop is known to be working\n-    requires_botocore: marks tests that need botocore (but not boto3)\n-    requires_boto3: marks tests that need botocore and boto3\n-filterwarnings =\ndiff --git a/tests/test_crawler.py b/tests/test_crawler.py\nindex 853acf2ded3..a77531f6216 100644\n--- a/tests/test_crawler.py\n+++ b/tests/test_crawler.py\n@@ -899,7 +899,7 @@ def test_shutdown_forced(self):\n         p.expect_exact(\"shutting down gracefully\")\n         # sending the second signal too fast often causes problems\n         d = Deferred()\n-        reactor.callLater(0.1, d.callback, None)\n+        reactor.callLater(0.01, d.callback, None)\n         yield d\n         p.kill(sig)\n         p.expect_exact(\"forcing unclean shutdown\")\ndiff --git a/tests/test_spiderloader/__init__.py b/tests/test_spiderloader/__init__.py\nindex d2ff9ba488f..9b53b9b9631 100644\n--- a/tests/test_spiderloader/__init__.py\n+++ b/tests/test_spiderloader/__init__.py\n@@ -144,9 +144,10 @@ def test_syntax_error_exception(self):\n             self.assertRaises(SyntaxError, SpiderLoader.from_settings, settings)\n \n     def test_syntax_error_warning(self):\n-        with warnings.catch_warnings(record=True) as w, mock.patch.object(\n-            SpiderLoader, \"_load_spiders\"\n-        ) as m:\n+        with (\n+            warnings.catch_warnings(record=True) as w,\n+            mock.patch.object(SpiderLoader, \"_load_spiders\") as m,\n+        ):\n             m.side_effect = SyntaxError\n             module = \"tests.test_spiderloader.test_spiders.spider1\"\n             settings = Settings(\n", "problem_statement": "Migrate from setup.py to pyproject.toml\nWe should migrate to the modern declarative setuptools metadata approach as discussed in https://setuptools.pypa.io/en/latest/userguide/quickstart.html and https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html, but only after the 2.12 release.\nMigrate to pyproject.toml\nCloses #6514\n", "hints_text": "\n", "created_at": "2024-11-16T20:48:39Z"}
{"repo": "scrapy/scrapy", "pull_number": 6542, "instance_id": "scrapy__scrapy-6542", "issue_numbers": ["6505"], "base_commit": "ab5cb7c7d9e268b501009d991d97ca19b6f7fe96", "patch": "diff --git a/scrapy/contracts/__init__.py b/scrapy/contracts/__init__.py\nindex 9071395e3d9..3b4f932a014 100644\n--- a/scrapy/contracts/__init__.py\n+++ b/scrapy/contracts/__init__.py\n@@ -38,9 +38,7 @@ def add_pre_hook(self, request: Request, results: TestResult) -> Request:\n             assert cb is not None\n \n             @wraps(cb)\n-            def wrapper(  # pylint: disable=inconsistent-return-statements\n-                response: Response, **cb_kwargs: Any\n-            ) -> list[Any]:\n+            def wrapper(response: Response, **cb_kwargs: Any) -> list[Any]:\n                 try:\n                     results.startTest(self.testcase_pre)\n                     self.pre_process(response)\n@@ -51,13 +49,10 @@ def wrapper(  # pylint: disable=inconsistent-return-statements\n                     results.addError(self.testcase_pre, sys.exc_info())\n                 else:\n                     results.addSuccess(self.testcase_pre)\n-                finally:\n-                    cb_result = cb(response, **cb_kwargs)\n-                    if isinstance(cb_result, (AsyncGenerator, CoroutineType)):\n-                        raise TypeError(\"Contracts don't support async callbacks\")\n-                    return list(  # pylint: disable=return-in-finally\n-                        cast(Iterable[Any], iterate_spider_output(cb_result))\n-                    )\n+                cb_result = cb(response, **cb_kwargs)\n+                if isinstance(cb_result, (AsyncGenerator, CoroutineType)):\n+                    raise TypeError(\"Contracts don't support async callbacks\")\n+                return list(cast(Iterable[Any], iterate_spider_output(cb_result)))\n \n             request.callback = wrapper\n \n@@ -69,9 +64,7 @@ def add_post_hook(self, request: Request, results: TestResult) -> Request:\n             assert cb is not None\n \n             @wraps(cb)\n-            def wrapper(  # pylint: disable=inconsistent-return-statements\n-                response: Response, **cb_kwargs: Any\n-            ) -> list[Any]:\n+            def wrapper(response: Response, **cb_kwargs: Any) -> list[Any]:\n                 cb_result = cb(response, **cb_kwargs)\n                 if isinstance(cb_result, (AsyncGenerator, CoroutineType)):\n                     raise TypeError(\"Contracts don't support async callbacks\")\n@@ -86,8 +79,7 @@ def wrapper(  # pylint: disable=inconsistent-return-statements\n                     results.addError(self.testcase_post, sys.exc_info())\n                 else:\n                     results.addSuccess(self.testcase_post)\n-                finally:\n-                    return output  # pylint: disable=return-in-finally\n+                return output\n \n             request.callback = wrapper\n \n", "test_patch": "diff --git a/tests/test_contracts.py b/tests/test_contracts.py\nindex d578b3af450..b0cb92d12d9 100644\n--- a/tests/test_contracts.py\n+++ b/tests/test_contracts.py\n@@ -556,3 +556,61 @@ def test_inherited_contracts(self):\n \n         requests = self.conman.from_spider(spider, self.results)\n         self.assertTrue(requests)\n+\n+\n+class CustomFailContractPreProcess(Contract):\n+    name = \"test_contract\"\n+\n+    def pre_process(self, response):\n+        raise KeyboardInterrupt(\"Pre-process exception\")\n+\n+\n+class CustomFailContractPostProcess(Contract):\n+    name = \"test_contract\"\n+\n+    def post_process(self, response):\n+        raise KeyboardInterrupt(\"Post-process exception\")\n+\n+\n+class CustomContractPrePostProcess(unittest.TestCase):\n+\n+    def setUp(self):\n+        self.results = TextTestResult(stream=None, descriptions=False, verbosity=0)\n+\n+    def test_pre_hook_keyboard_interrupt(self):\n+        spider = TestSpider()\n+        response = ResponseMock()\n+        contract = CustomFailContractPreProcess(spider.returns_request)\n+        conman = ContractsManager([contract])\n+\n+        try:\n+            request = conman.from_method(spider.returns_request, self.results)\n+            contract.add_pre_hook(request, self.results)\n+            # Expect this to raise a KeyboardInterrupt\n+            request.callback(response, **request.cb_kwargs)\n+        except KeyboardInterrupt as e:\n+            self.assertEqual(str(e), \"Pre-process exception\")\n+        else:\n+            self.fail(\"KeyboardInterrupt not raised\")\n+\n+        self.assertFalse(self.results.failures)\n+        self.assertFalse(self.results.errors)\n+\n+    def test_post_hook_keyboard_interrupt(self):\n+        spider = TestSpider()\n+        response = ResponseMock()\n+        contract = CustomFailContractPostProcess(spider.returns_request)\n+        conman = ContractsManager([contract])\n+\n+        try:\n+            request = conman.from_method(spider.returns_request, self.results)\n+            contract.add_post_hook(request, self.results)\n+            # Expect this to raise a KeyboardInterrupt\n+            request.callback(response, **request.cb_kwargs)\n+        except KeyboardInterrupt as e:\n+            self.assertEqual(str(e), \"Post-process exception\")\n+        else:\n+            self.fail(\"KeyboardInterrupt not raised\")\n+\n+        self.assertFalse(self.results.failures)\n+        self.assertFalse(self.results.errors)\n", "problem_statement": "return in finally can swallow exception\n### Description\r\n\r\nThere are two places in `scrapy/contracts/__init__.py` where a `finally:` body contains a `return` statement, which would swallow any in-flight exception. \r\n\r\nThis means that if a `BaseException` (such as `KeyboardInterrupt`) is raised from the body, or any exception is raised from one of the `except:` clause, it will not propagate on as expected. \r\n\r\nThe pylint warning about this was suppressed in [this commit](https://github.com/scrapy/scrapy/commit/991121fa91aee4d428ae09e75427d4e91970a41b) but it doesn't seem like there was justification for that.\r\n\r\nThese are the two locations:\r\nhttps://github.com/scrapy/scrapy/blame/b4bad97eae6bcce790a626d244c63589f4134408/scrapy/contracts/__init__.py#L56\r\n\r\nhttps://github.com/scrapy/scrapy/blame/b4bad97eae6bcce790a626d244c63589f4134408/scrapy/contracts/__init__.py#L86\r\n\r\nSee also https://docs.python.org/3/tutorial/errors.html#defining-clean-up-actions.\n", "hints_text": "> If the finally clause executes a [break](https://docs.python.org/3/reference/simple_stmts.html#break), [continue](https://docs.python.org/3/reference/simple_stmts.html#continue) or [return](https://docs.python.org/3/reference/simple_stmts.html#return) statement, exceptions are not re-raised.\r\n\r\nTIL\nHey,  \r\nWhat about re-raising the issue with a general raise statement in every except block along with putting the return statement outside the finally block?  \r\nIf this solution seems promising, I'd like to contribute to the same. I would appreciate your insights.\nI don't remember why I silenced them :-/\nIs this issue still open?\n@divyranjan17 it is.\r\n\r\n@AdityaS8804 I don't think that makes sense to me.\nFrom a quick look, it seems to me that simply taking the return part out of finally, and put it after the try-except blocks, could be all that\u2019s needed. But we should also first write tests that detect the issues before we address those issues.\n> From a quick look, it seems to me that simply taking the return part out of finally, and put it after the try-except blocks, could be all that\u2019s needed.\r\n\r\nThis matches my first impression.\nIs there a way to reproduce a this failure?\n> Is there a way to reproduce a this failure?\r\n\r\nFor the first issue, for example, it seems like raising `KeyboardInterrupt` from an implementation of https://docs.scrapy.org/en/2.11/topics/contracts.html#scrapy.contracts.Contract.pre_process should see that exception raise, but will instead silence it.\n\r\nI can think of 3 ways to tackle this issue\r\n\r\n#### Solution 1: Using a Temporary Variable for Return Value\r\nWe can capture the callback result in a variable outside the `finally` block and then return it at the end of the function. By avoiding `return` inside `finally`, exceptions propagate naturally, allowing errors to be handled as expected.\r\n\r\n**Simply:**\r\n\r\n- Store the callback output in a variable (e.g., `cb_result`).\r\n- Avoid using `return` in the `finally` block.\r\n- Return `cb_result` at the end of the function, outside of any `try/finally` structure.\r\n\r\n```python\r\ncb_result = None\r\ntry:\r\n    cb_result = cb(response, **cb_kwargs)\r\nfinally:\r\n    pass  # Any final cleanup here\r\nreturn list(iterate_spider_output(cb_result))\r\n```\r\n\r\n#### Solution 2: Separating Error Logging and Result Processing\r\n- Create a helper function, `log_results()`, to handle logging outcomes.\r\n- Call `log_results()` within `try/except` blocks to process success or errors.\r\n- Return `cb_result` outside of the `try` block without `finally`.\r\n\r\n```python\r\ndef log_results(testcase, exception_info=None):\r\n    if exception_info:\r\n        # Log failure\r\n```\r\n\r\n#### Solution 3: Wrapping Return Values with Exception Handling\r\n- Define `process_result` to manage callback outputs while capturing exceptions.\r\n- Invoke `process_result` instead of a direct return in the callback.\r\n- Ensure all exception info is handled without using a return in `finally`.\r\n\r\n```python\r\ndef process_result(cb, response, **cb_kwargs):\r\n    try:\r\n        cb_result = cb(response, **cb_kwargs)\r\n    except Exception as exc:\r\n        log_error(exc)\r\n    return list(iterate_spider_output(cb_result))\r\n```\r\n\nIs this AI-generated?\nyes Sol.1 and Sol.3 were suggested by github copilot \nThat's unfortunate, especially as the way forward was already suggested earlier.\n[Here](https://github.com/scrapy/scrapy/issues/6505#issuecomment-2434584223), specifically.\n> [Here](https://github.com/scrapy/scrapy/issues/6505#issuecomment-2434584223), specifically.\r\n\r\nUnderstood", "created_at": "2024-11-14T03:19:30Z"}
{"repo": "scrapy/scrapy", "pull_number": 6540, "instance_id": "scrapy__scrapy-6540", "issue_numbers": ["6534"], "base_commit": "b042ad255db139adc740cd97047b6607889f9f1c", "patch": "diff --git a/docs/topics/email.rst b/docs/topics/email.rst\nindex d6a7ad354cb..8f7a2357a5a 100644\n--- a/docs/topics/email.rst\n+++ b/docs/topics/email.rst\n@@ -27,13 +27,13 @@ the standard ``__init__`` method:\n \n     mailer = MailSender()\n \n-Or you can instantiate it passing a Scrapy settings object, which will respect\n-the :ref:`settings <topics-email-settings>`:\n+Or you can instantiate it passing a :class:`scrapy.Crawler` instance, which\n+will respect the :ref:`settings <topics-email-settings>`:\n \n .. skip: start\n .. code-block:: python\n \n-    mailer = MailSender.from_settings(settings)\n+    mailer = MailSender.from_crawler(crawler)\n \n And here is how to use it to send an e-mail (without attachments):\n \n@@ -81,13 +81,13 @@ rest of the framework.\n     :param smtpssl: enforce using a secure SSL connection\n     :type smtpssl: bool\n \n-    .. classmethod:: from_settings(settings)\n+    .. classmethod:: from_crawler(crawler)\n \n-        Instantiate using a Scrapy settings object, which will respect\n-        :ref:`these Scrapy settings <topics-email-settings>`.\n+        Instantiate using a :class:`scrapy.Crawler` instance, which will\n+        respect :ref:`these Scrapy settings <topics-email-settings>`.\n \n-        :param settings: the e-mail recipients\n-        :type settings: :class:`scrapy.settings.Settings` object\n+        :param crawler: the crawler\n+        :type settings: :class:`scrapy.Crawler` object\n \n     .. method:: send(to, subject, body, cc=None, attachs=(), mimetype='text/plain', charset=None)\n \ndiff --git a/docs/topics/request-response.rst b/docs/topics/request-response.rst\nindex 7c15b67e8f3..710e2e1314e 100644\n--- a/docs/topics/request-response.rst\n+++ b/docs/topics/request-response.rst\n@@ -488,7 +488,7 @@ A request fingerprinter is a class that must implement the following method:\n    :param request: request to fingerprint\n    :type request: scrapy.http.Request\n \n-Additionally, it may also implement the following methods:\n+Additionally, it may also implement the following method:\n \n .. classmethod:: from_crawler(cls, crawler)\n    :noindex:\n@@ -504,13 +504,6 @@ Additionally, it may also implement the following methods:\n    :param crawler: crawler that uses this request fingerprinter\n    :type crawler: :class:`~scrapy.crawler.Crawler` object\n \n-.. classmethod:: from_settings(cls, settings)\n-\n-   If present, and ``from_crawler`` is not defined, this class method is called\n-   to create a request fingerprinter instance from a\n-   :class:`~scrapy.settings.Settings` object. It must return a new instance of\n-   the request fingerprinter.\n-\n .. currentmodule:: scrapy.http\n \n The :meth:`fingerprint` method of the default request fingerprinter,\ndiff --git a/scrapy/core/downloader/contextfactory.py b/scrapy/core/downloader/contextfactory.py\nindex f80f832a706..8e17eab9aa7 100644\n--- a/scrapy/core/downloader/contextfactory.py\n+++ b/scrapy/core/downloader/contextfactory.py\n@@ -21,6 +21,7 @@\n     ScrapyClientTLSOptions,\n     openssl_methods,\n )\n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.misc import build_from_crawler, load_object\n \n if TYPE_CHECKING:\n@@ -69,6 +70,31 @@ def from_settings(\n         method: int = SSL.SSLv23_METHOD,\n         *args: Any,\n         **kwargs: Any,\n+    ) -> Self:\n+        warnings.warn(\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return cls._from_settings(settings, method, *args, **kwargs)\n+\n+    @classmethod\n+    def from_crawler(\n+        cls,\n+        crawler: Crawler,\n+        method: int = SSL.SSLv23_METHOD,\n+        *args: Any,\n+        **kwargs: Any,\n+    ) -> Self:\n+        return cls._from_settings(crawler.settings, method, *args, **kwargs)\n+\n+    @classmethod\n+    def _from_settings(\n+        cls,\n+        settings: BaseSettings,\n+        method: int = SSL.SSLv23_METHOD,\n+        *args: Any,\n+        **kwargs: Any,\n     ) -> Self:\n         tls_verbose_logging: bool = settings.getbool(\n             \"DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING\"\ndiff --git a/scrapy/dupefilters.py b/scrapy/dupefilters.py\nindex d37d2741a48..7b8eea135e7 100644\n--- a/scrapy/dupefilters.py\n+++ b/scrapy/dupefilters.py\n@@ -1,9 +1,11 @@\n from __future__ import annotations\n \n import logging\n+import warnings\n from pathlib import Path\n from typing import TYPE_CHECKING\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.job import job_dir\n from scrapy.utils.request import (\n     RequestFingerprinter,\n@@ -26,6 +28,15 @@\n class BaseDupeFilter:\n     @classmethod\n     def from_settings(cls, settings: BaseSettings) -> Self:\n+        warnings.warn(\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return cls()\n+\n+    @classmethod\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         return cls()\n \n     def request_seen(self, request: Request) -> bool:\n@@ -72,17 +83,31 @@ def from_settings(\n         *,\n         fingerprinter: RequestFingerprinterProtocol | None = None,\n     ) -> Self:\n-        debug = settings.getbool(\"DUPEFILTER_DEBUG\")\n-        return cls(job_dir(settings), debug, fingerprinter=fingerprinter)\n+        warnings.warn(\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return cls._from_settings(settings, fingerprinter=fingerprinter)\n \n     @classmethod\n     def from_crawler(cls, crawler: Crawler) -> Self:\n         assert crawler.request_fingerprinter\n-        return cls.from_settings(\n+        return cls._from_settings(\n             crawler.settings,\n             fingerprinter=crawler.request_fingerprinter,\n         )\n \n+    @classmethod\n+    def _from_settings(\n+        cls,\n+        settings: BaseSettings,\n+        *,\n+        fingerprinter: RequestFingerprinterProtocol | None = None,\n+    ) -> Self:\n+        debug = settings.getbool(\"DUPEFILTER_DEBUG\")\n+        return cls(job_dir(settings), debug, fingerprinter=fingerprinter)\n+\n     def request_seen(self, request: Request) -> bool:\n         fp = self.request_fingerprint(request)\n         if fp in self.fingerprints:\ndiff --git a/scrapy/extensions/feedexport.py b/scrapy/extensions/feedexport.py\nindex 6ab88dbb467..27f0b79ae01 100644\n--- a/scrapy/extensions/feedexport.py\n+++ b/scrapy/extensions/feedexport.py\n@@ -62,6 +62,11 @@ def build_storage(\n     preargs: Iterable[Any] = (),\n     **kwargs: Any,\n ) -> _StorageT:\n+    warnings.warn(\n+        \"scrapy.extensions.feedexport.build_storage() is deprecated, call the builder directly.\",\n+        category=ScrapyDeprecationWarning,\n+        stacklevel=2,\n+    )\n     kwargs[\"feed_options\"] = feed_options\n     return builder(*preargs, uri, *args, **kwargs)\n \n@@ -248,8 +253,7 @@ def from_crawler(\n         *,\n         feed_options: dict[str, Any] | None = None,\n     ) -> Self:\n-        return build_storage(\n-            cls,\n+        return cls(\n             uri,\n             access_key=crawler.settings[\"AWS_ACCESS_KEY_ID\"],\n             secret_key=crawler.settings[\"AWS_SECRET_ACCESS_KEY\"],\n@@ -323,10 +327,9 @@ def from_crawler(\n         *,\n         feed_options: dict[str, Any] | None = None,\n     ) -> Self:\n-        return build_storage(\n-            cls,\n+        return cls(\n             uri,\n-            crawler.settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\"),\n+            use_active_mode=crawler.settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\"),\n             feed_options=feed_options,\n         )\n \n@@ -407,15 +410,12 @@ def start_exporting(self) -> None:\n             self.exporter.start_exporting()\n             self._exporting = True\n \n-    def _get_instance(\n-        self, objcls: type[BaseItemExporter], *args: Any, **kwargs: Any\n-    ) -> BaseItemExporter:\n-        return build_from_crawler(objcls, self.crawler, *args, **kwargs)\n-\n     def _get_exporter(\n         self, file: IO[bytes], format: str, *args: Any, **kwargs: Any\n     ) -> BaseItemExporter:\n-        return self._get_instance(self.exporters[format], file, *args, **kwargs)\n+        return build_from_crawler(\n+            self.exporters[format], self.crawler, file, *args, **kwargs\n+        )\n \n     def finish_exporting(self) -> None:\n         if self._exporting:\n@@ -692,34 +692,8 @@ def _storage_supported(self, uri: str, feed_options: dict[str, Any]) -> bool:\n     def _get_storage(\n         self, uri: str, feed_options: dict[str, Any]\n     ) -> FeedStorageProtocol:\n-        \"\"\"Fork of create_instance specific to feed storage classes\n-\n-        It supports not passing the *feed_options* parameters to classes that\n-        do not support it, and issuing a deprecation warning instead.\n-        \"\"\"\n         feedcls = self.storages.get(urlparse(uri).scheme, self.storages[\"file\"])\n-        crawler = getattr(self, \"crawler\", None)\n-\n-        def build_instance(\n-            builder: type[FeedStorageProtocol], *preargs: Any\n-        ) -> FeedStorageProtocol:\n-            return build_storage(\n-                builder, uri, feed_options=feed_options, preargs=preargs\n-            )\n-\n-        instance: FeedStorageProtocol\n-        if crawler and hasattr(feedcls, \"from_crawler\"):\n-            instance = build_instance(feedcls.from_crawler, crawler)\n-            method_name = \"from_crawler\"\n-        elif hasattr(feedcls, \"from_settings\"):\n-            instance = build_instance(feedcls.from_settings, self.settings)\n-            method_name = \"from_settings\"\n-        else:\n-            instance = build_instance(feedcls)\n-            method_name = \"__new__\"\n-        if instance is None:\n-            raise TypeError(f\"{feedcls.__qualname__}.{method_name} returned None\")\n-        return instance\n+        return build_from_crawler(feedcls, self.crawler, uri, feed_options=feed_options)\n \n     def _get_uri_params(\n         self,\ndiff --git a/scrapy/extensions/memusage.py b/scrapy/extensions/memusage.py\nindex 73d864d5dc1..d7f810107bd 100644\n--- a/scrapy/extensions/memusage.py\n+++ b/scrapy/extensions/memusage.py\n@@ -48,7 +48,7 @@ def __init__(self, crawler: Crawler):\n         self.check_interval: float = crawler.settings.getfloat(\n             \"MEMUSAGE_CHECK_INTERVAL_SECONDS\"\n         )\n-        self.mail: MailSender = MailSender.from_settings(crawler.settings)\n+        self.mail: MailSender = MailSender.from_crawler(crawler)\n         crawler.signals.connect(self.engine_started, signal=signals.engine_started)\n         crawler.signals.connect(self.engine_stopped, signal=signals.engine_stopped)\n \ndiff --git a/scrapy/extensions/statsmailer.py b/scrapy/extensions/statsmailer.py\nindex 600eebcf2de..22162864205 100644\n--- a/scrapy/extensions/statsmailer.py\n+++ b/scrapy/extensions/statsmailer.py\n@@ -33,7 +33,7 @@ def from_crawler(cls, crawler: Crawler) -> Self:\n         recipients: list[str] = crawler.settings.getlist(\"STATSMAILER_RCPTS\")\n         if not recipients:\n             raise NotConfigured\n-        mail: MailSender = MailSender.from_settings(crawler.settings)\n+        mail: MailSender = MailSender.from_crawler(crawler)\n         assert crawler.stats\n         o = cls(crawler.stats, recipients, mail)\n         crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\ndiff --git a/scrapy/mail.py b/scrapy/mail.py\nindex ce7beb77307..3c40fea34c6 100644\n--- a/scrapy/mail.py\n+++ b/scrapy/mail.py\n@@ -7,6 +7,7 @@\n from __future__ import annotations\n \n import logging\n+import warnings\n from email import encoders as Encoders\n from email.mime.base import MIMEBase\n from email.mime.multipart import MIMEMultipart\n@@ -19,6 +20,7 @@\n from twisted.internet import ssl\n from twisted.internet.defer import Deferred\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.python import to_bytes\n \n@@ -32,6 +34,7 @@\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy.crawler import Crawler\n     from scrapy.settings import BaseSettings\n \n \n@@ -72,6 +75,19 @@ def __init__(\n \n     @classmethod\n     def from_settings(cls, settings: BaseSettings) -> Self:\n+        warnings.warn(\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return cls._from_settings(settings)\n+\n+    @classmethod\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n+        return cls._from_settings(crawler.settings)\n+\n+    @classmethod\n+    def _from_settings(cls, settings: BaseSettings) -> Self:\n         return cls(\n             smtphost=settings[\"MAIL_HOST\"],\n             mailfrom=settings[\"MAIL_FROM\"],\ndiff --git a/scrapy/middleware.py b/scrapy/middleware.py\nindex b6a4278952b..2b67dcd21a1 100644\n--- a/scrapy/middleware.py\n+++ b/scrapy/middleware.py\n@@ -2,12 +2,13 @@\n \n import logging\n import pprint\n+import warnings\n from collections import defaultdict, deque\n from typing import TYPE_CHECKING, Any, TypeVar, cast\n \n-from scrapy.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.utils.defer import process_chain, process_parallel\n-from scrapy.utils.misc import build_from_crawler, build_from_settings, load_object\n+from scrapy.utils.misc import build_from_crawler, load_object\n \n if TYPE_CHECKING:\n     from collections.abc import Callable, Iterable\n@@ -20,7 +21,7 @@\n \n     from scrapy import Spider\n     from scrapy.crawler import Crawler\n-    from scrapy.settings import Settings\n+    from scrapy.settings import BaseSettings, Settings\n \n     _P = ParamSpec(\"_P\")\n \n@@ -50,8 +51,33 @@ def __init__(self, *middlewares: Any) -> None:\n     def _get_mwlist_from_settings(cls, settings: Settings) -> list[Any]:\n         raise NotImplementedError\n \n+    @staticmethod\n+    def _build_from_settings(objcls: type[_T], settings: BaseSettings) -> _T:\n+        if hasattr(objcls, \"from_settings\"):\n+            instance = objcls.from_settings(settings)  # type: ignore[attr-defined]\n+            method_name = \"from_settings\"\n+        else:\n+            instance = objcls()\n+            method_name = \"__new__\"\n+        if instance is None:\n+            raise TypeError(f\"{objcls.__qualname__}.{method_name} returned None\")\n+        return cast(_T, instance)\n+\n     @classmethod\n     def from_settings(cls, settings: Settings, crawler: Crawler | None = None) -> Self:\n+        warnings.warn(\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return cls._from_settings(settings, crawler)\n+\n+    @classmethod\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n+        return cls._from_settings(crawler.settings, crawler)\n+\n+    @classmethod\n+    def _from_settings(cls, settings: Settings, crawler: Crawler | None = None) -> Self:\n         mwlist = cls._get_mwlist_from_settings(settings)\n         middlewares = []\n         enabled = []\n@@ -61,7 +87,7 @@ def from_settings(cls, settings: Settings, crawler: Crawler | None = None) -> Se\n                 if crawler is not None:\n                     mw = build_from_crawler(mwcls, crawler)\n                 else:\n-                    mw = build_from_settings(mwcls, settings)\n+                    mw = MiddlewareManager._build_from_settings(mwcls, settings)\n                 middlewares.append(mw)\n                 enabled.append(clspath)\n             except NotConfigured as e:\n@@ -82,10 +108,6 @@ def from_settings(cls, settings: Settings, crawler: Crawler | None = None) -> Se\n         )\n         return cls(*middlewares)\n \n-    @classmethod\n-    def from_crawler(cls, crawler: Crawler) -> Self:\n-        return cls.from_settings(crawler.settings, crawler)\n-\n     def _add_middleware(self, mw: Any) -> None:\n         if hasattr(mw, \"open_spider\"):\n             self.methods[\"open_spider\"].append(mw.open_spider)\ndiff --git a/scrapy/pipelines/files.py b/scrapy/pipelines/files.py\nindex 4a8639c220b..196b54acb7f 100644\n--- a/scrapy/pipelines/files.py\n+++ b/scrapy/pipelines/files.py\n@@ -12,6 +12,7 @@\n import logging\n import mimetypes\n import time\n+import warnings\n from collections import defaultdict\n from contextlib import suppress\n from ftplib import FTP\n@@ -24,16 +25,17 @@\n from twisted.internet.defer import Deferred, maybeDeferred\n from twisted.internet.threads import deferToThread\n \n-from scrapy.exceptions import IgnoreRequest, NotConfigured\n+from scrapy.exceptions import IgnoreRequest, NotConfigured, ScrapyDeprecationWarning\n from scrapy.http import Request, Response\n from scrapy.http.request import NO_CALLBACK\n from scrapy.pipelines.media import FileInfo, FileInfoOrError, MediaPipeline\n-from scrapy.settings import Settings\n+from scrapy.settings import BaseSettings, Settings\n from scrapy.utils.boto import is_botocore_available\n from scrapy.utils.datatypes import CaseInsensitiveDict\n+from scrapy.utils.deprecate import method_is_overridden\n from scrapy.utils.ftp import ftp_store_file\n from scrapy.utils.log import failure_to_exc_info\n-from scrapy.utils.python import to_bytes\n+from scrapy.utils.python import get_func_args, global_object_name, to_bytes\n from scrapy.utils.request import referer_str\n \n if TYPE_CHECKING:\n@@ -46,6 +48,7 @@\n     from typing_extensions import Self\n \n     from scrapy import Spider\n+    from scrapy.crawler import Crawler\n \n \n logger = logging.getLogger(__name__)\n@@ -443,12 +446,24 @@ def __init__(\n         store_uri: str | PathLike[str],\n         download_func: Callable[[Request, Spider], Response] | None = None,\n         settings: Settings | dict[str, Any] | None = None,\n+        *,\n+        crawler: Crawler | None = None,\n     ):\n         store_uri = _to_string(store_uri)\n         if not store_uri:\n             raise NotConfigured\n \n-        if isinstance(settings, dict) or settings is None:\n+        if crawler is not None:\n+            if settings is not None:\n+                warnings.warn(\n+                    f\"FilesPipeline.__init__() was called with a crawler instance and a settings instance\"\n+                    f\" when creating {global_object_name(self.__class__)}. The settings instance will be ignored\"\n+                    f\" and crawler.settings will be used. The settings argument will be removed in a future Scrapy version.\",\n+                    category=ScrapyDeprecationWarning,\n+                    stacklevel=2,\n+                )\n+            settings = crawler.settings\n+        elif isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n         cls_name = \"FilesPipeline\"\n         self.store: FilesStoreProtocol = self._get_store(store_uri)\n@@ -467,10 +482,54 @@ def __init__(\n             resolve(\"FILES_RESULT_FIELD\"), self.FILES_RESULT_FIELD\n         )\n \n-        super().__init__(download_func=download_func, settings=settings)\n+        super().__init__(\n+            download_func=download_func,\n+            settings=settings if not crawler else None,\n+            crawler=crawler,\n+        )\n \n     @classmethod\n     def from_settings(cls, settings: Settings) -> Self:\n+        warnings.warn(\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return cls._from_settings(settings, None)\n+\n+    @classmethod\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n+        if method_is_overridden(cls, FilesPipeline, \"from_settings\"):\n+            warnings.warn(\n+                f\"{global_object_name(cls)} overrides FilesPipeline.from_settings().\"\n+                f\" This method is deprecated and won't be called in future Scrapy versions,\"\n+                f\" please update your code so that it overrides from_crawler() instead.\",\n+                category=ScrapyDeprecationWarning,\n+            )\n+            o = cls.from_settings(crawler.settings)\n+            o._finish_init(crawler)\n+            return o\n+        return cls._from_settings(crawler.settings, crawler)\n+\n+    @classmethod\n+    def _from_settings(cls, settings: Settings, crawler: Crawler | None) -> Self:\n+        cls._update_stores(settings)\n+        store_uri = settings[\"FILES_STORE\"]\n+        if \"crawler\" in get_func_args(cls.__init__):\n+            o = cls(store_uri, crawler=crawler)\n+        else:\n+            o = cls(store_uri, settings=settings)\n+            if crawler:\n+                o._finish_init(crawler)\n+            warnings.warn(\n+                f\"{global_object_name(cls)}.__init__() doesn't take a crawler argument.\"\n+                \" This is deprecated and the argument will be required in future Scrapy versions.\",\n+                category=ScrapyDeprecationWarning,\n+            )\n+        return o\n+\n+    @classmethod\n+    def _update_stores(cls, settings: BaseSettings) -> None:\n         s3store: type[S3FilesStore] = cast(type[S3FilesStore], cls.STORE_SCHEMES[\"s3\"])\n         s3store.AWS_ACCESS_KEY_ID = settings[\"AWS_ACCESS_KEY_ID\"]\n         s3store.AWS_SECRET_ACCESS_KEY = settings[\"AWS_SECRET_ACCESS_KEY\"]\n@@ -494,9 +553,6 @@ def from_settings(cls, settings: Settings) -> Self:\n         ftp_store.FTP_PASSWORD = settings[\"FTP_PASSWORD\"]\n         ftp_store.USE_ACTIVE_MODE = settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\")\n \n-        store_uri = settings[\"FILES_STORE\"]\n-        return cls(store_uri, settings=settings)\n-\n     def _get_store(self, uri: str) -> FilesStoreProtocol:\n         if Path(uri).is_absolute():  # to support win32 paths like: C:\\\\some\\dir\n             scheme = \"file\"\ndiff --git a/scrapy/pipelines/images.py b/scrapy/pipelines/images.py\nindex 2c4c9376e49..e86e7c4930e 100644\n--- a/scrapy/pipelines/images.py\n+++ b/scrapy/pipelines/images.py\n@@ -8,25 +8,19 @@\n \n import functools\n import hashlib\n+import warnings\n from contextlib import suppress\n from io import BytesIO\n-from typing import TYPE_CHECKING, Any, cast\n+from typing import TYPE_CHECKING, Any\n \n from itemadapter import ItemAdapter\n \n-from scrapy.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.http import Request, Response\n from scrapy.http.request import NO_CALLBACK\n-from scrapy.pipelines.files import (\n-    FileException,\n-    FilesPipeline,\n-    FTPFilesStore,\n-    GCSFilesStore,\n-    S3FilesStore,\n-    _md5sum,\n-)\n+from scrapy.pipelines.files import FileException, FilesPipeline, _md5sum\n from scrapy.settings import Settings\n-from scrapy.utils.python import to_bytes\n+from scrapy.utils.python import get_func_args, global_object_name, to_bytes\n \n if TYPE_CHECKING:\n     from collections.abc import Callable, Iterable\n@@ -38,6 +32,7 @@\n     from typing_extensions import Self\n \n     from scrapy import Spider\n+    from scrapy.crawler import Crawler\n     from scrapy.pipelines.media import FileInfoOrError, MediaPipeline\n \n \n@@ -64,6 +59,8 @@ def __init__(\n         store_uri: str | PathLike[str],\n         download_func: Callable[[Request, Spider], Response] | None = None,\n         settings: Settings | dict[str, Any] | None = None,\n+        *,\n+        crawler: Crawler | None = None,\n     ):\n         try:\n             from PIL import Image\n@@ -74,9 +71,24 @@ def __init__(\n                 \"ImagesPipeline requires installing Pillow 4.0.0 or later\"\n             )\n \n-        super().__init__(store_uri, settings=settings, download_func=download_func)\n+        super().__init__(\n+            store_uri,\n+            settings=settings if not crawler else None,\n+            download_func=download_func,\n+            crawler=crawler,\n+        )\n \n-        if isinstance(settings, dict) or settings is None:\n+        if crawler is not None:\n+            if settings is not None:\n+                warnings.warn(\n+                    f\"ImagesPipeline.__init__() was called with a crawler instance and a settings instance\"\n+                    f\" when creating {global_object_name(self.__class__)}. The settings instance will be ignored\"\n+                    f\" and crawler.settings will be used. The settings argument will be removed in a future Scrapy version.\",\n+                    category=ScrapyDeprecationWarning,\n+                    stacklevel=2,\n+                )\n+            settings = crawler.settings\n+        elif isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n \n         resolve = functools.partial(\n@@ -108,32 +120,21 @@ def __init__(\n         )\n \n     @classmethod\n-    def from_settings(cls, settings: Settings) -> Self:\n-        s3store: type[S3FilesStore] = cast(type[S3FilesStore], cls.STORE_SCHEMES[\"s3\"])\n-        s3store.AWS_ACCESS_KEY_ID = settings[\"AWS_ACCESS_KEY_ID\"]\n-        s3store.AWS_SECRET_ACCESS_KEY = settings[\"AWS_SECRET_ACCESS_KEY\"]\n-        s3store.AWS_SESSION_TOKEN = settings[\"AWS_SESSION_TOKEN\"]\n-        s3store.AWS_ENDPOINT_URL = settings[\"AWS_ENDPOINT_URL\"]\n-        s3store.AWS_REGION_NAME = settings[\"AWS_REGION_NAME\"]\n-        s3store.AWS_USE_SSL = settings[\"AWS_USE_SSL\"]\n-        s3store.AWS_VERIFY = settings[\"AWS_VERIFY\"]\n-        s3store.POLICY = settings[\"IMAGES_STORE_S3_ACL\"]\n-\n-        gcs_store: type[GCSFilesStore] = cast(\n-            type[GCSFilesStore], cls.STORE_SCHEMES[\"gs\"]\n-        )\n-        gcs_store.GCS_PROJECT_ID = settings[\"GCS_PROJECT_ID\"]\n-        gcs_store.POLICY = settings[\"IMAGES_STORE_GCS_ACL\"] or None\n-\n-        ftp_store: type[FTPFilesStore] = cast(\n-            type[FTPFilesStore], cls.STORE_SCHEMES[\"ftp\"]\n-        )\n-        ftp_store.FTP_USERNAME = settings[\"FTP_USER\"]\n-        ftp_store.FTP_PASSWORD = settings[\"FTP_PASSWORD\"]\n-        ftp_store.USE_ACTIVE_MODE = settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\")\n-\n+    def _from_settings(cls, settings: Settings, crawler: Crawler | None) -> Self:\n+        cls._update_stores(settings)\n         store_uri = settings[\"IMAGES_STORE\"]\n-        return cls(store_uri, settings=settings)\n+        if \"crawler\" in get_func_args(cls.__init__):\n+            o = cls(store_uri, crawler=crawler)\n+        else:\n+            o = cls(store_uri, settings=settings)\n+            if crawler:\n+                o._finish_init(crawler)\n+            warnings.warn(\n+                f\"{global_object_name(cls)}.__init__() doesn't take a crawler argument.\"\n+                \" This is deprecated and the argument will be required in future Scrapy versions.\",\n+                category=ScrapyDeprecationWarning,\n+            )\n+        return o\n \n     def file_downloaded(\n         self,\ndiff --git a/scrapy/pipelines/media.py b/scrapy/pipelines/media.py\nindex b10ec147b34..6d7808c31b4 100644\n--- a/scrapy/pipelines/media.py\n+++ b/scrapy/pipelines/media.py\n@@ -2,6 +2,7 @@\n \n import functools\n import logging\n+import warnings\n from abc import ABC, abstractmethod\n from collections import defaultdict\n from typing import (\n@@ -20,12 +21,14 @@\n from twisted.python.failure import Failure\n from twisted.python.versions import Version\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http.request import NO_CALLBACK, Request\n from scrapy.settings import Settings\n from scrapy.utils.datatypes import SequenceExclude\n from scrapy.utils.defer import defer_result, mustbe_deferred\n from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.misc import arg_to_iter\n+from scrapy.utils.python import get_func_args, global_object_name\n \n if TYPE_CHECKING:\n     from collections.abc import Callable\n@@ -38,7 +41,6 @@\n     from scrapy.http import Response\n     from scrapy.utils.request import RequestFingerprinter\n \n-\n _T = TypeVar(\"_T\")\n \n \n@@ -51,13 +53,13 @@ class FileInfo(TypedDict):\n \n FileInfoOrError = Union[tuple[Literal[True], FileInfo], tuple[Literal[False], Failure]]\n \n-\n logger = logging.getLogger(__name__)\n \n \n class MediaPipeline(ABC):\n     crawler: Crawler\n     _fingerprinter: RequestFingerprinter\n+    _modern_init = False\n \n     LOG_FAILED_RESULTS: bool = True\n \n@@ -74,10 +76,22 @@ def __init__(\n         self,\n         download_func: Callable[[Request, Spider], Response] | None = None,\n         settings: Settings | dict[str, Any] | None = None,\n+        *,\n+        crawler: Crawler | None = None,\n     ):\n         self.download_func = download_func\n \n-        if isinstance(settings, dict) or settings is None:\n+        if crawler is not None:\n+            if settings is not None:\n+                warnings.warn(\n+                    f\"MediaPipeline.__init__() was called with a crawler instance and a settings instance\"\n+                    f\" when creating {global_object_name(self.__class__)}. The settings instance will be ignored\"\n+                    f\" and crawler.settings will be used. The settings argument will be removed in a future Scrapy version.\",\n+                    category=ScrapyDeprecationWarning,\n+                    stacklevel=2,\n+                )\n+            settings = crawler.settings\n+        elif isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n         resolve = functools.partial(\n             self._key_for_pipe, base_class_name=\"MediaPipeline\", settings=settings\n@@ -87,6 +101,27 @@ def __init__(\n         )\n         self._handle_statuses(self.allow_redirects)\n \n+        if crawler:\n+            self._finish_init(crawler)\n+            self._modern_init = True\n+        else:\n+            warnings.warn(\n+                f\"MediaPipeline.__init__() was called without the crawler argument\"\n+                f\" when creating {global_object_name(self.__class__)}.\"\n+                f\" This is deprecated and the argument will be required in future Scrapy versions.\",\n+                category=ScrapyDeprecationWarning,\n+                stacklevel=2,\n+            )\n+\n+    def _finish_init(self, crawler: Crawler) -> None:\n+        # This was done in from_crawler() before 2.12, now it's done in __init__()\n+        # if the crawler was passed to it and may be needed to be called in other\n+        # deprecated code paths explicitly too. After the crawler argument of __init__()\n+        # becomes mandatory this should be inlined there.\n+        self.crawler = crawler\n+        assert crawler.request_fingerprinter\n+        self._fingerprinter = crawler.request_fingerprinter\n+\n     def _handle_statuses(self, allow_redirects: bool) -> None:\n         self.handle_httpstatus_list = None\n         if allow_redirects:\n@@ -112,13 +147,19 @@ def _key_for_pipe(\n     @classmethod\n     def from_crawler(cls, crawler: Crawler) -> Self:\n         pipe: Self\n-        try:\n+        if hasattr(cls, \"from_settings\"):\n             pipe = cls.from_settings(crawler.settings)  # type: ignore[attr-defined]\n-        except AttributeError:\n+        elif \"crawler\" in get_func_args(cls.__init__):\n+            pipe = cls(crawler=crawler)\n+        else:\n             pipe = cls()\n-        pipe.crawler = crawler\n-        assert crawler.request_fingerprinter\n-        pipe._fingerprinter = crawler.request_fingerprinter\n+            warnings.warn(\n+                f\"{global_object_name(cls)}.__init__() doesn't take a crawler argument.\"\n+                \" This is deprecated and the argument will be required in future Scrapy versions.\",\n+                category=ScrapyDeprecationWarning,\n+            )\n+        if not pipe._modern_init:\n+            pipe._finish_init(crawler)\n         return pipe\n \n     def open_spider(self, spider: Spider) -> None:\ndiff --git a/scrapy/spidermiddlewares/urllength.py b/scrapy/spidermiddlewares/urllength.py\nindex 191adb6cd32..a1cd1bb7cfa 100644\n--- a/scrapy/spidermiddlewares/urllength.py\n+++ b/scrapy/spidermiddlewares/urllength.py\n@@ -7,9 +7,10 @@\n from __future__ import annotations\n \n import logging\n+import warnings\n from typing import TYPE_CHECKING, Any\n \n-from scrapy.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.http import Request, Response\n \n if TYPE_CHECKING:\n@@ -19,6 +20,7 @@\n     from typing_extensions import Self\n \n     from scrapy import Spider\n+    from scrapy.crawler import Crawler\n     from scrapy.settings import BaseSettings\n \n \n@@ -31,6 +33,19 @@ def __init__(self, maxlength: int):\n \n     @classmethod\n     def from_settings(cls, settings: BaseSettings) -> Self:\n+        warnings.warn(\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return cls._from_settings(settings)\n+\n+    @classmethod\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n+        return cls._from_settings(crawler.settings)\n+\n+    @classmethod\n+    def _from_settings(cls, settings: BaseSettings) -> Self:\n         maxlength = settings.getint(\"URLLENGTH_LIMIT\")\n         if not maxlength:\n             raise NotConfigured\ndiff --git a/scrapy/utils/misc.py b/scrapy/utils/misc.py\nindex 12c09839f0f..a408a205dda 100644\n--- a/scrapy/utils/misc.py\n+++ b/scrapy/utils/misc.py\n@@ -26,7 +26,6 @@\n \n     from scrapy import Spider\n     from scrapy.crawler import Crawler\n-    from scrapy.settings import BaseSettings\n \n \n _ITERABLE_SINGLE_VALUES = dict, Item, str, bytes\n@@ -150,7 +149,7 @@ def create_instance(objcls, settings, crawler, *args, **kwargs):\n     \"\"\"\n     warnings.warn(\n         \"The create_instance() function is deprecated. \"\n-        \"Please use build_from_crawler() or build_from_settings() instead.\",\n+        \"Please use build_from_crawler() instead.\",\n         category=ScrapyDeprecationWarning,\n         stacklevel=2,\n     )\n@@ -176,7 +175,7 @@ def create_instance(objcls, settings, crawler, *args, **kwargs):\n def build_from_crawler(\n     objcls: type[T], crawler: Crawler, /, *args: Any, **kwargs: Any\n ) -> T:\n-    \"\"\"Construct a class instance using its ``from_crawler`` constructor.\n+    \"\"\"Construct a class instance using its ``from_crawler`` or ``from_settings`` constructor.\n \n     ``*args`` and ``**kwargs`` are forwarded to the constructor.\n \n@@ -186,6 +185,14 @@ def build_from_crawler(\n         instance = objcls.from_crawler(crawler, *args, **kwargs)  # type: ignore[attr-defined]\n         method_name = \"from_crawler\"\n     elif hasattr(objcls, \"from_settings\"):\n+        warnings.warn(\n+            f\"{objcls.__qualname__} has from_settings() but not from_crawler().\"\n+            \" This is deprecated and calling from_settings() will be removed in a future\"\n+            \" Scrapy version. You can implement a simple from_crawler() that calls\"\n+            \" from_settings() with crawler.settings.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n         instance = objcls.from_settings(crawler.settings, *args, **kwargs)  # type: ignore[attr-defined]\n         method_name = \"from_settings\"\n     else:\n@@ -196,26 +203,6 @@ def build_from_crawler(\n     return cast(T, instance)\n \n \n-def build_from_settings(\n-    objcls: type[T], settings: BaseSettings, /, *args: Any, **kwargs: Any\n-) -> T:\n-    \"\"\"Construct a class instance using its ``from_settings`` constructor.\n-\n-    ``*args`` and ``**kwargs`` are forwarded to the constructor.\n-\n-    Raises ``TypeError`` if the resulting instance is ``None``.\n-    \"\"\"\n-    if hasattr(objcls, \"from_settings\"):\n-        instance = objcls.from_settings(settings, *args, **kwargs)  # type: ignore[attr-defined]\n-        method_name = \"from_settings\"\n-    else:\n-        instance = objcls(*args, **kwargs)\n-        method_name = \"__new__\"\n-    if instance is None:\n-        raise TypeError(f\"{objcls.__qualname__}.{method_name} returned None\")\n-    return cast(T, instance)\n-\n-\n @contextmanager\n def set_environ(**kwargs: str) -> Iterator[None]:\n     \"\"\"Temporarily set environment variables inside the context manager and\n", "test_patch": "diff --git a/tests/test_dupefilters.py b/tests/test_dupefilters.py\nindex 9ba8bd64f40..4fd648f4834 100644\n--- a/tests/test_dupefilters.py\n+++ b/tests/test_dupefilters.py\n@@ -33,14 +33,6 @@ def from_crawler(cls, crawler):\n         return df\n \n \n-class FromSettingsRFPDupeFilter(RFPDupeFilter):\n-    @classmethod\n-    def from_settings(cls, settings, *, fingerprinter=None):\n-        df = super().from_settings(settings, fingerprinter=fingerprinter)\n-        df.method = \"from_settings\"\n-        return df\n-\n-\n class DirectDupeFilter:\n     method = \"n/a\"\n \n@@ -56,16 +48,6 @@ def test_df_from_crawler_scheduler(self):\n         self.assertTrue(scheduler.df.debug)\n         self.assertEqual(scheduler.df.method, \"from_crawler\")\n \n-    def test_df_from_settings_scheduler(self):\n-        settings = {\n-            \"DUPEFILTER_DEBUG\": True,\n-            \"DUPEFILTER_CLASS\": FromSettingsRFPDupeFilter,\n-        }\n-        crawler = get_crawler(settings_dict=settings)\n-        scheduler = Scheduler.from_crawler(crawler)\n-        self.assertTrue(scheduler.df.debug)\n-        self.assertEqual(scheduler.df.method, \"from_settings\")\n-\n     def test_df_direct_scheduler(self):\n         settings = {\n             \"DUPEFILTER_CLASS\": DirectDupeFilter,\ndiff --git a/tests/test_middleware.py b/tests/test_middleware.py\nindex a42c7b3d1e2..3a1cf19ad30 100644\n--- a/tests/test_middleware.py\n+++ b/tests/test_middleware.py\n@@ -2,7 +2,7 @@\n \n from scrapy.exceptions import NotConfigured\n from scrapy.middleware import MiddlewareManager\n-from scrapy.settings import Settings\n+from scrapy.utils.test import get_crawler\n \n \n class M1:\n@@ -23,8 +23,6 @@ def open_spider(self, spider):\n     def close_spider(self, spider):\n         pass\n \n-    pass\n-\n \n class M3:\n     def process(self, response, request, spider):\n@@ -83,7 +81,7 @@ def test_enabled(self):\n         self.assertEqual(mwman.middlewares, (m1, m2, m3))\n \n     def test_enabled_from_settings(self):\n-        settings = Settings()\n-        mwman = TestMiddlewareManager.from_settings(settings)\n+        crawler = get_crawler()\n+        mwman = TestMiddlewareManager.from_crawler(crawler)\n         classes = [x.__class__ for x in mwman.middlewares]\n         self.assertEqual(classes, [M1, M3])\ndiff --git a/tests/test_pipeline_files.py b/tests/test_pipeline_files.py\nindex 47840caaa16..9dcb3e4d18d 100644\n--- a/tests/test_pipeline_files.py\n+++ b/tests/test_pipeline_files.py\n@@ -2,6 +2,7 @@\n import os\n import random\n import time\n+import warnings\n from datetime import datetime\n from io import BytesIO\n from pathlib import Path\n@@ -25,7 +26,6 @@\n     GCSFilesStore,\n     S3FilesStore,\n )\n-from scrapy.settings import Settings\n from scrapy.utils.test import (\n     assert_gcs_environ,\n     get_crawler,\n@@ -217,8 +217,8 @@ class CustomFilesPipeline(FilesPipeline):\n             def file_path(self, request, response=None, info=None, item=None):\n                 return f'full/{item.get(\"path\")}'\n \n-        file_path = CustomFilesPipeline.from_settings(\n-            Settings({\"FILES_STORE\": self.tempdir})\n+        file_path = CustomFilesPipeline.from_crawler(\n+            get_crawler(None, {\"FILES_STORE\": self.tempdir})\n         ).file_path\n         item = {\"path\": \"path-to-store-file\"}\n         request = Request(\"http://example.com\")\n@@ -235,7 +235,9 @@ def tearDown(self):\n     def test_item_fields_default(self):\n         url = \"http://www.example.com/files/1.txt\"\n         item = self.item_class(name=\"item1\", file_urls=[url])\n-        pipeline = FilesPipeline.from_settings(Settings({\"FILES_STORE\": self.tempdir}))\n+        pipeline = FilesPipeline.from_crawler(\n+            get_crawler(None, {\"FILES_STORE\": self.tempdir})\n+        )\n         requests = list(pipeline.get_media_requests(item, None))\n         self.assertEqual(requests[0].url, url)\n         results = [(True, {\"url\": url})]\n@@ -247,13 +249,14 @@ def test_item_fields_default(self):\n     def test_item_fields_override_settings(self):\n         url = \"http://www.example.com/files/1.txt\"\n         item = self.item_class(name=\"item1\", custom_file_urls=[url])\n-        pipeline = FilesPipeline.from_settings(\n-            Settings(\n+        pipeline = FilesPipeline.from_crawler(\n+            get_crawler(\n+                None,\n                 {\n                     \"FILES_STORE\": self.tempdir,\n                     \"FILES_URLS_FIELD\": \"custom_file_urls\",\n                     \"FILES_RESULT_FIELD\": \"custom_files\",\n-                }\n+                },\n             )\n         )\n         requests = list(pipeline.get_media_requests(item, None))\n@@ -371,8 +374,10 @@ def test_different_settings_for_different_instances(self):\n         different settings.\n         \"\"\"\n         custom_settings = self._generate_fake_settings()\n-        another_pipeline = FilesPipeline.from_settings(Settings(custom_settings))\n-        one_pipeline = FilesPipeline(self.tempdir)\n+        another_pipeline = FilesPipeline.from_crawler(\n+            get_crawler(None, custom_settings)\n+        )\n+        one_pipeline = FilesPipeline(self.tempdir, crawler=get_crawler(None))\n         for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n             default_value = self.default_cls_settings[pipe_attr]\n             self.assertEqual(getattr(one_pipeline, pipe_attr), default_value)\n@@ -385,7 +390,7 @@ def test_subclass_attributes_preserved_if_no_settings(self):\n         If subclasses override class attributes and there are no special settings those values should be kept.\n         \"\"\"\n         pipe_cls = self._generate_fake_pipeline()\n-        pipe = pipe_cls.from_settings(Settings({\"FILES_STORE\": self.tempdir}))\n+        pipe = pipe_cls.from_crawler(get_crawler(None, {\"FILES_STORE\": self.tempdir}))\n         for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n             custom_value = getattr(pipe, pipe_ins_attr)\n             self.assertNotEqual(custom_value, self.default_cls_settings[pipe_attr])\n@@ -398,7 +403,7 @@ def test_subclass_attrs_preserved_custom_settings(self):\n         \"\"\"\n         pipeline_cls = self._generate_fake_pipeline()\n         settings = self._generate_fake_settings()\n-        pipeline = pipeline_cls.from_settings(Settings(settings))\n+        pipeline = pipeline_cls.from_crawler(get_crawler(None, settings))\n         for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n             value = getattr(pipeline, pipe_ins_attr)\n             setting_value = settings.get(settings_attr)\n@@ -414,8 +419,8 @@ def test_no_custom_settings_for_subclasses(self):\n         class UserDefinedFilesPipeline(FilesPipeline):\n             pass\n \n-        user_pipeline = UserDefinedFilesPipeline.from_settings(\n-            Settings({\"FILES_STORE\": self.tempdir})\n+        user_pipeline = UserDefinedFilesPipeline.from_crawler(\n+            get_crawler(None, {\"FILES_STORE\": self.tempdir})\n         )\n         for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n             # Values from settings for custom pipeline should be set on pipeline instance.\n@@ -433,7 +438,9 @@ class UserDefinedFilesPipeline(FilesPipeline):\n \n         prefix = UserDefinedFilesPipeline.__name__.upper()\n         settings = self._generate_fake_settings(prefix=prefix)\n-        user_pipeline = UserDefinedFilesPipeline.from_settings(Settings(settings))\n+        user_pipeline = UserDefinedFilesPipeline.from_crawler(\n+            get_crawler(None, settings)\n+        )\n         for pipe_attr, settings_attr, pipe_inst_attr in self.file_cls_attr_settings_map:\n             # Values from settings for custom pipeline should be set on pipeline instance.\n             custom_value = settings.get(prefix + \"_\" + settings_attr)\n@@ -448,7 +455,7 @@ def test_custom_settings_and_class_attrs_for_subclasses(self):\n         pipeline_cls = self._generate_fake_pipeline()\n         prefix = pipeline_cls.__name__.upper()\n         settings = self._generate_fake_settings(prefix=prefix)\n-        user_pipeline = pipeline_cls.from_settings(Settings(settings))\n+        user_pipeline = pipeline_cls.from_crawler(get_crawler(None, settings))\n         for (\n             pipe_cls_attr,\n             settings_attr,\n@@ -463,8 +470,8 @@ class UserDefinedFilesPipeline(FilesPipeline):\n             DEFAULT_FILES_RESULT_FIELD = \"this\"\n             DEFAULT_FILES_URLS_FIELD = \"that\"\n \n-        pipeline = UserDefinedFilesPipeline.from_settings(\n-            Settings({\"FILES_STORE\": self.tempdir})\n+        pipeline = UserDefinedFilesPipeline.from_crawler(\n+            get_crawler(None, {\"FILES_STORE\": self.tempdir})\n         )\n         self.assertEqual(\n             pipeline.files_result_field,\n@@ -484,7 +491,7 @@ def test_user_defined_subclass_default_key_names(self):\n         class UserPipe(FilesPipeline):\n             pass\n \n-        pipeline_cls = UserPipe.from_settings(Settings(settings))\n+        pipeline_cls = UserPipe.from_crawler(get_crawler(None, settings))\n \n         for pipe_attr, settings_attr, pipe_inst_attr in self.file_cls_attr_settings_map:\n             expected_value = settings.get(settings_attr)\n@@ -495,8 +502,8 @@ class CustomFilesPipelineWithPathLikeDir(FilesPipeline):\n             def file_path(self, request, response=None, info=None, *, item=None):\n                 return Path(\"subdir\") / Path(request.url).name\n \n-        pipeline = CustomFilesPipelineWithPathLikeDir.from_settings(\n-            Settings({\"FILES_STORE\": Path(\"./Temp\")})\n+        pipeline = CustomFilesPipelineWithPathLikeDir.from_crawler(\n+            get_crawler(None, {\"FILES_STORE\": Path(\"./Temp\")})\n         )\n         request = Request(\"http://example.com/image01.jpg\")\n         self.assertEqual(pipeline.file_path(request), Path(\"subdir/image01.jpg\"))\n@@ -687,3 +694,75 @@ def _prepare_request_object(item_url, flags=None):\n         item_url,\n         meta={\"response\": Response(item_url, status=200, body=b\"data\", flags=flags)},\n     )\n+\n+\n+# this is separate from the one in test_pipeline_media.py to specifically test FilesPipeline subclasses\n+class BuildFromCrawlerTestCase(unittest.TestCase):\n+    def setUp(self):\n+        self.tempdir = mkdtemp()\n+        self.crawler = get_crawler(None, {\"FILES_STORE\": self.tempdir})\n+\n+    def tearDown(self):\n+        rmtree(self.tempdir)\n+\n+    def test_simple(self):\n+        class Pipeline(FilesPipeline):\n+            pass\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 0)\n+            assert pipe.store\n+\n+    def test_has_old_init(self):\n+        class Pipeline(FilesPipeline):\n+            def __init__(self, store_uri, download_func=None, settings=None):\n+                super().__init__(store_uri, download_func, settings)\n+                self._init_called = True\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 2)\n+            assert pipe._init_called\n+\n+    def test_has_from_settings(self):\n+        class Pipeline(FilesPipeline):\n+            _from_settings_called = False\n+\n+            @classmethod\n+            def from_settings(cls, settings):\n+                o = super().from_settings(settings)\n+                o._from_settings_called = True\n+                return o\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 3)\n+            assert pipe.store\n+            assert pipe._from_settings_called\n+\n+    def test_has_from_crawler_and_init(self):\n+        class Pipeline(FilesPipeline):\n+            _from_crawler_called = False\n+\n+            @classmethod\n+            def from_crawler(cls, crawler):\n+                settings = crawler.settings\n+                store_uri = settings[\"FILES_STORE\"]\n+                o = cls(store_uri, crawler=crawler)\n+                o._from_crawler_called = True\n+                return o\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 0)\n+            assert pipe.store\n+            assert pipe._from_crawler_called\ndiff --git a/tests/test_pipeline_images.py b/tests/test_pipeline_images.py\nindex dfeead999d5..3ffef410249 100644\n--- a/tests/test_pipeline_images.py\n+++ b/tests/test_pipeline_images.py\n@@ -13,7 +13,7 @@\n from scrapy.http import Request, Response\n from scrapy.item import Field, Item\n from scrapy.pipelines.images import ImageException, ImagesPipeline\n-from scrapy.settings import Settings\n+from scrapy.utils.test import get_crawler\n \n skip_pillow: str | None\n try:\n@@ -33,7 +33,8 @@ class ImagesPipelineTestCase(unittest.TestCase):\n \n     def setUp(self):\n         self.tempdir = mkdtemp()\n-        self.pipeline = ImagesPipeline(self.tempdir)\n+        crawler = get_crawler()\n+        self.pipeline = ImagesPipeline(self.tempdir, crawler=crawler)\n \n     def tearDown(self):\n         rmtree(self.tempdir)\n@@ -123,8 +124,8 @@ def thumb_path(\n             ):\n                 return f\"thumb/{thumb_id}/{item.get('path')}\"\n \n-        thumb_path = CustomImagesPipeline.from_settings(\n-            Settings({\"IMAGES_STORE\": self.tempdir})\n+        thumb_path = CustomImagesPipeline.from_crawler(\n+            get_crawler(None, {\"IMAGES_STORE\": self.tempdir})\n         ).thumb_path\n         item = {\"path\": \"path-to-store-file\"}\n         request = Request(\"http://example.com\")\n@@ -218,8 +219,8 @@ class ImagesPipelineTestCaseFieldsMixin:\n     def test_item_fields_default(self):\n         url = \"http://www.example.com/images/1.jpg\"\n         item = self.item_class(name=\"item1\", image_urls=[url])\n-        pipeline = ImagesPipeline.from_settings(\n-            Settings({\"IMAGES_STORE\": \"s3://example/images/\"})\n+        pipeline = ImagesPipeline.from_crawler(\n+            get_crawler(None, {\"IMAGES_STORE\": \"s3://example/images/\"})\n         )\n         requests = list(pipeline.get_media_requests(item, None))\n         self.assertEqual(requests[0].url, url)\n@@ -232,13 +233,14 @@ def test_item_fields_default(self):\n     def test_item_fields_override_settings(self):\n         url = \"http://www.example.com/images/1.jpg\"\n         item = self.item_class(name=\"item1\", custom_image_urls=[url])\n-        pipeline = ImagesPipeline.from_settings(\n-            Settings(\n+        pipeline = ImagesPipeline.from_crawler(\n+            get_crawler(\n+                None,\n                 {\n                     \"IMAGES_STORE\": \"s3://example/images/\",\n                     \"IMAGES_URLS_FIELD\": \"custom_image_urls\",\n                     \"IMAGES_RESULT_FIELD\": \"custom_images\",\n-                }\n+                },\n             )\n         )\n         requests = list(pipeline.get_media_requests(item, None))\n@@ -389,9 +391,8 @@ def test_different_settings_for_different_instances(self):\n         have different settings.\n         \"\"\"\n         custom_settings = self._generate_fake_settings()\n-        default_settings = Settings()\n-        default_sts_pipe = ImagesPipeline(self.tempdir, settings=default_settings)\n-        user_sts_pipe = ImagesPipeline.from_settings(Settings(custom_settings))\n+        default_sts_pipe = ImagesPipeline(self.tempdir, crawler=get_crawler(None))\n+        user_sts_pipe = ImagesPipeline.from_crawler(get_crawler(None, custom_settings))\n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             expected_default_value = self.default_pipeline_settings.get(pipe_attr)\n             custom_value = custom_settings.get(settings_attr)\n@@ -407,7 +408,9 @@ def test_subclass_attrs_preserved_default_settings(self):\n         from class attributes.\n         \"\"\"\n         pipeline_cls = self._generate_fake_pipeline_subclass()\n-        pipeline = pipeline_cls.from_settings(Settings({\"IMAGES_STORE\": self.tempdir}))\n+        pipeline = pipeline_cls.from_crawler(\n+            get_crawler(None, {\"IMAGES_STORE\": self.tempdir})\n+        )\n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             # Instance attribute (lowercase) must be equal to class attribute (uppercase).\n             attr_value = getattr(pipeline, pipe_attr.lower())\n@@ -421,7 +424,7 @@ def test_subclass_attrs_preserved_custom_settings(self):\n         \"\"\"\n         pipeline_cls = self._generate_fake_pipeline_subclass()\n         settings = self._generate_fake_settings()\n-        pipeline = pipeline_cls.from_settings(Settings(settings))\n+        pipeline = pipeline_cls.from_crawler(get_crawler(None, settings))\n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             # Instance attribute (lowercase) must be equal to\n             # value defined in settings.\n@@ -439,8 +442,8 @@ def test_no_custom_settings_for_subclasses(self):\n         class UserDefinedImagePipeline(ImagesPipeline):\n             pass\n \n-        user_pipeline = UserDefinedImagePipeline.from_settings(\n-            Settings({\"IMAGES_STORE\": self.tempdir})\n+        user_pipeline = UserDefinedImagePipeline.from_crawler(\n+            get_crawler(None, {\"IMAGES_STORE\": self.tempdir})\n         )\n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             # Values from settings for custom pipeline should be set on pipeline instance.\n@@ -458,7 +461,9 @@ class UserDefinedImagePipeline(ImagesPipeline):\n \n         prefix = UserDefinedImagePipeline.__name__.upper()\n         settings = self._generate_fake_settings(prefix=prefix)\n-        user_pipeline = UserDefinedImagePipeline.from_settings(Settings(settings))\n+        user_pipeline = UserDefinedImagePipeline.from_crawler(\n+            get_crawler(None, settings)\n+        )\n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             # Values from settings for custom pipeline should be set on pipeline instance.\n             custom_value = settings.get(prefix + \"_\" + settings_attr)\n@@ -473,7 +478,7 @@ def test_custom_settings_and_class_attrs_for_subclasses(self):\n         pipeline_cls = self._generate_fake_pipeline_subclass()\n         prefix = pipeline_cls.__name__.upper()\n         settings = self._generate_fake_settings(prefix=prefix)\n-        user_pipeline = pipeline_cls.from_settings(Settings(settings))\n+        user_pipeline = pipeline_cls.from_crawler(get_crawler(None, settings))\n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             custom_value = settings.get(prefix + \"_\" + settings_attr)\n             self.assertNotEqual(custom_value, self.default_pipeline_settings[pipe_attr])\n@@ -484,8 +489,8 @@ class UserDefinedImagePipeline(ImagesPipeline):\n             DEFAULT_IMAGES_URLS_FIELD = \"something\"\n             DEFAULT_IMAGES_RESULT_FIELD = \"something_else\"\n \n-        pipeline = UserDefinedImagePipeline.from_settings(\n-            Settings({\"IMAGES_STORE\": self.tempdir})\n+        pipeline = UserDefinedImagePipeline.from_crawler(\n+            get_crawler(None, {\"IMAGES_STORE\": self.tempdir})\n         )\n         self.assertEqual(\n             pipeline.images_result_field,\n@@ -506,7 +511,7 @@ def test_user_defined_subclass_default_key_names(self):\n         class UserPipe(ImagesPipeline):\n             pass\n \n-        pipeline_cls = UserPipe.from_settings(Settings(settings))\n+        pipeline_cls = UserPipe.from_crawler(get_crawler(None, settings))\n \n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             expected_value = settings.get(settings_attr)\ndiff --git a/tests/test_pipeline_media.py b/tests/test_pipeline_media.py\nindex c979e45d70a..58a2d367825 100644\n--- a/tests/test_pipeline_media.py\n+++ b/tests/test_pipeline_media.py\n@@ -1,5 +1,7 @@\n from __future__ import annotations\n \n+import warnings\n+\n from testfixtures import LogCapture\n from twisted.internet import reactor\n from twisted.internet.defer import Deferred, inlineCallbacks\n@@ -11,7 +13,6 @@\n from scrapy.http.request import NO_CALLBACK\n from scrapy.pipelines.files import FileException\n from scrapy.pipelines.media import MediaPipeline\n-from scrapy.settings import Settings\n from scrapy.spiders import Spider\n from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.signal import disconnect_all\n@@ -175,8 +176,8 @@ def test_default_process_item(self):\n \n \n class MockedMediaPipeline(UserDefinedPipeline):\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n+    def __init__(self, *args, crawler=None, **kwargs):\n+        super().__init__(*args, crawler=crawler, **kwargs)\n         self._mockcalled = []\n \n     def download(self, request, info):\n@@ -377,7 +378,7 @@ def test_key_for_pipe(self):\n class MediaPipelineAllowRedirectSettingsTestCase(unittest.TestCase):\n \n     def _assert_request_no3xx(self, pipeline_class, settings):\n-        pipe = pipeline_class(settings=Settings(settings))\n+        pipe = pipeline_class(crawler=get_crawler(None, settings))\n         request = Request(\"http://url\")\n         pipe._modify_media_request(request)\n \n@@ -410,3 +411,115 @@ def test_subclass_specific_setting(self):\n         self._assert_request_no3xx(\n             UserDefinedPipeline, {\"USERDEFINEDPIPELINE_MEDIA_ALLOW_REDIRECTS\": True}\n         )\n+\n+\n+class BuildFromCrawlerTestCase(unittest.TestCase):\n+    def setUp(self):\n+        self.crawler = get_crawler(None, {\"FILES_STORE\": \"/foo\"})\n+\n+    def test_simple(self):\n+        class Pipeline(UserDefinedPipeline):\n+            pass\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 0)\n+\n+    def test_has_old_init(self):\n+        class Pipeline(UserDefinedPipeline):\n+            def __init__(self):\n+                super().__init__()\n+                self._init_called = True\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 2)\n+            assert pipe._init_called\n+\n+    def test_has_from_settings(self):\n+        class Pipeline(UserDefinedPipeline):\n+            _from_settings_called = False\n+\n+            @classmethod\n+            def from_settings(cls, settings):\n+                o = cls()\n+                o._from_settings_called = True\n+                return o\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 1)\n+            assert pipe._from_settings_called\n+\n+    def test_has_from_settings_and_init(self):\n+        class Pipeline(UserDefinedPipeline):\n+            _from_settings_called = False\n+\n+            def __init__(self, store_uri, settings):\n+                super().__init__()\n+                self._init_called = True\n+\n+            @classmethod\n+            def from_settings(cls, settings):\n+                store_uri = settings[\"FILES_STORE\"]\n+                o = cls(store_uri, settings=settings)\n+                o._from_settings_called = True\n+                return o\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 1)\n+            assert pipe._from_settings_called\n+            assert pipe._init_called\n+\n+    def test_has_from_crawler_and_init(self):\n+        class Pipeline(UserDefinedPipeline):\n+            _from_crawler_called = False\n+\n+            def __init__(self, store_uri, settings, *, crawler):\n+                super().__init__(crawler=crawler)\n+                self._init_called = True\n+\n+            @classmethod\n+            def from_crawler(cls, crawler):\n+                settings = crawler.settings\n+                store_uri = settings[\"FILES_STORE\"]\n+                o = cls(store_uri, settings=settings, crawler=crawler)\n+                o._from_crawler_called = True\n+                return o\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 0)\n+            assert pipe._from_crawler_called\n+            assert pipe._init_called\n+\n+    def test_has_from_crawler(self):\n+        class Pipeline(UserDefinedPipeline):\n+            _from_crawler_called = False\n+\n+            @classmethod\n+            def from_crawler(cls, crawler):\n+                settings = crawler.settings\n+                o = super().from_crawler(crawler)\n+                o._from_crawler_called = True\n+                o.store_uri = settings[\"FILES_STORE\"]\n+                return o\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            # this and the next assert will fail as MediaPipeline.from_crawler() wasn't called\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 0)\n+            assert pipe._from_crawler_called\ndiff --git a/tests/test_spidermiddleware_urllength.py b/tests/test_spidermiddleware_urllength.py\nindex 9111e4c82ab..1a0f2e223c4 100644\n--- a/tests/test_spidermiddleware_urllength.py\n+++ b/tests/test_spidermiddleware_urllength.py\n@@ -3,7 +3,6 @@\n from testfixtures import LogCapture\n \n from scrapy.http import Request, Response\n-from scrapy.settings import Settings\n from scrapy.spidermiddlewares.urllength import UrlLengthMiddleware\n from scrapy.spiders import Spider\n from scrapy.utils.test import get_crawler\n@@ -12,12 +11,10 @@\n class TestUrlLengthMiddleware(TestCase):\n     def setUp(self):\n         self.maxlength = 25\n-        settings = Settings({\"URLLENGTH_LIMIT\": self.maxlength})\n-\n-        crawler = get_crawler(Spider)\n+        crawler = get_crawler(Spider, {\"URLLENGTH_LIMIT\": self.maxlength})\n         self.spider = crawler._create_spider(\"foo\")\n         self.stats = crawler.stats\n-        self.mw = UrlLengthMiddleware.from_settings(settings)\n+        self.mw = UrlLengthMiddleware.from_crawler(crawler)\n \n         self.response = Response(\"http://scrapytest.org\")\n         self.short_url_req = Request(\"http://scrapytest.org/\")\ndiff --git a/tests/test_utils_misc/__init__.py b/tests/test_utils_misc/__init__.py\nindex 4d8e715210d..f71b2b034a9 100644\n--- a/tests/test_utils_misc/__init__.py\n+++ b/tests/test_utils_misc/__init__.py\n@@ -10,7 +10,6 @@\n from scrapy.utils.misc import (\n     arg_to_iter,\n     build_from_crawler,\n-    build_from_settings,\n     create_instance,\n     load_object,\n     rel_has_nofollow,\n@@ -197,39 +196,6 @@ def _test_with_crawler(mock, settings, crawler):\n         with self.assertRaises(TypeError):\n             build_from_crawler(m, crawler, *args, **kwargs)\n \n-    def test_build_from_settings(self):\n-        settings = mock.MagicMock()\n-        args = (True, 100.0)\n-        kwargs = {\"key\": \"val\"}\n-\n-        def _test_with_settings(mock, settings):\n-            build_from_settings(mock, settings, *args, **kwargs)\n-            if hasattr(mock, \"from_settings\"):\n-                mock.from_settings.assert_called_once_with(settings, *args, **kwargs)\n-                self.assertEqual(mock.call_count, 0)\n-            else:\n-                mock.assert_called_once_with(*args, **kwargs)\n-\n-        # Check usage of correct constructor using three mocks:\n-        #   1. with no alternative constructors\n-        #   2. with from_settings() constructor\n-        #   3. with from_settings() and from_crawler() constructor\n-        spec_sets = (\n-            [\"__qualname__\"],\n-            [\"__qualname__\", \"from_settings\"],\n-            [\"__qualname__\", \"from_settings\", \"from_crawler\"],\n-        )\n-        for specs in spec_sets:\n-            m = mock.MagicMock(spec_set=specs)\n-            _test_with_settings(m, settings)\n-            m.reset_mock()\n-\n-        # Check adoption of crawler settings\n-        m = mock.MagicMock(spec_set=[\"__qualname__\", \"from_settings\"])\n-        m.from_settings.return_value = None\n-        with self.assertRaises(TypeError):\n-            build_from_settings(m, settings, *args, **kwargs)\n-\n     def test_set_environ(self):\n         assert os.environ.get(\"some_test_environ\") is None\n         with set_environ(some_test_environ=\"test_value\"):\ndiff --git a/tests/test_utils_request.py b/tests/test_utils_request.py\nindex 965d050a4da..0a3e3b00be5 100644\n--- a/tests/test_utils_request.py\n+++ b/tests/test_utils_request.py\n@@ -8,6 +8,7 @@\n \n import pytest\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Request\n from scrapy.utils.python import to_bytes\n from scrapy.utils.request import (\n@@ -384,7 +385,9 @@ def fingerprint(self, request):\n             \"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter,\n             \"FINGERPRINT\": b\"fingerprint\",\n         }\n-        crawler = get_crawler(settings_dict=settings)\n+        with warnings.catch_warnings():\n+            warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n+            crawler = get_crawler(settings_dict=settings)\n \n         request = Request(\"http://www.example.com\")\n         fingerprint = crawler.request_fingerprinter.fingerprint(request)\ndiff --git a/tests/test_webclient.py b/tests/test_webclient.py\nindex cce119001ac..1797d5e1fcb 100644\n--- a/tests/test_webclient.py\n+++ b/tests/test_webclient.py\n@@ -9,25 +9,18 @@\n \n import OpenSSL.SSL\n from twisted.internet import defer, reactor\n-from twisted.trial import unittest\n-from twisted.web import resource, server, static, util\n-\n-try:\n-    from twisted.internet.testing import StringTransport\n-except ImportError:\n-    # deprecated in Twisted 19.7.0\n-    # (remove once we bump our requirement past that version)\n-    from twisted.test.proto_helpers import StringTransport\n-\n from twisted.internet.defer import inlineCallbacks\n+from twisted.internet.testing import StringTransport\n from twisted.protocols.policies import WrappingFactory\n+from twisted.trial import unittest\n+from twisted.web import resource, server, static, util\n \n from scrapy.core.downloader import webclient as client\n from scrapy.core.downloader.contextfactory import ScrapyClientContextFactory\n from scrapy.http import Headers, Request\n-from scrapy.settings import Settings\n-from scrapy.utils.misc import build_from_settings\n+from scrapy.utils.misc import build_from_crawler\n from scrapy.utils.python import to_bytes, to_unicode\n+from scrapy.utils.test import get_crawler\n from tests.mockserver import (\n     BrokenDownloadResource,\n     ErrorResource,\n@@ -469,22 +462,22 @@ class WebClientCustomCiphersSSLTestCase(WebClientSSLTestCase):\n \n     def testPayload(self):\n         s = \"0123456789\" * 10\n-        settings = Settings({\"DOWNLOADER_CLIENT_TLS_CIPHERS\": self.custom_ciphers})\n-        client_context_factory = build_from_settings(\n-            ScrapyClientContextFactory, settings\n+        crawler = get_crawler(\n+            settings_dict={\"DOWNLOADER_CLIENT_TLS_CIPHERS\": self.custom_ciphers}\n         )\n+        client_context_factory = build_from_crawler(ScrapyClientContextFactory, crawler)\n         return getPage(\n             self.getURL(\"payload\"), body=s, contextFactory=client_context_factory\n         ).addCallback(self.assertEqual, to_bytes(s))\n \n     def testPayloadDisabledCipher(self):\n         s = \"0123456789\" * 10\n-        settings = Settings(\n-            {\"DOWNLOADER_CLIENT_TLS_CIPHERS\": \"ECDHE-RSA-AES256-GCM-SHA384\"}\n-        )\n-        client_context_factory = build_from_settings(\n-            ScrapyClientContextFactory, settings\n+        crawler = get_crawler(\n+            settings_dict={\n+                \"DOWNLOADER_CLIENT_TLS_CIPHERS\": \"ECDHE-RSA-AES256-GCM-SHA384\"\n+            }\n         )\n+        client_context_factory = build_from_crawler(ScrapyClientContextFactory, crawler)\n         d = getPage(\n             self.getURL(\"payload\"), body=s, contextFactory=client_context_factory\n         )\n", "problem_statement": "Don't ship `build_from_settings()`\nWe discussed this with @kmike and decided that we want to ship `build_from_crawler()` but not `build_from_settings()` as a last-minute follow-up to #5523/#6169. This, to my knowledge, has two consequences:\r\n\r\n1. We need to change `scrapy.middleware.MiddlewareManager` to require a `Crawler` instance for building.\r\n2. Users that use `create_instance()` and pass `settings` but not `crawler` will need to change the logic when migrating to `build_from_crawler()`, but we think they should normally have a Crawler instance there.\r\n\r\n`build_from_crawler()` also has a wrong docstring as it doesn't mention `from_settings()`.\n", "hints_text": "`build_from_settings()` is also used in a test where we create a Settings instance to create a component instance with it and don't need a Crawler - this should be easy to fix.\r\n\r\nRegarding `scrapy.middleware.MiddlewareManager`: it currently has a `from_crawler()` that just calls `from_settings()` and `from_settings()` that can optionally take a Crawler instance. If the Crawler instance is passed, it is also passed to created middlewares via `build_from_crawler()` and if it isn't passed, the middlewares are created with `build_from_settings()`. So the ideal state is simple: it has a `from_crawler()` but not `from_settings()` and always passed the Crawler instance to created middleware. But as a temporary backwards-compatible state we want to keep both methods (with appropriate deprecation warnings) and be able to create middlewares without a Crawler instance, for which we should either keep (inlined?) a `build_from_settings()` function or use `create_instance()` there (which will print additional warnings). Thoughts?\r\n\r\nIn Scrapy itself `MiddlewareManager` is not used directly but only as a base class for: `scrapy.extension.ExtensionManager`, `scrapy.core.spidermw.SpiderMiddlewareManager`, `scrapy.core.downloader.middleware.DownloaderMiddlewareManager`, `scrapy.pipelines.ItemPipelineManager`. None of these override either `from_crawler()` or `from_settings()`. Instances of all of these are created via `from_crawler()`. Only `ItemPipelineManager` can be replaced (via `ITEM_PROCESSOR`) but it still needs to implement `from_crawler()` due to the previous statement. So we can safely drop the code path that doesn't take and pass a Crawler instance.\n(Additionally, if we don't have `build_from_settings()` we will be able deprecate the whole `from_settings()` interface later, though I don't know if we should)\n> we should either keep (inlined?) a `build_from_settings()` function or use `create_instance()` there (which will print additional warnings). Thoughts?\r\n\r\nI think it\u2019s best to avoid the `create_instance()` warning, be it by silencing it or by using an in-lined `build_from_settings()`, no opinion on that part. The `from_settings()` class method should emit its own, specific deprecation warning.\r\n\r\n> Additionally, if we don't have `build_from_settings()` we will be able deprecate the whole `from_settings()` interface later, though I don't know if we should\r\n\r\nUnless there is a good reason not to, it sounds consistent with deprecating `create_instance()` without adding `build_from_settings()`, so we should consider doing it for 2.12 already.", "created_at": "2024-11-12T16:34:43Z"}
=======
{"repo": "scrapy/scrapy", "pull_number": 5950, "instance_id": "scrapy__scrapy-5950", "issue_numbers": ["5992"], "base_commit": "510574216d70ec84d75639ebcda360834a992e47", "patch": "diff --git a/docs/index.rst b/docs/index.rst\nindex 5404969e02e..8798aebd132 100644\n--- a/docs/index.rst\n+++ b/docs/index.rst\n@@ -222,6 +222,7 @@ Extending Scrapy\n    :hidden:\n \n    topics/architecture\n+   topics/addons\n    topics/downloader-middleware\n    topics/spider-middleware\n    topics/extensions\n@@ -235,6 +236,9 @@ Extending Scrapy\n :doc:`topics/architecture`\n     Understand the Scrapy architecture.\n \n+:doc:`topics/addons`\n+    Enable and configure third-party extensions.\n+\n :doc:`topics/downloader-middleware`\n     Customize how pages get requested and downloaded.\n \ndiff --git a/docs/topics/addons.rst b/docs/topics/addons.rst\nnew file mode 100644\nindex 00000000000..1bf2172bd40\n--- /dev/null\n+++ b/docs/topics/addons.rst\n@@ -0,0 +1,193 @@\n+.. _topics-addons:\n+\n+=======\n+Add-ons\n+=======\n+\n+Scrapy's add-on system is a framework which unifies managing and configuring\n+components that extend Scrapy's core functionality, such as middlewares,\n+extensions, or pipelines. It provides users with a plug-and-play experience in\n+Scrapy extension management, and grants extensive configuration control to\n+developers.\n+\n+\n+Activating and configuring add-ons\n+==================================\n+\n+During :class:`~scrapy.crawler.Crawler` initialization, the list of enabled\n+add-ons is read from your ``ADDONS`` setting.\n+\n+The ``ADDONS`` setting is a dict in which every key is an add-on class or its\n+import path and the value is its priority.\n+\n+This is an example where two add-ons are enabled in a project's\n+``settings.py``::\n+\n+    ADDONS = {\n+        'path.to.someaddon': 0,\n+        SomeAddonClass: 1,\n+    }\n+\n+\n+Writing your own add-ons\n+========================\n+\n+Add-ons are Python classes that include the following method:\n+\n+.. method:: update_settings(settings)\n+\n+    This method is called during the initialization of the\n+    :class:`~scrapy.crawler.Crawler`. Here, you should perform dependency checks\n+    (e.g. for external Python libraries) and update the\n+    :class:`~scrapy.settings.Settings` object as wished, e.g. enable components\n+    for this add-on or set required configuration of other extensions.\n+\n+    :param settings: The settings object storing Scrapy/component configuration\n+    :type settings: :class:`~scrapy.settings.Settings`\n+\n+They can also have the following method:\n+\n+.. classmethod:: from_crawler(cls, crawler)\n+   :noindex:\n+\n+   If present, this class method is called to create an add-on instance\n+   from a :class:`~scrapy.crawler.Crawler`. It must return a new instance\n+   of the add-on. The crawler object provides access to all Scrapy core\n+   components like settings and signals; it is a way for the add-on to access\n+   them and hook its functionality into Scrapy.\n+\n+   :param crawler: The crawler that uses this add-on\n+   :type crawler: :class:`~scrapy.crawler.Crawler`\n+\n+The settings set by the add-on should use the ``addon`` priority (see\n+:ref:`populating-settings` and :func:`scrapy.settings.BaseSettings.set`)::\n+\n+    class MyAddon:\n+        def update_settings(self, settings):\n+            settings.set(\"DNSCACHE_ENABLED\", True, \"addon\")\n+\n+This allows users to override these settings in the project or spider\n+configuration. This is not possible with settings that are mutable objects,\n+such as the dict that is a value of :setting:`ITEM_PIPELINES`. In these cases\n+you can provide an add-on-specific setting that governs whether the add-on will\n+modify :setting:`ITEM_PIPELINES`::\n+\n+    class MyAddon:\n+        def update_settings(self, settings):\n+            if settings.getbool(\"MYADDON_ENABLE_PIPELINE\"):\n+                settings[\"ITEM_PIPELINES\"][\"path.to.mypipeline\"] = 200\n+\n+If the ``update_settings`` method raises\n+:exc:`scrapy.exceptions.NotConfigured`, the add-on will be skipped. This makes\n+it easy to enable an add-on only when some conditions are met.\n+\n+Fallbacks\n+---------\n+\n+Some components provided by add-ons need to fall back to \"default\"\n+implementations, e.g. a custom download handler needs to send the request that\n+it doesn't handle via the default download handler, or a stats collector that\n+includes some additional processing but otherwise uses the default stats\n+collector. And it's possible that a project needs to use several custom\n+components of the same type, e.g. two custom download handlers that support\n+different kinds of custom requests and still need to use the default download\n+handler for other requests. To make such use cases easier to configure, we\n+recommend that such custom components should be written in the following way:\n+\n+1. The custom component (e.g. ``MyDownloadHandler``) shouldn't inherit from the\n+   default Scrapy one (e.g.\n+   ``scrapy.core.downloader.handlers.http.HTTPDownloadHandler``), but instead\n+   be able to load the class of the fallback component from a special setting\n+   (e.g. ``MY_FALLBACK_DOWNLOAD_HANDLER``), create an instance of it and use\n+   it.\n+2. The add-ons that include these components should read the current value of\n+   the default setting (e.g. ``DOWNLOAD_HANDLERS``) in their\n+   ``update_settings()`` methods, save that value into the fallback setting\n+   (``MY_FALLBACK_DOWNLOAD_HANDLER`` mentioned earlier) and set the default\n+   setting to the component provided by the add-on (e.g.\n+   ``MyDownloadHandler``). If the fallback setting is already set by the user,\n+   they shouldn't change it.\n+3. This way, if there are several add-ons that want to modify the same setting,\n+   all of them will fallback to the component from the previous one and then to\n+   the Scrapy default. The order of that depends on the priority order in the\n+   ``ADDONS`` setting.\n+\n+\n+Add-on examples\n+===============\n+\n+Set some basic configuration:\n+\n+.. code-block:: python\n+\n+    class MyAddon:\n+        def update_settings(self, settings):\n+            settings[\"ITEM_PIPELINES\"][\"path.to.mypipeline\"] = 200\n+            settings.set(\"DNSCACHE_ENABLED\", True, \"addon\")\n+\n+Check dependencies:\n+\n+.. code-block:: python\n+\n+    class MyAddon:\n+        def update_settings(self, settings):\n+            try:\n+                import boto\n+            except ImportError:\n+                raise NotConfigured(\"MyAddon requires the boto library\")\n+            ...\n+\n+Access the crawler instance:\n+\n+.. code-block:: python\n+\n+    class MyAddon:\n+        def __init__(self, crawler) -> None:\n+            super().__init__()\n+            self.crawler = crawler\n+\n+        @classmethod\n+        def from_crawler(cls, crawler):\n+            return cls(crawler)\n+\n+        def update_settings(self, settings):\n+            ...\n+\n+Use a fallback component:\n+\n+.. code-block:: python\n+\n+    from scrapy.core.downloader.handlers.http import HTTPDownloadHandler\n+\n+\n+    FALLBACK_SETTING = \"MY_FALLBACK_DOWNLOAD_HANDLER\"\n+\n+\n+    class MyHandler:\n+        lazy = False\n+\n+        def __init__(self, settings, crawler):\n+            dhcls = load_object(settings.get(FALLBACK_SETTING))\n+            self._fallback_handler = create_instance(\n+                dhcls,\n+                settings=None,\n+                crawler=crawler,\n+            )\n+\n+        def download_request(self, request, spider):\n+            if request.meta.get(\"my_params\"):\n+                # handle the request\n+                ...\n+            else:\n+                return self._fallback_handler.download_request(request, spider)\n+\n+\n+    class MyAddon:\n+        def update_settings(self, settings):\n+            if not settings.get(FALLBACK_SETTING):\n+                settings.set(\n+                    FALLBACK_SETTING,\n+                    settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"],\n+                    \"addon\",\n+                )\n+            settings[\"DOWNLOAD_HANDLERS\"][\"https\"] = MyHandler\ndiff --git a/docs/topics/api.rst b/docs/topics/api.rst\nindex 26834487998..16c28405cfb 100644\n--- a/docs/topics/api.rst\n+++ b/docs/topics/api.rst\n@@ -137,6 +137,7 @@ Settings API\n         SETTINGS_PRIORITIES = {\n             \"default\": 0,\n             \"command\": 10,\n+            \"addon\": 15,\n             \"project\": 20,\n             \"spider\": 30,\n             \"cmdline\": 40,\ndiff --git a/docs/topics/settings.rst b/docs/topics/settings.rst\nindex 3e06d84f90b..602ab587d7e 100644\n--- a/docs/topics/settings.rst\n+++ b/docs/topics/settings.rst\n@@ -40,8 +40,9 @@ precedence:\n  1. Command line options (most precedence)\n  2. Settings per-spider\n  3. Project settings module\n- 4. Default settings per-command\n- 5. Default global settings (less precedence)\n+ 4. Settings set by add-ons\n+ 5. Default settings per-command\n+ 6. Default global settings (less precedence)\n \n The population of these settings sources is taken care of internally, but a\n manual handling is possible using API calls. See the\n@@ -89,7 +90,13 @@ project, it's where most of your custom settings will be populated. For a\n standard Scrapy project, this means you'll be adding or changing the settings\n in the ``settings.py`` file created for your project.\n \n-4. Default settings per-command\n+4. Settings set by add-ons\n+--------------------------\n+\n+:ref:`Add-ons <topics-addons>` can modify settings. They should do this with\n+this priority, though this is not enforced.\n+\n+5. Default settings per-command\n -------------------------------\n \n Each :doc:`Scrapy tool </topics/commands>` command can have its own default\n@@ -97,7 +104,7 @@ settings, which override the global default settings. Those custom command\n settings are specified in the ``default_settings`` attribute of the command\n class.\n \n-5. Default global settings\n+6. Default global settings\n --------------------------\n \n The global defaults are located in the ``scrapy.settings.default_settings``\n@@ -201,6 +208,16 @@ to any particular component. In that case the module of that component will be\n shown, typically an extension, middleware or pipeline. It also means that the\n component must be enabled in order for the setting to have any effect.\n \n+.. setting:: ADDONS\n+\n+ADDONS\n+------\n+\n+Default: ``{}``\n+\n+A dict containing paths to the add-ons enabled in your project and their\n+priorities. For more information, see :ref:`topics-addons`.\n+\n .. setting:: AWS_ACCESS_KEY_ID\n \n AWS_ACCESS_KEY_ID\n@@ -964,7 +981,6 @@ some of them need to be enabled through a setting.\n For more information See the :ref:`extensions user guide  <topics-extensions>`\n and the :ref:`list of available extensions <topics-extensions-ref>`.\n \n-\n .. setting:: FEED_TEMPDIR\n \n FEED_TEMPDIR\ndiff --git a/scrapy/addons.py b/scrapy/addons.py\nnew file mode 100644\nindex 00000000000..02dd4fde85b\n--- /dev/null\n+++ b/scrapy/addons.py\n@@ -0,0 +1,54 @@\n+import logging\n+from typing import TYPE_CHECKING, Any, List\n+\n+from scrapy.exceptions import NotConfigured\n+from scrapy.settings import Settings\n+from scrapy.utils.conf import build_component_list\n+from scrapy.utils.misc import create_instance, load_object\n+\n+if TYPE_CHECKING:\n+    from scrapy.crawler import Crawler\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class AddonManager:\n+    \"\"\"This class facilitates loading and storing :ref:`topics-addons`.\"\"\"\n+\n+    def __init__(self, crawler: \"Crawler\") -> None:\n+        self.crawler: \"Crawler\" = crawler\n+        self.addons: List[Any] = []\n+\n+    def load_settings(self, settings: Settings) -> None:\n+        \"\"\"Load add-ons and configurations from a settings object.\n+\n+        This will load the add-on for every add-on path in the\n+        ``ADDONS`` setting and execute their ``update_settings`` methods.\n+\n+        :param settings: The :class:`~scrapy.settings.Settings` object from \\\n+            which to read the add-on configuration\n+        :type settings: :class:`~scrapy.settings.Settings`\n+        \"\"\"\n+        enabled: List[Any] = []\n+        for clspath in build_component_list(settings[\"ADDONS\"]):\n+            try:\n+                addoncls = load_object(clspath)\n+                addon = create_instance(\n+                    addoncls, settings=settings, crawler=self.crawler\n+                )\n+                addon.update_settings(settings)\n+                self.addons.append(addon)\n+            except NotConfigured as e:\n+                if e.args:\n+                    logger.warning(\n+                        \"Disabled %(clspath)s: %(eargs)s\",\n+                        {\"clspath\": clspath, \"eargs\": e.args[0]},\n+                        extra={\"crawler\": self.crawler},\n+                    )\n+        logger.info(\n+            \"Enabled addons:\\n%(addons)s\",\n+            {\n+                \"addons\": enabled,\n+            },\n+            extra={\"crawler\": self.crawler},\n+        )\ndiff --git a/scrapy/crawler.py b/scrapy/crawler.py\nindex 69ff07bb719..bf69cee2626 100644\n--- a/scrapy/crawler.py\n+++ b/scrapy/crawler.py\n@@ -18,6 +18,7 @@\n from zope.interface.verify import verifyClass\n \n from scrapy import Spider, signals\n+from scrapy.addons import AddonManager\n from scrapy.core.engine import ExecutionEngine\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.extension import ExtensionManager\n@@ -68,6 +69,9 @@ def __init__(\n         self.settings: Settings = settings.copy()\n         self.spidercls.update_settings(self.settings)\n \n+        self.addons: AddonManager = AddonManager(self)\n+        self.addons.load_settings(self.settings)\n+\n         self.signals: SignalManager = SignalManager(self)\n \n         self.stats: StatsCollector = load_object(self.settings[\"STATS_CLASS\"])(self)\ndiff --git a/scrapy/middleware.py b/scrapy/middleware.py\nindex 03e92b56506..04b838d2d11 100644\n--- a/scrapy/middleware.py\n+++ b/scrapy/middleware.py\n@@ -46,10 +46,9 @@ def from_settings(cls, settings: Settings, crawler=None):\n                 enabled.append(clspath)\n             except NotConfigured as e:\n                 if e.args:\n-                    clsname = clspath.split(\".\")[-1]\n                     logger.warning(\n-                        \"Disabled %(clsname)s: %(eargs)s\",\n-                        {\"clsname\": clsname, \"eargs\": e.args[0]},\n+                        \"Disabled %(clspath)s: %(eargs)s\",\n+                        {\"clspath\": clspath, \"eargs\": e.args[0]},\n                         extra={\"crawler\": crawler},\n                     )\n \ndiff --git a/scrapy/settings/__init__.py b/scrapy/settings/__init__.py\nindex 8b3bdbabe27..0f5cf85acc0 100644\n--- a/scrapy/settings/__init__.py\n+++ b/scrapy/settings/__init__.py\n@@ -9,6 +9,7 @@\n SETTINGS_PRIORITIES = {\n     \"default\": 0,\n     \"command\": 10,\n+    \"addon\": 15,\n     \"project\": 20,\n     \"spider\": 30,\n     \"cmdline\": 40,\ndiff --git a/scrapy/settings/default_settings.py b/scrapy/settings/default_settings.py\nindex a4cb555bd9d..ef1b7ea99b4 100644\n--- a/scrapy/settings/default_settings.py\n+++ b/scrapy/settings/default_settings.py\n@@ -17,6 +17,8 @@\n from importlib import import_module\n from pathlib import Path\n \n+ADDONS = {}\n+\n AJAXCRAWL_ENABLED = False\n \n ASYNCIO_EVENT_LOOP = None\ndiff --git a/scrapy/utils/misc.py b/scrapy/utils/misc.py\nindex 70187ba748a..b3c28da9239 100644\n--- a/scrapy/utils/misc.py\n+++ b/scrapy/utils/misc.py\n@@ -67,7 +67,7 @@ def load_object(path: Union[str, Callable]) -> Any:\n         if callable(path):\n             return path\n         raise TypeError(\n-            \"Unexpected argument type, expected string \" f\"or object, got: {type(path)}\"\n+            f\"Unexpected argument type, expected string or object, got: {type(path)}\"\n         )\n \n     try:\ndiff --git a/sep/sep-021.rst b/sep/sep-021.rst\ndeleted file mode 100644\nindex e8affa94332..00000000000\n--- a/sep/sep-021.rst\n+++ /dev/null\n@@ -1,113 +0,0 @@\n-=======  ===================\n-SEP      21\n-Title    Add-ons\n-Author   Pablo Hoffman\n-Created  2014-02-14\n-Status   Draft\n-=======  ===================\n-\n-================\n-SEP-021: Add-ons\n-================\n-\n-This proposal introduces add-ons, a unified way to manage Scrapy extensions,\n-middlewares and pipelines.\n-\n-Scrapy currently supports many hooks and mechanisms for extending its\n-functionality, but no single entry point for enabling and configuring them.\n-Instead, the hooks are spread over:\n-\n-* Spider middlewares (SPIDER_MIDDLEWARES)\n-* Downloader middlewares (DOWNLOADER_MIDDLEWARES)\n-* Downloader handlers (DOWNLOADER_HANDLERS)\n-* Item pipelines (ITEM_PIPELINES)\n-* Feed exporters and storages (FEED_EXPORTERS, FEED_STORAGES)\n-* Overridable components (DUPEFILTER_CLASS, STATS_CLASS, SCHEDULER, SPIDER_MANAGER_CLASS, ITEM_PROCESSOR, etc)\n-* Generic extensions (EXTENSIONS)\n-* CLI commands (COMMANDS_MODULE)\n-\n-One problem of this approach is that enabling an extension often requires\n-modifying many settings, often in a coordinated way, which is complex and error\n-prone. Add-ons are meant to fix this by providing a simple mechanism for\n-enabling extensions.\n-\n-Design goals and non-goals\n-==========================\n-\n-Goals:\n-\n-* simple to manage: adding or removing extensions should be just a matter of\n-  adding or removing lines in a ``scrapy.cfg`` file\n-* backward compatibility with enabling extension the \"old way\" (i.e. modifying\n-  settings directly)\n-\n-Non-goals:\n-\n-* a way to publish, distribute or discover extensions (use pypi for that)\n-\n-\n-Managing add-ons\n-================\n-\n-Add-ons are defined in the ``scrapy.cfg`` file, inside the ``[addons]``\n-section.\n-\n-To enable the \"httpcache\" addon, either shipped with Scrapy or in the Python\n-search path, create an entry for it in your ``scrapy.cfg``, like this::\n-\n-    [addons]\n-    httpcache = \n-\n-You may also specify the full path to an add-on (which may be either a .py file\n-or a folder containing __init__.py)::\n-\n-    [addons]\n-    mongodb_pipeline = /path/to/mongodb_pipeline.py\n-\n-\n-Writing add-ons\n-===============\n-\n-Add-ons are Python modules that implement the following callbacks.\n-\n-addon_configure\n----------------\n-\n-Receives the Settings object and modifies it to enable the required components.\n-If it raises an exception, Scrapy will print it and exit.\n-\n-Examples:\n-\n-.. code-block:: python\n-\n-    def addon_configure(settings):\n-        settings.overrides[\"DOWNLOADER_MIDDLEWARES\"].update(\n-            {\n-                \"scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware\": 900,\n-            }\n-        )\n-\n-.. code-block:: python\n-\n-    def addon_configure(settings):\n-        try:\n-            import boto\n-        except ImportError:\n-            raise RuntimeError(\"boto library is required\")\n-\n-\n-crawler_ready\n--------------\n-\n-``crawler_ready`` receives a Crawler object after it has been initialized and\n-is meant to be used to perform post-initialization checks like making sure the\n-extension and its dependencies were configured properly. If it raises an\n-exception, Scrapy will print and exit.\n-\n-Examples:\n-\n-.. code-block:: python\n-\n-    def crawler_ready(crawler):\n-        if \"some.other.addon\" not in crawler.extensions.enabled:\n-            raise RuntimeError(\"Some other addon is required to use this addon\")\n", "test_patch": "diff --git a/tests/test_addons.py b/tests/test_addons.py\nnew file mode 100644\nindex 00000000000..5d053ed52d9\n--- /dev/null\n+++ b/tests/test_addons.py\n@@ -0,0 +1,158 @@\n+import itertools\n+import unittest\n+from typing import Any, Dict\n+\n+from scrapy import Spider\n+from scrapy.crawler import Crawler, CrawlerRunner\n+from scrapy.exceptions import NotConfigured\n+from scrapy.settings import BaseSettings, Settings\n+from scrapy.utils.test import get_crawler\n+\n+\n+class SimpleAddon:\n+    def update_settings(self, settings):\n+        pass\n+\n+\n+def get_addon_cls(config: Dict[str, Any]) -> type:\n+    class AddonWithConfig:\n+        def update_settings(self, settings: BaseSettings):\n+            settings.update(config, priority=\"addon\")\n+\n+    return AddonWithConfig\n+\n+\n+class CreateInstanceAddon:\n+    def __init__(self, crawler: Crawler) -> None:\n+        super().__init__()\n+        self.crawler = crawler\n+        self.config = crawler.settings.getdict(\"MYADDON\")\n+\n+    @classmethod\n+    def from_crawler(cls, crawler: Crawler):\n+        return cls(crawler)\n+\n+    def update_settings(self, settings):\n+        settings.update(self.config, \"addon\")\n+\n+\n+class AddonTest(unittest.TestCase):\n+    def test_update_settings(self):\n+        settings = BaseSettings()\n+        settings.set(\"KEY1\", \"default\", priority=\"default\")\n+        settings.set(\"KEY2\", \"project\", priority=\"project\")\n+        addon_config = {\"KEY1\": \"addon\", \"KEY2\": \"addon\", \"KEY3\": \"addon\"}\n+        testaddon = get_addon_cls(addon_config)()\n+        testaddon.update_settings(settings)\n+        self.assertEqual(settings[\"KEY1\"], \"addon\")\n+        self.assertEqual(settings[\"KEY2\"], \"project\")\n+        self.assertEqual(settings[\"KEY3\"], \"addon\")\n+\n+\n+class AddonManagerTest(unittest.TestCase):\n+    def test_load_settings(self):\n+        settings_dict = {\n+            \"ADDONS\": {\"tests.test_addons.SimpleAddon\": 0},\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        manager = crawler.addons\n+        self.assertIsInstance(manager.addons[0], SimpleAddon)\n+\n+    def test_notconfigured(self):\n+        class NotConfiguredAddon:\n+            def update_settings(self, settings):\n+                raise NotConfigured()\n+\n+        settings_dict = {\n+            \"ADDONS\": {NotConfiguredAddon: 0},\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        manager = crawler.addons\n+        self.assertFalse(manager.addons)\n+\n+    def test_load_settings_order(self):\n+        # Get three addons with different settings\n+        addonlist = []\n+        for i in range(3):\n+            addon = get_addon_cls({\"KEY1\": i})\n+            addon.number = i\n+            addonlist.append(addon)\n+        # Test for every possible ordering\n+        for ordered_addons in itertools.permutations(addonlist):\n+            expected_order = [a.number for a in ordered_addons]\n+            settings = {\"ADDONS\": {a: i for i, a in enumerate(ordered_addons)}}\n+            crawler = get_crawler(settings_dict=settings)\n+            manager = crawler.addons\n+            self.assertEqual([a.number for a in manager.addons], expected_order)\n+            self.assertEqual(crawler.settings.getint(\"KEY1\"), expected_order[-1])\n+\n+    def test_create_instance(self):\n+        settings_dict = {\n+            \"ADDONS\": {\"tests.test_addons.CreateInstanceAddon\": 0},\n+            \"MYADDON\": {\"MYADDON_KEY\": \"val\"},\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        manager = crawler.addons\n+        self.assertIsInstance(manager.addons[0], CreateInstanceAddon)\n+        self.assertEqual(crawler.settings.get(\"MYADDON_KEY\"), \"val\")\n+\n+    def test_settings_priority(self):\n+        config = {\n+            \"KEY\": 15,  # priority=addon\n+        }\n+        settings_dict = {\n+            \"ADDONS\": {get_addon_cls(config): 1},\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        self.assertEqual(crawler.settings.getint(\"KEY\"), 15)\n+\n+        settings = Settings(settings_dict)\n+        settings.set(\"KEY\", 0, priority=\"default\")\n+        runner = CrawlerRunner(settings)\n+        crawler = runner.create_crawler(Spider)\n+        self.assertEqual(crawler.settings.getint(\"KEY\"), 15)\n+\n+        settings_dict = {\n+            \"KEY\": 20,  # priority=project\n+            \"ADDONS\": {get_addon_cls(config): 1},\n+        }\n+        settings = Settings(settings_dict)\n+        settings.set(\"KEY\", 0, priority=\"default\")\n+        runner = CrawlerRunner(settings)\n+        crawler = runner.create_crawler(Spider)\n+        self.assertEqual(crawler.settings.getint(\"KEY\"), 20)\n+\n+    def test_fallback_workflow(self):\n+        FALLBACK_SETTING = \"MY_FALLBACK_DOWNLOAD_HANDLER\"\n+\n+        class AddonWithFallback:\n+            def update_settings(self, settings):\n+                if not settings.get(FALLBACK_SETTING):\n+                    settings.set(\n+                        FALLBACK_SETTING,\n+                        settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"],\n+                        \"addon\",\n+                    )\n+                settings[\"DOWNLOAD_HANDLERS\"][\"https\"] = \"AddonHandler\"\n+\n+        settings_dict = {\n+            \"ADDONS\": {AddonWithFallback: 1},\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        self.assertEqual(\n+            crawler.settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"], \"AddonHandler\"\n+        )\n+        self.assertEqual(\n+            crawler.settings.get(FALLBACK_SETTING),\n+            \"scrapy.core.downloader.handlers.http.HTTPDownloadHandler\",\n+        )\n+\n+        settings_dict = {\n+            \"ADDONS\": {AddonWithFallback: 1},\n+            \"DOWNLOAD_HANDLERS\": {\"https\": \"UserHandler\"},\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        self.assertEqual(\n+            crawler.settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"], \"AddonHandler\"\n+        )\n+        self.assertEqual(crawler.settings.get(FALLBACK_SETTING), \"UserHandler\")\ndiff --git a/tests/test_middleware.py b/tests/test_middleware.py\nindex 00ff746ee5a..a42c7b3d1e2 100644\n--- a/tests/test_middleware.py\n+++ b/tests/test_middleware.py\n@@ -39,7 +39,7 @@ def close_spider(self, spider):\n         pass\n \n     def __init__(self):\n-        raise NotConfigured\n+        raise NotConfigured(\"foo\")\n \n \n class TestMiddlewareManager(MiddlewareManager):\ndiff --git a/tests/test_utils_deprecate.py b/tests/test_utils_deprecate.py\nindex 2d9210410d4..eedb6f6af9c 100644\n--- a/tests/test_utils_deprecate.py\n+++ b/tests/test_utils_deprecate.py\n@@ -296,3 +296,7 @@ def test_unmatched_path_stays_the_same(self):\n             output = update_classpath(\"scrapy.unmatched.Path\")\n         self.assertEqual(output, \"scrapy.unmatched.Path\")\n         self.assertEqual(len(w), 0)\n+\n+    def test_returns_nonstring(self):\n+        for notastring in [None, True, [1, 2, 3], object()]:\n+            self.assertEqual(update_classpath(notastring), notastring)\n", "problem_statement": "NotConfigured logging breaks when the component is added by class object\nAs the log message for components that raise `NotConfigured` with a message assumes `clsname` is an import path string, it raises an AttributeError when it's a class instance. https://github.com/scrapy/scrapy/blob/bddbbc522aef00dc150e479e6288041cee2e95c9/scrapy/middleware.py#L49\n", "hints_text": "", "created_at": "2023-06-14T14:17:09Z"}
>>>>>>> e3de0db9b9f2245fc8e62a519e819b2b55a383f0
