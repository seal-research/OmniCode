{"repo": "scrapy/scrapy", "pull_number": 5950, "instance_id": "scrapy__scrapy-5950", "issue_numbers": ["5992"], "base_commit": "510574216d70ec84d75639ebcda360834a992e47", "patch": "diff --git a/docs/index.rst b/docs/index.rst\nindex 5404969e02e..8798aebd132 100644\n--- a/docs/index.rst\n+++ b/docs/index.rst\n@@ -222,6 +222,7 @@ Extending Scrapy\n    :hidden:\n \n    topics/architecture\n+   topics/addons\n    topics/downloader-middleware\n    topics/spider-middleware\n    topics/extensions\n@@ -235,6 +236,9 @@ Extending Scrapy\n :doc:`topics/architecture`\n     Understand the Scrapy architecture.\n \n+:doc:`topics/addons`\n+    Enable and configure third-party extensions.\n+\n :doc:`topics/downloader-middleware`\n     Customize how pages get requested and downloaded.\n \ndiff --git a/docs/topics/addons.rst b/docs/topics/addons.rst\nnew file mode 100644\nindex 00000000000..1bf2172bd40\n--- /dev/null\n+++ b/docs/topics/addons.rst\n@@ -0,0 +1,193 @@\n+.. _topics-addons:\n+\n+=======\n+Add-ons\n+=======\n+\n+Scrapy's add-on system is a framework which unifies managing and configuring\n+components that extend Scrapy's core functionality, such as middlewares,\n+extensions, or pipelines. It provides users with a plug-and-play experience in\n+Scrapy extension management, and grants extensive configuration control to\n+developers.\n+\n+\n+Activating and configuring add-ons\n+==================================\n+\n+During :class:`~scrapy.crawler.Crawler` initialization, the list of enabled\n+add-ons is read from your ``ADDONS`` setting.\n+\n+The ``ADDONS`` setting is a dict in which every key is an add-on class or its\n+import path and the value is its priority.\n+\n+This is an example where two add-ons are enabled in a project's\n+``settings.py``::\n+\n+    ADDONS = {\n+        'path.to.someaddon': 0,\n+        SomeAddonClass: 1,\n+    }\n+\n+\n+Writing your own add-ons\n+========================\n+\n+Add-ons are Python classes that include the following method:\n+\n+.. method:: update_settings(settings)\n+\n+    This method is called during the initialization of the\n+    :class:`~scrapy.crawler.Crawler`. Here, you should perform dependency checks\n+    (e.g. for external Python libraries) and update the\n+    :class:`~scrapy.settings.Settings` object as wished, e.g. enable components\n+    for this add-on or set required configuration of other extensions.\n+\n+    :param settings: The settings object storing Scrapy/component configuration\n+    :type settings: :class:`~scrapy.settings.Settings`\n+\n+They can also have the following method:\n+\n+.. classmethod:: from_crawler(cls, crawler)\n+   :noindex:\n+\n+   If present, this class method is called to create an add-on instance\n+   from a :class:`~scrapy.crawler.Crawler`. It must return a new instance\n+   of the add-on. The crawler object provides access to all Scrapy core\n+   components like settings and signals; it is a way for the add-on to access\n+   them and hook its functionality into Scrapy.\n+\n+   :param crawler: The crawler that uses this add-on\n+   :type crawler: :class:`~scrapy.crawler.Crawler`\n+\n+The settings set by the add-on should use the ``addon`` priority (see\n+:ref:`populating-settings` and :func:`scrapy.settings.BaseSettings.set`)::\n+\n+    class MyAddon:\n+        def update_settings(self, settings):\n+            settings.set(\"DNSCACHE_ENABLED\", True, \"addon\")\n+\n+This allows users to override these settings in the project or spider\n+configuration. This is not possible with settings that are mutable objects,\n+such as the dict that is a value of :setting:`ITEM_PIPELINES`. In these cases\n+you can provide an add-on-specific setting that governs whether the add-on will\n+modify :setting:`ITEM_PIPELINES`::\n+\n+    class MyAddon:\n+        def update_settings(self, settings):\n+            if settings.getbool(\"MYADDON_ENABLE_PIPELINE\"):\n+                settings[\"ITEM_PIPELINES\"][\"path.to.mypipeline\"] = 200\n+\n+If the ``update_settings`` method raises\n+:exc:`scrapy.exceptions.NotConfigured`, the add-on will be skipped. This makes\n+it easy to enable an add-on only when some conditions are met.\n+\n+Fallbacks\n+---------\n+\n+Some components provided by add-ons need to fall back to \"default\"\n+implementations, e.g. a custom download handler needs to send the request that\n+it doesn't handle via the default download handler, or a stats collector that\n+includes some additional processing but otherwise uses the default stats\n+collector. And it's possible that a project needs to use several custom\n+components of the same type, e.g. two custom download handlers that support\n+different kinds of custom requests and still need to use the default download\n+handler for other requests. To make such use cases easier to configure, we\n+recommend that such custom components should be written in the following way:\n+\n+1. The custom component (e.g. ``MyDownloadHandler``) shouldn't inherit from the\n+   default Scrapy one (e.g.\n+   ``scrapy.core.downloader.handlers.http.HTTPDownloadHandler``), but instead\n+   be able to load the class of the fallback component from a special setting\n+   (e.g. ``MY_FALLBACK_DOWNLOAD_HANDLER``), create an instance of it and use\n+   it.\n+2. The add-ons that include these components should read the current value of\n+   the default setting (e.g. ``DOWNLOAD_HANDLERS``) in their\n+   ``update_settings()`` methods, save that value into the fallback setting\n+   (``MY_FALLBACK_DOWNLOAD_HANDLER`` mentioned earlier) and set the default\n+   setting to the component provided by the add-on (e.g.\n+   ``MyDownloadHandler``). If the fallback setting is already set by the user,\n+   they shouldn't change it.\n+3. This way, if there are several add-ons that want to modify the same setting,\n+   all of them will fallback to the component from the previous one and then to\n+   the Scrapy default. The order of that depends on the priority order in the\n+   ``ADDONS`` setting.\n+\n+\n+Add-on examples\n+===============\n+\n+Set some basic configuration:\n+\n+.. code-block:: python\n+\n+    class MyAddon:\n+        def update_settings(self, settings):\n+            settings[\"ITEM_PIPELINES\"][\"path.to.mypipeline\"] = 200\n+            settings.set(\"DNSCACHE_ENABLED\", True, \"addon\")\n+\n+Check dependencies:\n+\n+.. code-block:: python\n+\n+    class MyAddon:\n+        def update_settings(self, settings):\n+            try:\n+                import boto\n+            except ImportError:\n+                raise NotConfigured(\"MyAddon requires the boto library\")\n+            ...\n+\n+Access the crawler instance:\n+\n+.. code-block:: python\n+\n+    class MyAddon:\n+        def __init__(self, crawler) -> None:\n+            super().__init__()\n+            self.crawler = crawler\n+\n+        @classmethod\n+        def from_crawler(cls, crawler):\n+            return cls(crawler)\n+\n+        def update_settings(self, settings):\n+            ...\n+\n+Use a fallback component:\n+\n+.. code-block:: python\n+\n+    from scrapy.core.downloader.handlers.http import HTTPDownloadHandler\n+\n+\n+    FALLBACK_SETTING = \"MY_FALLBACK_DOWNLOAD_HANDLER\"\n+\n+\n+    class MyHandler:\n+        lazy = False\n+\n+        def __init__(self, settings, crawler):\n+            dhcls = load_object(settings.get(FALLBACK_SETTING))\n+            self._fallback_handler = create_instance(\n+                dhcls,\n+                settings=None,\n+                crawler=crawler,\n+            )\n+\n+        def download_request(self, request, spider):\n+            if request.meta.get(\"my_params\"):\n+                # handle the request\n+                ...\n+            else:\n+                return self._fallback_handler.download_request(request, spider)\n+\n+\n+    class MyAddon:\n+        def update_settings(self, settings):\n+            if not settings.get(FALLBACK_SETTING):\n+                settings.set(\n+                    FALLBACK_SETTING,\n+                    settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"],\n+                    \"addon\",\n+                )\n+            settings[\"DOWNLOAD_HANDLERS\"][\"https\"] = MyHandler\ndiff --git a/docs/topics/api.rst b/docs/topics/api.rst\nindex 26834487998..16c28405cfb 100644\n--- a/docs/topics/api.rst\n+++ b/docs/topics/api.rst\n@@ -137,6 +137,7 @@ Settings API\n         SETTINGS_PRIORITIES = {\n             \"default\": 0,\n             \"command\": 10,\n+            \"addon\": 15,\n             \"project\": 20,\n             \"spider\": 30,\n             \"cmdline\": 40,\ndiff --git a/docs/topics/settings.rst b/docs/topics/settings.rst\nindex 3e06d84f90b..602ab587d7e 100644\n--- a/docs/topics/settings.rst\n+++ b/docs/topics/settings.rst\n@@ -40,8 +40,9 @@ precedence:\n  1. Command line options (most precedence)\n  2. Settings per-spider\n  3. Project settings module\n- 4. Default settings per-command\n- 5. Default global settings (less precedence)\n+ 4. Settings set by add-ons\n+ 5. Default settings per-command\n+ 6. Default global settings (less precedence)\n \n The population of these settings sources is taken care of internally, but a\n manual handling is possible using API calls. See the\n@@ -89,7 +90,13 @@ project, it's where most of your custom settings will be populated. For a\n standard Scrapy project, this means you'll be adding or changing the settings\n in the ``settings.py`` file created for your project.\n \n-4. Default settings per-command\n+4. Settings set by add-ons\n+--------------------------\n+\n+:ref:`Add-ons <topics-addons>` can modify settings. They should do this with\n+this priority, though this is not enforced.\n+\n+5. Default settings per-command\n -------------------------------\n \n Each :doc:`Scrapy tool </topics/commands>` command can have its own default\n@@ -97,7 +104,7 @@ settings, which override the global default settings. Those custom command\n settings are specified in the ``default_settings`` attribute of the command\n class.\n \n-5. Default global settings\n+6. Default global settings\n --------------------------\n \n The global defaults are located in the ``scrapy.settings.default_settings``\n@@ -201,6 +208,16 @@ to any particular component. In that case the module of that component will be\n shown, typically an extension, middleware or pipeline. It also means that the\n component must be enabled in order for the setting to have any effect.\n \n+.. setting:: ADDONS\n+\n+ADDONS\n+------\n+\n+Default: ``{}``\n+\n+A dict containing paths to the add-ons enabled in your project and their\n+priorities. For more information, see :ref:`topics-addons`.\n+\n .. setting:: AWS_ACCESS_KEY_ID\n \n AWS_ACCESS_KEY_ID\n@@ -964,7 +981,6 @@ some of them need to be enabled through a setting.\n For more information See the :ref:`extensions user guide  <topics-extensions>`\n and the :ref:`list of available extensions <topics-extensions-ref>`.\n \n-\n .. setting:: FEED_TEMPDIR\n \n FEED_TEMPDIR\ndiff --git a/scrapy/addons.py b/scrapy/addons.py\nnew file mode 100644\nindex 00000000000..02dd4fde85b\n--- /dev/null\n+++ b/scrapy/addons.py\n@@ -0,0 +1,54 @@\n+import logging\n+from typing import TYPE_CHECKING, Any, List\n+\n+from scrapy.exceptions import NotConfigured\n+from scrapy.settings import Settings\n+from scrapy.utils.conf import build_component_list\n+from scrapy.utils.misc import create_instance, load_object\n+\n+if TYPE_CHECKING:\n+    from scrapy.crawler import Crawler\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class AddonManager:\n+    \"\"\"This class facilitates loading and storing :ref:`topics-addons`.\"\"\"\n+\n+    def __init__(self, crawler: \"Crawler\") -> None:\n+        self.crawler: \"Crawler\" = crawler\n+        self.addons: List[Any] = []\n+\n+    def load_settings(self, settings: Settings) -> None:\n+        \"\"\"Load add-ons and configurations from a settings object.\n+\n+        This will load the add-on for every add-on path in the\n+        ``ADDONS`` setting and execute their ``update_settings`` methods.\n+\n+        :param settings: The :class:`~scrapy.settings.Settings` object from \\\n+            which to read the add-on configuration\n+        :type settings: :class:`~scrapy.settings.Settings`\n+        \"\"\"\n+        enabled: List[Any] = []\n+        for clspath in build_component_list(settings[\"ADDONS\"]):\n+            try:\n+                addoncls = load_object(clspath)\n+                addon = create_instance(\n+                    addoncls, settings=settings, crawler=self.crawler\n+                )\n+                addon.update_settings(settings)\n+                self.addons.append(addon)\n+            except NotConfigured as e:\n+                if e.args:\n+                    logger.warning(\n+                        \"Disabled %(clspath)s: %(eargs)s\",\n+                        {\"clspath\": clspath, \"eargs\": e.args[0]},\n+                        extra={\"crawler\": self.crawler},\n+                    )\n+        logger.info(\n+            \"Enabled addons:\\n%(addons)s\",\n+            {\n+                \"addons\": enabled,\n+            },\n+            extra={\"crawler\": self.crawler},\n+        )\ndiff --git a/scrapy/crawler.py b/scrapy/crawler.py\nindex 69ff07bb719..bf69cee2626 100644\n--- a/scrapy/crawler.py\n+++ b/scrapy/crawler.py\n@@ -18,6 +18,7 @@\n from zope.interface.verify import verifyClass\n \n from scrapy import Spider, signals\n+from scrapy.addons import AddonManager\n from scrapy.core.engine import ExecutionEngine\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.extension import ExtensionManager\n@@ -68,6 +69,9 @@ def __init__(\n         self.settings: Settings = settings.copy()\n         self.spidercls.update_settings(self.settings)\n \n+        self.addons: AddonManager = AddonManager(self)\n+        self.addons.load_settings(self.settings)\n+\n         self.signals: SignalManager = SignalManager(self)\n \n         self.stats: StatsCollector = load_object(self.settings[\"STATS_CLASS\"])(self)\ndiff --git a/scrapy/middleware.py b/scrapy/middleware.py\nindex 03e92b56506..04b838d2d11 100644\n--- a/scrapy/middleware.py\n+++ b/scrapy/middleware.py\n@@ -46,10 +46,9 @@ def from_settings(cls, settings: Settings, crawler=None):\n                 enabled.append(clspath)\n             except NotConfigured as e:\n                 if e.args:\n-                    clsname = clspath.split(\".\")[-1]\n                     logger.warning(\n-                        \"Disabled %(clsname)s: %(eargs)s\",\n-                        {\"clsname\": clsname, \"eargs\": e.args[0]},\n+                        \"Disabled %(clspath)s: %(eargs)s\",\n+                        {\"clspath\": clspath, \"eargs\": e.args[0]},\n                         extra={\"crawler\": crawler},\n                     )\n \ndiff --git a/scrapy/settings/__init__.py b/scrapy/settings/__init__.py\nindex 8b3bdbabe27..0f5cf85acc0 100644\n--- a/scrapy/settings/__init__.py\n+++ b/scrapy/settings/__init__.py\n@@ -9,6 +9,7 @@\n SETTINGS_PRIORITIES = {\n     \"default\": 0,\n     \"command\": 10,\n+    \"addon\": 15,\n     \"project\": 20,\n     \"spider\": 30,\n     \"cmdline\": 40,\ndiff --git a/scrapy/settings/default_settings.py b/scrapy/settings/default_settings.py\nindex a4cb555bd9d..ef1b7ea99b4 100644\n--- a/scrapy/settings/default_settings.py\n+++ b/scrapy/settings/default_settings.py\n@@ -17,6 +17,8 @@\n from importlib import import_module\n from pathlib import Path\n \n+ADDONS = {}\n+\n AJAXCRAWL_ENABLED = False\n \n ASYNCIO_EVENT_LOOP = None\ndiff --git a/scrapy/utils/misc.py b/scrapy/utils/misc.py\nindex 70187ba748a..b3c28da9239 100644\n--- a/scrapy/utils/misc.py\n+++ b/scrapy/utils/misc.py\n@@ -67,7 +67,7 @@ def load_object(path: Union[str, Callable]) -> Any:\n         if callable(path):\n             return path\n         raise TypeError(\n-            \"Unexpected argument type, expected string \" f\"or object, got: {type(path)}\"\n+            f\"Unexpected argument type, expected string or object, got: {type(path)}\"\n         )\n \n     try:\ndiff --git a/sep/sep-021.rst b/sep/sep-021.rst\ndeleted file mode 100644\nindex e8affa94332..00000000000\n--- a/sep/sep-021.rst\n+++ /dev/null\n@@ -1,113 +0,0 @@\n-=======  ===================\n-SEP      21\n-Title    Add-ons\n-Author   Pablo Hoffman\n-Created  2014-02-14\n-Status   Draft\n-=======  ===================\n-\n-================\n-SEP-021: Add-ons\n-================\n-\n-This proposal introduces add-ons, a unified way to manage Scrapy extensions,\n-middlewares and pipelines.\n-\n-Scrapy currently supports many hooks and mechanisms for extending its\n-functionality, but no single entry point for enabling and configuring them.\n-Instead, the hooks are spread over:\n-\n-* Spider middlewares (SPIDER_MIDDLEWARES)\n-* Downloader middlewares (DOWNLOADER_MIDDLEWARES)\n-* Downloader handlers (DOWNLOADER_HANDLERS)\n-* Item pipelines (ITEM_PIPELINES)\n-* Feed exporters and storages (FEED_EXPORTERS, FEED_STORAGES)\n-* Overridable components (DUPEFILTER_CLASS, STATS_CLASS, SCHEDULER, SPIDER_MANAGER_CLASS, ITEM_PROCESSOR, etc)\n-* Generic extensions (EXTENSIONS)\n-* CLI commands (COMMANDS_MODULE)\n-\n-One problem of this approach is that enabling an extension often requires\n-modifying many settings, often in a coordinated way, which is complex and error\n-prone. Add-ons are meant to fix this by providing a simple mechanism for\n-enabling extensions.\n-\n-Design goals and non-goals\n-==========================\n-\n-Goals:\n-\n-* simple to manage: adding or removing extensions should be just a matter of\n-  adding or removing lines in a ``scrapy.cfg`` file\n-* backward compatibility with enabling extension the \"old way\" (i.e. modifying\n-  settings directly)\n-\n-Non-goals:\n-\n-* a way to publish, distribute or discover extensions (use pypi for that)\n-\n-\n-Managing add-ons\n-================\n-\n-Add-ons are defined in the ``scrapy.cfg`` file, inside the ``[addons]``\n-section.\n-\n-To enable the \"httpcache\" addon, either shipped with Scrapy or in the Python\n-search path, create an entry for it in your ``scrapy.cfg``, like this::\n-\n-    [addons]\n-    httpcache = \n-\n-You may also specify the full path to an add-on (which may be either a .py file\n-or a folder containing __init__.py)::\n-\n-    [addons]\n-    mongodb_pipeline = /path/to/mongodb_pipeline.py\n-\n-\n-Writing add-ons\n-===============\n-\n-Add-ons are Python modules that implement the following callbacks.\n-\n-addon_configure\n----------------\n-\n-Receives the Settings object and modifies it to enable the required components.\n-If it raises an exception, Scrapy will print it and exit.\n-\n-Examples:\n-\n-.. code-block:: python\n-\n-    def addon_configure(settings):\n-        settings.overrides[\"DOWNLOADER_MIDDLEWARES\"].update(\n-            {\n-                \"scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware\": 900,\n-            }\n-        )\n-\n-.. code-block:: python\n-\n-    def addon_configure(settings):\n-        try:\n-            import boto\n-        except ImportError:\n-            raise RuntimeError(\"boto library is required\")\n-\n-\n-crawler_ready\n--------------\n-\n-``crawler_ready`` receives a Crawler object after it has been initialized and\n-is meant to be used to perform post-initialization checks like making sure the\n-extension and its dependencies were configured properly. If it raises an\n-exception, Scrapy will print and exit.\n-\n-Examples:\n-\n-.. code-block:: python\n-\n-    def crawler_ready(crawler):\n-        if \"some.other.addon\" not in crawler.extensions.enabled:\n-            raise RuntimeError(\"Some other addon is required to use this addon\")\n", "test_patch": "diff --git a/tests/test_addons.py b/tests/test_addons.py\nnew file mode 100644\nindex 00000000000..5d053ed52d9\n--- /dev/null\n+++ b/tests/test_addons.py\n@@ -0,0 +1,158 @@\n+import itertools\n+import unittest\n+from typing import Any, Dict\n+\n+from scrapy import Spider\n+from scrapy.crawler import Crawler, CrawlerRunner\n+from scrapy.exceptions import NotConfigured\n+from scrapy.settings import BaseSettings, Settings\n+from scrapy.utils.test import get_crawler\n+\n+\n+class SimpleAddon:\n+    def update_settings(self, settings):\n+        pass\n+\n+\n+def get_addon_cls(config: Dict[str, Any]) -> type:\n+    class AddonWithConfig:\n+        def update_settings(self, settings: BaseSettings):\n+            settings.update(config, priority=\"addon\")\n+\n+    return AddonWithConfig\n+\n+\n+class CreateInstanceAddon:\n+    def __init__(self, crawler: Crawler) -> None:\n+        super().__init__()\n+        self.crawler = crawler\n+        self.config = crawler.settings.getdict(\"MYADDON\")\n+\n+    @classmethod\n+    def from_crawler(cls, crawler: Crawler):\n+        return cls(crawler)\n+\n+    def update_settings(self, settings):\n+        settings.update(self.config, \"addon\")\n+\n+\n+class AddonTest(unittest.TestCase):\n+    def test_update_settings(self):\n+        settings = BaseSettings()\n+        settings.set(\"KEY1\", \"default\", priority=\"default\")\n+        settings.set(\"KEY2\", \"project\", priority=\"project\")\n+        addon_config = {\"KEY1\": \"addon\", \"KEY2\": \"addon\", \"KEY3\": \"addon\"}\n+        testaddon = get_addon_cls(addon_config)()\n+        testaddon.update_settings(settings)\n+        self.assertEqual(settings[\"KEY1\"], \"addon\")\n+        self.assertEqual(settings[\"KEY2\"], \"project\")\n+        self.assertEqual(settings[\"KEY3\"], \"addon\")\n+\n+\n+class AddonManagerTest(unittest.TestCase):\n+    def test_load_settings(self):\n+        settings_dict = {\n+            \"ADDONS\": {\"tests.test_addons.SimpleAddon\": 0},\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        manager = crawler.addons\n+        self.assertIsInstance(manager.addons[0], SimpleAddon)\n+\n+    def test_notconfigured(self):\n+        class NotConfiguredAddon:\n+            def update_settings(self, settings):\n+                raise NotConfigured()\n+\n+        settings_dict = {\n+            \"ADDONS\": {NotConfiguredAddon: 0},\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        manager = crawler.addons\n+        self.assertFalse(manager.addons)\n+\n+    def test_load_settings_order(self):\n+        # Get three addons with different settings\n+        addonlist = []\n+        for i in range(3):\n+            addon = get_addon_cls({\"KEY1\": i})\n+            addon.number = i\n+            addonlist.append(addon)\n+        # Test for every possible ordering\n+        for ordered_addons in itertools.permutations(addonlist):\n+            expected_order = [a.number for a in ordered_addons]\n+            settings = {\"ADDONS\": {a: i for i, a in enumerate(ordered_addons)}}\n+            crawler = get_crawler(settings_dict=settings)\n+            manager = crawler.addons\n+            self.assertEqual([a.number for a in manager.addons], expected_order)\n+            self.assertEqual(crawler.settings.getint(\"KEY1\"), expected_order[-1])\n+\n+    def test_create_instance(self):\n+        settings_dict = {\n+            \"ADDONS\": {\"tests.test_addons.CreateInstanceAddon\": 0},\n+            \"MYADDON\": {\"MYADDON_KEY\": \"val\"},\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        manager = crawler.addons\n+        self.assertIsInstance(manager.addons[0], CreateInstanceAddon)\n+        self.assertEqual(crawler.settings.get(\"MYADDON_KEY\"), \"val\")\n+\n+    def test_settings_priority(self):\n+        config = {\n+            \"KEY\": 15,  # priority=addon\n+        }\n+        settings_dict = {\n+            \"ADDONS\": {get_addon_cls(config): 1},\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        self.assertEqual(crawler.settings.getint(\"KEY\"), 15)\n+\n+        settings = Settings(settings_dict)\n+        settings.set(\"KEY\", 0, priority=\"default\")\n+        runner = CrawlerRunner(settings)\n+        crawler = runner.create_crawler(Spider)\n+        self.assertEqual(crawler.settings.getint(\"KEY\"), 15)\n+\n+        settings_dict = {\n+            \"KEY\": 20,  # priority=project\n+            \"ADDONS\": {get_addon_cls(config): 1},\n+        }\n+        settings = Settings(settings_dict)\n+        settings.set(\"KEY\", 0, priority=\"default\")\n+        runner = CrawlerRunner(settings)\n+        crawler = runner.create_crawler(Spider)\n+        self.assertEqual(crawler.settings.getint(\"KEY\"), 20)\n+\n+    def test_fallback_workflow(self):\n+        FALLBACK_SETTING = \"MY_FALLBACK_DOWNLOAD_HANDLER\"\n+\n+        class AddonWithFallback:\n+            def update_settings(self, settings):\n+                if not settings.get(FALLBACK_SETTING):\n+                    settings.set(\n+                        FALLBACK_SETTING,\n+                        settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"],\n+                        \"addon\",\n+                    )\n+                settings[\"DOWNLOAD_HANDLERS\"][\"https\"] = \"AddonHandler\"\n+\n+        settings_dict = {\n+            \"ADDONS\": {AddonWithFallback: 1},\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        self.assertEqual(\n+            crawler.settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"], \"AddonHandler\"\n+        )\n+        self.assertEqual(\n+            crawler.settings.get(FALLBACK_SETTING),\n+            \"scrapy.core.downloader.handlers.http.HTTPDownloadHandler\",\n+        )\n+\n+        settings_dict = {\n+            \"ADDONS\": {AddonWithFallback: 1},\n+            \"DOWNLOAD_HANDLERS\": {\"https\": \"UserHandler\"},\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        self.assertEqual(\n+            crawler.settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"], \"AddonHandler\"\n+        )\n+        self.assertEqual(crawler.settings.get(FALLBACK_SETTING), \"UserHandler\")\ndiff --git a/tests/test_middleware.py b/tests/test_middleware.py\nindex 00ff746ee5a..a42c7b3d1e2 100644\n--- a/tests/test_middleware.py\n+++ b/tests/test_middleware.py\n@@ -39,7 +39,7 @@ def close_spider(self, spider):\n         pass\n \n     def __init__(self):\n-        raise NotConfigured\n+        raise NotConfigured(\"foo\")\n \n \n class TestMiddlewareManager(MiddlewareManager):\ndiff --git a/tests/test_utils_deprecate.py b/tests/test_utils_deprecate.py\nindex 2d9210410d4..eedb6f6af9c 100644\n--- a/tests/test_utils_deprecate.py\n+++ b/tests/test_utils_deprecate.py\n@@ -296,3 +296,7 @@ def test_unmatched_path_stays_the_same(self):\n             output = update_classpath(\"scrapy.unmatched.Path\")\n         self.assertEqual(output, \"scrapy.unmatched.Path\")\n         self.assertEqual(len(w), 0)\n+\n+    def test_returns_nonstring(self):\n+        for notastring in [None, True, [1, 2, 3], object()]:\n+            self.assertEqual(update_classpath(notastring), notastring)\n", "problem_statement": "NotConfigured logging breaks when the component is added by class object\nAs the log message for components that raise `NotConfigured` with a message assumes `clsname` is an import path string, it raises an AttributeError when it's a class instance. https://github.com/scrapy/scrapy/blob/bddbbc522aef00dc150e479e6288041cee2e95c9/scrapy/middleware.py#L49\n", "hints_text": "", "created_at": "2023-06-14T14:17:09Z"}